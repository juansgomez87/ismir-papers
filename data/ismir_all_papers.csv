conference_index,Authors,Title,Year,Link,Authors with Affiliations,Abstract,first_country,first_aff_cat,first_aff_cat_UN,title_tsne_2d,abstract_tsne_2d,title_tsne_3d,abstract_tsne_3d,title_umap_2d,abstract_umap_2d,title_umap_3d,abstract_umap_3d
15,Perfecto Herrera-Boyer;Xavier Amatriain;Eloi Batlle;Xavier Serra,Towards Instrument Segmentation for Music Content Description: a Critical Review of Instrument Classification Techniques.,2000,https://doi.org/10.5281/zenodo.1416768,Perfecto Herrera+Audiovisual Institute - Pompeu Fabra University>ESP>education;Xavier Amatriain+Audiovisual Institute - Pompeu Fabra University>ESP>education;Eloi Batlle+Audiovisual Institute - Pompeu Fabra University>ESP>education;Xavier Serra+Audiovisual Institute - Pompeu Fabra University>ESP>education,"A system capable of describing the musical content of any kind of sound file or sound stream, as it is supposed to be done in MPEG7-compliant applications, should provide an account of the different moments where a certain instrument can be listened to. In this paper we concentrate on reviewing the different techniques that have been so far proposed for automatic classification of musical instruments. As most of the techniques to be discussed are usable only in ""solo"" performances we will evaluate their applicability to the more complex case of describing sound mixes. We conclude this survey discussing the necessity of developing new strategies for classifying sound mixes without a priori separation of sound sources.",ESP,education,Developed economies,"[9.638732, -22.524591]","[10.469433, -10.670507]","[13.665941, -6.8779383, -0.7025985]","[10.8167715, -2.1155171, -7.8257227]","[8.927084, 7.129373]","[8.505183, 3.859205]","[11.16401, 12.559836, 0.4202104]","[10.130916, 7.7189746, 10.110913]"
33,Alexandra L. Uitdenbogerd;Justin Zobel,Music Ranking Techniques Evaluated.,2000,https://doi.org/10.5281/zenodo.1414990,Alexandra L. Uitdenbogerd+RMIT University>AUS>education;Justin Zobel+RMIT University>AUS>education,"Several techniques have been proposed for matching melody queries to stored music. In previous work [2], we found that local alignment, a technique derived from bioinformatics, was more effective than the n-gram methods derived from information retrieval that are used in other work. In this paper we explore a broader range of n-gram techniques, and test them with both manual queries and queries automatically extracted from MIDI files. Our experiments show that alternative - indeed, simpler - n-gram matching techniques than those tested in earlier work can be as effective as local alignment; one highly effective technique is to simply count the number of 5-grams in common between query and stored piece of music. N-grams are particularly effective for short queries and manual queries, while local alignment is superior for the automatic queries. In our experiments we have used a database of 10 466 standard MIDI files down loaded from the internet. These were processed to extract their melodies as described in [1]. In that earlier paper, we used volunteer listeners to judge a small set of extracted melodies. We discovered that our simple algorithm that chooses notes starting at each instance to be melody notes and that we have named all mono, was the most successful in extracting melodies. However, we continued to test all of our melody extraction algorithms in our second paper [2] in the context of melodic similarity measurement. In this experiment we used three methods of melodic standardisation to produce the representations that would be used for matching. They were: contour, which uses a three-symbol alphabet to represent where a melody goes up, down or stays the same; modulo-12 interval (pitch distance between notes), which keeps the size of intervals and their direction but reduced to one octave; and exact interval, which keeps the exact interval size and direction. All these represented pitch information only. A set of 28 automatically extracted melody queries were used to locate other versions of the same piece of music. The results were evaluated by computing the eleven-point precision average and the precision at a recall level of 20. Again the all mono method was successful, as were the entropy channel and all channels methods. The entropy channel method applies the all mono technique to each channel and then selects the melody channel based on its first order predictive entropy. The all channels technique also applies the all mono algorithm to each channel and then retains them all for the matching process. In addition, we confirmed that contour was insufficient for good matching, but that the use of interval sequences worked well. Queries of 30 intervals produced good answers. The similarity measurement techniques tested were local alignment, longest common subsequence, an n-gram counting technique that included the frequency of occurrence of each n-gram, and the Ukkonen n-gram measure. Local alignment was a clear winner. However, local alignment is a slow technique. It would be useful to have a good n-gram-based technique as indexes can be built to speed up matches. This led to our current exploration of n-grams. In this current set of experiments we used the same experimental framework as that used in [2], but also tested a set of manually produced queries. These manual queries were created by a classically trained pianist with perfect pitch, who listened to the source MI DI files and played a melodic fragment to represent each piece. We explored the variables of n-gram length, n-gram counting method, song-length normalisation, melody extraction and query length. The results clearly showed that n-grams of length five work best using the count distinct method, where term",AUS,education,Developed economies,"[-21.08313, 19.704937]","[11.746189, 14.645237]","[-11.124107, 4.015583, -3.8157134]","[7.405152, -9.796072, 11.801821]","[13.803839, 8.953736]","[9.071015, 0.720461]","[14.181225, 14.754733, -1.2784551]","[10.562432, 6.0801897, 13.047316]"
32,Alexandra L. Uitdenbogerd,"Music IR: Past, Present, and Future.",2000,https://doi.org/10.5281/zenodo.1417545,Alexandra L. Uitdenbogerd+RMIT University>AUS>education;Abhijit Chattaraj+RMIT University>AUS>education;Justin Zobel+RMIT University>AUS>education,"Music Information Retrieval has a longer history than most people realise, with systems developed in the 1960's. The field has its roots in information retrieval, musicology and music psychology. Information retrieval has provided us with a framework for evaluating retrieval systems. Musicologists have applied various techniques to measure stylistic parameters of composers and general similarity of musical works. Music perception research has taught us that contour is the most important feature of melody for listeners (Dowling 1978). Precursors to computerised music IR are the incipit and theme indexes such as Barlow and Morganstern's dictionary of musical themes (Barlow and Morganstern 1948), however, on-line collections with incipit indexes were soon to follow (for example Hudson 1970). The current trend in music IR research has been to develop systems that allow the location of answers to queries presented as hummed melodies, or entered in some other way. The earliest system we have found was that developed by Yamamoto in 1988, which only allowed exact matches (Kageyama et al 1993). The nineties have shown a rapid increase in music IR research, with systems papers being published every year since 1993 and more than a dozen published in 1998 (eg. Uitdenbogerd and Zobel 1998, Blackburn and de Roure1998, Hewlett and Selfridge-Field 1998). In addition, interesting developments have occurred in the field of audio retrieval. Despite this growth of interest in the field, the comparison of systems is difficult because of the lack of a common data set, queries, or relevance judgements. MIR systems can be evaluated in a similar manner to other systems. As with traditional IR, we need to decide what is a query, what is an answer, and what is meant by relevance, that is, what constitutes similarity. The usual definition of an ad hoc music query is a melody fragment consisting of a sequence of notes. The relevance of answers produced by a system will depend on the user's need. For example, relevant answers for a search for copyright similarity may be different to those for a user who half remembers a song. In earlier work, we collected queries and relevance judgements from a collection of 10,466 MIDI files by assuming that alternative arrangements or performances of a piece chosen as a query were relevant, and used these to measure effectiveness with the standard techniques of eleven-point precision averages and precision at k pieces retrieved. These were used to determine which matching techniques were most successful. Whether these relevance judgements correspond to - or yield the same ranking of techniques as - human relevance judgements was an open question. In recent work, we collected manual queries by having an expert musician listen to MIDI files and create a melodic query for each file. The files were selected so as to allow comparison with our earlier work with automatic queries and relevance assessments. We also developed a system for collecting music relevance judgements from listeners to permit a more established approach to system evaluation. Users were presented with a MIDI file melody and were required to determine whether each answer was highly similar or not.",AUS,education,Developed economies,"[-29.916534, 34.342907]","[15.498054, 12.642212]","[-25.811138, 6.9877872, -11.399521]","[3.8832886, -1.6280199, 12.352969]","[14.455782, 8.034402]","[9.732415, 1.043556]","[14.684289, 14.390193, -1.9501102]","[11.1236, 6.1265793, 12.731263]"
29,Jochen Schimmelpfennig;Frank Kurth,MCML - Music Contents Markup Language.,2000,https://doi.org/10.5281/zenodo.1415526,Jochen Schimmelpfennig+University of Bonn>DEU>education;Frank Kurth+University of Bonn>DEU>education,"We present an XML-based description interface for various types of musical contents - MCML, Music Contents Markup Language. An application of MCML currently developed within our group is a music browser system which enables a content based navigation in digital music files. Another major application of a music contents annotation interface is the description and handling of query results to digital music libraries.",DEU,education,Developed economies,"[8.55994, 27.276123]","[3.86474, 39.77771]","[-14.357376, -9.850958, 15.875904]","[-8.290957, -7.412841, 21.34302]","[13.440669, 7.069021]","[10.175561, 0.14743622]","[13.961083, 12.947972, -1.738012]","[10.785785, 4.7973075, 12.033606]"
28,Perry Roland,XML4MIR: Extensible Markup Language for Music Information Retrieval.,2000,https://doi.org/10.5281/zenodo.1417167,Perry Roland+University of Virginia>USA>education,This paper evaluates the role of standards in information exchange and suggests the adoption of XML standards for music representation and meta-data to serve as the basis for music information retrieval.,USA,education,Developed economies,"[-12.428668, 27.349113]","[4.523566, 40.609066]","[-12.267823, 2.5963495, -13.674577]","[-7.4384623, -6.9264126, 23.506878]","[13.698008, 7.237609]","[10.374798, 0.14874618]","[13.995171, 13.425606, -1.8946698]","[10.958243, 4.7616854, 12.07949]"
27,Jeremy Pickens,A Comparison of Language Modeling and Probabilistic Text Information Retrieval Approaches to Monophonic Music Retrieval.,2000,https://doi.org/10.5281/zenodo.1415100,Jeremy Pickens+University of Massachusetts>USA>education,"With interest in music information retrieval increasing the need for retrieval systems unique to music is also growing. Despite its unique properties music shares many similarities with text. The goal of this paper is to explore some of the capabilities and limitations of current text information retrieval systems as applied to the task of music retrieval. Monophonic music is converted into text and retrieval experiments are run using two different text information retrieval systems in various configurations. Finally, we will discuss whether the techniques applied here are generalizable to the larger problem of polyphonic music retrieval.",USA,education,Developed economies,"[-13.038134, 18.647131]","[14.944968, 16.52254]","[-3.2039807, 3.349462, -12.708741]","[5.770429, -6.2460065, 11.230106]","[13.518663, 8.207725]","[9.621892, 0.95592177]","[13.495509, 14.665608, -1.7283866]","[11.060715, 6.0096354, 12.776555]"
26,Donald MacLellan;Carola Boehm,MuTaTeD'll: A System for Music Information Retrieval of Encoded Music.,2000,https://doi.org/10.5281/zenodo.1417755,Donald MacLellan+University of Glasgow>GBR>education;Carola Boehm+University of Glasgow>GBR>education,"""MuTaTeD'II started in November 1999, building on the results of the MuTaTeD project. Its aim is to design and implement a music information retrieval system with delivery/access services for encoded music. The prototype service will provide a user friendly, web-based search/browse/query interface to access musical content.""",GBR,education,Developed economies,"[-17.268696, 20.388]","[20.070362, 29.438957]","[-9.068663, 4.264379, -10.429024]","[1.994815, -2.9709115, 17.597]","[13.752458, 7.6873426]","[10.784867, 0.5124417]","[13.876815, 14.333927, -1.8784826]","[11.654167, 4.942326, 12.448415]"
24,Karen Lin;Tim Bell,Integrating Paper and Digital Music Information Systems.,2000,https://doi.org/10.5281/zenodo.1416544,Karen Lin+University of Canterbury>NZL>education;Tim Bell+University of Canterbury>NZL>education,"Active musicians generally rely on extensive personal paper-based music information retrieval systems containing scores, parts, compositions, and arrangements of published and hand-written music. Many have a bias against using computers to store, edit and retrieve music, and prefer to work in the paper domain rather than using digital documents, despite the flexibility and powerful retrieval opportunities available. In this paper we propose a model of operation that blurs the boundaries between the paper and digital domains, offering musicians the best of both worlds. A survey of musicians identifies the problems and potential of working with digital tools, and we propose a system using colour printing and scanning technology that simplifies the process of moving music documents between the two domains.",NZL,education,Developed economies,"[-23.166243, 32.35572]","[24.409008, 28.852146]","[-23.663504, 5.751279, -19.485672]","[5.2053246, -2.6449265, 23.809925]","[14.42902, 7.65641]","[10.7779455, 0.7174108]","[14.55399, 14.062808, -2.2484498]","[11.432012, 4.925524, 12.566525]"
23,Mary Levering,"Intellectual Property Rights in Musical Works: Overview, Digital Library Issues and Related Initiatives.",2000,https://doi.org/10.5281/zenodo.1416786,Mary Levering+U.S. Copyright Office>USA>facility,"Our nation’s Founding Fathers incorporated copyright principles into the U.S. Constitution in order to foster the creation and dissemination of intellectual works for the public good. The federal copyright law supports these goals by permitting authors to reap rewards from their creative efforts for limited periods of time, while also benefitting subsequent creators by allowing them to use and build on prior works under certain circumstances. Thus, in order to stimulate originality and creativity, creators and owners of musical works are granted an exclusive bundle of rights in their copyrighted musical works for the enrichment of our society. Awareness of these rights is essential for all potential users, even for academics and scholars who wish to use these musical works for educational or other research purposes. Many musical works, especially those embodied in phonorecords, CD-ROMs and other audio-visual formats, have multiple layers of intellectual property rights involved. Scholars and institutions must respect these rights when digitizing, transmitting, retrieving or otherwise using copyrighted musical works in electronic formats. Even when underlying musical compositions are in the public domain, the version being used may include new copyrightable authorship that must be considered. Such authorship could include new arrangements, revised lyrics or music, other editorial revisions of the musical work itself, or—where the work is fixed as a series of sounds—it could include all or some new sounds, a remix of sounds, and so forth.  Understanding concepts such as (1) the whole range of rightsholders’ exclusive rights in musical works, (2) the statutory limitations on these rights, including how the Fair Use doctrine can be applied reasonably for educational and scholarly purposes, (3) when authorization is needed, (4) who the rightsholders are, and (5) how to secure required permissions efficiently and easily, have all become increasingly important.",USA,facility,Developed economies,"[-25.31823, 26.169703]","[25.0421, 40.664543]","[-20.787596, 6.5714684, -12.219215]","[-0.7288918, 8.293602, 26.296135]","[14.656441, 7.863552]","[11.14471, 0.32327533]","[14.639123, 14.58915, -2.2705421]","[11.836839, 4.491305, 12.026153]"
22,Steve Larson,"Searching for Meaning: Melodic Patterns, Combinations, and Embellishments.",2000,https://doi.org/10.5281/zenodo.1415738,Steve Larson+University of Oregon>USA>education,"I am interested in the search for musical patterns -- not so much because I want to find particular patterns, but because I want to understand musical meaning and I believe that musical meaning is something that listeners create when they relate musical patterns to one another, and when they relate musical patterns to other sorts of patterns.     I describe a theory of musical meaning that argues that experienced listeners of tonal music hear musical motion metaphorically, as purposeful action within a dynamic field of musical forces (musical gravity, magnetism, and inertia). That theory has been used to clarify issues in Schenkerian theory, to analyze music in many styles, to improve the training of musicians, to account for experimental results in melodic expectation, to explain striking regularities in published analyses of tonal music, and even to illuminate the phenomenon of ""swing"" in jazz.     The assumptions of that theory generate a small set of patterns and pattern combinations. This small set of patterns crops up over and over again in tonal music. Recognizing these patterns and their significance requires seeing how they are embellished in particular pieces. I illustrate these patterns, their combinations and embellishments, and something about their meaning by analyzing several folk songs, including ""Ah, vous dirai-je, Maman"" and Mozart's variations on it.     The ubiquity of this small set of patterns raises interesting questions about searching for musical patterns, about the role of computers in information retrieval vs their role in musical artificial intelligence, and about musical meaning. My presentation ends with a series of such questions.",USA,education,Developed economies,"[5.4595075, 20.246162]","[-0.3218141, 15.919038]","[3.2308254, 2.9400306, 2.1860154]","[-5.6704974, -7.10618, 7.955931]","[12.141989, 9.810328]","[8.680236, 1.6961843]","[12.659465, 15.360146, -0.89833033]","[10.271118, 6.749064, 12.347376]"
21,Youngmoo E. Kim;Wei Chai;Ricardo García;Barry Vercoe,Analysis of a Contour-based Representation for Melody.,2000,https://doi.org/10.5281/zenodo.1416760,Youngmoo E. Kim+MIT Media Lab>USA>education;Wei Chai+MIT Media Lab>USA>education;Ricardo Garcia+MIT Media Lab>USA>education;Barry Vercoe+MIT Media Lab>USA>education,"Identifying a musical work from a melodic fragment is a task that most people are able to accomplish with relative ease. For some time now researchers have worked to give computers this ability as well, as it would be the cornerstone of any query-by-humming system. To accomplish this, it is reasonable to study how humans are able to perform this task, and to assess what features we use to determine melodic similarity. Research has shown that melodic contour is an important feature in determining melodic similarity, but it is also clear that rhythmic information is important as well. The goal of this research is to explore what variation of contour and rhythmic information can result in the most efficient, robust, and scalable representation for melody. We intend for this to be the basis of a query-by-humming system that will be used to test the validity of our proposed representation. The importance of melodic contour The literature suggests that a coarse melodic contour description is more important to listeners than strict intervals in determining melodic similarity. Experiments have shown that interval direction alone (i.e. the 3-level +/-/0 contour representation) is an important element of melody recognition. There is, of course, anecdotal and experimental evidence that humans use more than just interval direction (a 3-level contour) in assessing melodic similarity. In an experiment by Lindsay (1996), subjects were asked to repeat (sing) a melody that was played for them. He found that while there was some correlation between sung interval accuracy and musical experience, even musically inexperienced subjects were able to negotiate different interval sizes fairly successfully. From a practical standpoint, a 3-level representation will generally require longer queries to arrive at a unique match. Given the perceptual and practical considerations, we chose to explore finer (5- and 7-level) contour divisions for our representation. Proposed melody representation We used a triple <T P B> to represent each melody, where T is the time signature of the song, P is the pitch contour vector, and B is the beat number vector. The range of values of P vary depending on the number of levels of contour used, but follow the pattern of 0, +, -, ++, --, +++, etc. The first value of B is the location of the first note within its measure in beats (according to the time signature). Successive values of B are incremented according to the number of beats between successive notes. Values of B are quantized to the nearest whole beat. Additionally, we used a vector Q to represent different contour resolutions and quantization boundaries. The length of Q indirectly reveals the number of levels of contour being used, and the individual values of Q indicate the absolute value of the quantization boundaries (in number of half-steps). For example, Q = [0 1] represents that we quantize interval changes into three levels, 0 for no change, + for an ascending interval (a boundary at one half-step or more), and - for a descending interval. This representation is equivalent to the popular +/-/0 or U/D/R (up/down/repeat) representation. Q = [0 1 3] represents a quantization of intervals into five levels, 0 for no change, + for an ascending half-step or whole-step (1 or 2 half-steps), ++ for ascending at least a minor third (3 or more half-steps), - for a descending half-step or whole-step, and -- for a descent of at least a minor third. Thus far, we have assembled a data set of 50 multi-track MIDI files, containing a mixture of popular and classical music. The popular music selections span a variety of different countries. All selected songs had a separate monophonic melody sound track.",USA,education,Developed economies,"[6.6502814, -5.1510515]","[4.800372, -7.2851152]","[8.389119, 5.729413, -3.6056683]","[6.356943, -8.480327, 0.1519745]","[10.6945915, 10.038485]","[8.228821, 1.5286275]","[11.530977, 15.253802, -0.8568794]","[9.955914, 6.9496713, 12.619792]"
20,Francis J. Kiernan,Score-based Style Recognition Using Artificial Neural Networks.,2000,https://doi.org/10.5281/zenodo.1416626,Francis J. Kiernan+University of Jyväskylä>FIN>education,"The original idea was to develop a system for musicological analysis that was capable of assisting in the resolution of issues concerning compositional authenticity. Based on explicit rule-based interrogation of a musical score the system gathers statistical information by way of a data extraction engine, the subsequent neural network implicitly forms an abstract impression of habitual characteristics within the composition. It must not be assumed that this system aims towards the modeling of human musical perception, as it is the author’s belief that score-based analyses cannot adequately meet this task. Initially the system was developed to explore the authenticity of flute compositions attributed to Frederick II “The Great”. Since there has long been musicological debate concerning this matter it was decided to acquire information from both performers of period instruments and musicologists concerning the characteristics and signatures required in the discrimination task. Based on free and multiple-choice reports returned a rule base was compiled that was then implemented into the system.",FIN,education,Developed economies,"[-23.582691, -18.185034]","[14.915331, -29.9411]","[-8.796387, 4.5865006, 22.835827]","[-1.5461384, -4.601665, 7.9697695]","[12.4354, 10.684975]","[8.773122, 1.7174653]","[13.41888, 14.15951, 1.2209154]","[10.285677, 6.3832107, 12.106053]"
19,Kjell Lemström;Sami Perttu,SEMEX - An efficient Music Retrieval Prototype.,2000,https://doi.org/10.5281/zenodo.1415908,Kjell Lemström+University of Helsinki>FIN>education;Sami Perttu+University of Helsinki>FIN>education,"We present an efficient prototype for music information retrieval. The prototype uses bit-parallel algorithms for locating transposition invariant matches of monophonic query melodies within monophonic or polyphonic music stored in a database. When dealing with monophonic music, we employ a fast approximate bit-parallel algorithm with special edit distance metrics. The fast scanning phase is succeeded by verification where a separate metrics is used for ranking matches. We also offer the possibility to search for exact occurrences of a ‘distributed’ melody within polyphonic databases via a bit-parallel filtering technique. In our experiments with a database of 2 million musical elements (notes in a monophonic and chords in a polyphonic database) the responses were obtained within one second in both cases. Furthermore, our prototype is capable of using various interval classes in matching, producing more approximation when it is needed.",FIN,education,Developed economies,"[-15.739672, 21.079796]","[11.167334, 16.032948]","[-8.274893, 4.7271357, -12.719211]","[8.574042, -9.952887, 9.239732]","[13.697893, 7.84944]","[9.078705, 0.7097218]","[13.786249, 14.480589, -2.0033696]","[10.712091, 6.164553, 13.221601]"
18,Özgür Izmirli,Using a Spectral Flatness Based Feature for Audio Segmentation and Retrieval.,2000,https://doi.org/10.5281/zenodo.1416438,Ozgur Izmirli+Connecticut College>USA>education,"A method that utilizes a spectral flatness based tonality feature for segmentation and content-based retrieval of audio is outlined. The method uses the tonality measure which is derived from the discrete bark spectrum as a means of detecting transitions between tonal and noise-like parts of the audio input. The meaning of ‘tonal’ in this context is different from the music-theoretical meaning and implies that there are dominant sinusoidal components in the spectrum, but, does not indicate that they are consonant or harmonic in any sense. Segmentation is performed by determining the times of these transitions, hence providing reference points for search purposes. Search is carried out by pivoting the query information on these reference points. The cumulative distance between the tonality pattern in successive frames of the query and the candidate sound fragments is used as a measure of similarity. In order to quantify the tonality, the input is processed as follows: the signal is sampled at 22050 Hz and a 2048-point FFT is performed on each frame using a Hanning analysis window. The window is hopped every 71 msecs. which corresponds to approximately 30 % overlap with the previous window. A pre-emphasis filter is applied to compensate for the reduced sensitivity of the human ear at low frequencies. The bark-band filter outputs are calculated from the FFT output by integration of the power spectral density within the critical bands. This is followed by a stage in which tonality determination is carried out for each critical band. The Spectral Flatness Measure (SFM) and the corresponding tonality coefficient (Johnston 1988) are used to quantify the tonal quality, i.e. how much tone-like the sound is as opposed to being noise-like. SFM is defined by the ratio of the geometric mean to the arithmetic mean of the power spectral density components in each critical band. A tonality vector is defined to be the collection of tonality coefficients for a single frame. More specifically, the tonality vector contains a tonality coefficient for each critical band. In order to perform segmentation, the tonal-to-noise and the noise-to-tonal transitions are obtained. These transitions are calculated, as a sequence bj (j is the time index), from the rate of change of the smoothed tonality vector with respect to time. The smoothing occurs due to averaging of the tonality vectors across several frames. By detecting rapid changes in the summary tonality in either direction, segmentation decisions are made. The sequence bj is an indicator for the overall tonal quality change of the input signal. Whenever tonal inputs become dominant in the input signal, for example, with the start of a vocal or solo instrument, the value of the indicator increases during the transition. That is to say, an increase in the value of b (over time) is interpreted as a transition from a noisy part to a more tonal part. Conversely, the decrease in the value of b, possibly extending to the negative extreme, indicates a transition from tonal to noisy spectra. The search for an audio fragment is done by comparing a short sequence of tonality vectors in the database with the same number of tonality vectors in the search sequence. As an exhaustive search is very costly for this purpose, a selective search that uses segments corresponding to relatively high perceptual entropy is performed. The determination of these segments is performed by the use of the b sequence. The starting times of these segments are called anchor times and are classified as either ‘tonal-to-noise’, or, ‘noise-to-tonal’. The noise-to-tonal anchor",USA,education,Developed economies,"[-13.115867, -16.966888]","[-4.9501157, -5.3920226]","[3.2266312, -6.2193117, -13.075151]","[9.714668, -17.012873, -0.41363096]","[12.479443, 8.123608]","[7.5262594, 2.259179]","[12.905087, 13.862527, 0.46427634]","[10.24987, 7.8500366, 11.963174]"
34,Thomas von Schroeter;Shyamala Doraisamy;Stefan M. Rüger,From Raw Polyphonic Audio to Locating Recurring Themes.,2000,https://doi.org/10.5281/zenodo.1416124,"Thomas von Schroeter+Imperial College of Science, Technology and Medicine>GBR>education;Shyamala Doraisamy+University Putra Malaysia>MYS>education;Stefan M Rüger+Imperial College of Science, Technology and Medicine>GBR>education",We present research studies of two related strands in content-based music retrieval: the automatic transcription of raw audio from a single polyphonic instrument with discrete pitch (eg piano) and the location of recurring themes from a Humdrum score.,GBR,education,Developed economies,"[12.705043, 19.753632]","[13.497603, 12.098897]","[-7.482537, -14.688693, 14.4724]","[4.1559496, -7.402455, 11.439502]","[12.378005, 6.9206104]","[9.39879, 0.9782866]","[13.851605, 12.839136, -1.165224]","[10.829937, 6.1378055, 12.686251]"
17,Mari Itoh,Subject Search for Music: Quantitative Analysis of Access Point Selection.,2000,https://doi.org/10.5281/zenodo.1414950,Mari Itoh+Aichi Shukutoku University>JPN>education,"""""",JPN,education,Developed economies,"[-21.34049, 25.273132]","[50.850346, 42.74437]","[-16.121521, 11.514686, -13.620884]","[6.2571464, 3.9593842, 28.366924]","[14.460857, 7.9329348]","[-9.072144, 9.962547]","[14.452186, 14.856098, -2.232607]","[-11.688255, 1.7411269, 1.0097158]"
14,Michael Good,Representing Music Using XML.,2000,https://doi.org/10.5281/zenodo.1415032,Michael Good+Recordare>USA>company,"Why does the world need another music representation language? Beyond MIDI describes over 20 different languages or musical codes (Selfridge-Field, 1997). Most commercial music programs have their own internal, proprietary music representation and file format. Music's complexity has led to this proliferation of languages and formats. Sequencers, notation programs, analysis tools, and retrieval tools all need musical information optimized in different ways.  Yet no music interchange language has been widely adopted since MIDI. MIDI has contributed to enormous growth in the electronic music industry, but has many well-known limitations for notation, analysis, and retrieval. These include its lack of representation of musical concepts such as rests and enharmonic pitches (distinguishing Db from C#), as well as notation concepts such as stem direction and beaming. Other interchange formats such as NIFF and SMDL overcome these restrictions, but have not been widely adopted.  Successful interchange formats such as MIDI and HTML share a common trait that NIFF and SMDL lack. MIDI and HTML skillfully balance simplicity and power. They are simple enough for many people to learn, and powerful enough for many real-world applications. The simplicity makes it easy for software developers to implement the standards and to develop encoding tools for musicians. This helps circumvent the “chicken-and-egg” problem with new formats.  XML (Extensible Markup Language) is a World Wide Web Consortium (W3C) recommendation for representing structured data in text, designed for ease of usage over the Internet by a wide variety of applications. XML is a meta-markup language that lets designers and communities develop their own representation languages for different applications. Like HTML and MIDI, it balances simplicity and power in a way that has made it very attractive to software developers.  The common base of XML technology lets developers of new languages focus on representation issues instead of low-level software development. All XML-based languages can be processed by a variety of XML tools available from multiple vendors. Since XML files are text files, users of XML files always have generic text-based tools available as a lowest common denominator. XML documents are represented in Unicode, providing support for international score exchange.  MusicXML is an XML-based music interchange language. It represents common western musical notation from the 17th century onwards, including both classical and popular music. The language is designed to be extensible to future coverage of early music and less standard 20th and 21st century scores. Non-western musical notations would use a separate XML language. As an interchange language, it is designed to be sufficient, not optimal, for diverse musical applications. MusicXML is not intended to supersede other languages that are optimized for specific musical applications, but to support sharing of musical data between applications.  The current MusicXML software runs on Windows. As of September 2000, it reads 100% of the MuseData format plus portions of NIFF and Finale’s Enigma Transportable Files (ETF). It writes to Standard MIDI Files in Format 1, MuseData files, and Sibelius. The NIFF, ETF, and MIDI converters use XML versions of these languages as intermediate structures. MusicXML is defined using an XML Document Type Definition (DTD) at www.musicxml.com/xml.html. XML Schemas address some shortcomings of DTDs, but are not yet a W3C recommendation.",USA,company,Developed economies,"[9.54668, 27.458784]","[0.20159236, 37.606174]","[-13.30762, -9.552531, 13.8495]","[-14.355842, -8.320286, 20.232145]","[13.354231, 6.966847]","[8.718635, 0.81674373]","[13.910755, 12.797692, -1.6712852]","[9.988801, 5.4636087, 11.382741]"
13,Anastasia Georgaki;Spyros Raptis;Stelios Bakamidis,A Music Interface for Visually Impaired People in the WEDELMUSIC Environment. Design and Architecture.,2000,https://doi.org/10.5281/zenodo.1417291,Anastasia Georgaki+Institute for Language and Speech Processing>GRC>facility;Spyros Raptis+Institute for Language and Speech Processing>GRC>facility;Stelios Bakamidis+Institute for Language and Speech Processing>GRC>facility,"In this poster we present the architecture of a new music interface for blind musicians, integrated in the WEDELMUSIC environment which is under development at ILSP. Our scope is to facilitate the access of visually impaired persons to musical databases (scores, audio and MIDI files) via Internet and give them the possibility to edit and create musical scores.",GRC,facility,Developed economies,"[-24.533127, 30.79913]","[11.578398, 40.312904]","[-20.483105, 9.1967745, -21.794025]","[4.992688, -5.541192, 25.992022]","[14.513371, 7.407718]","[10.55791, 0.7173912]","[14.602788, 14.137755, -2.3553298]","[11.294466, 5.055646, 12.372545]"
12,Jonathan Foote,ARTHUR: Retrieving Orchestral Music by Long-Term Structure.,2000,https://doi.org/10.5281/zenodo.1416644,"Jonathan Foote+FX Palo Alto Laboratory, Inc.>USA>company","""We introduce an audio retrieval-by-example system for orchestral music. Unlike many other approaches, this system is based on analysis of the audio waveform and does not rely on symbolic or MIDI representations. ARTHUR retrieves audio on the basis of long-term structure, specifically the variation of soft and louder passages. The long-term structure is determined from envelope of audio energy versus time in one or more frequency bands. Similarity between energy profiles is calculated using dynamic programming. Given an example audio document, other documents in a collection can be ranked by similarity of their energy profiles. Experiments are presented for a modest corpus that demonstrate excellent results in retrieving different performances of the same orchestral work, given an example performance or short excerpt as a query.""",USA,company,Developed economies,"[-9.486128, 19.68928]","[13.505544, 7.200712]","[-3.1179492, 0.13519725, -13.249997]","[4.7845907, -5.134094, 7.0847445]","[13.065299, 7.5948343]","[9.642636, 1.7119008]","[13.366244, 14.108908, -1.8725343]","[10.939572, 6.487521, 12.432879]"
11,Jon W. Dunn,Beyond VARIATIONS: Creating a Digital Music Library.,2000,https://doi.org/10.5281/zenodo.1415148,Jon W. Dunn+Indiana University>USA>education,"This presentation will focus primarily on work being done at Indiana University in the area of digital music libraries, with some discussion of related efforts.  Indiana University’s VARIATIONS system serves as both an early example of a working digital library supporting music content and an early application of World Wide Web technologies to music. Since April 1996, the system has provided online access within the William and Gayle Cook Music Library to sound recordings from the library’s collections. Unlike many early university-based digital library projects whose primary goals were to provide broader access to unique and/or archival collections, VARIATIONS has built its digital collection from standard musical repertoire identified as central to the teaching mission of the Indiana University School of Music.  At present, the collection available in VARIATIONS encompasses over 6800 sound recording titles, representing a broad range of musical material reflecting the curriculum of the IU School of Music, including operas, songs, instrumental music, jazz, rock, and world music. Recordings are digitized at CD-quality by library staff and stored as both WAV-format “archival” copies and MPEG service copies for delivery to users. Users are able to locate sound recordings either by browsing reserve lists (organized by course number, instructor, composer, and title) or by searching IU’s online library catalog system, IUCAT. Sound files are streamed to users via an IBM VideoCharger server and a customized player application.  VARIATIONS has become an integral part of music library services and the instructional process within the School of Music at IU, but despite its success, VARIATIONS lacks many elements that one might expect to find in a “digital music library.” One such element is access to formats other than audio. In fact, the very name VARIATIONS was intended in part to convey the idea that musical information by its nature comes in many different formats (including sound, notation, time-based information, text, video) and that access to all of these formats in an integrated fashion is a desirable goal for a digital music library system.  Indiana University has recently embarked on a four-year project to dramatically expand upon the current VARIATIONS system to create a Digital Music Library with funding from the National Science Foundation and National Endowment for the Humanities as part of the federal Digital Library Initiatives – Phase 2 (DLI2) program. Lead Principal Investigator for the project is Dr. Michael A. McRobbie, Vice President for Information Technology and Professor of Philosophy and Computer Science at IU. This project, involving faculty from music, law, and library and information science, and librarians and staff from both the library system and information technology services department, centers around three main tasks: First, the project will create a Digital Music Library (DML) testbed system. Secondly, applications will be developed for music education and research based upon the collections and functionality provided by the DML testbed. Finally, the DML will be used as a foundation for research in the areas of instruction, usability, and intellectual property rights.",USA,education,Developed economies,"[-20.637938, 33.09988]","[22.361887, 30.335659]","[-21.568048, 6.862431, -16.377876]","[2.6014569, -4.473468, 20.443365]","[14.549867, 7.4276323]","[10.952816, 0.3615962]","[14.602131, 14.176312, -2.4077895]","[11.671691, 4.8577065, 12.552405]"
10,Maxime Crochemore;Costas S. Iliopoulos;Yoan J. Pinzón,Finding Motifs with Gaps.,2000,https://doi.org/10.5281/zenodo.1415986,Maxime Crochemore+Institut Gaspard-Monge>FRA>education;Wojciech Rytter+University of Warsaw>POL>education|University of Liverpool>GBR>education;Costas S. Iliopoulos+King's College London>GBR>education|Curtin University of Technology>AU>education;Yoan J. Pinzon+King's College London>GBR>education|Curtin University of Technology>AU>education,"This paper focuses on a set of string pattern-matching problems that arise in musical analysis, and especially in musical information retrieval. A musical score can be viewed as a string: at a very rudimentary level, the alphabet could simply be the set of notes in the chromatic or diatonic notation, or the set of intervals that appear between notes (e.g. pitch may be represented as MIDI numbers and pitch intervals as number of semitones). An important example of flexibility required in score searching arises from the nature of polyphonic music. Within a certain time span each of the simultaneously-performed voices in a musical composition does not, typically, contain the same number of notes. So ‘melodic events’ occurring in one voice may be separated from their neighbours in a score by intervening events in other voices. Since we cannot generally rely on voice information being present in the score we need to allow for temporal ‘gaps’ between events in the matched pattern. Typically, the magnitude of such a gap (which might be expressed as a maximum time value, or, more probably, as a maximum number of skipped event-time-slots) will be a parameter set by the user. In our mathematical treatment the allowance for gaps in the query and the score being searched is represented by the constant α. Fig. 1 shows a short example from a musical score in monophonic format in which we attempt to match a pattern y (also known as the ‘query’) within a music score t (that we will call the ‘text’). This pattern can only be matched by allowing gaps of up to two spaces between pitches in the pattern; Note that the matching of the pattern to the score can be actually ‘approximate’ (see Cambouropolos et al 1999), in that the difference between the pitches and the sequence of musical events could be bounded by a constant δ (for simplicity we set δ to be zero in this example). We can see that there is an occurrence of the pattern in the text, starting at position three because y1, y2, y3, y4 and y5 matches exactly x3, x5, x8, x9 and x11, respectively, with a sequence of gaps G=(g1=1, g2=2, g3=0, g4=1). g1 is the number of spaces between the first two matched pitches in the text (i.e. between (x3=8) and (x5=3)), g2 is the number of spaces between the second and the third matched pitches in the text, and so on. Clearly, this is a valid match because all the gaps are less than or equal to two, which was the given gap restriction. If we want to find matches with a gap of up to one, then this match won’t be a valid one. Fig. 2 shows a similar example but for δ=1 so that the matched pitches don’t need to be exact, an ‘error’ of up to 1 is now allowed. Therefore, the first pitch in the pattern (8) matches the third pitch in the text (7) because |8-7|=1 and that is less or equal to our allowed error of one. The second pitch in the pattern (3) matches the fifth pitch in the text (4) and so on. The sequence of gaps remains exactly the same. The problem of matching with gaps can be formally defined as follows: given a musical sequence x (call the ‘text’) and a motif y (call the ‘pattern’) find all occurrences of y in x such that yi = xji ∀ i ∈ {1..m}, where m is the length of y. Note that y occurs at position j1 of x with a gap sequence G=(g1, g2, …, gm-1), where gi =|ji - ji+1-1| ∀ i ∈ {1..m-1}. We will consider this problem under a variety of conditions: the motif matching can be either exact or approximate. The gaps can be bounded, unbounded or all the same length. We have designed efficient algorithms and implementations of all the above variants.",FRA,education,Developed economies,"[16.017332, 25.508003]","[8.040826, 15.195531]","[-3.8187091, -16.375349, 9.394367]","[5.2835956, -9.762678, 6.557484]","[11.545944, 7.4918556]","[8.518386, 1.0603523]","[12.424333, 13.162601, -0.72936803]","[10.177902, 6.5499654, 12.902507]"
9,Dave Cliff;Heppie Freeburn,Exploration of Point-Distribution Models for Similarity-based Classification and Indexing of Polyphonic Music.,2000,https://doi.org/10.5281/zenodo.1416572,Dave Cliff+Hewlett-Packard Labs>GBR>company;Heppie Freeburn+Hewlett-Packard Labs>GBR>company,"Similarity is an intuitive criterion for indexing and classification of digital audio files in music information retrieval systems. While significant work has been done on similarity-based approaches to monophonic music, methods for reliably dealing with databases of arbitrary polyphonic music remain elusive. In this paper we describe our ongoing research in exploring the use of high-order multivariate statistical techniques for similarity-based classification of polyphonic music in digital audio files. The statistical techniques we employ, known as point distribution models (PDMs), have recently proven to be of surprising value in computer vision research for rating visual similarity; here we are attempting to apply PDMs to musical similarity. This involves creating neural networks that approximate the statistical processing, to save on potentially explosive storage and processor requirements. This paper reports on work in progress: our results to date are inconclusive and somewhat negative. We describe our rationale for exploring PDMs in polyphonic music similarity-rating and discuss the problems we have encountered so far, with the intention of encouraging other members of the music information retrieval community to explore this and related approaches.",GBR,company,Developed economies,"[-9.398507, 14.665601]","[23.440603, 6.4974003]","[-2.7940886, 6.9881663, -6.2657743]","[12.491193, 0.59966373, 7.7283726]","[12.699926, 8.828303]","[10.385848, 2.156385]","[13.275954, 14.56519, -0.49637812]","[11.986887, 6.565396, 12.488695]"
8,Michael Clausen;R. Engelbrecht;D. Meyer;J. Schmitz,PROMS: A Web-based Tool for Searching in Polyphonic Music.,2000,https://doi.org/10.5281/zenodo.1417139,"M. Clausen+Institut für Informatik V, Universität Bonn>DEU>education;R. Engelbrecht+Institut für Informatik V, Universität Bonn>DEU>education;D. Meyer+Institut für Informatik V, Universität Bonn>DEU>education;J. Schmitz+Institut für Informatik V, Universität Bonn>DEU>education","One major task of a digital music library (DML) is to provide techniques to locate a queried musical pattern in all pieces of music in the database containing that pattern. For a survey of several computational tasks related to this kind of data retrieval we refer to Crawford et al. [3]. Existing DMLs like MELDEX [1], Themefinder [4], and the Sonoda-Muraoka-System [7] work with melody databases relying on score-like information. Retrieval and matching are performed in a fault-tolerant way by string-based methods which mainly take into account pitch information. Generally, rhythm plays only a subordinate role. The music dictionary of Barlow and Morgenstern [2] shows that music retrieval based on pitch information only leads to results with typically too many false matches. (An example of such absurd matches is given in Selfridge-Field [6], p. 27.) We are convinced that both pitch and rhythm are crucial for recognizing melodies. In the more general context of polyphonic music, one is even forced to consider pitch and rhythm information. PROMS, a web-based computer-music service under development at the University of Bonn, Germany, is part of the MiDiLiB project [5]. The aim of PROMS is to design and to implement PROcedures for Music Search. Our discussion will take place in a rather general setting: we assume that our database contains several kinds of music such as polyphonic and homophonic music as well as melodies. We also use score-like information. A query to the database is a fragment of a piece of music. This could be a melody or a certain figure of an accompaniment. The task is now to locate efficiently all occurrences of this fragment in all pieces of music in the database. We have designed and implemented a web-based computer-music service that enables searching in polyphonic music. In contrast to the above mentioned systems, PROMS is not string-based but set-oriented. The basic objects within the PROMS system are single notes, specified by its onset time t, its pitch p, and its duration d. A piece of music is then a finite set M of notes. Our database consists of a sequence of N pieces of music in the MIDI format: D1, . . . , DN. Similarly, a query is itself a finite set Q of notes. Thus there is no principal difference between a piece Di of music in the database and a query Q. However, typically, Di is much larger than Q. An occurrence is a pair.",DEU,education,Developed economies,"[-11.97396, 18.216568]","[12.007516, 14.096686]","[-3.4498467, 0.044335626, -10.519446]","[6.010917, -9.639091, 11.959553]","[13.136006, 7.909702]","[8.879776, 0.9158021]","[13.060937, 14.274008, -1.8160706]","[10.364813, 6.1821275, 12.669698]"
7,G. Sayeed Choudhury;M. Droetboom;Tim DiLauro;Ichiro Fujinaga;Brian Harrington,Optical Music Recognition System within a Large-Scale Digitization Project.,2000,https://doi.org/10.5281/zenodo.1415730,G. Sayeed Choudhury+Johns Hopkins University>USA>education;Tim DiLauro+Johns Hopkins University>USA>education;Michael Droettboom+Johns Hopkins University>USA>education;Ichiro Fujinaga+Johns Hopkins University>USA>education;Brian Harrington+Johns Hopkins University>USA>education;Karl MacMillan+Johns Hopkins University>USA>education,"An adaptive optical music recognition system is being developed as part of an experiment in creating a comprehensive framework of tools to manage the workflow of large-scale digitization projects. This framework will support the path from physical object and/or digitized material into a digital library repository, and offer effective tools for incorporating metadata and perusing the content of the resulting multimedia objects.",USA,education,Developed economies,"[39.52982, 20.754063]","[-18.746513, 39.059776]","[18.599133, 13.27709, 11.19356]","[-11.108878, -22.413582, 5.927855]","[8.643593, 6.2125106]","[6.6591935, -0.50403786]","[10.684905, 11.111785, -0.115603276]","[8.024021, 4.2575035, 10.663429]"
6,Arbee L. P. Chen,"Music Representation, Indexing and Retrieval at NTHU.",2000,https://doi.org/10.5281/zenodo.1417981,Arbee L. P. Chen+National Tsing Hua University>TWN>education,"""In this extended abstract, our work on the representation, indexing and retrieval of music data is summarized. We treat the rhythm, melody, and chords of a music object as music features and develop various data structures and algorithms to efficiently perform approximate and partial matching for the retrieval of music data. In we present the techniques for retrieving songs by music segments. A music segment consists of a segment type and the associated beat and pitch information. We also propose multi-feature index structures for exact and approximate searching on different features. The problem of feature extraction is also studied. The repeating pattern is defined as a sequence of notes, which appears more than once in music objects. Choosing repeating patterns as the feature to represent the music objects meets both efficiency and semantic-richness requirements for content-based music data retrieval. We propose approaches to efficiently discover the repeating patterns of music objects. We have also implemented Muse, a prototype system for content-based music data retrieval to illustrate the feasibility of the concepts we propose.""",TWN,education,Developing economies,"[-15.728897, 24.683945]","[4.463617, 14.741144]","[-7.321328, 6.9821067, -15.99734]","[1.7216583, -8.229279, 7.143534]","[13.853738, 7.823309]","[9.394839, 1.3095727]","[13.781053, 14.552768, -2.1356497]","[11.025979, 6.5507245, 12.648452]"
5,Wei Chai;Barry Vercoe,Using User Models in Music Information Retrieval Systems.,2000,https://doi.org/10.5281/zenodo.1415898,Wei Chai+Massachusetts Institute of Technology>USA>education;Barry Vercoe+Massachusetts Institute of Technology>USA>education,"Most websites providing music services only support category-based browsing and/or text-based searching. There has been some research to improve the interface either for pull applications, e.g. query-by-humming systems, or for push applications, e.g. collaborative-filtering-based or feature-based music recommendation systems. However, for content-based search or feature-based filtering systems, one important problem is to describe music by its parameters or features, so that search engines or information filtering agents can use them to measure the similarity of the target (user’s query or preference) and the candidates. MPEG7 (formally called “Multimedia Content Description Interface”) is an international standard, which describes the multimedia content data to allow universal indexing, retrieval, filtering, control, and other activities supported by rich metadata. However, the metadata about the multimedia content itself are still insufficient, because many features of multimedia content are quite perceptual and user-dependent. For example, emotional features are very important for multimedia retrieval, but they are hard to be described by a universal model since different users may have different emotional responses to the same multimedia content. We therefore turn to user modeling techniques and representations to describe the properties of each user, so that the retrieval will be more accurate. Besides, user modeling can be used to reduce the search space, make push service easier and improve the user interface. There are several important issues in user modeling for music information retrieval purpose or even more general multimedia retrieval. 1) How to model the user? User-programmed, machine-learning and knowledge-engineered methods can be used. 2) What information is needed to describe a user for music IR purpose? It may include both the user’s indirect information (e.g. age, sex, citizenship, education, music experience, etc.) and direct information (e.g. user’s interests, definition of qualitative features, appreciation habit, etc.). 3) How to represent, use and share the user model? Similar to MPEG7 concepts, we can use a standard language in text format to represent the user model, so that search engines or information filtering agents can use it to refine the result easily and efficiently, without repeating the long-time observation and learning of the user’s behavior. User modeling can be done on client-side or server-side. Issues including easy/hard to obtain the user information, hard/easy to use collaborative filtering model, far from/close to the music data, more/less privacy or safety, more scalable/higher load on the server, etc., need to be considered when choosing either of the paradigms. We adopted the client-side user modeling paradigm in our MusicCat system. It is an agent that allows the user to define contexts and corresponding features of music that he wants to hear in those contexts correspondingly. Besides, the user can also define qualitative features of music based on quantitative features. For examples, the user just needs to tell the agent what kind of music he prefers to hear at what kind of context, like “I need fast and exciting music when I’m happy”, “I need soft music to wake me up every morning at 8:00”, “I need slow classical music, when I’m thinking”, “I need rhythmic music when I’m walking”, etc. Or, the user can define qualitative features, like “Romantic music for me means slow music with titles or lyric including word love”, “My favorite music includes É” etc. Then, when the moment comes - the user tells the agent or a pre-defined time approaches, the agent can automatically, randomly and repeatedly choose music from the user’s collection according to the pre-defined constraints. So far, MusicCat uses a profile-based user interface. And only midi files are used in the system.",USA,education,Developed economies,"[-21.824614, 23.114119]","[17.098757, 27.078678]","[-13.697843, 9.369559, -10.6291065]","[0.21240519, -8.279894, 19.103298]","[14.704959, 8.168343]","[10.763026, 0.9381051]","[14.465402, 14.923702, -1.9245179]","[11.799878, 5.206955, 12.510064]"
0,David Bainbridge 0001,The role of Music IR in the New Zealand Digital Music Library project.,2000,https://doi.org/10.5281/zenodo.1416260,David Bainbridge+University of Waikato>NZL>education,"""This extended abstract describes the computer music work that forms part of the New Zealand Digital Library (NZDL) project. In keeping with the scope of the general project, the music work investigates data acquisition, retrieval, presentation and scalability. These parts are described in turn in the text below.""",NZL,education,Developed economies,"[-29.769665, 34.56978]","[23.085915, 32.71763]","[-24.80672, 6.77811, -12.867084]","[3.2053418, 0.33247894, 21.869654]","[14.584243, 7.717474]","[11.106307, 0.27463734]","[14.686086, 14.340962, -2.2321558]","[11.603721, 4.6776757, 12.386793]"
1,Eloi Batlle;Pedro Cano,Automatic Segmentation for Music Classification using Competitive Hidden Markov Models.,2000,https://doi.org/10.5281/zenodo.1416764,Eloi Batlle+Audiovisual Institute. Universitat Pompeu Fabra>ESP>education;Pedro Cano+Audiovisual Institute. Universitat Pompeu Fabra>ESP>education,"Music information retrieval has become a major topic in the last few years and we can find a wide range of applications that use it. For this reason, audio databases start growing in size as more and more digital audio resources have become available. However, the usefulness of an audio database relies not only on its size but also on its organization and structure. Therefore, much effort must be spent in the labeling process whose complexity grows with database size and diversity.  In this paper we introduce a new audio classification tool and we use its properties to develop an automatic system to segment audio material in a fully unsupervised way. The audio segments obtained with this process are automatically labeled in a way that two segments with similar psychoacoustics properties get the same label. By doing so, the audio signal is automatically segmented into a sequence of abstract acoustic events. This is specially useful to classify huge multimedia databases where a human driven segmentation is not practicable. This automatic classification allow a fast indexing and retrieval of audio fragments. This audio segmentation is done using competitive hidden Markov models as the main classification engine and, thus, no previous classified or hand-labeled data is needed. This powerful classification tool also has a great flexibility and offers the possibility to customize the matching criterion as well as the average segment length according to the application needs.  The first stage in a classification system is the parameterization part where some features are obtained from the raw audio signal. The aim of this parameterization stage is to calculate a set of values that represent the main characteristics of the audio samples. The nature of the parameterization is strongly dependent on the application since it will determine the set of abstract acoustic units and, therefore, the underlying structure of a certain sound. The main classification engine in our system is based on Hidden Markov Models (HMM). HMMs have proven to be a very powerful tool to statistically model a process that varies in time. The idea behind them is very simple. Consider a stochastic process from an unknown source and we only have access to its output in time. HMMs are well suited to model this kind of events. From this point of view, HMMs can be seen as a doubly embedded stochastic process with a process that is not observable (hidden process) and can only be observed through another stochastic process (observable process) that produces the time set of observations. However, after examining the audio segmentation problem, we can see that usual HMM training is completely useless because we do not have a priori information about the segmentation. In traditional HMM the training procedure uses a labeled observation sequence and parameter estimation is based on learning from examples. For example, in speech recognition, the training database is labeled with the phonemes of the speech sequences. By doing so, HMMs learn from examples of each phoneme and they use this knowledge to identify similar patterns in the recognition stage. But the problem we are dealing with in automatic music segmentation is very different (and by far more difficult). Our main goal is to blindly identify similar (in some sense) audio segments and therefore we do not have any previous knowledge to train traditional HMMs. Thus, this is clearly an example of HMMs unsupervised training. Competitive Hidden Markov Models have proved to be specially well suited for this kind of situations. CoHMM are different to HMM mainly in the training stage since the recognition stage (or classification) shares the common algorithm. In this paper we also show some results that prove that coHMM converge to a realistic segmentation architecture.",ESP,education,Developed economies,"[-1.3061064, -1.0872275]","[-1.5032098, -1.6346706]","[3.4867415, -2.4958825, 0.7791821]","[0.36822674, -2.4271657, -3.5753357]","[11.677076, 8.139409]","[8.137285, 3.0581703]","[12.523, 14.036536, 0.04079788]","[10.305152, 7.610666, 11.310007]"
2,Juan Pablo Bello;Giuliano Monti;Mark B. Sandler,Techniques for Automatic Music Transcription.,2000,https://doi.org/10.5281/zenodo.1414872,Juan Pablo Bello+King's College London>GBR>education;Giuliano Monti+King's College London>GBR>education;Mark Sandler+King's College London>GBR>education,"Two systems are reviewed than perform automatic music transcription. The first perform monophonic transcription using an autocorrelation pitch tracker. The algorithm takes advantage of some heuristic parameters related to the similarity between image and sound in the collector. The detection is correct between notes B1 to E6 and further timbre analysis will provide the necessary parameters to reproduce a similar copy of the original sound. The second system is able to analyse simple polyphonic tracks. It is composed of a blackboard system, receiving its input from a segmentation routine in the form of an averaged STFT matrix. The blackboard contents an hypotheses database, an scheduler and knowledge sources, one of which is a neural network chord recogniser with the ability to reconfigure the operation of the system, allowing it to output more than one note hypothesis at the time. Some examples are provided to illustrate the performance and the weaknesses of the current implementation. Next steps for further development are defined.",GBR,education,Developed economies,"[26.622974, -8.761928]","[-34.204117, 16.134735]","[15.558528, -3.9497423, 11.407757]","[-24.850887, -8.86832, -2.0988102]","[9.6119175, 7.6636634]","[6.425588, 3.5026903]","[11.828429, 12.205449, -0.08694154]","[9.427273, 8.456104, 11.649151]"
16,David Huron,Perceptual and Cognitive Applications in Music Information Retrieval.,2000,https://doi.org/10.5281/zenodo.1414794,David Huron+Ohio State University>USA>education,"Music librarians and cataloguers have traditionally created indexes that allow users to access musical works using standard reference information, such as the name of the composer or the title of the work. While this basic information remains important, these standard reference tags have surprisingly limited applicability in most music-related queries. Music is used for an extraordinary variety of purposes: the restaurateur seeks music that targets a certain clientele; the aerobics instructor seeks a certain tempo; the film director seeks music conveying a certain mood; an advertiser seeks a tune that is highly memorable; the physiotherapist seeks music that will motivate a patient; the truck driver seeks music that will keep him/her alert. Although there are many other uses for music, music’s preeminent functions are social and psychological. The most useful retrieval indexes are those that facilitate searching according to such social and psychological functions. Typically, such indexes will focus on stylistic, mood, and similarity information. In attempting to build such musical indexes, two general questions arise: (1) What is the best taxonomic system by which to classify moods, styles, and other musical characteristics? (2) How can we create automated systems that will reliably characterize recordings or scores? Internet-based music distribution has brought these two questions to the fore. In the case of proprietary musical databases, the second problem can be centrally managed, and perhaps addressed using manual methods. However, past experience with Internet access to text documents implies that non-proprietary decentralized indexing is likely to prove more popular and successful. That is, future music indexes will likely resemble web-wide search engines (like Infoseek or Google) rather than closed proprietary systems (like Beatscape or Encyclopedia Britannica). The problem of building musical web crawlers that traverse the web and index music-related files (such as MP3) should prove challenging. The enabling technology for such musical web crawlers will need to draw extensively on research in music perception and cognition. Consider two sample problems: music summarization and mood characterization. Music Summarization Before downloading or streaming an entire work, there is great benefit to hearing a brief illustrative excerpt — a musical equivalent of the “thumbnails” commonly used in electronic picture galleries. Not all",USA,education,Developed economies,"[-20.079245, 18.562965]","[17.2821, 10.666326]","[-12.120655, 9.701657, -7.1780276]","[3.718115, 0.9307898, 10.503527]","[14.028303, 8.2402935]","[10.954773, 1.2078608]","[14.074155, 14.717018, -1.7058105]","[12.443895, 4.8571463, 11.607488]"
3,Daniel Bendor;Mark B. Sandler,Time Domain Extraction of Vibrato from Monophonic Instruments.,2000,https://doi.org/10.5281/zenodo.1416810,Daniel Bendor+University of Maryland at College Park>USA>education;Mark Sandler+King's College London>GBR>education,"""""",USA,education,Developed economies,"[51.325092, 18.005583]","[50.511845, 42.987965]","[9.090557, -28.765938, 14.911289]","[4.4389615, 4.1239285, 30.739704]","[10.904262, 7.1609716]","[-9.075183, 9.965491]","[11.585205, 13.077238, -0.2275241]","[-11.724269, 1.7771337, 1.0457213]"
4,Alain Bonardi,IR for Contemporary Music: What the Musicologist Needs.,2000,https://doi.org/10.5281/zenodo.1415912,Alain Bonardi+IRCAM>FRA>facility,"Active listening is the core of musical activity Listening does not only concern receiving musical information. On the contrary, it is “active” and based on a set of interactions between listeners and musical documents—including automatic music information research and extraction—so as to discover intentions. This recognition process is based on the observation of regularities and rules, in order to build “forms” from all indications, information and redundancies. The listener interprets all the signs that are meaningful for him as intentions, attributed to the composer. Features of computer assisted listening Let us specify further the active listening situation for the musicologist, taking for example the consultation of a document in such a digital library as IRCAM’s. The musicologist is facing a computer screen, while handling scores and books. This terminal allows him, among many other possibilities, to listen to music, to access musical data bases and hypermedia analyses. The musicologist is handling several devices on several media at the same time. First of all, the listener needs a framework that takes him/her into account. The purpose is to set the conditions of possibility of listening by restricting the heuristics of “forms”. It is therefore necessary to set a listening framework for the musicologist, to assist him in discovering the “intentions” of music. The main feature of this listening environment is thus its capacity to enable its user to vary the music representation. In the same way that working on a musical piece leads us either to read it silently or to sol-fa, or to hum it, or to play it, the musicologist’s environment must enable rapid changes of the representation of abstract objects. This is very important: a critical part of the analysis work consists in associating varied representations and contexts. Its purpose is the emergence of meaning from numerous and dissimilar elements that views imagined by the musicologist manage to reconcile. Musical databases help weave these links. In such an environment as the IRCAM Digital Library, we can either: •= associate various representations of music: hypermedia analyses offer sonograms as well as scores or formal schemes. •= or associate various contexts: the musicologist can easily know which works are contemporary with the Marteau sans Maître by Boulez using roughly the same instruments, or explore the musical production of the year 1954.",FRA,facility,Developed economies,"[-29.923605, 34.442673]","[33.613407, 36.222233]","[-26.092415, 6.283525, -11.976058]","[-0.26400936, 15.397518, 17.425165]","[14.621383, 7.9065433]","[11.496757, 0.7030142]","[14.690663, 14.499652, -2.0212383]","[12.284814, 4.5822825, 11.783006]"
7,Michael Droettboom,Expressive and Efficient Retrieval of Symbolic Musical Data.,2001,https://doi.org/10.5281/zenodo.1417741,Michael Droettboom+The Peabody Institute The Johns Hopkins University>USA>education;Ichiro Fujinaga+The Peabody Institute The Johns Hopkins University>USA>education;Karl MacMillan+The Peabody Institute The Johns Hopkins University>USA>education;Mark Patton+Milton S. Eisenhower Library The Johns Hopkins University>USA>education;James Warner+Milton S. Eisenhower Library The Johns Hopkins University>USA>education;G. Sayeed Choudhury+Milton S. Eisenhower Library The Johns Hopkins University>USA>education;Tim DiLauro+Milton S. Eisenhower Library The Johns Hopkins University>USA>education,"The ideal content-based musical search engine for large corpora must be both expressive enough to meet the needs of a diverse user base and efficient enough to perform queries in a reasonable amount of time. In this paper, we present such a system, based on an existing advanced natural language search engine. In our design, musically meaningful searching is simply a special case of more general search techniques. This approach has allowed us to create an extremely powerful and fast search engine with minimal effort.",USA,education,Developed economies,"[16.696657, 16.277407]","[18.960897, 19.399214]","[-3.9978387, -9.144887, 17.00861]","[7.897114, -5.7961, 16.109922]","[11.927837, 7.14139]","[10.272395, 0.81917894]","[13.497998, 12.484231, -1.0860176]","[11.476118, 5.5837946, 12.844425]"
19,Josh Reiss,Efficient Multidimensional Searching Routines.,2001,https://doi.org/10.5281/zenodo.1415546,"Josh Reiss+Queen Mary, University of London>GBR>education;Jean-Julien Aucouturier+Sony Computer Science Laboratory>FRA>company;Mark Sandler+Queen Mary, University of London>GBR>education","The problem of Music Information Retrieval can often be formalized as “searching for multidimensional trajectories”. It is well known that string-matching techniques provide robust and effective theoretic solutions to this problem. However, for low dimensional searches, especially queries concerning a single vector as opposed to a series of vectors, there are a wide variety of other methods available. In this work we examine and benchmark those methods and attempt to determine if they may be useful in the field of information retrieval. Notably, we propose the use of KD-Trees for multidimensional near-neighbor searching. We show that a KD-Tree is optimized for multidimensional data, and is preferred over other methods that have been suggested, such as the K-Tree, the box-assisted sort and the multidimensional quick-sort.",GBR,education,Developed economies,"[-0.016309557, 36.468872]","[21.735744, 15.517651]","[-9.037191, -7.287406, -23.550074]","[11.949866, -7.46544, 7.6920233]","[14.618459, 6.2849417]","[9.839369, 1.0388297]","[13.289682, 15.170532, -2.7399008]","[11.347389, 6.177727, 13.059272]"
18,Christopher Raphael,Automated Rhythm Transcription.,2001,https://doi.org/10.5281/zenodo.1416122,"Christopher Raphael+University of Massachusetts, Amherst>USA>education","We present a technique that, given a sequence of musical note onset times, performs simultaneous identification of the notated rhythm and the variable tempo associated with the times. Our formulation is probabilistic: We develop a stochastic model for the interconnected evolution of a rhythm process, a tempo process, and an observable process. This model allows the globally optimal identification of the most likely rhythm and tempo sequence, given the observed onset times. We demonstrate applications to a sequence of times derived from a sampled audio file and to MIDI data.",USA,education,Developed economies,"[42.70018, -0.87760556]","[-19.39483, -3.4238646]","[8.693931, -15.625371, 21.767105]","[-1.5225698, 17.95217, -11.461649]","[9.628936, 6.736841]","[5.9729857, 1.994941]","[11.703643, 11.855949, -0.18372217]","[8.061947, 6.943407, 11.09022]"
17,Emanuele Pollastri,An Audio Front End for Query-by-Humming Systems.,2001,https://doi.org/10.5281/zenodo.1415056,Goffredo Haus+Università Statale di Milano>ITA>education;Emanuele Pollastri+Università Statale di Milano>ITA>education,"In this paper, the problem of processing audio signals is addressed in the context of query-by-humming systems. Since singing is naturally used as input, we aim to develop a front end dedicated to the symbolic translation of voice into a sequence of pitch and duration pairs. This operation is crucial for the effectiveness of searching for music by melodic similarity. In order to identify and segment a tune, well-known signal processing techniques are applied to the singing voice. After detecting pitch, a novel post-processing stage is proposed to adjust the intonation of the user. A global refinement is based on a relative scale estimated out of the most frequent errors made by singers. Four rules are then employed to eliminate local errors. This front end has been tested with five subjects and four short tunes, detecting some 90% of right notes. Results have been compared to other approximation methods like rounding to the nearest absolute tone/interval and an example of adaptive moving tuning, achieving respectively 74%, 80% and 44% of right estimations. A special session of tests has been conducted to verify the capability of the system in detecting vibrato/legato notes. Finally, issues about the best representation for the translated symbols are briefly discussed.",ITA,education,Developed economies,"[-4.019029, 37.330154]","[-0.42772183, -12.653387]","[-15.999582, -7.51814, -23.850292]","[7.5456038, -12.89591, -14.076833]","[14.860369, 6.143905]","[7.140944, 2.0176938]","[13.187408, 15.333151, -2.9170353]","[9.061629, 7.5887346, 11.675578]"
16,François Pachet,A Naturalist Approach to Music File Name Analysis.,2001,https://doi.org/10.5281/zenodo.1415856,François Pachet+Sony CSL-Paris>FRA>company;Damien Laigre+Sony CSL-Paris>FRA>company,"Music title identification is a key ingredient of content-based electronic music distribution. Because of the lack of standards in music identification – or the lack of enforcement of existing standards – there is a huge amount of unidentified music files in the world. We propose here an identification mechanism that exploits the information possibly contained in the file name itself. We study large corpora of files whose names are decided by humans without particular constraints other than readability, and draw various hypotheses concerning the natural syntaxes that emerge from these corpora. A central hypothesis is the local syntactic consistency, which claims that file name syntaxes, whatever they are, are locally consistent within clusters of related music files. These heuristics allow to parse successfully file names without knowing their syntax a priori, using statistical measures on clusters of files, rather than on parsing files on a strict individual basis. Based on these validated hypothesis we propose a heuristics-based parsing system and illustrate it in the context of an Electronic Music Distribution project.",FRA,company,Developed economies,"[1.0262933, 42.88107]","[-6.542706, 18.033007]","[-3.4411294, 17.86916, -19.452818]","[-6.976913, 8.12749, 9.603081]","[13.660009, 8.5399]","[9.052379, 1.5632889]","[13.8298, 14.866508, -1.5904202]","[10.484583, 6.117749, 12.239333]"
15,Donncha Ó Maidín,Score Processing For MIR.,2001,https://doi.org/10.5281/zenodo.1416442,Donncha S. Ó Maidín+University of Limerick>IRL>education;Margaret Cahill+University of Limerick>IRL>education,"The focus of this paper is on the design and use of a music score representation. The structure of the representation is discussed and illustrated with sample algorithms, including some from music information retrieval. The score representation was designed for the development of general algorithms and applications. The common container-iterator paradigm is used, in which the score is modelled as a container of objects, such as clefs, key signatures, time signatures, notes, rests and barlines. Access to objects within the score is achieved through iterators. These iterators provide the developer with a mechanism for accessing the information content of the score. The iterators are designed to achieve a high level of data hiding, so that the user is shielded from the substantial underlying complexity of score representation, while at the same time, having access to the score’s full information content.",IRL,education,Developed economies,"[-8.3168, 57.091915]","[7.6544337, 36.562996]","[-35.760464, -1.1285071, -3.5305686]","[-4.221817, -12.248378, 18.646393]","[13.580975, 4.7428994]","[10.149595, 0.5557912]","[15.00681, 11.123093, -1.4953526]","[10.88093, 5.229697, 12.296902]"
14,Takuichi Nishimura;Hiroki Hashiguchi;Junko Takita;J. Xin Zhang;Masataka Goto;Ryuichi Oka,Music Signal Spotting Retrieval by a Humming Query Using Start Frame Feature Dependent Continuous Dynamic Programming.,2001,https://doi.org/10.5281/zenodo.1417191,Takuichi Nishimura+Real World Computing Partnership>JPN>facility|National Institute of Advanced Industrial Science and Technology>JPN>facility;J. Xin Zhang+Media Drive Co.>JPN>company;Hiroki Hashiguchi+Real World Computing Partnership>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology>JPN>facility;Junko Takita+Mathematical Systems Inc.>JPN>company;Ryuichi Oka+Real World Computing Partnership>JPN>facility,"We have developed a music retrieval method that takes a humming query and finds similar audio intervals (segments) in a music audio database. This method can also address a personally recorded video database containing melodies in its audio track. Our previous retrieving method took too much time to retrieve a segment: for example, a 60-minute database required about 10-minute computation on a personal computer. In this paper, we propose a new high-speed retrieving method, called start frame feature dependent continuous Dynamic Programming, which assumes that the pitch of the interval start point is accurate. Test results show that the proposed method reduces retrieval time to about 1/40 of present methods.",JPN,facility,Developed economies,"[-11.193181, 22.714048]","[8.227638, 11.847661]","[-5.773378, -2.201508, -14.848753]","[7.807969, -15.999745, 10.223987]","[13.7763195, 7.5365877]","[8.8111725, 0.56196827]","[13.3348055, 14.868819, -2.3023353]","[10.364084, 6.010965, 13.241341]"
13,Colin Meek,Thematic Extractor.,2001,https://doi.org/10.5281/zenodo.1414828,Colin Meek+University of Michigan>USA>education;William P. Birmingham+University of Michigan>USA>education,"We have created a system that identifies musical keywords or themes. The system searches for all patterns composed of melodic (intervallic for our purposes) repetition in a piece. This process generally uncovers a large number of patterns, many of which are either uninteresting or only superficially important. Filters reduce the number or prevalence, or both, of such patterns. Patterns are then rated according to perceptually significant characteristics. The top-ranked patterns correspond to important thematic or motivic musical content, as has been verified by comparisons with published musical thematic catalogs. The system operates robustly across a broad range of styles, and relies on no meta-data on its input, allowing it to independently and efficiently catalog multimedia data.",USA,education,Developed economies,"[-7.7230062, 42.351646]","[16.607765, 10.584881]","[-9.079873, -15.846504, 14.820352]","[3.7554464, -0.32412443, 9.453642]","[12.434722, 6.8335805]","[9.6015215, 1.5724767]","[14.043223, 12.8405905, -1.0838891]","[11.124188, 6.405022, 12.382572]"
12,Adam Lindsay;Youngmoo Kim,"Adventures in Standardization, or How We Learned to Stop Worrying and Love MPEG-7.",2001,https://doi.org/10.5281/zenodo.1418071,Adam Lindsay+Lancaster University>GBR>education;Youngmoo Kim+MIT Media Lab>USA>facility,"The authors give a brief account of their combined 7+ years in multimedia standardization, namely in the MPEG arena. They discuss specifics on musical content description in MPEG-7 Audio and other items relevant to Music Information Retrieval among the MPEG-7 Multimedia Description Schemes. In the presentation, they will give a historical overview of the MPEG-7 standard, its motivations, and what led to its current state.",GBR,education,Developed economies,"[-18.434576, 43.02546]","[16.269218, 27.427135]","[-23.07044, 0.55447096, -24.880466]","[-0.7544309, -8.459718, 17.732676]","[13.718873, 7.1318936]","[10.725312, 0.9087522]","[14.0997095, 13.795794, -2.2134438]","[11.673317, 5.1647363, 12.505281]"
11,Andreas Kornstädt,The JRing System for Computer-Assisted Musicological Analysis.,2001,https://doi.org/10.5281/zenodo.1416100,Andreas Kornstädt+Universität Hamburg>DEU>education,"Among other factors, high complexity and mandatory expert computer knowledge make many music IR and music analysis systems unsuitable for the majority of largely computer-illiterate musicologists. The JRing system offers highly flexible yet intuitively usable search and comparison operations to aid musicologists during score analysis. This paper discusses the requirement analysis that led to JRing’s inception, its IR tools and graphical user interface plus the kind of musical material it works on and the Humdrum-based technical realization of IR operations.",DEU,education,Developed economies,"[-3.4260764, 26.718548]","[0.42491612, 31.602354]","[-17.992947, -12.472288, -1.5677012]","[-9.560889, -2.9124699, 16.996952]","[13.25422, 7.6847873]","[10.105995, 0.77211493]","[13.777947, 13.943125, -1.776215]","[10.663299, 5.30774, 11.844573]"
20,Richard P. Smiraglia,Musical Works as Information Retrieval Entities: Epistemological Perspectives.,2001,https://doi.org/10.5281/zenodo.1416512,Richard P. Smiraglia+Long Island University>USA>education,"Musical works form a key entity for music information retrieval. Explicit linkage of relationships among entities is critical for document-based information retrieval. Works contain representations of recorded knowledge. Core bodies of work—canons—function to preserve and disseminate the parameters of a culture. A musical work is an intellectual sonic conception. Musical works take documentary form in a variety of instantiations. Epistemology for documentary analysis provides key perceptual information about the objects of knowledge organization. Works are carriers of knowledge, representing deliberately-constructed packages of both rational and empirical evidence of human knowledge. Smiraglia (2001) suggests the parameters of a theory of the work, incorporating the tools of epistemology to comprehend works by expressing theoretical parameters in the context of a taxonomic definition. A work is a signifying, concrete set of ideational conceptions that finds realization through semantic or symbolic expression. Semiotic analysis suggests a variety of cultural and social roles for works. Musical works, defined as entities for information retrieval, are seen to constitute sets of varying instantiations of abstract creations. Variability over time, demonstrated empirically, is an innate aspect of the set of all instantiations of a musical work, leading to complexity in the information retrieval domain.",USA,education,Developed economies,"[-25.4474, 22.043184]","[24.951258, 41.47857]","[-17.17363, 10.383844, -5.5554686]","[-1.7632248, 9.939713, 25.821716]","[14.282163, 8.310516]","[10.876012, 0.5311713]","[14.46618, 14.765767, -1.6999862]","[12.097469, 4.8406024, 11.67796]"
10,Holger H. Hoos;Kai Renz;Marko Görg,GUIDO/MIR - an Experimental Musical Information Retrieval System based on GUIDO Music Notation.,2001,https://doi.org/10.5281/zenodo.1417517,Holger H. Hoos+University of British Columbia>CAN>education;Kai Renz+Technische Universität Darmstadt>DEU>education;Marko Görg+Unknown>Unknown>Unknown,"Musical databases are growing in number, size, and complexity, and they are becoming increasingly relevant for a broad range of academic as well as commercial applications. The features and performance of musical database systems critically depend on two factors: The nature and representation of the information stored in the database, and the search and retrieval mechanisms available to the user. In this paper, we present an experimental database and retrieval system for score-level musical information based on GUIDO Music Notation as the underlying music representation. We motivate and describe the database design as well as the flexible and efficient query and retrieval mechanism, a query-by-example technique based on probabilistic matching over a clustered dataset. This approach has numerous advantages, and based on experience with a first, experimental implementation, we believe it provides a solid foundation for powerful, efficient, and usable database and retrieval systems for structured musical information.",CAN,education,Developed economies,"[-17.9127, 21.248606]","[15.508062, 15.511224]","[-10.829288, 2.5371642, -8.876227]","[9.652685, -6.6989574, 12.129396]","[13.768959, 7.556025]","[9.916448, 0.68685263]","[13.759568, 14.437882, -1.9919183]","[11.184155, 5.661395, 12.889737]"
8,Adriane Durey,Melody Spotting Using Hidden Markov Models.,2001,https://doi.org/10.5281/zenodo.1415680,Adriane Swalm Durey+Georgia Institute of Technology>USA>education|Center for Signal and Image Processing>USA>facility;Mark A. Clements+Georgia Institute of Technology>USA>education|Center for Signal and Image Processing>USA>facility,"""""",USA,education,Developed economies,"[8.0076, -14.06655]","[50.723434, 43.19595]","[14.570627, 2.160745, -1.2341892]","[7.182327, 5.1557384, 31.429106]","[10.232099, 9.861052]","[-9.072253, 9.962645]","[11.129538, 14.897882, -0.5332705]","[-11.7171955, 1.7700518, 1.0386379]"
6,Matthew J. Dovey,A Technique for Regular Expression Style Searching in Polyphonic Music.,2001,https://doi.org/10.5281/zenodo.1416140,Matthew J. Dovey+Kings College London>GBR>education,"This paper discussed some of the ongoing investigative work on integrating these two systems conducted as part of the NSF/JISC funded OMRAS (Online Music Retrieval and Searching) project into polyphonic searching of music. It describes a simple and efficient “piano-roll” based algorithm for locating a polyphonic query within a large polyphonic text. It then describes ways in which this algorithm can be modified without affecting the performance to allow more freedom in the how a match is made, allowing queries which involve something akin to polyphonic regular expressions to be located in the text.",GBR,education,Developed economies,"[11.6052265, 9.10092]","[13.820562, 16.633926]","[4.5115485, -12.744738, 3.0756981]","[6.6230307, -7.484589, 11.028677]","[12.346002, 7.7203083]","[9.315253, 0.7069732]","[12.863929, 14.145341, -1.7434244]","[10.760758, 6.028297, 13.088692]"
5,Shyamala Doraisamy,An Approach Towards A Polyphonic Music Retrieval System.,2001,https://doi.org/10.5281/zenodo.1415622,Shyamala Doraisamy+Imperial College>GBR>education|Imperial College>GBR>education;Stefan M Rüger+Imperial College>GBR>education|Imperial College>GBR>education,"Most research on music retrieval systems is based on monophonic musical sequences. In this paper, we investigate techniques for a full polyphonic music retrieval system. A method for indexing polyphonic music data files using the pitch and rhythm dimensions of music information is introduced. Our strategy is to use all combinations of monophonic musical sequences from polyphonic music data. ‘Musical words’ are then obtained using the n-gram approach enabling text retrieval methods to be used for polyphonic music retrieval. Here we extend the n-gram technique to encode rhythmic as well as interval information, using the ratios of onset time differences between two adjacent pairs of pitch events. In studying the precision in which intervals are to be represented, a mapping function is formulated in dividing intervals into smaller classes. To overcome the quantisation problems that arise with using rhythmic information from performance data, an encoding mechanism using ratio bins is also adopted. We present results from retrieval experiments with a database of 3096 polyphonic pieces.",GBR,education,Developed economies,"[-11.811795, 18.982485]","[8.897177, 15.17538]","[-2.8376665, 1.7179137, -11.723494]","[5.6696777, -9.403512, 8.007938]","[13.104128, 7.965756]","[9.01114, 0.77131206]","[13.115353, 14.410698, -1.779412]","[10.551133, 6.3234954, 13.099538]"
4,Roger B. Dannenberg,Music Information Retrieval as Music Understanding.,2001,https://doi.org/10.5281/zenodo.1418263,Roger B. Dannenberg+Carnegie Mellon University>USA>education,"Much of the difficulty in Music Information Retrieval can be traced to problems of good music representations, understanding music structure, and adequate models of music perception. In short, the central problem of Music Information Retrieval is Music Understanding, a topic that also forms the basis for much of the work in the fields of Computer Music and Music Perception. It is important for all of these fields to communicate and share results. With this goal in mind, the author’s work on Music Understanding in interactive systems, including computer accompaniment and style recognition, is discussed.",USA,education,Developed economies,"[-19.124676, 19.221483]","[13.761141, 26.141527]","[-12.068742, 7.8476005, -8.103909]","[1.0377868, -4.496553, 14.260389]","[13.949683, 8.195049]","[10.229375, 1.0377145]","[14.125678, 14.616373, -1.7587733]","[11.100116, 5.5439625, 12.292294]"
3,Arbee L. P. Chen,Building a Platform for Performance Study of Various Music Information Retrieval Approaches.,2001,https://doi.org/10.5281/zenodo.1414822,Jia-Lien Hsu+National Tsing Hua University>TWN>education|Unknown>Unknown>Unknown;Arbee L. P. Chen+National Tsing Hua University>TWN>education|Unknown>Unknown>Unknown,"In this paper, we describe the Ultima project which aims to construct a platform for evaluating various approaches of music information retrieval. Three approaches with the corresponding tree-based, list-based, and (n-gram+tree)-based index structures are implemented. A series of experiments has been carried out. With the support of the experiment results, we compare the performance of index construction and query processing of the three approaches and give a summary for efficient content-based music information retrieval.",TWN,education,Developing economies,"[-19.98639, 22.190794]","[17.97928, 18.929964]","[-13.3036, 6.199003, -11.3544235]","[6.6388865, -4.898709, 14.768543]","[14.010098, 7.7666707]","[10.192967, 0.8825338]","[14.094797, 14.632918, -2.051547]","[11.458549, 5.6413255, 12.814761]"
2,William P. Birmingham,MUSART: Music Retrieval Via Aural Queries.,2001,https://doi.org/10.5281/zenodo.1415810,William P. Birmingham+University of Michigan>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education;Gregory H. Wakefield+University of Michigan>USA>education;Mark Bartsch+University of Michigan>USA>education;David Bykowski+University of Michigan>USA>education;Dominic Mazzoni+University of Michigan>USA>education;Colin Meek+University of Michigan>USA>education;Maureen Mellody+University of Michigan>USA>education;William Rand+University of Michigan>USA>education,"MUSART is a research project developing and studying new techniques for music information retrieval. The MUSART architecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling, melodic contour, and phonetic streams for music retrieval. To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music. Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality.",USA,education,Developed economies,"[-16.098421, 21.504343]","[16.162294, 19.204319]","[-9.072239, 4.293395, -14.223046]","[4.0144334, -5.5602927, 13.861923]","[13.791472, 7.7958455]","[9.759351, 0.89042914]","[13.849345, 14.563913, -2.0903313]","[11.016435, 5.8181157, 12.788352]"
1,Jérôme Barthélemy,Figured Bass and Tonality Recognition.,2001,https://doi.org/10.5281/zenodo.1417161,Jerome Barthélemy+Ircam>FRA>facility;Alain Bonardi+Ircam>FRA>facility,"In the course of the WedelMusic project [15], we are currently implementing retrieval engines based on musical content automatically extracted from a musical score. By musical content, we mean not only main melodic motives, but also harmony, or tonality. In this paper, we first review previous research in the domain of harmonic analysis of tonal music. We then present a method for automated harmonic analysis of a music score based on the extraction of a figured bass. The figured bass is determined by means of a template-matching algorithm, where templates for chords can be entirely and easily redefined by the end-user. We also address the problem of tonality recognition with a simple algorithm based on the figured bass. Limitations of the method are discussed. Results are shown and compared to previous research. Finally, potential uses for Music Information Retrieval are discussed.",FRA,facility,Developed economies,"[-1.0542161, -18.995846]","[13.307689, 11.725919]","[0.90328413, -11.745604, 25.855562]","[3.3641245, -8.553055, 11.812615]","[11.040672, 9.200712]","[8.422718, 1.8226894]","[12.1032505, 13.597931, -0.2765822]","[10.28536, 7.406015, 12.236032]"
0,Eric Allamanche,Content-based Identification of Audio Material Using MPEG-7 Low Level Description.,2001,https://doi.org/10.5281/zenodo.1417853,Eric Allamanche+Fraunhofer IIS-A>DEU>facility;Bernhard Fröba+Fraunhofer IIS-A>DEU>facility;Jürgen Herre+Fraunhofer IIS-A>DEU>facility;Thorsten Kastner+Fraunhofer IIS-A>DEU>facility;Oliver Hellmuth+Fraunhofer IIS-A>DEU>facility;Markus Cremer+Fraunhofer IIS-A / AEMT>DEU>facility,"""Along with investigating similarity metrics between audio material, the topic of robust matching of pairs of audio content has gained wide interest recently. In particular, if this matching process is carried out using a compact representation of the audio content ('audio fingerprint'), it is possible to identify unknown audio material by means of matching it to a database with the fingerprints of registered works. This paper presents a system for reliable, fast and robust identification of audio material which can be run on the resources provided by today's standard computing platforms. The system is based on a general pattern recognition paradigm and exploits low level signal features standardized within the MPEG-7 framework, thus enabling interoperability on a world-wide scale. Compared to similar systems, particular attention is given to issues of robustness with respect to common signal distortions, i.e. recognition performance for processed/modified audio signals. The system's current performance figures are benchmarked for a range of real-world signal distortions, including low bitrate coding and transmission over an acoustic channel. A number of interesting applications are discussed.""",DEU,facility,Developed economies,"[-20.948566, -3.3339083]","[23.624714, -21.770517]","[-2.119987, -3.9282408, -19.281694]","[15.820188, -12.555976, 2.7200096]","[13.349869, 7.4748]","[8.452623, -0.0032911417]","[13.838056, 13.916641, -1.7612662]","[10.137038, 5.2971654, 12.918157]"
9,Ludger Hofmann-Engl,Towards a Cognitive Model of Melodic Similarity.,2001,https://doi.org/10.5281/zenodo.1417359,Ludger Hofmann-Engl+Keele University>GBR>education,"In recent years the interest in melodic similarity has mushroomed mainly due to the increased importance of music information retrieval (MIR). A great number of similarity models and algorithms have been developed, but little or no attention has been paid to cognitive or perceptual aspects to the issue at hand. Questions, about the relevant parameters and the appropriate implementation are under-researched as are experimental data. This paper focuses on the pitch aspect of melodic similarity, scrutinising the term pitch replacing it by a less ambivalent and overused term, which we will call meloton. Based on the term meloton the paper suggests to approach the issue of ‘melotonic’ similarity from a transformational angle, where transformations are executed as reflections and translations. ‘Melotonic’ similarity then is seen as related to the transformation process in form of a transpositional and interval vector. Finally, melotonic similarity as portrait in a psychological context emerges as a multi-facet phenomenon requiring the development of flexible models.",GBR,education,Developed economies,"[1.0931498, 17.583387]","[15.265126, 1.0869249]","[1.1449453, 5.1410303, 2.1908998]","[6.5952997, 2.363042, 7.511939]","[12.300758, 9.769702]","[9.411265, 1.9022108]","[12.791666, 15.499919, -0.76388747]","[11.271058, 6.9615927, 12.941281]"
21,George Tzanetakis,Automatic Musical Genre Classification of Audio Signals.,2001,https://doi.org/10.5281/zenodo.1415058,George Tzanetakis+Princeton University>USA>education;Georg Essl+Princeton University>USA>education;Perry Cook+Princeton University>USA>education,"Musical genres are categorical descriptions that are used to describe music. They are commonly used to structure the increasing amounts of music available in digital form on the Web and are important for music information retrieval. Genre categorization for audio has traditionally been performed manually. A particular musical genre is characterized by statistical properties related to the instrumentation, rhythmic structure and form of its members. In this work, algorithms for the automatic genre categorization of audio signals are described. More specifically, we propose a set of features for representing texture and instrumentation. In addition a novel set of features for representing rhythmic structure and strength is proposed. The performance of those feature sets has been evaluated by training statistical pattern recognition classifiers using real world audio collections. Based on the automatic hierarchical genre classification two graphical user interfaces for browsing and interacting with large audio collections have been developed.",USA,education,Developed economies,"[-25.740883, -15.2974]","[15.178445, -8.899568]","[-13.751584, 2.3677697, 19.140312]","[7.621663, 7.6170244, -6.1856647]","[12.70634, 10.79055]","[9.346835, 3.2692285]","[13.592223, 14.133029, 1.3083194]","[11.11829, 7.3078856, 10.88263]"
24,Hui Jin;H. V. Jagadish,Indexing Hidden Markov Models for Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1418259,Hui Jin+The University of Michigan>USA>education;H. V. Jagadish+The University of Michigan>USA>education,"""""",USA,education,Developed economies,"[-14.330331, 19.233833]","[49.873096, 43.57941]","[-6.4509954, 8.756528, -13.610239]","[5.3823586, 5.7825327, 28.739132]","[13.663946, 8.070743]","[-9.072841, 9.963215]","[13.648262, 14.716059, -1.9048886]","[-11.712908, 1.7657329, 1.0343124]"
23,Keiji Hirata;Shu Matsuda,Interactive Music Summarization based on GTTM.,2002,https://doi.org/10.5281/zenodo.1417481,Keiji Hirata+NTT Communication Science Laboratories>JPN>facility;Shu Matsuda+Digital Art Creation>JPN>company,"This paper presents a music summarization system called “Papipuun” that we are developing. Papipuun performs quick listening in a manner similar to a stylus skipping on a scratched record, but the skipping occurs correctly at punctuations of musical phrases, not arbitrarily. First, we developed a method for representing polyphony based on time-span reduction in the generative theory of tonal music (GTTM) and the deductive object-oriented database (DOOD). The operation, least upper bound, plays an important role in similarity checking of polyphonies represented in our method. Next, in a preprocessing phase, a user analyzes a set piece by the time-span reduction, using a dedicated tool called TS-Editor. For the real-time phase, the user interacts with the main system, Summarizer, to perform music summarization. Summarizer discovers a piece structure by means of similarity checking. When the user identifies the fragments to be skipped, Summarizer deletes them and concatenates the rest. Papipuun can produce a music summarization of good quality, reflecting the atmosphere of an entire piece through interaction with the user.",JPN,facility,Developed economies,"[-6.5477877, 19.605385]","[-9.135386, 24.134806]","[2.1647031, 11.563028, -12.591656]","[-14.833486, 3.428201, 10.289402]","[12.908726, 8.320251]","[9.007923, 1.4962895]","[13.370989, 14.339615, -1.0952734]","[10.171741, 6.0642734, 12.024011]"
22,Harriette Hemmasi,Why not MARC?,2002,https://doi.org/10.5281/zenodo.1417491,Harriette Hemmasi+Indiana University>USA>education,"Traditional library cataloging records in the United States, based on AACR2R cataloging rules and MARC standards, constitute a solid foundation for many of the descriptive metadata elements needed for searching and retrieving works of music. However, there are significant weaknesses associated with these records and the online environment in which they live as users seek access to digitized representations of music. While music metadata in the library catalog records offer less than a perfect solution, they can and should have an important role in the total solution. Variations, the Indiana University Digital Music Library, builds on the advantages of AACR2R and MARC and offers a domain-specific data model and search environment that address many of the identified problems.",USA,education,Developed economies,"[-10.158385, 45.98278]","[21.816347, 30.414255]","[-29.130505, -12.066742, 2.6472557]","[2.5519452, -2.982924, 20.011078]","[12.7330885, 6.80937]","[10.967798, 0.2783933]","[13.536018, 13.152612, -1.7706919]","[11.700998, 4.80022, 12.461388]"
0,Amar Chaudhary,An Extensible Representation for Playlists.,2002,https://doi.org/10.5281/zenodo.1415696,Amar Chaudhary+Creative Advanced Technology Ctr.>USA>company,"The increasing availability of digital music has created a greater need for methods to organize large collections of music. The eXtensible PlayList (XPL) representation allows users to express playlists with varying degrees of specificity. XPL handles references to exact files or URLs as well as rules for selecting content based on metadata constraints. XPL also allows the transitions between tracks in a playlist to be specified. This paper describes the features of XPL, a system for rendering XPL specifications and use of an advanced XPL renderer in an existing application.",USA,company,Developed economies,"[-41.93632, 38.1529]","[28.15984, 24.059246]","[-5.193897, 30.575972, -1.5188968]","[9.729561, -3.7008455, 25.896084]","[16.09023, 8.253466]","[11.382102, 1.244333]","[16.439821, 14.858691, -1.6840454]","[12.286285, 5.1555185, 13.081231]"
1,Jean-Julien Aucouturier;François Pachet,Music Similarity Measures: What's the use?,2002,https://doi.org/10.5281/zenodo.1418257,Jean-Julien Aucouturier+SONY Computer Science Lab.>FRA>company;Francois Pachet+SONY Computer Science Lab.>FRA>company,"Electronic Music Distribution (EMD) is in demand of robust, automatically extracted music descriptors. We introduce a timbral similarity measures for comparing music titles. This measure is based on a Gaussian model of cepstrum coefficients. We describe the timbre extractor and the corresponding timbral similarity relation. We describe experiments in assessing the quality of the similarity relation, and show that the measure is able to yield interesting similarity relations, in particular when used in conjunction with other similarity relations. We illustrate the use of the descriptor in several EMD applications developed in the context of the Cuidado European project.",FRA,company,Developed economies,"[-5.1845117, 14.449603]","[21.956306, 5.7545257]","[-6.031034, 8.723967, -1.5979085]","[12.253714, 0.8504703, 5.093986]","[13.156636, 9.34062]","[10.175151, 2.1946437]","[13.642072, 15.184051, -0.7136612]","[11.734773, 6.7297945, 12.41484]"
2,David Bainbridge 0001;John R. McPherson,Forming a Corpus of Voice Queries for Music Information Retrieval.,2002,https://doi.org/10.5281/zenodo.1417301,David Bainbridge+University of Waikato>NZL>education;John R. McPherson+University of Waikato>NZL>education;Sally Jo Cunningham+University of Waikato>NZL>education,"The use of audio queries for searching multimedia content has increased rapidly with the rise of music information retrieval; there are now many Internet-accessible systems that take audio queries as input. However, testing the robustness of such a system can be problematic, as there is currently no standard test-bed of queries and music files available. A corpus of audio queries would aid researchers in the development of both audio signal processing techniques and audio query systems. Such a corpus would also be essential for making empirical comparisons between different systems and methods. We propose a pilot study that will field test a procedure for collecting audio queries. The lessons learned in the pilot study will guide us in refining the collection methodology, and we will make a final set of queries freely available to MIR researchers. The participants for this pilot study will be attendees of the ISMIR 2002 Conference.",NZL,education,Developed economies,"[-21.200594, 1.8712945]","[19.872831, 26.904066]","[9.987573, 14.593389, -11.467817]","[3.227476, 0.22885421, 15.3369465]","[10.829181, 11.053787]","[11.016674, 0.6821334]","[11.982566, 15.519326, 0.19174434]","[11.819527, 5.101022, 12.278887]"
3,Stephan Baumann 0001;Andreas Klüter,Super-convenience for Non-musicans: Querying MP3 and the Semantic Web.,2002,https://doi.org/10.5281/zenodo.1417231,Stephan Baumann+German Research Center for AI (DFKI)>DEU>facility;Andreas Klüter+sonicson GmbH>DEU>company,"""Digital music distribution, the success of MP3 and the actual activities concerning the semantic web of music require for convenient music information retrieval. In this paper we will give an overview about the concepts behind our “super-convenience” approach for MIR. By using natural language as input for human-oriented queries to large-scale music collections we were able to address the needs of non-musicians. The entire system is applicable for future semantic web services, existing music websites and mobile devices. Beside the framework we present a novel idea to incorporate the processing of lyrics based on standard information retrieval methods, i.e the vector space model.""",DEU,facility,Developed economies,"[-20.196447, 23.434408]","[20.059359, 25.621574]","[-15.046292, 6.17136, -14.3188925]","[4.3855443, 0.23568541, 17.618774]","[14.256014, 7.8470206]","[10.99089, 0.51781666]","[14.387231, 14.464932, -2.0998623]","[11.823506, 5.0945683, 12.246412]"
4,Ann Blandford;Hanna Stelmaszewska,Usability of Musical Digital Libraries: a Multimodal Analysis.,2002,https://doi.org/10.5281/zenodo.1417171,Ann Blandford+University College London>GBR>education|UCL Interaction Centre (UCLIC)>GBR>education;Hanna Stelmaszewska+University College London>GBR>education|UCL Interaction Centre (UCLIC)>GBR>education,"There has been substantial research on technical aspects of musical digital libraries, but comparatively little on usability aspects. We have evaluated four web-accessible music libraries, focusing particularly on features that are particular to music libraries, such as music retrieval mechanisms. Although the original focus of the work was on how modalities are combined within the interactions with such libraries, that was not where the main difficulties were found. Libraries were generally well designed for use of different modalities. The main challenges identified relate to the details of melody matching and to simplifying the choices of file format. These issues are discussed in detail.",GBR,education,Developed economies,"[-20.411642, 32.03817]","[23.312359, 28.467083]","[-21.990662, 8.867043, -17.931564]","[5.137327, -3.6394193, 21.72901]","[14.477016, 7.382216]","[10.764789, 0.68482673]","[14.454014, 14.090166, -2.4665692]","[11.522043, 4.948277, 12.623927]"
5,Pedro Cano;Martin Kaltenbrunner;Fabien Gouyon;Eloi Batlle,On the use of FastMap for Audio Retrieval and Browsing.,2002,https://doi.org/10.5281/zenodo.1415250,Pedro Cano+Universitat Pompeu Fabra>ESP>education;Martin Kaltenbrunner+Universitat Pompeu Fabra>ESP>education;Fabien Gouyon+Universitat Pompeu Fabra>ESP>education;Eloi Batlle+Universitat Pompeu Fabra>ESP>education,"In this article, a heuristic version of Multidimensional Scaling (MDS) named FastMap is used for audio retrieval and browsing. FastMap, like MDS, maps objects into an Euclidean space, such that similarities are preserved. In addition of being more efficient than MDS it allows query-by-example type of query, which makes it suitable for a content-based retrieval purposes.",ESP,education,Developed economies,"[-9.224852, 29.02713]","[23.449322, 15.316313]","[-5.0783157, 2.7261202, -20.19952]","[14.710996, -5.9013023, 9.376329]","[13.738233, 7.77552]","[10.412792, 1.5034624]","[13.620462, 14.305422, -2.116053]","[11.7886715, 5.9542565, 12.779813]"
7,L. P. Clarisse;Jean-Pierre Martens;Micheline Lesaffre;Bernard De Baets;Hans De Meyer;Marc Leman,An Auditory Model Based Transcriber of Singing Sequences.,2002,https://doi.org/10.5281/zenodo.1416074,L. P. Clarisse+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>education;J. P. Martens+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>education;M. Lesaffre+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>education;B. De Baets+Ghent University>BEL>education;H. De Meyer+Ghent University>BEL>education;M. Leman+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>education,"""In this paper, a new system for the automatic transcription of singing sequences into a sequence of pitch and duration pairs is presented. Although such a system may have a wider range of applications, it was mainly developed to become the acoustic module of a query-by-humming (QBH) system for retrieving pieces of music from a digitized musical library. The first part of the paper is devoted to the systematic evaluation of a variety of state-of-the art transcription systems. The main result of this evaluation is that there is clearly a need for more accurate systems. Especially the segmentation was experienced as being too error prone (20% segmentation errors). In the second part of the paper, a new auditory model based transcription system is proposed and evaluated. The results of that evaluation are very promising. Segmentation errors vary between 0 and 7%, dependent on the amount of lyrics that is used by the singer. The paper ends with the description of an experimental study that was issued to demonstrate that the accuracy of the newly proposed transcription system is not very sensitive to the choice of the free parameters, at least as long as they remain in the vicinity of the values one could forecast on the basis of their meaning.""",BEL,education,Developed economies,"[-1.8344109, -34.964684]","[-0.18970028, -12.234367]","[20.069487, 4.7644014, -14.085314]","[7.6171355, -12.69531, -15.018646]","[9.643159, 10.849161]","[7.171352, 2.081622]","[11.077904, 14.966683, 0.7556108]","[9.179664, 7.7133784, 11.637666]"
8,Matthew L. Cooper;Jonathan Foote,Automatic Music Summarization via Similarity Analysis.,2002,https://doi.org/10.5281/zenodo.1417026,Matthew Cooper+FX Palo Alto Laboratory>USA>company|FX Palo Alto Laboratory>USA>company;Jonathan Foote+FX Palo Alto Laboratory>USA>company|FX Palo Alto Laboratory>USA>company,"We present methods for automatically producing summary excerpts or thumbnails of music. To find the most representative excerpt, we maximize the average segment similarity to the entire work. After window-based audio parameterization, a quantitative similarity measure is calculated between every pair of windows, and the results are embedded in a 2-D similarity matrix. Summing the similarity matrix over the support of a segment results in a measure of how similar that segment is to the whole. This measure is maximized to find the segment that best represents the entire work. We discuss variations on the method, and present experimental results for orchestral music, popular songs, and jazz. These results demonstrate that the method finds significantly representative excerpts, using very few assumptions about the source audio.",USA,company,Developed economies,"[-6.5929303, 19.53995]","[22.839155, 8.950965]","[1.1636044, 12.213577, -12.232583]","[0.26424003, -1.8949484, 10.645366]","[12.858546, 8.370374]","[10.160212, 1.9944882]","[13.3545, 14.440484, -0.9810794]","[11.225976, 6.312498, 12.056023]"
9,Roger B. Dannenberg;Ning Hu,Pattern Discovery Techniques for Music Audio.,2002,https://doi.org/10.5281/zenodo.1417177,Roger B. Dannenberg+Carnegie Mellon University>USA>education;Ning Hu+Carnegie Mellon University>USA>education,"Human listeners are able to recognize structure in music through the perception of repetition and other relationships within a piece of music. This work aims to automate the task of music analysis. Music is “explained” in terms of embedded relationships, especially repetition of segments or phrases. The steps in this process are the transcription of audio into a representation with a similarity or distance metric, the search for similar segments, forming clusters of similar segments, and explaining music in terms of these clusters. Several transcription methods are considered: monophonic pitch estimation, chroma (spectral) representation, and polyphonic transcription followed by harmonic analysis. Also, several algorithms that search for similar segments are described. These techniques can be used to perform an analysis of musical structure, as illustrated by examples.",USA,education,Developed economies,"[14.68048, 21.660822]","[-2.8878791, 0.3329354]","[-0.78661156, -11.148557, 8.7111435]","[2.1819196, -1.4308076, -6.346381]","[11.565424, 7.579782]","[7.781503, 2.748448]","[12.62942, 13.262983, -0.5608046]","[10.2435, 7.747867, 11.500658]"
10,Dave Datta,Managing Metadata.,2002,https://doi.org/10.5281/zenodo.1415230,David Datta+All Media Guide>USA>company,"The All Media Guide (AMG) is a technology company that maintains the world’s largest database of metadata relating to the entertainment industries. This document describes some of the goals of AMG, the issues uncovered during the evolution of our databases, and discusses some of the implementations we have chosen.",USA,company,Developed economies,"[-23.796661, 43.236294]","[17.841146, 27.737833]","[-26.846125, -0.9172675, -8.836214]","[1.1471003, -9.044339, 20.537977]","[14.396606, 8.99012]","[10.831988, 0.8741849]","[15.23744, 13.459589, -1.2362432]","[11.728472, 5.0747395, 12.473917]"
11,Shyamala Doraisamy;Stefan M. Rüger,A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music.,2002,https://doi.org/10.5281/zenodo.1416022,Shyamala Doraisamy+Imperial College>GBR>education;Stefan Rüger+Imperial College>GBR>education,"In this paper we investigate the retrieval performance of monophonic queries made on a polyphonic music database using the n-gram approach for full-music indexing. The pitch and rhythm dimensions of music are used, and the musical words (a term coined by Downie [2]) generated enable text retrieval methods to be used with music retrieval. We outline an experimental framework for a comparative and fault-tolerance study of various n-gramming strategies and encoding precision using six experimental databases. For monophonic queries we focus in particular on query-by-humming (QBH) systems. Error models addressed in several QBH studies are surveyed for the fault-tolerance study. Our experiments show that different n-gramming strategies and encoding precision differ widely in their effectiveness. We present the results of our comparative and fault-tolerance study on a collection of 5380 polyphonic music pieces encoded in the MIDI format.",GBR,education,Developed economies,"[-10.254088, 18.510765]","[11.2269125, 14.375838]","[0.43059045, 1.422222, -11.597455]","[7.307301, -11.109602, 11.623293]","[12.993406, 7.950869]","[9.058828, 0.56154317]","[12.955759, 14.370205, -1.8063419]","[10.591378, 6.00353, 13.170187]"
12,J. Stephen Downie;Sally Jo Cunningham,Toward a Theory of Music Information Retrieval Queries: System Design Implications.,2002,https://doi.org/10.5281/zenodo.1417565,J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Sally Jo Cunningham+University of Waikato>NZL>education,"This paper analyzes a set of 161 music-related information requests posted to the rec.music.country.old-time newsgroup. These postings are categorized by the types of detail used to characterize the poster's information need, the type of music information requested, the intended use for the information, and additional social and contextual elements present in the postings. The results of this analysis suggest that similar studies of 'native' music information requests can be used to inform the design of effective, usable music information retrieval interfaces.",USA,education,Developed economies,"[-19.948719, 23.008512]","[33.02234, 30.161366]","[-12.84996, 7.615093, -12.156832]","[8.045412, 6.244542, 22.085117]","[14.36462, 7.898856]","[12.096443, 0.9763975]","[14.1835985, 14.809487, -2.1675453]","[12.574093, 4.768708, 12.398451]"
13,Daniel P. W. Ellis;Brian Whitman;Adam Berenzweig;Steve Lawrence,The Quest for Ground Truth in Musical Artist Similarity.,2002,https://doi.org/10.5281/zenodo.1415602,Daniel P. W. Ellis+Columbia University>USA>education;Brian Whitman+MIT Media Lab>USA>education;Adam Berenzweig+Columbia University>USA>education;Steve Lawrence+NEC Research Institute>USA>facility,"It would be interesting and valuable to devise an automatic measure of the similarity between two musicians based only on an analysis of their recordings. To develop such a measure, however, presupposes some ‘ground truth’ training data describing the actual similarity between certain pairs of artists that constitute the desired output of the measure. Since artist similarity is wholly subjective, such data is not easily obtained. In this paper, we describe several attempts to construct a full matrix of similarity measures between a set of some 400 popular artists by regularizing limited subjective judgment data. We also detail our attempts to evaluate these measures by comparison with direct subjective similarity judgments collected via a web-based survey in April 2002. Overall, we find that subjective artist similarities are quite variable between users—casting doubt on the concept of a single ‘ground truth’. Our best measure, however, gives reasonable agreement with the subjective data, and forms a usable stand-in. In addition, our evaluation methodology may be useful for comparing other measures of artist similarity.",USA,education,Developed economies,"[-1.6485538, 14.066171]","[33.078083, 7.177986]","[-6.619288, 5.561094, 5.208397]","[13.181724, 5.8463335, 9.924003]","[13.069136, 9.57299]","[11.543011, 2.4533086]","[13.5342865, 15.288883, -0.67862475]","[12.969501, 6.2035055, 12.37156]"
14,Yazhong Feng;Yueting Zhuang;Yunhe Pan,Popular Music Retrieval by Independent Component Analysis.,2002,https://doi.org/10.5281/zenodo.1416098,Yazhong Feng+Zhejiang University>CHN>education;Yueting Zhuang+Zhejiang University>CHN>education;Yunhe Pan+Zhejiang University>CHN>education,"""""",CHN,education,Developing economies,"[-10.334156, 10.599067]","[51.412685, 42.637775]","[-0.44445372, 11.814, -7.0879116]","[5.9575295, 3.1215878, 29.85249]","[13.340023, 8.484236]","[-9.072453, 9.962819]","[13.608044, 14.74196, -1.4815145]","[-11.714849, 1.7676638, 1.0362436]"
15,Jonathan Foote;Matthew L. Cooper;Unjung Nam,Audio Retrieval by Rhythmic Similarity.,2002,https://doi.org/10.5281/zenodo.1417603,"Jonathan Foote+FX Palo Alto Laboratory, Inc.>USA>company|Stanford University>USA>education;Matthew Cooper+FX Palo Alto Laboratory, Inc.>USA>company|Stanford University>USA>education;Unjung Nam+Stanford University>USA>education","We present a method for characterizing both the rhythm and tempo of music. We also present ways to quantitatively measure the rhythmic similarity between two or more works of music. This allows rhythmically similar works to be retrieved from a large collection. A related application is to sequence music by rhythmic similarity, thus providing an automatic “disc jockey” function for musical libraries. Besides specific analysis and retrieval methods, we present small-scale experiments that demonstrate ranking and retrieving musical audio by rhythmic similarity.",USA,company,Developed economies,"[3.6432924, 13.477161]","[12.611632, 6.289174]","[-5.220053, 2.2633054, -5.5887656]","[6.102627, -5.148496, 5.829359]","[12.916275, 8.313044]","[6.547827, 1.5389735]","[12.825757, 14.6158495, -1.4442604]","[8.8393135, 6.495591, 12.135585]"
16,Ichiro Fujinaga;Jenn Riley,Digital Image Capture of Musical Scores.,2002,https://doi.org/10.5281/zenodo.1416554,Ichiro Fujinaga+Johns Hopkins University>USA>education;Jenn Riley+Indiana University>USA>education,"Musical scores have small details and complex markings, and are difficult to digitally capture and deliver well. All capture decisions should be made with a clear idea of the purpose of the resulting digital images, but master images must be flexible enough to fulfill unanticipated future uses. In order to provide a framework for decision-making in musical score digitization projects, best practices for detail and color capture are presented. Recommendations for file formats for archival storage, web delivery and printing of musical materials are presented.",USA,education,Developed economies,"[-12.901754, 4.364538]","[10.391233, 38.00077]","[-8.135231, -14.246932, -10.392658]","[4.0184417, -2.1899586, 25.79539]","[12.111629, 6.9151692]","[10.546124, 0.609331]","[13.068636, 12.817651, -1.4841553]","[11.269525, 4.9932017, 12.44134]"
17,Joe Futrelle;J. Stephen Downie,Interdisciplinary Communities and Research Issues in Music Information Retrieval.,2002,https://doi.org/10.5281/zenodo.1416406,Joe Futrelle+University of Illinois>USA>education;J. Stephen Downie+University of Illinois>USA>education,"Music Information Retrieval (MIR) is an interdisciplinary research area that has grown out of the need to manage burgeoning collections of music in digital form. Its diverse disciplinary communities have yet to articulate a common research agenda or agree on methodological principles and metrics of success. In order for MIR to succeed, researchers need to work with real user communities and develop research resources such as reference music collections, so that the wide variety of techniques being developed in MIR can be meaningfully compared with one another. Out of these efforts, a common MIR practice can emerge.",USA,education,Developed economies,"[-23.082653, 21.842472]","[28.214512, 33.306965]","[-16.4616, 6.30213, -8.657496]","[3.888246, 4.7363467, 18.210978]","[14.354903, 8.097349]","[11.5923605, 0.57982945]","[14.52456, 14.700713, -2.0519838]","[12.026315, 4.5925546, 12.154795]"
18,Gordon Geekie,Carnatic Ragas as Music Information Retrieval Entities.,2002,https://doi.org/10.5281/zenodo.1415994,Gordon Geekie+Manchester Metropolitan University>GBR>education,"Carnatic music is the ‘art’ music of the four southern States of India (Andhra Pradesh, Karnataka, Kerala and Tamilnadu). One difference between Carnatic music and the better-known Hindusthani music of North India is its embeddedness in a religious-philosophical context. This context crucially determines the objects of knowledge organization and the indigenous theory of musical affect. The author presents the view that a digital library of Carnatic music should contain the objects of knowledge organization and their interrelationships as conceived by indigenous practitioners and audiences, rather than by Western specialists or North Indian practitioners. The author demonstrates how three features of Carnatic music (viz. aural transmission, improvisation and cultural context) have particular implications for the development of a digital library. Aural transmission results in musical documents being less important sources of information than recordings. Improvisation results in a highly transformational and often ambiguous relationship between (intra)musical signifiers and signified, causing problems of classification and machine recognition. The cultural context favours the prioritisation of emotional affect over introductory ease of listening and even technical recording quality in the selection of the recordings to be included in a digital library of Carnatic music.",GBR,education,Developed economies,"[3.9733539, -0.54207313]","[3.0128596, -16.538904]","[6.1409273, 1.3882089, -17.53298]","[16.183357, -13.876824, -8.415881]","[11.35918, 10.422322]","[7.5337577, 1.1287794]","[11.733849, 15.108945, -1.5997862]","[8.966621, 7.0425677, 12.616242]"
19,Masataka Goto;Hiroki Hashiguchi;Takuichi Nishimura;Ryuichi Oka,"RWC Music Database: Popular, Classical and Jazz Music Databases.",2002,https://doi.org/10.5281/zenodo.1416474,Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroki Hashiguchi+Mejiro University>JPN>education;Takuichi Nishimura+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Ryuichi Oka+University of Aizu>JPN>education,"This paper describes the design policy and specifications of the RWC Music Database, a music database (DB) that is available to researchers for common use and research purposes. Various commonly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Music Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.",JPN,facility,Developed economies,"[-19.884447, 11.980699]","[21.929857, 32.437107]","[-16.022762, -5.129738, -8.377636]","[1.8895118, -0.85544205, 22.663595]","[13.232071, 7.4800153]","[10.992183, 0.2693142]","[14.103799, 13.845748, -1.5848998]","[11.650835, 4.7468863, 12.343629]"
20,Jaap Haitsma;Ton Kalker,A Highly Robust Audio Fingerprinting System.,2002,https://doi.org/10.5281/zenodo.1417973,Jaap Haitsma+Philips Research>NLD>company|Philips Research>Unknown>Unknown;Ton Kalker+Philips Research>NLD>company|Philips Research>Unknown>Unknown,"Imagine the following situation. You’re in your car, listening to the radio and suddenly you hear a song that catches your attention. It’s the best new song you have heard for a long time, but you missed the announcement and don’t recognize the artist. Still, you would like to know more about this music. What should you do? You could call the radio station, but that’s too cumbersome. Wouldn’t it be nice if you could push a few buttons on your mobile phone and a few seconds later the phone would respond with the name of the artist and the title of the music you’re listening to? Perhaps even sending an email to your default email address with some supplemental information. In this paper we present an audio fingerprinting system, which makes the above scenario possible. By using the fingerprint of an unknown audio clip as a query on a fingerprint database, which contains the fingerprints of a large library of songs, the audio clip can be identified. At the core of the presented system are a highly robust fingerprint extraction method and a very efficient fingerprint search strategy, which enables searching a large fingerprint database with only limited computing resources.",NLD,company,Developed economies,"[-15.796833, -26.102415]","[26.24151, -22.097271]","[6.1526437, -12.424177, -23.60609]","[19.32196, -12.943721, 5.5169625]","[9.066973, 4.4715695]","[8.551425, -0.03301394]","[10.563334, 11.498816, -1.9192337]","[10.197164, 5.25253, 12.967277]"
21,Toni Heittola;Anssi Klapuri,Locating Segments with Drums in Music Signals.,2002,https://doi.org/10.5281/zenodo.1418137,Toni Heittola+Tampere University of Technology>FIN>education;Anssi Klapuri+Tampere University of Technology>FIN>education,"A system is described which segments musical signals according to the presence or absence of drum instruments. Two different yet approximately equally accurate approaches were taken to solve the problem. The first is based on periodicity detection in the amplitude envelopes of the signal at subbands. The band-wise periodicity estimates are aggregated into a summary autocorrelation function, the characteristics of which reveal the drums. The other mechanism applies straightforward acoustic pattern recognition with mel-frequency cepstrum coefficients as features and a Gaussian mixture model classifier. The integrated system achieves 88 % correct segmentation over a database of 28 hours of music from different musical genres. For the both methods, errors occur for borderline cases with soft percussive-like drum accompaniment, or transient-like instrumentation without drums.",FIN,education,Developed economies,"[24.296747, -45.44541]","[-12.992515, 1.3515514]","[18.167273, -18.458805, 0.97587425]","[9.101151, 7.8306446, -14.700801]","[7.8904123, 7.015933]","[8.517122, 3.9489684]","[10.246767, 11.778593, 0.8774753]","[9.856655, 7.5716786, 10.154004]"
25,Jürgen Kilian;Holger H. Hoos,Voice Separation - A Local Optimization Approach.,2002,https://doi.org/10.5281/zenodo.1417645,Jürgen Kilian+Darmstadt University of Technology>DEU>education|University of British Columbia>CAN>education;Holger H. Hoos+Darmstadt University of Technology>DEU>education|University of British Columbia>CAN>education,"Voice separation, along with tempo detection and quantisation, is one of the basic problems of computer-based transcription of music. An adequate separation of notes into different voices is crucial for obtaining readable and usable scores from performances of polyphonic music recorded on keyboard (or other polyphonic) instruments; for improving quantisation results within a transcription system; and in the context of music retrieval systems that primarily support monophonic queries. In this paper we propose a new voice separation algorithm based on a stochastic local search method. Different from many previous approaches, our algorithm allows chords in the individual voices; its behaviour is controlled by a small number of intuitive and musically motivated parameters; and it is fast enough to allow interactive optimisation of the result by adjusting the parameters in real-time. We demonstrate that compared to existing approaches, our new algorithm generates better solutions for a number of typical voice separation problems. We also show how by changing its parameters it is possible to create score output suitable for different needs, piano-style orchestral scores.",DEU,education,Developed economies,"[-0.48032755, -45.653088]","[-43.26981, -31.319622]","[29.162067, 7.38909, -4.0219207]","[-5.171714, -4.690862, -26.732603]","[8.854227, 10.593658]","[6.7561507, 5.0796494]","[10.88594, 14.369382, 1.4738034]","[9.716806, 8.265149, 9.67286]"
26,Ja-Young Kim;Nicholas J. Belkin,Categories of Music Description and Search Terms and Phrases Used by Non-Music Experts.,2002,https://doi.org/10.5281/zenodo.1417763,Ja-Young Kim+Rutgers University>USA>education;Nicholas J. Belkin+Rutgers University>USA>education,"Previous research has demonstrated that people listen to music for various reasons. The purpose of this study was to investigate people’s perception of music, and thus their music information needs. These ideas were examined by presenting 22 participants with 7 classical musical pieces, asking one-half of them to write words descriptive of each piece, and the other half words they would use if searching for each piece. All the words used by all subjects in both tasks were classified into 7 categories. The two most frequently appearing categories were emotions and occasions or filmed events regardless of the task type. These subjects, none of whom had formal training in music, almost never used words related to formal features of music, rather using words indicating other features, most of which have not been considered in existing or proposed music IR systems. These results suggest that music IR research should be extended to consider needs other than finding known items, or items identified by formal characteristics, and that understanding music information needs of users should be prioritized to design more sophisticated music IR systems.",USA,education,Developed economies,"[-23.384438, 19.25924]","[33.694283, 36.11503]","[-15.850523, 7.4371142, -4.159199]","[0.90542454, 14.552578, 17.59613]","[14.262554, 8.248534]","[12.235725, 0.8202675]","[14.33772, 14.859364, -1.8999915]","[12.671914, 4.4641466, 11.532508]"
32,Matija Marolt;Sasa Divjak,On detecting repeated notes in piano music.,2002,https://doi.org/10.5281/zenodo.1416078,Matija Marolt+University of Ljubljana>SVN>education;Sasa Divjak+University of Ljubljana>SVN>education,"One of the problems encountered in music transcription is to produce an algorithm that detects whether a note should be repeated, when a new onset is found during its duration, or not; with other words whether two or more shorter notes should be produced instead of a single longer note. The paper describes our approach to solving this problem, implemented within our system for transcription of piano music [4]. The approach is based on a multilayer perceptron neural network, trained to recognize repeated notes. We compare this method to a more naive method that tracks the amplitude of the first partial of each note and also present performance statistics of our system on transcriptions of several real piano recordings.",SVN,education,Developed economies,"[29.533262, 2.2886486]","[-27.396326, -24.669325]","[6.7964025, -17.811, 4.6281886]","[-10.205729, -10.322736, -12.323699]","[10.060706, 7.096599]","[7.775452, 5.2834277]","[11.8932705, 12.121547, -0.5961244]","[8.980062, 6.66197, 9.1113615]"
28,Olivier Lartillot,Integrating Pattern Matching into an Analogy-Oriented Pattern Discovery Framework.,2002,https://doi.org/10.5281/zenodo.1417048,Olivier Lartillot+Ircam – Centre Pompidou>FRA>facility,"We claim that the core mechanism of a sufficiently general MIR system should be expressed in symbolic terms. We defend the idea that music database should be pre-analyzed before being scanned for MIR queries. We suggest a new vision of automated pattern analysis that generalizes the multiple viewpoint approach by adding a new paradigm based on analogy and temporal approach of musical scores. Through a chronological scanning of the score, analogies are inferred between local relationships — namely, notes and intervals — and global structures — namely, patterns — whose paradigms are stored inside an abstract pattern trie (APT). Basic mechanisms for inference of new patterns are described. The same pattern-matching algorithm used for pattern discovery during pre-analysis of musical works is reused during MIR applications. Such an elastic vision of music enables a generalized understanding of its plastic expression. This project, in an early stage, introduces a broader paradigm of automated music analysis.",FRA,facility,Developed economies,"[15.158966, 26.068996]","[2.3769083, 19.265347]","[-5.519217, -14.339991, 10.166427]","[-3.1914203, -11.017268, 9.013058]","[11.564153, 7.517319]","[9.259832, 1.323888]","[12.498148, 13.170951, -0.6981158]","[10.635949, 5.803013, 11.40862]"
33,John R. McPherson,Introducing Feedback into an Optical Music Recogniition System.,2002,https://doi.org/10.5281/zenodo.1417725,John R. McPherson+University of Waikato>NZL>education,"Optical Music Recognition is the process of converting a graphical representation of music (such as sheet music) into a symbolic format (for example, a format that is understood by music software). Music notation is rich in structural information, and the relative positions of objects can often help to identify them. When objects are unidentified or mis-identified, many current systems “coerce” the set of objects into some semantic representation, for example by modifying the detected durations. This could cause correctly identified symbols to be modified. The knowledge that the current set of identified symbols cannot be semantically parsed could instead be used to re-examine some of the symbols before deciding whether or not the classification is correct. This paper describes work in progress involving the use of feedback between the various phases of the optical music recognition process to automatically correct mistakes, such as symbolic classification errors or mis-detected staff systems.",NZL,education,Developed economies,"[17.15871, -28.505634]","[-18.736279, 35.691505]","[16.768177, 17.24839, 12.089831]","[-11.801874, -17.525793, 3.6441631]","[8.687035, 6.147858]","[6.727981, -0.47843766]","[10.719053, 11.200414, -0.15093958]","[8.089993, 4.266991, 10.676812]"
34,Colin Meek;William P. Birmingham,Johnny Can't Sing: A Comprehensive Error Model for Sung Music Queries.,2002,https://doi.org/10.5281/zenodo.1418065,Colin Meek+University of Michigan>USA>education|Advanced Technologies Laboratory>USA>facility;William Birmingham+University of Michigan>USA>education|Advanced Technologies Laboratory>USA>facility,"We propose a model for errors in sung queries, a variant of the Hidden Markov Model (HMM). This is related to the problem of identifying the degree of similarity between a query and a potential target in a database of musical works, in the music retrieval framework. The model comprehensively expresses the types of error or variation between target and query: cumulative and non-cumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. Results of experiments demonstrating the robustness of the model are presented.",USA,education,Developed economies,"[-21.879412, 0.46703795]","[10.074731, 13.148094]","[10.114368, 14.5115595, -8.166245]","[7.687684, -13.601354, 14.280328]","[10.718612, 11.211658]","[8.958656, 0.46746787]","[11.922577, 15.593365, 0.32386887]","[10.50248, 5.9231014, 13.220881]"
35,Massimo Melucci;Nicola Orio,A Comparison of Manual and Automatic Melody Segmentation.,2002,https://doi.org/10.5281/zenodo.1416914,Massimo Melucci+University of Padua>ITA>education;Nicola Orio+University of Padua>ITA>education,"""""",ITA,education,Developed economies,"[5.242782, -9.021916]","[49.818527, 43.048153]","[11.761684, 7.391486, -3.994439]","[6.9404545, 6.3641276, 30.076693]","[10.371753, 9.8877]","[-9.07948, 9.969835]","[11.305628, 14.988742, -0.55365866]","[-11.72409, 1.7769163, 1.0454961]"
36,Thomas Noll;Jörg Garbers;Karin Höthker;Christian Spevak;Tillman Weyde,Opuscope - Towards a Corpus-Based Music Repository.,2002,https://doi.org/10.5281/zenodo.1417411,Thomas Noll+Technische Universität Berlin>DEU>education;Jörg Garbers+Technische Universität Berlin>DEU>education;Karin Höthker+Universität Karlsruhe>DEU>education;Christian Spevak+Universität Karlsruhe>DEU>education;Tillman Weyde+Universität Osnabrück>DEU>education,"Opuscope is an initiative targeted at sharing musical corpora and their analyses between researchers. The Opuscope repository will contain musical corpora of high quality which can be annotated with hand-made or algorithmic musical analyses. So, analytical results obtained by others can be used as a starting point for one’s own investigations. Experiments performed on Opuscope corpora can easily be compared to other approaches, since an unequivocal mechanism for describing a certain corpus will be provided.",DEU,education,Developed economies,"[-19.709513, 3.0224173]","[-6.020117, 13.921243]","[9.281563, 15.793167, -13.3321]","[-8.526356, 5.3109922, 6.530194]","[11.011096, 10.691523]","[8.684561, 1.6027889]","[12.3874855, 14.794647, -0.8641794]","[10.008721, 6.3120484, 12.048889]"
37,Bryan Pardo;William P. Birmingham,Encoding Timing Information for Musical Query Matching.,2002,https://doi.org/10.5281/zenodo.1415776,Bryan Pardo+University of Michigan>USA>education|University of Michigan>USA>education;William Birmingham+University of Michigan>USA>education,We compare representing note timing as Inter Onset Intervals (IOIs) and as the ratio of adjacent IOI values. A variety of log2 and linear quantizations of IOI and IOI ratios are considered for each representation. The utility of encoding with a particular quantization is measured by the ability of a simple string-matcher to differentiate between themes in a melodic corpus. Results indicate that time is best represented by IOI ratios quantized to a logarithmic scale.,USA,education,Developed economies,"[15.451256, -8.573422]","[-27.202017, 7.26844]","[-0.3981683, -16.04288, -2.0922232]","[-2.6070676, -10.587058, 1.2281559]","[11.558094, 6.2997584]","[7.93637, 1.3187742]","[12.099807, 13.227732, -1.8797238]","[9.780193, 6.744875, 12.64748]"
38,Jouni Paulus;Anssi Klapuri,Measuring the similarity of Rhythmic Patterns.,2002,https://doi.org/10.5281/zenodo.1414712,Jouni Paulus+Tampere University of Technology>FIN>education;Anssi Klapuri+Tampere University of Technology>FIN>education,"A system is described which measures the similarity of two arbitrary rhythmic patterns. The patterns are represented as acoustic signals, and are not assumed to have been performed with similar sound sets. Two novel methods are presented that constitute the algorithmic core of the system. First, a probabilistic musical meter estimation process is described, which segments a continuous musical signal into patterns. As a side-product, the method outputs tatum, tactus (beat), and measure lengths. A subsequent process performs the actual similarity measurements. Acoustic features are extracted which model the fluctuation of loudness and brightness within the pattern, and dynamic time warping is then applied to align the patterns to be compared. In simulations, the system behaved consistently by assigning high similarity measures to similar musical rhythms, even when performed using different sound sets.",FIN,education,Developed economies,"[44.50569, 7.8521833]","[-22.089638, 2.6980944]","[-7.967437, -23.132603, 3.6040626]","[1.4563493, 14.616636, -4.2981625]","[12.009524, 5.4761343]","[6.2984037, 1.5554606]","[11.402164, 14.17217, -2.116579]","[8.251305, 6.6788254, 11.924304]"
39,Steffen Pauws,"CubyHum: a fully operational ""query by humming"" system.",2002,https://doi.org/10.5281/zenodo.1415614,Steffen Pauws+Philips Research Eindhoven>NLD>company,"'Query by humming' is an interaction concept in which the identity of a song has to be revealed fast and orderly from a given sung input using a large database of known melodies. In short, it tries to detect the pitches in a sung melody and compares these pitches with symbolic representations of the known melodies. Melodies that are similar to the sung pitches are retrieved. Approximate pattern matching in the melody comparison process compensates for the errors in the sung melody by using classical dynamic programming. A filtering method is used to save computation in the dynamic programming framework. This paper presents the algorithms for pitch detection, note onset detection, quantization, melody encoding and approximate pattern matching as they have been implemented in the CubyHum software system. Since human reproduction of melodies is imperfect, findings from an experimental singing study were a crucial input to the development of the algorithms. Future research should pay special attention to the reliable detection of note onsets in any preferred singing style. In addition, research on index methods and fast bit-parallelism algorithms for approximate pattern matching need to be further pursued to decrease computational requirements when dealing with large melody databases.",NLD,company,Developed economies,"[-3.9333413, 37.421013]","[7.8425164, 10.567387]","[-15.164647, -7.27234, -24.825659]","[5.916069, -15.103303, 8.484265]","[14.855451, 6.138774]","[8.579209, 0.6880757]","[13.198453, 15.355771, -2.9463365]","[10.177786, 6.143014, 13.172093]"
40,Steffen Pauws;Berry Eggen,PATS: Realization and user evaluation of an automatic playlist generator.,2002,https://doi.org/10.5281/zenodo.1417971,Steffen Pauws+Philips Research Eindhoven>NLD>company;Berry Eggen+Philips Research Eindhoven>NLD>company|Technische Universiteit Eindhoven>NLD>education,"A means to ease selecting preferred music referred to as Personalized Automatic Track Selection (PATS) has been developed. PATS generates playlists that suit a particular context-of-use, that is, the real-world environment in which the music is heard. To create playlists, it uses a dynamic clustering method in which songs are grouped based on their attribute similarity. The similarity measure selectively weighs attribute-values, as not all attribute-values are equally important in a context-of-use. An inductive learning algorithm is used to reveal the most important attribute-values for a context-of-use from preference feedback of the user. In a controlled user experiment, the quality of PATS-compiled and randomly assembled playlists for jazz music was assessed in two contexts-of-use. The quality of the randomly assembled playlists was used as base-line. The two contexts-of-use were ‘listening to soft music’ and ‘listening to lively music’. Playlist quality was measured by precision (songs that suit the context-of-use), coverage (songs that suit the context-of-use but that were not already contained in previous playlists) and a rating score. Results showed that PATS playlists contained increasingly more preferred music (increasingly higher precision), covered more preferred music in the collection (higher coverage), and were rated higher than randomly assembled playlists.",NLD,company,Developed economies,"[-39.93027, 38.927044]","[37.208622, 19.52233]","[-3.1641765, 29.074894, -2.8207366]","[13.654817, 4.645649, 18.589025]","[16.18997, 8.161148]","[12.313905, 1.6068454]","[16.558935, 14.814817, -1.7443459]","[13.201769, 5.0726104, 12.742043]"
41,Geoffroy Peeters;Amaury La Burthe;Xavier Rodet,Toward Automatic Music Audio Summary Generation from Signal Analysis.,2002,https://doi.org/10.5281/zenodo.1417885,Geoffroy Peeters+IRCAM>FRA>facility;Amaury La Burthe+IRCAM>FRA>facility;Xavier Rodet+IRCAM>FRA>facility,"This paper deals with the automatic generation of music audio summaries from signal analysis without the use of any other information. The strategy employed here is to consider the audio signal as a succession of “states” (at various scales) corresponding to the structure (at various scales) of a piece of music. This is, of course, only applicable to certain kinds of musical genres based on some kind of repetition. From the audio signal, we first derive dynamic features representing the time evolution of the energy content in various frequency bands. These features constitute our observations from which we derive a representation of the music in terms of “states”. Since human segmentation and grouping performs better upon subsequent hearings, this “natural” approach is followed here. The first pass of the proposed algorithm uses segmentation in order to create “templates”. The second pass uses these templates in order to propose a structure of the music using unsupervised learning methods (K-means and hidden Markov model). The audio summary is finally constructed by choosing a representative example of each state. Further refinements of the summary audio signal construction, uses overlap-add, and a tempo detection/beat alignment in order to improve the audio quality of the created summary.",FRA,facility,Developed economies,"[-2.4960654, -12.500798]","[-2.0958993, -0.42668578]","[12.592254, -6.5760484, -16.319595]","[2.052682, -1.8577064, -4.0671473]","[12.631062, 8.274712]","[7.959545, 2.996336]","[13.089239, 14.093733, -0.87542945]","[10.170993, 7.755286, 11.363067]"
42,Jeremy Pickens;Juan Pablo Bello;Tim Crawford;Matthew J. Dovey;Giuliano Monti;Mark B. Sandler,Polyphonic Score Retrieval Using Polyphonic Audio Queries: A Harmonic Modeling Approach.,2002,https://doi.org/10.5281/zenodo.1418091,"Jeremy Pickens+Center for Intelligent Information Retrieval, University of Massachusetts, Amherst>USA>education;Juan Pablo Bello+Department of Electronic Engineering, Queen Mary, University of London>GBR>education;Giuliano Monti+Department of Electronic Engineering, Queen Mary, University of London>GBR>education;Tim Crawford+Department of Electronic Engineering, Queen Mary, University of London>GBR>education;Matthew Dovey+Department of Electronic Engineering, Queen Mary, University of London>GBR>education;Mark Sandler+Department of Electronic Engineering, Queen Mary, University of London>GBR>education;Don Byrd+Department of Electronic Engineering, Queen Mary, University of London>GBR>education","This paper extends the familiar “query by humming” music retrieval framework into the polyphonic realm. As humming in multiple voices is quite difficult, the task is more accurately described as “query by audio example”, onto a collection of scores. To our knowledge, we are the first to use polyphonic audio queries to retrieve from polyphonic symbolic collections. Furthermore, as our results will show, we will not only use an audio query to retrieve a known-item symbolic piece, but we will use it to retrieve an entire set of real-world composed variations on that piece, also in the symbolic format. The harmonic modeling approach which forms the basis of this work is a new and valuable technique which has both wide applicability and future potential.",USA,education,Developed economies,"[-7.282552, -4.684419]","[11.230548, 11.529988]","[14.012062, -4.7490115, -10.730662]","[5.355026, -12.146755, 12.554607]","[12.900993, 8.027926]","[8.832755, 0.5795968]","[12.73439, 14.226427, -1.5289898]","[10.397022, 6.06319, 13.072183]"
43,Anna Pienimäki,Indexing Music Databases Using Automatic Extraction of Frequent Phrases.,2002,https://doi.org/10.5281/zenodo.1416632,Anna Pienimäki+University of Helsinki>FIN>education,"The Music Information Retrieval methods can be classified into online and offline methods. The main drawback in most of the offline algorithms is the space the indexing structure requires. The amount of data stored into the structure can however be reduced by storing only the suitable index terms or phrases instead of the whole contents of the database. Repetition is agreed to be one of the most important factors of musical meaningfulness. Therefore repetitive musical phrases are suitable for indexing purposes. The extraction of such phrases can be done by applying an existing text mining method to musical data. Because of the differences between text and musical data the application requires some technical modification of the method. This paper introduces a text mining-based music database indexing method that extracts maximal frequent phrases from musical data and sorts them by their length, frequency and personality. The implementation of the method found three different types of phrases from the test corpus consisting of Irish folk music tunes. The suitable two types of phrases out of three are easily recognized and separated from the set of all phrases to form an index data for the database.",FIN,education,Developed economies,"[-12.751491, 23.711132]","[18.216236, 17.283411]","[-3.869046, 7.004069, -16.176672]","[9.558391, -1.2684153, 13.043869]","[13.656142, 7.9408407]","[9.86875, 1.355668]","[13.532051, 14.716672, -2.0095642]","[11.406603, 5.944152, 12.589617]"
44,Emanuele Pollastri,Some Considerations About Processing Singing Voice for Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1416494,Emanuele Pollastri+Università degli Studi di Milano>ITA>education,"The audio processing and post-processing of singing hold a fundamental role in the context of query-by-humming applications. Through the analysis of a sung query, we should perform some kind of meta-information extraction and this topic deserves the interest of the present paper. Considering the raw output of a pitch tracking algorithm, the issues of note estimation and the study of singing accuracy have been addressed. Further, we report an experiment on the deviations from pure tone intonation in performances of untrained singers.",ITA,education,Developed economies,"[-4.1153917, -34.548946]","[-0.55950606, -12.976833]","[19.610296, 9.255853, -10.5520315]","[7.1155024, -13.601767, -13.361133]","[10.133286, 11.081732]","[7.056108, 2.0868983]","[11.328503, 15.253676, 0.7008957]","[9.025585, 7.6830053, 11.547054]"
45,Christopher Raphael,Automatic Transcription of Piano Music.,2002,https://doi.org/10.5281/zenodo.1414952,"Christopher Raphael+University of Massachusetts, Amherst>USA>education","""A hidden Markov model approach to piano music transcription is presented. The main difficulty in applying traditional HMM techniques is the large number of chord hypotheses that must be considered. We address this problem by using a trained likelihood model to generate reasonable hypotheses for each frame and construct the search graph out of these hypotheses. Results are presented using a recording of a movement from Mozart’s Sonata 18, K. 570.""",USA,education,Developed economies,"[30.434391, -5.2430277]","[-9.30332, -5.9949956]","[15.84735, -4.4621797, 13.546464]","[1.6420202, -6.635554, -9.388536]","[9.801269, 7.2847505]","[6.8749323, 3.1499426]","[11.997069, 11.639826, -0.26900774]","[9.347034, 8.0394125, 11.426611]"
46,Andreas Rauber;Elias Pampalk;Dieter Merkl,Using Psycho-Acoustic Models and Self-Organizing Maps to Create a Hierarchical Structuring of Music by Musical Styles.,2002,https://doi.org/10.5281/zenodo.1417143,Andreas Rauber+Vienna University of Technology>AUT>education;Elias Pampalk+Austrian Research Institute for Artificial Intelligence>AUT>facility;Dieter Merkl+Vienna University of Technology>AUT>education,"With the advent of large musical archives the need to provide an organization of these archives becomes eminent. While artist-based organizations or title indexes may help in locating a specific piece of music, a more intuitive, genre-based organization is required to allow users to browse an archive and explore its contents. Yet, currently these organizations following musical styles have to be designed manually. In this paper we propose an approach to automatically create a hierarchical organization of music archives following their perceived sound similarity. More specifically, characteristics of frequency spectra are extracted and transformed according to psycho-acoustic models. Subsequently, the Growing Hierarchical Self-Organizing Map, a popular unsupervised neural network, is used to create a hierarchical organization, offering both an interface for interactive exploration as well as retrieval of music according to perceived sound similarity.",AUT,education,Developed economies,"[-5.6811886, 7.6259594]","[27.63158, 17.156397]","[-7.8598986, -3.3110752, -0.30228987]","[16.278332, -6.9080906, 24.159529]","[12.267003, 8.507846]","[11.124292, 1.8947136]","[13.166379, 14.066704, -0.6729998]","[12.352607, 5.8473477, 13.206333]"
48,Jungmin Song;So-Young Bae;Kyoungro Yoon,Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System.,2002,https://doi.org/10.5281/zenodo.1418309,Jungmin Song+LG Electronics>KOR>company;So Young Bae+LG Electronics>KOR>company;Kyoungro Yoon+LG Electronics>KOR>company,"Recently a great attention is paid to content-based multimedia retrieval that enables users to find and locate audio-visual materials according to the intrinsic characteristics of the target. Query-by-humming (QBH) is also an application that makes retrieval based on major characteristics of music, that is, ""melody"". There have been some researches on QBH system, most of which are to retrieve music from symbolic music data by humming query. However, when the usability of technology is taken into consideration, retrieval of music in the form of polyphonic raw audio would be more useful and needed in the applications such as internet music search or music juke box, where the music data is stored not in symbolic form but in raw digital audio signal because such music data is more natural format for consumption. Our focus is on the realization of query-by-humming technology for an easy-to-use application, which entails full automation of all the processes of the system, including melody information extraction from polyphonic raw audio. In our system, melody feature of music database and humming is not represented by distinct note information but by the probability of note occurrence. Similarity is then measured between the melody features of humming and music data using DP matching method. This paper presents developed algorithms and experimental results for key steps of QBH system including the melody feature extraction method from polyphonic audio and humming, their representation for matching, and matching method between represented melody information from polyphonic audio and humming.",KOR,company,Developing economies,"[6.6453967, -16.154268]","[11.587456, 10.85084]","[15.063257, 1.348689, -7.793809]","[4.0701365, -11.431979, 13.327186]","[14.829698, 6.199587]","[8.778751, 0.60812324]","[13.015613, 15.289358, -2.7416477]","[10.342821, 5.9778824, 13.053818]"
49,Timo Sorsa;Katriina Halonen,Mobile Melody Recognition System with Voice-Only User Interface.,2002,https://doi.org/10.5281/zenodo.1415866,Timo Sorsa+Nokia Research Center>FIN>company;Katriina Halonen+Nokia Research Center>FIN>company,A melody recognition system with a voice-only user interface is presented in this paper. By integrating speech recognition and melody recognition technology we have built an end-to-end melody retrieval system that allows a users to do voice controlled melodic queries and melody generation using a dial-in service with a mobile phone.,FIN,company,Developed economies,"[8.126663, -12.938949]","[13.654563, 24.318897]","[16.627516, 3.2072864, -0.34802628]","[0.87659407, -13.197489, 15.89277]","[10.1832075, 9.893884]","[10.232029, 0.9659347]","[11.13208, 14.902684, -0.5288756]","[10.959207, 5.4449515, 12.339463]"
50,George Tzanetakis;Andrey Ermolinskiy;Perry R. Cook,Pitch Histograms in Audio and Symbolic Music Information Retrieval.,2002,https://doi.org/10.5281/zenodo.1416146,George Tzanetakis+Princeton University>USA>education;Andrey Ermolinskyi+Princeton University>USA>education;Perry Cook+Princeton University>USA>education,"In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques. In this paper, Pitch Histograms are defined and proposed as a way to represent the pitch content of music signals both in symbolic and audio form. This representation is evaluated in the context of automatic musical genre classification. A multiple-pitch detection algorithm for polyphonic signals is used to calculate Pitch Histograms for audio signals. In order to evaluate the extent and significance of errors resulting from the automatic multiple-pitch detection, automatic musical genre classification results from symbolic and audio data are compared. The comparison indicates that Pitch Histograms provide valuable information for musical genre classification. The results obtained for both symbolic and audio cases indicate that although pitch errors degrade classification performance for the audio case, Pitch Histograms can be effectively used for classification in both cases.",USA,education,Developed economies,"[-4.8122997, 23.736767]","[-5.4959207, -8.0472975]","[-1.7542522, 1.953741, -18.714062]","[4.6992254, -12.896364, -7.749505]","[12.593011, 7.8229103]","[7.051207, 2.43301]","[12.726607, 14.350837, -1.5791298]","[9.311451, 7.419105, 11.196389]"
51,Alexandra L. Uitdenbogerd;Ron G. van Schyndel,A Review of Factors Affecting Music Recommender Success.,2002,https://doi.org/10.5281/zenodo.1417783,Alexandra Uitdenbogerd+RMIT University>AUS>education;Ron van Schyndel+RMIT University>AUS>education,"""""",AUS,education,Developed economies,"[-42.58129, 23.930845]","[50.935055, 43.949024]","[-14.388053, 23.648306, -8.69295]","[3.9771626, 5.645971, 30.167732]","[15.635686, 9.073423]","[-9.049102, 9.939462]","[15.543028, 15.477211, -1.3772882]","[-11.750598, 1.8033956, 1.0719726]"
52,Hugues Vinet;Perfecto Herrera;François Pachet,The CUIDADO Project.,2002,https://doi.org/10.5281/zenodo.1416940,Hugues Vinet+IRCAM>FRA>facility|IUA-Universitat Pompeu Fabra>ESP>education|SONY-CSL>FRA>company;Perfecto Herrera+IUA-Universitat Pompeu Fabra>ESP>education;François Pachet+SONY-CSL>FRA>company,"The CUIDADO Project (Content-based Unified Interfaces and Descriptors for Audio/music Databases available Online) aims at developing a new chain of applications through the use of audio/music content descriptors, in the spirit of the MPEG-7 standard. The project includes the design of appropriate description structures, the development of extractors for deriving high-level information from audio signals, and the design and implementation of two applications: the Sound Palette and the Music Browser. These applications include new features, which systematically exploit high-level descriptors and provide users with content-based access to large catalogues of audio/music material. The Sound Palette focuses on audio samples and targets professional users, whereas the Music Browser addresses a broader user target through the management of Popular music titles. After a presentation of the project objectives and methodology, we describe the original features of the two applications based on the systematic use of descriptors and the technical architecture framework on which they rely.",FRA,facility,Developed economies,"[-16.69536, 39.269627]","[15.874536, 27.702538]","[-21.069248, -10.84738, -10.18122]","[-1.2016554, -7.1441193, 17.088219]","[14.084381, 8.020219]","[10.758626, 1.1150562]","[14.32915, 14.036117, -1.8869348]","[11.682566, 5.253947, 12.438288]"
53,Chaokun Wang;Jianzhong Li;Shengfei Shi,A Kind of Content-Based Music Information Retrieval Method in Peer-to-peer Environment.,2002,https://doi.org/10.5281/zenodo.1417441,Chaokun Wang+Harbin Institute of Technology>CHN>education;Jianzhong Li+Harbin Institute of Technology>CHN>education;Shengfei Shi+Harbin Institute of Technology>CHN>education,"""In this paper, we propose four peer-to-peer models for content-based music information retrieval (CBMIR) and carefully evaluate them on network load, retrieval time, system update and robustness qualitatively and quantitatively. And we bring forward an algorithm to improve the speed of CBP2PMIR and a simple but effective method to filter out the replica in the final results. And we present the architecture of QUIND, a content-based peer-to-peer music information retrieval system, which can implement CBMIR. QUIND combines content-based music information retrieval technologies and peer-to-peer environments, and has strong robustness and good expansibility. Music stored and shared on each PC makes up of the whole available music resource. When a user puts forward a music request, e.g. a song or a melody, QUIND can retrieve a lot of similar music quickly and accurately according to the content of music. After the user selects his favorite ones, he can download and enjoy them.""",CHN,education,Developing economies,"[-19.606476, 25.064112]","[48.887856, 19.610788]","[-12.975908, 9.984978, -15.347153]","[22.776226, -0.8189874, 14.608387]","[14.358242, 7.846015]","[11.964925, 1.848452]","[14.2645, 14.885905, -2.321136]","[12.936402, 5.2992716, 12.536207]"
54,Brian Whitman;Paris Smaragdis,Combining Musical and Cultural Features for Intelligent Style Detection.,2002,https://doi.org/10.5281/zenodo.1417471,Brian Whitman+MIT Media Lab>USA>education;Paris Smaragdis+MIT Media Lab>USA>education,"""""",USA,education,Developed economies,"[-23.815256, -17.744019]","[51.488228, 43.00346]","[-9.266183, 3.8409185, 21.563282]","[7.5143123, 4.190282, 29.906832]","[12.443167, 10.696577]","[-9.071951, 9.962302]","[13.4267435, 14.138848, 1.2354378]","[-11.721776, 1.7744634, 1.0430248]"
30,Beth Logan,Content-Based Playlist Generation: Exploratory Experiments.,2002,https://doi.org/10.5281/zenodo.1418061,Beth Logan+Hewlett-Packard Labs>USA>company,"""""",USA,company,Developed economies,"[-40.30077, 39.238033]","[49.818527, 43.048153]","[-1.8120862, 30.142063, -2.734572]","[6.9404545, 6.3641276, 30.076693]","[16.162868, 8.226634]","[-9.091628, 9.981962]","[16.545677, 14.83735, -1.7307692]","[-11.727294, 1.7800968, 1.0486728]"
56,Cheng Yang,MACSIS: A Scalable Acoustic Index for Content-Based Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1416662,Cheng Yang+Stanford University>USA>education,"""""",USA,education,Developed economies,"[-14.475586, 20.950619]","[51.10883, 42.265224]","[-6.3691287, 4.7492933, -13.894158]","[5.9199033, 4.762833, 30.161226]","[13.631604, 7.906997]","[-9.072447, 9.962791]","[13.663289, 14.594856, -1.9233594]","[-11.727936, 1.7807176, 1.049291]"
31,Thomas Mandl 0001;Christa Womser-Hacker,Learning to cope with Diversity in Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1416560,Thomas Mandl+University of Hildesheim>DEU>education|University of Hildesheim>DEU>education;Christa Womser-Hacker+University of Hildesheim>DEU>education|University of Hildesheim>DEU>education,"""""",DEU,education,Developed economies,"[-19.474827, 20.1955]","[49.97895, 42.444275]","[-10.877247, 11.255948, -11.301758]","[6.018176, 3.8558378, 31.508652]","[14.261877, 8.135158]","[-9.086902, 9.976926]","[14.42558, 14.85582, -1.8618648]","[-11.7082615, 1.761043, 1.0296149]"
29,Jin Ha Lee;J. Stephen Downie;Allen Renear,Representing Traditional Korean Music Notation in XML.,2002,https://doi.org/10.5281/zenodo.1418277,Jin Ha Lee+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Allen Renear+University of Illinois at Urbana-Champaign>USA>education,"XML promises to provide a powerful interoperable general framework for the development of music representation systems. Unfortunately current XML encoding systems for music focus almost exclusively on Western music from the 17th century onwards, and on the Western notation system, Common Music Notation (CMN). This is regrettably limiting, with cultural, theoretical, and practical consequences for MIR. In order to ensure that music information retrieval (MIR) systems have full theoretic generality, and wide practical application, we have begun a project to explore the representation, in XML, of a genre of traditional Korean music which has a distinctive notation system called Chôngganbo. Our project takes seriously the specific notational expression of musical intention and intends to ultimately contribute to the analysis of theoretical issues in music representation, as well as to the improvement of methods for representing Korean music specifically.",USA,education,Developed economies,"[9.351683, 26.626482]","[1.6077571, 40.142197]","[-12.263718, -10.565737, 15.4882145]","[-11.736523, -6.6506405, 21.927574]","[13.0468855, 6.9593782]","[9.994753, 0.12178785]","[13.858897, 12.697757, -1.6057816]","[10.573696, 4.835892, 11.910186]"
27,Youngmoo E. Kim;Brian Whitman,Singer Identification in Popular Music using Warped Linear Prediction.,2002,https://doi.org/10.5281/zenodo.1416954,Youngmoo E. Kim+MIT Media Lab>USA>education;Brian Whitman+MIT Media Lab>USA>education,"In most popular music, the vocals sung by the lead singer are the focal point of the song. The unique qualities of a singer’s voice make it relatively easy for us to identify a song as belonging to that particular artist. With little training, if one is familiar with a particular singer’s voice one can usually recognize that voice in other pieces, even when hearing a song for the first time. The research presented in this paper attempts to automatically establish the identity of a singer using acoustic features extracted from songs in a database of popular music. As a first step, an untrained algorithm for automatically extracting vocal segments from within songs is presented. Once these vocal segments are identified, they are presented to a singer identification system that has been trained on data taken from other songs by the same artists in the database.",USA,education,Developed economies,"[-13.218052, -37.18386]","[11.725006, -20.899445]","[16.560688, 14.717266, -21.176403]","[11.55034, -3.47364, -16.809378]","[10.224005, 11.600026]","[8.493974, 3.5980606]","[11.446808, 15.615381, 0.72396404]","[10.668247, 7.811654, 10.093571]"
55,Geraint A. Wiggins;Kjell Lemström;David Meredith 0001,"SIA(M)ESE: An Algorithm for Transposition Invariant, Polyphonic Content-Based Music Retrieval.",2002,https://doi.org/10.5281/zenodo.1415960,Geraint A. Wiggins+City University>GBR>education|City University>GBR>education;Kjell Lemström+University of Helsinki>FIN>education;David Meredith+City University>GBR>education,"We introduce a novel algorithm for transposition-invariant content-based polyphonic music retrieval. Our SIA(M)ESE algorithm is capable of finding transposition invariant occurrences of a given template, in a database of polyphonic music called a dataset. We allow arbitrary gapping, i.e., between musical events in the dataset that have been found to match points in the template, there may be any finite number of other intervening events. SIA(M)ESE can be implemented so that it finds all transposition-invariant complete matches for a d-dimensional template of size n in a d-dimensional dataset of size m in a worst-case running time of O(n^2 m log m); another implementation finds even the incomplete matches in O(n^2 m^2) time. The algorithm is generalizable to any arbitrary, multidimensional translation invariant pattern matching problem, where the events are representable by points in a multidimensional dataset.",GBR,education,Developed economies,"[-12.009515, 20.845783]","[9.388327, 17.358505]","[-5.6721, 3.2609413, -10.940961]","[8.156885, -11.310603, 5.743113]","[13.393015, 7.971765]","[8.992996, 0.88966024]","[13.358332, 14.506902, -1.9013208]","[10.653162, 6.4253488, 13.303684]"
47,Eric Schreier,About this Business of Metadata.,2002,https://doi.org/10.5281/zenodo.1414742,Eric D. Scheirer+Bose Corporation>USA>company,"A brief discussion presents some of the opportunities and challenges involved with creating metadata-centric businesses that bring Music Information Retrieval technologies to the marketplace. In particular, two related difficulties -- that of the difficulty of proving incremental value for new metadata systems, and that of the relative influidity of the marketplace for MIR -- are highlighted. Potential directions for resolving these issues are also discussed.",USA,company,Developed economies,"[-23.744383, 43.254616]","[21.880896, 34.203186]","[-27.334394, -1.685098, -8.409695]","[-0.085985236, 0.6292639, 20.228352]","[14.42847, 8.995693]","[11.429251, 0.22727248]","[15.243362, 13.470712, -1.2561185]","[11.911288, 4.576367, 12.152797]"
37,Frank Seifert 0001;Wolfgang Benn,Music identification by leadsheets: Converging perceptive and productive musical principles for estimation of semantic similarity of musical documents.,2003,https://doi.org/10.5281/zenodo.1417759,Frank Seifert+University of Technology>DEU>education;Wolfgang Benn+University of Technology>DEU>education,Most experimental research on content-based automatic recognition and identification of musical documents is founded on statistical distribution of timbre or simple retrieval mechanisms like comparison of melodic segments. Therefore often a vast number of relevant and irrelevant hits including multiple appearances of the same documents are returned or the actual document can’t be revealed at all. To improve this situation we propose a model for recognition of music that enables identification and comparison of musical documents without dependence on their actual instantiation. The resulting structures enclose musical meaning and can be used for estimation of identity and semantic relationship between musical documents.,DEU,education,Developed economies,"[-3.1673002, 12.472212]","[14.547955, 8.6029415]","[-3.9437895, 8.495292, 2.685374]","[2.630138, -3.9410152, 8.534505]","[12.693394, 9.090603]","[9.157689, 1.8947409]","[13.352819, 14.775689, -0.7043582]","[10.835576, 6.4351773, 12.181426]"
22,Arie Livshin;Xavier Rodet,The importance of cross database evaluation in musical instrument sound classification: A critical approach.,2003,https://doi.org/10.5281/zenodo.1417771,Arie Livshin+IRCAM Centre Pompidou>FRA>facility|Unknown>Unknown>Unknown;Xavier Rodet+IRCAM Centre Pompidou>FRA>facility|Unknown>Unknown>Unknown,"In numerous articles (Martin and Kim, 1998; Fraser and Fujinaga, 1999; and many others) sound classification algorithms are evaluated using ""self classification"" - the learning and test groups are randomly selected out of the same sound database. We will show that ""self classification"" is not necessarily a good statistic for the ability of a classification algorithm to learn, generalize or classify well. We introduce the alternative ""Minus-1 DB"" evaluation method and demonstrate that it does not have the shortcomings of ""self classification"". The importance of cross database evaluation will be demonstrated through a variety of classification experiments.",FRA,facility,Developed economies,"[-20.673489, -8.530452]","[18.58635, -2.7805452]","[-18.072985, -3.3985496, 9.689068]","[12.21224, 4.3548393, -1.7626585]","[12.506738, 10.212191]","[9.365882, 3.5462973]","[13.486198, 13.67781, 1.0096729]","[11.106066, 7.1788697, 10.520362]"
23,Namunu Chinthaka Maddage;Changsheng Xu;Ye Wang,An SVM-based classification approach to musical audio.,2003,https://doi.org/10.5281/zenodo.1415610,Namunu Chinthaka Maddage+Institute for Inforcomm Research>SGP>facility|Institute for Inforcomm Research>SGP>facility;Changsheng Xu+Institute for Inforcomm Research>SGP>facility;Ye Wang+National University of Singapore>SGP>education,"This paper describes an automatic hierarchical music classification approach based on support vector machines (SVM). Based on the proposed method, the music is classified into coursed classes such as vocal, instrumental or vocal mixed with instrumental music. These main classes are further sub-classed according to gender and instrument type. A novel method, Correction Algorithm for Music Sequence (CAMS) has been developed to improve the classification efficiency.",SGP,facility,Developing economies,"[-24.349577, -14.56857]","[18.571457, -8.91896]","[-11.204842, 2.2128334, 17.567408]","[11.772048, 10.968547, -6.9200463]","[12.466782, 10.522633]","[9.450566, 3.548048]","[13.510616, 13.919852, 1.0802178]","[11.519499, 7.1704865, 10.734718]"
42,Wei-Ho Tsai;Hsin-Min Wang;Dwight Rodgers;Shih-Sian Cheng;Hung-Ming Yu,Blind clustering of popular music recordings based on singer voice characteristics.,2003,https://doi.org/10.5281/zenodo.1415112,Wei-Ho Tsai+Academia Sinica>TWN>education;Hsin-Min Wang+Academia Sinica>TWN>education;Dwight Rodgers+Academia Sinica>TWN>education;Shi-Sian Cheng+Academia Sinica>TWN>education;Hung-Ming Yu+Academia Sinica>TWN>education,"This paper presents an effective technique for automatically clustering undocumented music recordings based on their associated singer. This serves as an indispensable step towards indexing and content-based information retrieval of music by singer. The proposed clustering system operates in an unsupervised manner, in which no prior information is available regarding the characteristics of singer voices, nor the population of singers. Methods are presented to separate vocal from non-vocal regions, to isolate the singers’ vocal characteristics from the background music, to compare the similarity between singers’ voices, and to determine the total number of unique singers from a collection of songs. Experimental evaluations conducted on a 200-track pop music database confirm the validity of the proposed system.",TWN,education,Developing economies,"[-10.932305, -36.242798]","[27.468035, 13.095344]","[18.712797, 15.176262, -17.427782]","[18.204905, 0.47447166, 7.416272]","[10.241563, 11.289896]","[10.577393, 2.37232]","[11.480593, 15.438181, 0.64964217]","[11.997879, 6.380114, 12.188736]"
25,Colin Meek;William P. Birmingham,The dangers of parsimony in query-by-humming applications.,2003,https://doi.org/10.5281/zenodo.1415828,Colin Meek+University of Michigan>USA>education|University of Michigan>USA>education;William P. Birmingham+University of Michigan>USA>education|University of Michigan>USA>education,"Query-by-humming systems attempt to address the needs of the non-expert user, for whom the most natural query format – for the purposes of finding a tune, hook or melody of unknown providence – is to sing it. While human listeners are quite tolerant of error in these queries, a music retrieval mechanism must explicitly model such errors in order to perform its task. We will present a unifying view of existing models, illuminating the assumptions underlying their respective designs, and demonstrating where such assumptions succeed and fail, through analysis and real-world experiments.",USA,education,Developed economies,"[-2.6683753, 36.94516]","[10.724166, 12.206943]","[-12.82497, -9.076839, -23.668156]","[6.659278, -12.54896, 12.914526]","[14.827991, 6.14545]","[8.871645, 0.47948432]","[13.2227125, 15.3111925, -2.934594]","[10.464089, 5.8985925, 13.213219]"
26,T. Olson;S. J. Downie,Chopin early editions: The construction and usage of a collection of digital scores.,2003,https://doi.org/10.5281/zenodo.1417397,Tod A. Olson+The University of Chicago Library>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"The University of Chicago Library has digitized a collection of 19th century music scores. The online collection is generated programmatically from the scanned images and human-created descriptive and structural metadata, encoded as METS objects, and delivered using the Greenstone Digital Library software. Use statistics are analyzed and possible future directions for the collection are discussed.",USA,education,Developed economies,"[4.8070173, 29.366732]","[16.759708, 20.895752]","[-6.6874976, 0.05695301, -29.792282]","[4.1508613, -7.1898317, 16.8401]","[11.383334, 5.1008935]","[10.105861, 0.7713443]","[11.236226, 13.02213, -2.5303888]","[11.169462, 5.3910527, 12.62584]"
27,Nicola Orio;M. Sisti Sette,An HMM-based pitch tracker for audio queries.,2003,https://doi.org/10.5281/zenodo.1417601,Nicola Orio+University of Padova>ITA>education|Unknown>Unknown>Unknown;Matteo Sisti Sette+University of Padova>ITA>education|Unknown>Unknown>Unknown,"In this paper we present an approach to the transcription of musical queries based on a hidden Markov model (HMM). The HMM is used to model the audio features related to the singing voice, and the transcription is obtained through Viterbi decoding. We report our preliminary work on evaluation of the system.",ITA,education,Developed economies,"[27.026848, -22.960741]","[-0.3477553, -10.581824]","[11.987139, -17.518452, -9.552118]","[2.640836, -6.91309, -10.024755]","[9.960485, 5.5929995]","[6.8309855, 3.003623]","[10.751819, 13.199118, -0.95599556]","[9.282383, 7.9584317, 11.307754]"
28,Elias Pampalk;Simon Dixon;Gerhard Widmer,Exploring music collections by browsing different views.,2003,https://doi.org/10.5281/zenodo.1416876,Elias Pampalk+Austrian Research Institute for Artificial Intelligence (OeFAI)>AUT>facility;Simon Dixon+Austrian Research Institute for Artificial Intelligence (OeFAI)>AUT>facility;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OeFAI)>AUT>facility|University of Vienna>AUT>education,"The availability of large music collections calls for ways to efficiently access and explore them. We present a new approach which combines descriptors derived from audio analysis with meta-information to create different views of a collection. Such views can have a focus on timbre, rhythm, artist, style or other aspects of music. For each view the pieces of music are organized on a map in such a way that similar pieces are located close to each other. The maps are visualized using an Islands of Music metaphor where islands represent groups of similar pieces. The maps are linked to each other using a new technique to align self-organizing maps. The user is able to browse the collection and explore different aspects by gradually changing focus from one view to another. We demonstrate our approach on a small collection using a meta-information-based view and two views generated from audio analysis, namely, beat periodicity as an aspect of rhythm and spectral information as an aspect of timbre.",AUT,facility,Developed economies,"[-17.045523, 27.553722]","[27.7219, 18.666073]","[-10.182958, 6.677614, -19.500645]","[14.803806, -4.9668097, 22.838762]","[14.25181, 7.507065]","[11.184062, 1.7491829]","[14.220129, 14.403674, -2.4735022]","[12.332052, 5.6842, 13.184307]"
29,R. Mitchell Parry;Irfan A. Essa,Rhythmic similarity through elaboration.,2003,https://doi.org/10.5281/zenodo.1416738,Mitchell Parry+Georgia Institute of Technology>USA>education|GVU Center>USA>education;Irfan Essa+Georgia Institute of Technology>USA>education|GVU Center>USA>education,Rhythmic similarity techniques for audio tend to evaluate how close to identical two rhythms are. This paper proposes a similarity metric based on rhythmic elaboration that matches rhythms that share the same beats regardless of tempo or identicalness. Elaborations can help an application decide where to transition between songs. Potential applications include automatically generating a non-stop music mix or sonically browsing a music library.,USA,education,Developed economies,"[44.77609, 7.2410913]","[-22.523304, 2.7839386]","[-8.617472, -24.40792, 5.1327324]","[0.16271135, 14.723402, -4.0684805]","[11.979528, 5.5417943]","[6.232726, 1.5095333]","[11.412659, 14.155349, -2.0880334]","[8.278245, 6.573986, 11.981506]"
31,Jeremy Pickens,Key-specific shrinkage techniques for harmonic models.,2003,https://doi.org/10.5281/zenodo.1414776,Jeremy Pickens+University of Massachusetts>USA>education,"""""",USA,education,Developed economies,"[35.050644, -14.27232]","[50.35755, 43.812466]","[12.279423, -21.501986, 15.021688]","[5.3823586, 5.7825327, 28.739132]","[9.127332, 9.070265]","[-9.071819, 9.962139]","[11.395231, 13.4235325, 0.19564776]","[-11.695555, 1.748339, 1.0169115]"
32,Christopher Raphael;Josh Stoddard,Harmonic analysis with probabilistic graphical models.,2003,https://doi.org/10.5281/zenodo.1415574,Christopher Raphael+University of Massachusetts>USA>education|University of Massachusetts>USA>education;Josh Stoddard+University of Massachusetts>USA>education|University of Massachusetts>USA>education,"A technique for harmonic analysis is presented that partitions a piece of music into contiguous regions and labels each with the key, mode, and functional chord, e.g. tonic, dominant, etc. The analysis is performed with a hidden Markov model and, as such, is automatically trainable from generic MIDI files and capable of finding the globally optimal harmonic labeling. Experiments are presented highlighting our current state of the art. An extension to a more complex probabilistic graphical model is outlined in which music is modeled as a collection of voices that evolve independently given the harmonic progression.",USA,education,Developed economies,"[35.660465, -14.391174]","[-7.242096, -0.13311225]","[11.544399, -21.894886, 13.339551]","[1.1667503, -5.38655, -6.1248655]","[9.063343, 9.060969]","[7.0632415, 3.111043]","[11.399833, 13.416555, 0.20399413]","[9.695213, 8.1534815, 11.84678]"
33,Julien Ricard;Perfecto Herrera,Using morphological description for generic sound retrieval.,2003,https://doi.org/10.5281/zenodo.1416042,Julien Ricard+Pompeu Fabra University>ESP>education;Perfecto Herrera+Pompeu Fabra University>ESP>education,"Systems for sound retrieval are usually “source-centred”. This means that retrieval is based on using the proper keywords that define or specify a sound source. Although this type of description is of great interest, it is very difficult to implement it into realistic automatic labelling systems because of the necessity of dealing with thousands of categories, hence with thousands of different sound models. Moreover, digitally synthesised or transformed sounds, which are frequently used in most of the contemporary popular music, have no identifiable sources. We propose a description framework, based on Schaeffer’s research on a generalised solfeggio which could be applied to any type of sounds. He defined some morphological description criteria, based on intrinsic perceptual qualities of sound, which doesn’t refer to the cause or the meaning of a sound. We describe more specifically experiments on automatic extraction of morphological descriptors.",ESP,education,Developed economies,"[-22.279654, -5.5690346]","[-2.1551306, 20.159933]","[-22.225306, -4.7531223, -17.026062]","[-8.981855, -12.245855, 13.31828]","[13.012952, 8.247817]","[9.009124, 2.7972953]","[13.05368, 14.349508, -1.3671974]","[10.624436, 6.656391, 10.998346]"
21,Dan Liu 0001;Lie Lu;HongJiang Zhang,Automatic mood detection from acoustic music data.,2003,https://doi.org/10.5281/zenodo.1418335,Dan Liu+Tsinghua University>CHN>education;Lie Lu+Microsoft Research Asia>CHN>company;Hong-Jiang Zhang+Microsoft Research Asia>CHN>company,"Music mood describes the inherent emotional meaning of a music clip. It is helpful in music understanding, music search and some music-related applications. In this paper, a hierarchical framework is presented to automate the task of mood detection from acoustic music data, by following some music psychological theories in western cultures. Three feature sets, intensity, timbre and rhythm, are extracted to represent the characteristics of a music clip. Moreover, a mood tracking approach is also presented for a whole piece of music. Experimental evaluations indicate that the proposed algorithms produce satisfactory results.",CHN,education,Developing economies,"[-55.64724, 3.3096774]","[53.00996, -6.1195583]","[-19.887196, 23.451712, 4.1244755]","[10.561797, 20.679758, 7.0948105]","[13.5626335, 12.6385145]","[13.172876, 3.8231826]","[16.157072, 14.8478985, 1.5397105]","[14.2219, 5.014455, 10.630744]"
34,P. Roland,Design patterns in XML music representation.,2003,https://doi.org/10.5281/zenodo.1417445,Perry Roland+University of Virginia>USA>education,"Design patterns attempt to formalize the discussion of recurring problems and their solutions. This paper introduces several XML design patterns and demonstrates their usefulness in the development of XML music representations. The patterns have been grouped into several categories of desirable outcome of the design process – modularity, separation of data and meta-data, reduction of learning requirements, assistance to tool development, and increase in legibility and understandability. The Music Encoding Initiative (MEI) DTD, from which the examples are drawn, the examples, and other materials related to MEI are available at http://www.people.virginia.edu/~pdr4h/.",USA,education,Developed economies,"[9.79518, 27.356926]","[2.8764188, 40.696533]","[-12.487026, -9.88774, 13.349793]","[-9.995098, -7.6467752, 22.995092]","[13.195266, 6.9135823]","[10.085996, 0.10233039]","[13.837365, 12.713572, -1.6158075]","[10.661692, 4.8090496, 11.961615]"
36,Anthony Seeger,"I Found It, How Can I Use It?"" - Dealing With the Ethical and Legal Constraints of Information Access.",2003,https://doi.org/10.5281/zenodo.1417719,Anthony Seeger+Smithsonian Institution>USA>facility,"It is very easy to find music on the Internet today, but how it may be used is the source of considerable conflict, front-page news stories, and increasingly of scholarly reflection. One of the frustrations for libraries, archives, and patrons alike is the gulf between the information about a holding and actual access to it. But users are not the only ones to have an opinion about free access. Local musicians feel that everyone profits from their cultural heritage but them; researchers find themselves held responsible for research recordings made decades earlier and largely forgotten; and some communities seek to protect music that was never meant to be commercialized, and is considered to be secret or divine. Caught in the middle between angry patrons, angry companies, and angry artists, what are music librarians and archivists supposed to do? Using his own experience as a researcher, archivist, and record producer, the author discusses the issues and makes some suggestions that can help those who wish to use the music they can so easily find out about. It is a great honor to be with you at the ISMIR 2003. I have devoted much of my life to making information available for eventual retrieval, and it is nice to be among specialists in doing just that. As a researcher, I have made field recordings among indigenous peoples in remote jungles of Brazil. As the director of an audiovisual archive I wanted to make available as much information as possible about the collections by publishing printed catalogs, entering collection-level information on OCLC, creating in-house databases, and revising depositors’ contracts. As a record company director I have produced hundreds of CDs with extensive liner notes, maintained a vast back catalog in print, and moved early to supplying information on the Internet. As the archival consultant to the Smithsonian Institution's GlobalSound Internet music project, I have continued to search for new ways to make information about music, and music itself, available to as wide a public as we can reach. A huge amount of music is available on the Internet today, and even more music is signaled in myriad archives catalogues. More will music will certainly become available. One problem we face is finding what is there (and also what isn’t). Another problem is finding out how we may use it. The short summary of my talk would be: although you can find it, a variety of forces (not all of them related to greed) shape the way music should be used. As specialists in information retrieval, we must also become specialists in helping others learn not only the techniques of finding music, but also the ethics of using it.",USA,facility,Developed economies,"[-22.874237, 44.80845]","[26.80456, 38.982674]","[-30.375868, -2.1988413, -8.59533]","[0.7863655, 6.2565174, 23.219366]","[14.676566, 8.710931]","[11.646843, 0.16783527]","[15.3391485, 13.245995, -1.3616196]","[12.04847, 4.2840767, 12.14913]"
38,Alexander Sheh;Daniel P. W. Ellis,Chord segmentation and recognition using EM-trained hidden markov models.,2003,https://doi.org/10.5281/zenodo.1416734,Alexander Sheh+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Automatic extraction of content description from commercial audio recordings has a number of important applications, from indexing and retrieval through to novel musicological analyses based on very large corpora of recorded performances. Chord sequences are a description that captures much of the character of a piece in a compact form and using a modest lexicon. Chords also have the attractive property that a piece of music can (mostly) be segmented into time intervals that consist of a single chord, much as recorded speech can (mostly) be segmented into time intervals that correspond to specific words. In this work, we build a system for automatic chord transcription using speech recognition tools. For features we use “pitch class profile” vectors to emphasize the tonal content of the signal, and we show that these features far outperform cepstral coefficients for our task. Sequence recognition is accomplished with hidden Markov models (HMMs) directly analogous to subword models in a speech recognizer, and trained by the same Expectation-Maximization (EM) algorithm. Crucially, this allows us to use as input only the chord sequences for our training examples, without requiring the precise timings of the chord changes — which are determined automatically during training. Our results on a small set of 20 early Beatles songs show frame-level accuracy of around 75% on a forced-alignment task.",USA,education,Developed economies,"[55.767406, -5.934999]","[-32.3835, 19.390411]","[29.380196, -12.069851, 18.375505]","[-23.677736, -5.8217416, 0.17589848]","[6.619425, 8.726754]","[6.0943084, 3.741493]","[11.89769, 10.332414, 2.12323]","[9.6693535, 8.989399, 12.163253]"
39,Jonah Shifrin;William P. Birmingham,Effectiveness of HMM-based retrieval on large databases.,2003,https://doi.org/10.5281/zenodo.1417187,Jonah Shifrin+University of Michigan>USA>education|University of Michigan>USA>education;William Birmingham+University of Michigan>USA>education|University of Michigan>USA>education,"We have investigated the performance of a hidden Markov model QBH retrieval system on a large musical database. The database is synthetic, generated from statistics gleaned from our (smaller) database of musical excerpts from various genres. This paper reports the performance of several variations of our retrieval system against different types of synthetic queries on the large database, where we can control the errors injected into the queries. We note several trends, among the most interesting is that as queries get longer (i.e., more notes) the retrieval performance improves.",USA,education,Developed economies,"[-1.403178, 35.690826]","[10.168099, 13.408385]","[-10.599342, -5.699103, -22.862015]","[8.274306, -13.917958, 15.015572]","[14.715645, 6.256509]","[8.981273, 0.48118088]","[13.278847, 15.2048, -2.7787836]","[10.5656395, 5.867296, 13.170624]"
40,Ferréol Soulez;Xavier Rodet;Diemo Schwarz,Improving polyphonic and poly-instrumental music to score alignment.,2003,https://doi.org/10.5281/zenodo.1415542,Ferréol Soulez+IRCAM – Centre Pompidou>FRA>facility;Xavier Rodet+IRCAM – Centre Pompidou>FRA>facility;Diemo Schwarz+IRCAM – Centre Pompidou>FRA>facility,"Music alignment links events in a score and points on the audio performance time axis. All the parts of a recording can be thus indexed according to score information. The automatic alignment presented in this paper is based on a dynamic time warping method. Local distances are computed using the signal’s spectral features through an attack plus sustain note modeling. The method is applied to mixtures of harmonic sustained instruments, excluding percussion for the moment. Good alignment has been obtained for polyphony of up to five instruments. The method is robust for difficulties such as trills, vibratos and fast sequences. It provides an accurate indicator giving position of score interpretation errors and extra or forgotten notes. Implementation optimizations allow aligning long sound files in a relatively short time. Evaluation results have been obtained on piano jazz recordings.",FRA,facility,Developed economies,"[17.6634, -12.567531]","[-16.137243, -13.164319]","[0.89880383, -13.935226, -6.5178633]","[0.16743076, -19.748882, -3.4061153]","[10.885395, 6.44394]","[6.2241974, 0.7595062]","[12.002735, 12.665155, -1.4864722]","[8.23181, 5.847214, 10.900181]"
41,Haruto Takeda;Takuya Nishimoto;Shigeki Sagayama,Automatic rhythm transcription from multiphonic MIDI signals.,2003,https://doi.org/10.5281/zenodo.1415222,Haruto Takeda+The University of Tokyo>JPN>education;Takuya Nishimoto+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"For automatically transcribing human-performed polyphonic music recorded in the MIDI format, rhythm and tempo are decomposed through probabilistic modeling using Viterbi search in HMM for recognizing the rhythm and EM Algorithm for estimating the tempo. Experimental evaluation are also presented.",JPN,education,Developed economies,"[42.378433, -0.9951902]","[-17.21863, -4.047064]","[8.78081, -14.488032, 21.911386]","[3.6372924, -0.6629853, -11.89162]","[9.869121, 7.080947]","[6.2437425, 1.9974344]","[12.027789, 11.818253, -0.34481055]","[8.264447, 6.892612, 11.048663]"
49,Gavin Wood;Simon O'Keefe,Quantitative comparisons into content-based music recognition with the self organising map.,2003,https://doi.org/10.5281/zenodo.1415644,Gavin Wood+University of York>GBR>education|University of York>GBR>education;Simon O’Keefe+University of York>GBR>education|University of York>GBR>education,"With so much modern music being so widely available both in electronic form and in more traditional physical formats, a great opportunity exists for the development of a general-purpose recognition and music classification system. We describe an ongoing investigation into the subject of musical recognition purely by the sonic content from a standard recording.",GBR,education,Developed economies,"[-5.941516, 7.8112965]","[10.88098, -10.5281515]","[-7.553051, -2.4198105, -1.3282852]","[10.800261, -1.3296887, -6.9484344]","[12.474901, 8.783769]","[9.041583, 3.3961442]","[13.27308, 14.375687, -0.56191355]","[10.737971, 7.4106655, 10.567335]"
47,Esko Ukkonen;Kjell Lemström;Veli Mäkinen,Geometric algorithms for transposition invariant content based music retrieval.,2003,https://doi.org/10.5281/zenodo.1417477,Esko Ukkonen+University of Helsinki>FIN>education;Kjell Lemström+University of Helsinki>FIN>education;Veli Mäkinen+University of Helsinki>FIN>education,"We represent music as sets of points or sets of horizontal line segments in the Euclidean plane. Via this geometric representation we cast transposition invariant content-based music retrieval problems as ones of matching sets of points or sets of horizontal line segments in plane under translations. For finding the exact occurrences of a point set (the query pattern) of size within another point set (representing the database) of size, we give an algorithm with running time, and for finding partial occurrences another algorithm with running time. We also use the total length of the overlap between the line segments of a translated query and a database (i.e., the shared time) as a quality measure of an occurrence and present an algorithm for finding translations giving the largest possible overlap. Some experimental results on the performance of the algorithms are reported.",FIN,education,Developed economies,"[-12.18926, 21.40227]","[10.57671, 17.071642]","[-4.941545, 4.1530085, -9.620523]","[8.977559, -9.80461, 7.5039873]","[13.400132, 8.042123]","[9.180644, 0.7876484]","[13.314445, 14.574266, -1.9269984]","[10.818382, 6.2858872, 13.316022]"
46,Alexandra L. Uitdenbogerd;Yaw Wah Yap,Was Parsons right? An experiment in usability of music representations for melody-based music retrieval.,2003,https://doi.org/10.5281/zenodo.1418225,Alexandra L. Uitdenbogerd+RMIT University>AUS>education;Yaw Wah Yap+RMIT University>AUS>education,"In 1975 Parsons developed his dictionary of musical themes based on a simple contour representation. The motivation was that people with little training in music would be able to identify pieces of music. We decided to test whether people of various levels of musical skill could indeed make use of a text representation to describe a simple melody query. The results indicate that the task is beyond those who are unmusical, and that a scale numeric representation is easier than a contour one for those of moderate musical skill. Further, a common error when using the scale representation still yields a more accurate contour representation than if a user is asked to enter a contour query. We observed an average query length of about seven symbols for the retrieval task.",AUS,education,Developed economies,"[-24.649412, 22.260885]","[15.491116, 12.586092]","[-15.536593, 11.107228, -6.7268734]","[3.8810444, -2.6459818, 11.823165]","[14.333368, 8.116219]","[9.567738, 1.1147329]","[14.327928, 14.882736, -1.766981]","[11.034263, 6.1993523, 12.74759]"
45,George Tzanetakis;Jun Gao;Peter Steenkiste,A scalable peer-to-peer system for music content and information retrieval.,2003,https://doi.org/10.5281/zenodo.1417451,George Tzanetakis+Carnegie Mellon University>USA>education;Jun Gao+Carnegie Mellon University>USA>education;Peter Steenkiste+Carnegie Mellon University>USA>education,"Currently a large percentage of Internet traffic consists of music files, typically stored in MP3 compressed audio format, shared and exchanged over Peer-to-Peer (P2P) networks. Searching for music is performed by specifying keywords and naive string matching techniques. In the past years the emerging research area of Music Information Retrieval (MIR) has produced a variety of new ways of looking at the problem of music search. Such MIR techniques can significantly enhance the ways user search for music over P2P networks. In order for that to happen there are two main challenges that need to be addressed: 1) scalability to large collections and number of peers, 2) richer set of search semantics that can support MIR especially when retrieval is content-based. In this paper, we describe a scalable P2P system that uses Rendezvous Points (RPs) for music metadata registration and query resolution, that supports attribute-value search semantics as well as content-based retrieval. The performance of the system has been evaluated in large scale usage scenarios using “real” automatically calculated musical content descriptors.",USA,education,Developed economies,"[-19.62542, 25.295975]","[48.925636, 19.561274]","[-13.702193, 9.600615, -16.504086]","[23.277548, -0.7276813, 14.336771]","[14.474549, 7.844474]","[11.871535, 1.8249376]","[14.34105, 14.906981, -2.2752938]","[12.832547, 5.3568125, 12.527318]"
44,Rainer Typke;Panos Giannopoulos;Remco C. Veltkamp;Frans Wiering;René van Oostrum,Using transportation distances for measuring melodic similarity.,2003,https://doi.org/10.5281/zenodo.1417513,Rainer Typke+University of Utrecht>NLD>education|Institute of Information and Computing Sciences>Unknown>Unknown;Panos Giannopoulos+University of Utrecht>NLD>education|Institute of Information and Computing Sciences>Unknown>Unknown;Remco C. Veltkamp+University of Utrecht>NLD>education|Institute of Information and Computing Sciences>Unknown>Unknown;Frans Wiering+University of Utrecht>NLD>education|Institute of Information and Computing Sciences>Unknown>Unknown;René van Oostrum+University of Utrecht>NLD>education|Institute of Information and Computing Sciences>Unknown>Unknown,"Most of the existing methods for measuring melodic similarity use one-dimensional textual representations of music notation, so that melodic similarity can be measured by calculating editing distances. We view notes as weighted points in a two-dimensional space, with the coordinates of the points reflecting the pitch and onset time of notes and the weights of points depending on the corresponding notes’ duration and importance. This enables us to measure similarity by using the Earth Mover’s Distance (EMD) and the Proportional Transportation Distance (PTD), a pseudo-metric for weighted point sets which is based on the EMD. A comparison of our experiment results with earlier work shows that by using weighted point sets and the EMD/PTD instead of Howard’s method (1998) using the DARMS encoding for determining melodic similarity, it is possible to group together about twice as many known occurrences of a melody within the RISM A/II collection. Also, the percentage of successfully identified authors of anonymous incipits can almost be doubled by comparing weighted point sets instead of looking for identical representations in Plaine & Easie encoding as Schlichte did in 1990.",NLD,education,Developed economies,"[0.35118976, 16.969677]","[15.6773815, 5.2172065]","[0.5354263, 6.7234783, -0.37516928]","[7.578501, -0.4447033, 3.708919]","[12.314413, 9.76816]","[9.331197, 1.7120394]","[12.8055, 15.535926, -0.7662495]","[11.21627, 6.982042, 13.055267]"
43,Robert J. Turetsky;Daniel P. W. Ellis,Ground-truth transcriptions of real music from force-aligned MIDI syntheses.,2003,https://doi.org/10.5281/zenodo.1417667,Robert J. Turetsky+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Many modern polyphonic music transcription algorithms are presented in a statistical pattern recognition framework. But without a large corpus of real-world music transcribed at the note level, these algorithms are unable to take advantage of supervised learning methods and also have difficulty reporting a quantitative metric of their performance, such as a Note Error Rate. We attempt to remedy this situation by taking advantage of publicly-available MIDI transcriptions. By force-aligning synthesized audio generated from a MIDI transcription with the raw audio of the song it represents we can correlate note events within the MIDI data with the precise time in the raw audio where that note is likely to be expressed. Having these alignments will support the creation of a polyphonic transcription system based on labeled segments of produced music. But because the MIDI transcriptions we find are of variable quality, an integral step in the process is automatically evaluating the integrity of the alignment before using the transcription as part of any training set of labeled examples. Comparing a library of 40 published songs to freely available MIDI files, we were able to align 31 (78%). We are building a collection of over 500 MIDI transcriptions matching songs in our commercial music collection, for a potential total of 35 hours of note-level transcriptions, or some 1.5 million note events.",USA,education,Developed economies,"[40.181087, -0.66136056]","[-13.269568, -12.210545]","[6.7629747, -11.839649, 23.035402]","[-2.355443, -15.830445, -6.1018224]","[9.936845, 7.2158327]","[6.6848183, 1.8515444]","[12.2918005, 11.525825, -0.438623]","[8.712882, 6.5119176, 10.551667]"
35,B. Schwartz,Music Notation as a MEI Feasability Test.,2003,https://doi.org/10.5281/zenodo.1416136,Baron Schwartz+University of Virginia>USA>education,"This project demonstrated that enough information can be retrieved from MEI, an XML format for musical information representation, to transform it into music notation with good fidelity. The process involved writing an XSLT script to transform files into Mup, an intermediate format, then processing the Mup into PostScript, the de facto page description language for high-quality printing. The results show that the MEI format represents musical information such that it may be retrieved simply, with good recall and precision.",USA,education,Developed economies,"[10.208705, 23.672646]","[2.2290032, 39.43672]","[-10.979357, -13.468671, 18.203884]","[-10.668045, -7.6081862, 20.360062]","[12.28766, 6.897721]","[10.002249, 0.109302446]","[13.713613, 12.467779, -1.3574991]","[10.518254, 4.835987, 11.895639]"
20,Tao Li 0001;Mitsunori Ogihara,Detecting emotion in music.,2003,https://doi.org/10.5281/zenodo.1417293,Tao Li+University of Rochester>USA>education;Mitsunori Ogihara+University of Rochester>USA>education,"""""",USA,education,Developed economies,"[-59.31072, 0.49268776]","[50.663967, 42.148617]","[-27.136318, 22.29314, 3.9356961]","[5.1760592, 7.0238843, 30.382269]","[14.011262, 12.926656]","[-9.0546, 9.944914]","[16.053831, 14.374496, 1.8419735]","[-11.726933, 1.779752, 1.0483315]"
24,Martin F. McKinney;Jeroen Breebaart,Features for audio and music classification.,2003,https://doi.org/10.5281/zenodo.1415026,Martin F. McKinney+Philips Research Laboratories>NLD>company;Jeroen Breebaart+Philips Research Laboratories>NLD>company,"Four audio feature sets are evaluated in their ability to classify five general audio classes and seven popular music genres. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for both music and audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features.",NLD,company,Developed economies,"[-23.78808, -13.771145]","[16.203588, -16.506237]","[-11.783, -0.1272161, 16.967127]","[14.303059, 2.6990743, -11.887679]","[12.364506, 10.396478]","[9.139242, 3.6761093]","[13.429729, 13.902325, 1.0013967]","[10.943039, 7.611356, 10.53297]"
18,Kjell Lemström;Veli Mäkinen;Anna Pienimäki;M. Turkia;Esko Ukkonen,The C-BRAHMS project.,2003,https://doi.org/10.5281/zenodo.1417551,Kjell Lemström+University of Helsinki>FIN>education;Veli Mäkinen+University of Helsinki>FIN>education;Anna Pienimäki+University of Helsinki>FIN>education;Mika Turkia+University of Helsinki>FIN>education;Esko Ukkonen+University of Helsinki>FIN>education,The C-BRAHMS project develops computational methods for content-based retrieval and analysis of music data. A summary of the recent algorithmic and experimental developments of the project is given. The search engine developed by the project is available at http://www.cs.helsinki.fi/group/cbrahms.,FIN,education,Developed economies,"[1.3358005, 25.241032]","[16.744307, 19.331532]","[-15.621869, -6.756181, -2.8531253]","[4.9883432, -5.3767695, 14.754643]","[12.593187, 7.107232]","[9.93901, 0.9357203]","[13.178455, 13.456158, -1.7111993]","[11.187922, 5.6316285, 12.696354]"
1,Vlora Arifi;Michael Clausen;Frank Kurth;Meinard Müller,"Automatic synchronization of music data in score-, MIDI- and PCM-format.",2003,https://doi.org/10.5281/zenodo.1417848,Vlora Arifi+Universität Bonn>DEU>education;Michael Clausen+Universität Bonn>DEU>education;Frank Kurth+Universität Bonn>DEU>education;Meinard Müller+Universität Bonn>DEU>education,"In this paper we present algorithms for the automatic time-synchronization of score-, MIDI- or PCM-data streams representing the same polyphonic piano piece.",DEU,education,Developed economies,"[25.178076, -30.452293]","[-17.21571, -9.640424]","[-1.9822787, -19.7193, -13.4813595]","[0.7350343, -17.63296, -1.401126]","[11.07888, 5.9646215]","[6.024018, 1.1758032]","[12.131833, 12.418317, -2.0192294]","[8.210498, 6.206633, 11.144783]"
2,David Bainbridge 0001;Sally Jo Cunningham;J. Stephen Downie,Analysis of queries to a Wizard-of-Oz MIR system: Challenging assumptions about what people really want.,2003,https://doi.org/10.5281/zenodo.1416850,David Bainbridge+University of Waikato>NZL>education;Sally Jo Cunningham+University of Waikato>NZL>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"How do users of music information retrieval (MIR) systems express their needs? Using a Wizard of Oz approach to system evaluation, combined with a grounded theory analysis of 502 real-world music queries posted to Google Answers, this paper addresses this pivotal question.",NZL,education,Developed economies,"[-6.6360354, 57.256634]","[28.556273, 31.900326]","[-34.90946, -0.39298415, -8.637322]","[4.2237463, 6.6562862, 16.358227]","[13.631594, 4.7505255]","[11.946401, 0.7570791]","[15.102818, 11.172035, -1.5481193]","[12.443827, 4.5290895, 12.244636]"
19,Micheline Lesaffre;Koen Tanghe;Gaëtan Martens;Dirk Moelants;Marc Leman;Bernard De Baets;Hans De Meyer;Jean-Pierre Martens,The MAMI query-by-voice experiment: collecting and annotating vocal queries for music information retrieval.,2003,https://doi.org/10.5281/zenodo.1418133,Micheline Lesaffre+Ghent University>BEL>education|IPEM>BEL>facility;Koen Tanghe+Ghent University>BEL>education|IPEM>BEL>facility;Gaëtan Martens+Ghent University>BEL>education|IPEM>BEL>facility;Dirk Moelants+Ghent University>BEL>education|IPEM>BEL>facility;Marc Leman+Ghent University>BEL>education|IPEM>BEL>facility;Bernard De Baets+Ghent University>BEL>education|IPEM>BEL>facility;Hans De Meyer+Ghent University>BEL>education|IPEM>BEL>facility;Jean-Pierre Martens+Ghent University>BEL>education|IPEM>BEL>facility,The MIR research community requires coordinated strategies in dealing with databases for system development and experimentation. Manually annotated files can accelerate the development of accurate analysis tools for music information retrieval. This paper presents background information on an annotated database of vocal queries that is freely available on the Internet. First we outline the design and set up of the experiment through which the vocal queries were generated. Then attention is drawn to the manual annotation of the vocal queries.,BEL,education,Developed economies,"[-21.403805, 1.6772399]","[19.80202, 26.954113]","[10.159414, 13.896073, -10.504899]","[2.8094964, -0.012138302, 16.174839]","[10.754781, 11.165202]","[10.987007, 0.6757477]","[11.934759, 15.543196, 0.21856062]","[11.604657, 5.0723486, 11.946715]"
3,Adam Berenzweig;Beth Logan;Daniel P. W. Ellis;Brian Whitman,A large-scale evalutation of acoustic and subjective music similarity measures.,2003,https://doi.org/10.5281/zenodo.1417010,Adam Berenzweig+Columbia University>USA>education;Beth Logan+HP Labs>USA>company;Daniel P. W. Ellis+Columbia University>USA>education;Brian Whitman+MIT Media Lab>USA>education,"Subjective similarity between musical pieces and artists is an elusive concept, but one that must be pursued in support of applications to provide automatic organization of large music collections. In this paper, we examine both acoustic and subjective approaches for calculating similarity between artists, comparing their performance on a common database of 400 popular artists. Specifically, we evaluate acoustic techniques based on Mel-frequency cepstral coefficients and an intermediate ‘anchor space’ of genre classification, and subjective techniques which use data from The All Music Guide, from a survey, from playlists and personal collections, and from web-text mining. We find the following: (1) Acoustic-based measures can achieve agreement with ground truth data that is at least comparable to the internal agreement between different subjective sources. However, we observe significant differences between superficially similar distribution modeling and comparison techniques. (2) Subjective measures from diverse sources show reasonable agreement, with the measure derived from co-occurrence in personal music collections being the most reliable overall. (3) Our methodology for large-scale cross-site music similarity evaluations is practical and convenient, yielding directly comparable numbers for different approaches. In particular, we hope that our information-retrieval-based approach to scoring similarity measures, our paradigm of sharing common feature representations, and even our particular dataset of features for 400 artists, will be useful to other researchers.",USA,education,Developed economies,"[-4.1409273, 14.380705]","[32.749702, 7.0311475]","[-6.93915, 6.689671, -0.77615577]","[12.974177, 5.060976, 9.222061]","[13.060345, 9.33615]","[11.441593, 2.4610295]","[13.543608, 15.218016, -0.6748423]","[12.837099, 6.2902484, 12.313276]"
4,Elaine Chew;Yun-Ching Chen,Determining context-defining windows: Pitch spelling using the spiral array.,2003,https://doi.org/10.5281/zenodo.1418037,Elaine Chew+University of Southern California>USA>education|Integrated Media Systems Center>USA>facility;Yun-Ching Chen+University of Southern California>USA>education|Integrated Media Systems Center>USA>facility,"This paper presents algorithms for pitch spelling using the Spiral Array model. Accurate pitch spelling, assigning contextually consistent letter names to pitch numbers (for example, MIDI), is a critical component of music transcription and analysis systems. The local context is found to be more important than the global, but a combination of both achieves the best results.",USA,education,Developed economies,"[27.690094, -19.385038]","[-10.249223, -10.442404]","[17.574959, -18.918915, -9.769442]","[-1.1974739, -10.694569, -4.5443664]","[9.909727, 5.76394]","[6.9453626, 2.464069]","[10.915739, 13.263375, -0.7900857]","[8.919907, 7.189068, 10.940214]"
5,Roger B. Dannenberg;William P. Birmingham;George Tzanetakis;Colin Meek;Ning Hu;Bryan Pardo,The MUSART testbed for query-by-humming evaluation.,2003,https://doi.org/10.5281/zenodo.1415978,Roger B. Dannenberg+Carnegie Mellon University>USA>education;William P. Birmingham+University of Michigan>USA>education;George Tzanetakis+Carnegie Mellon University>USA>education;Colin Meek+Carnegie Mellon University>USA>education;Ning Hu+Carnegie Mellon University>USA>education;Bryan Pardo+University of Michigan>USA>education,"Evaluating music information retrieval systems is acknowledged to be a difficult problem. We have created a database and a software testbed for the systematic evaluation of various query-by-humming (QBH) search systems. As might be expected, different queries and different databases lead to wide variations in observed search precision. “Natural” queries from two sources led to lower performance than that typically reported in the QBH literature. These results point out the importance of careful measurement and objective comparisons to study retrieval algorithms. This study compares search algorithms based on note-interval matching with dynamic programming, fixed-frame melodic contour matching with dynamic time warping, and a hidden Markov model. An examination of scaling trends is encouraging: precision falls off very slowly as the database size increases. This trend is simple to compute and could be useful to predict performance on larger databases.",USA,education,Developed economies,"[-3.2534568, 37.131226]","[10.059885, 12.245863]","[-13.758703, -7.6157475, -23.610302]","[7.7959933, -13.414597, 11.847078]","[14.835353, 6.1219006]","[8.950577, 0.49550143]","[13.202253, 15.35436, -2.954342]","[10.431269, 5.914724, 13.233414]"
0,E. Allamanche;Jürgen Herre;Oliver Hellmuth;T. Kastner;C. Ertel,A multiple feature model for musical similarity retrieval.,2003,https://doi.org/10.5281/zenodo.1416684,"Eric Allamanche+Fraunhofer Institut Integrierte Schaltungen, IIS>DEU>facility;Jürgen Herre+Fraunhofer Institut Integrierte Schaltungen, IIS>DEU>facility;Oliver Hellmuth+Fraunhofer Institut Integrierte Schaltungen, IIS>DEU>facility;Thorsten Kastner+Fraunhofer Institut Integrierte Schaltungen, IIS>DEU>facility;Christian Ertel+Fraunhofer Institut Integrierte Schaltungen, IIS>DEU>facility","Despite the “fuzzy” nature of musical similarity, which varies from one person to another, perceptual low level features combined with appropriate classification schemes have proven to perform satisfactorily for this task. Since a single feature only captures some selective characteristics of an audio signal, this information may, in some cases, not be sufficient to properly identify similarities between songs. This paper presents a system which combines a set of acoustic features for the task of retrieving similar sounding songs. The methodology for optimum feature selection and combination is explained, and the system’s performance is assessed by means of a subjective listening test.",DEU,facility,Developed economies,"[-10.565702, 15.064804]","[15.900833, 7.834119]","[-5.1394935, 9.706636, -6.4582076]","[11.303174, 0.9847681, -1.1408268]","[12.932438, 8.939669]","[9.760868, 2.2897835]","[13.475242, 14.641762, -0.70596653]","[11.477012, 6.6711507, 12.152896]"
7,Tom De Mulder;Jean-Pierre Martens;Micheline Lesaffre;Marc Leman;Bernard De Baets,An auditory model based transriber of vocal queries.,2003,https://doi.org/10.5281/zenodo.1416492,Tom De Mulder+Ghent University>BEL>education;Jean-Pierre Martens+Ghent University>BEL>education;Micheline Lesaffre+Ghent University>BEL>education;Marc Leman+Ghent University>BEL>education;Bernard De Baets+Ghent University>BEL>education;Hans De Meyer+Ghent University>BEL>education,"""In this paper a new auditory model-based transcriber of vocal melodic queries is presented. Our experiments show that the new system can transcribe queries with an accuracy between 76 % (whistling) and 85 % (singing with syllables), and that it outperforms four state-of-the-art systems it was compared with.""",BEL,education,Developed economies,"[-1.8358961, -34.974056]","[0.25939238, -11.916726]","[20.357693, 4.1706724, -13.44849]","[8.132035, -13.189176, -16.22448]","[9.479764, 10.865047]","[7.1519403, 2.0916314]","[10.999842, 14.886223, 0.9620545]","[9.14816, 7.725114, 11.597024]"
8,Simon Dixon;Elias Pampalk;Gerhard Widmer,Classification of dance music by periodicity patterns.,2003,https://doi.org/10.5281/zenodo.1414936,Simon Dixon+Austrian Research Institute for AI>AUT>facility;Elias Pampalk+Austrian Research Institute for AI>AUT>facility;Gerhard Widmer+Austrian Research Institute for AI>AUT>facility|University of Vienna>AUT>education,"This paper addresses the genre classification problem for a specific subset of music, standard and Latin ballroom dance music, using a classification method based only on timing information. We compare two methods of extracting periodicities from audio recordings in order to find the metrical hierarchy and timing patterns by which the style of the music can be recognised: the first method performs onset detection and clustering of inter-onset intervals; the second uses autocorrelation on the amplitude envelopes of band-limited versions of the signal as its method of periodicity detection. The relationships between periodicities are then used to find the metrical hierarchy and to estimate the tempo at the beat and measure levels of the hierarchy. The periodicities are then interpreted as musical note values, and the estimated tempo, meter and the distribution of periodicities are used to predict the style of music using a simple set of rules. The methods are evaluated with a test set of standard and Latin dance music, for which the style and tempo are given on the CD cover, providing a “ground truth” by which the automatic classification can be measured.",AUT,facility,Developed economies,"[-25.040997, -21.400787]","[-21.923143, 0.28745687]","[-7.197272, -22.240263, -2.321797]","[2.965143, 10.003833, -5.4067826]","[11.786387, 5.3249793]","[6.164047, 1.8225207]","[11.561897, 13.894615, -2.2888176]","[8.285975, 6.8871017, 11.756213]"
6,Stephen Davison,The Sheet Music Consortium: A Specialized Open Archives Initiative harvester project.,2003,https://doi.org/10.5281/zenodo.1417731,"Stephen Davison+University of California, Los Angeles>USA>education;Cynthia Requardt+The Johns Hopkins University>USA>education;Kristine Brancolini+Indiana University>USA>education","The Open Archives Initiative (OAI) Sheet Music Project is a consortium of institutions building OAI-compliant data providers, a metadata harvester, and a web-based service provider for digital sheet music collections. The project aims to test the viability of the OAI standard for providing access to sheet music collections on the web, and to build a permanent and increasingly participatory service for the discovery of digital sheet music. The service provider design has been informed by detailed usability testing, and by limitations imposed by the variations in metadata harvested from the different participating collections. Advanced services in addition to basic searching and browsing have been developed, including the ability to save and share subsets across participating collections. Harvesting and searching strategies for overcoming metadata limitations are being developed. The consortium is seeking additional participants with digital sheet music collections, and is exploring the possibility of incorporating scores and audio into the project. Digital sheet music collections were among the earliest substantial music collections to appear on the web. Most significantly, digital sheet music collections have been mounted by the Library of Congress, Johns Hopkins University, and Duke University. There are many other important collections, resulting in a rich distributed research resource for music. Sheet music collections have become the focus of digitization projects for a number of reasons relating to the publication format, its component parts, and the resulting problems in providing access, either through traditional cataloging systems, or by other means. A piece of sheet music generally consists of a number of different components, brought together for a specific publication. These include the music itself, usually consisting of between two and eight pages; a cover page, that often includes graphic artwork and/or a photographic reproduction; and advertisements, either for additional sheet music from the same publisher, or from other vendors. The most common sheet music genre is popular song, and the text of these songs comprises another important element of the publication. It seems likely that providing access to the cover art of sheet music has been the prime motivation for many sheet music projects. The graphic artwork found on published sheet music is often very decorative, and provides information of interest to a wide variety of scholars, including historians of art, cultural historians, sociologists, and so on. Much as sound recordings are today, sheet music of the C19th and early C20th documents taste, attitudes, and societal concerns, across time, and in different geographical locations. Sheet music may also reflect the concerns of specific groups of people (e.g. political publications), or entire nations (e.g. nationalistic publications). Songs that become perennial favorites were often republished with changes in text (e.g. taking out insulting epithets; or perhaps writing completely new words for a favorite tune), new cover graphic arts; or simply changes in the advertising. All of these changes document changing attitudes both musical and non-musical. Traditional cataloging schemes have had a difficult time capturing the disparate elements that make up a single piece of sheet music, let alone the relationships between repeated publications of a single work. In addition sheet music has usually been published as simply music, with little recorded about the cover art or the text. For example, an AACR2 catalog record for a piece of sheet music would typically record the genre (e.g. “Piano music” or “Songs with piano”); include transcriptions of title, attribution, and publication information; and provide access points to composer and lyricist. Information about the cover art and subject of the song—other than that obvious from the title —typically would be absent. These difficulties, both the physical structure and the cataloging problem, along with the recognition that sheet music has unique research potential, have meant that sheet music collections have often found a home among institutional “Special Collections,” rather than in the music library. These problems, along with interest from a wide variety of users, have encouraged the creation of digital sheet.",USA,education,Developed economies,"[1.9421662, 29.12235]","[24.437729, 36.9999]","[-8.387787, 1.3718464, -25.32106]","[-2.6870284, 5.000853, 23.577875]","[12.125372, 7.036656]","[11.206371, 0.15896468]","[12.482692, 12.07464, -1.3419538]","[11.837987, 4.573222, 12.290949]"
10,J. Stephen Downie,Toward the scientific evaluation of music information retrieval systems.,2003,https://doi.org/10.5281/zenodo.1417121,J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"This paper outlines the findings-to-date of a project to assist in the efforts being made to establish a TREC-like evaluation paradigm within the Music Information Retrieval (MIR) research community. The findings and recommendations are based upon expert opinion garnered from members of the Information Retrieval (IR), Music Digital Library (MDL) and MIR communities with regard to the construction and implementation of scientifically valid evaluation frameworks. Proposed recommendations include the creation of data-rich query records that are both grounded in real-world requirements and neutral with respect to retrieval technique(s) being examined; adoption, and subsequent validation, of a “reasonable person” approach to “relevance” assessment; and, the development of a secure, yet accessible, research environment that allows researchers to remotely access the large-scale testbed collection.",USA,education,Developed economies,"[-20.964012, 20.742836]","[27.425945, 32.857994]","[-14.292676, 4.788679, -10.158611]","[3.474216, 4.0020747, 16.446234]","[14.07941, 7.9048305]","[11.79461, 0.6085227]","[14.220903, 14.559911, -1.903033]","[12.236008, 4.3813553, 12.323196]"
11,Jana Eggink;Guy J. Brown,Application of missing feature theory to the recognition of musical instruments in polyphonic audio.,2003,https://doi.org/10.5281/zenodo.1416262,Jana Eggink+University of Sheffield>GBR>education;Guy J. Brown+University of Sheffield>GBR>education,"A system for musical instrument recognition based on a Gaussian Mixture Model (GMM) classifier is introduced. To enable instrument recognition when more than one sound is present at the same time, ideas from missing feature theory are incorporated. Specifically, frequency regions that are dominated by energy from an interfering tone are marked as unreliable and excluded from the classification process. The approach has been evaluated on clean and noisy monophonic recordings, and on combinations of two instrument sounds. These included random chords made from two isolated notes and combinations of two realistic phrases taken from commercially available compact discs. Classification results were generally good, not only when the decision between reliable and unreliable features was based on the knowledge of the clean signal, but also when it was solely based on the harmonic overtone series of the interfering sound.",GBR,education,Developed economies,"[6.4525666, -21.551615]","[-11.442021, 1.1635877]","[17.65653, -6.7188373, -6.0970078]","[11.552798, -0.48398188, -12.337724]","[8.8405285, 7.313608]","[8.414438, 3.723775]","[11.1282, 12.709318, 0.5946395]","[10.30412, 7.910256, 10.3204]"
12,Olivier Gillet;Gaël Richard,Automatic labeling of tabla signals.,2003,https://doi.org/10.5281/zenodo.1418281,Olivier K. Gillet+GET-ENST (TELECOM Paris)>FRA>education|GET-ENST (TELECOM Paris)>FRA>education;Gaël Richard+GET-ENST (TELECOM Paris)>FRA>education|GET-ENST (TELECOM Paris)>FRA>education,"Most of the recent developments in the field of music indexing and music information retrieval are focused on western music. In this paper, we present an automatic music transcription system dedicated to Tabla - a North Indian percussion instrument. Our approach is based on three main steps: firstly, the audio signal is segmented in adjacent segments where each segment represents a single stroke. Secondly, rhythmic information such as relative durations are calculated using beat detection techniques. Finally, the transcription (recognition of the strokes) is performed by means of a statistical model based on Hidden Markov Model (HMM). The structure of this model is designed in order to represent the time dependencies between successives strokes and to take into account the specificities of the tabla score notation (transcription symbols may be context dependent). Realtime transcription of Tabla soli (or performances) with an error rate of 6.5% is made possible with this transcriber. The transcription system, along with some additional features such as sound synthesis or phrase correction, are integrated in a user-friendly environment called Tablascope.",FRA,education,Developed economies,"[34.484432, -47.211]","[-14.841965, -2.037311]","[28.337584, -17.355913, 3.781253]","[4.327051, 3.5885944, -15.025966]","[7.7485785, 7.8019996]","[6.662591, 2.906077]","[11.048154, 11.453923, 1.1624168]","[8.888046, 7.3245316, 10.6786175]"
13,Masataka Goto,Music scene description project: Toward audio-based real-time music understanding.,2003,https://doi.org/10.5281/zenodo.1415684,Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper reports a research project intended to build a real-time music-understanding system producing intuitively meaningful descriptions of real-world musical audio signals, such as the melody lines and chorus sections. This paper also introduces our efforts to add correct descriptions (metadata) to the pieces in a music database.",JPN,facility,Developed economies,"[-14.58229, 9.954914]","[-4.1134105, 17.559559]","[-8.048465, 14.645219, -25.269588]","[-4.4272037, -0.49388194, 7.7494106]","[13.083136, 7.986116]","[10.017938, 1.276528]","[13.624594, 13.872354, -1.5663604]","[10.417518, 5.5996084, 11.396244]"
14,Masataka Goto;Hiroki Hashiguchi;Takuichi Nishimura;Ryuichi Oka,RWC Music Database: Music genre database and musical instrument sound database.,2003,https://doi.org/10.5281/zenodo.1415536,Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroki Hashiguchi+Mejiro University>JPN>education;Takuichi Nishimura+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Ryuichi Oka+University of Aizu>JPN>education,"This paper describes the design policy and specifications of the RWC Music Database, a copyright-cleared music database (DB) compiled specifically for research purposes. Shared DBs are common in other research fields and have made significant contributions to progress in those fields. The field of music information processing, however, has lacked a common DB of musical pieces or a large-scale DB of musical instrument sounds. We therefore recently constructed the RWC Music Database comprising four original component DBs: Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). In this paper we report the construction of two additional component DBs: Music Genre Database (100 pieces) and Musical Instrument Sound Database (50 instruments). It is our hope that our DB will make a significant contribution to future advances in the field of music information processing.",JPN,facility,Developed economies,"[-19.899296, 11.876437]","[21.835371, 32.441414]","[-16.268337, -4.457198, -8.923304]","[1.5725178, -1.1287161, 22.76723]","[13.211734, 7.5343685]","[10.970603, 0.19932012]","[14.200166, 13.829526, -1.4558662]","[11.70588, 4.751897, 12.329013]"
15,S. Harford,"Automatic segmentation, learning and retrieval of melodies using a self-organizing neural network.",2003,https://doi.org/10.5281/zenodo.1416182,Steven Harford+Dublin City University>IRL>education,"We introduce a neural network, known as SONNET-MAP, capable of automatic segmentation, learning and retrieval of melodies. SONNET-MAP is a synthesis of the SONNET (Self-Organizing Neural Network) architecture (Nigrin, 1993) and an associative map derived from ARTMAP (Carpenter, Grossberg, and Reynolds, 1991). SONNET-MAP automatically segments a melody based on pitch and rhythmic grouping cues. Separate SONNET modules represent the pitch and rhythm dimensions of each segmented phrase independently, with two associative maps fusing these representations at the phrase level. Further SONNET modules aggregate these phrases forming a hierarchical memory structure that encompasses the entire melody. In addition, melodic queries may be used to retrieve any encoded melody. As far as we are aware, SONNET-MAP is the first self-organizing neural network architecture capable of automatically segmenting and retrieving melodies based on both pitch and rhythm.",IRL,education,Developed economies,"[7.2570343, -7.8571715]","[-1.2952986, -32.212467]","[9.435482, 5.958896, 0.69444346]","[-0.20996572, 4.5856185, -26.714657]","[10.504412, 9.785872]","[9.249965, 5.75185]","[11.3939295, 15.079832, -0.821046]","[9.842639, 5.5790424, 9.230647]"
16,Sung-Phil Heo;Motoyuki Suzuki;Akinori Ito;Shozo Makino,Three-dimensional continuous DP algorithm for multiple pitch candidates in a music information retrieval system.,2003,https://doi.org/10.5281/zenodo.1416862,Sung-Phil Heo+Tohoku University>JPN>education;Motoyuki Suzuki+Tohoku University>JPN>education;Akinori Ito+Tohoku University>JPN>education;Shozo Makino+Tohoku University>JPN>education,"This paper treats theoretical and practical issues that implement a music information retrieval system based on query by humming. In order to extract accuracy features from the user’s humming, we propose a new retrieval method based on multiple pitch candidates. Extracted multiple pitches have shown to be very important parameters in determining melodic similarity, but it is also clear that the confidence measures feature which are obtained from the power are important as well. Furthermore, we propose extending the traditional DP algorithm to three dimensions so that multiple pitch candidates can be treated. Simultaneously, at the melody representation technique, we propose the DP paths are changed dynamically to be able to take relative values so that they can respond to insert or omit notes.",JPN,education,Developed economies,"[-5.2722807, 23.425753]","[11.314251, 10.750766]","[-0.73698133, 2.4488738, -17.170546]","[4.084241, -12.494705, 12.42668]","[12.991541, 8.038639]","[8.80523, 0.5801669]","[12.892148, 14.448367, -1.7387125]","[10.429869, 5.924097, 13.008225]"
17,Olivier Lartillot,Discovering musical pattern through perceptual heuristics.,2003,https://doi.org/10.5281/zenodo.1417285,Olivier Lartillot+IRCAM - Centre Pompidou>FRA>facility,"This paper defends the view that the intricate difficulties challenging the emerging domain of Musical Pattern Discovery, which is dedicated to the automation of motivic analysis, will be overcome only through a thorough taking into account of the specificity of music as a perceptive object. Actual musical patterns, although constantly transformed, are nevertheless perceived by the listener as musical identities. Such dynamical properties of human perception, not reducible to geometrical models, will only be explained with the notions of contexts and expectations. This paper sketches the general principles of a new approach that attempts to build such a general perceptual system. On a sub-cognitive level, patterns are discovered through the detection, by an associative memory, of local similarities. On a cognitive level, patterns are managed by a general logical framework that avoids irrelevant inferences and combinatorial explosion. In this way, actual musical patterns that convey musical significance are discovered. This approach, offering promising results, is a first step toward a complete system of automated music analysis and an explicit modeling of basic mechanisms for music understanding.",FRA,facility,Developed economies,"[-12.763899, 0.8845479]","[0.08021819, 16.540579]","[-4.613671, 4.3241305, 10.687871]","[-4.5281024, -7.482667, 7.070276]","[11.381316, 8.324607]","[8.851059, 1.7019871]","[13.02487, 13.423184, -0.52715397]","[10.505265, 6.699573, 12.31896]"
9,Shyamala Doraisamy;Stefan M. Rüger,Position Indexing of Adjacent and Concurrent N-Grams for Polyphonic Music Retrieval.,2003,https://doi.org/10.5281/zenodo.1417919,Shyamala Doraisamy+Imperial College London>GBR>education|Unknown>Unknown>Unknown;Stefan Rüger+Imperial College London>GBR>education|Unknown>Unknown>Unknown,"In this paper we examine the retrieval performance of adjacent and concurrent n-grams generated from polyphonic music data. We deploy a method to index polyphonic music using a word position indexer with the n-gram approach. Using all possible combinations of monophonic sequences from polyphonic music data, “overlaying” word locations within a document are obtained, such as needed with polyphony (i.e. where more than one word can assume the same word position). The feasibility in utilising the position information of polyphonic ‘musical words’ is investigated using various proximity-based and structured query operators available with text retrieval system. Our experiments show that nested phrase operators improve the retrieval performance and we present the results of our comparative study on a collection of 5456 polyphonic pieces encoded in the MIDI format.",GBR,education,Developed economies,"[-11.251311, 19.211435]","[13.194604, 17.190763]","[-0.9787773, 3.5028963, -11.7203865]","[6.6901436, -7.9274125, 9.255931]","[13.237086, 8.061634]","[9.261143, 0.7101304]","[13.145552, 14.517102, -1.8683151]","[10.821494, 6.118471, 13.173488]"
72,Christopher Raphael,Demonstration of 'Music Plus One'--- a System for Orchestral Musical Accompanimen.,2004,https://doi.org/10.5281/zenodo.1417899,Christopher Raphael+Indiana University>USA>education,"""""",USA,education,Developed economies,"[-12.579239, 5.2139306]","[51.479694, 43.439453]","[-10.357617, -13.848883, -8.465774]","[5.3623414, 5.6872306, 31.514923]","[12.352297, 7.5250015]","[-9.088388, 9.978777]","[13.326749, 13.231397, -1.2871732]","[-11.714436, 1.7670275, 1.0355657]"
73,Josh Reiss;Mark B. Sandler,Audio Issues In MIR Evaluation.,2004,https://doi.org/10.5281/zenodo.1415840,"Josh Reiss+Queen Mary, University of London>GBR>education;Mark Sandler+Queen Mary, University of London>GBR>education","Several projects are underway to create music testbeds to suit the needs of the music analysis and music information retrieval (MIR) communities. There are also plans to unify testbeds into a distributed grid. Thus the issue of audio file formats has come to the forefront. The creators of a music library or MIR testbed are confronted with many questions pertaining to file formats, their quality, metadata, and copyright issues. We discuss the various formats, their advantages and disadvantages, and give a set of guidelines and recommendations. This document is a positional paper. It is intended to foster discussion and not as a definitive statement. Nevertheless, it is hoped that the proposals put forth here may serve as a guideline to use in construction of an MIR evaluation testbed.",GBR,education,Developed economies,"[-12.035057, 57.382496]","[16.035883, 32.60743]","[-38.908054, 2.2659435, -1.9357285]","[0.5228128, 3.4084377, 14.933471]","[13.435492, 4.947136]","[11.506443, 0.6589741]","[14.817689, 11.348258, -1.405352]","[11.832023, 4.8184295, 11.994337]"
74,Vegard Sandvold;Fabien Gouyon;Perfecto Herrera,Drum sound classification in polyphonic audio recordings using localized sound models.,2004,https://doi.org/10.5281/zenodo.1415808,Vegard Sandvold+University of Oslo>NOR>education|Universitat Pompeu Fabra>ESP>education|Universitat Pompeu Fabra>ESP>education;Fabien Gouyon+Universitat Pompeu Fabra>ESP>education|Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education|Universitat Pompeu Fabra>ESP>education,"This paper deals with automatic percussion classification in polyphonic audio recordings, focusing on kick, snare and cymbal sounds. We present a feature-based sound modeling approach that combines general, prior knowledge about the sound characteristics of percussion instrument families (general models) with on-the-fly acquired knowledge of recording-specific sounds (localized models). This way, high classification accuracy can be obtained with remarkably simple sound models. The accuracy is on average around 20% higher than with general models alone.",NOR,education,Developed economies,"[23.428663, -47.468513]","[-14.60432, 1.8592216]","[16.581709, -21.387573, 0.43119276]","[8.580397, 10.472146, -13.155333]","[7.8905787, 7.0594673]","[8.531812, 4.013639]","[10.276609, 11.794218, 0.8825559]","[9.783491, 7.3215823, 10.113052]"
75,Ryan Scherle;Donald Byrd,The Anatomy of a Bibliographic Search System for Music.,2004,https://doi.org/10.5281/zenodo.1417789,Ryan Scherle+Indiana University>USA>education;Donald Byrd+Indiana University>USA>education,"Traditional library catalog systems have been effective in providing access to collections of books, films, and other material. However, they have many limitations when it comes to finding musical information, which has significantly different, and in many ways more complex, structure. The Variations2 search system is an alternative designed specifically to aid users in searching for music. It leverages a rich set of bibliographic data records, expressing relationships between creators of music and their creations. These records enable musicians to search for music using familiar terms and relationships, rather than trying to decipher the methods libraries typically use to organize musical items. This paper describes the design and implementation of the system that makes these searches possible.",USA,education,Developed economies,"[-25.990355, 23.93907]","[22.08312, 29.356152]","[-18.716398, 5.9403777, -8.236965]","[4.079519, -3.3039942, 19.521341]","[14.410846, 8.057612]","[10.821634, 0.58632404]","[14.535557, 14.780268, -2.0337486]","[11.733435, 4.9646525, 12.649466]"
76,Shai Shalev-Shwartz;Joseph Keshet;Yoram Singer,Learning to Align Polyphonic Music.,2004,https://doi.org/10.5281/zenodo.1416546,Shai Shalev-Shwartz+The Hebrew University>ISR>education;Joseph Keshet+The Hebrew University>ISR>education;Yoram Singer+The Hebrew University>ISR>education,"We describe an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acoustic counterpart. Our method employs a supervised learning approach by using a training set of aligned symbolic and acoustic representations. The alignment function we devise is based on mapping the input acoustic-symbolic representation along with the target alignment into an abstract vector-space. Building on techniques used for learning support vector machines (SVM), our alignment function distills to a classifier in the abstract vector-space which separates correct alignments from incorrect ones. We describe a simple iterative algorithm for learning the alignment function and discuss its formal properties. We use our method for aligning MIDI and MP3 representations of polyphonic recordings of piano music. We also compare our discriminative approach to a generative method based on a generalization of hidden Markov models. In all of our experiments, the discriminative method outperforms the HMM-based method.",ISR,education,Developing economies,"[17.369253, -12.227363]","[-14.182297, -10.7137985]","[1.7379876, -13.121473, -4.9417896]","[-0.962438, -16.836163, -7.6514683]","[10.977513, 6.781265]","[6.61683, 1.1648206]","[12.084243, 12.805396, -1.2950928]","[8.6336, 6.3199983, 10.671492]"
85,Richard Terrat,Pregroup Grammars for Chords.,2004,https://doi.org/10.5281/zenodo.1416702,Richard G. Terrat+LIRMM/CNRS>FRA>facility|IRCAM>FRA>facility,"Pregroups had been conceived as an algebraic tool to recognize grammatically well-formed sentences in natural languages [3]. Here we wish to use pregroups to recognize well-formed chords of pitches, for a given definition of those chords. We show how a judicious choice of basic and simple types allows a context-free grammatical description. Then we use the robustness property to extend the set of well-formed chords in a simple way. Finally we argue in favor of an utilization of pregroups grammars for the recognition and classification of chord sequences.",FRA,facility,Developed economies,"[52.969715, 1.575178]","[-21.094034, 22.656288]","[20.409534, -17.553389, 21.978662]","[-19.441603, 2.283702, 6.556752]","[7.460147, 8.562456]","[7.0125484, 3.2906055]","[12.163086, 10.810036, 1.4958583]","[9.687237, 8.133633, 12.399586]"
77,Prarthana Shrestha;Ton Kalker,Audio Fingerprinting In Peer-to-peer Networks.,2004,https://doi.org/10.5281/zenodo.1417457,Prarthana Shrestha+Eindhoven University of Technology>NLD>education;Ton Kalker+Eindhoven University of Technology>NLD>education,"Despite the immense potential of Peer-to-Peer (P2P) networks in facilitating collaborative applications, they have become largely known as a free haven for pirated music swapping. In this paper, we present an approach wherein the collective computational power of the P2P networks is exploited to combat the problem of unauthorized music file sharing. We propose a distributed system based on audio fingerprinting, that makes it possible to recognize the music content present in the network. When the contents are identified, the network can take special measures against the use or sharing of unauthorized music. This proposed system is self-adapting, and robust. The foregoing properties make the system particularly suitable for use in dynamic and heterogeneous environment of P2P networks. In order to investigate the behavior of the proposed system, a system-level model has been created using the Parallel Object Oriented Specification Language (POOSL). This model was used to investigate an optimal system configuration that maximizes the identification of the content.",NLD,education,Developed economies,"[-17.248156, -26.322254]","[49.882095, 19.764088]","[4.252947, -13.793709, -25.395487]","[23.303175, -2.1758003, 13.31278]","[9.093662, 4.4315434]","[11.880257, 1.8704911]","[10.579572, 11.506519, -1.9591355]","[12.863655, 5.374913, 12.51978]"
83,Sara Taheri-Panah;Andrew MacFarlane 0001,Music Information Retrieval systems: why do individuals use them and what are their needs?.,2004,https://doi.org/10.5281/zenodo.1416334,S. Taheri-Panah+City University>GBR>education;A. MacFarlane+City University>GBR>education,"To date there has been very little research conducted on the behaviour of music information retrieval (MIR) users, in spite of the immense popularity of free music retrieval systems available on the Internet. In this study we examine the issue of music seeking behaviour through the examination of users life style effect of three different age groups using questionnaires. It was found that lifestyles had a significant impact on users need for music and hence their music seeking behaviour. The importance of social networks in music information seeking was reinforced in this study. An experiment was conducted with three different types of search on the Kazaa MIR system and the participants interviewed in order to collect data. Users found the Kazaa system intuitive and easy to use. Searchers used both song titles and lyrics for finding relevant music items. The insights provided by this study can be of assistance in the development of user focused Internet MIR systems.",GBR,education,Developed economies,"[-22.293726, 23.209066]","[38.718708, 31.258759]","[-15.419502, 7.895825, -11.197038]","[7.5414796, 10.4257145, 18.98215]","[14.522578, 7.9516873]","[12.623157, 0.9160387]","[14.542108, 14.690845, -2.0762067]","[13.093483, 4.3010006, 12.164952]"
82,Iman S. H. Suyoto;Alexandra L. Uitdenbogerd,Exploring Microtonal Matching.,2004,https://doi.org/10.5281/zenodo.1416656,Iman S. H. Suyoto+RMIT University>AUS>education;Alexandra L. Uitdenbogerd+RMIT University>AUS>education,"Most research into music information retrieval thus far has only examined music from the western tradition. However, music of other origins often conforms to different tuning systems. Therefore there are problems both in representing this music as well as finding matches to queries from these diverse tuning systems. We discuss the issues associated with microtonal music retrieval and present some preliminary results from an experiment in applying scoring matrices to microtonal matching.",AUS,education,Developed economies,"[19.835869, 21.856485]","[16.251038, 15.653108]","[-1.9880586, -12.157133, 2.894856]","[8.311655, -5.505223, 11.0571785]","[11.124857, 7.1118646]","[9.761903, 0.9244547]","[12.318888, 13.155164, -1.1212147]","[11.128243, 5.860095, 12.844823]"
81,Craig Stuart Sapp;Yi-Wen Liu;Eleanor Selfridge-Field,Search Effectiveness Measures for Symbolic Music Queries in Very Large Databases.,2004,https://doi.org/10.5281/zenodo.1417913,Craig Stuart Sapp+Stanford University>USA>education;Yi-Wen Liu+Stanford University>USA>education;Eleanor Selfridge-Field+Stanford University>USA>education,"In the interest of establishing robust benchmarks for search efficiency, we conducted a series of tests on symbolic databases of musical incipits and themes taken from several diverse repertories. The results we report differ from existing studies in four respects: (1) the data quantity is much larger (c. 100,000 entries); (2) the levels of melodic and rhythmic precision are more refined; (3) anchored and unanchored searches were differentiated; and (4) results from joint pitch-and-rhythm searches were compared with those for pitch-only searches. The search results were evaluated using a theoretical approach which seeks to rank the number of symbols required to achieve “sufficient uniqueness”. How far into a melody must a search go in order to find an item which is unmatched by any other of the available items? How much does the answer depend on the specificity of the query? How much does anchoring the query matter? How much does the result depend on the nature of the repertory? We offer experimental results for these questions.",USA,education,Developed economies,"[-3.9272513, 32.499947]","[13.583295, 13.703638]","[-11.310577, -2.2218606, -19.67671]","[8.14302, -10.148661, 15.262454]","[14.380791, 7.087658]","[9.150006, 0.8277655]","[13.925586, 14.80765, -2.4000545]","[10.708455, 6.174309, 12.93961]"
80,Josh Stoddard;Christopher Raphael;Paul E. Utgoff,Well-Tempered Spelling: A Key Invariant Pitch Spelling Algorithm.,2004,https://doi.org/10.5281/zenodo.1414762,Joshua Stoddard+University of Massachusetts Amherst>USA>education;Christopher Raphael+University of Massachusetts Amherst>USA>education;Paul E. Utgoff+University of Massachusetts Amherst>USA>education,"In this paper is described a data-driven algorithm for the functionally correct spelling of MIDI pitch values in terms of Western musical notation. Input is in the form of MIDI files containing accurate pitch and rhythmic information with corresponding ground-truth spelling information for training and evaluation. The algorithm recovers harmonic information from the MIDI data and spells pitches according to their relation to the local tonic. The algorithm achieved 94.98% accuracy on the pitches that required accidentals in the local key and 99.686% overall. Voice-leading resolution was found to be the best feature of those used to infer the correct spelling. Also, this paper outlines great potential for improvement under this model.",USA,education,Developed economies,"[26.964066, -19.185593]","[-10.412219, -10.348992]","[17.962711, -17.201324, -10.278065]","[-1.660601, -11.233031, -4.2285566]","[9.858836, 5.843154]","[7.012807, 2.2929838]","[10.925599, 13.270421, -0.7552225]","[9.003435, 7.0505013, 11.0412]"
79,J. Stephen Downie;Joe Futrelle;David K. Tcheng,"The International Music Information Retrieval Systems Evaluation Laboratory: Governance, Access and Security.",2004,https://doi.org/10.5281/zenodo.1415120,J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Joe Futrelle+University of Illinois at Urbana-Champaign>USA>education;David Tcheng+University of Illinois at Urbana-Champaign>USA>education,"The IMIRSEL (International Music Information Retrieval Systems Evaluation Laboratory) project provides an unprecedented platform for evaluating Music Information Retrieval (MIR) and Music Digital Library (MDL) techniques, by bringing together large corpora and significant computational resources with the necessary rights management and technical infrastructure to support a variety of MIR/MDL research areas. The standardized research collection being deployed represents a large and diverse corpus of musical examples, which we are hosting in our secure environment for use in evaluating MIR/MDL algorithms. Grid services and NCSA's D2K machine learning environment provide a powerful, high-performance, and secure framework for designing, optimising, and executing complex MIR/MDL evaluation applications. IMIRSEL provides a community resource for researchers who would otherwise not be able to afford the content rights and computational resources to carry out large-scale MIR/MDL evaluations.",USA,education,Developed economies,"[-23.906624, 25.104969]","[16.114212, 31.16421]","[-17.317844, 5.801214, -11.865596]","[-1.3817269, 2.2929368, 14.785699]","[14.42598, 7.9506907]","[11.153708, 0.6589357]","[14.580983, 14.604752, -2.1251802]","[11.624933, 5.15634, 11.631109]"
78,Jane Singer,Creating a nested melodic representation: competition and cooperation among bottom-up and top-down Gestalt principles.,2004,https://doi.org/10.5281/zenodo.1417965,Jane Singer+The Hebrew University of Jerusalem>ISR>education,"A set of principles (based on Gestalt theory) governing how we group notes into meaningful groups has been widely accepted in the literature. Based on these principles, many divergent theories of melodic segmentation and representation have been proposed. However, these theories have not succeeded in achieving a comprehensive and verifiable representation of melody. This is largely due to the fact that multiple competing segmenting factors produce, for any single melody, a large number of possible segmentations and therefore representations. Here a model is proposed, which incorporates widely accepted principles of segmentation. These rules govern three types of factors: (1) changes in proximity (for producing disjunctive segmentation), (2) changes in overall contour and intervallic texture and (3) patterns and periodicity that create parallelism among segments. Because of the nature of the segmentation rules, these same rules establish the attributes of the groups they produce. Based on original research in Singer 2004, principles for establishing preferences among competing rules are formulated in order to create a few preferred representations for approximately 1,000 monophonic folksongs.",ISR,education,Developing economies,"[5.7843018, 19.257418]","[3.0237517, -7.321486]","[5.5047007, 3.2447875, 1.0518726]","[-4.584724, -7.999585, 1.6335027]","[12.180939, 9.818095]","[8.076601, 1.6407497]","[12.643174, 15.40761, -0.91849416]","[9.8334875, 7.0686584, 12.431578]"
84,Haruto Takeda;Takuya Nishimoto;Shigeki Sagayama,Rhythm and Tempo Recognition of Music Performance from a Probabilistic Approach.,2004,https://doi.org/10.5281/zenodo.1418209,Haruto Takeda+The University of Tokyo>JPN>education;Takuya Nishimoto+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"This paper concerns both rhythm recognition and tempo analysis of expressive music performance based on a probabilistic approach. In rhythm recognition, the modern continuous speech recognition technique is applied to find the most likely intended note sequence from the given sequence of fluctuating note durations in the performance. Combining stochastic models of note durations deviating from the nominal lengths and a probabilistic grammar representing possible sequences of notes, the problem is formulated as a maximum a posteriori estimation that can be implemented using efficient search based on the Viterbi algorithm. With this, significant improvements compared with conventional “quantization” techniques were found. Tempo analysis is performed by fitting the observed tempo with parametric tempo curves in order to extract tempo dynamics and characteristics of performance to use. Tempo-change timings and parameter values in tempo curve models are estimated through the segmental k-means algorithm. Experimental results of rhythm recognition and tempo analysis applied to classical and popular music performances are also demonstrated.",JPN,education,Developed economies,"[39.675896, -26.13801]","[-17.343138, -4.1486216]","[-2.0853257, -27.96689, -1.6308101]","[2.9451916, -1.3078325, -12.31416]","[11.52693, 4.5515785]","[6.1271405, 2.228292]","[11.029364, 13.411728, -2.725074]","[8.126223, 7.0609202, 10.986026]"
86,Adam R. Tindale;Ajay Kapur;George Tzanetakis;Ichiro Fujinaga,Retrieval of percussion gestures using timbre classification techniques.,2004,https://doi.org/10.5281/zenodo.1416612,Adam Tindale+McGill University>CAN>education;Ajay Kapur+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Musicians are able to recognise the subtle differences in timbre produced by different playing techniques on an instrument, yet there has been little research into achieving this with a computer. This paper will demonstrate an automatic system that can successfully recognise different timbres produced by different performance techniques and classify them using signal processing and classification tools. Success rates over 90% are achieved when classifying snare drum timbres produced by different playing techniques.",CAN,education,Developed economies,"[22.583857, -46.825356]","[-14.477141, 2.4558523]","[17.630386, -20.453247, -3.0579295]","[7.982149, 11.135425, -14.68123]","[7.9993763, 7.0938654]","[8.609467, 3.9521158]","[10.405735, 11.92092, 0.81268764]","[9.826018, 7.3598294, 10.167814]"
49,Tin Lay Nwe;Ye Wang,Automatic Detection Of Vocal Segments In Popular Songs.,2004,https://doi.org/10.5281/zenodo.1417846,Tin Lay Nwe+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"This paper presents a technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. The proposed technique uses acoustic features which are suitable to distinguish vocal and non-vocal signals. We employ the Hidden Markov Model (HMM) classifier for vocal and non-vocal classification. In contrast to conventional HMM training methods which employ one model for each class, we create an HMM model space (multi-model HMMs) for segmentation with improved accuracy. In addition, we employ an automatic bootstrapping process which adapts the test song’s own models for better classification accuracy. Experimental evaluations conducted on a database of 20 popular music songs show the validity of the proposed approach.",SGP,education,Developing economies,"[-9.245759, -33.30784]","[8.410963, -13.587238]","[16.507845, 13.882095, -13.503133]","[9.522218, 1.0409297, -16.217731]","[10.206048, 11.13818]","[8.358874, 3.4394832]","[11.421221, 15.2163725, 0.66354865]","[10.5872555, 7.73234, 10.339113]"
88,Godfried T. Toussaint,A Comparison of Rhythmic Similarity Measures.,2004,https://doi.org/10.5281/zenodo.1416812,Godfried T. Toussaint+McGill University>CAN>education,"Traditionally, rhythmic similarity measures are compared according to how well rhythms may be recognized with them, how efficiently they can be retrieved from a database, or how well they model human perception and cognition. In contrast, here similarity measures are compared on the basis of how much insight they provide about the structural inter-relationships that exist within families of rhythms, when phylogenetic trees and graphs are computed from the distance matrices determined by these similarity measures. Phylogenetic analyses yield insight into the evolution of rhythms and may uncover interesting ancestral rhythms.",CAN,education,Developed economies,"[44.703503, 7.834547]","[-22.704103, 4.0433564]","[-9.03497, -23.286682, 3.7415528]","[-0.06871164, 17.283245, -3.7765331]","[12.01047, 5.4240923]","[6.1483955, 1.5345625]","[11.401503, 14.207792, -2.121878]","[8.16243, 6.676785, 11.982894]"
24,Slim Essid;Gaël Richard;Bertrand David,Musical instrument recognition based on class pairwise feature selection.,2004,https://doi.org/10.5281/zenodo.1418253,Slim ESSID+GET-ENST (Télécom Paris)>FRA>education;Gaël RICHARD+GET-ENST (Télécom Paris)>FRA>education;Bertrand DAVID+GET-ENST (Télécom Paris)>FRA>education,"In this work, musical instrument recognition is considered on solo music from real world performance. A large sound database is used that consists of musical phrases excerpted from commercial recordings with different instrument instances, different players, and varying recording conditions. The proposed recognition scheme exploits class pairwise feature selection based on inertia ratio maximization. Moreover, new signal processing features based on octave band energy measures are introduced that prove to be useful. Classification is performed using Gaussian Mixture Models in a one vs one fashion in association with a data rescaling procedure as pre-processing. Experimental results show that substantial improvement in recognition success is thus achieved.",FRA,education,Developed economies,"[8.537098, -22.140768]","[-11.105919, 1.4231659]","[16.397736, -7.9011736, -3.5954702]","[10.551658, -0.62776595, -12.221782]","[8.792301, 7.31133]","[8.578779, 3.6236737]","[11.170194, 12.548393, 0.5664157]","[10.323482, 7.793419, 10.326362]"
25,Klaus Frieler,Beat and meter extraction using gaussified onsets.,2004,https://doi.org/10.5281/zenodo.1417851,Klaus Frieler+University of Hamburg>DEU>education,"Rhythm, beat and meter are key concepts of music in general. Many efforts had been made in the last years to automatically extract beat and meter from a piece of music given either in audio or symbolical representation (see e.g. [11] for an overview). In this paper we propose a new method for extracting beat, meter and phase information from a list of unquantized onset times. The procedure relies on a novel method called ’Gaussiﬁcation’ and adopts correlation techniques combined with findings from music psychology for parameter settings.",DEU,education,Developed economies,"[34.132336, -29.87738]","[-24.34652, -2.3131847]","[5.894284, -25.619356, -4.729228]","[-1.8508584, 10.160038, -7.5656323]","[10.69608, 4.4896526]","[5.625272, 1.9441414]","[10.422591, 13.152803, -2.1977942]","[7.776921, 7.095696, 11.362985]"
26,Emilia Gómez;Perfecto Herrera,Estimating The Tonality Of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies.,2004,https://doi.org/10.5281/zenodo.1418007,Emilia Gómez+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"In this paper we evaluate two methods for key estimation from polyphonic audio recordings. Our goal is to compare between a strategy using a cognition-inspired model and several machine learning techniques to find a model for tonality (mode and key note) determination of polyphonic music from audio files. Both approaches have as an input a vector of values related to the intensity of each of the pitch classes of a chromatic scale. In this study, both methods are explained and evaluated in a large database of audio recordings of classical pieces.",ESP,education,Developed economies,"[3.9070604, -19.41198]","[-7.297438, -2.5808315]","[15.659543, 6.8120065, 11.242566]","[5.560742, -5.5168786, -4.342266]","[9.320127, 7.21806]","[7.3003817, 2.7711802]","[11.516388, 12.727011, -0.003468535]","[10.206096, 8.21199, 11.752023]"
27,Masataka Goto;Katunobu Itou;Koji Kitayama;Tetsunori Kobayashi,Speech-Recognition Interfaces for Music Information Retrieval: 'Speech Completion' and 'Speech Spotter'.,2004,https://doi.org/10.5281/zenodo.1417339,Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility|Waseda University>JPN>education;Katunobu Itou+Nagoya University>JPN>education;Koji Kitayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility|Waseda University>JPN>education;Tetsunori Kobayashi+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility|Waseda University>JPN>education,"This paper describes music information retrieval (MIR) systems featuring automatic speech recognition. Although various interfaces for MIR have been proposed, speech-recognition interfaces suitable for retrieving musical pieces have not been studied. We propose two different speech-recognition interfaces for MIR, speech completion and speech spotter, and describe two MIR-based hands-free jukebox systems that enable a user to retrieve and play back a musical piece by saying its title or the artist’s name. The first is a music-retrieval system with the speech-completion interface that is suitable for music stores and car-driving situations. When a user can remember only part of the name of a musical piece or an artist and utters only a remembered fragment, the system helps the user recall and enter the name by completing the fragment. The second is a background-music playback system with the speech-spotter interface that can enrich human-human conversation. When a user is talking to another person, the system allows the user to enter voice commands for music-playback control by spotting a special voice-command utterance in face-to-face or telephone conversations. Our experimental results from use of these systems have demonstrated the effectiveness of the speech-completion and speech-spotter interfaces.",JPN,facility,Developed economies,"[-17.724825, 17.816168]","[13.408733, 24.730337]","[-8.596344, 1.0748314, -16.793224]","[0.5307367, -14.658882, 16.475872]","[13.653935, 8.107544]","[10.358193, 1.0340981]","[13.582696, 14.542866, -1.9690319]","[10.98798, 5.408324, 12.270118]"
28,Fabien Gouyon;Simon Dixon,Dance music classification: A tempo-based approach.,2004,https://doi.org/10.5281/zenodo.1416636,Fabien Gouyon+Universitat Pompeu Fabra>ESP>education|Austrian Research Institute for AI>AUT>facility;Simon Dixon+Austrian Research Institute for AI>AUT>facility,"Recent research has studied the relevance of various features for automatic genre classification, showing the particular importance of tempo in dance music classification. We complement this work by considering a domain-specific learning methodology, where the computed tempo is used to select an expert classifier which has been specialized on its own tempo range. This enables the all-class learning task to be reduced to a set of two- and three-class learning tasks. Current results are around 70% classification accuracy (8 ballroom dance music classes, 698 instances, baseline 15.9%).",ESP,education,Developed economies,"[-25.056873, -21.423811]","[17.052538, -8.235905]","[-6.9559298, -22.800661, -3.1067617]","[5.282232, 10.325677, -5.4667916]","[11.745697, 5.225799]","[9.46503, 3.2616904]","[11.506704, 13.823555, -2.3640628]","[11.238754, 7.2849298, 10.979224]"
29,Maarten Grachten;Josep Lluís Arcos;Ramón López de Mántaras,Melodic Similarity: Looking for a Good Abstraction Level.,2004,https://doi.org/10.5281/zenodo.1417403,Maarten Grachten+IIIA-CSIC - Artificial Intelligence Research Institute>ESP>facility|CSIC - Spanish Council for Scientific Research>ESP>facility;Josep-Lluís Arcos+IIIA-CSIC - Artificial Intelligence Research Institute>ESP>facility|CSIC - Spanish Council for Scientific Research>ESP>facility;Ramon López de Mántaras+IIIA-CSIC - Artificial Intelligence Research Institute>ESP>facility|CSIC - Spanish Council for Scientific Research>ESP>facility,"Computing melodic similarity is a very general problem with diverse musical applications ranging from music analysis to content-based retrieval. Choosing the appropriate level of representation is a crucial issue and depends on the type of application. Our research interest concerns the development of a CBR system for expressive music processing. In that context, a well chosen distance measure for melodies is a crucial issue. In this paper we propose a new melodic similarity measure based on the I/R model for melodic structure and compare it with other existing measures. The experimentation shows that the proposed measure provides a good compromise between discriminatory power and ability to recognize phrases from the same song.",ESP,facility,Developed economies,"[1.1515188, 18.223259]","[16.737848, 4.6540327]","[2.2587185, 4.788838, 0.8076854]","[6.761828, 0.8789584, 5.7426033]","[12.221924, 9.769717]","[9.448862, 1.8546646]","[12.70002, 15.496207, -0.83317703]","[11.315119, 6.9650464, 13.021452]"
71,Christopher Raphael,A Hybrid Graphical Model for Aligning Polyphonic Audio with Musical Scores.,2004,https://doi.org/10.5281/zenodo.1416500,Christopher Raphael+Indiana University>USA>education,"We present a new method for establishing an alignment between a polyphonic musical score and a corresponding sampled audio performance. The method uses a graphical model containing both discrete variables, corresponding to score position, as well as a continuous latent tempo process. We use a simple data model based only on the pitch content of the audio signal. The data interpretation is defined to be the most likely configuration of the hidden variables, given the data, and we develop computational methodology for this task using a variant of dynamic programming involving parametrically represented continuous variables. Experiments are presented on a 55-minute hand-marked orchestral test set.",USA,education,Developed economies,"[17.678282, -12.8890705]","[-16.61357, -11.7297735]","[1.6644043, -11.974631, -6.4011965]","[2.2246845, -19.064024, -3.5155818]","[10.980224, 6.5347195]","[6.2426972, 1.0399499]","[12.01625, 12.7075, -1.425013]","[8.320769, 6.140412, 10.909665]"
102,Kazuyoshi Yoshii;Masataka Goto;Hiroshi G. Okuno,Automatic Drum Sound Description for Real-World Music Using Template Adaptation and Matching Methods.,2004,https://doi.org/10.5281/zenodo.1415958,Kazuyoshi Yoshii+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroshi G. Okuno+Kyoto University>JPN>education,"This paper presents an automatic description system of drum sounds for real-world musical audio signals. Our system can represent onset times and names of drums by means of drum descriptors defined in the context of MPEG-7. For their automatic description, drum sounds must be identified in such polyphonic signals. The problem is that acoustic features of drum sounds vary with each musical piece and precise templates for them cannot be prepared in advance. To solve this problem, we propose new template-adaptation and template-matching methods. The former method adapts a single seed template prepared for each kind of drums to the corresponding drum sound appearing in an actual musical piece. The latter method then can detect all the onsets of each drum by using the corresponding adapted template. The onsets of bass and snare drums in any piece can thus be identified. Experimental results showed that the accuracy of identifying bass and snare drums in popular music was about 90%. Finally, we define drum descriptors in the MPEG-7 format and demonstrate an example of the automatic drum sound description for a piece of popular music.",JPN,education,Developed economies,"[23.96386, -44.496315]","[-15.230055, 3.2139928]","[19.362692, -16.89861, -1.4141477]","[5.894904, 9.118786, -14.207475]","[7.910079, 7.096384]","[8.517142, 4.02456]","[10.272855, 11.753853, 0.89868784]","[9.745764, 7.387669, 10.148893]"
101,Dan Yang 0002;Won-Sook Lee,Disambiguating Music Emotion Using Software Agents.,2004,https://doi.org/10.5281/zenodo.1415272,Dan Yang+University of Ottawa>CAN>education;Won Sook Lee+University of Ottawa>CAN>education,"Annotating music poses a cognitive load on listeners and this potentially interferes with the emotions being reported. One solution is to let software agents learn to make the annotator’s task easier and more efficient. Emo is a music annotation prototype that combines inputs from both human and software agents to better study human listening. A compositional theory of musical meaning provides the overall heuristics for the annotation process, with the listener drawing upon different influences such as acoustics, lyrics and cultural metadata to focus on a specific musical mood. Software agents track the way these choices are made from the influences available. A functional theory of human emotion provides the basis for introducing necessary bias into the machine learning agents. Conflicting positive and negative emotions can be separated on the basis of their different function (reward-approach and threat-avoidance) or dysfunction (psychotic). Negative emotions have strong ambiguity and these are the focus of the experiment. The results of mining psychological features of lyrics are promising, recognisable in terms of common sense ideas of emotion and in terms of accuracy. Further ideas for deploying agents in this model of music annotation are presented.",CAN,education,Developed economies,"[-57.367077, 0.7184386]","[49.874954, -6.3540025]","[-25.168121, 19.555737, 2.5875187]","[11.8096075, 19.544567, 3.9131389]","[13.838445, 12.759541]","[12.948104, 4.0538487]","[16.123007, 14.665432, 1.6105573]","[14.064543, 5.148268, 10.375906]"
100,Otto Wüst;Òscar Celma,An MPEG-7 Database System and Application for Content-Based Management and Retrieval of Music.,2004,https://doi.org/10.5281/zenodo.1418375,Otto Wust+Universitat Pompeu Fabra>ESP>education|Music Technology Group>Unknown>Unknown;Oscar Celma+Universitat Pompeu Fabra>ESP>education|Music Technology Group>Unknown>Unknown,"Computer users are gaining access to and are starting to accumulate moderately large collections of multimedia files, in particular of audio content, and therefore demand new applications and systems capable of effectively retrieving and manipulating these multimedia objects. Content-based retrieval of multimedia files is typically based on searching within a feature space, defined as a collection of parameters that have been extracted from the content and which describe it in a relevant way for that particular retrieval application. The MPEG-7 standard offers tools to model these metadata in an interoperable and extensible way, and can therefore be considered as a framework for building content-based audio retrieval systems. This paper highlights the most relevant aspects considered during the design and implementation of a DBMS-driven MPEG-7 layer on top of which a content-based music retrieval system has been built. A particular focus is set on the data modeling and database architecture issues.",ESP,education,Developed economies,"[-18.052021, 22.975828]","[17.092894, 26.924547]","[-12.395761, 6.4258604, -15.353425]","[-0.38233924, -7.6998343, 19.602945]","[13.923213, 7.668756]","[10.692043, 0.86929655]","[14.008184, 14.523095, -2.1321309]","[11.686399, 5.2253613, 12.487799]"
99,Gavin Wood;Simon O'Keefe,A Case Study of Distributed Music Audio Analysis Using the Geddei Processing Framework.,2004,https://doi.org/10.5281/zenodo.1414810,Gavin Wood+Unknown>Unknown>Unknown|University of York>GBR>education|Unknown>Unknown>Unknown,"Audio signal processing and refinement is an important part of a content-based music information retrieval system. As the our repertoire of techniques becomes more varied, there are greater requirements of computation power. Distributed storage techniques have become widespread and almost invisible with the advent of file-sharing systems, online digital music stores and online storage services. Even discounting data with potential copyright entanglements, there is a vast amount that is ripe for analysis, and thus parallelised and distributed processing techniques seem increasingly appropriate. Existing frameworks are already capable of a significant amount of audio analysis for music information retrieval. However they are by and large ignorant of distribution and parallelisation. There are middleware libraries to help with aspects of distributed computing, but combining the two can be cumbersome and inefficient. This paper provides a brief description of a software framework that can process audio in a scalable and distributed manner: Geddei. The paper then takes an interesting and relevant signal analysis task often used for music information retrieval and implements it under the Geddei framework. The ease of use is discussed and various measurements taken of Geddei, both in comparison to itself under different circumstances, and ‘reference code’ that was used in a previous study. We discuss the problems with the distribution of the task with Geddei and offer some possible solutions.",Unknown,Unknown,Unknown,"[-12.236826, 10.267147]","[11.584647, 27.667341]","[-2.180008, -5.2923293, -13.848565]","[-1.9506524, -1.3438073, 13.815619]","[12.653854, 8.27951]","[10.528389, 1.2419486]","[13.341961, 14.223021, -0.44383672]","[11.468904, 5.5456257, 11.905731]"
98,Brian Whitman;Dan Ellis,Automatic Record Reviews.,2004,https://doi.org/10.5281/zenodo.1416646,Brian Whitman+MIT Media Lab>USA>education|Columbia University>USA>education;Daniel P. W. Ellis+LabROSA>USA>company,"Record reviews provide a unique and focused source of linguistic data that can be related to musical recordings, to provide a basis for computational music understanding systems with applications in similarity, recommendation and classification. We analyze a large testbed of music and a corpus of reviews for each work to uncover patterns and develop mechanisms for removing reviewer bias and extraneous non-musical discussion. By building upon work in grounding free text against audio signals we invent an “automatic record review” system that labels new music audio with maximal semantic value for future retrieval tasks. In effect, we grow an unbiased music editor trained from the consensus of the online reviews we have gathered.",USA,education,Developed economies,"[-34.91927, -3.6880746]","[45.997513, 1.8340207]","[-27.417025, -0.014753528, 14.062309]","[18.817078, 15.732002, 6.162159]","[13.997345, 9.581497]","[11.52592, 2.9944677]","[14.729172, 13.721506, -0.6521346]","[12.839776, 6.043413, 11.516682]"
97,Tillman Weyde,The Influence of Pitch on Melodic Segmentation.,2004,https://doi.org/10.5281/zenodo.1415556,Tillman Weyde+University of Osnabrück>DEU>education,"Melodic segmentation is an important topic for music information retrieval, because it divides melodies into musically relevant units. Most influential theories on melodic segmentation of the last decades have stressed the role of pitch for melodic segmentation. The general assumption was, that relatively large changes or distances in any musical parameter like pitch, time, dynamics, or melodic movement mark segment boundaries. This has generally been accepted despite the lack of empirical studies. Here an empirical study is presented, that investigates the influence of inter-onset-intervals (IOI), intensity accents, pitch intervals, and pitch interval direction changes. The results show a significant influence only for IOIs and intensity, but neither for pitch intervals nor for changes in interval direction. The validity of the results and possible explanations are discussed and directions of further investigations are outlined.",DEU,education,Developed economies,"[4.7277684, -6.816063]","[2.5608408, -7.7040315]","[9.952732, 7.2369943, -7.821506]","[-4.0874124, -8.571988, 0.43776515]","[10.763309, 10.051113]","[7.8486485, 1.719449]","[11.615749, 15.139921, -0.5959239]","[9.773098, 7.0619264, 12.455861]"
96,Kristopher West;Stephen Cox,Features and classifiers for the automatic classification of musical audio signals.,2004,https://doi.org/10.5281/zenodo.1418025,Kris West+University of East Anglia>GBR>education;Stephen Cox+University of East Anglia>GBR>education,"Several factors affecting the automatic classification of musical audio signals are examined. Classification is performed on short audio frames and results are reported as “bag of frames” accuracies, where the audio is segmented into 23ms analysis frames and a majority vote is taken to decide the final classification. The effect of different parameterisations of the audio signal is examined. The effect of the inclusion of information on the temporal variation of these features is examined and finally, the performance of several different classifiers trained on the data is compared. A new classifier is introduced, based on the unsupervised construction of decision trees and either linear discriminant analysis or a pair of single Gaussian classifiers. The classification results show that the topology of the new classifier gives it a significant advantage over other classifiers, by allowing the classifier to model much more complex distributions within the data than Gaussian schemes do.",GBR,education,Developed economies,"[-24.634842, -15.199688]","[16.335861, -16.4996]","[-12.44059, 1.7917967, 18.798336]","[15.044335, 2.3609936, -11.911635]","[12.332253, 10.474282]","[9.185779, 3.6682098]","[13.307757, 13.934975, 1.1380527]","[10.947329, 7.597003, 10.571476]"
95,Emmanuel Vincent;Xavier Rodet,Instrument identification in solo and ensemble music using Independent Subspace Analysis.,2004,https://doi.org/10.5281/zenodo.1416524,Emmanuel Vincent+IRCAM>FRA>facility|Unknown>Unknown>Unknown;Xavier Rodet+IRCAM>FRA>facility|Unknown>Unknown>Unknown,"We investigate the use of Independent Subspace Analysis (ISA) for instrument identification in musical recordings. We represent short-term log-power spectra of possibly polyphonic music as weighted non-linear combinations of typical note spectra plus background noise. These typical note spectra are learnt either on databases containing isolated notes or on solo recordings from different instruments. We show that this model has some theoretical advantages over methods based on Gaussian Mixture Models (GMM) or on linear ISA. Preliminary experiments with five instruments and test excerpts taken from commercial CDs give promising results. The performance on clean solo excerpts is comparable with existing methods and shows limited degradation under reverberant conditions. Applied to a difficult duo excerpt, the model is also able to identify the right pair of instruments and to provide an approximate transcription of the notes played by each instrument.",FRA,facility,Developed economies,"[7.140861, -22.490082]","[-44.84194, -22.726362]","[19.312397, -6.4929676, -3.6153145]","[-2.129007, -13.373299, -31.687838]","[8.880512, 7.2655816]","[6.6561317, 4.755952]","[11.142412, 12.583344, 0.49996924]","[9.906122, 8.422893, 10.12397]"
94,Fabio Vignoli;Rob van Gulik;Huub van de Wetering,"Mapping Music In The Palm Of Your Hand, Explore And Discover Your Collection.",2004,https://doi.org/10.5281/zenodo.1416960,Rob van Gulik+Technische Universiteit Eindhoven>NLD>education|Philips Research Laboratories>Unknown>company;Fabio Vignoli+Technische Universiteit Eindhoven>NLD>education|Philips Research Laboratories>Unknown>company;Huub van de Wetering+Technische Universiteit Eindhoven>NLD>education,"The trends of miniaturization and increasing storage capabilities for portable music players made it possible to carry increasingly more music on small portable devices, but it also introduced negative consequences for the user interface and navigation. Finding music in large collections can be hard if one does not know exactly what to look for. In this paper a novel user interface to browse and navigate through music on small devices is proposed, together with the enabling algorithms. The goal of this interface is to enable the users to explore and discover their entire collection and to support non-specific searches. To this end, a new way to visualize and navigate through music is introduced: the artist map. The artist map is designed to provide an overview of an entire music collection, or a subset thereof, by clearly visualizing the similarity between artists, computed from the music itself. Contextual information (e.g. mood, genre) is added by coloring and by attribute magnets. The artist map is implemented by a graph-drawing algorithm, which uses an improved energy model. The proposed algorithm and interface have been implemented in a prototype and will be tested with ‘real’ users.",NLD,education,Developed economies,"[-17.533804, 28.644522]","[28.860914, 21.627842]","[-10.972485, 4.1774755, -20.865925]","[12.319561, -3.1647832, 22.512096]","[14.340644, 7.515144]","[11.4579115, 1.5044684]","[14.2685375, 14.335579, -2.4800773]","[12.356373, 5.29905, 13.1617155]"
93,Fabio Vignoli,Digital Music Interaction Concepts: A User Study.,2004,https://doi.org/10.5281/zenodo.1414994,Fabio Vignoli+Philips Research Laboratories>NLD>company,The popularity of digital music has recently rapidly increased. The widespread use on computers and portable players and its availability through the Internet have modified the interaction issues from availability towards choice. The user is confronted daily with an enormous amount of music. This situation shapes the need for the development of new user interfaces to access and retrieve music that takes full advantage of the music being digital. This paper reports the results of various user tests aimed at investigating how music listeners organize and access their digital music collection. The aim of the study is to investigate novel interaction concepts to access and retrieve music from large personal collections. The outcome of these studies was an interaction concept based on the notion of similarity of music items (artists and songs). This concept was further refined and developed into a demonstrator eventually tested with users.,NLD,company,Developed economies,"[-24.495604, 29.765108]","[29.144472, 26.413565]","[-18.973347, 10.61389, -20.6026]","[8.599768, -1.1203963, 22.854698]","[14.837182, 7.7970586]","[11.58817, 1.1170455]","[14.848921, 14.430627, -2.1584897]","[12.295814, 4.9258957, 12.883485]"
92,George Tzanetakis;Ajay Kapur;Manj Benning,Query-by-Beat-Boxing: Music Retrieval For The DJ.,2004,https://doi.org/10.5281/zenodo.1418033,Ajay Kapur+University of Victoria>CAN>education;Manj Benning+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"BeatBoxing is a type of vocal percussion, where musicians use their lips, cheeks, and throat to create different beats. It is commonly used by hiphop and rap artists. In this paper, we explore the use of BeatBoxing as a query mechanism for music information retrieval and more specifically the retrieval of drum loops. A classification system that automatically identifies the individual beat boxing sounds and can map them to corresponding drum sounds has been developed. In addition, the tempo of BeatBoxing is automatically detected and used to dynamically browse a database of music. We also describe some experiments in extracting structural representations of rhythm and their use for style classification of drum loops.",CAN,education,Developed economies,"[-15.730182, 21.944246]","[-15.146083, 4.7882204]","[-8.501648, 2.0172591, -12.226468]","[4.641641, 8.896202, -10.9686985]","[13.640766, 7.8705716]","[8.697386, 3.9226189]","[13.722885, 14.5332, -2.0373218]","[9.859528, 7.255782, 10.34124]"
91,Rainer Typke;Frans Wiering;Remco C. Veltkamp,A search method for notated polyphonic music with pitch and tempo fluctuations.,2004,https://doi.org/10.5281/zenodo.1417947,Rainer Typke+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"We compare two methods of measuring melodic similarity for symbolically represented polyphonic music. Both exploit advantages of transportation distances such as continuity and partial matching in the pitch dimension. By segmenting queries and database documents, one of them also offers partial matching in the time dimension. This method can find short queries in long database documents and is more robust against pitch and tempo fluctuations in the queries or database documents than the method that uses transportation distances alone. We compare the use of transportation distances with and without segmentation for the RISM A/II collection and find that segmentation improves recall and precision. With everything else being equal, the segmented search found 80 out of 114 relevant documents, while the method relying solely on transportation distances found only 60.",NLD,education,Developed economies,"[7.9972973, -17.851307]","[15.572884, 5.8337903]","[6.2950983, -12.124812, 0.41687682]","[8.16569, -1.5915616, 4.064093]","[13.006108, 7.968312]","[9.326685, 1.6155982]","[12.975442, 14.348389, -1.8057231]","[10.963182, 6.842341, 12.9155655]"
90,Wei-Ho Tsai;Hsin-Min Wang,Towards Automatic Identification Of Singing Language In Popular Music Recordings.,2004,https://doi.org/10.5281/zenodo.1417511,Wei-Ho Tsai+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown;Hsin-Min Wang+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown,"The automatic analysis of singing from music is an important and challenging issue within the research target of content-based retrieval of music information. As part of this research target, this study presents a first attempt to automatically identify the language sung in a music recording. It is assumed that each language has its own set of constraints that specify which of the basic linguistic events present in a singing process are allowed to follow another. The acoustic structure of individual languages may, thus, be characterized by statistically modeling those constraints. To this end, the proposed method employs vector clustering to convert a singing signal from its spectrum-based feature representation into a sequence of smaller basic phonological units. The dynamic characteristics of the sequence are then analyzed by using bigram language models. Since the vector clustering is performed in an unsupervised manner, the resulting system does not use sophisticated linguistic knowledge and, thus, is easily portable to new language sets. In addition, to eliminate the interference of background music, we leverage the statistical estimation of a piece’s music background so that the vector clustering is relevant to the solo singing voices in the accompanied signals.",TWN,education,Developing economies,"[-9.146408, -32.566605]","[-2.9602027, -1.1523857]","[15.819442, 12.590732, -14.806041]","[3.7525442, -1.7224475, -5.001479]","[10.214372, 11.172495]","[8.037403, 3.1523697]","[11.457077, 15.289128, 0.598826]","[10.369554, 7.9303007, 11.202676]"
89,Ken'ichi Toyoda;Kenzi Noike;Haruhiro Katayose,Utility System For Constructing Database Of Performance Deviations.,2004,https://doi.org/10.5281/zenodo.1417953,Ken’ichi Toyoda+Kwansei Gakuin University>JPN>education;Kenzi Noike+Kwansei Gakuin University>JPN>education;Haruhiro Katayose+Kwansei Gakuin University>JPN>education,"Demand for music databases is increasing for the studies of musicology and music informatics. Our goal is to construct databases that contain deviations of tempo, and dynamics, start-timing, and duration of each note. This paper describes a procedure based on hybrid use of DP Matching and HMM that efficiently extracts deviations from MIDI-formatted expressive human performances. The algorithm of quantizing the start-timing of the notes has been successfully tested on a database of ten expressive piano performances. It gives an accuracy of 92.9% when one note per bar is given as the guide. This paper also introduces tools provided so that the public can make use of our database on the web.",JPN,education,Developed economies,"[1.2880218, 33.28057]","[-15.79107, -4.770173]","[-10.422992, -12.860058, -17.367773]","[0.12087484, -15.853622, 0.22605336]","[12.997682, 6.8642344]","[6.749832, 1.0837029]","[13.21741, 13.183106, -1.8515179]","[8.800186, 6.1567793, 11.132192]"
87,Marc Torrens;Patrick Hertzog;Josep Lluís Arcos,Visualizing and Exploring Personal Music Libraries.,2004,https://doi.org/10.5281/zenodo.1414746,"Marc Torrens+MusicStrands Inc.>USA>company;Patrick Hertzog+EPFL>CHE>education;Josep-Lluís Arcos+IIIA, CSIC>ESP>facility","Nowadays, music fans are beginning to massively use mobile digital music players and dedicated software to organize and play large collections of music. In this context, users deal with huge music libraries containing thousands of tracks. Such a huge volume of music easily overwhelms users when selecting the music to listen or when organizing their collections. Music player software with visualizations based on textual lists and organizing features such as smart playlists are not really enough for helping users to efficiently manage their libraries. Thus, we propose new graphical visualizations and their associated features to allow users to better organize their personal music libraries and therefore also to ease selection later on.",USA,company,Developed economies,"[-20.08766, 33.044746]","[28.319681, 23.086353]","[-20.411055, 7.3631916, -16.51378]","[11.482332, -4.5365653, 24.356657]","[14.443799, 7.335577]","[11.42073, 1.4580703]","[14.508511, 14.193585, -2.429823]","[12.34614, 5.27837, 13.216732]"
70,Aggelos Pikrakis;Iasonas Antonopoulos;Sergios Theodoridis,Music meter and tempo tracking from raw polyphonic audio.,2004,https://doi.org/10.5281/zenodo.1416348,Aggelos Pikrakis+University of Athens>GRC>education;Iasonas Antonopoulos+University of Athens>GRC>education;Sergios Theodoridis+University of Athens>GRC>education,"This paper presents a method for the extraction of music meter and tempo from raw polyphonic audio recordings, assuming that music meter remains constant throughout the recording. Although this assumption can be restrictive for certain musical genres, it is acceptable for a large corpus of folklore eastern music styles, including Greek traditional dance music. Our approach is based on the self-similarity analysis of the audio recording and does not assume the presence of percussive instruments. Its novelty lies in the fact that music meter and tempo are jointly determined. The method has been applied to a variety of musical genres, in the context of Greek traditional music where music meter can be 2/4, 3/4, 4/4, 5/4, 7/8, 9/8, 12/8 and tempo ranges from 40bpm to 330bpm. Experiments have, so far, demonstrated the efficiency of our method (music meter and tempo were successfully extracted for over 95% of the recordings).",GRC,education,Developed economies,"[38.545593, -27.136007]","[-21.228786, 0.6725312]","[1.8897282, -28.498528, -3.2569373]","[1.4985831, 10.4910555, -4.108019]","[11.362097, 4.444814]","[6.322553, 1.6193848]","[10.850571, 13.372325, -2.6943252]","[8.423858, 6.8266015, 11.936642]"
31,AnYuan Guo;Hava T. Siegelmann,Time-Warped Longest Common Subsequence Algorithm for Music Retrieval.,2004,https://doi.org/10.5281/zenodo.1417165,An Yuan Guo+University of Massachusetts>USA>education;Hava Siegelmann+University of Massachusetts>USA>education,"Recent advances in music information retrieval have enabled users to query a database by singing or humming into a microphone. The queries are often inaccurate versions of the original songs due to singing errors and errors introduced in the music transcription process. In this paper, we present the Time-Warped Longest Common Subsequence algorithm (T-WLCS), which deals with singing errors involving rhythmic distortions. The algorithm is employed on song retrieval tasks, where its performance is compared to the longest common subsequence algorithm.",USA,education,Developed economies,"[-9.957324, 21.380157]","[8.923304, 13.2063]","[-1.2578454, 6.4546695, -13.298007]","[9.289513, -13.271889, 10.696617]","[13.3491335, 8.004145]","[8.9110985, 0.53486323]","[13.321707, 14.545652, -1.9474447]","[10.441946, 5.9620905, 13.244206]"
68,Jose Pedro;Vadim Tarasov;Eloi Batlle;Enric Guaus;Jaume Masip,Industrial audio fingerprinting distributed system with CORBA and Web Services.,2004,https://doi.org/10.5281/zenodo.1414792,Jose P. G. Mahedero+Universitat Pompeu Fabra>ESP>education;Vadim Tarasov+Universitat Pompeu Fabra>ESP>education;Eloi Batlle+Universitat Pompeu Fabra>ESP>education;Enric Guaus+Universitat Pompeu Fabra>ESP>education;Jaume Masip+Universitat Pompeu Fabra>ESP>education,"With digital technologies, music content providers face serious challenges to protect their rights. Due to the widespread nature of music sources, it is very difficult to centralize the audio management. Audio fingerprinting allows the identification of audio content regardless of the audio format and without the need of additional metadata. Monitoring the audio being broadcasted by the TV and radio stations of a country requires the design and implementation of a scalable, robust and modular framework. We have chosen CORBA as distributed environment. The whole functionality needs to be decoupled from clients. To do so, Web services have been deployed. The audio identification core uses a Hidden Markov Model-based audio fingerprinting technology. The paper discusses the design and implementation issues of a complete distributing system that automatically monitors audio content, specifically music and commercials. Today, a working prototype of such a system already exists, and is dedicated to monitoring several radio and tv stations in Spain.",ESP,education,Developed economies,"[-16.470846, -26.875984]","[12.347249, 30.337742]","[5.7565846, -14.763408, -23.818083]","[8.136411, -11.676386, 20.489754]","[9.071606, 4.4732966]","[8.801843, 0.024313042]","[10.597095, 11.53606, -1.9647077]","[10.561674, 5.1989827, 12.782585]"
38,Peter Jan O. Doets;Reginald L. Lagendijk,Stochastic Model of a Robust Audio Fingerprinting System.,2004,https://doi.org/10.5281/zenodo.1416490,P.J.O. Doets+Delft University of Technology>NLD>education;R.L. Lagendijk+Delft University of Technology>NLD>education,"An audio fingerprint is a compact representation of the perceptually relevant parts of audio content. A suitable audio fingerprint can be used to identify audio files, even if they are severely degraded due to compression or other types of signal processing operations. When degraded, the fingerprint closely resembles the fingerprint of the original, but is not identical. We plan to use a fingerprint not only to identify the song but also to assess the perceptual quality of the compressed content. In order to develop such a fingerprinting scheme, a model is needed to assess the behavior of a fingerprint subject to compression. In this paper we present the initial outlines of a model for an existing robust fingerprinting system to develop a more theoretical foundation. The model describes the stochastic behavior of the system when the input signal is a stationary (stochastic) signal. In this paper the input is assumed to be white noise. Initial theoretical results are reported and validated with experimental data.",NLD,education,Developed economies,"[-15.6531315, -26.15735]","[24.226416, -23.355534]","[7.2723613, -11.812971, -23.900455]","[16.212881, -14.282048, 5.3502417]","[9.069814, 4.457556]","[8.447505, -0.09332796]","[10.585846, 11.504873, -1.9223936]","[10.123486, 5.2521014, 12.996061]"
39,Tristan Jehan,Perceptual Segment Clustering For Music Description And Time-axis Redundancy Cancellation.,2004,https://doi.org/10.5281/zenodo.1416854,Tristan Jehan+Massachusetts Institute of Technology>USA>education,"Repeating sounds and patterns are widely exploited throughout music. However, although analysis and music information retrieval applications are often concerned with processing speed and music description, they typically discard the benefits of sound redundancy cancellation. We propose a perceptually grounded model for describing music as a sequence of labeled sound segments, for reducing data complexity, and for compressing audio.",USA,education,Developed economies,"[-6.8414526, 1.0276031]","[-1.0556319, 18.89756]","[0.46394417, -1.1024119, -4.2554283]","[-5.130902, -13.329073, 12.818075]","[12.250637, 8.499411]","[9.306781, 2.2469232]","[12.937916, 14.321914, -0.20925131]","[10.897432, 6.4846606, 11.957457]"
40,Steve Jones;Sally Jo Cunningham;Matt Jones 0001,Organizing digital music for use: an examination of personal music collections.,2004,https://doi.org/10.5281/zenodo.1416298,Sally Jo Cunningham+University of Waikato>NZL>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Matt Jones+University of Waikato>NZL>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Steve Jones+University of Waikato>NZL>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Current research on music information retrieval and music digital libraries focuses on providing access to huge, public music collections. In this paper we consider a different, but related, problem: supporting an individual in maintaining and using a personal music collection. We analyze organization and access techniques used to manage personal music collections (primarily CDs and MP3 files), and from these behaviors, to suggest user behaviors that should be supported in a personal music digital library (that is, a digital library of an individual’s personal music collection).",NZL,education,Developed economies,"[-27.165201, 28.380634]","[29.045788, 26.807764]","[-18.277037, 15.840387, -19.05381]","[7.5169306, -0.9541943, 22.888384]","[14.790942, 7.9136915]","[11.558127, 0.84788334]","[14.785453, 14.630318, -2.1663818]","[12.084529, 4.7789707, 12.636226]"
41,Jürgen Kilian;Holger H. Hoos,MusicBLAST - Gapped Sequence Alignment for MIR.,2004,https://doi.org/10.5281/zenodo.1416984,Jürgen Kilian+Darmstadt University of Technology>DEU>education|University of British Columbia>CAN>education;Holger H. Hoos+University of British Columbia>CAN>education,"We propose an algorithm, MusicBLAST, for approximate pattern search/matching on symbolic musical data. MusicBLAST is based on the BLAST algorithm, one of the most commonly used algorithms for similarity search on biological sequence data. MusicBLAST can be used in combination with an arbitrary similarity measure (e.g., melodic, rhythmic or combined) and retrieves multiple occurrences of a given search pattern and its variations. Different from many other pattern matching techniques, it can find incomplete and imperfect occurrences of a given pattern, and produces a significance measure for the accuracy and quality of its results. Like BLAST — and different from many musical pattern matching approaches — MusicBLAST retrieves heuristically optimised bi-directional alignments searching iteratively in forward and backward direction by starting at a dedicated seed note position of a performance.",DEU,education,Developed economies,"[-11.851543, 55.10689]","[5.884988, 7.459814]","[-38.03109, 0.33973345, 1.2272441]","[-6.1275635, 21.752544, 3.6420958]","[13.320412, 5.0927434]","[8.274428, 0.838539]","[14.730628, 11.408391, -1.3814561]","[10.296885, 6.495703, 13.181888]"
42,Peter Knees;Elias Pampalk;Gerhard Widmer,Artist Classification with Web-Based Data.,2004,https://doi.org/10.5281/zenodo.1417189,Peter Knees+Austrian Research Institute for Artificial Intelligence>AUT>facility;Elias Pampalk+Austrian Research Institute for Artificial Intelligence>AUT>facility;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence>AUT>facility|Medical University of Vienna>AUT>education,"Manifold approaches exist for organization of music by genre and/or style. In this paper we propose the use of text categorization techniques to classify artists present on the Internet. In particular, we retrieve and analyze webpages ranked by search engines to describe artists in terms of word occurrences on related pages. To classify artists we primarily use support vector machines. We present 3 experiments in which we address the following issues. First, we study the performance of our approach compared to previous work. Second, we investigate how daily fluctuations in the Internet affect our approach. Third, on a set of 224 artists from 14 genres we study (a) how many artists are necessary to define the concept of a genre, (b) which search engines perform best, (c) how to formulate search queries best, (d) which overall performance we can expect for classification, and finally (e) how our approach is suited as a similarity measure for artists.",AUT,facility,Developed economies,"[-39.54082, 6.2397695]","[38.236843, 5.7122693]","[-23.101759, 8.048117, 9.032424]","[17.591974, 8.437653, 6.084282]","[14.179811, 10.042731]","[11.757453, 2.73902]","[14.970991, 14.760269, -0.021610465]","[13.013542, 5.9700594, 11.908989]"
43,Ian Knopke,"Sound, Music and Textual Associations on the World Wide Web.",2004,https://doi.org/10.5281/zenodo.1416144,Ian Knopke+McGill University>CAN>education,"Sound files on the World Wide Web are accessed from web pages. To date, this relationship has not been explored extensively in the MIR literature. This paper details a series of experiments designed to measure the similarity between the public text visible on a web page and the linked sound files, the name of which is normally unseen by the user. A collection of web pages was retrieved from the web using a specially-constructed crawler. Sound file information and associated text were parsed from the pages and analyzed for similarity using common IR techniques such as TFIDF cosine measures. The results are intended to be used in the improvement of a web crawler for audio and music, as well as for MIR purposes in general.",CAN,education,Developed economies,"[-24.215921, 27.85259]","[20.471535, 23.871258]","[-16.34334, 0.077541724, -16.464684]","[7.0080748, -1.1301751, 16.753485]","[14.158962, 7.7717185]","[10.975772, 0.5957829]","[14.393107, 14.211029, -1.873134]","[11.904875, 5.1408496, 12.3574505]"
44,Arvindh Krishnaswamy,Melodic Atoms for Transcribing Carnatic Music.,2004,https://doi.org/10.5281/zenodo.1417859,Arvindh Krishnaswamy+Center for Computer Research in Music and Acoustics>USA>facility,"We had introduced a set of 2D melodic units to transcribe Carnatic music previously, and we now provide some illustrative examples using real pitch tracks to further our discussion on this topic.",USA,facility,Developed economies,"[6.973197, -1.7701131]","[1.0296181, -18.60135]","[7.3634515, 0.03563367, -13.368348]","[11.684942, -15.776161, -7.399067]","[11.264242, 10.154363]","[7.3902946, 1.546991]","[11.674662, 15.170209, -1.2932794]","[8.9732685, 7.2567425, 12.267884]"
37,Akinori Ito;Sung-Phil Heo;Motoyuki Suzuki;Shozo Makino,Comparison Of Features For DP-Matching Based Query-by-Humming System.,2004,https://doi.org/10.5281/zenodo.1416178,Akinori Ito+Tohoku University>JPN>education|Korea Telecom Research & Development Group>KOR>company;Sung-Phil Heo+Korea Telecom Research & Development Group>KOR>company|Tohoku University>JPN>education;Motoyuki Suzuki+Tohoku University>JPN>education;Shozo Makino+Tohoku University>JPN>education,"In this paper, we compared three kinds of similarity measures for DP-matching based query-by-humming music retrieval experiments. First, a DP matching-based algorithm is formulated using the similarity between a deltaPitch of an input humming and that of a song in the database. Then the three similarities are introduced: distance-based similarity, quantization-based similarity and fuzzy quantization-based similarity. The three similarities are compared by experiments. From the experimental results, the distance-based one gave the best recall rate. In addition, we examined the combination of distance-based and fuzzy-quantization-based similarities. The experimental result showed that the recall rate was improved by the combination.",JPN,education,Developed economies,"[-2.3443043, 38.001675]","[12.51833, 10.505232]","[-13.55058, -8.893441, -26.365538]","[4.486299, -11.552049, 14.911578]","[14.863778, 6.134688]","[9.062034, 0.6284347]","[13.191481, 15.343894, -2.9293754]","[10.640465, 5.9619904, 13.04535]"
45,Pedro Kröger,CsoundXML: a meta-language in XML for sound synthesis.,2004,https://doi.org/10.5281/zenodo.1415652,Pedro Kröger+Federal University at Bahia>BRA>education,"The software sound synthesis is closely related to the Music N programs started with Music I in 1957. Although Music N has many advantages such as unit generators and a flexible score language, it presents a few problems like limitations on instrument reuse, inflexibility of use of parameters, lack of a built-in graphical interface, and usually only one paradigm for scores. Some solutions concentrate in new from-scratch Music N implementations, while others focus in building user tools like pre-processors and graphical utilities. Nevertheless, new implementations in general focus in specific groups of problems leaving others unsolved. The user tools solve only one problem with no connection with others. In this paper we investigate the problem of creating a meta-language for sound synthesis. This constitutes an elegant solution for the above cited problems, without the need of a yet new acoustic compiler implementation, allowing a tight integration which is difficult to obtain with the present user tools.",BRA,education,Developing economies,"[9.763024, 28.375736]","[-0.8495379, 32.575256]","[-14.814617, -9.602467, 12.41838]","[-10.673446, -5.4442267, 15.857977]","[13.231742, 6.9517136]","[9.987024, 0.72058743]","[13.888642, 12.6252985, -1.4671756]","[10.541357, 5.384615, 11.578857]"
47,Mika Kuuskankare;Mikael Laurson,Expressive Notation Package - an Overview.,2004,https://doi.org/10.5281/zenodo.1415084,Mika Kuuskankare+Sibelius Academy>FIN>education;Mikael Laurson+Sibelius Academy>FIN>education,The purpose of this paper is to give the reader a concise overview of Expressive Notation Package 2.0 (henceforward ENP). ENP is music notation program that belongs to a family of music and sound related software packages developed at Sibelius Academy in Finland. ENP has been used in various research projects during the past several years.,FIN,education,Developed economies,"[20.07876, 29.607533]","[-0.6637598, 34.93808]","[-11.306035, -15.3532715, 8.689581]","[-14.673266, -5.5029697, 16.619905]","[11.764102, 6.745828]","[9.772865, 0.6256911]","[13.115166, 12.31564, -1.4343245]","[10.393156, 5.3826165, 11.762869]"
23,Dan Ellis;John Arroyo,Eigenrhythms: Drum pattern basis sets for classification and generation.,2004,https://doi.org/10.5281/zenodo.1415948,Daniel P. W. Ellis+Columbia University>USA>education;John Arroyo+Columbia University>USA>education,"We took a collection of 100 drum beats from popular music tracks and estimated the measure length and downbeat position of each one. Using these values, we normalized each pattern to form an ensemble of aligned drum patterns. Principal Component Analysis on this data set results in a set of basis ‘patterns’ that can be combined to give approximations and interpolations of all the examples. We use this low-dimension representation of the drum patterns as a space for classification and visualization, and discuss its application to generating continua of rhythms. Our classification results were very modest – about 20% correct on a 10-way genre classification task – but we show that the projection into principal component space reveals aspects of the rhythm that are largely orthogonal to genre but are still perceptually relevant.",USA,education,Developed economies,"[26.992636, -49.09498]","[-17.818516, 2.3979378]","[22.687384, -24.011518, 0.619179]","[4.948443, 11.598758, -9.652063]","[7.760245, 7.014192]","[8.466265, 3.5547738]","[10.252453, 11.647308, 0.8908264]","[9.934813, 7.4029856, 10.493001]"
54,Anna Lubiw;Luke Tanur,Pattern Matching in Polyphonic Music as a Weighted Geometric Translation Problem.,2004,https://doi.org/10.5281/zenodo.1417969,Anna Lubiw+University of Waterloo>CAN>education|University of Waterloo>CAN>education;Luke Tanur+University of Waterloo>CAN>education|University of Waterloo>CAN>education,"We consider the music pattern matching problem—to find occurrences of a small fragment of music called the “pattern” in a larger body of music called the “score”—as a problem of translating a set of horizontal line segments in the plane to find the best match in a larger set of horizontal line segments. Our contribution is that we use fairly general weight functions to measure the quality of a match, thus enabling approximate pattern matching. We give an algorithm with running time O(nm log m), where n is the size of the score and m is the size of the pattern. We show that the problem, in this geometric formulation, is unlikely to have a significantly faster algorithm because it is at least as hard as a basic problem called 3-SUM that is conjectured to have no subquadratic algorithm. We present some examples to show the potential of this method for finding minor variations of a theme, and for finding polyphonic musical patterns in a polyphonic score.",CAN,education,Developed economies,"[11.591136, 10.739791]","[6.522049, 17.164986]","[1.8839742, -13.4408045, 4.172383]","[3.6566565, -11.717506, 4.976592]","[11.534416, 7.4638405]","[8.8397, 0.9718399]","[12.439122, 13.392826, -1.1418805]","[10.490405, 6.565581, 13.17878]"
53,Maurício A. Loureiro;Hugo Bastos de Paula;Hani C. Yehia,Timbre Classification Of A Single Musical Instrument.,2004,https://doi.org/10.5281/zenodo.1416320,Mauricio A. Loureiro+Federal University of Minas Gerais>BRA>education;Hugo B. de Paula+Federal University of Minas Gerais>BRA>education;Hani C. Yehia+Federal University of Minas Gerais>BRA>education,"In order to map the spectral characteristics of the large variety of sounds a musical instrument may produce, different notes were performed and sampled in several intensity levels across the whole extension of a clarinet. Amplitude and frequency time-varying curves of partials were measured by Discrete Fourier Transform. A limited set of orthogonal spectral bases was derived by Principal Component Analysis techniques. These bases defined spectral sub-spaces capable of representing all tested sounds and of grouping them according to the distance metrics of the representation. A clustering algorithm was used to infer timbre classes. Preliminary tests with resynthesized sounds with normalized pitch showed a strong relation between the perceived timbre and the cluster label to which the notes were assigned. Self-Organizing Maps lead to results similar to those obtained by PCA representation and K-means clustering algorithm.",BRA,education,Developing economies,"[9.512075, -28.520315]","[-41.71877, -19.927713]","[18.640882, -11.293933, -3.130424]","[-1.9619889, -10.491076, -33.455093]","[8.832951, 7.428266]","[6.826647, 4.5314903]","[11.067632, 12.689369, 0.6204183]","[10.061878, 8.313318, 10.488221]"
52,Beth Logan,Music Recommendation from Song Sets.,2004,https://doi.org/10.5281/zenodo.1416658,Beth Logan+Hewlett Packard Labs>USA>company,"We motivate the problem of music recommendation based solely on acoustics from groups of related songs or ‘song sets’. We propose four solutions which can be used with any acoustic-based similarity measure. The first builds a model for each song set and recommends new songs according to their distance from this model. The next three approaches recommend songs according to the average, median and minimum distance to songs in the song set. For a similarity measure based on K-means models of MFCC features, experiments on a database of 18647 songs indicated that the minimum distance technique is the most effective, returning a valid recommendation as one of the top 5 32.5% of the time. The approach based on the median distance was the next best, returning a valid recommendation as one of the top 5 29.5% of the time.",USA,company,Developed economies,"[-47.358818, 26.902119]","[37.004524, 13.987712]","[-9.945531, 27.197311, -11.790872]","[15.712295, 4.089191, 12.93838]","[15.851159, 9.146785]","[12.349841, 2.1069126]","[15.814677, 15.708464, -1.4435879]","[13.428294, 5.4735713, 12.734838]"
51,Ming Li;Ronan Sleep,Improving Melody Classification by Discriminant Feature Extraction and Fusion.,2004,https://doi.org/10.5281/zenodo.1416728,Ming Li+University of East Anglia>GBR>education;Ronan Sleep+University of East Anglia>GBR>education,"We propose a general approach to discriminant feature extraction and fusion, built on an optimal feature transformation for discriminant analysis. Our experiments indicate that our approach can dramatically reduce the dimensionality of original feature space whilst improving its discriminant power. Our feature fusion method can be carried out in the reduced lower-dimensional subspace, resulting in a further improvement in accuracy. Our experiments concern the classification of music styles based only on the pitch sequence derived from monophonic melodies.",GBR,education,Developed economies,"[7.6188164, -11.352637]","[23.048214, -8.613112]","[14.752966, 6.0103817, -0.26752755]","[16.248676, 3.6123102, -4.2090936]","[10.203217, 9.948932]","[9.607763, 3.4549015]","[11.137654, 14.882774, -0.48640552]","[11.397031, 7.342771, 10.7602625]"
50,Micheline Lesaffre;Marc Leman;Bernard De Baets;Jean-Pierre Martens,Methodological Considerations Concerning Manual Annotation Of Musical Audio In Function Of Algorithm Development.,2004,https://doi.org/10.5281/zenodo.1414874,Micheline Lesaffre+Ghent University>BEL>education;Marc Leman+Ghent University>BEL>education;Bernard De Baets+Ghent University>BEL>education;Jean-Pierre Martens+Ghent University>BEL>education,"In research on musical audio-mining, annotated music databases are needed which allow the development of computational tools that extract from the musical audio stream the kind of high-level content that users can deal with in Music Information Retrieval (MIR) contexts. The notion of musical content, and therefore the notion of annotation, is ill-defined, however, both in the syntactic and semantic sense. As a consequence, annotation has been approached from a variety of perspectives (but mainly linguistic-symbolic oriented), and a general methodology is lacking. This paper is a step towards the definition of a general framework for manual annotation of musical audio in function of a computational approach to musical audio-mining that is based on algorithms that learn from annotated data.",BEL,education,Developed economies,"[5.0667233, 7.1261353]","[-4.2463665, 13.195886]","[-12.912171, -5.63739, 5.8144526]","[-5.3121195, 3.9557354, 6.5173044]","[12.137787, 8.424831]","[9.139883, 2.0022686]","[13.169178, 13.649626, -0.8090171]","[10.657352, 6.1407657, 11.729625]"
46,Frank Kurth;Meinard Müller;Andreas Ribbrock;Tido Röder;David Damm;Christian Fremerey,A Prototypical Service for Real-Time Access to Local Context-Based Music Information.,2004,https://doi.org/10.5281/zenodo.1414926,Frank Kurth+University of Bonn>DEU>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Meinard Müller+University of Bonn>DEU>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Andreas Ribbrock+University of Bonn>DEU>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Tido Röder+University of Bonn>DEU>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;David Damm+University of Bonn>DEU>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Christian Fremerey+University of Bonn>DEU>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"In this contribution we propose a generic service for real-time access to context-based music information such as lyrics or score data. In our web-based client-server scenario, a client application plays back a particular (waveform) audio recording. During playback, the client connects to a server which in turn identifies the particular piece of audio as well as the current playback position. Subsequently, the server delivers local, i.e., position specific, context-based information on the audio piece to the client. The client then synchronously displays the received information during acoustic playback. We demonstrate how such a service can be established using recent MIR (Music Information Retrieval) techniques such as audio identification and synchronization and present two particular application scenarios.",DEU,education,Developed economies,"[-31.488188, 28.216038]","[11.765632, 31.845577]","[-20.333391, 12.541804, -13.965773]","[-7.495957, 1.1809272, 19.543444]","[14.741265, 7.743241]","[11.070887, 0.93270123]","[14.785259, 14.472986, -2.131478]","[11.538456, 4.94094, 12.342916]"
36,Jia-Lien Hsu;Arbee L. P. Chen;Hung-Chen Chen,Finding Approximate Repeating Patterns from Sequence Data.,2004,https://doi.org/10.5281/zenodo.1415530,Jia-Lien Hsu+Fu Jen Catholic University>TWN>education;Arbee L.P. Chen+National Chengchi University>TWN>education;Hung-Chen Chen+National Tsing Hua University>TWN>education,"In this paper, an application of feature extraction from music data is first introduced to motivate our research of finding approximate repeating patterns from sequence data. An approximate repeating pattern is defined as a sequence of symbols which appears more than once under certain approximation types in a data sequence. By using the ‘cut’ and ‘pattern_join’ operators, we develop a level-wise approach to solve the problem of finding approximate repeating patterns.",TWN,education,Developing economies,"[16.348644, 24.562126]","[4.4036565, 14.700063]","[-2.1397705, -16.472816, 10.177511]","[1.7122391, -8.158937, 6.2980704]","[11.513236, 7.416136]","[8.991825, 1.2599783]","[12.403962, 13.12388, -0.79344606]","[10.626344, 6.764581, 12.692328]"
35,Robyn Holmes;Marie-Louise Ayres,MusicAustralia: towards a national music information infrastructure.,2004,https://doi.org/10.5281/zenodo.1418097,Robyn Holmes+National Library of Australia>AUS>facility;Marie-Louise Ayres+National Library of Australia>AUS>facility,"MusicAustralia is a national music discovery service, developed by the National Library of Australia and ScreenSound Australia, National Film and Sound Archive. The service aims to provide seamless access to music and music information resources, in multiple formats, from custodians across all cultural sectors. This paper describes the development of the service, including its architecture, and content base. Service development to date has concentrated on metadata contribution and discovery strategies, together with development of the national digital music collection. In the future, digital content developed to populate the service could be subjected to Music Information Retrieval applications, to further enrich understanding of Australian music. The paper finishes by examining the challenges of achieving these advanced services in an environment where MIR research is relatively undeveloped.",AUS,facility,Developed economies,"[-26.922104, 33.15383]","[23.297777, 34.473618]","[-19.040163, 1.3717577, -6.00695]","[0.95518583, 2.1067123, 21.107315]","[14.267493, 8.401743]","[11.429656, 0.23459406]","[14.720299, 14.199175, -1.8043424]","[11.937985, 4.5695324, 12.213109]"
34,David Hirst,An Analytical Methodology for Acousmatic Music.,2004,https://doi.org/10.5281/zenodo.1415656,David Hirst+University of Melbourne>AUS>education,"This paper presents a procedure for the analysis of acousmatic music which was derived from the synthesis of top-down (knowledge driven) and bottom-up (data-driven) cognitive psychological views. The procedure is also a synthesis of research on primitive auditory scene analysis, combined with the research on acoustic, semantic, and syntactic factors in the perception of everyday environmental sounds. The procedure can be summarized as consisting of a number of steps: Segregation of sonic objects; Horizontal integration and/or segregation; Vertical integration and/or segregation; Assimilation and meaning.",AUS,education,Developed economies,"[2.066817, 3.9777102]","[-1.0972724, 17.069437]","[-4.353241, -6.959939, 6.9561834]","[1.7313244, 19.028667, 4.1767635]","[12.202657, 8.255903]","[8.9196415, 2.2331617]","[13.143484, 13.774915, -0.7859262]","[10.607711, 6.7175727, 12.073999]"
67,Steffen Pauws,Musical key extraction from audio.,2004,https://doi.org/10.5281/zenodo.1416326,Steffen Pauws+Philips Research Laboratories Eindhoven>NLD>company,"The realisation and evaluation of a musical key extraction algorithm that works directly on raw audio data is presented. Its implementation is based on models of human auditory perception and music cognition. It is straightforward and has minimal computing requirements. First, it computes a chromagram from non-overlapping 100 msecs time frames of audio; a chromagram represents the likelihood of the chroma occurrences in the audio. This chromagram is correlated with Krumhansl’s key profiles that represent the perceived stability of each chroma within the context of a particular musical key. The key profile that has maximum correlation with the computed chromagram is taken as the most likely key. An evaluation with 237 CD recordings of classical piano sonatas indicated a classification accuracy of 75.1%. By considering the exact, relative, dominant, sub-dominant and parallel keys as similar keys, the accuracy is even 94.1%.",NLD,company,Developed economies,"[31.303202, 13.535949]","[-6.664229, -3.453192]","[9.295348, -14.76834, 11.444996]","[7.3292165, -5.4629054, -5.5366707]","[10.712221, 7.5524526]","[7.4305654, 2.7493434]","[12.198508, 13.129683, 0.26196685]","[10.2377825, 8.2345085, 11.811295]"
66,Bryan Pardo,Tempo Tracking with a Single Oscillator.,2004,https://doi.org/10.5281/zenodo.1415090,Bryan Pardo+Northwestern University>USA>education,"I describe a simple on-line tempo tracker, based on phase and period locking a single oscillator to performance event timings. The tracker parameters are optimized on a corpus of solo piano performances by twelve musicians. The tracker is then tested on a second corpus of performances, played by the same twelve musicians. The performance of this tracker is compared to previously published results for a tempo tracker based on combining a tempogram and Kalman filter.",USA,education,Developed economies,"[41.21179, -29.494415]","[-29.490564, -2.315696]","[2.7091763, -32.610718, -2.8514109]","[-9.263315, 8.6274395, -8.225223]","[11.267455, 4.334988]","[5.3807955, 1.6270955]","[10.715599, 13.295694, -2.7126544]","[7.5244846, 6.6859264, 11.06902]"
65,Elias Pampalk,A Matlab Toolbox to Compute Music Similarity from Audio.,2004,https://doi.org/10.5281/zenodo.1418077,Elias Pampalk+Austrian Research Institute for Artificial Intelligence>AUT>facility,"A Matlab toolbox implementing music similarity measures for audio is presented. The implemented measures focus on aspects related to timbre and periodicities in the signal. This paper gives an overview of the implemented functions. In particular, the basics of the similarity measures are reviewed and some visualizations are discussed.",AUT,facility,Developed economies,"[-6.8161125, 14.332349]","[21.988197, 6.508189]","[-4.653706, 5.959902, -3.6861205]","[11.68124, -0.7976275, 5.866778]","[13.043359, 9.212936]","[9.882713, 2.015013]","[13.586247, 14.998632, -0.6313407]","[11.537484, 6.7829604, 12.437982]"
64,François Pachet;Aymeric Zils,Automatic extraction of music descriptors from acoustic signals.,2004,https://doi.org/10.5281/zenodo.1416106,François Pachet+Sony CSL Paris>FRA>company;Aymeric Zils+Sony CSL Paris>FRA>company,"High-Level music descriptors are key ingredients for music information retrieval systems. Although there is a long tradition in extracting information from acoustic signals, the field of music information extraction is largely heuristic in nature. We present here a heuristic-based generic approach for extracting automatically high-level music descriptors from acoustic signals. This approach is based on Genetic Programming, used to build relevant features as functions of mathematical and signal processing operators. The search of relevant features is guided by specialized heuristics that embody knowledge about the signal processing functions built by the system. Signal processing patterns are used in order to control the general processing methods. In addition, rewriting rules are introduced to simplify overly complex expressions, and a caching system further reduces the computing cost of each cycle. Finally, the features build by the system are combined into an optimized machine learning descriptor model, and an executable program is generated to compute the model on any audio signal. In this paper, we describe the overall system and compare its results against traditional approaches in musical feature extraction à la Mpeg7.",FRA,company,Developed economies,"[-8.866241, -14.001535]","[10.376193, -6.7108397]","[1.1428108, -1.3917346, -14.935118]","[8.170974, 2.6054368, -4.6736627]","[12.632959, 8.277087]","[9.222643, 2.9229593]","[13.141268, 13.897396, 0.51011336]","[10.824202, 6.718461, 10.925673]"
63,Giovanna Neve;Nicola Orio,Indexing and Retrieval of Music Documents through Pattern Analysis and Data Fusion Techniques.,2004,https://doi.org/10.5281/zenodo.1416372,Giovanna Neve+University of Padova>ITA>education|University of Padova>ITA>education;Nicola Orio+University of Padova>ITA>education|University of Padova>ITA>education,"""One of the challenges of music information retrieval is the automatic extraction of effective content descriptors of music documents, which can be used at indexing and at retrieval time to match queries with documents. In this paper it is proposed to index music documents with frequent musical patterns. A musical pattern is a sequence of features in the score that is repeated at least twice: features can regard perceptually relevant characteristics, such as rhythm, pitch, or both. Data fusion techniques are applied to merge the results obtained using different features. A set of experimental tests has been carried out on retrieval effectiveness, robustness to query errors, and dependency on query length on a collection of Beatles’ songs using a set of queries. The proposed approach gave good results, both using single features and, in particular, merging the rank lists obtained by different features with a data fusion approach.""",ITA,education,Developed economies,"[-12.51006, 24.32282]","[18.81526, 17.702374]","[-4.282767, 8.802871, -15.262935]","[9.236955, -3.1141374, 13.492874]","[13.582065, 8.013486]","[9.949209, 1.3684781]","[13.586512, 14.690623, -1.9967132]","[11.474823, 5.9173307, 12.648355]"
62,Andrew Nesbit;Lloyd Hollenberg;Anthony Senyard,Towards Automatic Transcription of Australian Aboriginal Music.,2004,https://doi.org/10.5281/zenodo.1416874,Andrew Nesbit+University of Melbourne>AUS>education;Lloyd Hollenberg+University of Melbourne>AUS>education;Anthony Senyard+University of Melbourne>AUS>education,"We describe a system designed for automatic extraction and segmentation of didjeridu and clapsticks from certain styles of traditional Aboriginal Australian music. For didjeridu, we locate the start of notes using a complex-domain note onset detection algorithm, and use the detected onsets as cues for determining the harmonic series of sinusoids belonging to the didjeridu. The harmonic series is hypothesised, based on prior knowledge of the fundamental frequency of the didjeridu, and the most likely hypothesis is assumed. For clapsticks, we use independent subspace analysis to split the signal into harmonic and percussive components, followed by classification of the independent components. Finally, we identify areas in which the system can be enhanced to improve accuracy and also to extract a wider range of musically-relevant features. These include algorithms such as high frequency content techniques, and also computing the morphology of the didjeridu.",AUS,education,Developed economies,"[26.539434, -10.196494]","[-13.099485, -2.667848]","[18.339167, -2.2360394, 8.634525]","[2.8425817, 3.1145048, -11.23385]","[9.679009, 7.5877523]","[7.3239136, 2.6642375]","[11.722465, 12.303666, -0.041171424]","[9.555265, 7.5111384, 11.247549]"
61,Tomoyasu Nakano;Jun Ogata;Masataka Goto;Yuzuru Hiraga,A Drum Pattern Retrieval Method by Voice Percussion.,2004,https://doi.org/10.5281/zenodo.1417569,Tomoyasu Nakano+University of Tsukuba>JPN>education;Jun Ogata+University of Tsukuba>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Yuzuru Hiraga+University of Tsukuba>JPN>education,"This paper presents a method for voice percussion recognition and its application to drum pattern retrieval. Recognition of voice percussion (verbalized expression of drum sound by voice) requires an approach that is different from existing methods. Individual differences in both vocal characteristics and the kinds of verbal expressions used add further complication to the task. The approach taken in this study uses onomatopoeia as internal representation of drum sounds, and combines the recognition of voice percussion with the retrieval of intended drum patterns. This scheme is intended to deal with the two types of individual differences mentioned above. In a recognition experiment with 200 utterances of voice percussion, our method achieved a recognition rate of 91.0% for the highest-tuned setting.",JPN,education,Developed economies,"[23.801529, -46.56013]","[-14.595995, 0.52581227]","[18.33832, -20.08604, -1.5769508]","[6.614813, 6.4137144, -14.26044]","[7.8670444, 7.0252795]","[8.482666, 3.9088428]","[10.205709, 11.757449, 0.9223826]","[9.730512, 7.37125, 10.247981]"
60,Meinard Müller;Frank Kurth;Tido Röder,Towards an Efficient Algorithm for Automatic Score-to-Audio Synchronization.,2004,https://doi.org/10.5281/zenodo.1416302,Meinard Müller+Universität Bonn>DEU>education;Frank Kurth+Universität Bonn>DEU>education;Tido Röder+Universität Bonn>DEU>education,"In the last few years, several algorithms for the automatic alignment of audio and score data corresponding to the same piece of music have been proposed. Among the major drawbacks to these approaches are the long running times as well as the large memory requirements. In this paper we present an algorithm, which solves the synchronization problem accurately and efficiently for complex, polyphonic piano music. In a first step, we extract from the audio data stream a set of highly expressive features encoding note onset candidates separately for all pitches. This makes computations efficient since only a small number of such features is sufficient to solve the synchronization task. Based on a suitable matching model, the best match between the score and the feature parameters is computed by dynamic programming (DP). To further cut down the computational cost in the synchronization process, we introduce the concept of anchor matches, matches which can be easily established. Then the DP-based technique is locally applied between adjacent anchor matches. Evaluation results have been obtained on complex polyphonic piano pieces including Chopin’s Etudes Op. 10.",DEU,education,Developed economies,"[24.012388, -30.009258]","[-16.073088, -12.585435]","[-1.2768241, -17.98081, -14.588518]","[0.43296048, -18.661491, -2.303711]","[11.000842, 5.8690906]","[6.270294, 0.8428942]","[11.923001, 12.454943, -2.0599914]","[8.328914, 5.979902, 10.93532]"
59,Daniel Müllensiefen;Klaus Frieler,Optimizing Measures Of Melodic Similarity For The Exploration Of A Large Folk Song Database.,2004,https://doi.org/10.5281/zenodo.1418031,Daniel Müllensiefen+University of Hamburg>DEU>education|University of Hamburg>DEU>education;Klaus Frieler+University of Hamburg>DEU>education|University of Hamburg>DEU>education,"This investigation aims at finding an optimal way of measuring the similarity of melodies. The applicability for an automated analysis and classification was tested on a folk song collection from Luxembourg that had been thoroughly analysed by an expert ethnomusicologist. Firstly a systematization of the currently available approaches to similarity measurements of melodies was done. About 50 similarity measures were implemented which differ in the way of transforming musical data and in the computational algorithms. Three listener experiments were conducted to compare the performance of the different measures to human experts’ ratings. Then an optimized model was obtained by using linear regression, which combines the output of several measures representing different musical dimensions. The performance of this optimized measure was compared with the classification work of a human ethnomusicologist on a collection of 577 Luxembourg folksongs.",DEU,education,Developed economies,"[-0.7280647, 16.518309]","[16.793463, 2.6469808]","[2.267634, 10.376395, -0.86447364]","[6.7811875, 3.9623897, 3.8593864]","[12.447461, 9.845246]","[9.610628, 1.9677203]","[12.740365, 15.385054, -0.6040846]","[11.367788, 7.055767, 13.028196]"
58,R. Mitchell;Irfan A. Essa,Feature Weighting for Segmentation.,2004,https://doi.org/10.5281/zenodo.1414976,R. Mitchell Parry+Georgia Institute of Technology>USA>education;Irfan Essa+Georgia Institute of Technology>USA>education,"This paper proposes the use of feature weights to reveal the hierarchical nature of music audio. Feature weighting has been exploited in machine learning, but has not been applied to music audio segmentation. We describe both a global and a local approach to automatic feature weighting. The global approach assigns a single weighting to all features in a song. The local approach uses the local separability directly. Both approaches reveal structure that is obscured by standard features, and emphasize segments of a particular size.",USA,education,Developed economies,"[-6.024328, -11.960643]","[-0.008905567, 3.2589855]","[6.0463576, -7.621591, -9.225487]","[0.63463444, 1.9024523, -0.30356318]","[11.462495, 8.694779]","[8.525907, 3.200259]","[12.497588, 14.053391, 0.6222346]","[10.643563, 7.326023, 11.238772]"
57,Martin F. McKinney;Dirk Moelants,Extracting the perceptual tempo from music.,2004,https://doi.org/10.5281/zenodo.1415146,Martin F. McKinney+Philips Research Laboratories>NLD>company;Dirk Moelants+Ghent University>BEL>education,The study presented here outlines a procedure for measuring and quantitatively representing the perceptual tempo of a musical excerpt. We also present a method for applying such measures of perceptual tempo to the design of automatic tempo-trackers in order to more accurately represent the perceived beat in music.,NLD,company,Developed economies,"[39.886665, -26.73438]","[-29.17191, -1.9504786]","[-0.11961194, -28.955652, -1.863727]","[-8.347246, 9.544083, -7.220422]","[11.460437, 4.502631]","[5.3671584, 1.6073076]","[10.882625, 13.392524, -2.796198]","[7.439623, 6.7444663, 11.118782]"
30,Matthias Gruhne;Christian Uhle;Christian Dittmar;Markus Cremer,Extraction of Drum Patterns and their Description within the MPEG-7 High-Level-Framework.,2004,https://doi.org/10.5281/zenodo.1414888,Matthias Gruhne+Fraunhofer IDMT>DEU>facility;Christian Uhle+Fraunhofer IDMT>DEU>facility;Christian Dittmar+Fraunhofer IDMT>DEU>facility;Markus Cremer+Fraunhofer IDMT>DEU>facility,"A number of metadata standards have been published in recent years due to the increasing availability of multimedia content and the resulting issue of sorting and retrieving this content. One of the most recent efforts for a well defined metadata description is the ISO/IEC MPEG-7 standard, which takes a very broad approach towards the definition of metadata. Herein, not merely hand annotated textual information can be transported and stored, but also more signal specific data that can in most cases be automatically retrieved from the multimedia content itself. In this publication an algorithm for the automated transcription of rhythmic (percussive) accompaniment in modern day popular music is described. However, the emphasis here is not a precise transcription, but on capturing the “rhythmic gist” of the piece of music in order to allow a more abstract comparison of musical pieces by their dominant rhythmic patterns. A small-scale evaluation of the algorithm is presented along with an example representation of the thus gained semantically meaningful metadata using description methods currently discussed within MPEG-7.",DEU,facility,Developed economies,"[24.987139, -46.72422]","[-4.5878506, 16.56404]","[21.092936, -19.473814, -1.8134222]","[-5.1577883, 1.0664071, 8.851201]","[7.7847486, 6.9514494]","[9.929374, 0.4772912]","[10.186618, 11.7373085, 0.92705065]","[10.367431, 5.426014, 11.665471]"
56,Cory McKay;Ichiro Fujinaga,Automatic Genre Classification Using Large High-Level Musical Feature Sets.,2004,https://doi.org/10.5281/zenodo.1416158,Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper presents a system that extracts 109 musical features from symbolic recordings (MIDI, in this case) and uses them to classify the recordings by genre. The features used here are based on instrumentation, texture, rhythm, dynamics, pitch statistics, melody and chords. The classification is performed hierarchically using different sets of features at different levels of the hierarchy. Which features are used at each level, and their relative weightings, are determined using genetic algorithms. Classification is performed using a novel ensemble of feedforward neural networks and k-nearest neighbour classifiers. Arguments are presented emphasizing the importance of using high-level musical features, something that has been largely neglected in automatic classification systems to date in favour of low-level features. The effect on classification performance of varying the number of candidate features is examined in order to empirically demonstrate the importance of using a large variety of musically meaningful features. Two differently sized hierarchies are used in order to test the performance of the system under different conditions. Very encouraging classification success rates of 98% for root genres and 90% for leaf genres are obtained for a hierarchical taxonomy consisting of 9 leaf genres.",CAN,education,Developed economies,"[-28.433704, -14.165149]","[16.346527, -5.7251177]","[-16.140373, 2.6019766, 17.425043]","[9.854748, 8.190266, -5.0449486]","[12.820906, 10.802414]","[9.359898, 3.2749562]","[13.836602, 14.202603, 1.336334]","[10.979576, 7.1136255, 10.777994]"
55,Matija Marolt,Gaussian Mixture Models For Extraction Of Melodic Lines From Audio Recordings.,2004,https://doi.org/10.5281/zenodo.1416576,Matija Marolt+University of Ljubljana>SVN>education,"The presented study deals with extraction of melodic line(s) from polyphonic audio recordings. We base our work on the use of expectation maximization algorithm, which is employed in a two-step procedure that finds melodic lines in audio signals. In the first step, EM is used to find regions in the signal with strong and stable pitch (melodic fragments). In the second step, these fragments are grouped into clusters according to their properties (pitch, loudness...). The obtained clusters represent distinct melodic lines. Gaussian Mixture Models, trained with EM are used for clustering. The paper presents the entire process in more detail and gives some initial results.",SVN,education,Developed economies,"[4.501445, -11.612771]","[-0.9141827, -6.511269]","[13.246539, 6.1046386, -6.174176]","[3.351364, -2.0695374, -8.603989]","[10.325482, 9.906535]","[7.534947, 2.9960368]","[11.154859, 14.939622, -0.43943074]","[10.038597, 8.023958, 11.452176]"
48,Olivier Lartillot,A multi-parametric and redundancy-filtering approach to pattern identification.,2004,https://doi.org/10.5281/zenodo.1416426,Olivier Lartillot+University of Jyväskylä>FIN>education,"This paper presents the principles of a new approach aimed at automatically discovering motivic patterns in monodies. It is shown that, for the results to agree with the listener’s understanding, computer modelling needs to follow as closely as possible the strategies undertaken during the listening process. Motivic patterns, which may progressively follow different musical dimensions, are discovered through an adaptive incremental identification in a multi-dimensional parametric space. The combinatorial redundancy that would logically result from the model is carefully limited with the help of particular heuristics. In particular, a notion of specificity relation between pattern descriptions is defined, unifying suffix relation – between patterns – and inclusion relation – between the multi-parametric descriptions of patterns. This enables to discard redundant patterns, whose descriptions are less specific than other patterns and whose occurrences are included in the occurrences of the more specific patterns. Resulting analyzes come close to the structures actually perceived by the listener.",FIN,education,Developed economies,"[16.621181, 23.531792]","[0.6306897, 16.61519]","[0.62111187, -15.420631, 10.710632]","[-4.1436653, -8.100849, 6.3347454]","[11.546168, 7.441374]","[8.828967, 1.5249451]","[12.479595, 13.142306, -0.6361104]","[10.406277, 6.7502737, 12.482134]"
32,Jin Ha Lee;J. Stephen Downie,"Survey Of Music Information Needs, Uses, And Seeking Behaviours: Preliminary Findings.",2004,https://doi.org/10.5281/zenodo.1417637,Jin Ha Lee+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"User studies focusing upon real-life music information needs, uses and seeking behaviours are still very scarce in the music information retrieval (MIR) and music digital library (MDL) fields. We are conducting a multi-group survey in an attempt to acquire information that can help eradicate false assumptions in designing MIR systems. Our goal is to provide an empirical basis for MIR/MDL system development. In this paper, we present our preliminary findings and analyses based on the 427 user responses we have received to date. Two major themes have been uncovered thus far that could have a significant influence the future development of successful MIR/MDL systems. First, people display “public information-seeking” behaviours by making use of collective knowledge and/or opinions of others about music such as reviews, ratings, recommendations, etc. in their music information-seeking. Second, respondents expressed needs for contextual metadata in addition to traditional bibliographic metadata.",USA,education,Developed economies,"[-33.40891, 23.171707]","[30.040161, 32.24728]","[-18.87723, 15.334996, -8.421452]","[6.2464623, 7.950375, 18.101393]","[14.998824, 8.3737]","[12.320876, 0.85939723]","[14.965112, 15.039516, -1.7170038]","[12.746988, 4.4191027, 12.190022]"
33,Pierre Hanna;Nicolas Louis;Myriam Desainte-Catherine;Jenny Benois-Pineau,Audio Features for Noisy Sound Segmentation.,2004,https://doi.org/10.5281/zenodo.1415214,Pierre Hanna+SCRIME - LaBRI>FRA>facility|Université de Bordeaux 1>FRA>education;Nicolas Louis+SCRIME - LaBRI>FRA>facility|Université de Bordeaux 1>FRA>education;Myriam Desainte-Catherine+SCRIME - LaBRI>FRA>facility|Université de Bordeaux 1>FRA>education;Jenny Benois-Pineau+SCRIME - LaBRI>FRA>facility|Université de Bordeaux 1>FRA>education,"Automatic audio classification usually considers sounds as music, speech, silence or noise, but works about the noise class are rare. Audio features are generally specific to speech or music signals. In this paper, we present a new audio feature sets that lead to the definition of four classes: colored, pseudo-periodic, impulsive and sinusoids within noises. This classification relies on works about the perception of noises. This audio feature set is experimented for noisy sound segmentation. Noise-to-noise transitions are characterized by means of statistical decision model based on Bayesian framework. This statistical method has been trained and experimented both on synthetic and real audio corpus. Using proposed feature set increases the discriminant power of Bayesian decision approach compared to a usual feature set.",FRA,facility,Developed economies,"[-14.042035, -17.360527]","[16.156595, -17.379305]","[4.899888, -6.795595, -11.895977]","[13.871867, 3.0886838, -13.380399]","[11.495729, 8.747601]","[9.142368, 3.5547743]","[12.608234, 14.051977, 0.68305445]","[10.892701, 7.579757, 10.631797]"
69,Anna Pienimäki;Kjell Lemström,Clustering Symbolic Music Using Paradigmatic and Surface Level Analyses.,2004,https://doi.org/10.5281/zenodo.1417447,Anna Pienimäki+University of Helsinki>FIN>education;Kjell Lemström+University of Helsinki>FIN>education,"In this paper, we describe a novel automatic cluster analy- sis method for symbolic music. The method contains both a surface level and a paradigmatic level analysing block and works in two phases. In the first phase, each music document of a collection is analysed separately: They are first divided into phrases that are consequently fed on a harmonic analyser. The paradigmatic structure of a given music document is achieved comparing both the melodic and the harmonic similarities among its phrases. In the second phase, the collection of music documents is clus- tered on the ground of their paradigmatic structures and surface levels. Our experimental results show that the novel method finds some interesting, underlying similari- ties that cannot be found using only surface level analysis.",FIN,education,Developed economies,"[13.017553, 16.279667]","[-6.5170465, 20.378067]","[-5.4336853, -7.745957, 11.20206]","[-10.716319, -8.379493, 7.8192024]","[11.857317, 7.2589664]","[8.699831, 1.9829817]","[13.395571, 12.462821, -0.9834682]","[10.370867, 6.8621564, 12.241952]"
22,Jana Eggink;Guy J. Brown,Extracting Melody Lines From Complex Audio.,2004,https://doi.org/10.5281/zenodo.1418003,Jana Eggink+University of Sheffield>GBR>education;Guy J. Brown+University of Sheffield>GBR>education,"""We propose a system which extracts the melody line played by a solo instrument from complex audio. At every time frame multiple fundamental frequency (F0) hypotheses are generated, and later processing uses various knowledge sources to choose the most likely succession of F0s. Knowledge sources include an instrument recognition module and temporal knowledge about tone durations and interval transitions, which are integrated in a probabilistic search. The proposed system improved the number of frames with correct F0 estimates by 14% compared to a baseline system which simply uses the strongest F0 at every point in time. The number of spurious tones was reduced to nearly a third compared to the baseline system, resulting in significantly smoother melody lines.""",GBR,education,Developed economies,"[5.0896063, -12.145852]","[-14.58298, -5.5198073]","[13.760847, 4.7291346, -4.779464]","[4.320624, -3.9678054, -12.955949]","[10.204017, 9.953676]","[6.51796, 2.6092892]","[11.068473, 14.908615, -0.45031032]","[9.093973, 7.743464, 11.00516]"
12,Michael A. Casey;Tim Crawford,Automatic Location And Measurement Of Score-based Gestures In Audio Recordings.,2004,https://doi.org/10.5281/zenodo.1415184,Michael Casey+Goldsmiths College University of London>GBR>education;Tim Crawford+Goldsmiths College University of London>GBR>education,"This paper reports on our first experiments in using the feature extraction tools of the MPEG-7 international standard for multimedia content description on a novel problem, the automatic identification and analysis of score-based performance features in audio recordings of music. Our test material consists of recordings of two pieces of 17th- and 18th-century lute music in which our aim is to recognise and isolate performance features such as trills and chord-spreadings. Using the audio tools from the MPEG-7 standard facilitates interoperability and allows us to share both score and audio metadata. As well as using low-level audio MIR techniques within this MPEG-7 context, the work has potential importance as an ‘ornamentation filter’ for MIR systems. It may also form a useful component in methods for instrumental performer identification.",GBR,education,Developed economies,"[21.820208, -17.354757]","[13.967883, 32.166447]","[-1.6443982, -13.693981, -16.226332]","[-1.3107501, 8.440383, 13.250969]","[10.769698, 6.031451]","[10.819599, 1.0653194]","[11.705727, 12.463753, -1.7868801]","[11.108156, 5.3254957, 11.320005]"
20,Michael Droettboom;Ichiro Fujinaga,Micro-level groundtruthing environment for OMR.,2004,https://doi.org/10.5281/zenodo.1417217,Michael Droettboom+The Johns Hopkins University>USA>education|Digital Knowledge Center>USA>facility;Ichiro Fujinaga+McGill University>CAN>education|CIRMMT>CAN>facility,"A simple framework for evaluating OMR at the symbol level is presented. While a true evaluation of an OMR system requires a high-level analysis, the automation of which is a largely unsolved problem, many high-level errors are correlated to these more tractably-analyzed lower-level errors.",USA,education,Developed economies,"[33.37348, 32.49137]","[-18.544386, 37.153744]","[5.7456627, -21.049362, 24.571888]","[-11.06224, -19.397064, 5.776326]","[9.709931, 6.1781845]","[6.644713, -0.57213235]","[12.664136, 11.269947, -1.5220497]","[7.956323, 4.1656637, 10.601988]"
10,Paul Brossier;Juan Pablo Bello;Mark D. Plumbley,Fast labelling of notes in music signals.,2004,https://doi.org/10.5281/zenodo.1416132,"Paul M. Brossier+Queen Mary College, University of London>GBR>education;Juan P. Bello+Queen Mary College, University of London>GBR>education;Mark D. Plumbley+Queen Mary College, University of London>GBR>education","We present a new system for the estimation of note attributes from a live monophonic music source, within a short time delay and without any previous knowledge of the signal. The labelling is based on the temporal segmentation and the successive estimation of the fundamental frequency of the current note object. The setup, implemented around a small C library, is directed at the robust note segmentation of a variety of audio signals. A system for evaluation of performances is also presented. The further extension to polyphonic signals is considered, as well as design concerns such as portability and integration in other software environments.",GBR,education,Developed economies,"[1.8330224, -18.127659]","[-18.67361, -7.5173173]","[3.515466, -19.198565, 5.499203]","[-4.0348625, 0.8412666, -9.407803]","[10.180007, 7.618313]","[6.1640654, 2.522387]","[11.943075, 12.959547, -0.1246827]","[8.476849, 7.4260106, 10.913733]"
9,Stephan Baumann 0001;Tim Pohle;Shankar Vembu,Towards a Socio-cultural Compatibility of MIR Systems.,2004,https://doi.org/10.5281/zenodo.1417733,Stephan Baumann+German Research Center for Artificial Intelligence>DEU>facility|Technical University of Hamburg>DEU>education;Tim Pohle+German Research Center for Artificial Intelligence>DEU>facility|Technical University of Hamburg>DEU>education;Vembu Shankar+German Research Center for Artificial Intelligence>DEU>facility|Technical University of Hamburg>DEU>education,"Future MIR systems will be of great use and pleasure for potential users. If researchers have a clear picture about their “customers” in mind they can aim at building and evaluating their systems exactly inside the different socio-cultural environments of such music listeners. Since music is in most cases embedded into a socio-cultural process we propose especially to evaluate MIR applications outside the lab during daily activities. For this purpose we designed a mobile music recommendation system relying on a trimodal music similarity metric, which allows for subjective on-the-fly adjustments of recommendations. It offers online access to large-scale metadata repositories as well as an audio database containing 1000 songs. We did first small-scale evaluations of this approach and came to interesting results regarding the perception of song similarity concerning the relations between sound, cultural issues and lyrics. Our paper will also give insights to the three different underlying approaches for song similarity computation (sound, cultural issues, lyrics), focusing in detail on a novel clustering of album reviews as found at online music retailers. Keywords: Socio-cultural issues in MIR, multimodal song similarity, ecological validation.",DEU,facility,Developed economies,"[-8.796214, 58.274204]","[34.577156, 12.55166]","[-35.342014, 1.5722905, -6.508093]","[8.474811, 6.105427, 16.103525]","[13.609106, 4.7399063]","[12.096301, 2.0019462]","[15.064301, 11.105419, -1.507472]","[13.057339, 5.2371554, 12.273601]"
8,Roberto Basili 0001;Alfredo Serafini;Armando Stellato,Classification of musical genre: a machine learning approach.,2004,https://doi.org/10.5281/zenodo.1416754,Roberto Basili+University of Rome Tor Vergata>ITA>education;Alfredo Serafini+University of Rome Tor Vergata>ITA>education;Armando Stellato+University of Rome Tor Vergata>ITA>education,"In this paper, we investigate the impact of machine learning algorithms in the development of automatic music classification models aiming to capture genres distinctions. The study of genres as bodies of musical items aggregated according to subjective and local criteria requires corresponding inductive models of such a notion. This process can be thus modeled as an example-driven learning task. We investigated the impact of different musical features on the inductive accuracy by first creating a medium-sized collection of examples for widely recognized genres and then evaluating the performances of different learning algorithms. In this work, features are derived from the MIDI transcriptions of the song collection.",ITA,education,Developed economies,"[-28.768764, -13.847536]","[17.967003, -6.789955]","[-16.347036, 3.1513436, 15.82355]","[11.559845, 9.151174, -3.7563105]","[12.89331, 10.841835]","[9.64157, 3.377807]","[13.902353, 14.24966, 1.3737797]","[11.359085, 7.0736494, 10.8099575]"
7,David Bainbridge 0001;Sally Jo Cunningham;J. Stephen Downie,Visual Collaging Of Music In A Digital Library.,2004,https://doi.org/10.5281/zenodo.1415832,David Bainbridge+University of Waikato>NZL>education;Sally Jo Cunningham+University of Waikato>NZL>education;J. Stephen Downie+University of Illinois>USA>education,"This article explores the role visual browsing can play within a digital music library. The context to the work is provided through a review of related techniques drawn from the fields of digital libraries and human computer interaction. Implemented within the open source digital library toolkit Greenstone, a prototype system is described that combines images located through textual metadata with a visualisation technique known as collaging to provide a leisurely, undirected interaction with a music collection. Emphasis in the article is given to the augmentations of the basic technique to work in the musical domain.",NZL,education,Developed economies,"[-20.067356, 32.438423]","[24.255548, 27.641684]","[-20.52333, 7.438728, -18.385311]","[6.4159017, -4.7692847, 23.063322]","[14.400374, 7.267865]","[10.973176, 0.86417687]","[14.472942, 14.096562, -2.4894142]","[11.608321, 4.955616, 12.670669]"
6,David Bainbridge 0001;Sally Jo Cunningham;J. Stephen Downie,GREENSTONE as a Music Digital Library Toolkit.,2004,https://doi.org/10.5281/zenodo.1417573,David Bainbridge+University of Waikato>NZL>education;Sally Jo Cunningham+University of Waikato>NZL>education;J. Stephen Downie+University of Illinois>USA>education,"""""",NZL,education,Developed economies,"[-20.541958, 35.693047]","[51.16438, 43.63698]","[-22.37665, 4.103529, -15.154347]","[4.475263, 4.004884, 28.983242]","[14.343283, 7.5134435]","[-9.058836, 9.949154]","[14.572382, 14.279136, -2.3479059]","[-11.706615, 1.759435, 1.0280142]"
5,Jean-Julien Aucouturier;François Pachet;Peter Hanappe,From Sound Sampling To Song Sampling.,2004,https://doi.org/10.5281/zenodo.1416028,Jean-Julien Aucouturier+SONY CSL Paris>FRA>company;Francois Pachet+SONY CSL Paris>FRA>company;Peter Hanappe+SONY CSL Paris>FRA>company,"This paper proposes to use the techniques of Music Information Retrieval in the context of Music Interaction. We describe a system, the SongSampler, inspired by the technology of audio sampling, which automatically samples a song to produce an instrument (typically using a MIDI keyboard) that plays sounds found in the original audio file. Playing with such an instrument creates an original situation in which listeners play their own music with the sounds of their favourite tunes, in a constant interaction with a music database. The paper describes the main technical issues at stake concerning the integration of music information retrieval in an interactive instrument, and reports on preliminary experiments.",FRA,company,Developed economies,"[24.233038, -11.820263]","[13.6998415, 26.366762]","[18.889202, 1.7063639, 13.152265]","[0.5187469, -3.880013, 14.709219]","[9.742562, 7.719137]","[10.535227, 0.91795623]","[11.834032, 12.665714, -0.13003357]","[11.103145, 5.231823, 12.185683]"
4,Jean-Julien Aucouturier;François Pachet,Tools and Architecture for the Evaluation of Similarity Measures : Case Study of Timbre Similarity.,2004,https://doi.org/10.5281/zenodo.1416562,Jean-Julien Aucouturier+SONY CSL Paris>FRA>company|SONY CSL Paris>FRA>company;Francois Pachet+SONY CSL Paris>FRA>company|SONY CSL Paris>FRA>company,"The systematic testing of the very many parameters and algorithmic variants involved in the design of high-level music descriptors at large, and similarity measure in particular, is a daunting task, which requires the building of a general architecture which is nearly as complex as a full-fledged Music Browsing system. In this paper, we report on experiments done in an attempt to improve the performance of the music similarity measure described in [2], using the Cuidado Music Browser ([8]). We do not principally report on the actual results of the evaluation, but rather on the methodology and the various tools that were built to support such a task. We show that many non-technical browsing features are useful at various stages of the evaluation process, and in turn that some of the tools developed for the expert user can be reinjected into the Music Browser, and benefit the non-technical user.",FRA,company,Developed economies,"[-3.352917, 16.152761]","[31.804878, 18.797636]","[-8.750245, 4.8761244, -0.18891154]","[11.750834, 0.5194166, 17.91856]","[13.03199, 9.348134]","[11.2175455, 1.3487892]","[13.392296, 15.182247, -0.6137831]","[12.263507, 5.3175287, 12.732276]"
3,Miguel A. Alonso;Gaël Richard;Bertrand David,Tempo And Beat Estimation Of Musical Signals.,2004,https://doi.org/10.5281/zenodo.1415784,Miguel Alonso+ENST-GET>FRA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Bertrand David+ENST-GET>FRA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Gaël Richard+ENST-GET>FRA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Tempo estimation is fundamental in automatic music processing and in many multimedia applications. This paper presents an automatic tempo tracking system that processes audio recordings and determines the beats per minute and temporal beat location. The concept of spectral energy flux is deﬁned and leads to an efﬁcient note onset detector. The algorithm involves three stages: a front-end analysis that efﬁciently extracts onsets, a periodicity detection block and the temporal estimation of beat locations. The performance of the proposed method is evaluated using a large database of 489 excerpts from several musical genres. The global recognition rate is 89.7 %. Results are discussed and compared to other tempo estimation systems.",FRA,education,Developed economies,"[39.160072, -25.881027]","[-25.539043, -4.194933]","[0.48919365, -27.227924, -0.95411247]","[-3.9273314, 8.800361, -9.434363]","[11.412431, 4.5061145]","[5.475567, 2.0628908]","[10.848186, 13.303302, -2.7333262]","[7.6563644, 7.1938057, 11.066305]"
2,Philippe Aigrain,Whose future is it?,2004,https://doi.org/10.5281/zenodo.1416404,Philippe Aigrain+Sopinspace>ESP>company,"""""",ESP,company,Developed economies,"[-10.552965, 46.116627]","[50.274815, 42.322147]","[-29.477436, -10.552369, 1.8852497]","[7.397981, 5.3288116, 28.545868]","[13.532858, 5.9392033]","[-9.059143, 9.949457]","[14.860904, 12.128166, -1.6634362]","[-11.709571, 1.7622988, 1.0308529]"
1,Norman H. Adams;Mark A. Bartsch;Jonah Shifrin;Gregory H. Wakefield,Time Series Alignment for Music Information Retrieval.,2004,https://doi.org/10.5281/zenodo.1415694,Norman H. Adams+University of Michigan>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Mark A. Bartsch+University of Michigan>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Jonah B. Shifrin+University of Michigan>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Gregory H. Wakefield+University of Michigan>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Time series representations are common in MIR applications such as query-by-humming, where a sung query might be represented by a series of ‘notes’ for database retrieval. While such a transcription into a sequence of (pitch, duration) pairs is convenient and musically intuitive, there is no evidence that it is an optimal representation. The present work explores three time series representations for sung queries: a sequence of notes, a ‘smooth’ pitch contour, and a novel sequence of pitch histograms. Dynamic alignment procedures are described for the three representations. Multiple continuity constraints are explored and a modified dynamic alignment procedure is described for the histogram representation. We measure the performance of the three representations using a collection of naturally sung queries applied to a target database of varying size. The results show that the note representation lends itself to rapid retrieval whereas the contour representation lends itself to robust performance. The histogram representation yields performance nearly as robust as the contour representation, but with computational complexity similar to the note representation.",USA,education,Developed economies,"[-9.684724, 21.415512]","[8.279552, 9.537034]","[0.16091844, 7.1339536, -13.216241]","[4.2411017, -14.555394, 10.505509]","[13.267313, 7.7618203]","[8.714313, 0.59971696]","[13.291297, 14.525585, -1.8803619]","[10.288906, 6.0223465, 13.102383]"
0,Samer M. Abdallah;Mark D. Plumbley,Polyphonic transcription by non-negative sparse coding of power spectra.,2004,https://doi.org/10.5281/zenodo.1415072,"Samer A. Abdallah+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility;Mark D. Plumbley+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility","We present a system for adaptive spectral basis decomposition that learns to identify independent spectral features given a sequence of short-term Fourier spectra. When applied to recordings of polyphonic piano music, the individual notes are identified as salient features, and hence each short-term spectrum is decomposed into a sum of note spectra; the resulting encoding can be used as a basis for polyphonic transcription. The system is based on a probabilistic model equivalent to a form of noisy independent component analysis (ICA) or sparse coding with non-negativity constraints. We introduce a novel modification to this model that recognises that a short-term Fourier spectrum can be thought of as a noisy realisation of the power spectral density of an underlying Gaussian process, where the noise is essentially multiplicative and non-Gaussian. Results are presented for an analysis of a live recording of polyphonic piano music.",GBR,education,Developed economies,"[30.821934, -10.632063]","[-45.4988, -22.494612]","[8.427639, -5.316152, 8.543722]","[-2.4583027, -14.117403, -30.794281]","[9.337442, 7.8597403]","[6.457808, 4.6619935]","[11.767158, 12.283665, 0.2764942]","[9.744349, 8.632834, 10.043943]"
13,Òscar Celma,Architecture for an MPEG-7 Web Browser.,2004,https://doi.org/10.5281/zenodo.1417229,Oscar Celma+Universitat Pompeu Fabra>ESP>education,The MPEG-7 standard provides description mechanisms and taxonomy management for multimedia documents. There are several approaches to design a multimedia database system using MPEG-7 descriptors. We discuss two of them: relational databases and native XML databases. We have implemented a search and retrieval application for MPEG-7 descriptions based on the latter.,ESP,education,Developed economies,"[-18.434961, 43.008175]","[17.09333, 26.769598]","[-22.13791, 0.54389393, -24.65634]","[-1.244054, -8.364419, 20.037674]","[13.806658, 7.1624]","[10.721933, 0.78841436]","[14.100725, 13.845503, -2.2295034]","[11.669328, 5.1648684, 12.455758]"
21,Tuomas Eerola;Petri Toiviainen,MIR In Matlab: The MIDI Toolbox.,2004,https://doi.org/10.5281/zenodo.1416234,Tuomas Eerola+University of Jyväskylä>FIN>education;Petri Toiviainen+University of Jyväskylä>FIN>education,"""The MIDI Toolbox is a compilation of functions for analyzing and visualizing MIDI files in the Matlab computing environment. In this article, the basic issues of the Toolbox are summarized and demonstrated with examples ranging from melodic contour, similarity, key-finding, meter-finding to segmentation. The Toolbox is based on symbolic musical data but signal processing methods are applied to cover such aspects of musical behaviour as geometric representations and short-term memory. Besides simple manipulation and filtering functions, the toolbox contains cognitively inspired analytic techniques that are suitable for context-dependent musical analysis, a prerequisite for many music information retrieval applications.""",FIN,education,Developed economies,"[-13.876352, 56.73497]","[2.1629622, 30.012516]","[-35.06317, 3.945357, -1.4374268]","[-9.945868, -4.251431, 10.04181]","[13.16498, 5.244392]","[8.736092, 1.3945762]","[14.482784, 11.452524, -1.2524475]","[10.092825, 6.0165186, 11.702624]"
104,Mark Zadel;Ichiro Fujinaga,Web Services for Music Information Retrieval.,2004,https://doi.org/10.5281/zenodo.1417069,Mark Zadel+McGill University>CAN>education|Unknown>Unknown>Unknown;Ichiro Fujinaga+McGill University>CAN>education|Unknown>Unknown>Unknown,"In the emerging world of networked and distributed digital libraries, the Web services framework will be a key to facilitating simple inter-application communication between them. Yet, despite the popularity of Web services in the business sector and their seemingly obvious applicability to the digital library domain, and to MIR in particular, the adoption of these new protocols has not been widespread. To demonstrate the tremendous potential of Web services for MIR, this paper presents an application using the Google and Amazon.com databases to generate clusters of related musical artists based on cultural metadata. The use of cultural metadata to determine artist relatedness is valuable and interesting because it captures emergent popular opinion about music. Starting from an initial seed artist, Amazon Listmania! lists are traversed to find potentially related artists. Google is used to determine which of these candidates are in fact related by assessing the co-occurrence of the two artists’ names on Internet web pages. A list of artists related to the seed is returned once a given number of artists is found. The positive results generated by the system illustrate the use of Web services for exploiting the vast amount of untapped data that are available today and highlight their importance for the future, when even more musical data will become available.",CAN,education,Developed economies,"[-18.918472, 22.237684]","[21.070553, 36.078957]","[-13.438287, 4.6067414, -13.252307]","[-1.4773288, 1.0244796, 22.945824]","[14.293098, 7.69147]","[11.147785, 0.20744558]","[14.33683, 14.525241, -2.1571593]","[12.20635, 5.0496535, 12.100532]"
19,Shyamala Doraisamy;Stefan M. Rüger,A Polyphonic Music Retrieval System Using N-Grams.,2004,https://doi.org/10.5281/zenodo.1417961,Shyamala Doraisamy+University Putra Malaysia>MYS>education;Stefan Rüger+Imperial College London>GBR>education,"This paper describes the development of a polyphonic music retrieval system with the n-gram approach. Musical n-grams are constructed from polyphonic musical performances in MIDI using the pitch and rhythm dimensions of music. These are encoded using text characters enabling the musical words generated to be indexed with existing text search engines. The Lemur Toolkit was adapted for the development of a demonstrator system on a collection of around 10,000 polyphonic MIDI performances. The indexing, search and retrieval with musical n-grams and this toolkit have been extensively evaluated through a series of experimental work over the past three years, published elsewhere. We discuss how the system works internally and describe our proposal for enhancements to Lemur towards the indexing of ‘overlaying’ as opposed to indexing a ‘bag of terms’. This includes enhancements to the parser for a ‘polyphonic musical word indexer’ to incorporate within document position information when indexing adjacent and concurrent musical words. For retrieval of these ‘overlaying’ musical words, a new proximity-based operator and a ranking function is proposed.",MYS,education,Developing economies,"[-11.390362, 19.012531]","[13.152251, 17.313574]","[-1.5127414, 2.2609816, -11.635721]","[6.0615683, -7.7947245, 9.139216]","[13.126904, 7.9869127]","[9.308089, 0.79452354]","[13.053222, 14.45588, -1.7667203]","[10.783327, 6.050433, 13.015775]"
18,Simon Dixon;Fabien Gouyon;Gerhard Widmer,Towards Characterisation of Music via Rhythmic Patterns.,2004,https://doi.org/10.5281/zenodo.1416220,Simon Dixon+Austrian Research Institute for AI>AUT>facility;Fabien Gouyon+Universitat Pompeu Fabra>ESP>education;Gerhard Widmer+Medical University of Vienna>AUT>education,"A central problem in music information retrieval is finding suitable representations which enable efficient and accurate computation of musical similarity and identity. Low level audio features are ideal for calculating identity, but are of limited use for similarity measures, as many aspects of music can only be captured by considering high level features. We present a new method of characterising music by typical bar-length rhythmic patterns which are automatically extracted from the audio signal, and demonstrate the usefulness of this representation by its application in a genre classification task. Recent work has shown the importance of tempo and periodicity features for genre recognition, and we extend this research by employing the extracted temporal patterns as features. Standard classification algorithms are utilised to discriminate 8 classes of Standard and Latin ballroom dance music (698 pieces). Although pattern extraction is error-prone, and patterns are not always unique to a genre, classification by rhythmic pattern alone achieves up to 50% correctness (baseline 16%), and by combining with other features, a classification rate of 96% is obtained.",AUT,facility,Developed economies,"[7.795344, 9.242257]","[15.122665, -9.033011]","[-5.3362374, -19.02403, 1.6619165]","[6.5446806, 7.84562, -6.4531507]","[11.886143, 5.754657]","[9.288482, 3.112077]","[11.55829, 13.986918, -1.9680868]","[11.052524, 7.341637, 10.933579]"
17,Matthew E. P. Davies;Mark D. Plumbley,Causal Tempo Tracking of Audio.,2004,https://doi.org/10.5281/zenodo.1417873,Matthew E. P. Davies+Queen Mary University of London>GBR>education;Mark D. Plumbley+Queen Mary University of London>GBR>education,"We introduce a causal approach to tempo tracking for musical audio signals. Our system is designed towards an eventual real-time implementation; requiring minimal high-level knowledge of the musical audio. The tempo tracking system is divided into two sections: an onset analysis stage, used to derive a rhythmically meaningful representation from the input audio, followed by a beat matching algorithm using auto- and cross-correlative methods to generate short term predictions of future beats in the audio. The algorithm is evaluated over a range of musical styles by comparing the predicted output to beats tapped by a musician. An investigation is also presented into three rhythmically complex beat tracking problems, where the tempo is not constant. Preliminary results demonstrate good accuracy for this type of system.",GBR,education,Developed economies,"[39.5044, -28.026066]","[-28.139088, -3.1244755]","[1.349102, -30.730627, -3.518162]","[-7.086681, 8.03301, -8.649324]","[11.348242, 4.408448]","[5.3597593, 1.7449663]","[10.793694, 13.341424, -2.6954284]","[7.5735435, 6.7334805, 11.040569]"
16,Laurent Daudet;Gaël Richard;Pierre Leveau,Methodology and Tools for the evaluation of automatic onset detection algorithms in music.,2004,https://doi.org/10.5281/zenodo.1417247,Pierre Leveau+Laboratoire d’Acoustique Musicale>FRA>facility;Laurent Daudet+Laboratoire d’Acoustique Musicale>FRA>facility;Gaël Richard+GET - ENST (Télécom Paris)>FRA>education,"This paper addresses the problem of the performance evaluation of algorithms for the automatic detection of note onsets in music signals. Our experiments show that creating a database of reference files with reliable human-annotated onset times is a complex task, since its subjective part cannot be neglected. This work provides a methodology to construct such a database. With the use of a carefully designed software tool, called SOL (Sound Onset Labellizer), we can obtain a set of reference onset times that are cross-validated amongst different expert listeners. We show that the mean error of annotated times across test subjects is very much signal-dependent. This value can be used, when evaluating automatic labelling, as an indication of the relevant tolerance window. The SOL annotation software is to be released freely for research purposes. Our test library, 17 short sequences containing about 750 onsets, comes from copyright-free music or from the public RWC database. The corresponding validated onset labels are also freely distributed, and are intended to form the starting point for the definition of a reliable benchmark.",FRA,facility,Developed economies,"[29.672913, -27.37214]","[-20.523254, -8.030344]","[6.4196053, -21.344982, -7.1155887]","[-2.8136468, 3.1186872, -11.25672]","[10.31291, 5.0148106]","[5.9384522, 2.462338]","[10.373431, 13.27602, -1.516585]","[8.244185, 7.2968736, 10.837477]"
15,Roger B. Dannenberg;Ning Hu,Understanding Search Performance in Query-by-Humming Systems.,2004,https://doi.org/10.5281/zenodo.1416900,Roger B. Dannenberg+Carnegie Mellon University>USA>education;Ning Hu+Carnegie Mellon University>USA>education,"Previous work in Query-by-Humming systems has left open many questions. Although a variety of techniques have been explored, there has been relatively little work to compare them under controlled conditions, especially with “real” audio queries from human subjects. Previous work comparing note-interval matching, melodic contour matching, and HMM-based matching is extended with comparisons to the Phillips CubyHum algorithm and various n-gram search algorithms. We also explore the sensitivity of note-interval dynamic programming searches to different parameters and consider two-stage searches combining a fast n-gram search with a more precise but slower dynamic programming algorithm.",USA,education,Developed economies,"[-1.5709789, 36.98832]","[9.593249, 11.827586]","[-10.967466, -7.89252, -24.237976]","[7.396921, -14.370741, 11.870899]","[14.73459, 6.208265]","[8.722151, 0.57033074]","[13.327175, 15.216396, -2.843185]","[10.283338, 6.0610447, 13.194954]"
14,Fernando William Cruz;Edilson Ferneda;Márcio da Costa P. Brandão;Evandro de Barros Costa;Hyggo Oliveira de Almeida;Murilo Bastos da Cunha;Rafael de Sousa;João Denicol;Carlos da Silva,A Brazilian Popular Music Oriented Digital Library For Musical Harmony E-Learning.,2004,https://doi.org/10.5281/zenodo.1417042,Fernando W. Cruz+Catholic University of Brasília>BRA>education;Edilson Ferneda+Catholic University of Brasília>BRA>education;Márcio Brandão+University of Brasília>BRA>education;Evandro de B. Costa+Federal University of Alagoas>BRA>education;Hyggo O. de Almeida+Federal University of Campina Grande>BRA>education;Murilo B. da Cunha+University of Brasília>BRA>education;Rafael T. de Sousa Jr.+University of Brasília>BRA>education;João Ricardo E. Denicol+Catholic University of Brasília>BRA>education;Carlos Alan P. da Silva+Federal University of Campina Grande>BRA>education,"This poster presents a digital library proposal conceived for people interested in acquiring knowledge about Brazilian popular music harmony, particularly in Choro. This Brazilian musical style is a complex popular music form based on improvisation, although it contains classical music elements such as the counterpoint. We are proposing two ways of accessing the music virtual library content: a guided navigation mode, in which users interact with a cooperative Web-based learning system; and a free navigation mode, in which users can make their own queries, both through browsers or client applications.",BRA,education,Developing economies,"[-21.53423, 34.085457]","[24.580719, 27.024225]","[-23.691792, 7.5444574, -15.283868]","[7.560429, -4.7529955, 21.994873]","[14.577557, 7.618607]","[11.1992, 0.9803335]","[14.588832, 14.204771, -2.3583167]","[11.8127, 5.013066, 12.752452]"
103,Takuya Yoshioka;Tetsuro Kitahara;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Automatic Chord Transcription with Concurrent Recognition of Chord Symbols and Boundaries.,2004,https://doi.org/10.5281/zenodo.1415068,Takuya Yoshioka+Kyoto University>JPN>education;Tetsuro Kitahara+Kyoto University>JPN>education;Kazunori Komatani+Kyoto University>JPN>education;Tetsuya Ogata+Kyoto University>JPN>education;Hiroshi G. Okuno+Kyoto University>JPN>education,"This paper describes a method that recognizes musical chords from real-world audio signals in compact-disc recordings. The automatic recognition of musical chords is necessary for music information retrieval (MIR) systems, since the chord sequences of musical pieces capture the characteristics of their accompaniments. None of the previous methods can accurately recognize musical chords from complex audio signals that contain vocal and drum sounds. The main problem is that the chord-boundary-detection and chord-symbol-identification processes are inseparable because of their mutual dependency. In order to solve this mutual dependency problem, our method generates hypotheses about tuples of chord symbols and chord boundaries, and outputs the most plausible one as the recognition result. The certainty of a hypothesis is evaluated based on three cues: acoustic features, chord progression patterns, and bass sounds. Experimental results show that our method successfully recognized chords in seven popular music songs; the average accuracy of the results was around 77%.",JPN,education,Developed economies,"[50.97375, -4.7703843]","[-29.268087, 17.94377]","[22.87797, -11.994333, 14.933048]","[-26.204487, -8.912529, 2.0274758]","[7.0592747, 8.710271]","[6.22794, 3.6327622]","[12.016276, 10.656157, 1.801148]","[9.773551, 8.820994, 12.15099]"
11,Pedro Cano;Markus Koppenberger,The emergence of complex network patterns in music networks.,2004,https://doi.org/10.5281/zenodo.1417663,"Pedro Cano+Institut de l’Audiovisual, Universitat Pompeu Fabra>ESP>education;Markus Koppenberger+Institut de l’Audiovisual, Universitat Pompeu Fabra>ESP>education","Viewing biological, social or technological systems as networks formed by nodes and connections between them can help better understand them. We study the topology of several music networks, namely citation in allmusic.com and co-occurrence of artists in playlists. The analysis uncovers the emergence of complex network phenomena in music information networks built considering artists as nodes and its relations as links. The properties provide some hints on searchability and possible optimizations in the design of music recommendation systems. It may also provide a deeper understanding on the similarity measures that can be derived from existing music knowledge sources.",ESP,education,Developed economies,"[-38.949833, 11.729677]","[43.45541, 9.174683]","[-25.745275, 8.913313, 1.2856913]","[19.46457, 3.806878, 10.558996]","[14.57842, 9.474884]","[12.12007, 2.544945]","[14.954467, 14.678845, -0.68601763]","[13.269168, 5.6152887, 11.952737]"
6,Cory McKay;Rebecca Fiebrink;Daniel McEnnis;Beinan Li;Ichiro Fujinaga,ACE: A Framework for Optimizing Music Classification.,2005,https://doi.org/10.5281/zenodo.1415720,Cory McKay+McGill University>CAN>education;Rebecca Fiebrink+McGill University>CAN>education;Daniel McEnnis+McGill University>CAN>education;Beinan Li+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper presents ACE (Autonomous Classification Engine), a framework for using and optimizing classifiers. Given a set of feature vectors, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality reduction techniques in order to arrive at a good configuration for the problem at hand. In addition to evaluating classification methodologies in terms of success rates, functionality is also being incorporated into ACE allowing users to specify constraints on training and classification times as well as on the amount of time that ACE has to arrive at a solution. ACE is designed to facilitate classification for those new to pattern recognition as well as provide flexibility for those with more experience. ACE is packaged with audio and MIDI feature extraction software, although it can certainly be used with existing feature extractors. This paper includes a discussion of ways in which existing general-purpose classification software can be adapted to meet the needs of music researchers and shows how these ideas have been implemented in ACE. A standardized XML format for communicating features and other information to classifiers is proposed. A special emphasis is placed on the potential of classifier ensembles, which have remained largely untapped by the MIR community to date. A brief theoretical discussion of ensemble classification is presented in order to promote this powerful approach.",CAN,education,Developed economies,"[-22.224525, -11.1970625]","[16.434925, -5.2071323]","[-13.557576, -1.9291602, 11.984595]","[10.587802, 6.988276, -5.3353596]","[12.567896, 10.379874]","[9.407759, 3.2164578]","[13.58099, 13.889275, 0.92865413]","[11.039005, 6.9010477, 10.666033]"
84,Phillip B. Kirlin;Paul E. Utgoff,VOISE: Learning to Segregate Voices in Explicit and Implicit Polyphony.,2005,https://doi.org/10.5281/zenodo.1417225,Phillip B. Kirlin+University of Massachusetts Amherst>USA>education;Paul E. Utgoff+University of Massachusetts Amherst>USA>education,"Finding multiple occurrences of themes and patterns in music can be hampered due to polyphonic textures. This is caused by the complexity of music that weaves multiple independent lines of music together. We present and demonstrate a system, VoiSe, that is capable of isolating individual voices in both explicit and implicit polyphonic music. VoiSe is designed to work on a symbolic representation of a music score, and consists of two components: a same-voice predicate implemented as a learned decision tree, and a hard-coded voice numbering algorithm.",USA,education,Developed economies,"[3.1254785, -31.264301]","[-8.129066, 21.350462]","[21.407495, 12.598552, -1.3618768]","[-8.83765, -10.955523, 5.247544]","[9.66916, 8.086866]","[8.925531, 2.0495617]","[11.3091755, 14.126585, 0.33859068]","[10.434671, 6.256997, 11.603838]"
85,Tetsuro Kitahara;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,"Instrument Identification in Polyphonic Music: Feature Weighting with Mixed Sounds, Pitch-Dependent Timbre Modeling, and Use of Musical Context.",2005,https://doi.org/10.5281/zenodo.1415724,Tetsuro Kitahara+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility|Kyoto University>JPN>education;Kazunori Komatani+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Tetsuya Ogata+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroshi G. Okuno+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper addresses the problem of identifying musical instruments in polyphonic music. Musical instrument identification (MII) is an important task in music information retrieval because MII results make it possible to automatically retrieve certain types of music (e.g., piano sonata, string quartet). Only a few studies, however, have dealt with MII in polyphonic music. In MII in polyphonic music, there are three issues: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. For the first issue, templates of feature vectors representing timbres are extracted from not only isolated sounds but also sound mixtures. Because some features are not robust in the mixtures, features are weighted according to their robustness by using linear discriminant analysis. For the second issue, we use an F0-dependent multivariate normal distribution, which approximates the pitch dependency as a function of fundamental frequency. For the third issue, when the instrument of each note is identified, the a priori probability of the note is calculated from the a posteriori probabilities of temporally neighboring notes. Experimental results showed that recognition rates were improved from 60.8% to 85.8% for trio music and from 65.5% to 91.1% for duo music.",JPN,education,Developed economies,"[7.3752127, -21.795376]","[8.8453045, -8.40521]","[17.530018, -5.560007, -4.2486176]","[9.553404, 2.1201248, -8.525121]","[8.812492, 7.2943573]","[8.639387, 3.588062]","[11.07956, 12.614467, 0.46584406]","[10.3412075, 7.7809086, 10.405798]"
86,Peter Knees;Markus Schedl;Gerhard Widmer,Multiple Lyrics Alignment: Automatic Retrieval of Song Lyrics.,2005,https://doi.org/10.5281/zenodo.1415140,Peter Knees+Johannes Kepler University Linz>AUT>education;Markus Schedl+Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"We present an approach to automatically retrieve and extract lyrics of arbitrary songs from the Internet. It is intended to provide easy and convenient access to lyrics for users, as well as a basis for further research based on lyrics, e.g. semantic analysis. Due to the fact that many lyrics found on the web suffer from individual errors like typos, we make use of multiple versions from different sources to eliminate mistakes. This is accomplished by Multiple Sequence Alignment. The different sites are aligned and examined for matching sequences of words, finding those parts on the pages that are likely to contain the lyrics. This provides a means to find the most probable version of lyrics, i.e. a version with highest consensus among different sources.",AUT,education,Developed economies,"[-29.001211, -33.297714]","[34.227646, -11.912052]","[8.40975, 19.930496, -3.2612936]","[15.508374, -10.006018, 15.059857]","[11.398857, 11.746777]","[10.693157, 2.9128578]","[12.339438, 15.881576, 1.0920652]","[12.223594, 6.4640656, 11.628547]"
87,Catherine Lai;Beinan Li;Ichiro Fujinaga,Preservation Digitization of David Edelberg's Handel LP Collection: A Pilot Project.,2005,https://doi.org/10.5281/zenodo.1416616,Catherine Lai+McGill University>CAN>education;Beinan Li+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,This paper describes the digitization process for building an online collection of LPs and the procedure for creating the ground-truth data essential for developing an automated metadata and content capturing system.,CAN,education,Developed economies,"[3.0219128, 28.888445]","[19.773521, 34.202175]","[-7.957051, 0.9379248, -27.590315]","[-4.9563265, -3.9702675, 21.210182]","[12.165341, 6.9938803]","[10.7693, 0.14896077]","[12.513092, 12.145369, -1.3938096]","[11.476789, 4.7120314, 12.171074]"
91,Dominik Lübbers,SoniXplorer: Combining Visualization and Auralization for Content-Based Exploration of Music Collections.,2005,https://doi.org/10.5281/zenodo.1418021,Dominik Lübbers+RWTH Aachen University>DEU>education,"Music can be described best by music. However, current research in the design of user interfaces for the exploration of music collections has mainly focused on visualization aspects ignoring possible benefits from spatialized music playback. We describe our first development steps towards two novel user-interface designs: The Sonic Radar arranges a fixed number of prototypes resulting from a content-based clustering process in a circle around the user’s standpoint. To derive an auralization of the scene, we introduce the concept of an aural focus of perception that adapts well-known principles from the visual domain. The Sonic SOM is based on Kohonen’s Self-Organizing Map. It helps the user in understanding the structure of his music collection by positioning titles on a two-dimensional grid according to their high-dimensional similarity. We show how our auralization concept can be adapted to extend this visualization technique and thereby support multimodal navigation.",DEU,education,Developed economies,"[-16.842157, 30.568073]","[27.518494, 19.083235]","[-13.248079, 7.7316456, -21.914654]","[14.080098, -5.460755, 23.920033]","[14.23337, 7.2399774]","[11.199553, 1.7367564]","[14.191012, 14.000536, -2.5120883]","[12.327675, 5.685761, 13.222523]"
89,Wei Liang;Shuwu Zhang;Bo Xu 0002,A Hierarchical Approach for Audio Stream Segmentation and Classification.,2005,https://doi.org/10.5281/zenodo.1415166,"Wei Liang+Institute of Automation, Chinese Academy of Sciences>CHN>facility;Shuwu Zhang+Institute of Automation, Chinese Academy of Sciences>CHN>facility;Bo Xu+Institute of Automation, Chinese Academy of Sciences>CHN>facility","This paper describes a hierarchical approach for fast audio stream segmentation and classification. With this approach, the audio stream is firstly segmented into audio clips by MBCR (Multiple sub-Bands spectrum Centroid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips into six pre-defined categories in terms of discriminative background sounds, which is pure speech, pure music, song, speech with music, speech with noise and silence. The experiments on real TV program recordings showed that this approach has higher accuracy and recall rate for audio classification with a fast speed under noise environments.",CHN,facility,Developing economies,"[-15.515503, -18.159489]","[16.684156, -19.07568]","[8.149543, -6.2542086, -11.989751]","[17.566315, -0.2359592, -12.555776]","[11.444414, 8.840226]","[9.024406, 3.413373]","[12.619689, 14.071776, 0.78817314]","[10.9119, 7.5830936, 10.753208]"
90,Wei Liang;Shuwu Zhang;Bo Xu 0002,A Histogram Algorithm for Fast Audio Retrieval.,2005,https://doi.org/10.5281/zenodo.1415812,"Wei Liang+Institute of Automation, Chinese Academy of Sciences>CHN>facility;Shuwu Zhang+Institute of Automation, Chinese Academy of Sciences>CHN>facility;Bo Xu+Institute of Automation, Chinese Academy of Sciences>CHN>facility",This paper describes a fast audio detection method for specific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio features based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detection in TV program showed that the algorithm can achieve a very high overall precision and recall rate both about 97% with very fast search time about 1/40 on real time.,CHN,facility,Developing economies,"[-8.976285, 28.328436]","[17.291405, -19.65973]","[-3.9520617, 2.4297, -19.00839]","[18.89895, -0.50821435, -12.378605]","[13.156366, 8.034693]","[9.310705, 2.6818461]","[13.107224, 14.425491, -1.732406]","[11.016474, 6.995323, 11.366478]"
83,Jyh-Shing Roger Jang;Chao-Ling Hsu;Hong-Ru Lee,Continuous HMM and Its Enhancement for Singing/Humming Query Retrieval.,2005,https://doi.org/10.5281/zenodo.1414842,Jyh-Shing Roger Jang+National Tsing Hua University>TWN>education;Chao-Ling Hsu+National Tsing Hua University>TWN>education;Hong-Ru Lee+National Tsing Hua University>TWN>education,"The use of HMM (Hidden Markov Models) for speech recognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by singing/humming, has seldom been reported, partly due to the difference in acoustic characteristics between speech and singing/humming inputs. This paper will derive the formula of CHMM training for frame-based MRAI. In particular, we shall propose enhancement to CHMM and demonstrate that with the enhancement scheme, CHMM can compare favourably with DTW in both efficiency and effectiveness.",TWN,education,Developing economies,"[-3.6702714, 36.01663]","[-0.31672838, -10.280171]","[-12.88597, -5.0987144, -23.214817]","[3.9531806, -6.9268436, -10.6192875]","[14.835604, 6.2133913]","[6.8336535, 3.213606]","[13.167612, 15.3088455, -2.8714595]","[9.399471, 8.070825, 11.166029]"
88,Aristomenis S. Lampropoulos;Paraskevi S. Lampropoulou;George A. Tsihrintzis,Musical Genre Classification Enhanced by Improved Source Separation Technique.,2005,https://doi.org/10.5281/zenodo.1416292,Aristomenis S. Lampropoulos+University of Piraeus>GRC>education;Paraskevi S. Lampropoulou+University of Piraeus>GRC>education;George A. Tsihrintzis+University of Piraeus>GRC>education,"We present a system for musical genre classification based on audio features extracted from signals which correspond to distinct musical instrument sources. For the separation of the musical sources, we propose an innovative technique in which the convolutive sparse coding algorithm is applied to several portions of the audio signal. The system is evaluated and its performance is assessed.",GRC,education,Developed economies,"[-28.130936, -16.11224]","[14.293255, -8.428237]","[-14.616879, 4.7002563, 19.50957]","[15.95708, 7.8637276, -9.248111]","[12.689543, 10.858997]","[9.455614, 3.5559583]","[13.56749, 14.132282, 1.4251304]","[11.216771, 7.4614773, 10.676318]"
82,Özgür Izmirli,Tonal Similarity from Audio Using a Template Based Attractor Model.,2005,https://doi.org/10.5281/zenodo.1416688,Özgür İzmirli+Connecticut College>USA>education,"A model that calculates similarity of tonal evolution among pieces in an audio database is presented. The model employs a template based key finding algorithm. This algorithm is used in a sliding window fashion to obtain a sequence of tonal center estimates that delineate the trajectory of tonal evolution in tonal space. A chroma based representation is used to capture tonality information. Templates are formed from instrument sounds weighted according to pitch distribution profiles. For each window in the input audio, the chroma based representation is interpreted with respect to the precalculated templates that serve as attractor points in tonal space. This leads to a discretization in both time and tonal space making the output representation compact. Local and global variations in tempo are accounted for using dynamic time warping that employs a special type of music theoretical distance measure. Evaluation is given in two stages. The first is evaluation of the key finding model to assess its performance in key finding for raw audio input. The second is based on cross validation testing for pieces that have multiple performances in the database to determine the success of recall by distance.",USA,education,Developed economies,"[3.8953328, 14.507173]","[-4.458709, -5.01352]","[-3.209247, 3.4814758, -4.964515]","[8.05502, -17.4341, -0.7729012]","[12.84959, 9.185712]","[7.273334, 1.9214803]","[12.967504, 14.871766, -0.97465783]","[10.34601, 7.7789874, 11.9051285]"
77,Rob van Gulik;Fabio Vignoli,Visual Playlist Generation on the Artist Map.,2005,https://doi.org/10.5281/zenodo.1415206,Rob van Gulik+Utrecht University>NLD>education;Fabio Vignoli+Philips Research Laboratories>NLD>company,"This paper describes a visual playlist creation method based on a previously designed visualization technique for large music collections. The method gives users high-level control over the contents of a playlist as well as the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly portable. Future work includes an extensive user evaluation to compare the described method with alternative techniques and to measure its qualities, such as the perceived ease of use and perceived usefulness.",NLD,education,Developed economies,"[-38.993473, 37.770073]","[28.370438, 23.265743]","[-3.7192142, 27.941511, -0.855588]","[10.93773, -3.9567533, 24.65598]","[16.15552, 8.1827545]","[11.537027, 1.2805753]","[16.513557, 14.806717, -1.639252]","[12.392996, 5.1873918, 13.125154]"
80,Toru Hosoya;Motoyuki Suzuki;Akinori Ito;Shozo Makino,Lyrics Recognition from a Singing Voice Based on Finite State Automaton for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1417855,Toru Hosoya+Tohoku University>JPN>education;Motoyuki Suzuki+Tohoku University>JPN>education;Akinori Ito+Tohoku University>JPN>education;Shozo Makino+Tohoku University>JPN>education,"Recently, several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user’s singing voice. All of these systems use only the melody information for retrieval. Although the lyrics information is useful for retrieval, there have been few attempts to exploit lyrics in the user’s input. In order to develop a MIR system that uses lyrics and melody information, lyrics recognition is needed. Lyrics recognition from a singing voice is achieved by similar technology to that of speech recognition. The difference between lyrics recognition and general speech recognition is that the input lyrics are a part of the lyrics of songs in a database. To exploit linguistic constraints maximally, we described the recognition grammar using a finite state automaton (FSA) that accepts only lyrics in the database. In addition, we carried out a “singing voice adaptation” using a speaker adaptation technique. In our experimental results, about 86% retrieval accuracy was obtained.",JPN,education,Developed economies,"[-23.821629, -29.963509]","[11.220437, 8.125955]","[13.3094015, 15.94888, -9.091521]","[2.2991467, -15.419835, 15.57625]","[10.73057, 11.43631]","[9.3393955, 1.1554955]","[11.875239, 15.637735, 0.6827729]","[10.737066, 5.8287616, 12.617155]"
79,Helge Homburg;Ingo Mierswa;Bülent Möller;Katharina Morik;Michael Wurst,A Benchmark Dataset for Audio Classification and Clustering.,2005,https://doi.org/10.5281/zenodo.1417065,Helge Homburg+University of Dortmund>DEU>education|AI Unit>DEU>Unknown;Ingo Mierswa+University of Dortmund>DEU>education|AI Unit>DEU>Unknown;Bülent Müller+University of Dortmund>DEU>education|AI Unit>DEU>Unknown;Katharina Morik+University of Dortmund>DEU>education|AI Unit>DEU>Unknown;Michael Wurst+University of Dortmund>DEU>education|AI Unit>DEU>Unknown,"We present a freely available benchmark dataset for audio classification and clustering. This dataset consists of 10 seconds samples of 1886 songs obtained from the Garageband site. Beside the audio clips themselves, textual meta data is provided for the individual songs. The songs are classified into 9 genres. In addition to the genre information, our dataset also consists of 24 hierarchical cluster models created manually by a group of users. This enables a user centric evaluation of audio classification and clustering algorithms and gives researchers the opportunity to test the performance of their methods on heterogeneous data. We first give a motivation for assembling our benchmark dataset. Then we describe the dataset and its elements in more detail. Finally, we present some initial results using a set of audio features generated by a feature construction approach.",DEU,education,Developed economies,"[-19.313951, -15.818718]","[10.770568, 24.969105]","[-11.177795, -4.3103437, 16.962843]","[-3.1729074, 1.6937703, 12.404407]","[12.310481, 10.260518]","[10.117033, 3.5921955]","[13.3025675, 13.82908, 0.9587154]","[11.583238, 6.4448695, 10.952426]"
78,Peyman Heydarian;Joshua D. Reiss,The Persian Music and the Santur Instrument.,2005,https://doi.org/10.5281/zenodo.1415048,"Peyman Heydarian+Queen Mary, University of London>GBR>education;Joshua D. Reiss+Queen Mary, University of London>GBR>education","Persian music has had a profound effect on various Eastern musical cultures, and also influenced Southern European and Northern African music. The Santur, a hammered dulcimer, is one of the most important instruments in Persia. In this paper, Persian music and the Santur instrument are explained and analysed. Techniques for fundamental frequency detection are applied to data acquired from the Santur and results are reported.",GBR,education,Developed economies,"[7.7083526, 5.201029]","[1.2634813, -15.570347]","[-10.84933, -19.728302, -16.985039]","[14.069273, -15.542226, -11.143669]","[11.628833, 10.217909]","[7.1152344, 1.569015]","[12.271785, 15.054799, -1.3621209]","[8.87024, 7.0675936, 12.231803]"
92,Michael I. Mandel;Dan Ellis,Song-Level Features and Support Vector Machines for Music Classification.,2005,https://doi.org/10.5281/zenodo.1415024,Michael I. Mandel+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplar-based classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums.",USA,education,Developed economies,"[-24.716848, -12.575893]","[16.38803, -13.736438]","[-12.593924, 1.6632587, 15.284901]","[14.613009, 3.8623202, -8.075774]","[12.6311035, 10.420309]","[9.859097, 3.4235067]","[13.608243, 14.105381, 1.0065826]","[11.599997, 7.181512, 10.815484]"
76,David Gerhard,Pitch Track Target Deviation in Natural Singing.,2005,https://doi.org/10.5281/zenodo.1418115,David Gerhard+University of Regina>CAN>education,"Unlike fixed-pitch instruments such as the piano, human singing can stray from a target pitch by as much as a semitone while still being perceived as a single fixed note. This paper presents a study of the difference between target pitch and actualized pitch in natural singing. A set of 50 subjects singing the same melody and lyric is used to compare utterance styles. An algorithm for alignment of idealized template pitch tracks to measured frequency tracks is presented. Specific examples are discussed, and generalizations are made with respect to the types of deviations typical in human singing. Demographics, including the skill of the singer, are presented and discussed in the context of the pitch track deviation from the ideal.",CAN,education,Developed economies,"[-2.4212048, -29.529045]","[-2.7311704, -17.977757]","[18.052988, 4.0612965, -16.932007]","[7.435376, -15.821366, -13.45551]","[10.000658, 10.775235]","[6.9710064, 2.1279862]","[11.1501465, 15.148592, 0.41302502]","[8.95276, 7.6814723, 11.557636]"
75,Rebecca Fiebrink;Cory McKay;Ichiro Fujinaga,Combining D2K and JGAP for Efficient Feature Weighting for Classification Tasks in Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1415754,Rebecca Fiebrink+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Music classification continues to be an important component of music information retrieval research. An underutilized tool for improving the performance of classifiers is feature weighting. A major reason for its unpopularity, despite its benefits, is the potentially infinite calculation time it requires to achieve optimal results. Genetic algorithms offer potentially sub-optimal but reasonable solutions at much reduced calculation time, yet they are still quite costly. We investigate the advantages of implementing genetic algorithms in a parallel computing environment to make feature weighting an affordable instrument for researchers in MIR.",CAN,education,Developed economies,"[-26.324259, 10.604407]","[12.749072, 22.274158]","[-18.66521, -5.418682, 15.573728]","[12.30774, 14.724187, -2.819567]","[13.638878, 8.100542]","[9.990746, 2.6782138]","[13.800984, 14.127286, 0.5042136]","[11.622479, 6.3176274, 11.146163]"
74,Douglas Eck;Norman Casagrande,Finding Meter in Music Using An Autocorrelation Phase Matrix and Shannon Entropy.,2005,https://doi.org/10.5281/zenodo.1415650,Douglas Eck+University of Montreal>CAN>education|University of Montreal>CAN>education;Norman Casagrande+University of Montreal>CAN>education,"This paper introduces a novel way to detect metrical structure in music. We introduce a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting autocorrelation phase matrix is useful for several tasks involving metrical structure. First we can use the matrix to enhance standard autocorrelation by calculating the Shannon entropy at each lag. This approach yields improved results for autocorrelation-based tempo induction. Second, we can efficiently search the matrix for combinations of lags that suggest particular metrical hierarchies. This approach yields a good model for predicting the meter of a piece of music. Finally we can use the phase information in the matrix to align a candidate meter with music, making it possible to perform beat induction with an autocorrelation-based model. We present results for several meter prediction and tempo induction datasets, demonstrating that the approach is competitive with models designed specifically for these tasks. We also present preliminary beat induction results on a small set of artificial patterns.",CAN,education,Developed economies,"[10.135374, 17.316723]","[-21.729942, -2.9296103]","[-10.974161, -12.406037, 1.7669306]","[-2.7093818, 14.056377, -10.224382]","[12.00386, 6.5026116]","[5.6605105, 1.9823291]","[12.604633, 13.412248, -1.5949935]","[7.940393, 6.963834, 11.214562]"
73,Peter Jan O. Doets;Reginald L. Lagendijk,Extracting Quality Parameters for Compressed Audio from Fingerprints.,2005,https://doi.org/10.5281/zenodo.1416080,P.J.O. Doets+Delft University of Technology>NLD>education;R.L. Lagendijk+Delft University of Technology>NLD>education,"An audio fingerprint is a compact yet very robust representation of the perceptually relevant parts of audio content. It can be used to identify audio, even when severely distorted. Audio compression causes small changes in the fingerprint. We aim to exploit these small fingerprint differences due to compression to assess the perceptual quality of the compressed audio file. Analysis shows that for uncorrelated signals the Bit Error Rate (BER) is approximately inversely proportional to the square root of the Signal-to-Noise Ratio (SNR) of the signal. Experiments using real music confirm this relation. Further experiments show how the various local spectral characteristics cause a large variation in the behavior of the fingerprint difference as a function of SNR or the bitrate set for compression.",NLD,education,Developed economies,"[-14.61345, -25.106964]","[23.931576, -23.512262]","[7.112306, -10.046768, -21.470139]","[16.342878, -15.024776, 5.615619]","[9.045079, 4.5423594]","[8.425067, -0.11933733]","[10.661936, 11.550528, -1.8339952]","[10.093055, 5.2344265, 13.031962]"
72,Simon Dixon;Gerhard Widmer,MATCH: A Music Alignment Tool Chest.,2005,https://doi.org/10.5281/zenodo.1416952,Simon Dixon+Austrian Research Institute for Artificial Intelligence>AUT>facility;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,"We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasises note onsets in the alignment process. In tests with Classical and Romantic piano music, the average alignment error was 41ms (median 20ms), with only 2 out of 683 test cases failing to align. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed.",AUT,facility,Developed economies,"[16.627419, -11.422927]","[-17.344233, -15.41062]","[-0.6595256, -10.007171, -8.154187]","[1.1167157, -22.335138, -4.4103374]","[10.954543, 6.4974513]","[6.086055, 0.67173874]","[12.013929, 12.662004, -1.5423019]","[8.076912, 5.8176403, 10.917236]"
71,Ruth Dhanaraj;Beth Logan,Automatic Prediction of Hit Songs.,2005,https://doi.org/10.5281/zenodo.1417571,Ruth Dhanaraj+Hewlett Packard Labs>USA>company|Hewlett Packard Labs>USA>company;Beth Logan+Hewlett Packard Labs>USA>company,"We explore the automatic analysis of music to identify likely hit songs. We extract both acoustic and lyric information from each song and separate hits from non-hits using standard classifiers, specifically Support Vector Machines and boosting classifiers. Our features are based on global sounds learnt in an unsupervised fashion from acoustic data or global topics learnt from a lyrics database. Experiments on a corpus of 1700 songs demonstrate performance that is much better than random. The lyric-based features are slightly more useful than the acoustic features in correctly identifying hit songs. Concatenating the two features does not produce significant improvements. Analysis of the lyric-based features shows that the absence of certain semantic information indicates that a song is more likely to be a hit.",USA,company,Developed economies,"[-38.88029, -8.152463]","[25.139002, -5.223554]","[-7.9059258, 20.918736, 9.192584]","[8.657649, 11.915758, 1.4943737]","[13.902801, 10.456951]","[10.504279, 3.1666608]","[15.064923, 14.121972, 0.20994903]","[12.080102, 6.7138596, 11.236892]"
81,Xiao Hu 0001;J. Stephen Downie;Kris West;Andreas F. Ehmann,Mining Music Reviews: Promising Preliminary Results.,2005,https://doi.org/10.5281/zenodo.1417067,Xiao Hu+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Kris West+University of East Anglia>GBR>education;Andreas Ehmann+University of Illinois at Urbana-Champaign>USA>education,"In this paper we present a system for the automatic mining of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genre of the music reviewed and to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically mine arbitrary bodies of text, such as weblogs (blogs) for musically relevant information.",USA,education,Developed economies,"[-35.500282, -4.496852]","[45.97793, 1.8434129]","[-26.085459, 1.3451412, 13.760583]","[19.265162, 15.561245, 6.1388707]","[13.9442835, 9.998293]","[11.500996, 3.1112745]","[14.965693, 14.02621, -0.17003147]","[12.903051, 6.0971637, 11.404272]"
93,Daniel McEnnis;Cory McKay;Ichiro Fujinaga;Philippe Depalle,jAudio: An Feature Extraction Library.,2005,https://doi.org/10.5281/zenodo.1416648,Daniel McEnnis+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education;Philippe Depalle+McGill University>CAN>education,"jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straightforward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio files as input. In the GUI, users select the features that they wish to have extracted—letting jAudio take care of all dependency problems—and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML file or an ARFF file depending on the user’s preference.",CAN,education,Developed economies,"[15.519397, 38.388046]","[5.0497003, 32.61916]","[-17.033333, -14.605253, -9.827059]","[-10.05219, 6.592749, 22.191286]","[12.742822, 7.331539]","[10.594465, 1.1929879]","[13.934453, 13.115412, -0.70155656]","[11.165304, 5.475376, 11.393851]"
97,Robert Neumayer;Michael Dittenbach;Andreas Rauber,"PlaySOM and PocketSOMPlayer, Alternative Interfaces to Large Music Collections.",2005,https://doi.org/10.5281/zenodo.1414818,Robert Neumayer+Vienna University of Technology>AUT>education;Michael Dittenbach+iSpaces Group>AUT>company;Andreas Rauber+Vienna University of Technology>AUT>education,"With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. In this paper we present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information.",AUT,education,Developed economies,"[-19.727291, 29.49388]","[28.536943, 21.752048]","[-15.037288, 3.9084914, -20.598988]","[12.190293, -2.1139462, 21.608162]","[14.194047, 7.3593965]","[11.441437, 1.3573297]","[14.216603, 14.09758, -2.4988747]","[12.322624, 5.220031, 13.11468]"
95,Annamaria Mesaros;Jaakko Astola,The Mel-Frequency Cepstral Coefficients in the Context of Singer Identification.,2005,https://doi.org/10.5281/zenodo.1417539,Annamaria Mesaros+Technical University of Cluj Napoca>ROU>education;Jaakko Astola+Tampere University of Technology>FIN>education,"The singing voice is the oldest and most complex musical instrument. A familiar singer’s voice is easily recognizable for humans, even when hearing a song for the first time. On the other hand, for automatic identification this is a difficult task among sound source identification applications. The signal processing techniques aim to extract features that are related to identity characteristics. The research presented in this paper considers 32 Mel-Frequency Cepstral Coefficients in two subsets: the low order MFCCs characterizing the vocal tract resonances and the high order MFCCs related to the glottal wave shape. We explore possibilities to identify and discriminate singers using the two sets. Based on the results we can affirm that both subsets have their contribution in defining the identity of the voice, but the high order subset is more robust to changes in singing style.",ROU,education,Developed economies,"[-15.723375, -35.01906]","[13.475555, -16.108238]","[14.81112, 17.731215, -21.504417]","[14.699087, -0.73565006, -8.961088]","[10.197428, 11.495937]","[8.591088, 3.7326882]","[11.416318, 15.554351, 0.700327]","[10.710139, 7.9834943, 10.277879]"
0,Jin Ha Lee;J. Stephen Downie;Sally Jo Cunningham,Challenges in Cross-Cultural/Multilingual Music Information Seeking.,2005,https://doi.org/10.5281/zenodo.1416706,Jin Ha Lee+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Sally Jo Cunningham+University of Waikato>NZL>education,Understanding and meeting the needs of a broad range of music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multilingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from a Korean knowledge search portal Naver 지식 (knowledge) iN and 150 queries from Google Answers website. We conclude that new sets of access points must be developed to accommodate music queries that cross cultural or language boundaries.,USA,education,Developed economies,"[-28.280924, 21.588669]","[32.698265, 30.033108]","[-21.140562, 10.262036, -7.3591294]","[7.2243075, 7.052501, 23.206392]","[14.579205, 8.280313]","[12.126656, 0.9965513]","[14.636385, 14.827341, -1.8117278]","[12.5982485, 4.8027344, 12.327773]"
1,Cynthia M. Grund,"Music Information Retrieval, Memory and Culture: Some Philosohpical Remarks.",2005,https://doi.org/10.5281/zenodo.1415626,Cynthia M. Grund+University of Southern Denmark>DNK>education,"The burgeoning field of Music Information Retrieval (MIR) raises issues which are of interest within traditional areas of discussion in philosophy of music and of philosophy of culture in general. The purpose of this paper is twofold: the first goal is to highlight and briefly discuss a selection of these issues, while the second is to make a case for increased mutual awareness of each other on the parts of MIR and of humanistic research. Many traditional debates within the latter receive infusions of new perspectives from MIR, while research within MIR could be fruitfully pointed in directions suggested by questions of interest within traditional research in the humanities, e.g. the relationship of individual memory to cultural memory, issues regarding cross-cultural understanding and the importance of authenticity in artistic contexts.",DNK,education,Developed economies,"[-25.424896, 21.152132]","[29.425268, 35.81991]","[-14.075377, 8.972108, -7.4137955]","[0.12917674, 8.824425, 18.93028]","[14.385452, 8.294082]","[11.791941, 0.45551756]","[14.478843, 14.678882, -1.7665582]","[12.112633, 4.3999314, 11.869039]"
2,Noris Mohd. Norowi;Shyamala Doraisamy;Rahmita Wirza O. K. Rahmat,Factors Affecting Automatic Genre Classification: An Investigation Incorporating Non-Western Musical Forms.,2005,https://doi.org/10.5281/zenodo.1418067,Noris Mohd Norowi+University Putra Malaysia>MYS>education;Shyamala Doraisamy+University Putra Malaysia>MYS>education;Rahmita Wirza+University Putra Malaysia>MYS>education,"The number of studies investigating automated genre classification is growing following the increasing amounts of digital audio data available. The underlying techniques to perform automated genre classification in general include feature extraction and classification. In this study, MARSYAS was used to extract audio features and the suite of tools available in WEKA was used for the classification. This study investigates the factors affecting automated genre classification. As for the dataset, most studies in this area work with western genres and traditional Malay music is incorporated in this study. Eight genres were introduced; Dikir Barat, Etnik Sabah, Inang, Joget, Keroncong, Tumbuk Kalang, Wayang Kulit, and Zapin. A total of 417 tracks from various Audio Compact Discs were collected and used as the dataset. Results show that various factors such as the musical features extracted, classifiers employed, the size of the dataset, excerpt length, excerpt location and test set parameters improve classification results.",MYS,education,Developing economies,"[-29.2244, -14.537979]","[20.376795, -5.986544]","[-18.072638, 1.9491616, 15.741971]","[10.085118, 11.836721, -2.9266684]","[12.986912, 10.886206]","[9.770611, 3.2008076]","[13.919859, 14.324541, 1.3788087]","[11.591312, 7.006868, 10.97546]"
113,Yi Yu 0001;Chiemi Watanabe;Kazuki Joe,Towards a Fast and Efficient Match Algorithm for Content-Based Music Retrieval on Acoustic Data.,2005,https://doi.org/10.5281/zenodo.1415874,Yi Yu+Nara Women’s University>JPN>education;Chiemi Watanabe+Nara Women’s University>JPN>education;Kazuki Joe+Nara Women’s University>JPN>education,"In this paper we present a fast and efﬁcient match algorithm, which consists of two key techniques: Spectral Correlation Based Feature Merge (SCBFM) and Two-Step Retrieval (TSR). SCBFM can remove the redundant information. In consequence, the resulting feature sequence has a smaller size, requiring less storage and computation. In addition, most of the tempo variation is removed; thus a much simpler sequence match method can be adopted. Also, TSR relies on the characteristics of Mel-Frequency Cepstral Coefﬁcient (MFCC), where the precise match in the second step depends on the first step to filter out most of the dissimilar references with only the low order MFCC feature. As a result, the whole retrieval speed can be further improved. The experimental evaluation verifies that SCBFM-TSR yields more meaningful results in comparatively short time. The experiment results are analyzed with a theoretical approach that seeks to find the relation between Spectral Correlation (SC) threshold and storage, computation.",JPN,education,Developed economies,"[-13.973016, 20.818405]","[14.220605, 10.131609]","[-5.1152983, 5.1622596, -13.377588]","[13.100281, -8.520517, 1.3265464]","[13.443282, 8.003916]","[9.877558, 1.6600943]","[13.408382, 14.6102085, -1.8757262]","[11.311986, 6.3007984, 12.293734]"
112,Wen Xue;Mark Sandler,A Partial Searching Algorithm and Its Application for Polyphonic Music Transcription.,2005,https://doi.org/10.5281/zenodo.1415740,"Xue Wen+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility;Mark Sandler+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility","This paper proposes an algorithm for studying spectral contents of pitched sounds in real-world recordings. We assume that the 2nd-order difference, w.r.t. partial index, of a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. Given a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be associated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the algorithm to make it work with reverberant background, such as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level.",GBR,education,Developed economies,"[11.639299, 8.710024]","[-7.316903, -6.9196405]","[6.0419416, -12.05654, 3.1922789]","[-0.4480917, -15.482499, -14.8966]","[12.473982, 7.917193]","[6.925341, 2.836293]","[12.854943, 14.14163, -1.716924]","[9.345945, 8.181409, 10.961976]"
111,Tillman Weyde;Christian Datzko,Efficient Melody Retrieval with Motif Contour Classes.,2005,https://doi.org/10.5281/zenodo.1414738,Tillman Weyde+City University>GBR>education|University of Osnabrück>DEU>education;Christian Datzko+University of Osnabrück>DEU>education,"This paper describes the use of motif contour classes for efficient retrieval of melodies from music collections. Instead of extracting incipits or themes, complete monophonic pieces are indexed for their motifs, using classes of motif contours. Similarity relations between these classes can be used for a very efficient search. This can serve as a first level search, which can be refined by using more computationally intensive comparisons on its results. The model introduced has been implemented and tested using the MUSITECH framework. We present empirical and analytical results on the retrieval quality, the complexity, and quality/efficiency trade-off.",GBR,education,Developed economies,"[6.609685, -9.851126]","[15.123388, 13.958668]","[11.515617, 5.0261, -1.8104205]","[5.503535, -4.512651, 10.890812]","[10.653559, 9.942271]","[9.527337, 1.0259811]","[11.455102, 15.229059, -0.7902183]","[11.036187, 6.172141, 12.872981]"
110,Kris West;Stephen Cox,Finding An Optimal Segmentation for Audio Genre Classification.,2005,https://doi.org/10.5281/zenodo.1416746,Kris West+University of East Anglia>GBR>education;Stephen Cox+University of East Anglia>GBR>education,"In the automatic classification of music many different segmentations of the audio signal have been used to calculate features. These include individual short frames (23 ms), longer frames (200 ms), short sliding textural windows (1 sec) of a stream of 23 ms frames, large fixed windows (10 sec) and whole files. In this work we present an evaluation of these different segmentations, showing that they are sub-optimal for genre classification and introduce the use of an onset detection based segmentation, which appears to outperform all of the fixed and sliding windows segmentation schemes in terms of classification accuracy and model size.",GBR,education,Developed economies,"[-15.619552, -17.816183]","[-19.65792, -5.5584793]","[7.304785, -5.318961, -12.319053]","[-3.282955, 3.9446723, -7.715711]","[11.642091, 8.732799]","[5.821152, 2.4380264]","[12.653845, 14.080543, 0.75568014]","[8.094574, 7.4286637, 10.928837]"
109,Robert Young Walser,Herding Folksongs.,2005,https://doi.org/10.5281/zenodo.1415640,Robert Young Walser+University of Aberdeen>GBR>education,"Cataloging a large, multi-media collection of traditional song and drama in preparation for online presentation highlights issues of song identity and access in the context of contemporary digitized archives. In the James Madison Carpenter collection a particular folksong sung by a particular individual may exist in multiple manifestations: typed song text, sound recording(s), and/or manuscript music notation. While controlled vocabulary references such as Child and Roud numbers provide a degree of identification, such narrative- and text-centric tools are only partly effective in differentiating folkloric materials. Additional means are needed for identifying and controlling folk materials which are distinguished by other aspects of the song such as melody or non-narrative text. The Carpenter project team’s experience with Encoded Archival Description (EAD) illustrates the value of this platform-independent, widely recognized standard and suggests opportunities for further developments particularly suited to locating and retrieving folk music materials.",GBR,education,Developed economies,"[-30.88685, -4.223731]","[25.186281, 39.05383]","[1.0912403, 0.1510952, -33.88785]","[-0.31074908, 5.9889445, 25.627853]","[12.876364, 9.989026]","[11.331039, 0.13739657]","[13.1493435, 15.030537, -0.061420325]","[11.85386, 4.4511204, 12.114863]"
108,Elliot Sinyor;Cory McKay;Rebecca Fiebrink;Daniel McEnnis;Ichiro Fujinaga,Beatbox Classification Using ACE.,2005,https://doi.org/10.5281/zenodo.1414920,Elliot Sinyor+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;Rebecca Fiebrink+McGill University>CAN>education;Daniel McEnnis+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper describes the use of the Autonomous Classification Engine (ACE) to classify beatboxing (vocal percussion) sounds. A set of unvoiced percussion sounds belonging to five classes (bass drum, open hihat, closed hihat and two types of snare drum) were recorded and manually segmented. ACE was used to compare various classification techniques, both with and without feature selection. The best result was 95.55% accuracy using AdaBoost with C4.5 decision tress.",CAN,education,Developed economies,"[-21.179644, -11.81105]","[-14.161984, 1.7597326]","[-12.140534, -4.111002, 11.099829]","[9.669898, 10.711144, -12.501361]","[12.602677, 10.269439]","[8.785092, 3.873022]","[13.516943, 13.904677, 0.9467239]","[9.98115, 7.340588, 10.20809]"
107,Nicolas Scaringella;Giorgio Zoia,On the Modeling of Time Information for Automatic Genre Recognition Systems in Audio Signals.,2005,https://doi.org/10.5281/zenodo.1416064,Nicolas Scaringella+École Polytechnique Fédérale de Lausanne>CHE>education;Giorgio Zoia+École Polytechnique Fédérale de Lausanne>CHE>education,"The creation of huge databases coming from both restoration of existing analogue archives and new content is demanding fast and more and more reliable tools for content analysis and description, to be used for searches, content queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music catalogues, libraries and shops. Despite their use musical genres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the same pattern recognition architecture: extracting features from chunks of audio signal and classifying features independently. In this paper, we focus instead on the low-level temporal relationships between chunks when classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music segments to consolidate classification consistency by reducing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classification results are reported for a database of 1400 songs evenly distributed over 7 genres.   Keywords: musical genres, content analysis and indexing, machine learning, features extraction.",CHE,education,Developed economies,"[-26.144815, -15.6711855]","[14.294487, -9.801431]","[-15.331366, 1.5177443, 20.504282]","[7.492922, 5.778797, -7.307346]","[12.635906, 10.596932]","[9.601826, 3.1392636]","[13.675083, 14.222757, 1.2659321]","[11.360427, 7.1460166, 10.98463]"
106,Craig Stuart Sapp,Online Database of Scores in the Humdrum File Format.,2005,https://doi.org/10.5281/zenodo.1417281,"Craig Stuart Sapp+Stanford University>USA>education|Royal Holloway, University of London>GBR>education","""KernScores, an online library of musical data currently consisting of over 5 million notes, has been created to assist projects dealing with the computational analysis of musical scores. The online scores are in a format suitable for processing with the Humdrum Toolkit for Music Research, but the website also provides automatic translations into several other popular data formats for digital musical scores.""",USA,education,Developed economies,"[-21.050272, 15.147042]","[-2.8127716, 28.749044]","[-9.60265, -6.2638736, -15.919503]","[-11.470003, 0.6844145, 14.761113]","[13.178514, 7.027995]","[9.541616, 1.0294602]","[13.511843, 13.213378, -1.7073824]","[10.381357, 5.781666, 11.945305]"
105,Christopher Raphael,A Graphical Model for Recognizing Sung Melodies.,2005,https://doi.org/10.5281/zenodo.1417052,Christopher Raphael+Indiana University>USA>education,"A method is presented for automatic transcription of sung melodic fragments to score-like representation, including metric values and pitch. A joint model for pitch, rhythm, segmentation, and tempo is defined for a sung fragment. We then discuss the identification of the globally optimal musical transcription, given the observed audio data. A post process estimates the location of the tonic, so the transcription can be presented into the key of C. Experimental results are presented for a small test collection.",USA,education,Developed economies,"[8.307946, -6.540581]","[-5.0596967, -12.261039]","[10.018535, 10.894301, -0.2715287]","[3.8810465, -12.367573, -10.7061615]","[10.54861, 9.760432]","[7.029789, 2.4025767]","[11.502126, 15.064181, -0.83424026]","[9.1477995, 7.5533504, 11.337233]"
104,Aggelos Pikrakis;Sergios Theodoridis,A Novel HMM Approach to Melody Spotting in Raw Audio Recordings.,2005,https://doi.org/10.5281/zenodo.1414886,Aggelos Pikrakis+University of Athens>GRC>education|Unknown>Unknown>Unknown;Sergios Theodoridis+University of Athens>GRC>education|Unknown>Unknown>Unknown,"This paper presents a melody spotting system based on Variable Duration Hidden Markov Models (VDHMM’s), capable of locating monophonic melodies in a database of raw audio recordings. The audio recordings may either contain a single instrument performing in solo mode, or an ensemble of instruments where one of the instruments has a leading role. The melody to be spotted is presented to the system as a sequence of note durations and music intervals. In the sequel, this sequence is treated as a pattern prototype and based on it, a VDHMM is constructed. The probabilities of the associated VDHMM are determined according to a set of rules that account (a) for the allowable note duration flexibility and (b) with possible structural deviations from the prototype pattern. In addition, for each raw audio recording in the database, a sequence of note durations and music intervals is extracted by means of a multi pitch tracking algorithm. These sequences are subsequently fed as input to the constructed VDHMM that models the pattern to be located. The VDHMM employs an enhanced Viterbi algorithm, previously introduced by the authors, in order to account for pitch tracking errors and performance improvisations of the instrument players. For each audio recording in the database, the best-state sequence generated by the enhanced Viterbi algorithm is further post-processed in order to locate occurrences of the melody which is searched. Our method has been successfully tested with a variety of cello recordings in the context of Western Classical music, as well as with Greek traditional multi-instrument recordings, in which clarinet has a leading role.",GRC,education,Developed economies,"[8.005127, -14.236058]","[-14.941386, -5.5710225]","[14.226028, 1.5268958, -1.7969612]","[3.2336676, -3.864855, -12.771413]","[10.147886, 9.892732]","[6.417045, 2.7600737]","[11.051974, 14.777839, -0.3753859]","[8.770201, 7.4782233, 10.862821]"
103,Jeremy Pickens,Classifier Combination for Capturing Musical Variation.,2005,https://doi.org/10.5281/zenodo.1418219,Jeremy Pickens+King's College London>GBR>education,"At its heart, music information retrieval is characterized by the need to find the similarity between pieces of music. However, “similar” does not mean “the same”. Therefore, techniques for approximate matching are crucial to the development of good music information retrieval systems. Yet as one increases the level of approximation, one finds not only additional similar, relevant music, but also a larger number of not-as-similar, non-relevant music. The purpose of this work is to show that if two different retrieval systems do approximate matching in different manners, and both give decent results, they can be combined to give results better than either system individually. One need not sacrifice accuracy for the sake of flexibility.",GBR,education,Developed economies,"[-24.040121, -16.477772]","[16.862116, 15.680203]","[-9.1499815, 3.2127364, 19.523563]","[8.430775, -4.361445, 11.383849]","[12.387912, 10.56693]","[9.94111, 1.2412817]","[13.317663, 13.943303, 1.139542]","[11.388353, 6.034048, 12.809415]"
102,Geoffroy Peeters,Rhythm Classification Using Spectral Rhythm Patterns.,2005,https://doi.org/10.5281/zenodo.1417495,Geoffroy Peeters+IRCAM - Sound Analysis/Synthesis Team>FRA>facility,"In this paper, we study the use of spectral patterns to represent the characteristics of the rhythm of an audio signal. A function representing the position of onsets over time is first extracted from the audio signal. From this function we compute at each time a vector which represents the characteristics of the local rhythm. Three feature sets are studied for this vector. They are derived from the amplitude of the Discrete Fourier Transform, the Auto-Correlation Function and the product of the DFT and of a Frequency-Mapped ACF. The vectors are then sampled at some specific frequencies, which represents various ratios of the local tempo. The ability of the three feature sets to represent the rhythm characteristics of an audio item is evaluated through a classification task. We show that using such simple spectral representations allows obtaining results comparable to the state of the art.",FRA,facility,Developed economies,"[42.806374, 9.116271]","[-21.82886, -1.1214606]","[-6.1947865, -22.182838, 0.75081617]","[1.766712, 10.579315, -8.340684]","[11.885807, 5.499858]","[8.904644, 3.2863662]","[11.362533, 14.028527, -2.1576998]","[10.615616, 7.5117307, 10.795913]"
101,Steffen Pauws;Sander van de Wijdeven,User Evaluation of a New Interactive Playlist Generation Concept.,2005,https://doi.org/10.5281/zenodo.1415180,Steffen Pauws+Philips Research>NLD>company|Philips Research>Unknown>Unknown;Sander van de Wijdeven+Philips Research>NLD>company|Philips Research>Unknown>Unknown,"Selecting the ‘right’ songs and putting them in the ‘right’ order are key to a great music listening or dance experience. ‘SatisFly’ is an interactive playlist generation system in which the user can tell what kind of songs should be contained in what order in the playlist, while she navigates through the music collection. The system uses constraint satisfaction to generate a playlist that meets all user wishes. In a user evaluation, it was found that users created high-quality playlists in a swift way and with little effort using the system, while still having complete control on their music choices. The novel interactive way of creating a playlist, while browsing through the music collection, was highly appreciated. Ease of navigation through a music collection is still an issue that needs further attention.",NLD,company,Developed economies,"[-39.916595, 38.965588]","[33.742077, 22.505432]","[-2.6095362, 29.395418, -4.1012383]","[13.360439, 1.3296127, 24.705336]","[16.185518, 8.175781]","[11.746757, 1.4707838]","[16.567253, 14.834504, -1.7431735]","[12.817672, 5.1446385, 13.21937]"
3,Markus Schedl;Peter Knees;Gerhard Widmer,Discovering and Visualizing Prototypical Artists by Web-Based Co-Occurrence Analysis.,2005,https://doi.org/10.5281/zenodo.1418315,Markus Schedl+Johannes Kepler University (JKU)>AUT>education|Austrian Research Institute for Artificial Intelligence (ÖFAI)>AUT>facility;Peter Knees+Johannes Kepler University (JKU)>AUT>education;Gerhard Widmer+Johannes Kepler University (JKU)>AUT>education|Austrian Research Institute for Artificial Intelligence (ÖFAI)>AUT>facility,"Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository.",AUT,education,Developed economies,"[-39.512882, 6.6980305]","[37.63206, 6.5806413]","[-22.21687, 9.983796, 9.833655]","[17.413857, 6.8007946, 7.2100654]","[14.210806, 10.01743]","[11.777816, 2.625753]","[15.00096, 14.745561, -0.047526438]","[13.040228, 5.972038, 12.071856]"
99,Elias Pampalk;Arthur Flexer;Gerhard Widmer,Improvements of Audio-Based Music Similarity and Genre Classificaton.,2005,https://doi.org/10.5281/zenodo.1418083,Elias Pampalk+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Medical University of Vienna>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education,"Audio-based music similarity measures can be applied to automatically generate playlists or recommendations. In this paper spectral similarity is combined with complementary information from fluctuation patterns including two new descriptors derived thereof. The performance is evaluated in a series of experiments on four music collections. The evaluations are based on genre classification, assuming that very similar tracks belong to the same genre. The main findings are that, (1) although the improvements are substantial on two of the four collections our extensive experiments confirm earlier findings that we are approaching the limit of how far we can get using simple audio statistics. (2) We have found that evaluating similarity through genre classification is biased by the music collection (and genre taxonomy) used. Furthermore, (3) in a cross validation no pieces from the same artist should be in both training and test set.",AUT,facility,Developed economies,"[-5.166331, 15.784183]","[31.681759, 6.508978]","[-8.013559, 8.888295, 0.14005665]","[11.824322, 4.941848, 8.260822]","[13.205845, 9.307219]","[11.013403, 2.5064569]","[13.662421, 15.211751, -0.7282073]","[12.5074215, 6.479056, 12.266822]"
98,Giovanna Neve;Nicola Orio,Experiments on Segmentation Techniques for Music Documents Indexing.,2005,https://doi.org/10.5281/zenodo.1416996,Nicola Orio+University of Padova>ITA>education|Unknown>Unknown>Unknown;Giovanna Neve+University of Padova>ITA>education|Unknown>Unknown>Unknown,"This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units, which can be used as content descriptors of music documents. Four approaches have been implemented and compared on a test collection of real documents and queries, showing their impact on index term size and on retrieval effectiveness. From the results, simple but extensive approaches seem to give better performances than more sophisticated segmentation algorithms.",ITA,education,Developed economies,"[-12.355898, 24.486795]","[15.892816, 17.726734]","[-3.2876725, 9.552529, -15.335615]","[3.1999688, -5.2002635, 10.125735]","[13.491703, 7.9754086]","[9.720911, 1.186554]","[13.52844, 14.693476, -1.9467249]","[11.071245, 6.0739837, 12.588768]"
70,Sven Degroeve;Koen Tanghe;Bernard De Baets;Marc Leman;Jean-Pierre Martens,A Simulated Annealing Optimization of Audio Features for Drum Classification.,2005,https://doi.org/10.5281/zenodo.1417311,Sven Degroeve+Ghent University>BEL>education;Koen Tanghe+Ghent University>BEL>education;Bernard De Baets+Ghent University>BEL>education;Marc Leman+Ghent University>BEL>education;Jean-Pierre Martens+Ghent University>BEL>education,Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments.,BEL,education,Developed economies,"[23.433788, -47.774323]","[-12.609674, 3.274198]","[16.77096, -22.618595, 0.0684675]","[8.561955, 7.510186, -11.37323]","[7.837283, 7.069123]","[8.929623, 3.5034168]","[10.254653, 11.719667, 0.9438778]","[10.490993, 7.4915338, 10.425137]"
96,J. Enrique Muñoz Expósito;Sebastian García Galán;Nicolás Ruiz-Reyes;Pedro Vera-Candeas;F. Rivas-Peña,Speech/Music Discrimination Using a Single Warped LPC-Based Feature.,2005,https://doi.org/10.5281/zenodo.1417711,J.E. Muñoz-Expósito+University of Jaén>ESP>education;S. Garcia-Galán+University of Jaén>ESP>education;N. Ruiz-Reyes+University of Jaén>ESP>education;P. Vera-Candeas+University of Jaén>ESP>education;F. Rivas-Peña+University of Jaén>ESP>education,"Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in Tzanetakis and Cook (2002) is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for real-time multimedia applications.",ESP,education,Developed economies,"[-18.21565, -20.594046]","[15.369271, -18.55342]","[6.9957066, -5.1139193, -27.754852]","[15.753636, -0.5563843, -12.149191]","[12.226711, 10.394816]","[8.992783, 3.5089998]","[13.00969, 13.909517, 0.99649006]","[10.903155, 7.7472615, 10.682851]"
94,Anders Meng;John Shawe-Taylor,An Investigation of Feature Models for Music Genre Classification Using the Support Vector Classifier.,2005,https://doi.org/10.5281/zenodo.1416052,Anders Meng+Technical University of Denmark>DNK>education|University of Southampton>GBR>education;John Shawe-Taylor+University of Southampton>GBR>education,"In music genre classification the decision time is typically of the order of several seconds, however, most automatic music genre classification systems focus on short time features derived from 10 − 50ms. This work investigates two models, the multivariate Gaussian model and the multivariate autoregressive model for modelling short time features. Furthermore, it was investigated how these models can be integrated over a segment of short time features into a kernel such that a support vector machine can be applied. Two kernels with this property were considered, the convolution kernel and product probability kernel. In order to examine the different methods an 11 genre music setup was utilized. In this setup the Mel Frequency Cepstral Coefficients were used as short time features. The accuracy of the best performing model on this data set was ∼ 44% compared to a human performance of ∼ 52% on the same data set.",DNK,education,Developed economies,"[-27.844797, -12.977348]","[16.620537, -15.311486]","[-14.017855, 2.7578733, 15.412534]","[15.297423, 3.6749084, -10.455881]","[12.856966, 10.787922]","[9.226184, 3.7368782]","[13.890965, 14.193971, 1.3380225]","[11.041641, 7.5462437, 10.510103]"
5,Thomas Lidy;Andreas Rauber,Evaluation of Feature Extractors and Psycho-Acoustic Transformations for Music Genre Classification.,2005,https://doi.org/10.5281/zenodo.1416856,Thomas Lidy+Vienna University of Technology>AUT>education;Andreas Rauber+Vienna University of Technology>AUT>education,"We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy.",AUT,education,Developed economies,"[-27.006989, -13.123206]","[12.69916, -5.9444833]","[-14.751986, -0.46596605, 17.77326]","[6.7613125, 6.2640796, -3.9207375]","[12.829241, 10.892747]","[9.182838, 3.246669]","[13.784782, 14.136275, 1.2985146]","[10.957318, 7.30162, 10.851233]"
69,Christophe Dalitz;Thomas Karsten,Using the Gamera Framework for Building a Lute Tablature Recognition System.,2005,https://doi.org/10.5281/zenodo.1416378,Christoph Dalitz+Niederrhein University of Applied Sciences>DEU>education;Thomas Karsten+Niederrhein University of Applied Sciences>DEU>education,"In this article we describe an optical recognition system for historic lute tablature prints that we have built with the aid of the Gamera toolkit for document analysis and recognition. We give recognition rates for various historic sources and show that our system works quite well on printed tablature sources using movable types. For engraved and manuscript sources, we discuss some principal current limitations of our system and Gamera.",DEU,education,Developed economies,"[35.86636, -47.161747]","[-24.483633, 39.479122]","[28.105623, -14.890719, 3.6609352]","[-15.047375, -24.643671, 1.8663675]","[7.7955537, 7.869415]","[6.7374825, -0.7061043]","[11.148452, 11.465445, 1.1760088]","[7.814869, 4.028375, 10.563821]"
67,Wei Chai;Barry Vercoe,Detection of Key Change in Classical Piano Music.,2005,https://doi.org/10.5281/zenodo.1415538,Wei Chai+MIT Media Laboratory>USA>facility;Barry Vercoe+MIT Media Laboratory>USA>facility,"Tonality is an important aspect of musical structure. Detecting key of music is one of the major tasks in tonal analysis and will benefit semantic segmentation of music for indexing and searching. This paper presents an HMM-based approach for segmenting musical signals based on key change and identifying the key of each segment. Classical piano music was used in the experiment. The performance, evaluated by three proposed measures (recall, precision and label accuracy), demonstrates the promise of the method.",USA,facility,Developed economies,"[29.369942, 12.5632925]","[-6.552087, -1.2873569]","[7.8188834, -14.709765, 14.746806]","[3.885635, -5.0067234, -6.795991]","[10.456566, 7.234506]","[7.2373314, 2.9133775]","[12.167231, 12.16378, -0.39167443]","[10.086677, 8.259391, 11.875372]"
31,Pierre Roy;Jean-Julien Aucouturier;François Pachet;Anthony Beurivé,Exploiting the Tradeoff Between Precision and Cpu-Time to Speed Up Nearest Neighbor Search.,2005,https://doi.org/10.5281/zenodo.1417453,Pierre Roy+SONY Computer Science Laboratory>FRA>company;Jean-Julien Aucouturier+SONY Computer Science Laboratory>FRA>company;François Pachet+SONY Computer Science Laboratory>FRA>company;Anthony Beurivé+SONY Computer Science Laboratory>FRA>company,"We describe an incremental filtering algorithm to quickly compute the N nearest neighbors according to a similarity measure in a metric space. The algorithm exploits an intrinsic property of a large class of similarity measures for which some parameter p has a positive influence both on the precision and the cpu cost (precision-cpu time trade-off). The algorithm uses successive approximations of the measure to compute first cheap distances on the whole set of possible items, then more and more expensive measures on smaller and smaller sets. We illustrate the algorithm on the case of a timbre similarity algorithm, which compares gaussian mixture models using a Monte Carlo approximation of the Kullback-Leibler distance, where p is the number of points drawn from the distributions. We describe several Monte Carlo algorithmic variants, which improve the convergence speed of the approximation. On this problem, the algorithm performs more than 30 times faster than the naive approach.",FRA,company,Developed economies,"[0.2878112, 36.62922]","[26.47326, 5.943434]","[-7.9488873, -7.508098, -24.53799]","[20.961418, -5.5813375, 7.5912886]","[14.667854, 6.223972]","[10.656101, 2.2828288]","[13.244924, 15.234144, -2.7177062]","[12.157366, 6.471918, 12.552746]"
30,Ning Hu;Roger B. Dannenberg,A Bootstrap Method for Training an Accurate Audio Segmenter.,2005,https://doi.org/10.5281/zenodo.1416200,Ning Hu+Carnegie Mellon University>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education,"Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and hand-labeling is quite difficult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are first used to find the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to refine the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data.",USA,education,Developed economies,"[-14.389553, -17.551243]","[-19.759974, -9.408399]","[5.889784, -7.17676, -12.822727]","[-5.8602743, 2.8012648, -11.565167]","[11.5021, 8.762413]","[6.371714, 2.1041234]","[12.595151, 14.070522, 0.7463227]","[8.627874, 6.732934, 10.521013]"
29,Bryan Pardo;Manan Sanghi,Polyphonic Musical Sequence Alignment for Database Search.,2005,https://doi.org/10.5281/zenodo.1417909,Bryan Pardo+Northwestern University>USA>education;Manan Sanghi+Northwestern University>USA>education,"Finding the best matching database target to a melodic query has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces. However, most tonal music is polyphonic, with multiple concurrent musical lines. Such pieces are not adequately represented as strings. Moreover, users often represent polyphonic pieces in their queries by skipping from one part (the soprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. These approaches are compared using synthetic queries on a database of Bach pieces. Results indicate that when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not.",USA,education,Developed economies,"[15.537658, -9.253835]","[11.819921, 15.082642]","[-0.98785096, -14.765237, -2.9075406]","[8.752202, -9.629201, 11.543635]","[11.281711, 6.5610757]","[9.070013, 0.5990444]","[12.11926, 13.122919, -1.6291155]","[10.673042, 6.043969, 13.149848]"
28,Jeremy Pickens;Costas S. Iliopoulos,Markov Random Fields and Maximum Entropy Modeling for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1414716,Jeremy Pickens+King's College London>GBR>education|Unknown>Unknown>Unknown;Costas Iliopoulos+King's College London>GBR>education|Unknown>Unknown>Unknown,"Music information retrieval is characterized by a number of various user information needs. Systems are being developed that allow searchers to find melodies, rhythms, genres, and singers or artists, to name but a few. At the heart of all these systems is the need to find models or measures that answer the question “how similar are two given pieces of music”. However, similarity has a variety of meanings depending on the nature of the system being developed. More importantly, the features extracted from a music source are often either single-dimensional (i.e.: only pitch, or only rhythm, or only timbre) or else assumed to be orthogonal. In this paper we present a framework for developing systems which combine a wide variety of non-independent features without having to make the independence assumption. As evidence of effectiveness, we evaluate the system on the polyphonic theme similarity task over symbolic data. Nevertheless, we emphasize that the framework is general, and can handle a range of music information retrieval tasks.",GBR,education,Developed economies,"[-14.30621, 18.60198]","[17.060274, 8.408904]","[-6.8665476, 9.974486, -12.724732]","[7.0487857, -0.896661, 8.416818]","[13.652541, 8.3258505]","[9.549189, 1.6476928]","[13.636715, 14.764698, -1.7463614]","[11.151989, 6.556922, 12.54864]"
27,Norman H. Adams;Daniela Marquez;Gregory H. Wakefield,Iterative Deepening for Melody Alignment and Retrieval.,2005,https://doi.org/10.5281/zenodo.1415712,Norman Adams+University of Michigan>USA>education;Daniela Marquez+University of Michigan>USA>education;Gregory Wakefield+University of Michigan>USA>education,"For melodic theme retrieval there is a fundamental trade-off between retrieval performance and retrieval speed. Melodic representations of large dimension yield the best retrieval performance, but at high computational cost, and vice versa. In the present work we explore the use of iterative deepening to achieve robust retrieval performance, but without the accompanying computational burden. In particular, we propose the use of a smooth pitch contour that facilitates query and target representations of variable length. We implement an iterative query-by-humming system that yields a dramatic increase in speed, without degrading performance compared to contemporary retrieval systems. Furthermore, we expand the conventional iterative framework to retain the alignment paths found in each iteration. These alignment paths are used to adapt the alignment window of subsequent iterations, further expediting retrieval without degrading performance.",USA,education,Developed economies,"[13.287729, -10.325259]","[10.942719, 9.897239]","[12.551635, 3.6317027, 4.4094787]","[3.4733398, -12.77475, 10.846682]","[10.942873, 6.828887]","[8.90964, 0.65116096]","[11.946714, 13.055957, -1.2816776]","[10.433873, 6.02903, 13.131335]"
26,Olivier Lartillot,Efficient Extraction of Closed Motivic Patterns in Multi-Dimensional Symbolic Representations of Music.,2005,https://doi.org/10.5281/zenodo.1418129,Olivier Lartillot+University of Jyvaskyla>FIN>education,"An efﬁcient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is usually ﬁltered using global selective mechanisms, based on pattern frequency and length. The proposed approach is founded instead on the concept of closed pattern, and insures lossless compression through an adaptive selection of most speciﬁc descriptions in the multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling the ﬁltering of another form of combinatorial redundancy provoked by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalising particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but requires instead a complex and detailed modelling of the cognitive system ruling the listening processes. The resulting algorithm is able to offer for the ﬁrst time compact and relevant motivic analyses of monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex motivic transformations.",FIN,education,Developed economies,"[13.867375, 16.558233]","[1.03666, 16.641764]","[-4.0084004, -9.4764185, 12.887486]","[-3.7936544, -8.749621, 5.7855153]","[11.7562475, 7.2195244]","[8.795184, 1.4984566]","[13.3468075, 12.498283, -0.98359466]","[10.324031, 6.763301, 12.539988]"
25,Wei-Ho Tsai;Hung-Ming Yu;Hsin-Min Wang,Query-By-Example Technique for Retrieving Cover Versions of Popular Songs with Similar Melodies.,2005,https://doi.org/10.5281/zenodo.1415200,"Wei-Ho Tsai+Institute of Information Science, Academia Sinica>TWN>education|Academia Sinica>Unknown>Unknown;Hung-Ming Yu+Institute of Information Science, Academia Sinica>TWN>education|Academia Sinica>Unknown>Unknown;Hsin-Min Wang+Institute of Information Science, Academia Sinica>TWN>education|Academia Sinica>Unknown>Unknown","""Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of songs specified by users. The technique enables users to listen to songs with an identical tune, but performed by different singers, in different languages, genres, and so on. The proposed system is built on a query-by-example framework, which takes a fragment of the song submitted by the user as input, and returns songs similar to the query in terms of the main melody as output. To handle the likely discrepancies, e.g., tempos, transpositions, and accompaniments between cover versions and the original song, methods are presented to remove the non-vocal portions of the song, extract the sung notes from the accompanied vocals, and compare the similarities between the sung note sequences.""",TWN,education,Developing economies,"[8.095281, 40.728283]","[24.456638, 12.042537]","[-0.9113232, 11.464485, -22.16733]","[15.444453, -1.1247756, 6.5122705]","[16.031998, 11.052989]","[10.519331, 2.043242]","[12.897443, 17.260645, -0.39807498]","[11.895453, 6.192952, 12.174866]"
24,Rui Pedro Paiva,On the Detection of Melody Notes in Polyphonic Audio.,2005,https://doi.org/10.5281/zenodo.1417681,Rui Pedro Paiva+University of Coimbra>PRT>education;Teresa Mendes+University of Coimbra>PRT>education;Amílcar Cardoso+University of Coimbra>PRT>education,"This paper describes a method for melody detection in polyphonic musical signals. Our approach starts by obtaining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, low-salience and harmonically related notes are then eliminated. Finally, the notes comprising the melody are extracted. This is the main topic of this paper. We select the melody notes by making use of note saliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour. Finally, false positives in the extracted melody should be eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or durations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives.",PRT,education,Developed economies,"[6.092191, -17.328089]","[-2.0132177, -7.959993]","[16.281214, 0.7460696, -5.1200757]","[6.4726715, -5.616677, -13.8979]","[9.922459, 9.730524]","[6.7271724, 2.5540931]","[11.092666, 14.420634, -0.24381524]","[8.949834, 7.8557987, 11.066256]"
23,Emilios Cambouropoulos;Maxime Crochemore;Costas S. Iliopoulos;Manal Mohamed;Marie-France Sagot,A Pattern Extraction Algorithm for Abstract Melodic Representations that Allow Partial Overlapping of Intervallic Categories.,2005,https://doi.org/10.5281/zenodo.1415008,Emilios Cambouropoulos+University of Thessaloniki>GRC>education;Maxime Crochemore+University of Marne-la-Vallée>FRA>education;Costas Iliopoulos+King's College London>GBR>education;Manal Mohamed+King's College London>GBR>education;Marie-France Sagot+INRIA Rhône-Alpes>FRA>facility,"This paper proposes an efficient pattern extraction algorithm that can be applied on melodic sequences that are represented as strings of abstract intervallic symbols; the melodic representation introduces special “don’t care” symbols for intervals that may belong to two partially overlapping intervallic categories. As a special case the well established “step-leap” representation is examined. In the step-leap representation, each melodic diatonic interval is classified as a step (±s), a leap (±l) or a unison (u). Binary don’t care symbols are introduced to represent the possible overlapping between the various abstract categories e.g. ∗ = s, ∗ = l and # = −s, # = −l. For such a sequence, we are interested in finding maximal repeating pairs and repetitions with a hole (two matching subsequences separated with an intervening non-matching symbol). We propose an O(n + d(n − d) + z)-time algorithm for computing all such repetitions in a given sequence x = x[1..n] with d binary don’t care symbols, where z is the output size.",GRC,education,Developed economies,"[4.2082214, 17.102034]","[6.0326657, 13.892417]","[4.0966454, 4.5446343, -2.4220266]","[4.5108852, -8.237921, 4.8583403]","[11.920272, 9.800611]","[8.537143, 1.0475557]","[12.512688, 15.44765, -0.84950167]","[10.140773, 6.5933175, 12.894048]"
22,Graham E. Poliner;Daniel P. W. Ellis,A Classification Approach to Melody Transcription.,2005,https://doi.org/10.5281/zenodo.1414796,Graham E. Poliner+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. In contrast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Support Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems.",USA,education,Developed economies,"[8.4296255, -11.167421]","[7.339923, -5.5636277]","[14.988435, 5.367435, 1.2288504]","[1.5692881, -9.288145, -0.13996707]","[10.046908, 9.813109]","[8.254776, 2.4460683]","[11.130133, 14.710287, -0.49130514]","[9.882351, 7.365188, 11.902841]"
21,Rainer Typke;Frans Wiering;Remco C. Veltkamp,A Survey of Music Information Retrieval Systems.,2005,https://doi.org/10.5281/zenodo.1417383,Rainer Typke+Universiteit Utrecht>NLD>education;Frans Wiering+Universiteit Utrecht>NLD>education;Remco C. Veltkamp+Universiteit Utrecht>NLD>education,"This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks.",NLD,education,Developed economies,"[-17.409231, 21.575499]","[16.916128, 16.650541]","[-10.974481, 5.660178, -11.805289]","[7.2931685, -4.6874957, 12.600954]","[13.888249, 7.9369397]","[9.966388, 0.8405408]","[13.927742, 14.682195, -2.0181699]","[11.310906, 5.7391534, 12.816905]"
20,Richard Lobb;Tim Bell;David Bainbridge 0001,Fast Capture of Sheet Music for an Agile Digital Music Library.,2005,https://doi.org/10.5281/zenodo.1417989,Richard Lobb+University of Canterbury>NZL>education;Tim Bell+University of Canterbury>NZL>education;David Bainbridge+University of Waikato>NZL>education,"A personal digital music library needs to be “agile”, that is, it needs to make it easy to capture and index material on the fly. A digital camera is a particularly effective way of achieving this, but there are several issues with the quality of the captured image, including distortions in the shape of the image due to the camera not being aligned properly with the page, non-planarity of the page, lens distortion from close-up shots, and inconsistent lighting across the page. In this paper we explore ways to improve the quality of music images captured by a digital camera or an inexpensive scanner, where the user is not expected to pay a lot of attention to the process. Such pre-processing will significantly aid Music Information Retrieval indexing through Optical Music Recognition, for example. The research presented here is primarily based around using a Fast Fourier Transform (FFT) to determine the orientation of the page. We find that a windowed FFT is effective at correcting rotational errors, and we make significant progress towards removing perspective distortion introduced by the camera not being parallel with the music.",NZL,education,Developed economies,"[32.02363, 7.1116405]","[-19.498154, 40.893642]","[24.480968, 4.424899, 17.24746]","[-8.452996, -23.803852, 0.83005637]","[10.652525, 6.813207]","[6.6146817, -0.24997033]","[12.379724, 11.989975, -1.277715]","[7.9732904, 4.5872035, 10.738157]"
19,Ioannis Karydis;Alexandros Nanopoulos;Apostolos N. Papadopoulos;Dimitrios Katsaros 0001;Yannis Manolopoulos,Content-Based Music Information Retrieval in Wireless Ad-Hoc Networks.,2005,https://doi.org/10.5281/zenodo.1417665,Ioannis Karydis+Aristotle University>GRC>education;Alexandros Nanopoulos+Aristotle University>GRC>education;Dimitrios Katsaros+Aristotle University>GRC>education;Yannis Manolopoulos+Aristotle University>GRC>education;Apostolos Papadopoulos+Aristotle University>GRC>education,"This paper introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks. We investigate for the first time the challenges posed by the wireless medium and recognise the factors that require optimisation. We propose novel techniques, which attain a significant reduction in both response times and traffic, compared to naive approaches. Extensive experimental results illustrate the appropriateness and efficiency of the proposed method in this bandwidth-starving and volatile, due to mobility, environment.",GRC,education,Developed economies,"[-18.693573, 24.747286]","[48.63849, 20.442528]","[-11.020843, 10.70268, -15.078563]","[22.496464, -1.4239895, 16.018614]","[14.232126, 7.8087964]","[11.863781, 1.7844596]","[14.159631, 14.877927, -2.2801783]","[12.8219385, 5.2436833, 12.586873]"
18,Ajay Kapur;Richard I. McWalter;George Tzanetakis,New Music Interfaces for Rhythm-Based Retrieval.,2005,https://doi.org/10.5281/zenodo.1418313,Ajay Kapur+University of Victoria>CAN>education|University of Victoria>CAN>education|University of Victoria>CAN>education;Richard I. McWalter+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of performance practice.",CAN,education,Developed economies,"[-14.661217, 23.011292]","[8.622792, 32.87925]","[-7.926401, 0.16683255, -8.453278]","[-13.221223, 2.8530521, 19.524305]","[13.781983, 7.774865]","[10.958928, 1.0934652]","[13.496717, 14.502995, -2.1338718]","[11.024835, 5.1366224, 11.743885]"
17,Jenn Riley,Exploiting Musical Connections: A Proposal for Support of Work Relationships in a Digital Music Library.,2005,https://doi.org/10.5281/zenodo.1415660,Jenn Riley+Indiana University Digital Library Program>USA>education,"Musical works in the Western art music tradition exist in a complex, inter-related web. Works that are derivative or part of another work are common; however, most music information retrieval systems, including traditional library catalogs, don’t use these essential relationships to improve search results or provide information about them to end-users. As part of the NSF-funded Variations2 Digital Music Library project at Indiana University, we have developed a set of functional requirements defining how derivative and whole/part relationships between musical works should be acted upon in search results, and how these results should be displayed. This paper describes recent research into these relationships, provides examples why they are important in Western art music, outlines how Variations2 or any other music information retrieval system could use these relationships in matching user queries, and describes optimal displays of these relationships to end-users.",USA,education,Developed economies,"[-25.669983, 26.279106]","[22.255657, 29.051226]","[-20.21886, 8.398583, -12.471533]","[4.771809, -3.362191, 19.705244]","[14.696368, 7.829484]","[10.788287, 0.71958274]","[14.714209, 14.576221, -2.2719557]","[11.781695, 5.1745133, 12.692141]"
16,Hirokazu Kameoka;Takuya Nishimoto;Shigeki Sagayama,Harmonic-Temporal Clustering via Deterministic Annealing EM Algorithm for Audio Feature Extraction.,2005,https://doi.org/10.5281/zenodo.1417629,Hirokazu Kameoka+The University of Tokyo>JPN>education;Takuya Nishimoto+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"This paper proposes “harmonic-temporal structured clustering (HTC) method”, that allows simultaneous estimation of pitch, intensity, onset, duration, etc., of each underlying source in multi-stream audio signal, which we expect to be an effective feature extraction for MIR systems. STC decomposes the energy patterns diffused in time-frequency space, i.e., a time series of power spectrum, into distinct clusters such that each of them is originated from a single sound stream. It becomes clear that the problem is equivalent to geometrically approximating the observed time series of power spectrum by superimposed harmonic-temporal structured models (HTMs), whose parameters are directly associated with the specific acoustic characteristics. The update equations in DA(Deterministic Annealing)EM algorithm for the optimal parameter convergence are derived by formulating the model with Gaussian kernel representation. The experiment showed promising results, and verified the potential of the proposed method.",JPN,education,Developed economies,"[-11.445325, -15.429532]","[-48.910976, -32.203968]","[0.20529501, -4.6477566, -11.81852]","[-1.2758168, -5.014684, -30.27548]","[11.995979, 8.101395]","[6.7023787, 3.3157272]","[12.671129, 13.726111, 0.1795875]","[9.509883, 8.285753, 10.79628]"
15,Parag Chordia,Segmentation and Recognition of Tabla Strokes.,2005,https://doi.org/10.5281/zenodo.1416000,Parag Chordia+Stanford University>USA>education,"A system that segments and labels tabla strokes from real performances is described. Performance is evaluated on a large database taken from three performers under different recording conditions, containing a total of 16,834 strokes. The current work extends previous work by Gillet and Richard (2003) on categorizing tabla strokes, by using a larger, more diverse database that includes their data as a benchmark, and by testing neural networks and tree-based classification methods. First, the time-domain signal was segmented using complex-domain thresholding that looked for sudden changes in amplitude and phase discontinuities. At the optimal point on the ROC curve, false positives were less than 1% and false negatives were less than 2%. Then, classification was performed using a multivariate Gaussian model (mv gauss) as well as non-parametric techniques such as probabilistic neural networks (pnn), feed-forward neural networks (ffnn), and tree-based classifiers. Two evaluation protocols were used. The first used 10-fold cross validation. The recognition rate averaged over several experiments that contained 10-15 classes was 92% for the mv gauss, 94% for the ffnn and pnn, and 84% for the tree based classifier. To test generalization, a more difficult independent evaluation was undertaken in which no test strokes came from the same recording as the training strokes. The average recognition rate over a wide variety of test conditions was 76% for the mv gauss, 83% for the ffnn, 76% for the pnn, and 66% for the tree classifier.",USA,education,Developed economies,"[33.94038, -48.02429]","[-41.416233, -13.065418]","[27.458614, -18.850054, 4.932026]","[4.5780725, 7.2303443, -20.183416]","[7.7467246, 7.8124185]","[8.285191, 4.376673]","[11.040654, 11.461095, 1.1549165]","[9.221241, 6.9484982, 9.747391]"
14,Nick Collins,Using a Pitch Detector for Onset Detection.,2005,https://doi.org/10.5281/zenodo.1417309,Nick Collins+University of Cambridge>GBR>education,"A segmentation strategy is explored for monophonic instrumental pitched non-percussive material (PNP) which proceeds from the assertion that human-like event analysis can be founded on a notion of stable pitch percept. A constant-Q pitch detector following the work of Brown and Puckette provides pitch tracks which are post processed in such a way as to identify likely transitions between notes. A core part of this preparation of the pitch detector signal is an algorithm for vibrato suppression. An evaluation task is undertaken on slow attack and high vibrato PNP source files with human annotated onsets, exemplars of a difficult case in monophonic source segmentation. The pitch track onset detection algorithm shows an improvement over the previous best performing algorithm from a recent comparison study of onset detectors. Whilst further timbral cues must play a part in a general solution, the method shows promise as a component of a note event analysis system.",GBR,education,Developed economies,"[28.626451, -25.003569]","[-21.026377, -6.195317]","[10.427851, -18.489983, -8.037947]","[-0.012465983, 4.0607967, -13.593654]","[10.1904745, 5.217121]","[6.0879827, 2.6737802]","[10.502156, 13.205845, -1.2809457]","[8.450638, 7.6460776, 10.890171]"
13,Olivier Gillet;Gaël Richard,Drum Track Transcription of Polyphonic Music Using Noise Subspace Projection.,2005,https://doi.org/10.5281/zenodo.1415606,Olivier Gillet+GET/Télécom Paris>FRA>education|CNRS LTCI>FRA>facility;Gaël Richard+GET/Télécom Paris>FRA>education|CNRS LTCI>FRA>facility,"This paper presents a novel drum transcription system for polyphonic music. The use of a band-wise harmonic/noise decomposition allows the suppression of the deterministic part of the signal, which is mainly contributed by non-rhythmic instruments. The transcription is then performed on the residual noise signal, which contains most of the rhythmic information. This signal is segmented, and the events associated to each onset are classified by support vector machines (SVM) with probabilistic outputs. The features used for classification are directly extracted from the sub-band signals. An additional pre-processing stage in which the instances are reclassified using a localized model was also tested. This transcription method is evaluated on ten test sequences, each of them being performed by two drummers and being available with different mixing settings. The whole system achieves precision and recall rates of 84% for the bass drum and snare drum detection tasks.",FRA,education,Developed economies,"[25.712984, -44.39546]","[-15.275934, 2.1275806]","[16.212172, -19.654606, 2.7919345]","[7.7852054, 9.067737, -14.202391]","[7.829428, 7.1367087]","[8.534276, 4.0177965]","[10.378137, 11.784597, 0.86340034]","[9.707623, 7.3956065, 10.165886]"
12,Shoichiro Saito;Hirokazu Kameoka;Takuya Nishimoto;Shigeki Sagayama,Specmurt Analysis of Multi-Pitch Music Signals with Adaptive Estimation of Common Harmonic Structure .,2005,https://doi.org/10.5281/zenodo.1417707,Shoichiro Saito+The University of Tokyo>JPN>education;Hirokazu Kameoka+The University of Tokyo>JPN>education;Takuya Nishimoto+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"This paper describes a multi-pitch analysis method using specmurt analysis with iterative estimation of the quasi-optimal common harmonic structure function. Specmurt analysis (Sagayama et al., 2004) is based upon the idea that superimposed harmonic structure pattern can be expressed as a convolution of two components, a fundamental frequency distribution and a ‘common harmonic structure’ function if each underlying tone component has similar harmonic structure pattern. As proved in our previous work (Sagayama et al., 2004) inappropriate common structure function leads to inaccurate analysis results. The iterative algorithm proposed in this paper automatically chooses a proper structure, which results in finding concurrent multiple fundamental frequencies and reduces the dependency on heuristically chosen initial common harmonic structure. The experimental evaluation showed promising results.",JPN,education,Developed economies,"[35.467022, -19.462404]","[-45.67836, -15.686143]","[7.913061, -23.100882, 5.687244]","[3.3930497, -6.6213994, -30.934748]","[9.088613, 9.00807]","[6.646488, 2.6569371]","[11.07798, 13.525648, -0.09071918]","[9.206188, 8.171928, 10.937138]"
11,Perfecto Herrera;Òscar Celma;Jordi Massaguer;Pedro Cano;Emilia Gómez;Fabien Gouyon;Markus Koppenberger,MUCOSA: A Music Content Semantic Annotator.,2005,https://doi.org/10.5281/zenodo.1415980,Perfecto Herrera+Universitat Pompeu Fabra>ESP>education;Òscar Celma+Universitat Pompeu Fabra>ESP>education;Jordi Massaguer+Universitat Pompeu Fabra>ESP>education;Pedro Cano+Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Universitat Pompeu Fabra>ESP>education;Fabien Gouyon+Universitat Pompeu Fabra>ESP>education;Markus Koppenberger+Universitat Pompeu Fabra>ESP>education;David García+Universitat Pompeu Fabra>ESP>education;José-Pedro García+Universitat Pompeu Fabra>ESP>education;Nicolas Wack+Universitat Pompeu Fabra>ESP>education,"MUCOSA (Music Content Semantic Annotator) is an environment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with micro-annotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. across-files annotations), and a collaborative annotation subsystem, which manages large-scale annotation tasks that can be shared among different research centres. The annotation client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes tools for automatic generation of unary descriptors, invention of new descriptors, and propagation of descriptors across sub-collections or playlists. Finally, the collaborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results between several research institutions. A collection of annotated songs is available, as a “starter pack” to all the individuals or institutions that are eager to join this initiative.",ESP,education,Developed economies,"[-30.763193, 11.493244]","[15.678682, 42.879726]","[-17.977324, 7.4842606, 1.2127357]","[-9.35959, 6.503516, 16.90586]","[14.028981, 9.264784]","[10.954293, 1.4478278]","[14.873384, 13.932037, -0.8093307]","[11.984389, 5.3700023, 11.5018425]"
10,Mika Kuuskankare;Mikael Laurson,Annotating Musical Scores in ENP.,2005,https://doi.org/10.5281/zenodo.1417805,Mika Kuuskankare+Sibelius Academy>FIN>education;Mikael Laurson+Sibelius Academy>FIN>education,"The focus of this paper is on ENP-expressions that can be used for annotating ENP scores with user definable information. ENP is a music notation program written in Lisp and CLOS with a special focus on compositional and music analytical applications. We present number of built-in expressions suitable for visualizing, for example, music analytical information as a part of music notation. A Lisp and CLOS based system for creating user-definable annotation information is also presented along with some sample algorithms. Finally, our system for automatically analyzing and annotating an ENP score is illustrated through several examples including some dealing with music information retrieval.",FIN,education,Developed economies,"[-29.922173, 8.711297]","[-0.3933744, 34.49315]","[-16.140146, 2.5672328, 1.7225192]","[-13.767867, -4.7530212, 16.162466]","[12.640905, 7.144449]","[9.83388, 0.6669673]","[13.532244, 13.40723, -1.5049948]","[10.473192, 5.3955164, 11.856654]"
9,Christopher Harte;Mark B. Sandler;Samer A. Abdallah;Emilia Gómez,Symbolic Representation of Musical Chords: A Proposed Syntax for Text Annotations.,2005,https://doi.org/10.5281/zenodo.1415114,"Christopher Harte+Centre for Digital Music, Queen Mary, University of London>GBR>education;Mark Sandler+Centre for Digital Music, Queen Mary, University of London>GBR>education;Samer Abdallah+Centre for Digital Music, Queen Mary, University of London>GBR>education;Emilia Gómez+Music Technology Group, IUA, Universitat Pompeu Fabra>ESP>education","In this paper we propose a text representation for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs. When designing feature extraction algorithms, it is important to have a hand annotated test set providing a ground truth to compare results against. Hand labelling of chords in music files is a long and arduous task and there is no standard annotation methodology, which causes difficulties sharing with existing annotations. In this paper we address this problem by deﬁning a rigid, context-independent syntax for representing chord symbols in text, supported with a new database of annotations using this system.",GBR,education,Developed economies,"[52.498585, 0.65567434]","[-4.216506, 30.35914]","[20.564556, -16.30998, 20.106836]","[-16.175352, -2.098872, 12.742814]","[7.523605, 8.473562]","[6.803821, 3.3979592]","[12.1973095, 10.838615, 1.4309679]","[9.708125, 8.243428, 12.390065]"
8,Gavin Wood;Simon O'Keefe,On Techniques for Content-Based Visual Annotation to Aid Intra-Track Music Navigation.,2005,https://doi.org/10.5281/zenodo.1417401,Gavin Wood+University of York>GBR>education|University of York>GBR>education;Simon O’Keefe+University of York>GBR>education|University of York>GBR>education,"Despite the fact that people are increasingly listening to music electronically, the core interface of the common tools for playing the music have had very little improvement. In particular the tools for intra-track navigation have remained basically static, not taking advantage of recent studies into the field of audio jisting, summarising and segmentation. We introduce a novel mechanism for musical audio linear summarisation and modify a widely used open source media player to utilise several music information retrieval techniques directly in the graphical user interface. With a broad range of music, we provide a qualitative discussion on several techniques used for content-based music information retrieval and perform quantitative investigation to their usefulness.",GBR,education,Developed economies,"[-29.427732, 11.151133]","[13.95718, 27.301249]","[-20.19176, 5.5616713, 1.1049732]","[0.3874454, -2.668614, 12.087658]","[14.024833, 9.277643]","[10.512942, 1.0939013]","[14.750854, 14.063967, -0.876209]","[11.21606, 5.4638615, 12.208067]"
7,Koen Tanghe;Micheline Lesaffre;Sven Degroeve;Marc Leman;Bernard De Baets;Jean-Pierre Martens,Collecting Ground Truth Annotations for Drum Detection in Polyphonic Music.,2005,https://doi.org/10.5281/zenodo.1417715,Koen Tanghe+Ghent University>BEL>education|IPEM>BEL>facility;Micheline Lesaffre+Ghent University>BEL>education|IPEM>BEL>facility;Sven Degroeve+Ghent University>BEL>education;Marc Leman+Ghent University>BEL>education|IPEM>BEL>facility;Bernard De Baets+Ghent University>BEL>education;Jean-Pierre Martens+Ghent University>BEL>education,"In order to train and test algorithms that can automatically detect drum events in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for 49 real-world music fragments containing different drum event types. Apart from the drum events, the beat was also annotated. The annotators were experienced drummers or percussionists. This paper is primarily aimed towards other drum detection researchers, but might also be of interest to others dealing with automatic music analysis, manual annotation and data gathering. Its purpose is threefold: providing annotation data for algorithm training and evaluation, describing a practical way of setting up a drum annotation task, and reporting issues that came up during the annotation sessions while at the same time providing some thoughts on important points that could be taken into account when setting up similar tasks in the future.",BEL,education,Developed economies,"[23.356459, -45.652]","[-16.145346, 3.5958734]","[16.270752, -18.522108, 0.18646133]","[4.995792, 10.790844, -13.719472]","[7.9528894, 7.058812]","[8.436694, 4.122952]","[10.236502, 11.864362, 0.7906346]","[9.499251, 7.2529464, 10.0621805]"
32,Nancy Bertin;Alain de Cheveigné,Scalable Metadata and Quick Retrieval of Audio Signals.,2005,https://doi.org/10.5281/zenodo.1417079,Nancy Bertin+CNRS UMR 8581 - ENS>FRA>facility|Unknown>Unknown>Unknown;Alain de Cheveigné+CNRS UMR 8581 - ENS>FRA>facility|Unknown>Unknown>Unknown,"Audio search algorithms have reached a degree of speed and accuracy that allows them to search efficiently within large databases of audio. For speed, algorithms generally depend on precalculated indexing metadata. Unfortunately, the size of the metadata follows the same exponential trend as the audio data itself, and this may lead to an exponential increase in storage cost and search time. The concept of scalable metadata has been introduced to allow metadata to adjust to such trends and alleviate the effects of foreseeable increases of data and metadata size. Here, we argue that scalability fits the needs of the hierarchical structures that allow fast search, and illustrate this by adapting a state-of-the-art search algorithm to a scalable indexing structure. Scalability allows search algorithms to adapt to the increase of database size without loss of performance.",FRA,facility,Developed economies,"[-7.900977, 29.50944]","[17.940594, 24.630268]","[-21.60484, -3.2256396, -12.395807]","[-1.0498853, -5.234597, 20.574135]","[13.40523, 8.142136]","[10.352091, 0.8789575]","[13.741619, 14.067425, -1.7419496]","[11.703367, 5.65027, 12.738958]"
68,Sally Jo Cunningham;J. Stephen Downie;David Bainbridge 0001,"""The Pain, the Pain"": Modelling Music Information Behavior and the Songs We Hate.",2005,https://doi.org/10.5281/zenodo.1417209,Sally Jo Cunningham+University of Waikato>NZL>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;David Bainbridge+University of Waikato>NZL>education,"The paper presents a grounded theory analysis of 395 user responses to the survey question, “What is the worst song ever?” Important factors uncovered include: lyric quality, the “earworm” effect, voice quality, the influence of associated music videos, over-exposure, perceptions of pretentiousness, and associations with unpleasant personal experiences.",NZL,education,Developed economies,"[-36.380768, 19.655447]","[44.21276, 31.630129]","[-17.45456, 19.015392, -2.067496]","[8.485913, 16.703133, 14.608507]","[14.982815, 8.773803]","[12.846648, 1.221336]","[15.006329, 15.095041, -1.3066835]","[13.368355, 4.419956, 11.716697]"
33,Charles L. Parker,Applications of Binary Classification and Adaptive Boosting to the Query-By-Humming Problem.,2005,https://doi.org/10.5281/zenodo.1416482,Charles Parker+Oregon State University>USA>education,"In the ""query-by-humming"" problem, we attempt to retrieve a specific song from a target set based on a sung query. Recent evaluations of query-by-humming systems show that the state-of-the-art algorithm is a simple dynamic programming-based interval matching technique. Other techniques based on hidden Markov models are far more expensive computationally and do not appear to offer significant increases in performance. Here, we borrow techniques from artificial intelligence to create an algorithm able to outperform the current state-of-the-art with only a negligible increase in running time.",USA,education,Developed economies,"[-2.80926, 37.83754]","[8.976914, 12.026808]","[-12.864121, -7.267098, -25.592976]","[8.092931, -15.862381, 12.042639]","[14.852316, 6.117609]","[8.883158, 0.4744382]","[13.223266, 15.320257, -2.9357357]","[10.350363, 5.9964833, 13.278664]"
36,Richard Stenzel;Thomas Kamps,Improving Content-Based Similarity Measures by Training a Collaborative Model.,2005,https://doi.org/10.5281/zenodo.1416090,Richard Stenzel+Fraunhofer IPSI>DEU>facility;Thomas Kamps+Fraunhofer IPSI>DEU>facility,"We observed that for multimedia data – especially music - collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with >250,000,000 collaborative data points crawled from the web and >190,000 songs annotated with content-based sound feature sets. A song mentioned in a playlist is regarded as one collaborative data point. In this paper we present a novel approach to bridging the performance gap between collaborative and content-based similarity measures. In the initial training phase a model vector for each song is computed, based on collaborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using explicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale e-commerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vectors. These predicted model vectors are finally used to compute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that offer a collaborative similarity recommender as service to their customers.",DEU,facility,Developed economies,"[-42.176426, 30.706951]","[35.858074, 13.90474]","[-4.716244, 18.714277, -6.6899962]","[15.804371, 2.5465825, 14.224494]","[13.333047, 9.342169]","[12.19398, 2.0944693]","[13.871148, 15.1213045, -0.6107885]","[13.328616, 5.494835, 12.670986]"
66,Òscar Celma;Miquel Ramírez;Perfecto Herrera,Foafing the Music: A Music Recommendation System based on RSS Feeds and User Preferences.,2005,https://doi.org/10.5281/zenodo.1414800,Oscar Celma+Universitat Pompeu Fabra>ESP>education;Miquel Ramírez+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"In this paper we give an overview of the Foaﬁng the Music system. The system uses the Friend of a Friend (FOAF) and Rich Site Summary (RSS) vocabularies for recommending music to a user, depending on her musical tastes. Music information (new album releases, related artists’ news and available audio) is gathered from thousands of RSS feeds —an XML format for syndicating Web content. On the other hand, FOAF documents are used to deﬁne user preferences. The presented system provides music discovery by means of: user proﬁling —deﬁned in the user’s FOAF description—, context-based information —extracted from music related RSS feeds— and content-based de- scriptions —extracted from the audio itself.",ESP,education,Developed economies,"[-44.53868, 26.929031]","[34.9684, 17.874794]","[-12.008716, 25.086208, -13.469638]","[10.016286, 3.2992828, 18.289198]","[15.956072, 9.25276]","[12.054269, 1.7041843]","[15.812717, 15.722037, -1.4945455]","[12.782734, 5.188846, 12.581448]"
65,Domenico Cantone;Salvatore Cristofaro;Simone Faro,"Solving the (\delta, \alpha)-Approximate Matching Problem Under Transposition Invariance in Musical Sequences.",2005,https://doi.org/10.5281/zenodo.1416892,Domenico Cantone+Università di Catania>ITA>education;Salvatore Cristofaro+Università di Catania>ITA>education;Simone Faro+Università di Catania>ITA>education,"The δ-approximate matching problem arises in many questions concerning musical information retrieval and musical analysis. In the case in which gaps are not allowed between consecutive pitches of the melody, transposition invariance is automatically taken care of, provided that the musical melodies are encoded using the pitch interval encoding. However, in the case in which nonnull gaps are allowed between consecutive pitches of the melodies, transposition invariance is not dealt with properly by the algorithms present in literature. In this paper, we propose two slightly different variants of the approximate matching problem under transposition invariance and for each of them provide an algorithm, obtained by adapting an efficient algorithm for the δ-approximate matching problem with α-bounded gaps.",ITA,education,Developed economies,"[20.558214, 23.841373]","[7.8048415, 16.573965]","[0.50574696, -12.976698, 1.4783713]","[6.04458, -10.650916, 4.847063]","[11.563464, 7.3444166]","[8.787883, 0.95346296]","[12.369826, 13.426808, -1.2774731]","[10.439102, 6.5255003, 13.19507]"
64,Domenico Cantone;Salvatore Cristofaro;Simone Faro,"On Tuning the (\delta, \alpha)-Sequential-Sampling Algorithm for \delta-Approximate Matching with Alpha-Bounded Gaps in Musical Sequences.",2005,https://doi.org/10.5281/zenodo.1417791,Domenico Cantone+Università di Catania>ITA>education;Salvatore Cristofaro+Università di Catania>ITA>education;Simone Faro+Università di Catania>ITA>education,"We present a very efﬁcient variant of the (δ, α)-SEQUENTIAL-SAMPLING algorithm, recently introduced by the authors, for the δ-approximate string matching problem with α-bounded gaps, which often arises in many questions on musical information retrieval and musical analysis. Though it retains the same worst-case O(mn)-time and O(mα)-space complexity of its progenitor to compute the number of distinct δ-approximate α-gapped occurrences of a pattern of length m at each position in a text of length n, our new variant achieves an average O(n)-time complexity in practical cases. Extensive experimentations indicate that our algorithm is more efﬁcient than existing solutions for the same problem, especially in the case of long patterns.",ITA,education,Developed economies,"[20.571445, 23.87985]","[7.654134, 16.7686]","[0.82483816, -13.225485, 0.6896543]","[5.886032, -11.016366, 3.8525739]","[11.55686, 7.310067]","[8.834388, 0.9555396]","[12.350565, 13.37921, -1.3078961]","[10.462686, 6.510598, 13.2083]"
63,Margaret Cahill;Donncha Ó Maidín,Melodic Similarity Algorithms -- Using Similarity Ratings for Development and Early Evaluation.,2005,https://doi.org/10.5281/zenodo.1415642,Margaret Cahill+University of Limerick>IRL>education;Donncha Ó Maidín+University of Limerick>IRL>education,"This paper focuses on gathering similarity ratings for use in the construction, optimization and evaluation of melodic similarity algorithms. The approach involves conducting listening experiments to gather these ratings for a piece in Theme and Variation form.",IRL,education,Developed economies,"[1.1481031, 17.486954]","[16.801353, 3.8743453]","[1.8725551, 7.0571513, 1.7467343]","[5.098556, 2.5775065, 5.9591517]","[12.288138, 9.788201]","[9.528544, 1.9090259]","[12.707521, 15.488917, -0.79149437]","[11.333122, 6.9953566, 13.02423]"
62,Giordano Ribeiro de Eulalio Cabral;François Pachet;Jean-Pierre Briot,Automatic X Traditional Descriptor Extraction: the Case of Chord Recognition.,2005,https://doi.org/10.5281/zenodo.1415702,Giordano Cabral+Sony CSL Paris>FRA>company|LIP6 – Paris 6>FRA>education;François Pachet+Sony CSL Paris>FRA>company|LIP6 – Paris 6>FRA>education;Jean-Pierre Briot+LIP6 – Paris 6>FRA>education,"Audio descriptor extraction is the activity of finding mathematical models which describe properties of the sound, requiring signal processing skills. The scientific literature presents a vast collection of descriptors (e.g. energy, tempo, tonality) each one representing a significant effort of research in finding an appropriate descriptor for a particular application. The Extractor Discovery System (EDS) is a recent approach for the discovery of such descriptors, which aim is to extract them automatically. This system can be useful for both non experts – who can let the system work fully automatically – and experts – who can start the system with an initial solution expecting it to enhance their results. Nevertheless, EDS still needs to be massively tested. We consider that its comparison with the results of problems already studied would be very useful to validate it as an effective tool. This work intends to perform the first part of this validation, comparing the results from classic approaches with EDS results when operated by a completely naïve user building a guitar chord recognizer.",FRA,company,Developed economies,"[54.235725, -7.290102]","[10.887837, -6.121557]","[31.360361, -12.562532, 16.48344]","[7.892227, 3.2097793, -3.33718]","[6.804623, 8.646748]","[9.19253, 2.8718536]","[11.931788, 10.38996, 2.06369]","[10.846391, 6.8439646, 10.955396]"
4,Ian Knopke,Geospatial Location of Music and Sound Files for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1417765,Ian Knopke+McGill Music Technology>CAN>education,"A relatively new avenue of Web-based information retrieval research, intended to semantically improve information extraction, is the idea of using geographical information to accurately locate resources. This paper introduces a technique for locating sound and music files geographically. It uses information extracted from the Web relating to audio resources and combines it with geospatial location data to provide new information about audio usage in various countries. The results presented here illustrate the enormous potential for MIR to use the vast amount of audio materials on the Web within a physical and geographical context. Statistics of audio usage around the world are provided, as well as examples of other applications of these techniques.",CAN,education,Developed economies,"[-28.882853, 17.830954]","[20.451105, 24.339527]","[-20.313606, -5.4569325, 1.6089373]","[6.4585915, -0.41783747, 17.76863]","[13.947625, 8.2893505]","[10.983609, 0.5695639]","[14.058461, 14.602409, -1.8502415]","[11.919806, 5.1172266, 12.341279]"
60,Stuart Bray;George Tzanetakis,Distributed Audio Feature Extraction for Music.,2005,https://doi.org/10.5281/zenodo.1417563,Stuart Bray+University of Victoria>CAN>education|University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education|University of Victoria>CAN>education,"One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area where computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented.",CAN,education,Developed economies,"[-11.39291, -16.181885]","[11.153878, 27.45267]","[0.26707813, -5.1750913, -14.110827]","[-2.950386, -1.3398438, 13.430138]","[12.519489, 8.113725]","[10.492303, 1.310115]","[13.117947, 13.6274605, 0.32028332]","[11.360819, 5.6322923, 11.544172]"
58,Xavier Amatriain;Jordi Massaguer;David García;Ismael Mosquera,The CLAM Annotator: A Cross-Platform Audio Descriptors Editing Tool.,2005,https://doi.org/10.5281/zenodo.1416908,Xavier Amatriain+University of California>USA>education|CREATE>USA>company;Jordi Massaguer+Universitat Pompeu Fabra>ESP>education;David Garcia+Universitat Pompeu Fabra>ESP>education;Ismael Mosquera+Universitat Pompeu Fabra>ESP>education,"This paper presents the CLAM Annotator tool. This application has been developed in the context of the CLAM framework and can be used to manually edit any previously computed audio descriptors. The application offers a convenient GUI that allows to edit low-level frame descriptors, global descriptors of any kind and segmentation marks. It is designed in such a way that the interface adapts itself to a user-defined schema, offering possibilities to a large range of applications.",USA,education,Developed economies,"[-9.501495, -19.501394]","[7.0179353, 28.21349]","[-1.3059207, -9.09955, -19.480665]","[-4.565313, -4.817437, 11.657151]","[12.419013, 7.511813]","[10.356456, 1.396401]","[13.694165, 13.396578, -0.2772452]","[11.240394, 5.6800137, 11.476583]"
57,Samer A. Abdallah;Katy C. Noland;Mark B. Sandler;Michael A. Casey;Christophe Rhodes,Theory and Evaluation of a Bayesian Music Structure Extractor.,2005,https://doi.org/10.5281/zenodo.1416018,"Samer Abdallah+Queen Mary, University of London>GBR>education;Katy Noland+Queen Mary, University of London>GBR>education;Mark Sandler+Queen Mary, University of London>GBR>education;Michael Casey+Goldsmiths College, University of London>GBR>education;Christophe Rhodes+Goldsmiths College, University of London>GBR>education","We introduce a new model for extracting classified structural segments, such as intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify signal frames on the basis of their audio properties and then to agglomerate contiguous runs of similarly classified frames into texturally homogenous (or ‘self-similar’) segments which inherit the classification of their constituent frames. Our work extends previous work on automatic structure extraction by addressing the classification problem using an unsupervised Bayesian clustering model, the parameters of which are estimated using a variant of the expectation maximisation (EM) algorithm which includes deterministic annealing to help avoid local optima. The model identifies and classifies all the segments in a song, not just the chorus or longest segment. We discuss the theory, implementation, and evaluation of the model, and test its performance against a ground truth of human judgements. Using an analogue of a precision-recall graph for segment boundaries, our results indicate an optimal trade-off point at approximately 80% precision for 80% recall.",GBR,education,Developed economies,"[-0.8944372, 1.8553517]","[-1.7477349, 2.6292787]","[-1.556286, -4.663373, 2.434371]","[-0.33572525, -0.25052655, -3.1968572]","[11.787859, 8.226256]","[8.223728, 2.9475217]","[12.725092, 13.760009, -0.45578787]","[10.433252, 7.498177, 11.443493]"
56,Jean-Julien Aucouturier;François Pachet,Ringomatic: A Real-Time Interactive Drummer Using Constraint-Satisfaction and Drum Sound Descriptors.,2005,https://doi.org/10.5281/zenodo.1416532,Jean-Julien Aucouturier+SONY CSL Paris>FRA>company;François Pachet+SONY CSL Paris>FRA>company,"We describe a real-time musical agent that generates an audio drum-track by concatenating audio segments automatically extracted from pre-existing musical files. The drum-track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata automatically extracted from the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We report on several drum track audio descriptors designed for the system. We also describe a basic mechanism for controlling the tradeoff between the agent’s autonomy and reactivity, which we illustrate with experiments made in the context of a virtual duet between the system and a human pianist.",FRA,company,Developed economies,"[24.442219, -43.27629]","[-7.427573, 35.57168]","[20.27858, -16.327744, 0.91416126]","[-16.342003, 4.193682, 15.64593]","[7.9304914, 7.123818]","[9.437185, 5.7259417]","[10.326514, 11.689773, 0.94191283]","[10.108969, 5.348398, 10.313381]"
55,Masataka Goto;Takayuki Goto,"Musicream: New Music Playback Interface for Streaming, Sticking, Sorting, and Recalling Musical Pieces.",2005,https://doi.org/10.5281/zenodo.1415842,Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Takayuki Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper describes a novel music playback interface, called Musicream, which lets a user unexpectedly come across various musical pieces similar to those liked by the user. With most previous “query-by-example” interfaces used for similarity-based searching, for the same query and music collection a user will always receive the same list of musical pieces ranked by their similarity and opportunities to encounter unfamiliar musical pieces in the collection are limited. Musicream facilitates active, flexible, and unexpected encounters with musical pieces by providing four functions: the music-disc streaming function which creates a flow of many musical-piece entities (discs) from a (huge) music collection, the similarity-based sticking function which allows a user to easily pick out and listen to similar pieces from the flow, the meta-playlist function which can generate a playlist of playlists (ordered lists of pieces) while editing them with a high degree of freedom, and the time-machine function which automatically records all Musicream activities and allows a user to visit and retrieve a past state as if using a time machine. In our experiments, these functions were used seamlessly to achieve active and creative querying and browsing of music collections, confirming the effectiveness of Musicream.",JPN,facility,Developed economies,"[-19.726553, 29.595098]","[32.507973, 21.010197]","[-15.165281, 3.2301626, -21.597176]","[13.133567, 0.40388763, 21.25158]","[13.927154, 7.204752]","[11.528371, 1.4832708]","[13.997106, 13.718273, -2.3797204]","[12.61607, 5.2569737, 13.093053]"
54,Fabian Mörchen;Alfred Ultsch;Mario Nöcker;Christian Stamm,Databionic Visualization of Music Collections According to Perceptual Distance.,2005,https://doi.org/10.5281/zenodo.1417967,Fabian Mörchen+Philipps-University Marburg>DEU>education;Alfred Ultsch+Philipps-University Marburg>DEU>education;Mario Nöcker+Philipps-University Marburg>DEU>education;Christian Stamm+Philipps-University Marburg>DEU>education,"We describe the MusicMiner system for organizing large collections of music with databionic mining techniques. Low level audio features are extracted from the raw audio data on short time windows during which the sound is assumed to be stationary. Static and temporal statistics were consistently and systematically used for aggregation of low level features to form high level features. A supervised feature selection targeted to model perceptual distance between different sounding music lead to a small set of non-redundant sound features. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map, displaying local sound differences based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song.",DEU,education,Developed economies,"[-16.010967, 31.263937]","[27.654297, 17.90763]","[-12.949122, 6.8031588, -24.679495]","[15.531501, -6.161278, 22.923695]","[14.021584, 7.0526547]","[11.137301, 1.8603091]","[14.118665, 13.861485, -2.5047946]","[12.363183, 5.825995, 13.237079]"
53,Eric J. Isaacson,What You See Is What You Get: on Visualizing Music.,2005,https://doi.org/10.5281/zenodo.1415992,Eric Isaacson+Indiana University>USA>education,"Though music is fundamentally an aural phenomenon, we often communicate about music through visual means. The paper examines a number of visualization techniques developed for music, focusing especially on those developed for music analysis by specialists in the field, but also looking at some less successful approaches. It is hoped that, by presenting them in this way, those in the MIR community will develop a greater awareness of the kinds of musical problems music scholars are concerned with, and might lend a hand toward addressing them.",USA,education,Developed economies,"[-15.309897, 33.196373]","[12.788708, 35.578667]","[-14.678678, 10.666129, -24.366869]","[-2.3449728, 11.350408, 16.338657]","[13.811574, 6.969007]","[11.455797, 0.70158154]","[14.067981, 13.655854, -2.3629792]","[11.757075, 4.7246375, 11.650057]"
52,Frank Kurth;Meinard Müller;David Damm;Christian Fremerey;Andreas Ribbrock;Michael Clausen,Syncplayer - An Advanced System for Multimodal Music Access.,2005,https://doi.org/10.5281/zenodo.1416496,Frank Kurth+Universität Bonn>DEU>education;Meinard Müller+Universität Bonn>DEU>education;David Damm+Universität Bonn>DEU>education;Christian Fremerey+Universität Bonn>DEU>education;Andreas Ribbrock+Universität Bonn>DEU>education;Michael Clausen+Universität Bonn>DEU>education,"In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The recording is then identified by the SyncPlayer server, a process which is performed entirely content-based. Subsequently, the server delivers music-related data like scores or lyrics to the client, which are then displayed synchronously with audio playback using a multimodal visualization plug-in. In addition to visualization, the system provides functionality for content-based music retrieval and semi-manual content annotation. To the best of our knowledge, our system is moreover the first to systematically exploit automatically generated synchronization data for content-based symbolic browsing in high quality audio recordings. SyncPlayer has already proved to be a valuable tool for evaluating algorithms in MIR research on a larger scale. In this paper, we describe the technical background of the SyncPlayer framework in detail. We also give an overview of the underlying MIR techniques of audio matching, music synchronization, and text-based retrieval that are incorporated in the current version of the system.",DEU,education,Developed economies,"[-23.012915, 15.195311]","[11.657743, 32.194904]","[-15.527975, 1.5081558, -22.216684]","[-8.069583, 1.4738245, 20.129557]","[11.384802, 5.983257]","[11.261015, 0.92257017]","[12.388817, 12.624799, -2.2538917]","[11.646989, 4.812768, 12.222179]"
47,Norman Casagrande;Douglas Eck;Balázs Kégl,Frame-Level Audio Feature Extraction Using AdaBoost.,2005,https://doi.org/10.5281/zenodo.1414718,Norman Casagrande+University of Montreal>CAN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Douglas Eck+University of Montreal>CAN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Balázs Kegl+University of Montreal>CAN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"In this paper we adapt an AdaBoost-based image processing algorithm to the task of predicting whether an audio signal contains speech or music. We derive a frame-level discriminator that is both fast and accurate. Using a simple FFT and no built-in prior knowledge of signal structure we obtain an accuracy of 88% on frames sampled at 20ms intervals. When we smooth the output of the classifier with the output of the previous 40 frames our forecast rate rises to 93% on the Scheirer-Slaney (Scheirer and Slaney, 1997) database. To demonstrate the efficiency and effectiveness of the model, we have implemented it as a graphical real-time plugin to the popular Winamp audio player.",CAN,education,Developed economies,"[-11.12564, -17.393557]","[17.182596, -17.199982]","[0.45961413, -7.114705, -15.790202]","[16.290869, 2.2659326, -12.980449]","[12.439087, 7.994749]","[9.055519, 3.641577]","[13.161241, 13.593541, 0.3469931]","[10.881442, 7.6736183, 10.671858]"
46,Shankar Vembu;Stephan Baumann 0001,Separation of Vocals from Polyphonic Audio Recordings .,2005,https://doi.org/10.5281/zenodo.1414852,Shankar Vembu+German Research Centre for AI>DEU>facility|German Research Centre for AI>DEU>facility;Stephan Baumann+German Research Centre for AI>DEU>facility|German Research Centre for AI>DEU>facility,"Source separation techniques like independent component analysis and the more recent non-negative matrix factorization are gaining widespread use for the monaural separation of individual tracks present in a music sample. The underlying principle behind these approaches characterises only stationary signals and fails to separate non-stationary sources like speech or vocals. In this paper, we make an attempt to solve this problem and propose solutions to the extraction of vocal tracks from polyphonic audio recordings. We also present techniques to identify vocal sections in a music sample and design a classifier to perform a vocal–nonvocal segmentation task. Finally, we describe an application wherein we try to extract the melody from the separated vocal track using existing monophonic transcription techniques. The experimental work leads us to the conclusion that the quality of vocal source separation, albeit satisfactory, is not sufficient enough for further F0 analysis to extract the melody line from the vocal track. We identify areas that need further investigation to improve the quality of vocal source separation.",DEU,facility,Developed economies,"[0.19802682, -43.1417]","[-43.463707, -30.002445]","[26.053022, 4.8246245, -5.7834797]","[-5.964093, -6.8027554, -27.965214]","[8.948763, 10.56399]","[6.478299, 5.2472563]","[10.880233, 14.423647, 1.3787763]","[9.922429, 8.600286, 9.620765]"
45,Hiromasa Fujihara;Tetsuro Kitahara;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Singer Identification Based on Accompaniment Sound Reduction and Reliable Frame Selection.,2005,https://doi.org/10.5281/zenodo.1418285,Hiromasa Fujihara+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Tetsuro Kitahara+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazunori Komatani+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Tetsuya Ogata+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroshi G. Okuno+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper describes a method for automatic singer identification from polyphonic musical audio signals including sounds of various instruments. Because singing voices play an important role in musical pieces with a vocal part, the identification of singer names is useful for music information retrieval systems. The main problem in automatically identifying singers is the negative influences caused by accompaniment sounds. To solve this problem, we developed two methods, accompaniment sound reduction and reliable frame selection. The former method makes it possible to identify the singer of a singing voice after reducing accompaniment sounds. It first extracts harmonic components of the predominant melody from sound mixtures and then resynthesizes the melody by using a sinusoidal model driven by those components. The latter method then judges whether each frame of the obtained melody is reliable (i.e. little influenced by accompaniment sound) or not by using two Gaussian mixture models for vocal and non-vocal frames. It enables the singer identification using only reliable vocal portions of musical pieces. Experimental results with forty popular-music songs by ten singers showed that our method was able to reduce the influences of accompaniment sounds and achieved an accuracy of 95%, while the accuracy for a conventional method was 53%.",JPN,education,Developed economies,"[-12.473326, -37.38722]","[12.626673, -20.044674]","[17.850266, 14.153334, -20.044783]","[12.382565, -3.4065175, -14.649541]","[10.14527, 11.457178]","[8.355341, 3.583518]","[11.399022, 15.53639, 0.71306527]","[10.482529, 8.10266, 10.205599]"
44,Slim Essid;Gaël Richard;Bertrand David,Inferring Efficient Hierarchical Taxonomies for MIR Tasks: Application to Musical Instruments.,2005,https://doi.org/10.5281/zenodo.1416268,Slim Essid+GET-Télécom Paris>FRA>education|CNRS LTCI>FRA>facility;Gaël Richard+GET-Télécom Paris>FRA>education|CNRS LTCI>FRA>facility;Bertrand David+GET-Télécom Paris>FRA>education|CNRS LTCI>FRA>facility,"A number of approaches for automatic audio classification are based on hierarchical taxonomies since it is acknowledged that improved performance can be thereby obtained. In this paper, we propose a new strategy to automatically acquire hierarchical taxonomies, using machine learning methods, which are expected to maximize the performance of subsequent classification. It is shown that the optimal hierarchical taxonomy of musical instruments (in the sense of inter-class distances) does not follow the traditional and more intuitive instrument classification into instrument families.",FRA,education,Developed economies,"[-13.475622, 55.29465]","[17.972595, -3.351607]","[-34.199497, 1.4017624, -0.7496973]","[12.9617605, 6.3926406, -2.0847957]","[13.232653, 5.1843195]","[9.418657, 3.5404155]","[14.621701, 11.478237, -1.3169188]","[11.180548, 7.079575, 10.552556]"
43,J. Stephen Downie;Kris West;Andreas F. Ehmann;Emmanuel Vincent,The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview.,2005,https://doi.org/10.5281/zenodo.1416044,"J. Stephen Downie+Graduate School of Library and Information Science (GSLIS), University of Illinois at Urbana-Champaign>USA>education;Kris West+University of East Anglia>GBR>education;Andreas Ehmann+University of Illinois at Urbana-Champaign>USA>education;Emmanuel Vincent+Queen Mary University of London>GBR>education",This paper is an extended abstract which provides a brief preliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper concludes with a listing of targets items to be undertaken before MIREX 2006 to ensure the ongoing success of the MIREX framework.,USA,education,Developed economies,"[-21.034353, 21.083237]","[26.866312, 32.983223]","[-14.856064, 3.1694744, -10.553758]","[2.6917927, 5.4735813, 16.169918]","[14.038488, 7.8014584]","[11.798288, 0.636546]","[14.368283, 14.422569, -1.8983266]","[12.200634, 4.4448185, 12.246994]"
42,Jean-François Paiement;Douglas Eck;Samy Bengio,A Probabilistic Model for Chord Progressions.,2005,https://doi.org/10.5281/zenodo.1416922,Jean-François Paiement+IDIAP Research Institute>CHE>facility|IDIAP Research Institute>CHE>facility;Douglas Eck+University of Montreal>CAN>education;Samy Bengio+IDIAP Research Institute>CHE>facility|IDIAP Research Institute>CHE>facility,"""Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progressions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies.""",CHE,facility,Developed economies,"[58.59935, -1.9789586]","[-24.92934, 18.616165]","[28.369833, -17.272882, 19.376537]","[-21.406048, -3.4971676, 5.2042546]","[6.7838783, 8.733993]","[7.000209, 3.0564902]","[11.827125, 10.500382, 2.13314]","[9.772785, 8.267538, 12.179817]"
41,Juan Pablo Bello;Jeremy Pickens,A Robust Mid-Level Representation for Harmonic Content in Music Signals.,2005,https://doi.org/10.5281/zenodo.1417431,"Juan P. Bello+Queen Mary, University of London>GBR>education;Jeremy Pickens+Queen Mary, University of London>GBR>education","""When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research.""",GBR,education,Developed economies,"[33.322136, -18.851192]","[6.605443, 1.5648214]","[22.981365, -7.9556675, -10.269353]","[5.5902276, -4.1036153, 2.135788]","[9.261755, 9.222878]","[7.9716883, 2.1051621]","[11.323182, 13.573768, 0.2822243]","[9.934702, 7.0122833, 11.025399]"
40,Ching-Hua Chuan;Elaine Chew,Fuzzy Analysis in Pitch-Class Determination for Polyphonic Audio Key Finding.,2005,https://doi.org/10.5281/zenodo.1417297,Ching-Hua Chuan+University of Southern California>USA>education;Elaine Chew+University of Southern California>USA>education,"This paper presents a fuzzy analysis technique for pitch class determination that improves the accuracy of key finding from audio information. Errors in audio key finding, typically incorrect assignments of closely related keys, commonly result from imprecise pitch class determination and biases introduced by the quality of the sound. Our technique is motivated by hypotheses on the sources of audio key finding errors, and uses fuzzy analysis to reduce the errors caused by noisy detection of lower pitches, and to refine the biased raw frequency data, in order to extract more correct pitch classes. We compare the proposed system to two others, an earlier one employing only peak detection from FFT results, and another providing direct key finding from MIDI. All three used the same key finding algorithm (Chew’s Spiral Array CEG algorithm) and the same 410 classical music pieces (ranging from Baroque to Contemporary). Considering only the first 15 seconds of music in each piece, the proposed fuzzy analysis technique outperforms the peak detection method by 12.18% on average, matches the performance of direct key finding from MIDI 41.73% of the time, and achieves an overall maximum correct rate of 75.25% (compared to 80.34% for MIDI key finding).",USA,education,Developed economies,"[32.251884, 13.201798]","[-7.0209074, -4.1593394]","[9.917377, -12.570794, 11.137875]","[7.871825, -6.9827113, -6.070548]","[10.737812, 7.5366497]","[7.396396, 2.6849968]","[12.132079, 13.134267, 0.33286506]","[10.212572, 8.22751, 11.799228]"
39,Meinard Müller;Frank Kurth;Michael Clausen,Audio Matching via Chroma-Based Statistical Features.,2005,https://doi.org/10.5281/zenodo.1416800,Meinard Müller+Universität Bonn>DEU>education;Frank Kurth+Universität Bonn>DEU>education;Michael Clausen+Universität Bonn>DEU>education,"In this paper, we describe an efficient method for audio matching which performs effectively for a wide range of classical music. The basic goal of audio matching can be described as follows: consider an audio database containing several CD recordings for one and the same piece of music interpreted by various musicians. Then, given a short query audio clip of one interpretation, the goal is to automatically retrieve the corresponding excerpts from the other interpretations. To solve this problem, we introduce a new type of chroma-based audio feature that strongly correlates to the harmonic progression of the audio signal. Our feature shows a high degree of robustness to variations in parameters such as dynamics, timbre, articulation, and local tempo deviations. As another contribution, we describe a robust matching procedure, which allows to handle global tempo variations. Finally, we give a detailed account on our experiments, which have been carried out on a database of more than 110 hours of audio comprising a wide range of classical music.",DEU,education,Developed economies,"[-12.0877695, -20.204744]","[-2.637997, -4.18112]","[2.996888, -8.057244, -18.798357]","[4.597981, -18.28464, -0.5218862]","[12.151239, 7.6832056]","[7.1948047, 1.5603751]","[12.845543, 13.637569, 0.5020227]","[10.1896305, 7.3573256, 11.708594]"
38,David Meredith 0001;Geraint A. Wiggins,Comparing Pitch Spelling Algorithms.,2005,https://doi.org/10.5281/zenodo.1416366,"David Meredith+Goldsmiths’ College, University of London>GBR>education;Geraint A. Wiggins+Goldsmiths’ College, University of London>GBR>education","A pitch spelling algorithm predicts the pitch names of the notes in a musical passage when given the onset-time, MIDI note number and possibly the duration and voice of each note. Various versions of the algorithms of Longuet-Higgins, Cambouropoulos, Temperley and Sleator, Chew and Chen, and Meredith were run on a corpus containing 195972 notes, equally divided between eight classical and baroque composers. The standard deviation of the accuracies achieved by each algorithm over the eight composers was used as a measure of its style dependence (SD). Meredith’s ps1303 was the most accurate algorithm, spelling 99.43% of the notes correctly (SD = 0.54). The best version of Chew and Chen’s algorithm was the least dependent on style (SD = 0.35) and spelt 99.15% of the notes correctly. A new version of Cambouropoulos’s algorithm, combining features of all three versions described by Cambouropoulos himself, also spelt 99.15% of the notes correctly (SD = 0.47). The best version of Temperley and Sleator’s algorithm spelt 97.79% of the notes correctly, but nearly 70% of its errors were due to a single sudden enharmonic change. Longuet-Higgins’s algorithm spelt 98.21% of the notes correctly (SD = 1.79) but only when it processed the music a voice at a time.",GBR,education,Developed economies,"[26.884039, -19.298843]","[-0.6388704, -17.231905]","[17.593906, -16.958376, -11.377556]","[-3.2615106, -9.930797, -3.4244576]","[9.884924, 5.8096304]","[7.329009, 1.8900622]","[10.930356, 13.271873, -0.77089745]","[9.2107935, 7.0789466, 11.6395445]"
37,Fabio Vignoli;Steffen Pauws,A Music Retrieval System Based on User Driven Similarity and Its Evaluation.,2005,https://doi.org/10.5281/zenodo.1418359,Fabio Vignoli+Philips Research Laboratories>NLD>company;Steffen Pauws+Philips Research Laboratories>NLD>company,"Large music collections require new ways to let users interact with their music. The concept of finding ‘similar’ songs, albums, or artists provides handles to users for easy navigation and instant retrieval. This paper presents the realization and user evaluation of a music retrieval music that sorts songs on the basis of similarity to a given seed song. Similarity is based on a user-weighted combination of timbre, genre, tempo, year, and mood. A conclusive user evaluation assessed the usability of the system in comparison to two control systems in which the user control of defining the similarity measure was diminished.",NLD,company,Developed economies,"[-10.77035, 16.371273]","[32.19423, 18.63976]","[-6.319644, 8.057683, -6.371935]","[12.60744, 1.433769, 18.004889]","[13.471764, 8.647836]","[11.69567, 1.6080067]","[13.718333, 14.860587, -1.4886892]","[12.577465, 5.248532, 12.858645]"
34,Ming Li;Ronan Sleep,Genre Classification via an LZ78-Based String Kernel.,2005,https://doi.org/10.5281/zenodo.1415162,Ming Li+University of East Anglia>GBR>education;Ronan Sleep+University of East Anglia>GBR>education,"We develop the notion of normalized information distance (NID) into a kernel distance suitable for use with a Support Vector Machine classifier, and demonstrate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is efficient to compute, yet generates an accuracy which compares well with recent works.",GBR,education,Developed economies,"[-31.154732, -9.644412]","[23.70692, -14.539111]","[-20.43928, 2.8332884, 19.142752]","[20.790398, 3.4949841, -4.9027705]","[13.040981, 10.814731]","[10.161736, 3.3543706]","[13.899771, 14.310001, 1.2825367]","[11.759217, 7.0055003, 10.827999]"
61,John Ashley Burgoyne;Lawrence K. Saul,Learning Harmonic Relationships in Digital Audio with Dirichlet-Based Hidden Markov Models.,2005,https://doi.org/10.5281/zenodo.1414870,J. Ashley Burgoyne+University of Pennsylvania>USA>education;Lawrence K. Saul+University of Pennsylvania>USA>education,"Harmonic analysis is a standard musicological tool for understanding many pieces of Western classical music and making comparisons among them. Traditionally, this analysis is done on paper scores, and most past research in machine-assisted analysis has begun with digital representations of them. Human music students are also taught to hear their musical analyses, however, in both musical recordings and performances. Our approach attempts to teach machines to do the same, beginning with a corpus of recorded Mozart symphonies. The audio files are first transformed into an ordered series of normalized pitch class profile (PCP) vectors. Simplified rules of tonal harmony are encoded in a transition matrix. Classical music tends to change key more frequently than popular music, and so these rules account not only for chords, as most previous work has done, but also for the keys in which they function. A hidden Markov model (HMM) is used with this transition matrix to train Dirichlet distributions for major and minor keys on the PCP vectors. The system tracks chords and keys successfully and shows promise for a real-time implementation.",USA,education,Developed economies,"[34.13891, -13.217399]","[-19.944452, 15.43289]","[13.31215, -19.298006, 14.023614]","[1.8218026, -7.0661583, -6.7112255]","[9.380877, 8.9496975]","[7.0294237, 2.9144826]","[11.623458, 13.260724, 0.060088094]","[9.866074, 8.233277, 12.009681]"
100,Elias Pampalk;Tim Pohle;Gerhard Widmer,Dynamic Playlist Generation Based on Skipping Behavior.,2005,https://doi.org/10.5281/zenodo.1414932,Elias Pampalk+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Tim Pohle+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education,"Common approaches to creating playlists are to randomly shuffle a collection (e.g. iPod shuffle) or manually select songs. In this paper we present and evaluate heuristics to adapt playlists automatically given a song to start with (seed song) and immediate user feedback. Instead of rich metadata we use audio-based similarity. The user gives feedback by pressing a skip button if the user dislikes the current song. Songs similar to skipped songs are removed, while songs similar to accepted ones are added to the playlist. We evaluate the heuristics with hypothetical use cases. For each use case we assume a specific user behavior (e.g. the user always skips songs by a particular artist). Our results show that using audio similarity and simple heuristics it is possible to drastically reduce the number of necessary skips.",AUT,facility,Developed economies,"[-40.11809, 40.24481]","[34.824978, 21.078957]","[-1.5738593, 31.306313, -4.518838]","[14.743232, 2.2847154, 22.224098]","[16.229662, 8.174122]","[11.993393, 1.5948325]","[16.58941, 14.840036, -1.7607694]","[13.036275, 5.177223, 13.16417]"
41,Anssi Klapuri,Multiple Fundamental Frequency Estimation by Summing Harmonic Amplitudes.,2006,https://doi.org/10.5281/zenodo.1416740,Anssi Klapuri+Tampere University of Technology>FIN>education,"This paper proposes a conceptually simple and computa- tionally efﬁcient fundamental frequency (F0) estimator for polyphonic music signals. The studied class of estimators calculate the salience, or strength, of a F0 candidate as a weighted sum of the amplitudes of its harmonic partials. A mapping from the Fourier spectrum to a “F0 salience spec- trum” is found by optimization using generated training ma- terial. Based on the resulting function, three different esti- mators are proposed: a “direct” method, an iterative estima- tion and cancellation method, and a method that estimates multiple F0s jointly. The latter two performed as well as a considerably more complex reference method. The number of concurrent sounds is estimated along with their F0s.",FIN,education,Developed economies,"[36.91028, -18.682781]","[-46.312527, -16.384678]","[7.228726, -23.888222, 7.575776]","[2.9488962, -8.6790495, -31.30208]","[9.026054, 8.875319]","[6.64613, 2.6092691]","[11.1067705, 13.533299, -0.097201064]","[9.124056, 8.099346, 11.006872]"
27,Raphaël Clifford;Manolis Christodoulakis;Tim Crawford;David Meredith 0001;Geraint A. Wiggins,"A Fast, Randomised, Maximal Subset Matching Algorithm for Document-Level Music Retrieval.",2006,https://doi.org/10.5281/zenodo.1414878,"Raphaël Clifford+University of Bristol>GBR>education;Manolis Christodoulakis+King's College>GBR>education;Tim Crawford+Goldsmiths College, University of London>GBR>education;David Meredith+Goldsmiths College, University of London>GBR>education;Geraint Wiggins+Goldsmiths College, University of London>GBR>education","We present MSM, a new maximal subset matching algorithm, for MIR at score level with polyphonic texts and patterns. First, we argue that the problem MSM and its ancestors, the SIA family of algorithms, solve is 3SUM-hard and, therefore, subquadratic solutions must involve approximation. MSM is such a solution; we describe it, and argue that, at O(n log n) time with no large constants, it is orders of magnitude more time-efficient than its closest competitor. We also evaluate MSM’s performance on a retrieval problem addressed by the OMRAS project, and show that it outperforms OMRAS on this task by a considerable margin.",GBR,education,Developed economies,"[-13.721551, 20.684618]","[11.38832, 18.76687]","[-4.090735, 6.1640186, -12.951708]","[10.561828, -12.656434, 6.1676645]","[13.603312, 8.050515]","[9.150463, 0.78675103]","[13.538575, 14.674826, -1.8308403]","[10.744966, 6.29514, 13.2833805]"
28,Olivier Gillet;Gaël Richard,ENST-Drums: an extensive audio-visual database for drum signals processing.,2006,https://doi.org/10.5281/zenodo.1415902,"Olivier Gillet+GET / ENST, CNRS LTCI>FRA>facility;Gaël Richard+GET / ENST, CNRS LTCI>FRA>facility","One of the main bottlenecks in the progress of the Music Information Retrieval (MIR) research field is the limited access to common, large and annotated audio databases that could serve for technology development and/or evaluation. The aim of this paper is to present in detail the ENST-Drums database, emphasizing on both the content and the recording process. This audiovisual database of drum performances by three professional drummers was recorded on 8 audio channels and 2 video channels. The drum sequences are fully annotated and will be, for a large part, freely distributed for research purposes. The large variety in its content should serve research in various domains of audio signal processing involving drums, ranging from single drum event classification to complex multimodal drum track transcription and extraction from polyphonic music.",FRA,facility,Developed economies,"[25.032759, -45.72402]","[-14.715971, 4.5652637]","[20.163582, -18.754784, 0.55942345]","[5.512417, 8.290842, -11.798384]","[7.800625, 7.0581856]","[8.699394, 4.010522]","[10.23216, 11.688333, 0.94175875]","[9.829889, 7.2379236, 10.301436]"
29,Cory McKay;Daniel McEnnis;Ichiro Fujinaga,A Large Publicly Accassible Prototype Audio Database for Music Research.,2006,https://doi.org/10.5281/zenodo.1416652,Cory McKay+McGill University>CAN>education;Daniel McEnnis+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper introduces Codaich, a large and diverse publicly accessible database of musical recordings for use in music information retrieval (MIR) research. The issues that must be dealt with when constructing such a database are discussed, as are ways of addressing these problems. It is suggested that copyright restrictions may be overcome by allowing users to make customized feature extraction queries rather than allowing direct access to recordings themselves. The jMusicMetaManager software is introduced as a tool for improving metadata associated with recordings by automatically detecting inconsistencies and redundancies.",CAN,education,Developed economies,"[-19.458416, 11.13918]","[18.362377, 30.185678]","[-16.734724, -4.021967, -10.752715]","[-0.58389777, -1.6384335, 16.829718]","[13.216713, 7.5751085]","[10.941065, 0.5706689]","[14.1563, 13.7633705, -1.4057925]","[11.6388035, 5.091041, 12.072565]"
30,Alex Loscos;Ye Wang;Wei Jie Jonathan Boo,Low Level Descriptors for Automatic Violin Transcription.,2006,https://doi.org/10.5281/zenodo.1416020,Alex Loscos+Universitat Pompeu Fabra>ESP>education|National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education;Wei Jie Jonathan Boo+National University of Singapore>SGP>education,On top of previous work in automatic violin transcription we present a set of straight forward low level descriptors for assisting the transcription techniques and saving computational cost. Proposed descriptors have been tested against a database of 1500 violin notes and double stops.,ESP,education,Developed economies,"[26.133293, -13.751246]","[-38.1253, -6.4557953]","[15.586911, -10.220096, 8.624003]","[-9.2192955, -5.892704, -5.6033216]","[9.518332, 6.943541]","[7.2571573, 3.2225916]","[11.498789, 12.287847, -0.3872536]","[9.135454, 7.184091, 10.1789465]"
31,Motoyuki Suzuki;Toru Hosoya;Akinori Ito;Shozo Makino,Music Information Retrieval from a Singing Voice Based on Verification of Recognized Hypotheses.,2006,https://doi.org/10.5281/zenodo.1414786,Motoyuki Suzuki+Tohoku University>JPN>education;Toru Hosoya+Tohoku University>JPN>education;Akinori Ito+Tohoku University>JPN>education;Shozo Makino+Tohoku University>JPN>education,"Several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user’s singing voice. All of these systems use only melody information for retrieval, although lyrics information is also useful for retrieval. In this paper, we propose an MIR system that uses both melody and lyrics information in the singing voice. The MIR system verifies hypotheses output by a lyrics recognizer from a melodic point of view. Each hypothesis has time alignment information between the singing voice and recognized text, and the boundaries of each note can be estimated using the information. As a result, melody information is extracted from the singing voice. On the other hand, the melody information can be calculated from the musical score of the song because the recognized text must be a part of the lyrics of the song. The hypothesis is verified by calculating the similarity between the two types of melody information. From the experimental results, the verification method increased the retrieval accuracy. Especially, it was very effective when the number of words in the user’s singing voice was small. The proposed method increased the retrieval accuracy from 81.3% to 87.4% when the number of words was only three.",JPN,education,Developed economies,"[-21.04223, 0.78931814]","[11.28444, 8.179482]","[12.294165, 14.44343, -9.7402525]","[2.2969224, -15.148814, 15.016029]","[10.52806, 11.27047]","[9.497168, 1.676915]","[11.647437, 15.529621, 0.5346174]","[10.923402, 5.977529, 12.417371]"
32,Katsutoshi Itoyama;Tetsuro Kitahara;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Automatic Feature Weighting in Automatic Transcription of Specified Part in Polyphonic Music.,2006,https://doi.org/10.5281/zenodo.1417887,Katsutoshi Itoyama+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Tetsuro Kitahara+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Kazunori Komatani+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Tetsuya Ogata+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Hiroshi G. Okuno+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"We studied the problem of automatic music transcription (AMT) for polyphonic music. AMT is an important task for music information retrieval because AMT results enable retrieving musical pieces, high-level annotation, demixing, etc. We attempted to transcribe a part played by an instrument specified by users (specified part tracking). Only two timbre models are required in the specified part tracking to identify the specified musical instrument even when the number of instruments increases. This transcription is formulated into a time-series classification problem with multiple features. We furthermore attempted to automatically estimate weights of the features, because the importance of these features varies for each musical signal. We estimated quasi-optimal weights of the features using a genetic algorithm for each musical signal. We tested our AMT system using trio stereo musical signals. Accuracies with our feature weighting method were 69.8% on average, whereas those without feature weighting were 66.0%.",JPN,education,Developed economies,"[25.41161, -8.121282]","[9.478754, -7.9684935]","[13.0424795, -3.458592, 9.3104925]","[9.137097, 2.5755353, -6.9405403]","[9.584445, 7.7577767]","[8.91715, 3.0879714]","[11.827083, 12.471876, 0.02156028]","[10.679558, 7.0950737, 10.798493]"
33,Yipeng Li;DeLiang Wang,Singing Voice Separation from Monaural Recordings.,2006,https://doi.org/10.5281/zenodo.1416006,Yipeng Li+The Ohio State University>USA>education|The Ohio State University>USA>education;DeLiang Wang+The Ohio State University>USA>education|The Ohio State University>USA>education,"Separating singing voice from music accompaniment has wide applications in areas such as automatic lyrics recognition and alignment, singer identification, and music information retrieval. Compared to the extensive studies of speech separation, singing voice separation has been little explored. We propose a system to separate singing voice from music accompaniment from monaural recordings. The system has three stages. The singing voice detection stage partitions and classifies an input into vocal and non-vocal portions. Then the predominant pitch detection stage detects the pitch contour of the singing voice for vocal portions. Finally the separation stage uses the detected pitch contour to group the time-frequency segments of the singing voice. Quantitative results show that the system performs well in singing voice separation.",USA,education,Developed economies,"[-0.44808143, -42.750237]","[-43.416893, -30.775932]","[26.00238, 6.3149323, -6.497458]","[-5.3619027, -5.7861543, -27.424688]","[9.152772, 10.744026]","[6.7851577, 5.1483784]","[10.887687, 14.610485, 1.299559]","[10.077073, 8.339214, 9.744096]"
34,Emilia Gómez;Perfecto Herrera,The song remains the same: identifying versions of the same piece using tonal descriptors.,2006,https://doi.org/10.5281/zenodo.1417273,Emilia Gómez+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"Identifying versions of the same song by means of automatically extracted audio features is a complex task for a music information retrieval system, even though it may seem very simple for a human listener. The design of a system to perform this task gives the opportunity to analyze which features are relevant for music similarity. This paper focuses on the analysis of tonal similarity and its application to the identification of different versions of the same piece. This work formulates the situations where a song is versioned and several musical aspects are transformed with respect to the canonical version. A quantitative evaluation is made using tonal descriptors, including chroma representations and tonality. A simple similarity measure, based on Dynamic Time Warping over transposed chroma features, yields around 55% accuracy, which exceeds by far the expected random baseline rate.",ESP,education,Developed economies,"[3.2092159, 44.90017]","[19.92947, 7.5011935]","[2.2328658, 19.566525, -18.862005]","[12.917934, -1.8124049, 2.5108876]","[12.921321, 9.586811]","[9.944072, 2.326789]","[13.039476, 16.12941, -0.67699295]","[11.554482, 6.743837, 12.109953]"
35,Thomas Lidy;Andreas Rauber,Visually Profiling Radio Stations.,2006,https://doi.org/10.5281/zenodo.1418079,Thomas Lidy+Vienna University of Technology>AUT>education|Unknown>Unknown>Unknown;Andreas Rauber+Vienna University of Technology>AUT>education|Unknown>Unknown>Unknown,"The overwhelming number of radio stations, both online and over the air, makes the choice of an appropriate program difficult. By profiling the program content of radio stations using Self-Organizing Maps we provide a reflection of a station’s program type and give potential listeners a visual clue for selecting radio stations. Proﬁles of current broadcasts indicate which program type a station is currently playing. By creating radio station maps it is possible to directly pick a specific program type instead of having to search for a suitable radio station.",AUT,education,Developed economies,"[-34.22403, 34.27008]","[29.429705, 17.695019]","[-12.513415, 20.366716, -19.089329]","[13.067985, -8.260291, 24.45028]","[14.03973, 6.983178]","[11.29647, 1.787383]","[14.789462, 14.097041, -2.1379743]","[12.437123, 5.7582636, 13.164583]"
36,Meinard Müller;Henning Mattes;Frank Kurth,An Efficient Multiscale Approach to Audio Synchronization.,2006,https://doi.org/10.5281/zenodo.1417409,Meinard Müller+University of Bonn>DEU>education;Henning Mattes+University of Bonn>DEU>education;Frank Kurth+University of Bonn>DEU>education,"We present an efﬁcient and robust multiscale DTW (Ms-DTW) approach to music synchronization for time-aligning CD recordings of different interpretations of the same piece. The general strategy is to recursively project an alignment path computed at a coarse resolution level to the next higher level and then to reﬁne the projected path. As main contribu- tions, we address several crucial issues including the design and speciﬁcation of robust and scalable audio features, suit- able local cost measures, MsDTW levels, constraint regions, as well as sampling rate adaptation and structural enhance- ment strategies. Extensive experiments on Western classi- cal music show that our MsDTW-based algorithm yields the same alignment result as the classical DTW-based strategy while signiﬁcantly reducing the running time and memory requirements. Even for pieces of a duration of 10 to 15 min- utes, the alignment (based on previously extracted feature sequences) can be computed in less than a second.",DEU,education,Developed economies,"[22.860714, -30.741274]","[-17.224215, -17.624302]","[0.5990915, -19.245634, -16.885817]","[-0.5932831, -24.321974, -4.2479725]","[10.994847, 5.849681]","[6.051352, 0.6063396]","[11.86178, 12.353214, -2.0098631]","[8.145218, 5.8704762, 10.831411]"
37,Michael J. Bruderer;Martin F. McKinney;Armin Kohlrausch,Structural boundary perception in popular music.,2006,https://doi.org/10.5281/zenodo.1418339,Michael J. Bruderer+Technische Universiteit Eindhoven>NLD>education|Philips Research Laboratories>Unknown>company;Martin McKinney+Technische Universiteit Eindhoven>NLD>education|Philips Research Laboratories>Unknown>company;Armin Kohlrausch+Technische Universiteit Eindhoven>NLD>education|Philips Research Laboratories>Unknown>company,"The automatic extraction of musical structure from audio is an important aspect for many music information retrieval (MIR) systems. The criteria on which structural elements in music are defined in MIR systems is often not clearly stated but typically stem from (music) theoretical or signal-based properties. In many cases, however, perceptual-based criteria are the most relevant and systems need to be trained on or modeled after the perception of structural elements in music. Here, we investigate the perception of structural boundaries to Western popular music and examine the musical cues responsible for their perception. We make links to music theoretical descriptions of structural boundaries and to computational methods for extracting structure. The methods and data presented here are useful for developing and training systems for the automatic extraction of musical structure as it is perceived by listeners.",NLD,education,Developed economies,"[-1.6156291, 7.4279623]","[-2.4389324, 5.4163485]","[-5.9270077, -9.047687, -0.029224876]","[-3.4843976, 0.3975949, -0.8030743]","[12.061059, 8.650479]","[8.471712, 2.5732746]","[12.700183, 14.304745, -0.16626862]","[10.355701, 7.032975, 11.526643]"
38,Eric Nichols;Christofer Raphael,Globally Optimal Audio Partitioning.,2006,https://doi.org/10.5281/zenodo.1416846,Eric Nichols+Indiana University>USA>education|Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education,"We present a technique for partitioning an audio file into maximally-sized segments having nearly uniform spectral content, ideally corresponding to notes or chords. Our method uses dynamic programming to globally optimize a measure of simplicity or homogeneity of the intervals in the partition. Here we have focused on an entropy-like measure, though there is considerable flexibility in choosing this measure. Experiments are presented for several musical scenarios.",USA,education,Developed economies,"[-15.890182, -17.549131]","[-0.6690641, -1.2714374]","[7.2711935, -5.2574687, -14.176487]","[-0.32165712, -3.739816, -4.9729295]","[11.612376, 8.717476]","[7.950711, 2.992378]","[12.609049, 14.015863, 0.7807795]","[10.210912, 7.6803064, 11.384714]"
39,Arshia Cont,Realtime Multiple Pitch Observation using Sparse Non-negative Constraints.,2006,https://doi.org/10.5281/zenodo.1416770,"Arshia Cont+Ircam>FRA>facility|Center for Research in Computing and the Arts, UCSD>USA>education","In this paper we introduce a new approach for realtime multiple pitch observation of musical instruments. The proposed algorithm is quite different from others in the literature both in its purpose and approach. It is destined not for continuous multiple f0 recognition but rather for projection of the ongoing spectrum to learned pitch templates. The decomposition algorithm on the other hand, does not compromise signal processing models for pitches and consists of an algorithm for efficient decomposition of a spectrum using known pitch structures and based on sparse non-negative constraints. After introducing the algorithm along with evaluations, a real-time implementation of the algorithm is provided for free download for the MaxMSP realtime programming environment.",FRA,facility,Developed economies,"[25.843548, -22.681004]","[-7.27343, -8.873632]","[12.686741, -17.08174, -12.1047325]","[-0.25516167, -12.537705, -14.660605]","[9.976933, 5.609258]","[6.666413, 2.853818]","[10.811088, 13.176398, -0.8795401]","[9.088209, 8.010075, 10.819678]"
40,Alexander Lerch,On the Requirement of Automatic Tuning Frequency Estimation.,2006,https://doi.org/10.5281/zenodo.1414812,Alexander Lerch+zplane.development>DEU>company,The deviation of the tuning frequency from the standard tuning frequency 440 Hz is evaluated for a database of classical music. It is discussed if and under what circumstances such a deviation may affect the robustness of pitch-based systems for musical content analysis.,DEU,company,Developed economies,"[38.56589, -18.990997]","[-1.8246983, -15.166108]","[4.644839, -25.787844, 7.60658]","[8.287392, -12.963942, -9.586327]","[8.949082, 8.811769]","[6.9470334, 2.261192]","[11.017588, 13.607765, -0.18435487]","[9.152044, 7.543092, 11.3586035]"
42,Matti Ryynänen;Anssi Klapuri,Transcription of the Singing Melody in Polyphonic Music.,2006,https://doi.org/10.5281/zenodo.1418291,Matti Ryynänen+Tampere University Of Technology>FIN>education;Anssi Klapuri+Tampere University Of Technology>FIN>education,This paper proposes a method for the automatic transcription of singing melodies in polyphonic music. The method is based on multiple-F0 estimation followed by acoustic and musicological modeling. The acoustic model consists of separate models for singing notes and for no-melody segments. The musicological model uses key estimation and note bigrams to determine the transition probabilities between notes. Viterbi decoding produces a sequence of notes and rests as a transcription of the singing melody. The performance of the method is evaluated using the RWC popular music database for which the recall rate was 63% and precision rate 46%. A significant improvement was achieved compared to a baseline method from MIREX05 evaluations.,FIN,education,Developed economies,"[0.8197587, -32.240635]","[-3.2080133, -10.999719]","[18.281633, 1.6640198, -7.6525044]","[6.3784466, -9.507501, -15.097792]","[9.803425, 10.210291]","[6.89078, 2.831785]","[11.083319, 14.613132, 0.019861255]","[9.099283, 7.878651, 11.071289]"
43,Tim Pohle;Peter Knees;Markus Schedl;Gerhard Widmer,Independent Component Analysis for Music Similarity Computation.,2006,https://doi.org/10.5281/zenodo.1417877,Tim Pohle+Johannes Kepler University Linz>AUT>education;Peter Knees+Johannes Kepler University Linz>AUT>education;Markus Schedl+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"In the recent years, a number of publications have appeared that deal with automatically calculating the similarity of music tracks. Most of them are based on features that are not intuitively understandable to humans, as they do not have a musically meaningful counterpart, but are merely measures of basic physical properties of the audio signal. Furthermore, most of these algorithms do not take into account the temporal development of the audio signal, which certainly is an important aspect of music. All of them consider the musical signal as a whole, not trying to reconstruct the listening process of dividing the signal into a number of sources. In this work, we present a novel approach to fill this gap by combining a number of existing ideas. At the heart of our approach, Independent Component Analysis (ICA) decomposes an audio signal into individual parts that appear maximally independent from each other. We present one basic algorithm to use these components for similarity computations, and evaluate a number of modifications to it with respect to genre classification accuracy. Our results indicate that this approach is at least of similar quality as many existing feature extraction routines.",AUT,education,Developed economies,"[-9.997237, 10.490531]","[5.172784, 0.42481363]","[-0.8798113, 11.290528, -6.2702136]","[4.817552, 1.5465873, -4.032815]","[12.598234, 8.856164]","[6.8258963, 4.453198]","[13.3273325, 14.453041, -0.48505315]","[10.253778, 8.315057, 10.626101]"
45,Sally Jo Cunningham;David Bainbridge 0001;Annette Falconer,'More of an Art than a Science': Supporting the Creation of Playlists and Mixes.,2006,https://doi.org/10.5281/zenodo.1415662,Sally Jo Cunningham+University of Waikato>NZL>education;David Bainbridge+University of Waikato>NZL>education;Annette Falconer+Unknown>Unknown>Unknown,"This paper presents an analysis of how people construct playlists and mixes. Interviews with practitioners and postings made to a web site are analyzed using a grounded theory approach to extract themes and categorizations. The information sought is often encapsulated as music information retrieval tasks, albeit not as the traditional “known item search” paradigm. The collated data is analyzed and trends identified and discussed in relation to music information retrieval algorithms that could help support such activity.",NZL,education,Developed economies,"[-38.843838, 35.23448]","[33.53533, 31.82057]","[-6.3918233, 25.565268, -0.071790695]","[9.385296, 7.839389, 21.084112]","[15.765399, 8.388796]","[12.254483, 1.0407885]","[16.073654, 14.863779, -1.5953095]","[12.761155, 4.6957574, 12.41372]"
46,Alberto Novello;Martin F. McKinney;Armin Kohlrausch,Perceptual evaluation of music similarity.,2006,https://doi.org/10.5281/zenodo.1416700,Alberto Novello+Philips Research Laboratories>NLD>company;Martin F. McKinney+Philips Research Laboratories>NLD>company;Armin Kohlrausch+Philips Research Laboratories>NLD>company|Technische Universiteit Eindhoven>NLD>education,"This paper presents an empirical method for assessing music similarity on a set of stimuli using triadic comparisons in a balanced incomplete block design. We first evaluated the consistency of subjects in their rankings and then the concordance across subjects. The concordance was also evaluated for different subject populations to assess the influence of experience of the subject with the musical material. We finally analysed subjects’ ranking by the means of multidimensional scaling. Similarity judgments were found to be rather concordant across subjects. Significant differences between musicians and non-musicians and between subjects being familiar or non-familiar with the music were found for a small number of cases. Multidimensional scaling reveals a proximity of songs belonging to the same genre, congruent with the idea of genre being a perceptual dimension in subjects’ similarity ranking.",NLD,company,Developed economies,"[-3.1839359, 13.390887]","[21.117043, 2.2722342]","[-5.3065596, 7.574195, 1.117879]","[7.4989696, 6.8354664, 8.132403]","[12.975386, 9.321843]","[10.41791, 2.1273837]","[13.493246, 15.217128, -0.7112394]","[11.872117, 6.848067, 12.780636]"
47,Nuria Oliver;Lucas Kreger-Stickles,PAPA: Physiology and Purpose-Aware Automatic Playlist Generation.,2006,https://doi.org/10.5281/zenodo.1416794,Nuria Oliver+Microsoft Research>USA>company;Lucas Kreger-Stickles+Microsoft Research>USA>company,"In this paper we present PAPA, a novel approach for automatically generating playlists. The proposed framework utilizes the user’s physiological response to music, together with traditional song meta-data to generate a playlist the user will not only enjoy, but which will assist him or her in achieving various user-defined goals (“purpose”). In addition to outlining the generic framework, we present an exemplary application named MPTrain that (1) creates a playlist in real-time to assist users in achieving specific exercise goals; and (2) incorporates the user’s physiological response to the music to determine the next song to play.",USA,company,Developed economies,"[-40.933483, 39.15985]","[38.49261, 24.855764]","[-1.8694364, 30.465174, -0.91004515]","[-3.0888228, 20.540232, 18.917908]","[16.199104, 8.167492]","[12.067957, 1.550468]","[16.55217, 14.833259, -1.7339576]","[13.057298, 5.0733714, 13.048799]"
26,Michael A. Casey;Malcolm Slaney,Song Intersection by Approximate Nearest Neighbor Search.,2006,https://doi.org/10.5281/zenodo.1417827,"Michael Casey+Goldsmiths College, University of London>GBR>education|Yahoo! Research Inc.>USA>company;Malcolm Slaney+Yahoo! Research Inc.>USA>company","We present new methods for computing inter-song similarities using intersections between multiple audio pieces. The intersection contains portions that are similar, when one song is a derivative work of the other for example, in two different musical recordings. To scale our search to large song databases we have developed an algorithm based on locality-sensitive hashing (LSH) of sequences of audio features called audio shingles. LSH provides an efficient means to identify approximate nearest neighbors in a high-dimensional feature space. We combine these nearest neighbor estimates, each a match from a very large database of audio to a small portion of the query song, to form a measure of the approximate similarity. We demonstrate the utility of our methods on a derivative works retrieval experiment using both exact and approximate (LSH) methods. The results show that LSH is at least an order of magnitude faster than the exact nearest neighbor method and that accuracy is not impacted by the approximate method.",GBR,education,Developed economies,"[5.4853086, 39.537663]","[22.31086, 12.837405]","[0.9034508, 9.01648, -19.15249]","[15.342356, -5.1091495, 6.0055175]","[15.96102, 10.967085]","[10.351795, 2.069789]","[12.863583, 17.059402, -0.51289076]","[11.798653, 6.334385, 12.506446]"
25,Steffen Pauws;Wim Verhaegh;Mark Vossen,Fast Generation of Optimal Music Playlists using Local Search.,2006,https://doi.org/10.5281/zenodo.1417050,Steffen Pauws+Philips Research Europe>NLD>company;Wim Verhaegh+Philips Research Europe>NLD>company;Mark Vossen+Philips Research Europe>NLD>company,"We present an algorithm for use in an interactive music system that automatically generates music playlists that fit the music preferences given by a user. To this end, we introduce a formal model, define the problem of automatic playlist generation (APG) and indicate its NP-hardness. We use a local search (LS) procedure based on simulated annealing (SA) to solve the APG problem. In order to employ this LS procedure, we introduce an optimization variant of the APG problem, which includes the definition of penalty functions and a neighborhood structure. To improve upon the performance of the standard SA algorithm, we incorporated three heuristics referred to as song domain reduction, partial constraint voting, and two-level neighborhood structure. In tests, LS performed better than a constraint satisfaction (CS) solution in terms of run time, scalability and playlist quality.",NLD,company,Developed economies,"[-37.1019, 38.83952]","[34.366463, 22.324594]","[-1.3305084, 24.991863, -1.3695008]","[14.106341, 2.3933125, 24.449587]","[15.974068, 8.164583]","[12.001091, 1.5828682]","[16.31961, 14.838427, -1.7622238]","[13.046026, 5.1599236, 13.20868]"
24,Kyogu Lee;Malcolm Slaney,Automatic Chord Recognition from Audio Using a HMM with Supervised Learning.,2006,https://doi.org/10.5281/zenodo.1415158,Kyogu Lee+Stanford University>USA>education|Center for Computer Research in Music and Acoustics>USA>facility;Malcolm Slaney+Yahoo! Research>USA>company,"In this paper, we propose a novel method for obtaining labeled training data to estimate the parameters in a supervised learning model for automatic chord recognition. To this end, we perform harmonic analysis on symbolic data to generate label files. In parallel, we generate audio data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to estimate model parameters. Experimental results show higher performance in frame-level chord recognition than the previous approaches.",USA,education,Developed economies,"[55.553524, -6.430756]","[-32.629505, 20.805328]","[27.85202, -10.778915, 17.187216]","[-26.747038, -5.676738, 0.38241157]","[6.703078, 8.70184]","[6.1237826, 3.6865177]","[11.882694, 10.336375, 2.1240115]","[9.734094, 8.94232, 12.245533]"
23,Özgür Izmirli,Audio Key Finding Using Low-Dimensional Spaces.,2006,https://doi.org/10.5281/zenodo.1415858,Özgür İzmirli+Connecticut College>USA>education,"This paper presents two models of audio key finding: a template based correlational model and a template based model that uses a low-dimensional tonal representation. The first model uses a confidence weighted correlation to find the most probable key. The second model is distance based and employs dimensionality reduction to the tonal representation before generating a key estimate. Experiments to determine the dependence of key finding accuracy on dimensionality are presented. Results show that low dimensional representations, compared to commonly used 12 dimensions, may be utilized for key finding without sacrificing accuracy. The first model’s independently verified performance enabled it to be used as a benchmark for evaluation of the second model. Key finding accuracies for both models are given together with detailed results of the second model’s performance as a function of the number of dimensions used.",USA,education,Developed economies,"[32.48543, 15.216481]","[-5.378216, -3.4077523]","[8.3975315, -12.847023, 8.4638405]","[6.7681127, -4.2507358, -7.485944]","[10.764991, 7.534789]","[7.4103155, 2.7228696]","[12.122335, 13.1442, 0.31026357]","[10.263325, 8.247397, 11.804575]"
2,Jenn Riley;Constance A. Mayer,Ask a Librarian: The Role of Librarians in the Music Information Retrieval Community.,2006,https://doi.org/10.5281/zenodo.1414986,Jenn Riley+Indiana University>USA>education;Constance A. Mayer+University of Maryland>USA>education,"Participation from music librarians has been sparse in the first six ISMIR conferences, despite many potential areas of common interest. This paper makes an argument for the benefit to both the library and Music IR communities of increased representation of librarians at ISMIR. An analysis of conference programs and primary publications of two music library organizations to determine topics from the library literature relevant to Music IR research is presented. A discussion follows of expertise music librarians could potentially contribute to Music IR research and the ways in which Music IR research could further the work of music librarians, in each of the topics represented in the library literature.",USA,education,Developed economies,"[-24.278755, 24.164932]","[27.555635, 39.35222]","[-17.991928, 7.845321, -11.130648]","[1.9492255, 6.499354, 22.542158]","[14.588328, 7.8693237]","[11.91133, 0.28938115]","[14.573595, 14.654002, -2.194911]","[12.215678, 4.1788635, 12.186305]"
3,Xiao Hu 0001;J. Stephen Downie;Andreas F. Ehmann,Exploiting Recommended Usage Metadata: Exploratory Analyses.,2006,https://doi.org/10.5281/zenodo.1415998,Xiao Hu+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education,"In this paper, we conduct a series of exploratory analyses on the user-recommended usages of music as generated by 1,042 reviewers who have posted to www.epinions.com. Using hierarchical clustering methods on data derived from the co-occurrence analyses of usage and genre, usage and artist, and usage and album, we are able to conclude that further investigation of user-recommended usage metadata is warranted, especially with regard to its implications for future iterations of the Music Information Retrieval Evaluation eXchange (MIREX).",USA,education,Developed economies,"[-23.680311, 43.876873]","[35.42627, 27.505322]","[-28.35218, -1.095003, -10.187941]","[7.6406565, 6.364427, 17.425756]","[14.536081, 9.016575]","[12.315112, 1.2340397]","[15.281309, 13.460154, -1.2489197]","[12.993661, 4.7655725, 12.393075]"
4,Jarno Seppänen;Antti J. Eronen;Jarmo Hiipakka,Joint Beat & Tatum Tracking from Music Signals.,2006,https://doi.org/10.5281/zenodo.1417319,Jarno Seppänen+Nokia Research Center>FIN>company;Antti Eronen+Nokia Research Center>FIN>company;Jarmo Hiipakka+Nokia Research Center>FIN>company,"This paper presents a method for extracting two key metrical properties, the beat and the tatum, from acoustic signals of popular music. The method is computationally very efficient while performing comparably to earlier methods. High efficiency is achieved through multirate accent analysis, discrete cosine transform periodicity analysis, and phase estimation by adaptive comb filtering. During analysis, the music signals are first represented in terms of accentuation on four frequency subbands, and then the accent signals are transformed into periodicity domain. Beat and tatum periods and phases are estimated in a probabilistic setting, incorporating primitive musicological knowledge of beat–tatum relations, the prior distributions, and the temporal continuities of beats and tatums. In an evaluation with 192 songs, the beat tracking accuracy of the proposed method was found comparable to the state of the art. Complexity evaluation showed that the computational cost is less than 1% of earlier methods. The authors have written a real-time implementation of the method for the S60 smartphone platform.",FIN,company,Developed economies,"[33.221077, -34.43871]","[-24.596767, -3.2813823]","[8.893642, -31.124718, -10.076238]","[-1.1702361, 7.957875, -6.795498]","[10.45393, 4.30001]","[5.4481373, 2.0111794]","[10.262962, 12.884701, -2.2990355]","[7.6733656, 7.1655607, 11.083489]"
5,Nick Whiteley;Ali Taylan Cemgil;Simon J. Godsill,Bayesian Modelling of Temporal Structure in Musical Audio.,2006,https://doi.org/10.5281/zenodo.1415138,Nick Whiteley+University of Cambridge>GBR>education|Signal Processing Group>Unknown>Unknown;A. Taylan Cemgil+University of Cambridge>GBR>education|Signal Processing Group>Unknown>Unknown;Simon Godsill+University of Cambridge>GBR>education|Signal Processing Group>Unknown>Unknown,"This paper presents a probabilistic model of temporal structure in music which allows joint inference of tempo, meter and rhythmic pattern. The framework of the model naturally quantifies these three musical concepts in terms of hidden state-variables, allowing resolution of otherwise apparent ambiguities in musical structure. At the heart of the system is a probabilistic model of a hypothetical ‘bar-pointer’ which maps an input signal to one cycle of a latent, periodic rhythmical pattern. The system flexibly accommodates different input signals via two observation models: a Poisson points model for use with MIDI onset data and a Gaussian process model for use with raw audio signals. The discrete state-space permits exact computation of posterior probability distributions for the quantities of interest. Results are presented for both observation models, demonstrating the ability of the system to correctly detect changes in rhythmic pattern and meter, whilst tracking tempo.",GBR,education,Developed economies,"[20.179846, -3.601909]","[-19.357622, -3.4563375]","[0.6641678, -3.8222778, 6.218984]","[-2.136406, 18.160341, -11.196992]","[10.765124, 7.994981]","[6.064806, 2.0146098]","[12.138862, 13.167712, -0.23314665]","[8.134784, 6.9112587, 11.126653]"
6,Frank Kurth;Thorsten Gehrmann;Meinard Müller,The Cyclic Beat Spectrum: Tempo-Related Audio Features for Time-Scale Invariant Audio Identification.,2006,https://doi.org/10.5281/zenodo.1414980,Frank Kurth+University of Bonn>DEU>education;Thorsten Gehrmann+University of Bonn>DEU>education;Meinard Müller+University of Bonn>DEU>education,"In this paper, we present a novel set of tempo-related audio features for applications in audio retrieval. As opposed to existing feature sets commonly used in the retrieval domain which mainly focus on local spectral characteristics of the audio signal, our features capture its local temporal behaviour w.r.t. tempo, rhythm, and meter. As a key component to obtaining a high level of feature robustness we introduce the cyclic beat spectrum (CBS) consisting of residual tempo classes which are constructed similarly to the well-known pitch chroma classes. We illustrate the use of the newly constructed features by applying them to robust time-scale invariant audio identification.",DEU,education,Developed economies,"[37.51477, -27.14742]","[11.666878, -13.095233]","[0.12512797, -30.054092, -5.2270484]","[13.863087, 0.06494869, -5.1624293]","[11.321536, 4.5124283]","[9.138594, 3.2916498]","[10.826295, 13.341199, -2.5803218]","[10.916995, 7.481031, 10.791368]"
7,Donald Byrd;Megan Schindele,Prospects for Improving OMR with Multiple Recognizers.,2006,https://doi.org/10.5281/zenodo.1418307,Donald Byrd+Indiana University>USA>education|Indiana University>USA>education;Megan Schindele+Indiana University>USA>education,"OMR (Optical Music Recognition) programs have been available for years, but they still leave much to be desired in terms of accuracy. We studied the feasibility of achieving substantially better accuracy by using the output of several programs to “triangulate” and get better results than any of the individual programs; this multiple-recognizer approach has had some success with other media but, to our knowledge, has never been tried for music. A major obstacle is that the complexity of music notation is such that evaluating OMR accuracy is difficult for any but the simplest music. Nonetheless, existing programs have serious enough limitations that the multiple-recognizer approach is promising.",USA,education,Developed economies,"[33.350803, 31.471025]","[-20.288746, 37.515003]","[5.3092456, -21.943335, 22.405916]","[-11.334678, -20.922577, 3.4971876]","[9.69578, 6.1833677]","[6.6240273, -0.53903437]","[12.674628, 11.28004, -1.5465221]","[7.9705076, 4.1976757, 10.611982]"
9,Laurent Pugin,Optical Music Recognitoin of Early Typographic Prints using Hidden Markov Models.,2006,https://doi.org/10.5281/zenodo.1416974,Laurent Pugin+McGill University>CAN>education,"Music printed with movable type (typographic music) from the 16th and 17th centuries contains specific graphic features. In this paper, we present a technique and associated experiments for performing optical music recognition on such music prints using Hidden Markov Models (HMM). Our original approach avoids the difficult and unreliable removal of staff lines usually required before processing. The modeling of symbols on the staff is based on low-level simple features. We show that, using our technique, these features are robust enough to obtain good recognition rates even with poor quality images scanned from microfilm of originals. The music content retrieved by the optical recognition process can be put to significant use in, for example, the creation of searchable digital music libraries.",CAN,education,Developed economies,"[36.361282, 23.548117]","[-22.828253, 40.394512]","[18.452568, 13.083291, 5.0296183]","[-11.681649, -25.294094, 0.5941438]","[8.873737, 6.232582]","[6.7308197, -0.6871327]","[10.912238, 11.371696, -0.23235679]","[7.845184, 4.077664, 10.558623]"
10,Søren Tjagvad Madsen;Gerhard Widmer,Separating voices in MIDI.,2006,https://doi.org/10.5281/zenodo.1414752,Søren Tjagvad Madsen+Austrian Research Institute for Artificial Intelligence>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education,This paper presents an algorithm for converting midi events into logical voices. The algorithm is fundamentally based on the pitch proximity principle. New heuristics are introduced and evaluated in order to handle unsolved situations. The algorithm is tested on ground truth data: inventions and fugues by J. S. Bach. Due to its left to right processing it also runs on real time input.,AUT,facility,Developed economies,"[2.1829488, -45.694653]","[-11.009871, -9.392331]","[25.668936, 3.2242358, -2.773579]","[-1.887794, -12.8009615, -3.0101993]","[8.722176, 10.373386]","[7.0156083, 2.2430644]","[10.920406, 14.025207, 1.5206945]","[9.015349, 6.96128, 10.958786]"
11,David Rizo;Pedro J. Ponce de León;Carlos Pérez-Sancho;Antonio Pertusa;José Manuel Iñesta Quereda,A Pattern Recognition Approach for Melody Track Selection in MIDI Files.,2006,https://doi.org/10.5281/zenodo.1417419,David Rizo+Universidad de Alicante>ESP>education;Pedro J. Ponce de León+Universidad de Alicante>ESP>education;Carlos Pérez-Sancho+Universidad de Alicante>ESP>education;Antonio Pertusa+Universidad de Alicante>ESP>education;José M. Iñesta+Universidad de Alicante>ESP>education,"Standard MIDI files contain data that can be considered as a symbolic representation of music (a digital score), and most of them are structured as a number of tracks. One of them usually contains the melodic line of the piece, while the other tracks contain accompaniment music. The goal of this work is to identify the track that contains the melody using statistical properties of the musical content and pattern recognition techniques. Finding that track is very useful for a number of applications, like speeding up melody matching when searching in MIDI databases or motif extraction, among others. First, a set of descriptors from each track of the target file are extracted. These descriptors are the input to a random forest classifier that assigns the probability of being a melodic line to each track. The track with the highest probability is selected as the one containing the melodic line of that MIDI file. Promising results have been obtained testing a number of databases of different music styles.",ESP,education,Developed economies,"[11.343516, -13.952648]","[8.238498, 5.1030912]","[0.3927505, -12.627406, 14.651331]","[0.49352106, -4.9154058, 4.2008014]","[11.050971, 7.144832]","[8.653625, 1.3530737]","[12.633945, 12.148133, -0.7676491]","[10.227842, 6.6018534, 12.204646]"
48,Douglas Turnbull;Luke Barrington;Gert R. G. Lanckriet,Modeling music and words using a multi-class naïve Bayes approach.,2006,https://doi.org/10.5281/zenodo.1415782,Douglas Turnbull+UC San Diego>USA>education;Luke Barrington+UC San Diego>USA>education;Gert Lanckriet+UC San Diego>USA>education,We propose a query-by-text system for modeling a heterogeneous data set of music and words. We quantitatively show that our system can both annotate a novel song with semantically meaningful words and retrieve relevant unlabeled songs from a database given a text-based query. We explain two feature extraction methods useful for summarizing the audio content of a song. We describe a supervised multi-class naïve Bayes model and compare two parameter estimation techniques. Our approach is influenced by recent computer vision research on the related tasks of image annotation and retrieval.,USA,education,Developed economies,"[-25.76868, -10.530651]","[36.768997, -1.9781302]","[-10.66101, 2.976669, 12.832645]","[21.262287, 5.495839, 2.4882002]","[12.64306, 10.608878]","[11.284272, 3.4606864]","[13.684847, 14.112258, 1.1303099]","[12.820821, 6.384317, 11.2599]"
12,Amaury Hazan;Maarten Grachten;Rafael Ramírez 0001,Evolving Performance Models by Performance Similarity: Beyond Note-to-note Transformations.,2006,https://doi.org/10.5281/zenodo.1417691,Amaury Hazan+Pompeu Fabra University>ESP>education|Music Technology Group>ESP>facility;Maarten Grachten+Spanish Council for Scientific Research (IIIA - CSIC)>ESP>facility|Artificial Intelligence Research Institute>ESP>facility;Rafael Ramirez+Pompeu Fabra University>ESP>education|Music Technology Group>ESP>facility,"This paper focuses on expressive music performance modeling. We induce a population of score-driven performance models using a database of annotated performances extracted from saxophone acoustic recordings of jazz standards. In addition to note-to-note timing transformations that are invariably introduced in human renditions, more extensive alterations that lead to insertions and deletions of notes are usual in jazz performance. In spite of this, inductive approaches usually treat these latter alterations as artifacts. As a first step, we integrate part of the alterations occurring in jazz performances in an evolutionary regression tree model based on strongly typed genetic programming (STGP). This is made possible (i) by creating a new regression data type that includes a range of melodic alterations and (ii) by using a similarity measurement based on an edit-distance fit to human performance similarity judgments. Finally, we present the results of both learning and generalization experiments using a set of standards from the Real Book.",ESP,education,Developed economies,"[1.5966165, 32.999866]","[-35.032036, 8.327878]","[-9.297463, -14.222993, -16.92126]","[-18.197098, 3.9732702, -3.0095534]","[12.793197, 6.8122215]","[7.586588, 3.5522823]","[12.823768, 13.080188, -1.8863395]","[9.177917, 6.311609, 10.767666]"
14,Matthias Robine;Mathieu Lagrange,Evaluation of the Technical Leval of Saxophone Performers by Considering the Evolution of Spectral Parameters of the Sound.,2006,https://doi.org/10.5281/zenodo.1416620,"Matthias Robine+SCRIME – LaBRI, Université Bordeaux 1>FRA>education|Unknown>Unknown>Unknown;Mathieu Lagrange+SCRIME – LaBRI, Université Bordeaux 1>FRA>education|Unknown>Unknown>Unknown","We introduce in this paper a new method to evaluate the technical level of a musical performer, by considering only the evolutions of the spectral parameters during one tone. The proposed protocol may be considered as front end for music pedagogy related softwares that intend to provide feedback to the performer. Although this study only considers alto saxophone recordings, the evaluation protocol intends to be as generic as possible and may surely be considered for wider range of classical instruments from winds to bowed strings.",FRA,education,Developed economies,"[6.4330835, -28.792534]","[-28.167562, 10.929377]","[13.136992, -11.76709, -5.677644]","[-10.236521, -6.0978, -1.1895895]","[9.061923, 7.4148655]","[6.8467793, 2.2963514]","[11.115687, 12.836628, 0.3858569]","[9.105485, 7.3356557, 11.271272]"
15,James Bergstra;Alexandre Lacoste;Douglas Eck,Predicting genre labels for artist using FreeDB.,2006,https://doi.org/10.5281/zenodo.1416448,James Bergstra+Université de Montréal>CAN>education;Alexandre Lacoste+Université de Montréal>CAN>education;Douglas Eck+Université de Montréal>CAN>education,"This paper explores the value of FreeDB as a source of genre and music similarity information. FreeDB is a public, dynamic, uncurated database for identifying and labelling CDs with album, song, artist and genre information. One quality of FreeDB is that there is high variance in, e.g., the genre labels assigned to a particular disc. We investigate here the ability to use these genre labels to predict a more constrained set of “canonical” genres as decided by the curated but private database AllMusic (i.e. multi-class learning). This work is relevant for study in music similarity: we present an automatic, data-driven method for embedding artists in a continuous space that corresponds to genre similarity judgements over a large population of music fans. At the same time, we observe that FreeDB is a valuable resource to researchers developing music classification algorithms; it serves as a reference for what music is popular over a large population, and provides relevant targets for supervised learning algorithms.",CAN,education,Developed economies,"[-33.91661, -7.4815264]","[34.370872, 4.017269]","[-20.71824, 6.3536806, 8.797399]","[14.957694, 7.2402973, 5.5260177]","[13.735465, 10.212236]","[11.400925, 2.8508518]","[14.547624, 14.323258, 0.40747303]","[12.84651, 6.209434, 11.884918]"
16,Jeremy Reed;Chin-Hui Lee,A Study on Music Genre Classification Based on Universal Acoustic Models.,2006,https://doi.org/10.5281/zenodo.1417255,Jeremy Reed+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education;Chin-Hui Lee+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education,"Classification of musical genres gives a useful measure of similarity and is often the most useful descriptor of a musical piece. Previous techniques to use hidden Markov models (HMMs) for automatic genre classification have used a single HMM to model an entire song or genre. This paper provides a framework to give finer segmentation of HMMs through acoustic segment modeling. Modeling each of these acoustic segments with an HMM builds a timbral dictionary in the same fashion that one would create a phonetic dictionary for speech. A symbolic transcription is created by finding the most likely sequence of symbols. These transcriptions then serve as inputs into an efficient text classifier utilized to provide a solution to the genre classification problem. This paper demonstrates that language-ignorant approaches provide results that are consistent with the current state-of-the-art for the genre classification problem. However, the finer segmentation potentially allows for “musical language”-based syntactic rules to enhance performance.",USA,education,Developed economies,"[-28.449165, -12.84552]","[8.391747, -12.875195]","[-15.551071, 3.4738626, 13.986037]","[8.971609, 2.1902566, -15.412748]","[12.877926, 10.834697]","[8.27017, 3.2801266]","[13.889413, 14.20752, 1.3472788]","[10.555165, 7.668016, 10.78419]"
17,Arie Livshin;Xavier Rodet,"The Significance of the Non-Harmonic ""Noise"" Versis the Harmonic Series for Musical Instrument Recognition.",2006,https://doi.org/10.5281/zenodo.1416538,Arie Livshin+IRCAM Centre Pompidou>FRA>facility;Xavier Rodet+IRCAM Centre Pompidou>FRA>facility,"Sound produced by musical instruments with definite pitch consists of the Harmonic Series and the non-harmonic Residual. It is common to treat the Harmonic Series as the main characteristic of the timbre of pitched musical instruments. But does the Harmonic Series indeed contain the complete information required for discriminating among different musical instruments? Could the non-harmonic Residual, the “noise”, be used all by itself for instrument recognition? The paper begins by performing musical instrument recognition with an extensive sound collection using a large set of feature descriptors, achieving a high instrument recognition rate. Next, using Additive Analysis/Synthesis, each sound sample is resynthesized using solely its Harmonic Series. These “Harmonic” samples are then subtracted from the original samples to retrieve the non-harmonic Residuals. Instrument recognition is performed on the resynthesized and the “Residual” sound sets. The paper shows that the Harmonic Series by itself is indeed enough for achieving a high instrument recognition rate; however, the non-harmonic Residuals by themselves can also be used for distinguishing among musical instruments, although with lesser success. Using feature selection, the best 10 feature descriptors for instrument recognition out of our extensive feature set are presented for the Original, Harmonic and Residual sound sets.",FRA,facility,Developed economies,"[32.485916, -18.585424]","[-10.298043, -0.32254636]","[21.150023, -7.6527543, -9.470331]","[9.746882, 1.11465, -10.494741]","[9.185924, 9.050377]","[8.551691, 3.7046196]","[11.140453, 13.052249, 0.41621652]","[10.368782, 7.718763, 10.340689]"
18,Cory McKay;Ichiro Fujinaga,Musical genre classification: Is it worth pursuing and how can it be improved?,2006,https://doi.org/10.5281/zenodo.1417417,Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Research in automatic genre classification has been producing increasingly small performance gains in recent years, with the result that some have suggested that such research should be abandoned in favor of more general similarity research. It has been further argued that genre classification is of limited utility as a goal in itself because of the ambiguities and subjectivity inherent to genre. This paper presents a number of counterarguments that emphasize the importance of continuing research in automatic genre classification. Specific strategies for overcoming current performance limitations are discussed, and a brief review of background research in musicology and psychology relating to genre is presented. Insights from these highly relevant fields are generally absent from discourse within the MIR community, and it is hoped that this will help to encourage a more multi-disciplinary approach to automatic genre classification in the future.",CAN,education,Developed economies,"[-30.221685, -14.275149]","[28.301487, -1.2590917]","[-19.102333, 2.474271, 13.586846]","[13.220562, 11.624988, 2.6296327]","[13.036434, 10.828401]","[10.890544, 3.2990618]","[13.960534, 14.297677, 1.3636507]","[12.517847, 6.0981555, 11.175875]"
19,Aggelos Pikrakis;Theodoros Giannakopoulos;Sergios Theodoridis,A computationally efficient speech/music discriminator for radio recordings.,2006,https://doi.org/10.5281/zenodo.1414862,Aggelos Pikrakis+University of Athens>GRC>education;Theodoros Giannakopoulos+University of Athens>GRC>education;Sergios Theodoridis+University of Athens>GRC>education,"This paper presents a speech/music discriminator for radio recordings, based on a new and computationally efficient region growing technique, that bears its origins in the field of image segmentation. The proposed scheme operates on a single feature, a variant of the spectral entropy, which is extracted from the audio recording by means of a short-term processing technique. The proposed method has been tested on recordings from radio stations broadcasting over the Internet and, despite its simplicity, has proved to yield performance results comparable to more sophisticated approaches.",GRC,education,Developed economies,"[-18.15894, -20.610365]","[-0.13075456, -1.8069571]","[6.6963663, -5.8052955, -27.13164]","[17.130926, -3.1363142, -13.118626]","[12.154249, 10.141089]","[8.182906, 2.9897583]","[12.925501, 13.858771, 0.90578747]","[10.35641, 7.6392794, 11.295332]"
20,Arthur Flexer;Fabien Gouyon;Simon Dixon;Gerhard Widmer,Probabilistic Combination of Features for Music Classification.,2006,https://doi.org/10.5281/zenodo.1415524,Arthur Flexer+Medical University of Vienna>AUT>education;Fabien Gouyon+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Simon Dixon+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,We describe an approach to the combination of music similarity feature spaces in the context of music classification. The approach is based on taking the product of posterior probabilities obtained from separate classifiers for the different feature spaces. This allows for a different influence of the classifiers per song and an overall classification accuracy improving those resulting from individual feature spaces alone. This is demonstrated by combining spectral and rhythmic similarity for classification of ballroom dance music.,AUT,education,Developed economies,"[-24.931229, -11.778238]","[16.296942, -8.05358]","[-11.743997, 1.3234833, 13.503503]","[6.579847, 9.612018, -6.126998]","[12.588085, 10.481409]","[9.513698, 3.2725966]","[13.642763, 14.016034, 0.9917633]","[11.273168, 7.273255, 10.926127]"
21,Geoffroy Peeters,Chroma-based estimation of musical key from audio-signal analysis.,2006,https://doi.org/10.5281/zenodo.1416420,"Geoffroy Peeters+Ircam - Sound Analysis/Synthesis Team, CNRS - STMS>FRA>facility","This paper deals with the automatic estimation of key (key-note and mode) of a music track from the analysis of its audio signal. Such a system usually relies on a succession of processes, each one making hypotheses about either the signal content or the music content: spectral representation, mapping to chroma, decision about the global key of the music piece. We review here the underlying hypotheses, compare them and propose improvements over current state of the art. In particular, we propose the use of a Harmonic Peak Subtraction algorithm as a front-end of the system and evaluate the performance of an approach based on hidden Markov models. We then compare our approach with other approaches in an evaluation using a database of 302 baroque, classical and romantic music tracks.",FRA,facility,Developed economies,"[31.890625, 12.927485]","[-6.7471647, -1.1858301]","[9.925794, -13.269217, 12.778001]","[2.8896587, -5.582142, -5.7728524]","[10.8742895, 7.473245]","[7.188609, 2.8951423]","[12.324162, 13.181874, 0.35313803]","[10.070349, 8.210353, 11.789035]"
22,Katy C. Noland;Mark B. Sandler,Key Estimation Using a Hidden Markov Model.,2006,https://doi.org/10.5281/zenodo.1415800,"Katy Noland+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility;Mark Sandler+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility","A novel technique to estimate the predominant key in a musical excerpt is proposed. The key space is modelled by a 24-state Hidden Markov Model (HMM), where each state represents one of the 24 major and minor keys, and each observation represents a chord transition, or pair of consecutive chords. The use of chord transitions as the observations models a greater temporal dependency between consecutive chords than would observations of single chords. The key transition and chord emission probabilities are initialised using the results of perceptual tests in order to reflect the human expectation of harmonic relationships. HMM parameters are then trained on a per-song basis using hand-annotated chord symbols, before the model for each song is decoded to give the likelihood of each key at each time frame. Examples of the algorithm as a segmentation technique are given, and its capability to estimate the overall key of a song is evaluated using a data set of 110 Beatles songs, of which 91% were correctly classified. An extension to include operation from audio data instead of chord symbols is planned, which will enable application to general music retrieval purposes.",GBR,education,Developed economies,"[30.793268, 15.814827]","[-6.8181825, -1.0159452]","[6.2094975, -13.329828, 10.547057]","[2.9377294, -5.8929524, -6.87617]","[10.751087, 7.549936]","[6.906662, 3.1283395]","[12.2120695, 13.141237, 0.22385493]","[9.81869, 8.44182, 11.9390955]"
13,Megan A. Winget,Heroic Frogs Save the Bow: Performing Musician's Annotation and Interaction Behavior with Written Music.,2006,https://doi.org/10.5281/zenodo.1417709,Megan Winget+University of Texas at Austin>USA>education,"Although there have been a number of fairly recent studies in which researchers have explored the information seeking and management behaviors of people interacting with musical retrieval systems, there have been very few published studies of the interaction and use behaviors of musicians themselves. The qualitative research study reported here seeks to correct this deficiency in the literature. Drawing on data collected from nearly 300 annotated parts representing 15 unique works, and 20 musician interviews, we make a number of functionality recommendations for constructive music digital library tool development. For example, all musicians annotate their written music, although this action seems to become more important as the musician becomes more skilled. Musicians’ annotations are comprehensible to anyone who can read music, and are valuable as records of interpretation, interaction, and performance. Musicians annotate at the note (rather than at the phrase or movement) level, their annotations are standardized and formal, and are largely non-text. Music digital libraries that cater to musicians should attempt to provide annotation tools that work at the micro level, and extend the symbolic language of the primary document. Furthermore, preserving the annotations for future use would prove valuable for performance students, professionals, and historians alike.",USA,education,Developed economies,"[-32.798065, 18.410498]","[29.35364, 37.6476]","[-19.496033, 12.885377, -1.6934688]","[-2.366164, 9.259367, 21.585695]","[13.967574, 8.963702]","[11.6582575, 0.32275555]","[14.573143, 14.134973, -1.1981069]","[12.057829, 4.4108567, 11.781139]"
49,Markus Schedl;Tim Pohle;Peter Knees;Gerhard Widmer,Assigning and Visualizing Music Genres by Web-based Co-Occurrence Analysis.,2006,https://doi.org/10.5281/zenodo.1415176,Markus Schedl+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility;Tim Pohle+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility;Peter Knees+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility,"We explore a simple, web-based method for predicting the genre of a given artist based on co-occurrence analysis, i.e. analyzing co-occurrences of artist and genre names on music-related web pages. To this end, we use the page counts provided by Google to estimate the relatedness of an arbitrary artist to each of a set of genres. We investigate four different query schemes for obtaining the page counts and two different probabilistic approaches for predicting the genre of a given artist. Evaluation is performed on two test collections, a large one with a quite general genre taxonomy and a quite small one with rather specific genres. Since our approach yields estimates for the relatedness of an artist to every genre of a given genre set, we can derive genre distributions which incorporate information about artists that cannot be assigned a single genre. This allows us to overcome the inflexible artist-genre assignment usually used in music information systems. We present a simple method to visualize such genre distributions with our Traveller’s Sound Player. Finally, we briefly outline how to adapt the presented approach to extract other properties of music artists from the web.",AUT,education,Developed economies,"[-36.861565, 7.1014876]","[38.065475, 6.4348516]","[-20.56067, 9.3211155, 11.450273]","[17.43655, 7.7656527, 7.289962]","[13.693909, 9.812688]","[11.804487, 2.7121549]","[14.619676, 14.604037, -0.22816873]","[13.009558, 5.9279532, 11.947425]"
50,Gijs Geleijnse;Jan H. M. Korst,Web-Based Artist Categorization.,2006,https://doi.org/10.5281/zenodo.1417579,Gijs Geleijnse+Philips Research>NLD>company;Jan Korst+Philips Research>NLD>company,"We present a novel approach in categorizing artists into subjective categories such as genre. We base our method on co-occurrences on the web, found with the Google search engine. A direct mapping between artists and categories proved to be unreliable. We use the categories mapped to closely related artists to obtain a more reliable mapping. The method is tested on a genre classification test set with convincing results. Moreover, mood categorization is explored using the same techniques.",NLD,company,Developed economies,"[-39.717976, 6.2511234]","[38.722042, 5.5715203]","[-23.851425, 8.674036, 9.440493]","[16.979, 9.807377, 6.02689]","[14.204677, 10.050568]","[11.790961, 2.8538167]","[15.026499, 14.760157, -0.014201318]","[13.05754, 5.9715824, 11.807804]"
51,Eleanor Selfridge-Field,Social Cognition and Melodic Persistence: Where Metadata and Content Diverge.,2006,https://doi.org/10.5281/zenodo.1417699,Eleanor Selfridge-Field+Stanford University>USA>education,"The automatic retrieval of members of a tune family from a database of melodies is potentially complicated by well documented divergences between textual metadata and musical content. We examine recently reported cases of such divergences in search of musical features which persist even when titles change or the melodies themselves vary. We find that apart from meter and mode, the rate of preservation of searchable musical features is low. Social and gestural factors appear to play a varying role in establishing the “melodic” identity of widely transmitted songs. The rapid growth of social computing bring urgency to better understanding the different ways in which “same” or “similar” can be defined.",USA,education,Developed economies,"[-33.99852, 19.219685]","[16.134184, 10.38361]","[-18.787823, 16.05226, -2.4726963]","[4.7499514, -1.583627, 9.01872]","[14.556439, 9.191682]","[9.648621, 1.504053]","[14.543299, 15.326565, -1.0932562]","[11.138848, 6.313234, 12.636078]"
76,Ian Leue;Özgür Izmirli,Tempo Tracking With a Periodicity Comb Kernel.,2006,https://doi.org/10.5281/zenodo.1416266,Ian Leue+Connecticut College>USA>education|Center for Arts and Technology>USA>education;Ozgur Izmirli+Connecticut College>USA>education|Center for Arts and Technology>USA>education,"Automatic tempo extraction and beat tracking from audio is an important ability, with many applications in music information retrieval. This paper describes a method for tempo tracking which builds on current research in the field. In this algorithm, an autocorrelation surface is calculated from the output of a spectral energy flux onset novelty function. The most salient repetition rate is calculated by cross-correlating dilations of a comb-like prototype spanning multiple frames and the autocorrelation surface. The method addresses tempo tracking through time to account for pieces with variable tempos. In order to compare the performance of our method on music with strong and weak percussive onsets we have evaluated it on both classical music with and without percussion and popular music with percussion. Additionally, beats are phase-aligned and superimposed on the signal for aural evaluation. Results show the comb kernel to be a useful feature in determining the correct beat level.",USA,education,Developed economies,"[41.341232, -29.599909]","[-25.905394, -3.7386198]","[1.7123841, -33.439583, -3.338322]","[-3.9922526, 8.806859, -8.2565365]","[11.284961, 4.3394504]","[5.496667, 1.9201936]","[10.7118225, 13.24762, -2.842918]","[7.7038393, 7.078938, 11.206221]"
77,Masataka Goto,AIST Annotation for the RWC Music Database.,2006,https://doi.org/10.5281/zenodo.1418125,Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"In this paper, we introduce our activities regarding the manual annotation of the musical pieces of the RWC Music Database. Although the RWC Music Database is widely used, its annotated descriptions are not widely available. We therefore annotated a set of music-scene descriptions consisting of the beat structure, melody line, and chorus sections. We call this AIST Annotation. We also manually synchronized standard MIDI files with the corresponding audio signals at the beat level. We hope that the AIST Annotation will contribute to further advances in the field of music information processing.",JPN,facility,Developed economies,"[-20.45356, 12.11654]","[-4.3493013, 16.830677]","[-17.211483, -3.8663437, -7.45271]","[-4.2204404, 0.5473091, 8.045774]","[13.246895, 7.4992614]","[8.952113, 1.9756529]","[14.262653, 13.74397, -1.3034407]","[10.1925125, 5.7853746, 11.353379]"
78,Matthew D. Hoffman;Perry R. Cook,"Feature-Based Synthesis: A Tool for Evaluating, Designing, and Interacting with Music IR Systems.",2006,https://doi.org/10.5281/zenodo.1417515,Matt Hoffman+Princeton University>USA>education|Princeton University>USA>education;Perry R. Cook+Princeton University>USA>education|Princeton University>USA>education,"We present a general framework for performing feature-based synthesis – that is, for producing audio characterized by arbitrarily specified sets of perceptually motivated, quantifiable acoustic features of the sort used in many music information retrieval systems.",USA,education,Developed economies,"[23.5662, 8.530835]","[-1.3479933, 20.509228]","[-2.0814312, 1.6521045, 23.876476]","[-7.7524776, -13.479651, 13.726113]","[13.005444, 7.3125806]","[9.015603, 2.8773756]","[13.828765, 12.432845, -0.46923882]","[10.687969, 6.656533, 10.897591]"
79,Ajay Kapur;Eric Singer,A Retrieval Approach for Human/Robotic Musical Performance.,2006,https://doi.org/10.5281/zenodo.1414894,Matt Hoffman+Princeton University>USA>education|Princeton University>USA>education;Perry R. Cook+Princeton University>USA>education|Princeton University>USA>education,"We present a general framework for performing feature-based synthesis – that is, for producing audio characterized by arbitrarily specified sets of perceptually motivated, quantifiable acoustic features of the sort used in many music information retrieval systems.",USA,education,Developed economies,"[-31.998049, 15.729628]","[-1.3478796, 20.509184]","[-5.9966636, 27.239357, 14.640775]","[-7.7525973, -13.479558, 13.7269]","[13.462517, 8.07335]","[9.025889, 2.8826482]","[13.644931, 13.9962225, -1.5659366]","[10.691861, 6.6607146, 10.911238]"
80,Òscar Celma;Pedro Cano;Perfecto Herrera,Search Sounds: An audio crawler focused on weblogs.,2006,https://doi.org/10.5281/zenodo.1415142,Oscar Celma+Universitat Pompeu Fabra>ESP>education;Pedro Cano+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"In this paper we present a focused audio crawler that mines audio weblogs (MP3 blogs). This source of semi-structured information contains links to audio files, plus some textual information that is referring to the media file. A retrieval system —that exploits the mined data— fetches relevant audio files related to user’s text query. Based on these results, the user can navigate and discover new music by means of content-based audio similarity. The system is available at: http://www.searchsounds.net.",ESP,education,Developed economies,"[-7.5541716, 32.243145]","[20.424603, 23.416927]","[-17.4623, -2.9457803, -18.184622]","[7.692736, -2.044034, 16.437841]","[13.898409, 7.714197]","[10.760486, 0.6882445]","[13.8482685, 14.413404, -2.237744]","[11.916198, 5.4206815, 12.587809]"
81,Elias Pampalk;Masataka Goto,MusicRainbow: A New User Interface to Discover Artists Using Audio-based Similarity and Web-based Labeling.,2006,https://doi.org/10.5281/zenodo.1417313,Elias Pampalk+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"In this paper we present MusicRainbow which is a simple interface for discovering artists where colors encode different types of music. MusicRainbow is based on a new audio-based approach to compute artist similarity. This approach scores 15 percentage points higher in a genre classification task than the similarity computed on track level. Using a traveling salesman algorithm, similar artists are mapped near each other on a circular rainbow. Furthermore, we present a new approach of combining this audio-based information with information from the web. In particular, we label the rainbow and summarize the artists with words extracted from web pages related to the artists. We use different vocabularies for different hierarchical levels and heuristics to select the most descriptive labels. We conclude with a discussion of the results. The first impressions are very promising.",JPN,facility,Developed economies,"[-38.72872, 7.79568]","[37.283997, 6.6356587]","[-21.730427, 9.47368, 6.324787]","[17.427992, 5.8316193, 6.4798813]","[14.060564, 9.758733]","[11.687662, 2.5809982]","[14.728858, 14.624526, -0.31361502]","[12.954153, 6.032485, 12.049308]"
82,Gijs Geleijnse;Jan H. M. Korst,Efficient Lyrics Extraction from the Web.,2006,https://doi.org/10.5281/zenodo.1415982,Gijs Geleijnse+Philips Research>NLD>company;Jan Korst+Philips Research>NLD>company,"We present a novel method to extract lyrics from the Web. The aim is to extract a set of multiple versions of the lyrics to a song. Lyrics can be identified within a text by a regular expression. We use a projection of a document to efficiently identify lyrics within the document by mapping it to a regular expression. We describe a method to cluster the multiple versions of the lyrics by filtering out erroneous texts such as lyrics to other songs. For reasons of efficiency, we do this by comparing fingerprints instead of the texts themselves.",NLD,company,Developed economies,"[-28.150295, -30.734476]","[34.38199, -11.977568]","[11.072585, 23.064629, -4.3803353]","[15.906672, -9.581724, 14.978788]","[11.415892, 11.776081]","[10.705769, 2.8847086]","[12.37027, 15.926544, 1.0799608]","[12.237893, 6.4830413, 11.650987]"
83,Jin Ha Lee;M. Cameron Jones;J. Stephen Downie,Factors Affecting Response Rates for Real-Life MIR Queries.,2006,https://doi.org/10.5281/zenodo.1416840,Jin Ha Lee+University of Illinois at Urbana-Champaign>USA>education;M. Cameron Jones+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,In this poster we present preliminary findings of an exploratory study of natural language music information queries posted to the Google Answers web site. We discuss the proportion of queries answered as a function of time and attempt to identify factors which affect the probability of a query being answered.,USA,education,Developed economies,"[-7.419464, 57.12528]","[33.61491, 29.557053]","[-35.534824, -0.84323645, -7.142129]","[8.996808, 5.691787, 24.006025]","[13.597251, 4.684323]","[12.054721, 0.98592716]","[15.096931, 11.085263, -1.5449713]","[12.504495, 4.8615923, 12.46236]"
84,Peter van Kranenburg,Composer attribution by quantifying compositional strategies.,2006,https://doi.org/10.5281/zenodo.1415062,Peter van Kranenburg+Utrecht University>NLD>education,"Taking a theory of musical style, developed by Leonard B. Meyer, as a starting point, an experiment is described in which statistical pattern recognition algorithms are used to characterize a particular musical style with respect to other styles. The resulting description can be used in authorship discussions. In the current study, a number of disputed organ works from the Bach catalog is used to illustrate the possibilities of this approach.",NLD,education,Developed economies,"[-33.549725, 3.7947645]","[15.071529, -29.901152]","[0.7706272, 13.217115, 19.900778]","[-1.5832187, -5.116069, 8.530973]","[12.759914, 8.800845]","[8.796734, 1.6154116]","[13.766152, 13.776614, -0.31063]","[10.3544035, 6.3854084, 12.239098]"
75,Margaret Cahill;Donncha O'Maidín,Assessing the Performance of Melodic Similarity Algorithms Using Human Judgments of Similarity.,2006,https://doi.org/10.5281/zenodo.1417997,Margaret Cahill+University of Limerick>IRL>education;Donncha Ó Maidín+University of Limerick>IRL>education,"This paper outlines a project to identify reliable algorithms for measuring melodic similarity by using melodies extracted from a piece of music in Theme and Variations form, for which human judgements of similarity have been gathered.",IRL,education,Developed economies,"[0.9529198, 17.378166]","[16.516245, 3.6883297]","[0.7743824, 6.4781694, 1.4912026]","[5.9049196, 2.1783047, 5.3865767]","[12.441137, 9.707343]","[9.481774, 1.8654382]","[12.805228, 15.481357, -0.75437915]","[11.284268, 7.024736, 13.060346]"
85,Jochen Schwenninger;Raymond Brueckner;Daniel Willett;Marcus E. Hennecke,Language Identification in Vocal Music.,2006,https://doi.org/10.5281/zenodo.1416574,Jochen Schwenninger+University of Ulm>DEU>education|Harman/Becker Automotive Systems>DEU>company;Raymond Brueckner+Harman/Becker Automotive Systems>DEU>company;Daniel Willett+Harman/Becker Automotive Systems>DEU>company;Marcus Hennecke+Harman/Becker Automotive Systems>DEU>company,"Language identiﬁcation is an important ﬁeld in spoken lan- guage processing. The identiﬁcation of the language sung or spoken in music, however, has attracted only minor attention so far. This, however, is an important task when it comes to categorizing, classifying and labelling of music data. In this paper, we review our efforts of transferring well- established techniques from spoken language identiﬁcation to the area of language identiﬁcation in music. We present results of distinguishing German and English sung modern music and propose and evaluate techniques designed for im- proving the classiﬁcation performance. These techniques involve limiting the classiﬁcation on song segments that ap- pear to have vocals and on frames that are not distorted by heavy beat onsets.",DEU,education,Developed economies,"[-10.212399, -32.276264]","[10.488531, -23.378878]","[14.908396, 14.8743515, -15.505049]","[11.029, -1.5258486, -19.711494]","[10.256255, 11.162969]","[8.723684, 3.323835]","[11.478935, 15.278203, 0.5672094]","[10.909325, 7.4405293, 10.175659]"
87,Audrey Laplante;J. Stephen Downie,Everyday Life Music Information-Seeking Behaviour of Young Adults.,2006,https://doi.org/10.5281/zenodo.1417957,Audrey Laplante+McGill University>CAN>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"This poster presents the preliminary results of an ongoing qualitative study on the everyday-life music information-seeking behaviour of young adults. The data were collected through in-depth interviews and analyzed following a grounded theory approach. The analysis showed a strong penchant for informal channels (e.g., friends, relative) and, conversely, a distrust of experts. It also emerged that music seeking was mostly motivated by curiosity rather than by actual information needs, which in turn explains why browsing is such a popular strategy.",CAN,education,Developed economies,"[-33.465557, 23.130373]","[39.373985, 32.70816]","[-18.722704, 15.88091, -7.4825797]","[8.313354, 12.575353, 21.100935]","[15.149038, 8.552894]","[12.855863, 0.9653345]","[15.038384, 15.057628, -1.6470147]","[13.239331, 4.2392097, 12.010419]"
88,Cynthia M. Grund,A Philosophical Wish List for Research in Music Information Retrieval.,2006,https://doi.org/10.5281/zenodo.1415190,Cynthia M. Grund+University of Southern Denmark>DNK>education,"Within a framework provided by the traditional trio consisting of metaphysics, epistemology and ethics, a first stab is made at a wish list for MIR-research from a philosophical point of view. Since the tools of MIR are equipped to study language and its use from a purely sonic standpoint, MIR research could result in another revealing revolution within the linguistic turn in philosophy.",DNK,education,Developed economies,"[-22.641603, 21.675602]","[20.93348, 46.781292]","[-16.187704, 7.609025, -6.811631]","[-6.771841, 10.215225, 20.566875]","[14.381683, 8.008389]","[11.70835, 0.3559345]","[14.45767, 14.775758, -1.980569]","[11.861848, 4.540002, 11.546637]"
89,Daniel McEnnis;Cory McKay;Ichiro Fujinaga,jAudio: Additions and Improvements.,2006,https://doi.org/10.5281/zenodo.1416838,Daniel McEnnis+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"jAudio is an application designed to extract features for use in a variety of MIR tasks. It eliminates the need for re-implementing existing feature extraction algorithms and provides a framework that greatly facilitates the development and deployment of new features. Three classes of features are presented and explained—features, metafeatures, and aggregators. A detailed description of jAudio’s dependency resolution algorithm is also discussed. Finally, ways in which jAudio can be embedded in and integrated with new systems are discussed, along with a description of jAudio’s ability to add new features or aggregators, potentially at runtime.",CAN,education,Developed economies,"[-12.802013, 39.42597]","[5.141898, 32.783535]","[-15.878847, -12.416098, -11.188147]","[-9.804248, 7.0033064, 22.057232]","[13.086213, 7.077781]","[10.883827, 0.8242418]","[13.908696, 13.221493, -1.7314712]","[11.424147, 5.1973457, 11.502681]"
90,Stephen Sinclair;Michael Droettboom;Ichiro Fujinaga,Lilypond for pyScore: Approaching a universal translator for music notation.,2006,https://doi.org/10.5281/zenodo.1418245,Stephen Sinclair+McGill University>CAN>education|Schulich School of Music>CAN>education;Michael Droettboom+Johns Hopkins University>USA>education;Ichiro Fujinaga+McGill University>CAN>education|Schulich School of Music>CAN>education,"Several languages for music notation have been defined in recent years. pyScore, a framework for translating between notation formats, and new module for it which can generate input for the LilyPond music engraving system are described. This shows the potential for developing pyScore into a “universal translator” for musical scores.",CAN,education,Developed economies,"[20.285198, 14.846684]","[0.43242228, 38.85162]","[-3.4440699, -10.203845, 21.832775]","[-13.231683, -6.2562265, 19.56]","[11.732661, 6.9227657]","[9.680995, 0.46514052]","[13.4514265, 12.176912, -1.119335]","[10.316252, 5.1829033, 11.792999]"
91,Elias Pampalk;Martin Gasser,An Implementation of a Simple Playlist Generator Based on Audio Similarity Measures and User Feedback.,2006,https://doi.org/10.5281/zenodo.1415130,Elias Pampalk+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"This paper presents an implementation of a simple playlist generator. An audio-based music similarity measure and simple heuristics are used to create playlists given minimum user input. The ultimate goal of this work is to conduct a field study, i.e., to run the system on the users’ personal collection and study the usage behavior over a longer period of time. The functions include, for example, allowing the user to control the variance of the playlists in terms of how often the same song or songs from the same artists are repeated.",JPN,facility,Developed economies,"[-39.427242, 38.64096]","[34.75105, 21.026953]","[-3.2925944, 27.50834, -3.804641]","[14.38712, 2.4306126, 21.568008]","[16.149078, 8.202678]","[11.944742, 1.5505196]","[16.527721, 14.845839, -1.7295767]","[13.008249, 5.2193875, 13.100045]"
92,Rosemary Mountain,Name that mood! Describe that tune! Invitation to the IMP.,2006,https://doi.org/10.5281/zenodo.1418187,Rosemary Mountain+Concordia University>CAN>education,"The ongoing research project The Interactive Multimedia Playroom (IMP) was established to stimulate discourse about issues relating to our perception and description of sounds in artistic and multimedia contexts. Although it was originally conceived to help develop better analytical tools for music, the unique and playful design is well-adapted to helping establish common references for potential collaborators in media arts. As the team working on the project development includes experts in psychology as well as creative artists and theorists, the format of the project is being designed to maximize its transfer to psychological studies. Unlike most psychological studies, however, we are particularly interested in the reactions of those intimately involved in the arts, and ask participants to comment on the suitability of the terminology, perceived relevance of the questions, etc. It is believed that the issues being addressed by the project are fundamental ones which could have high relevance for MIR research, including descriptors, sound-image associations, and the recognition of salient characteristics of a musical excerpt.",CAN,education,Developed economies,"[-56.13723, 6.101969]","[31.596306, 36.718662]","[-15.340108, 25.823727, 3.1826017]","[0.17317669, 12.172923, 19.187946]","[13.582562, 12.47478]","[11.882997, 0.4841135]","[16.222649, 14.846236, 1.4030554]","[12.18142, 4.3856077, 11.642994]"
93,Youngmoo E. Kim;Donald S. Williamson;Sridhar Pilli,"Towards Quantifying the ""Album Effect"" in Artist Identification.",2006,https://doi.org/10.5281/zenodo.1415722,Youngmoo E. Kim+Drexel University>USA>education;Donald S. Williamson+Drexel University>USA>education;Sridhar Pilli+Drexel University>USA>education,"Recent systems for automatically identifying the performing artist from the acoustic signal of music have demonstrated reasonably high accuracy when discriminating between hundreds of known artists. A well-documented issue, however, is that the performance of these systems degrades when music from different albums is used for training and evaluation. Conversely, accuracy improves when systems are trained and evaluated using music from the same album. This performance characteristic has been labeled the “album effect”. The unfortunate corollary to this result is that the classification results of these systems are based not entirely on the music itself, but on other audio features common to the album that may be unrelated to the underlying music. We hypothesize that one of the primary reasons for this phenomenon is the production process of commercial recordings, specifically, post-production. Understanding the primary aspects of post-production, we can attempt to model its effect on the acoustic features used for classification. By quantifying and accounting for this transformation, we hope to improve future systems for automatic artist identification.",USA,education,Developed economies,"[-37.98495, 5.485786]","[20.77219, -10.111971]","[-24.023285, 5.581686, 9.466287]","[16.485598, 10.67615, -3.838615]","[13.928222, 10.130585]","[9.847408, 3.3313687]","[14.736206, 14.742542, 0.032654345]","[11.324619, 7.0244164, 10.622483]"
94,Janto Skowronek;Martin F. McKinney;Steven van de Par,Ground truth for automatic music mood classification.,2006,https://doi.org/10.5281/zenodo.1416696,Janto Skowronek+Philips Research Laboratories>NLD>company;Martin F. McKinney+Philips Research Laboratories>NLD>company;Steven van de Par+Philips Research Laboratories>NLD>company,"Automatic music classification based on audio signals provides a core technology for tools that help users to manage and browse their music collections. Since “mood” is also used as a browsing criterium, automatic mood classification could support the creation of the necessary metadata. We have developed a method to obtain a reliable “ground truth” database for automatic music mood classification. Our results confirm that excerpt selection is a non-trivial issue and that there are some mood labels that are relatively consistent across subjects.",NLD,company,Developed economies,"[-54.542057, 2.900502]","[52.035244, -5.05735]","[-19.530102, 24.62531, 5.9605656]","[12.534957, 21.07748, 8.447971]","[13.521916, 12.582578]","[13.14506, 3.7750483]","[16.124565, 14.890639, 1.4954404]","[14.2311945, 5.1338377, 10.711325]"
95,Adi Ruppin;Hezy Yeshurun,MIDI Music Genre Classification by Invariant Features.,2006,https://doi.org/10.5281/zenodo.1415744,Adi Ruppin+Tel Aviv University>ISR>education;Hezy Yeshurun+Tel Aviv University>ISR>education,"MIDI music genre classification methods are largely based on generic text classification techniques. We attempt to leverage music domain knowledge in order to improve classification results. We combine techniques of selection and extraction of musically invariant features with classification using compression distance similarity metric, which is an approximation of the theoretical, yet computationally intractable, Kolmogorov complexity. We introduce several methods for extracting features which are invariant under certain transformations commonly found in music. These methods, combined with data compression, generate a lossy compressed representation which attempts to preserve feature invariance. We analyze the performance of each method, thus gaining insight into the features that are significant to the human perception of music.",ISR,education,Developing economies,"[-25.972443, -14.039646]","[15.197552, -7.4682937]","[-11.399133, 4.727038, 16.1819]","[14.422428, 7.931165, -8.13932]","[12.658973, 10.685295]","[9.47474, 3.369142]","[13.521668, 13.831146, 1.2729816]","[11.20308, 7.3809953, 10.784576]"
86,Beinan Li;John Ashley Burgoyne;Ichiro Fujinaga,Extending Audacity for Audio Annotation.,2006,https://doi.org/10.5281/zenodo.1417963,Beinan Li+McGill University>CAN>education;John Ashley Burgoyne+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"By implementing a cached region selection scheme and automatic label completion, we extended an open-source audio editor to become a more convenient audio annotation tool for tasks such as ground-truth annotation for audio and music classification. A usability experiment was conducted with encouraging preliminary results.",CAN,education,Developed economies,"[-9.441479, -19.220705]","[33.639748, -6.3151507]","[-1.3555088, -10.20501, -20.145632]","[26.812214, 12.647628, -0.64259505]","[12.413763, 7.7613606]","[10.562502, 3.6714106]","[13.781067, 13.503695, -0.1585864]","[12.146504, 6.1445427, 10.888215]"
1,Daniel McEnnis;Cory McKay;Ichiro Fujinaga,Overview of OMEN.,2006,https://doi.org/10.5281/zenodo.1418171,Daniel McEnnis+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper introduces OMEN (On-demand Metadata Extraction Network), which addresses a fundamental problem in MIR: the lack of universal access to a large dataset containing significant amounts of copyrighted music. This is accomplished by utilizing the large collections of digitized music available at many libraries. Using OMEN, libraries will be able to perform on-demand feature extraction on site, returning feature values to researchers instead of providing direct access to the recordings themselves. This avoids copyright difficulties, since the underlying music never leaves the library that owns it. The analysis is performed using grid-style computation on library machines that are otherwise under-used (e.g., devoted to patron web and catalogue use).",CAN,education,Developed economies,"[32.317486, 32.44566]","[17.293812, 31.35056]","[3.5668833, -23.198832, 24.593733]","[-2.5510373, 2.0486467, 16.514133]","[9.770654, 6.1821632]","[11.175439, 0.5350144]","[12.716999, 11.296216, -1.5747123]","[11.6737995, 5.058025, 11.71363]"
74,Morteza Dehghani;Andrew M. Lovett,Efficient Genre Classification using Qualitative Representations.,2006,https://doi.org/10.5281/zenodo.1416964,Morteza Dehghani+Northwestern University>USA>education|Northwestern University>USA>education;Andrew M. Lovett+Northwestern University>USA>education|Northwestern University>USA>education,We have constructed a system that can compute a qualitative representation of music from high-level features extracted from MusicXML files. We use two cognitively motivated computational models called SME and SEQL to build generalizations of musical genres from these representations. We then categorize novel music pieces according to the generalizations. We demonstrate the feasibility of the system with training sets much smaller than those used in previous systems.,USA,education,Developed economies,"[-31.44896, -10.758041]","[-8.895387, 20.246567]","[-20.179073, 3.944506, 17.464031]","[-9.450432, -0.8443713, 8.324758]","[13.105604, 10.8794565]","[9.013612, 2.188023]","[14.021227, 14.3148, 1.3837283]","[10.364806, 6.156473, 11.462875]"
72,Matthias Eichner;Matthias Wolff;Rüdiger Hoffmann,Instrument classification using Hidden Markov Models.,2006,https://doi.org/10.5281/zenodo.1414960,Matthias Eichner+Technische Universität Dresden>DEU>education;Matthias Wolff+Technische Universität Dresden>DEU>education;Rüdiger Hoffmann+Technische Universität Dresden>DEU>education,"In this paper we present first results on musical instrument classification using an HMM based recognizer. The final goal of our work is to automatically evaluate instruments and to classify them according to their characteristics. The first step in this direction was to train a system that is able to recognize a particular instrument among others of the same kind (e.g. guitars). The recognition is based on solo music pieces played on the instrument under various conditions. For this purpose a database was designed and is currently being recorded that comprises four instrument types: classical guitar, violin, trumpet and clarinet. We briefly describe the classifier and give first experimental results on the classification of acoustic guitars.",DEU,education,Developed economies,"[8.599194, -23.926073]","[-10.6779, 2.141451]","[15.899555, -6.133442, 0.29586723]","[9.001897, -0.615292, -12.305401]","[8.858236, 7.076485]","[8.534268, 3.626645]","[11.055168, 12.45608, 0.38976982]","[10.33122, 7.6461973, 10.326243]"
52,David Temperley,A Probabilistic Model of Melody Perception.,2006,https://doi.org/10.5281/zenodo.1414988,David Temperley+Eastman School of Music>USA>education,"This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. (A “melody” is defined here as a sequence of pitches, without rhythmic information.) The model uses Bayesian reasoning. A generative probabilistic model is proposed, based on three principles: 1) melodies tend to remain within a narrow pitch range; 2) note-to-note intervals within a melody tend to be small; 3) notes tend to conform to a distribution (or “key-profile”) that depends on the key. The model is tested in three ways: on a key-finding task, on a melodic expectation task, and on an error-detection task.",USA,education,Developed economies,"[8.270259, -6.1366196]","[7.9980135, -31.429453]","[10.922831, 10.571318, -1.5460858]","[-6.148884, -12.79869, 1.852892]","[10.538862, 9.736174]","[7.1684012, 2.7739794]","[11.511285, 15.001614, -0.7602093]","[9.739318, 7.6294904, 12.106407]"
53,Matija Marolt,A Mid-level Melody-based Representation for Calculating Audio Similarity.,2006,https://doi.org/10.5281/zenodo.1416252,Matija Marolt+University of Ljubljana>SVN>education,"We propose a mid-level melody-based representation that incorporates melodic, rhythmic and structural aspects of a music signal and is useful for calculating audio similarity measures. Most current approaches to music similarity use either low-level signal features, such as MFCCs that mostly capture timbral characteristics of music and contain little semantic information, or require symbolic representations, which are difficult to obtain from audio signals. The proposed mid-level representation is our attempt to bridge the gap between audio and symbolic domains by providing an integrated melodic, rhythmic and structural representation of music signals. The representation is based on a set of melodic fragments extracted from prominent melodic lines, it is beat-synchronous, which makes it independent of tempo variations and contains information on repetitions of short melodic phrases within the analyzed piece. We show how it can be calculated automatically from polyphonic audio signals and demonstrate its use for discovering melodic similarities between songs. We present results obtained by using the representation for finding different interpretations of songs in a music collection.",SVN,education,Developed economies,"[1.7591585, 15.468211]","[6.775359, 1.6600618]","[-1.7787046, 4.9033265, -2.8567872]","[5.482147, -4.187048, 2.9448965]","[12.501549, 9.621378]","[8.838195, 1.8177911]","[12.893661, 15.259666, -0.7594833]","[10.490426, 6.907542, 12.44846]"
54,Sigurdur Sigurdsson;Kaare Brandt Petersen;Tue Lehn-Schiøler,Mel Frequency Cepstral Coefficients: An Evaluation of Robustness of MP3 Encoded Music.,2006,https://doi.org/10.5281/zenodo.1417149,Sigurdur Sigurdsson+Technical University of Denmark>DNK>education;Kaare Brandt Petersen+Technical University of Denmark>DNK>education;Tue Lehn-Schiøler+Technical University of Denmark>DNK>education,"In large MP3 databases, files are typically generated with different parameter settings, i.e., bit rate and sampling rates. This is of concern for MIR applications, as encoding difference can potentially confound meta-data estimation and similarity evaluation. In this paper we will discuss the influence of MP3 coding for the Mel frequency cepstral coefficients (MFCCs). The main result is that the widely used subset of the MFCCs is robust at bit rates equal or higher than 128 kbits/s, for the implementations we have investigated. However, for lower bit rates, e.g., 64 kbits/s, the implementation of the Mel filter bank becomes an issue.",DNK,education,Developed economies,"[-15.864218, -34.893562]","[15.892379, 24.01422]","[14.512646, 18.655033, -21.901508]","[-1.6427454, 22.96932, 13.010641]","[11.986568, 7.9034486]","[10.097695, 2.403487]","[12.34072, 13.669074, 0.053801235]","[11.458536, 6.550879, 11.4984]"
55,Jerónimo Arenas-García;Jan Larsen;Lars Kai Hansen;Anders Meng,Optimal filtering of dynamics in short-time features for music organization.,2006,https://doi.org/10.5281/zenodo.1415078,Jerónimo Arenas-García+Technical University of Denmark>DNK>education;Jan Larsen+Technical University of Denmark>DNK>education;Lars Kai Hansen+Technical University of Denmark>DNK>education;Anders Meng+Technical University of Denmark>DNK>education,"There is an increasing interest in customizable methods for organizing music collections. Relevant music characterization can be obtained from short-time features, but it is not obvious how to combine them to get useful information. In this work, a novel method, denoted as the Positive Constrained Orthonormalized Partial Least Squares (POPLS), is proposed. Working on the periodograms of MFCCs time series, this supervised method finds optimal filters which pick up the most discriminative temporal information for any music organization task. Two examples are presented in the paper, the first being a simple proof-of-concept, where an altosax with and without vibrato is modelled. A more complex 11 music genre classification setup is also investigated to illustrate the robustness and validity of the proposed method on larger datasets. Both experiments showed the good properties of our method, as well as superior performance when compared to a fixed filter bank approach suggested previously in the MIR literature. We think that the proposed method is a natural step towards a customized MIR application that generalizes well to a wide range of different music organization tasks.",DNK,education,Developed economies,"[-9.7671385, -1.2066034]","[14.073665, -10.710103]","[-2.592651, -1.0060058, 7.8471684]","[6.754146, 5.7886553, -8.75227]","[11.993577, 8.389686]","[9.62567, 3.1675]","[13.022885, 13.718503, -0.35441098]","[11.484346, 7.1483564, 10.972911]"
56,Kazuyoshi Yoshii;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Hybrid Collaborative and Content-based Music Recommendation Using Probabilistic Model with Latent User Preferences.,2006,https://doi.org/10.5281/zenodo.1416826,Kazuyoshi Yoshii+Kyoto University>JPN>education|JSPS Research Fellow>JPN>Unknown;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazunori Komatani+Kyoto University>JPN>education;Tetsuya Ogata+Kyoto University>JPN>education;Hiroshi G. Okuno+Kyoto University>JPN>education,"This paper presents a hybrid music recommendation method that solves problems of two prominent conventional methods: collaborative filtering and content-based recommendation. The former cannot recommend musical pieces that have no ratings because recommendations are based on actual user ratings. In addition, artist variety in recommended pieces tends to be poor. The latter, which recommends musical pieces that are similar to users’ favorites in terms of music content, has not been fully investigated. This induces unreliability in modeling of user preferences; the content similarity does not completely reflect the preferences. Our method integrates both rating and content data by using a Bayesian network called an aspect model. Unobservable user preferences are directly represented by introducing latent variables, which are statistically estimated. To verify our method, we conducted experiments by using actual audio signals of Japanese songs and the corresponding rating data collected from Amazon. The results showed that our method outperforms the two conventional methods in terms of recommendation accuracy and artist variety and can reasonably recommend pieces even if they have no ratings.",JPN,education,Developed economies,"[-46.376957, 28.324303]","[39.08291, 15.167423]","[-7.321753, 24.491552, -13.782821]","[17.976044, 5.085262, 16.244747]","[15.994129, 9.234873]","[12.585581, 2.0047028]","[15.786779, 15.702495, -1.4705609]","[13.574366, 5.3202643, 12.707612]"
57,Masatoshi Hamanaka,Music Scope Headphones: Natural User Interface for Selection of Music.,2006,https://doi.org/10.5281/zenodo.1416566,Masatoshi Hamanaka+Japan Science and Technology Agency>JPN>facility|Presto>JPN>company;Seunghee Lee+University of Tsukuba>JPN>education,"This paper describes a novel audio only interface for selecting music which enables us to select songs without having to click a mouse. Using previous music players with normal headphones, we can hear only one song at a time and we thus have to play pieces individually to select the one we want to hear from numerous new music files, which involves a large number of mouse operations. The main advantage of our headphones is that they detect natural movements, such as the head or hand moving when users are listening to music and they can focus on a particular musical source that they want to hear. By moving their head left or right, listeners can hear the source from a frontal position as the digital compass detects the change in the direction they are facing. By looking up or down, the tilt sensor will detect the change in the face’s angle of elevation; they can better hear the source that is allocated to a more distant or closer position. By putting their hand behind their ear, listeners can adjust the focus sensor on the headphones to focus on a particular musical source that they want to hear.",JPN,facility,Developed economies,"[-21.006592, 28.818602]","[30.177286, 23.66364]","[-17.471104, 5.7716274, -20.275305]","[11.797679, -0.9456803, 26.142149]","[14.3529625, 7.394832]","[11.212612, 1.2980852]","[14.364748, 14.216941, -2.3871107]","[12.0130205, 5.1120653, 13.00521]"
58,Nik Corthaut;Sten Govaerts;Erik Duval,Moody Tunes: The Rockanango Project.,2006,https://doi.org/10.5281/zenodo.1418211,Nik Corthaut+K.U. Leuven>BEL>education|K.U. Leuven>BEL>education|K.U. Leuven>BEL>education;Sten Govaerts+K.U. Leuven>BEL>education|K.U. Leuven>BEL>education|K.U. Leuven>BEL>education;Erik Duval+K.U. Leuven>BEL>education|K.U. Leuven>BEL>education|K.U. Leuven>BEL>education,"Wouldn’t it be nice if we had a tool that could offer people the right music for a specific time and place? For HORECA (hotel, restaurants and cafés) businesses, providing appropriate music is often not just nice, but essential. Typically this boils down to music that matches a certain situation on desired atmospheres, this will be defined as a musical context (MC). The developed tool, a music player, meeting the specific needs of HORECA, allows creation and management of those contexts. The user creates a musical context by selecting a number of appropriate atmospheres and can fine-tune the context with additional musical properties. The atmospheres are defined by a group of music experts, composed of DJs, music teachers, musicians, etc., who also manually annotate the properties of all musical content. To assist the music experts, a specially developed tool allows them to categorise and annotate the songs and evaluate their results. We provide insight on how we constructed and implemented our metadata schema and look at some existing schemas. The evaluation shows the economic value of such a system in the specific context of a HORECA business.",BEL,education,Developed economies,"[-55.659115, 7.222948]","[24.078709, 24.016748]","[-15.471241, 23.312994, 1.5823611]","[2.6819744, 1.9004021, 10.854012]","[13.526388, 12.518884]","[10.999302, 1.1478461]","[16.144615, 14.891588, 1.3852558]","[12.385993, 4.931993, 11.613616]"
59,John F. Woodruff;Bryan Pardo;Roger B. Dannenberg,Remixing Stereo Music with Score-Informed Source Separation.,2006,https://doi.org/10.5281/zenodo.1414898,John Woodruff+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education;Roger Dannenberg+Carnegie Mellon University>USA>education,"Musicians and recording engineers are often interested in manipulating and processing individual instrumental parts within an existing recording to create a remix of the recording. When individual source tracks for a stereo mixture are unavailable, remixing is typically difficult or impossible, since one cannot isolate the individual parts. We describe a method of informed source separation that uses knowledge of the written score and spatial information from an anechoic, stereo mixture to isolate individual sound sources, allowing remixing of stereo mixtures without access to the original source tracks. This method is tested on a corpus of string quartet performances, artificially created using Bach four-part chorale harmonizations and sample violin, viola and cello recordings. System performance is compared in cases where the algorithm has knowledge of the score and those in which it operates blindly. The results show that source separation performance is markedly improved when the algorithm has access to a well-aligned score.",USA,education,Developed economies,"[6.7707515, -46.38485]","[-38.93513, -27.154627]","[30.929092, -2.058281, -1.5800312]","[-12.797133, -4.545239, -30.636423]","[8.335904, 9.974546]","[6.659785, 5.488208]","[10.988462, 13.593986, 1.7090154]","[9.7164135, 8.350065, 9.431689]"
60,Tue Lehn-Schiøler;Jerónimo Arenas-García;Kaare Brandt Petersen;Lars Kai Hansen,A Genre Classification Plug-in for Data Collection.,2006,https://doi.org/10.5281/zenodo.1416202,Tue Lehn-Schiøler+The Technical University of Denmark>DNK>education;Jerónimo Arenas-García+The Technical University of Denmark>DNK>education;Kaare Brandt Petersen+The Technical University of Denmark>DNK>education;Lars Kai Hansen+The Technical University of Denmark>DNK>education,This demonstration illustrates how the methods developed in the MIR community can be used to provide real-time feedback to music users. By creating a genre classifier plug-in for a popular media player we present users with relevant information as they play their songs. The plug-in can furthermore be used as a data collection platform. After informed consent from a selected set of users the plug-in will report on music consumption behavior back to a central server.,DNK,education,Developed economies,"[-31.471056, -10.878336]","[30.881327, 31.456362]","[-19.717014, 5.170065, 17.383059]","[6.309545, 8.866335, 15.989635]","[13.140713, 10.845488]","[11.873386, 0.9562156]","[14.005806, 14.281803, 1.3028613]","[12.584417, 4.5742517, 12.227916]"
73,Rudolf Mayer;Thomas Lidy;Andreas Rauber,The Map of Mozart.,2006,https://doi.org/10.5281/zenodo.1416060,Rudolf Mayer+Vienna University of Technology>AUT>education;Thomas Lidy+Vienna University of Technology>AUT>education;Andreas Rauber+Vienna University of Technology>AUT>education,"We present a study on using a Mnemonic Self-Organizing Map for clustering a very homogeneous collection of music. In particular, we create a map containing the complete works of Wolfgang Amadeus Mozart. We study and analyze the clustering capabilities of the SOM on this very focused collection. We furthermore present a web-based application for exploring the map and accessing the music it represents.",AUT,education,Developed economies,"[1.6975254, 24.96189]","[27.452099, 17.126871]","[-15.692581, -6.1322103, -1.3511281]","[15.834516, -7.928356, 23.5209]","[12.497279, 7.0227413]","[11.086661, 1.8995099]","[13.40905, 13.313964, -1.6891655]","[12.338178, 5.881267, 13.23496]"
61,Vegard Sandvold;Thomas Aussenac;Òscar Celma;Perfecto Herrera,Good Vibrations: Music Discovery through Personal Musical Concepts.,2006,https://doi.org/10.5281/zenodo.1418275,Vegard Sandvold+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Thomas Aussenac+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Oscar Celma+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"We present here Good Vibrations, a tool for music tagging, exploration and discovery, shaped as a media player plugin, and intended for home users. The plugin allows the quick “invention” of concepts and properties that can be tagged to songs. After some hours of active tagging, the plugin starts automatically proposing the proper tags to the user, who is also allowed to correct them. The plugin generates playlists according to the user-defined concepts, and recommends related music either from the user’s personal collection or from the Internet (through it’s connection to Foafing the Music). The plugin runs, for the moment, in Nullsoft Winamp on Windows XP systems.",ESP,education,Developed economies,"[-31.27045, 22.816446]","[31.623926, 22.617655]","[-15.805781, 14.256296, -7.2920403]","[15.192567, -5.1294203, 18.1962]","[14.662953, 8.229606]","[11.507313, 1.8105963]","[14.437742, 14.836155, -1.8123138]","[12.626312, 5.4703403, 12.4851465]"
63,Steven van de Par;Martin F. McKinney;André Redert,Musical Key Extraction from Audio Using Profile Training.,2006,https://doi.org/10.5281/zenodo.1417879,Steven van de Par+Philips Research Laboratories Eindhoven>NLD>company;Martin McKinney+Philips Research Laboratories Eindhoven>NLD>company;André Redert+Philips Research Laboratories Eindhoven>NLD>company,"A new method is presented for extracting the musical key from raw audio data. The method is based on the extraction of chromagrams using a new approach for tonal component selection taking into account auditory masking. The extracted chromagrams were used to train three key profiles for major and three key profiles for minor keys. The three trained key profiles differ in their temporal weighting of information across the duration of the song. One profile is based on uniform weighting while the other two apply emphasis on the beginning and ending of the song, respectively. The actual key extraction is based on comparing the key profiles with three average chromagrams that were extracted from a particular piece of music using the same temporal weighting functions as used for the key profile training. A correct key classification of 98% was achieved using non-overlapping test and training sets drawn from a larger set of 237 CD recordings of classical piano sonatas.",NLD,company,Developed economies,"[31.266462, 13.585787]","[-6.4832997, -3.481384]","[10.05937, -15.333583, 11.254714]","[7.634817, -5.033713, -5.736716]","[10.825189, 7.5051208]","[7.3820906, 2.7142472]","[12.236601, 13.149644, 0.28915107]","[10.249451, 8.2164135, 11.80549]"
64,Martijn Bosma;Remco C. Veltkamp;Frans Wiering,Muugle: A Modular Music Information Retrieval Framework.,2006,https://doi.org/10.5281/zenodo.1415916,Martijn Bosma+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education,Muugle (Musical Utrecht University Global Lookup Engine) is a modular framework that allows the comparison of different MIR techniques and usability studies. A system overview and a discussion of a pilot usability experiment are given. A demo version of the framework can be found on http://give-lab.cs.uu.nl/muugle.,NLD,education,Developed economies,"[-17.017412, 20.779875]","[22.025818, 39.90451]","[-10.578549, 3.7808807, -11.8134165]","[-0.16741857, 5.7900066, 17.15029]","[13.707579, 7.788404]","[11.704117, 0.72733456]","[13.902265, 14.296808, -1.8416058]","[12.137081, 4.51139, 12.227642]"
65,Jörg Garbers,An Integrated MIR Programming and Testing Environment.,2006,https://doi.org/10.5281/zenodo.1414864,Jörg Garbers+Utrecht University>NLD>education,"The process of shaping a music information retrieval algorithm is highly connected with implementing it and testing suitable parameterizations. Often music information retrieval scientists do not have a programmer at hand and must implement their experimental setup themselves. This paper describes an integrated tool setup OHR consisting of the music (analysis) systems OpenMusic, Humdrum and Rubato and a system for form based parametrization and comparison of algorithms. These packages and their programming environments provide the scientist with frameworks and existing libraries for implementing and testing algorithms. They differ in the programming languages that they support and in the type of testing user interfaces that they allow the scientist to build easily. The systems and their components are integrated by using their scripting languages. We sketch an example of the integrated use of these systems.",NLD,education,Developed economies,"[-9.825214, 58.88941]","[0.1423318, 31.775518]","[-38.16301, 1.6176962, -6.191226]","[-9.948097, -3.8304887, 16.160748]","[13.561894, 4.7605515]","[10.195612, 0.79627275]","[14.985019, 11.116335, -1.4967643]","[10.729629, 5.33249, 11.922282]"
66,Dirk Moelants;Olmo Cornelis;Marc Leman;Jos Gansemans;Rita M. M. De Caluwe;Guy De Tré;Tom Matthé;Axel Hallez,Problems and Opportunities of Applying Data- & Audio-Mining Techniques to Ethnic Music.,2006,https://doi.org/10.5281/zenodo.1417787,Dirk Moelants+Ghent University>BEL>education;Olmo Cornelis+Ghent University>BEL>education;Marc Leman+Ghent University>BEL>education;Jos Gansemans+Royal Museum of Central-Africa>BEL>facility;Rita De Caluwe+Ghent University>BEL>education;Guy De Tré+Ghent University>BEL>education;Tom Matthé+Ghent University>BEL>education;Axel Hallez+Ghent University>BEL>education,"Current research in music information retrieval focuses on Western music. In music from other cultures, both musical structures and thinking about music can be very different. This creates problems for both the analysis of musical features and the construction of databases. On the other hand, a well-documented digitization offers interesting opportunities for the study and spread of ‘endangered’ music. Here, some general problems regarding the digital indexation of ethnic music are given, illustrated with a method for describing pitch structure, comparing Western standards with African music found in the digitization of the archives of the Royal Museum of Central-Africa in Tervuren (Brussels).",BEL,education,Developed economies,"[-28.946482, 20.72741]","[15.035757, 21.290527]","[-22.50225, 11.14102, -5.1614566]","[-2.9131374, 16.1815, 4.569837]","[14.271965, 8.509419]","[9.859759, 1.0377257]","[14.497852, 14.841703, -1.5880762]","[11.065201, 5.7667284, 12.454557]"
67,Beatriz Magalhães Castro;Luiza Beth Nunes Alonso;Edilson Ferneda;Murilo Bastos da Cunha;Fernando William Cruz;Márcio da Costa P. Brandão,BDB-MUS: a project for the preservation of Brazilian musical heritage.,2006,https://doi.org/10.5281/zenodo.1414998,Beatriz Magalhães Castro+University of Brasilia>BRA>education;Luiza Beth Nunes Alonso+Catholic University of Brasilia>BRA>education;Edilson Ferneda+Catholic University of Brasilia>BRA>education;Murilo Bastos da Cunha+University of Brasilia>BRA>education;Fernando William Cruz+University of Brasilia>BRA>education;Márcio da Costa P. Brandão+University of Brasilia>BRA>education,"This poster proposes a discussion on concepts evolving from the role of digital libraries on the preservation of tangible and intangible cultural inheritance, including concepts developed in 2003 by UNESCO and the World Summit on the Information Society. It further describes the construction and design process leading to the development of BDB-MUS – Brazilian Digital Music Library, which aims to establish national recommendations on metadata attributions, and to develop means for appropriation and retrieval of musical sources. The poster further explores the concept of digital music or culture within the aims and objectives of the project.",BRA,education,Developing economies,"[-17.210724, 38.802208]","[23.645256, 35.2938]","[-22.147835, -9.352303, -9.31896]","[1.2205392, 2.7159953, 23.688126]","[14.298759, 8.196613]","[11.32674, 0.12284813]","[14.342075, 14.171487, -1.8917072]","[11.806864, 4.445837, 12.141708]"
68,Rebecca Fiebrink;Ichiro Fujinaga,Feature Selection Pitfalls and Music Classification.,2006,https://doi.org/10.5281/zenodo.1415144,Rebecca Fiebrink+McGill University>CAN>education|Unknown>Unknown>Unknown;Ichiro Fujinaga+McGill University>CAN>education|Unknown>Unknown>Unknown,"Previous work has employed an approach to the evaluation of wrapper feature selection methods that may overstate their ability to improve classification accuracy, because of a phenomenon akin to overfitting. This paper discusses this phenomenon in the context of recent work in machine learning, demonstrates that previous work in MIR has indeed exaggerated the efficacy of feature selection for music classification, and presents new testing providing a more realistic analysis of feature selection’s impact on music classification accuracy.",CAN,education,Developed economies,"[-25.986805, -12.254477]","[20.924257, -6.8420105]","[-14.149882, -1.094438, 14.603722]","[12.064228, 13.09662, -2.8566248]","[12.74207, 10.511578]","[9.850863, 3.1538417]","[13.736838, 14.033376, 1.0601852]","[11.560093, 6.910089, 10.889899]"
69,Shyamala Doraisamy;Hamdan Adnan;Noris Mohd. Norowi,Towards a MIR System for Malaysian Music.,2006,https://doi.org/10.5281/zenodo.1414744,Shyamala Doraisamy+University Putra Malaysia>MYS>education;Hamdan Adnan+National Arts Academy>Unknown>Unknown;Noris Mohd. Norowi+University Putra Malaysia>MYS>education,"Systems for the archival of musical documents digitally and development of digital music libraries are currently being researched and developed extensively. However, adapting these systems for the archival and retrieval of Malaysian music materials might not be as straightforward due to the distinct differences in musical structure and modes of non-Western music. This paper covers the motivations for the creation of a MIR system for Malaysian Music and outlines the plans for its development.",MYS,education,Developing economies,"[-12.874544, 56.563126]","[23.431353, 34.474316]","[-36.626537, 2.6524951, -1.1050019]","[1.7234181, 2.2106767, 22.045752]","[13.330813, 5.1441355]","[11.275188, 0.140782]","[14.710517, 11.452245, -1.3364981]","[11.7728405, 4.5595984, 12.229455]"
70,Frank Seifert 0001;Katharina Rasch;Michael Rentzsch,Tempo Induction by Stream-Based Evaluation of Musical Events.,2006,https://doi.org/10.5281/zenodo.1418181,Frank Seifert+University of Technology>DEU>education;Katharina Rasch+University of Technology>DEU>education;Michael Rentzsch+University of Technology>DEU>education,"We present an approach for tempo induction that is based on a more perception-oriented analysis of inter-onset intervals. Therefore we utilize auditory grouping concepts and define some rules for their formation. Finally, we show preliminary results that confirm our aim of improving the quality of tempo induction by reducing the amount of perceptually irrelevant data.",DEU,education,Developed economies,"[40.418484, -26.725729]","[-30.036165, -6.8872747]","[-2.3538418, -29.882303, -1.1772503]","[-5.285773, 11.167405, -12.591704]","[11.410652, 4.3986864]","[5.130836, 1.8804027]","[10.934732, 13.340813, -2.8187466]","[7.3586106, 6.963988, 10.930239]"
71,Kurt Jacobson,A Multifaceted Approach to Music Similarity.,2006,https://doi.org/10.5281/zenodo.1417016,Kurt Jacobson+University of Miami>USA>education,"Previous work has explored the concept of music similarity measures and a variety of methods have been proposed for calculating such measures. This paper describes a system for music similarity which attempts to model and compare some of the more musically salient features of a set of audio signals. A model for timbre and a model for rhythm are implemented directly from previous work, and a model for song structure is developed. The different models are weighted and combined to provide an overall music similarity measure. The system is tested on a small set of popular music files spanning eleven different genres. The system is tuned to estimate genre boundaries using multidimensional scaling – a technique that allows for quick visualization of similarity data. An “automatic DJ” application, that generates playlists based on the music similarity models, serves as a subjective evaluation for the system.",USA,education,Developed economies,"[-6.259793, 13.753317]","[22.535574, 6.700212]","[-5.3178954, 10.040112, -0.012027176]","[12.692421, -0.58319, 6.925924]","[13.275927, 9.388086]","[10.284018, 2.125317]","[13.724977, 15.030023, -0.64580935]","[11.781731, 6.6686788, 12.579]"
62,Chris Cannam;Christian Landone;Mark B. Sandler;Juan Pablo Bello,The Sonic Visualiser: A Visualisation Platform for Semantic Descriptors from Musical Signals.,2006,https://doi.org/10.5281/zenodo.1416388,"Chris Cannam+Centre for Digital Music, Queen Mary University of London>GBR>education;Christian Landone+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education;Juan Pablo Bello+Centre for Digital Music, Queen Mary University of London>GBR>education","Sonic Visualiser is the name for an implementation of a system to assist study and comprehension of the contents of audio data, particularly of musical recordings. It is a C++ application with a Qt4 GUI that runs on Windows, Mac, and Linux. It embodies a number of concepts which are intended to improve interaction with audio data and features, most notably with respect to the representation of time-synchronous information. The architecture of the application allows for easy integration of third party algorithms for the extraction of low and mid-level features from musical audio data. This paper describes some basic principles and functionalities of Sonic Visualiser.",GBR,education,Developed economies,"[-11.8043995, 33.604507]","[6.474507, 30.047089]","[-4.6501017, -9.970073, -17.294218]","[-6.682379, -6.8962584, 13.862396]","[13.238711, 6.9476986]","[10.315305, 1.2498922]","[13.72849, 13.390251, -2.1020465]","[10.978386, 5.5596743, 11.678199]"
0,Catherine Lai;Ichiro Fujinaga,Data Dictionary: Metadata for Phonograph Records.,2006,https://doi.org/10.5281/zenodo.1417423,Catherine Lai+McGill University>CAN>education|McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education|McGill University>CAN>education,"The creation and maintenance of a metadata data dictionary is essential to large-scale digital repositories. It assists the process of data entry, ensures consistency of records, facilitates semantic compatibility and interoperability between systems, and, most importantly, forms the foundation for efficient and effective information retrieval infrastructure. In this paper we explain in detail the necessity of metadata data dictionaries to digitization projects and digital library retrieval services. We also describe the development process of our Data Dictionary for phonograph records. We then present the underlying data model of our Data Dictionary and provide information about the meaning and use of semantic units defined in the Data Dictionary. We stress the usefulness of the generation and maintenance of our Data Dictionary for MIR as it provides a means to ensure accurate, consistent, and comprehensive metadata annotation. For maximum interoperability between systems, digital repositories not only need to agree on the same metadata fields, but also the meanings of the fields. To this end, we believe our Data Dictionary is the cornerstone of optimal retrieval of music information about phonograph records.",CAN,education,Developed economies,"[-6.8923335, 29.656149]","[19.468988, 33.511936]","[-22.650244, -1.9193443, -10.07146]","[-2.4300604, -2.7322588, 21.401293]","[14.189884, 8.835305]","[10.981232, 0.14173557]","[14.869733, 13.48874, -1.2921144]","[11.784391, 4.839963, 11.999034]"
8,David Bainbridge 0001;Tim Bell,Identifying music documents in a collection of images.,2006,https://doi.org/10.5281/zenodo.1416424,David Bainbridge+University of Waikato>NZL>education;Tim Bell+University of Canterbury>NZL>education,"Digital libraries and search engines are now well-equipped to find images of documents based on queries. Many images of music scores are now available, often mixed up with textual documents and images. For example, using the Google “images” search feature, a search for “Beethoven” will return a number of scores and manuscripts as well as pictures of the composer. In this paper we report on an investigation into methods to mechanically determine if a particular document is indeed a score, so that the user can specify that only musical scores should be returned. The goal is to find a minimal set of features that can be used as a quick test that will be applied to large numbers of documents. A variety of filters were considered, and two promising ones (run-length ratios and Hough transform) were evaluated. We found that a method based around run-lengths in vertical scans (RL) that out-performs a comparable algorithm using the Hough transform (HT). On a test set of 1030 images, RL achieved recall and precision of 97.8% and 88.4% respectively while HT achieved 97.8% and 73.5%. In terms of processor time, RL was more than five times as fast as HT.",NZL,education,Developed economies,"[-13.561459, 25.822922]","[19.609413, 16.378931]","[-5.6838584, 10.262302, -18.629614]","[13.681995, -9.797776, 10.440788]","[13.611007, 7.6836925]","[8.865335, 0.19681825]","[13.63572, 14.30987, -2.1099315]","[10.680796, 5.4775767, 13.040778]"
0,François Deliège;Torben Bach Pedersen,Fuzzy Song Sets for Music Warehouses.,2007,https://doi.org/10.5281/zenodo.1415770,François Deliège+Aalborg University>DNK>education|Unknown>Unknown>Unknown;Torben Bach Pedersen+Aalborg University>DNK>education|Unknown>Unknown>Unknown,"The emergence of music recommendation systems calls for the development of new data management technologies able to query vast music collections. In this paper, we define fuzzy song sets and an algebra to manipulate them. We present a music warehouse prototype able to perform efficient nearest neighbor searches in an arbitrary song similarity space. Using fuzzy song sets, the music warehouse offers a practical solution to the all musical data management scenarios provided: song comparisons, user musical preferences and user feedback. We investigate three practical approaches to tackle the storage issues of fuzzy song sets: tables, arrays and bitmaps. Finally, we confront theoretical estimates to concrete implementation results and prove that, from a storage perspective, arrays and bitmaps are both effective data structure solutions.",DNK,education,Developed economies,"[-17.161623, 24.022238]","[25.944395, 21.361166]","[-12.36183, 2.92969, -16.85595]","[10.3886795, 1.2943171, 20.80225]","[14.1377325, 7.6405277]","[11.281138, 1.3206971]","[13.929293, 14.611711, -2.2144828]","[12.263239, 5.268213, 12.981421]"
95,Chuan Cao;Ming Li 0026;Jian Liu;Yonghong Yan 0002,Singing Melody Extraction in Polyphonic Music by Harmonic Tracking.,2007,https://doi.org/10.5281/zenodo.1414708,"Chuan Cao+Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences>CHN>facility;Ming Li+Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences>CHN>facility;Jian Liu+Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences>CHN>facility;Yonghong Yan+Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences>CHN>facility","This paper proposes an effective method for automatic melody extraction in polyphonic music, especially vocal melody songs. The method is based on subharmonic summation spectrum and harmonic structure tracking strategy. Performance of the method is evaluated using the LabROSA database. The pitch extraction accuracy of our method is 82.2% on the whole database, while 79.4% on the vocal part.",CHN,facility,Developing economies,"[-0.6235801, -31.463024]","[-2.9838474, -9.786326]","[17.786463, 4.334158, -6.7955184]","[6.6098313, -8.099797, -12.972528]","[9.81225, 10.128897]","[6.684361, 2.6613903]","[11.001571, 14.714483, -0.08709297]","[9.104203, 8.08474, 11.118785]"
94,Yasunori Ohishi;Masataka Goto;Katunobu Itou;Kazuya Takeda,A Stochastic Representation of the Dynamics of Sung Melody.,2007,https://doi.org/10.5281/zenodo.1414732,Yasunori Ohishi+Nagoya University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Katunobu Itou+Hosei University>JPN>education;Kazuya Takeda+Nagoya University>JPN>education,"In this paper, we propose a stochastic representation of a sung melodic contour, called stochastic phase representation (SPR), which can characterize both musical-note information and the dynamics of singing behaviors included in the melodic contour. The SPR is constructed by fitting probability distribution functions to F0 trajectories in the F0-∆F0 phase plane. Since fluctuations in singing can be easily separated by using SPR, we applied SPR to a melodic similarity measure for query-by-humming (QBH) applications. Our experimental results showed that the SPR-based similarity measure was superior to a conventional dynamic-programming-based method.",JPN,education,Developed economies,"[8.024366, -6.120767]","[3.0820978, -10.334621]","[10.019021, 11.821165, -1.7159675]","[12.511527, -11.085331, -14.30298]","[10.640758, 9.70442]","[8.222462, 1.1343355]","[11.5693865, 14.965922, -0.637483]","[9.909714, 6.7496514, 12.833061]"
93,Matthias Gruhne;Christian Dittmar;Konstantin Schmidt,Phoneme Recognition in Popular Music.,2007,https://doi.org/10.5281/zenodo.1417111,Matthias Gruhne+Fraunhofer IDMT>DEU>facility;Konstantin Schmidt+Fraunhofer IDMT>DEU>facility;Christian Dittmar+Fraunhofer IDMT>DEU>facility,"Automatic lyrics synchronization for karaoke applications is a major challenge in the field of music information retrieval. An important pre-requisite in order to precisely synchronize the music and corresponding text is the detection of single phonemes in the vocal part of polyphonic music. This paper describes a system, which detects the phonemes based on a state-of-the-art audio information retrieval system with harmonics extraction and synthesizing as pre-processing method. The extraction algorithm is based on common speech recognition low-level features, such as MFCC and LPC. In order to distinguish phonemes, three different classification techniques (SVM, GMM and MLP) have been used and their results are depicted in the paper.",DEU,facility,Developed economies,"[-10.023122, -30.644844]","[10.793265, -15.10116]","[12.3809595, 12.633195, -14.810884]","[13.654551, -3.8937478, -10.143661]","[10.145624, 11.049698]","[9.456189, 2.4080253]","[11.493907, 15.355782, 0.57792073]","[11.11034, 6.492591, 12.048625]"
92,Douglas Eck;Thierry Bertin-Mahieux;Paul Lamere,Autotagging Music Using Supervised Machine Learning.,2007,https://doi.org/10.5281/zenodo.1417869,Douglas Eck+Sun Microsystems>USA>company;Thierry Bertin-Mahieux+University of Montreal>CAN>education;Paul Lamere+Sun Microsystems>USA>company,Social tags are an important component of “Web2.0” music recommendation websites. In this paper we propose a method for predicting social tags using audio features and supervised learning. These automatically-generated tags (or “autotags”) can furnish information about music that is untagged or poorly tagged. The tags can also serve to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.,USA,company,Developed economies,"[-41.299717, -3.5697222]","[40.07445, 1.0302721]","[-12.35803, 14.522239, 12.318544]","[24.47944, 7.4238124, 5.8897443]","[14.4990225, 10.573195]","[11.728853, 3.2157865]","[15.586752, 14.068955, 0.14480585]","[13.166792, 6.271306, 11.42339]"
91,Michael I. Mandel;Daniel P. W. Ellis,A Web-Based Game for Collecting Music Metadata.,2007,https://doi.org/10.5281/zenodo.1414768,Michael I Mandel+Columbia University>USA>education;Daniel P W Ellis+Columbia University>USA>education,"We have designed a web-based game to make collecting descriptions of musical excerpts fun, easy, useful, and objective. Participants describe 10 second clips of songs and score points when their descriptions match those of other participants. The rules were designed to encourage users to be thorough and the clip length was chosen to make judgments more objective and specific. Analysis of preliminary data shows that we are able to collect objective and specific descriptions of clips and that players tend to agree with one another.",USA,education,Developed economies,"[-25.045778, 34.839485]","[34.683918, 25.052189]","[-24.204834, 2.277654, -7.4670286]","[3.2122457, 14.057515, 10.612648]","[14.286458, 9.0863285]","[10.962798, 1.9770905]","[15.103549, 13.770873, -1.0958122]","[12.793461, 5.5668864, 10.797481]"
90,Edith L. M. Law;Luis von Ahn;Roger B. Dannenberg;Mike Crawford,TagATune: A Game for Music and Sound Annotation.,2007,https://doi.org/10.5281/zenodo.1415568,Edith L. M. Law+Carnegie Mellon University>USA>education;Luis von Ahn+Carnegie Mellon University>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education;Mike Crawford+Carnegie Mellon University>USA>education,"Annotations of audio files can be used to search and index music and sound databases, provide data for system evaluation, and generate training data for machine learning. Unfortunately, the cost of obtaining a comprehensive set of annotations manually is high. One way to lower the cost of labeling is to create games with a purpose that people will voluntarily play, producing useful metadata as a by-product. TagATune is an audio-based online game that aims to extract descriptions of sounds and music from human players. This paper presents the rationale, design and preliminary results from a pilot study using a prototype of TagATune to label a subset of the FreeSound database.",USA,education,Developed economies,"[-32.47086, 11.103668]","[43.074196, -3.3524573]","[-15.644711, 9.221486, 1.8916812]","[26.128042, 11.477339, 0.6768157]","[14.182483, 9.33991]","[11.600283, 3.6259112]","[14.961004, 13.941658, -0.8037277]","[12.863386, 5.9044824, 10.914838]"
89,Karin Dressler;Sebastian Streich,Tuning Frequency Estimation Using Circular Statistics.,2007,https://doi.org/10.5281/zenodo.1416294,Karin Dressler+Fraunhofer IDMT>DEU>facility;Sebastian Streich+Pompeu Fabra University>ESP>education,"In this document a new approach on tuning frequency estimation based on circular statistics is presented. Two methods are introduced: the calculation of the tuning frequency over an entire audio piece, and the estimation of an adapting reference frequency for a single voice. The results for the tuning frequency estimation look very good for audio pieces where the dominant voices are tuned close to the equal-temperament scale and exhibit only moderate frequency dynamics. For the analysis of popular western music, the method does not achieve very robust results due to the strong frequency dynamics of the human singing voice. Nevertheless, the method could be improved by excluding the singing voice from the calculation taking only the accompaniment into account. The main advantage of the proposed method lies especially in the easy computation of an adaptive reference frequency using an exponential moving average. This adaptive reference can for example be used in the quantization of the singing voice into a note representation.",DEU,facility,Developed economies,"[38.572563, -18.986063]","[-1.3748596, -14.813047]","[5.038066, -26.345844, 8.136446]","[8.637817, -13.256374, -11.093907]","[8.916421, 8.792189]","[6.886398, 2.2209663]","[11.014961, 13.586758, -0.19326584]","[9.020395, 7.7222633, 11.519109]"
88,Alia Al Kasimi;Eric Nichols;Christopher Raphael,A Simple Algorithm for Automatic Generation of Polyphonic Piano Fingerings.,2007,https://doi.org/10.5281/zenodo.1415880,Alia Al Kasimi+Indiana University>USA>education;Eric Nichols+Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education,"""We present a novel method for assigning fingers to notes in a polyphonic piano score. Such a mapping (called a “fingering”) is of great use to performers. To accommodate performers’ unique hand shapes and sizes, our method relies on a simple, user-adjustable cost function. We use dynamic programming to search the space of all possible fingerings for the optimal fingering under this cost function. Despite the simplicity of the algorithm we achieve reasonable and useful results.""",USA,education,Developed economies,"[28.322063, -2.7459855]","[-46.442905, -4.2520814]","[21.906965, -1.8405111, 15.633774]","[-9.265133, -19.438696, -8.986863]","[9.844765, 7.262033]","[7.1366224, 4.1895123]","[12.054489, 11.645294, -0.37006932]","[8.907975, 6.6923876, 10.283069]"
87,Arnaud Moreau;Arthur Flexer,Drum Transcription in Polyphonic Music Using Non-Negative Matrix Factorisation.,2007,https://doi.org/10.5281/zenodo.1417455,Arnaud Moreau+Austrian Research Institute for Artificial Intelligence>AUT>facility|Medical University of Vienna>AUT>education;Arthur Flexer+Medical University of Vienna>AUT>education,We present a system that is based on the non-negative matrix factorisation (NMF) algorithm and is able to transcribe drum onset events in polyphonic music. The magnitude spectrogram representation of the input music is divided by the NMF algorithm into source spectra and corresponding time-varying gains. Each of these source components is classified as a drum instrument or non-drum sound and a peak-picking algorithm determines the onset times.,AUT,facility,Developed economies,"[26.909945, -44.12086]","[-50.75673, -22.504784]","[17.385132, -19.15969, 4.6485376]","[-3.8298688, -17.189632, -26.352722]","[7.7970195, 7.239053]","[6.1099052, 4.7110605]","[10.419723, 11.608699, 0.94891775]","[9.429892, 8.636073, 9.99834]"
85,Sten Govaerts;Nik Corthaut;Erik Duval,Mood-ex-Machina: Towards Automation of Moody Tunes.,2007,https://doi.org/10.5281/zenodo.1415918,Sten Govaerts+Katholieke Universiteit Leuven>BEL>education;Nik Corthaut+Katholieke Universiteit Leuven>BEL>education;Erik Duval+Katholieke Universiteit Leuven>BEL>education,"In 2006, the rockanango system was developed for music annotation by music experts. The system allows these experts to create new musical parameters within a flat data structure [1]. Rockanango is deployed in a commercial environment of hotels, restaurants and cafés. One of the main concerns is the time it takes to manually annotate the music and to introduce new parameters. In this paper, we investigate the possibilities to assist the experts by means of automatic metadata generation. Two case studies are described. One focuses on the use of association rules, in combination with lower level metadata like mode and key. The other case study concerns the generation of a topic or subject marker for songs through harvested lyrics and a keyword generator. From our evaluation, we conclude that the generated keywords are relevant and that the music experts value them higher than laymen. Data mining techniques provide means for monitoring the metadata in terms of interparametric relationships that can be used to generate metadata.",BEL,education,Developed economies,"[-55.60909, 6.6288285]","[37.15208, 1.4870206]","[-16.531994, 24.00648, 2.4037237]","[22.48122, 11.494863, 4.6333795]","[13.553423, 12.503729]","[11.335705, 2.8991547]","[16.162968, 14.881467, 1.4345196]","[12.517589, 5.745539, 11.477019]"
84,Janto Skowronek;Martin F. McKinney;Steven van de Par,A Demonstrator for Automatic Music Mood Estimation.,2007,https://doi.org/10.5281/zenodo.1417669,Janto Skowronek+Philips Research Laboratories>NLD>company;Martin McKinney+Philips Research Laboratories>NLD>company;Steven van de Par+Philips Research Laboratories>NLD>company,"Interest in automatic music mood classification is increasing because it could enable people to browse and manage their music collections by means of the music’s emotional expression complementary to the widely used music genres. We continue our work on designing a well defined ground-truth database for music mood classification and show a demonstrator of automatic mood estimation. While a subjective evaluation of this algorithm on arbitrary music is ongoing, the initial classification results are encouraging and suggest that an automatic prediction of music mood is possible.",NLD,company,Developed economies,"[-55.563534, 3.5551045]","[52.058975, -5.3048024]","[-18.849617, 23.838785, 3.6103666]","[12.132263, 21.76709, 8.159919]","[13.550887, 12.596813]","[13.169923, 3.872884]","[16.141802, 14.866248, 1.4835172]","[14.206507, 5.081807, 10.636524]"
83,Arthur Flexer,A Closer Look on Artist Filters for Musical Genre Classification.,2007,https://doi.org/10.5281/zenodo.1415668,Arthur Flexer+Medical University of Vienna>AUT>education,Musical genre classification is the automatic classification of audio signals into user defined labels describing pieces of music. A problem inherent to genre classification experiments in music information retrieval research is the use of songs from the same artist in both training and test sets. We show that this does not only lead to over-optimistic accuracy results but also selectively favours particular classification approaches. The advantage of using models of songs rather than models of genres vanishes when applying an artist filter. The same holds true for the use of spectral features versus fluctuation patterns for preprocessing of the audio files.,AUT,education,Developed economies,"[-29.767334, -13.554678]","[19.86344, -8.703365]","[-16.896704, 4.718081, 17.467548]","[14.343658, 9.8896055, -3.1554322]","[13.096236, 10.800561]","[9.870121, 3.3269887]","[14.031487, 14.317587, 1.2795408]","[11.595542, 6.9790845, 10.835847]"
82,Daniel P. W. Ellis,Classifying Music Audio with Timbral and Chroma Features.,2007,https://doi.org/10.5281/zenodo.1416906,Daniel P. W. Ellis+Columbia University>USA>education,"Music audio classification has most often been addressed by modeling the statistics of broad spectral features, which, by design, exclude pitch information and reflect mainly instrumentation. We investigate using instead beat-synchronous chroma features, designed to reflect melodic and harmonic content and be invariant to instrumentation. Chroma features are less informative for classes such as artist, but contain information that is almost entirely independent of the spectral features, and hence the two can be profitably combined: Using a simple Gaussian classifier on a 20-way pop music artist identification task, we achieve 54% accuracy with MFCCs, 30% with chroma vectors, and 57% by combining the two. All the data and Matlab code to obtain these results are available.",USA,education,Developed economies,"[-22.635561, -13.793452]","[14.470064, -14.924761]","[-8.761721, 1.0585942, 15.742356]","[14.802935, 0.60949636, -8.093372]","[12.103397, 7.8533015]","[9.395963, 3.4641063]","[13.048078, 13.771901, 0.63567287]","[11.094957, 7.60122, 10.696908]"
96,Annamaria Mesaros;Tuomas Virtanen;Anssi Klapuri,Singer Identification in Polyphonic Music Using Vocal Separation and Pattern Recognition Methods.,2007,https://doi.org/10.5281/zenodo.1417395,Annamaria Mesaros+Tampere University of Technology>FIN>education;Tuomas Virtanen+Tampere University of Technology>FIN>education;Anssi Klapuri+Tampere University of Technology>FIN>education,"This paper evaluates methods for singer identification in polyphonic music, based on pattern classification together with an algorithm for vocal separation. Classification strategies include the discriminant functions, Gaussian mixture model (GMM)-based maximum likelihood classifier and nearest neighbour classifiers using Kullback-Leibler divergence between the GMMs. A novel method of estimating the symmetric Kullback-Leibler distance between two GMMs is proposed. Two different approaches to singer identification were studied: one where the acoustic features were extracted directly from the polyphonic signal and one where the vocal line was first separated from the mixture using a predominant melody transcription system. The methods are evaluated using a database of songs where the level difference between the singing and the accompaniment varies. It was found that vocal line separation enables robust singer identification down to 0dB and -5dB singer-to-accompaniment ratios.",FIN,education,Developed economies,"[-12.210075, -38.012413]","[12.885356, -19.820883]","[19.510513, 13.692078, -20.949284]","[12.932741, -3.0972888, -13.843818]","[10.014137, 11.444399]","[8.257778, 3.6574345]","[11.337003, 15.5050745, 0.7712229]","[10.374311, 8.156754, 10.269043]"
81,David Little;David Raffensperger;Bryan Pardo,A Query by Humming System that Learns from Experience.,2007,https://doi.org/10.5281/zenodo.1416642,David Little+Northwestern University>USA>education|Northwestern University>USA>education|Northwestern University>USA>education;David Raffensperger+Northwestern University>USA>education|Northwestern University>USA>education|Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education|Northwestern University>USA>education|Northwestern University>USA>education,"Query-by-Humming (QBH) systems transcribe a sung or hummed query and search for related musical themes in a database, returning the most similar themes. Since it is not possible to predict all individual singer profiles before system deployment, a robust QBH system should be able to adapt to different singers after deployment. Currently deployed systems do not have this capability. We describe a new QBH system that learns from user provided feedback on the search results, letting the system improve while deployed, after only a few queries. This is made possible by a trainable note segmentation system, an easily parameterized singer error model and a straight-forward genetic algorithm. Results show significant improvement in performance given only ten example queries from a particular user.",USA,education,Developed economies,"[-4.4541197, 37.60785]","[9.843082, 10.446728]","[-15.536786, -5.914941, -25.700855]","[6.0109277, -15.193324, 13.989801]","[14.8685465, 6.1568303]","[8.769777, 0.4606037]","[13.181716, 15.343372, -2.9312778]","[10.411452, 5.923992, 13.198215]"
79,Elizabeth Davis,Finding Music in Scholarly Sets and Series: The Index to Printed Music (IPM).,2007,https://doi.org/10.5281/zenodo.1417747,Elizabeth Davis+Columbia University>USA>education,"The Index to Printed Music (IPM) provides access to sets and series of music published beginning in the 19th century. Prepared by scholars and researchers, these titles vary considerably in length (single to multiple volumes), types (topical, pedagogical, historical, etc.), format (treatises, dissertations, editions, etc.), and geographic origin (chiefly Europe and North America). Bibliographical access to their contents is not readily available through library cataloging or reference works. IPM provides title, format, genre, instrumentation, and other metadata access to these publications in three inter-connected databases: Bibliography, Index, and Names, available by subscription through NISC International, Inc. The Index Database now contains over 255,000 entries, the Bibliography Database over 10,000 entries, and the Names Database over 15,000 authority records. Having built this groundwork, future steps involve linking to full-text score images where available through non-commercial projects, and partnerships with publishers and commercial vendors.",USA,education,Developed economies,"[-26.031363, 23.691383]","[21.96316, 37.654236]","[-19.388973, 6.550341, -6.880707]","[-3.4162002, 2.6591835, 21.852474]","[14.380915, 8.027441]","[11.197517, 0.14736743]","[14.483667, 14.697185, -2.1326656]","[11.960365, 4.721927, 12.28512]"
78,Jin Ha Lee;J. Stephen Downie;M. Cameron Jones,Preliminary Analyses of Information Features Provided by Users for Identifying Music.,2007,https://doi.org/10.5281/zenodo.1415860,Jin Ha Lee+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;M. Cameron Jones+University of Illinois at Urbana-Champaign>USA>education,"This paper presents preliminary findings based on the analyses of user-provided information features found in 566 queries seeking help in the identification of particular music works or artists. Queries were drawn from the answers.google.com (Google Answers) website. The types and frequency of occurrences of different information features are compared with the results from previous studies of music queries. New feature types have also been developed to obtain a more comprehensive understanding of the kinds of information present in queries including such things as indications of uncertainty, associated use, and the “aboutness” of the underlying musical work. The presence of erroneous information in the queries is also discussed.",USA,education,Developed economies,"[-22.307808, 18.292099]","[33.327576, 29.807732]","[-13.249204, 9.7860775, -4.7272687]","[8.437952, 5.888064, 23.081686]","[14.277492, 8.309113]","[12.071035, 0.9830402]","[14.376085, 14.810114, -1.6272361]","[12.526928, 4.8083906, 12.428884]"
77,Andreas F. Ehmann;J. Stephen Downie;M. Cameron Jones,"The Music Information Retrieval Evaluation Exchange ""Do-It-Yourself"" Web Service.",2007,https://doi.org/10.5281/zenodo.1417949,Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;M. Cameron Jones+University of Illinois at Urbana-Champaign>USA>education,"The Do-It-Yourself (DIY) web service of the Music Information Retrieval Evaluation eXchange (MIREX) represents a means by which researchers can remotely submit, execute, and evaluate their Music Information Retrieval (MIR) algorithms against standardized datasets that are not otherwise freely distributable. Since its inception in 2005 at the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL), MIREX has, to date, required heavy interaction by IMIRSEL team members in the execution, debugging, and validation of submitted code. The goal of the MIREX DIY web service is to put such responsibilities squarely into the hands of submitters, and also enable the evaluations of algorithms year-round, as opposed to annual exchanges.",USA,education,Developed economies,"[-20.79008, 21.508133]","[26.078764, 33.184174]","[-14.93621, 3.931062, -12.110781]","[1.3876497, 5.8921695, 16.342041]","[14.376073, 7.7644987]","[11.650713, 0.66792053]","[14.473074, 14.524445, -2.0815654]","[12.163152, 4.4840126, 12.221771]"
76,Joan Serrà,A Qualitative Assessment of Measures for the Evaluation of a Cover Song Identification System.,2007,https://doi.org/10.5281/zenodo.1415174,Joan Serrà+Universitat Pompeu Fabra>ESP>education,"The evaluation of effectiveness in Information Retrieval systems has been developed in parallel to its evolution, generating a great amount of proposals to achieve this process. This paper focuses on a particular task of Music Information Retrieval: a system for Cover Song Identification. We present a concrete example and then try to elucidate which metrics work best to evaluate such a system. We end up with two evaluation measures suitable for this problem: bpref and Normalized Lift Curves.",ESP,education,Developed economies,"[8.165652, 44.569912]","[21.442871, 20.841476]","[5.2086334, 13.054521, -24.381351]","[6.975055, 1.4021959, 14.163748]","[16.099657, 11.114957]","[10.772596, 1.2226686]","[12.880651, 17.315424, -0.3627952]","[11.717344, 5.5865965, 12.507662]"
75,Daniel Müllensiefen;David Lewis 0001;Christophe Rhodes;Geraint A. Wiggins,Evaluating a Chord-Labelling Algorithm.,2007,https://doi.org/10.5281/zenodo.1417779,"Daniel Müllensiefen+Goldsmiths, University of London>GBR>education|Goldsmiths College>GBR>education;David Lewis+Goldsmiths, University of London>GBR>education|Goldsmiths College>GBR>education;Christophe Rhodes+Goldsmiths, University of London>GBR>education|Goldsmiths College>GBR>education;Geraint Wiggins+Goldsmiths, University of London>GBR>education|Goldsmiths College>GBR>education","This paper outlines a method for evaluating a new chord-labelling algorithm using symbolic data as input. Excerpts from full-score transcriptions of 40 pop songs are used. The accuracy of the algorithm’s output is compared with that of chord labels from published song books, as assessed by experts in pop music theory. We are interested not only in the accuracy of the two sets of labels but also in the question of potential harmonic ambiguity as reflected the judges’ assessments. We focus, in this short paper, on outlining the general approach of this research project.",GBR,education,Developed economies,"[53.62616, -3.4555979]","[-26.421476, 20.31119]","[26.169403, -15.685709, 14.105863]","[-25.007006, -2.8969955, 7.1273813]","[6.8704557, 8.647267]","[6.52653, 3.3714712]","[11.92863, 10.415124, 2.0625265]","[9.936158, 8.420286, 12.464673]"
74,Arshia Cont;Diemo Schwarz;Norbert Schnell;Christopher Raphael,Evaluation of Real-Time Audio-to-Score Alignment.,2007,https://doi.org/10.5281/zenodo.1416304,Arshia Cont+Ircam UMR CNRS 9912>Unknown>Unknown|Ircam–Centre Pompidou>FRA>facility;Diemo Schwarz+Ircam UMR CNRS 9912>Unknown>Unknown|Ircam–Centre Pompidou>FRA>facility;Norbert Schnell+Ircam UMR CNRS 9912>Unknown>Unknown|Ircam–Centre Pompidou>FRA>facility;Christopher Raphael+Indiana University>USA>education,"This article explains evaluation methods for real-time audio to score alignment, or score following, that allow for the quantitative assessment of the robustness and preciseness of an algorithm. The published ground truth database and the evaluation framework, including file formats for the score and the reference alignments, are presented. The work, started for MIREX 2006, is meant as a first step towards a standardized evaluation process contributing to the exchange and progress in this field.",Unknown,Unknown,Unknown,"[19.960306, -15.860457]","[-21.317112, -16.318804]","[0.98125976, -15.709119, -13.917156]","[-4.204771, -25.578989, -5.8258986]","[10.833494, 5.976574]","[6.333573, 0.8154262]","[11.687778, 12.501909, -1.7807231]","[8.39502, 5.84806, 10.732226]"
73,Matthias Varewyck;Jean-Pierre Martens,Assessment of State-of-the-Art Meter Analysis Systems with an Extended Meter Description Model.,2007,https://doi.org/10.5281/zenodo.1417367,Matthias Varewyck+Ghent University>BEL>education;Jean-Pierre Martens+Ghent University>BEL>education,"An extended meter description model capturing the hierarchical metrical structure of Western music is proposed. The model is applied for the quantitative evaluation of four state-of-the-art automatic meter analysis algorithms of musical audio. Evaluation results suggest that the best beat trackers reach a reasonable level of performance, but that none of the tested algorithms has the potential to perform a reliable bar onset tracking. Moreover, the front-ends of the best overall systems not necessarily seem to have the front-ends best encoding the time signature in their output. Therefore, further improvements of these systems should be attainable by a better combination of ideas that can be borrowed from existing algorithms.",BEL,education,Developed economies,"[45.850414, -19.330332]","[-23.503672, -2.6984112]","[-13.918563, -12.52646, 2.1349957]","[-2.9814355, 12.45678, -8.109884]","[11.997046, 6.23789]","[5.71258, 1.9450501]","[12.479888, 13.300367, -1.7525283]","[7.9720955, 7.046577, 11.352151]"
72,Xiao Hu 0001;Mert Bay;J. Stephen Downie,Creating a Simplified Music Mood Classification Ground-Truth Set.,2007,https://doi.org/10.5281/zenodo.1416920,Xiao Hu+University of Illinois at Urbana-Champaign>USA>education;Mert Bay+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"A standardized mood classification testbed is needed for formal cross-algorithm comparison and evaluation. In this poster, we present a simplification of the problems associated with developing a ground-truth set for the evaluation of mood-based Music Information Retrieval (MIR) systems. Using a dataset derived from Last.fm tags and the USPOP audio collection, we have applied a K-means clustering method to create a simple yet meaningful cluster-based set of high-level mood categories as well as a ground-truth dataset.",USA,education,Developed economies,"[-54.51442, 2.9527845]","[51.701813, -2.6964636]","[-20.336145, 25.314798, 6.0978746]","[14.335104, 19.623362, 9.549595]","[13.449077, 12.568402]","[13.124384, 3.725931]","[16.120533, 14.919685, 1.4508525]","[14.253452, 5.1997566, 10.781633]"
71,Ching-Hua Chuan;Elaine Chew,A Dynamic Programming Approach to the Extraction of Phrase Boundaries from Tempo Variations in Expressive Performances.,2007,https://doi.org/10.5281/zenodo.1416584,Ching-Hua Chuan+University of Southern California>USA>education|Integrated Media Systems Center>USA>facility;Elaine Chew+University of Southern California>USA>education|Integrated Media Systems Center>USA>facility,"We present an approach to phrase segmentation that starts with an expressive music performance. Previous research has shown that phrases are delineated by tempo speedups and slowdowns. We propose a dynamic programming algorithm for extracting phrases from tempo information. We test two hypotheses for modeling phrase tempo shapes: a quadratic model, and a spline curve. We test the two models on phrase extraction from performances of entire classical romantic pieces namely, Chopin’s Preludes Nos. 1 and 7. The algorithms determined 21 of the 26 phrase boundaries correctly from Arthur Rubinstein’s and Evgeny Kissin’s performances. We observe that not all tempo slowdowns signify a boundary (some are agogic accents), and multiple levels of phrasing strategies should be considered for detailed interpretation analyses.",USA,education,Developed economies,"[40.980705, -22.901432]","[-5.7072387, 5.9954977]","[0.96963006, -25.298613, -3.786475]","[-5.8768635, -4.84044, 0.04539759]","[11.475725, 4.492376]","[8.147268, 2.642175]","[10.939657, 13.352552, -2.770719]","[9.811231, 7.1764536, 11.613083]"
70,Aggelos Pikrakis;Sergios Theodoridis,An Application of Empirical Mode Decomposition on Tempo Induction from Music Recordings.,2007,https://doi.org/10.5281/zenodo.1418103,Aggelos Pikrakis+University of Athens>GRC>education;Sergios Theodoridis+University of Athens>GRC>education,"This paper presents an application of Empirical Mode Decomposition (EMD) on the induction of notated tempo from music recordings. At a first stage, EMD is employed as a means to segment music recordings into segments that exhibit similar rhythmic characteristics. At a second stage, EMD is used in order to analyze the diagonals of the Self-Similarity Matrix of each segment, so as to estimate the tempo of the recording. The proposed method has been employed on various music genres with music meters of 2/4, 3/4 and 4/4. Tempo has been assumed to remain approximately constant throughout each recording, ranging from 60bpm up to 220bpm.",GRC,education,Developed economies,"[39.414005, -27.208654]","[-27.042643, -5.203869]","[-2.0822752, -29.839989, -3.5374882]","[-3.3444192, 11.1427965, -10.438551]","[11.343203, 4.520645]","[5.3651648, 1.7942722]","[10.908232, 13.239123, -2.852891]","[7.5688567, 7.105555, 11.176113]"
69,Iasonas Antonopoulos;Aggelos Pikrakis;Sergios Theodoridis;Olmo Cornelis;Dirk Moelants;Marc Leman,Music Retrieval by Rhythmic Similarity Applied on Greek and African Traditional Music.,2007,https://doi.org/10.5281/zenodo.1417503,Iasonas Antonopoulos+University Of Athens>GRC>education;Aggelos Pikrakis+University Of Athens>GRC>education;Sergios Theodoridis+University Of Athens>GRC>education;Olmo Cornelis+Ghent University>BEL>education;Dirk Moelants+Ghent University>BEL>education;Marc Leman+Ghent University>BEL>education,"This paper presents a method for retrieving music recordings by means of rhythmic similarity in the context of traditional Greek and African music. To this end, Self Similarity Analysis is applied either on the whole recording or on instances of a music thumbnail that can be extracted from the recording with an optional thumbnailing scheme. This type of analysis permits the extraction of a rhythmic signature per music recording. Similarity between signatures is measured with a standard Dynamic Time Warping technique. The proposed method was evaluated on corpora of Greek and African traditional music where human improvisation plays a key role and music recordings exhibit a variety of music meters, tempi and instrumentation.",GRC,education,Developed economies,"[3.442171, 13.05254]","[-20.796408, 0.82411987]","[-6.201218, 1.8739247, -4.918698]","[1.473627, 11.370278, -3.571037]","[12.450189, 9.407256]","[6.294182, 1.582429]","[12.617751, 14.943282, -1.3722097]","[8.39712, 6.716389, 11.958336]"
68,Anja Volk;Jörg Garbers;Peter van Kranenburg;Frans Wiering;Remco C. Veltkamp;Louis P. Grijp,Applying Rhythmic Similarity Based on Inner Metric Analysis to Folksong Research.,2007,https://doi.org/10.5281/zenodo.1416830,Anja Volk+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Jörg Garbers+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Peter van Kranenburg+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Frans Wiering+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Remco C. Veltkamp+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Louis P. Grijp+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown,"In this paper we investigate the role of rhythmic similarity as part of melodic similarity in the context of Folksong research. We define a rhythmic similarity measure based on Inner Metric Analysis and apply it to groups of similar melodies. The comparison with a similarity measure of the SIMILE software shows that the two models agree on the number of melodies that are considered very similar, but disagree on the less similar melodies. In general, we achieve good results with the retrieval of melodies using rhythmic information, which demonstrates that rhythmic similarity is an important factor to consider in melodic similarity.",NLD,education,Developed economies,"[45.46073, 8.83786]","[15.866881, 2.6953754]","[-11.1536255, -22.301945, 4.011747]","[6.7429113, 2.443422, 2.4291484]","[12.023235, 5.492437]","[9.480421, 1.9048476]","[11.47875, 14.337095, -2.0669804]","[11.263593, 7.0856957, 13.108776]"
67,Klaus Frieler,Visualizing Music on the Metrical Circle.,2007,https://doi.org/10.5281/zenodo.1416798,Klaus Frieler+University of Hamburg>DEU>education,"In this paper we propose a novel method, called Metrical Circle Map, for exploring the cyclic aspects of musical time. To this end, we give a short formalization introducing the notion of Metrical Markov Chains as transition probabilities of segments on the metrical circle. As an illustration we present a compact visualization of the zeroth- and first order metrical Markov transitions of 61 Irish folk songs.",DEU,education,Developed economies,"[-16.056044, 33.048782]","[-16.09864, 12.470806]","[-16.05991, 9.129396, -26.022291]","[-12.381786, 1.7568055, 4.697057]","[13.828457, 6.929932]","[7.333078, 2.1572204]","[14.0113325, 13.670461, -2.4866452]","[9.229951, 6.9786944, 12.00149]"
80,Alexander Duda;Andreas Nürnberger;Sebastian Stober,Towards Query by Singing/Humming on Audio Databases.,2007,https://doi.org/10.5281/zenodo.1414804,Alexander Duda+Otto-von-Guericke-University Magdeburg>DEU>education;Andreas Nü rnberger+Otto-von-Guericke-University Magdeburg>DEU>education;Sebastian Stober+Otto-von-Guericke-University Magdeburg>DEU>education,"Current work on Query-by-Singing/Humming (QBSH) focuses mainly on databases that contain MIDI files. Here, we present an approach that works on real audio recordings that bring up additional challenges. To tackle the problem of extracting the melody of the lead vocals from recordings, we introduce a method inspired by the popular “karaoke effect” exploiting information about the spatial arrangement of voices and instruments in the stereo mix. The extracted signal time series are aggregated into symbolic strings preserving the local approximated values of a feature and revealing higher-level context patterns. This allows distance measures for string pattern matching to be applied in the matching process. A series of experiments are conducted to assess the discrimination and robustness of this representation. They show that the proposed approach provides a viable baseline for further development and point out several possibilities for improvement.",DEU,education,Developed economies,"[-4.075737, 36.215897]","[8.43675, 9.119045]","[-14.323562, -5.425229, -23.27592]","[4.397366, -16.026077, 10.626418]","[14.852121, 6.182473]","[8.452722, 0.5655087]","[13.235554, 15.273421, -2.8714123]","[10.219761, 5.9995975, 13.002033]"
66,Mika Kuuskankare;Mikael Laurson,Vivo - Visualizing Harmonic Progressions and Voice-Leading in PWGL.,2007,https://doi.org/10.5281/zenodo.1418189,Mika Kuuskankare+Sibelius Academy>FIN>education|CMT>Unknown>Unknown;Mikael Laurson+Sibelius Academy>FIN>education|CMT>Unknown>Unknown,"This paper describes a novel tool called VIVO (VIsual VOice-leading) that allows to visually define harmonic progressions and voice-leading rules. VIVO comprises of a compiler and a collection of specialized visualization devices. VIVO takes advantage of several music related applications collected under the umbrella of PWGL (PWGL is a free cross-platform visual programming language for music and sound related applications). Our music notation application–Expressive Notation Package or ENP–is used here to build the user-interface used to visually define harmony and voice-leading rules. These visualizations are converted to textual rules by the VIVO compiler. Finally, our rule-based compositional system, PWGLConstraints, is used generate the final musical output using these rules.",FIN,education,Developed economies,"[5.713878, 33.645927]","[-2.5168858, 33.017033]","[-21.471716, -18.927881, -16.409178]","[-12.122641, -5.634991, 13.496452]","[13.171175, 6.7262473]","[9.761747, 0.66925424]","[13.785144, 13.140866, -2.1236234]","[10.338423, 5.426003, 11.678385]"
97,Stanislaw Andrzej Raczynski;Nobutaka Ono;Shigeki Sagayama,Multipitch Analysis with Harmonic Nonnegative Matrix Approximation.,2007,https://doi.org/10.5281/zenodo.1417809,Stanisław A. Raczyński+The University of Tokyo>JPN>education;Nobutaka Ono+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"This paper presents a new approach to multipitch analysis by utilizing the Harmonic Nonnegative Matrix Approximation, a harmonically-constrained and penalized version of the Nonnegative Matrix Approximation (NNMA) method. It also includes a description of a note onset, offset and amplitude retrieval procedure based on that technique. Compared with the previous NNMA approaches, specific initialization of the basis matrix is employed – the basis matrix is initialized with zeros everywhere but at positions corresponding to harmonic frequencies of consequent notes of the equal temperament scale. This results in the basis containing nothing but harmonically structured vectors, even after the learning process, and the activity matrix’s rows containing peaks corresponding to note onset times and amplitudes. Furthermore, additional penalties of mutual uncorrelation and sparseness of rows are placed upon the activity matrix. The proposed method is able to uncover the underlying musical structure better than the previous NNMA approaches and makes the note detection process very straightforward.",JPN,education,Developed economies,"[37.110836, -14.115149]","[-49.19765, -23.04254]","[11.950107, -23.816994, 10.786437]","[-4.651137, -14.799436, -25.270565]","[8.960055, 8.997138]","[6.240644, 4.7443743]","[11.334654, 13.393208, 0.21660993]","[9.425002, 8.690122, 9.928877]"
99,Chunghsin Yeh;Niels Bogaards;Axel Röbel,Synthesized Polyphonic Music Database with Verifiable Ground Truth for Multiple F0 Estimation.,2007,https://doi.org/10.5281/zenodo.1415732,Chunghsin Yeh+IRCAM>FRA>facility|CNRS-STMS>FRA>facility;Niels Bogaards+IRCAM>FRA>facility;Axel Roebel+IRCAM>FRA>facility|CNRS-STMS>FRA>facility,"To study and to evaluate a multiple F0 estimation algorithm, a polyphonic database with verifiable ground truth is necessary. Real recordings with manual annotation as ground truth are often used for evaluation. However, ambiguities arise during manual annotation, which are often set up by subjective judgements. Therefore, in order to have access to verifiable ground truth, we propose a systematic method for creating a polyphonic music database. Multiple monophonic tracks are rendered from a given MIDI file, in which rendered samples are separated to prevent overlaps and to facilitate automatic annotation. F0s can then be reliably extracted as ground truth, which are stored using SDIF.",FRA,facility,Developed economies,"[1.2978455, -21.93134]","[-6.9254217, 10.660132]","[8.071973, -0.037673123, 7.14752]","[-2.6595418, 1.3771129, 8.306748]","[9.145299, 8.094187]","[7.5178156, 1.8656183]","[11.744795, 12.856481, 0.012711572]","[9.969351, 5.910464, 11.118097]"
1,Wei Peng 0001;Tao Li 0001;Mitsunori Ogihara,Music Clustering with Constraints.,2007,https://doi.org/10.5281/zenodo.1418087,Wei Peng+Florida International University>USA>education;Tao Li+Florida International University>USA>education;Mitsunori Ogihara+University of Rochester>USA>education,"This paper studies the problem of building clusters of music tracks in a collection of popular music in the presence of constraints. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). We present an approach based on the generalized constraint clustering algorithm by incorporating the constraints for grouping music by “similar” artists. The approach is evaluated on a data set consisting of 53 albums covering 41 popular artists. The “correctness” of the clusters generated is tested using artist similarity provided by All Music Guide.",USA,education,Developed economies,"[-6.6960335, 1.6157463]","[27.61381, 13.08185]","[0.7089921, -2.5040305, -5.4136586]","[18.572998, 1.2403249, 7.9800816]","[12.147433, 8.3279295]","[11.234616, 2.3727624]","[12.903096, 14.220135, -0.07587963]","[12.721507, 6.043275, 12.163144]"
126,M. Cameron Jones;J. Stephen Downie;Andreas F. Ehmann,Human Similarity Judgments: Implications for the Design of Formal Evaluations.,2007,https://doi.org/10.5281/zenodo.1416126,M. Cameron Jones+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education,"This paper presents findings of a series of analyses of human similarity judgments from the Symbolic Melodic Similarity, and Audio Music Similarity tasks from the Music Information Retrieval Evaluation Exchange (MIREX) 2006. The categorical judgment data generated by the evaluators is analyzed with regard to judgment stability, inter-grader reliability, and patterns of disagreement, both within and between the two tasks. An exploration of this space yields implications for the design of MIREX-like evaluations.",USA,education,Developed economies,"[-1.8887076, 22.018995]","[30.35912, 9.203235]","[-11.027162, 3.0327303, 0.35097772]","[7.8829603, 4.8120418, 10.953421]","[12.97246, 9.51446]","[11.061886, 2.121103]","[13.359448, 15.25393, -0.6314503]","[12.39743, 6.485534, 12.71785]"
125,Douglas Turnbull;Ruoran Liu;Luke Barrington;Gert R. G. Lanckriet,A Game-Based Approach for Collecting Semantic Annotations of Music.,2007,https://doi.org/10.5281/zenodo.1416464,"Douglas Turnbull+University of California, San Diego>USA>education;Ruoran Liu+University of California, San Diego>USA>education;Luke Barrington+University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education","Games based on human computation are a valuable tool for collecting semantic information about images. We show how to transfer this idea into the music domain in order to collect high-quality semantic information about songs. We present Listen Game, an online, multiplayer game that measures the semantic relationship between music and words. In the normal mode, a player sees a list of semantically related words (e.g., instruments, emotions, usages, genres) and is asked to pick the best and worst word to describe a song. In the freestyle mode, a user is asked to suggest a new word that describes the music. Each player receives real-time feedback about the agreement amongst all players. We show that we can use the data collected during a two-week pilot study of Listen Game to learn a supervised multiclass labeling (SML) model. We show that this SML model can annotate a novel song with meaningful words and retrieve relevant songs from a database of audio content.",USA,education,Developed economies,"[-31.821627, 10.673084]","[44.225636, -4.2098403]","[-16.086555, 7.752287, 1.9687364]","[14.376497, 19.213036, 0.4569565]","[14.105215, 9.306182]","[11.943434, 3.7257593]","[14.938112, 13.875657, -0.84513175]","[13.09768, 5.779092, 10.739783]"
124,Mohamed Sordo;Cyril Laurier;Òscar Celma,Annotating Music Collections: How Content-Based Similarity Helps to Propagate Labels.,2007,https://doi.org/10.5281/zenodo.1415708,Mohamed Sordo+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>facility;Cyril Laurier+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>facility;Oscar Celma+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>facility,"In this paper we present a way to annotate music collections by exploiting audio similarity. Similarity is used to propose labels (tags) to yet unlabeled songs, based on the content–based distance between them. The main goal of our work is to ease the process of annotating huge music collections, by using content-based similarity distances as a way to propagate labels among songs. We present two different experiments. The first one propagates labels that are related with the style of the piece, whereas the second experiment deals with mood labels. On the one hand, our approach shows that using a music collection annotated at 40% with styles, the collection can be automatically annotated up to 78% (that is, 40% already annotated and the rest, 38%, only using propagation), with a recall greater than 0.4. On the other hand, for a smaller music collection annotated at 30% with moods, the collection can be automatically annotated up to 65% (e.g. 30% plus 35% using propagation).",ESP,education,Developed economies,"[-30.188965, 11.148185]","[38.188652, 2.8152812]","[-19.12171, 6.8852086, 2.4877474]","[20.878605, 9.566102, 6.067181]","[13.9185, 9.370182]","[11.204481, 2.4853418]","[14.752552, 14.148254, -0.7567384]","[12.694646, 6.0367613, 11.72747]"
123,Gijs Geleijnse;Markus Schedl;Peter Knees,The Quest for Ground Truth in Musical Artist Tagging in the Social Web Era.,2007,https://doi.org/10.5281/zenodo.1416582,Gijs Geleijnse+Philips Research>NLD>company;Markus Schedl+Johannes Kepler University>AUT>education;Peter Knees+Johannes Kepler University>AUT>education,"Research in Web music information retrieval traditionally focuses on the classification, clustering or categorizing of music into genres or other subdivisions. However, current community-based web sites provide richer descriptors (i.e. tags) for all kinds of products. Although tags have no well-defined semantics, they have proven to be an effective mechanism to label and retrieve items. Moreover, these tags are community-based and hence give a description of a product through the eyes of a community rather than an expert opinion. In this work we focus on Last.fm, which is currently the largest music community web service. We investigate whether the tagging of artists is consistent with the artist similarities found with collaborative filtering techniques. As the Last.fm data shows to be both consistent and descriptive, we propose a method to use this community-based data to create a ground truth for artist tagging and artist similarity.",NLD,company,Developed economies,"[-42.891594, 4.314595]","[39.912548, 4.040572]","[-20.975437, 13.367458, 4.545576]","[20.895079, 7.6242557, 8.054809]","[14.374288, 10.21421]","[11.9880295, 2.7457962]","[15.208342, 14.646709, -0.048291277]","[13.183136, 5.900179, 11.881739]"
122,Klaus Seyerlehner;Gerhard Widmer;Dominik Schnitzer,From Rhythm Patterns to Perceived Tempo.,2007,https://doi.org/10.5281/zenodo.1418373,Klaus Seyerlehner+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility;Dominik Schnitzer+Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility,"There are many MIR applications for which we would like to be able to determine the perceived tempo of a song automatically. However, automatic tempo extraction itself is still an open problem. In general there are two tempo extraction methods, either based on the estimation of inter-onset intervals or based on self similarity computations. To predict a tempo the most significant time-lag or the most significant inter-onset-interval is used. We propose to use existing rhythm patterns and reformulate the tempo extraction problem in terms of a nearest neighbor classification problem. Our experiments, based on three different datasets, show that this novel approach performs at least comparably to state-of-the-art tempo extraction algorithms and could be useful to get a deeper insight into the relation between perceived tempo and rhythm patterns.",AUT,education,Developed economies,"[42.50166, -24.82612]","[-29.633564, -5.5591936]","[-5.5685496, -27.582972, 1.4346516]","[-6.159855, 12.619668, -10.513788]","[11.623841, 4.687236]","[5.209551, 1.8228476]","[10.92928, 13.645239, -2.6811335]","[7.410145, 7.010192, 11.064076]"
121,Laurent Pugin;John Ashley Burgoyne;Ichiro Fujinaga,MAP Adaptation to Improve Optical Music Recognition of Early Music Documents Using Hidden Markov Models.,2007,https://doi.org/10.5281/zenodo.1415922,Laurent Pugin+McGill University>CAN>education;John Ashley Burgoyne+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Despite steady improvement in optical music recognition (OMR), early documents remain challenging because of the high variability in their contents. In this paper, we present an original approach using maximum a posteriori (MAP) adaptation to improve an OMR tool for early typographic prints dynamically based on hidden Markov models. Taking advantage of the fact that during the normal usage of any OMR tool, errors will be corrected, and thus ground-truth produced, the system can be adapted in real-time. We experimented with five 16th-century music prints using 250 pages of music and two procedures in applying MAP adaptation. With only a handful of pages, both recall and precision rates improved even when the baseline was above 95 percent.",CAN,education,Developed economies,"[36.756977, 23.246367]","[-22.736763, 40.23386]","[18.528872, 13.091297, 6.4752073]","[-11.336742, -24.783953, -0.2218063]","[8.730944, 6.1934123]","[6.712944, -0.7001252]","[10.814967, 11.202082, -0.09406403]","[7.7987657, 4.0282, 10.55828]"
120,John Ashley Burgoyne;Laurent Pugin;Greg Eustace;Ichiro Fujinaga,A Comparative Survey of Image Binarisation Algorithms for Optical Recognition on Degraded Musical Sources.,2007,https://doi.org/10.5281/zenodo.1418203,John Ashley Burgoyne+Schulich School of Music of McGill University>CAN>education;Laurent Pugin+Schulich School of Music of McGill University>CAN>education;Greg Eustace+Schulich School of Music of McGill University>CAN>education;Ichiro Fujinaga+Schulich School of Music of McGill University>CAN>education,"Binarisation of greyscale images is a critical step in optical music recognition (OMR) preprocessing. Binarising music documents is particularly challenging because of the nature of music notation, even more so when the sources are degraded, e.g., with ink bleed-through from the other side of the page. This paper presents a comparative evaluation of 25 binarisation algorithms tested on a set of 100 music pages. A real-world OMR infrastructure for early music (Aruspix) was used to perform an objective, goal-directed evaluation of the algorithms’ performance. Our results differ significantly from the ones obtained in studies on non-music documents, which highlights the importance of developing tools specific to our community.",CAN,education,Developed economies,"[36.53482, 21.040552]","[-21.017904, 40.36262]","[19.407272, 10.02841, 7.542187]","[-10.972553, -23.657322, 3.1749868]","[8.684586, 6.2416077]","[6.630928, -0.68742925]","[10.760001, 11.253886, -0.0585469]","[7.8329253, 4.068981, 10.515592]"
119,Peter van Kranenburg;Jörg Garbers;Anja Volk;Frans Wiering;Louis P. Grijp;Remco C. Veltkamp,Towards Integration of MIR and Folk Song Research.,2007,https://doi.org/10.5281/zenodo.1414754,Peter van Kranenburg+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Jörg Garbers+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Anja Volk+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Frans Wiering+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Louis Grijp+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Remco C. Veltkamp+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown,"Folk song research (FSR) often deals with large collections of tunes that have various types of relations to each other. Computational methods can support the study of the contents of these collections. Music Information Retrieval (MIR) research provides such methods. Yet a fruitful cooperation of both disciplines is difficult to achieve. We present a role-model to structure this cooperation in which tasks and responsibilities are distributed among the roles of MIR, Computational Musicology (CM) and FSR.",NLD,education,Developed economies,"[-14.784932, 52.479786]","[25.9449, 34.91597]","[-35.23449, 1.4438732, 4.240731]","[3.6728098, 3.1354609, 19.004818]","[13.271713, 5.4194508]","[11.323837, 0.6528458]","[14.594991, 11.782078, -1.3235351]","[11.799604, 4.7504964, 12.249068]"
118,Jürgen Diet;Frank Kurth,The Probado Music Repository at the Bavarian State Library.,2007,https://doi.org/10.5281/zenodo.1417527,Jürgen Diet+Bavarian State Library>DEU>facility|University of Bonn>DEU>education;Frank Kurth+Bavarian State Library>DEU>facility|University of Bonn>DEU>education,"In this paper, we describe the Probado music repository which is currently set up at the Bavarian State Library, Munich, as part of the larger German Probado digital library initiative. Based on the FRBR approach, we propose a novel work-centric metadata model for organizing the document collection. The primary data contained in the repository currently consists of scanned sheet music and digitized audio recordings. The repository can be searched using both classical and content-based retrieval mechanisms. To this end, we propose a workflow for automated content-based document analysis and indexing.",DEU,facility,Developed economies,"[0.2267524, 25.57546]","[20.804468, 31.122112]","[-16.114613, -6.9992156, -4.788097]","[4.801088, -4.3773775, 16.56156]","[13.205825, 7.425978]","[10.841625, 0.33863196]","[13.771956, 13.819657, -1.8247126]","[11.655081, 4.8826275, 12.572052]"
117,Craig Stuart Sapp,Comparative Analysis of Multiple Musical Performances.,2007,https://doi.org/10.5281/zenodo.1417693,"Craig Stuart Sapp+Royal Holloway, University of London>GBR>education|Centre for the History and Analysis of Recorded Music (CHARM)>GBR>facility","A technique for comparing numerous performances of an identical selection of music is described. The basic methodology is to split a one-dimensional sequence into all possible sequential sub-sequences, perform some operation on these sequences, and then display a summary of the results as a two-dimensional plot; the horizontal axis being time and the vertical axis being sub-sequence length (longer lengths on top by convention). Most types of timewise data extracted from performances can be compared with this technique, although the current focus is on beat-level information for tempo and dynamics as well as commixtures of the two. The primary operation used on each sub-sequence is correlation between a reference performance and analogous segments of other performances, then selecting the best correlated performances for the summary display. The result is a useful navigational aid for coping with large numbers of performances of the same piece of music and for searching for possible influence between performances.",GBR,education,Developed economies,"[-14.517736, 6.436325]","[-26.69119, 5.3209777]","[-8.339148, -10.468743, -8.877255]","[-5.700659, 7.665149, -1.704842]","[12.453891, 7.3280935]","[6.001853, 1.4528676]","[13.324782, 13.243051, -1.4974738]","[8.276112, 6.469424, 11.754629]"
116,Wietse Balkema,Variable-Size Gaussian Mixture Models for Music Similarity Measures.,2007,https://doi.org/10.5281/zenodo.1415506,Wietse Balkema+Robert Bosch GmbH>DEU>company,"An algorithm to efficiently determine an appropriate number of components for a Gaussian mixture model is presented. For determining the optimal model complexity we do not use a classical iterative procedure, but use the strong correlation between a simple clustering method (BSAS [13]) and an MDL-based method [6]. This approach is computationally efficient and prevents the model from representing statistically irrelevant data. The performance of these variable size mixture models is evaluated with respect to hub occurrences, genre classification and computational complexity. Our variable size modelling approach marginally reduces the number of hubs, yields 3-4% better genre classification precision and is approximately 40% less computationally expensive.",DEU,company,Developed economies,"[-7.729813, 10.012088]","[28.68597, 4.7237034]","[-2.409773, 10.98666, -3.514116]","[23.120514, -2.1330843, 6.08276]","[12.722582, 8.997234]","[10.863914, 2.6286778]","[13.438009, 14.710659, -0.47244242]","[12.468936, 6.6684446, 12.334838]"
115,Jeremy Reed;Chin-Hui Lee,A Study on Attribute-Based Taxonomy for Music Information Retrieval.,2007,https://doi.org/10.5281/zenodo.1414910,Jeremy Reed+Georgia Institute of Technology>USA>education;Chin-Hui Lee+Georgia Institute of Technology>USA>education,"We propose an attribute-based taxonomy approach to providing alternative labels to music. Labels, such as genre, are often used as ground-truth for describing song similarity in music information retrieval (MIR) systems. A consistent labelling scheme is usually a key in determining quality of classifier learning in training and performance in testing of an MIR system. We examine links between conventional genre-based taxonomies and acoustical attributes available in text-based descriptions of songs. We show that the vector representation of each song based on these acoustic attributes enables a framework for unsupervised clustering of songs to produce alternative labels and quantitative measures of similarity between songs. Our experimental results demonstrate that this new set of labels are meaningful and classifiers based on these labels achieve similar or better results than those designed with existing genre-based labels.",USA,education,Developed economies,"[-19.327711, 21.2322]","[30.259018, -0.13579626]","[-11.816106, 5.6689863, -9.466756]","[14.783006, 8.566822, 2.790433]","[14.074656, 8.058557]","[10.804305, 3.1072345]","[14.220306, 14.782461, -1.8826354]","[12.427454, 6.3483553, 11.544365]"
98,Eric Nichols;Christopher Raphael,Automatic Transcription of Music Audio Through Continuous Parameter Tracking.,2007,https://doi.org/10.5281/zenodo.1416162,Eric Nichols+Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education,"We present a method for transcribing arbitrary pitched music into a piano-roll-like representation that also tracks the amplitudes of the notes over time. We develop a probabilistic model that gives the likelihood of a frame of audio data given a vector of amplitudes for the possible notes. Using an approximation of the log likelihood function, we develop an objective function that is quadratic in the time-varying amplitude variables, while also depending on the discrete piano-roll variables. We optimize this function using a variant of dynamic programming, by repeatedly growing and pruning our histories. We present results on a variety of different examples using several measures of performance including an edit-distance measure as well as a frame-by-frame measure.",USA,education,Developed economies,"[27.08933, -8.827102]","[-12.729587, -9.05137]","[15.38652, -5.5058417, 10.57614]","[0.63216573, -14.056761, -6.202628]","[9.716725, 7.6577916]","[6.692192, 2.687634]","[11.771895, 12.313759, -0.08396836]","[8.721473, 6.884146, 10.752447]"
114,Malcolm Slaney;William White,Similarity Based on Rating Data.,2007,https://doi.org/10.5281/zenodo.1416402,Malcolm Slaney+Yahoo! Research>USA>company|Yahoo! Media Innovation>USA>company;William White+Yahoo! Research>USA>company|Yahoo! Media Innovation>USA>company,"This paper describes an algorithm to measure the similarity of two multimedia objects, such as songs or movies, using users’ preferences. Much of the previous work on query-by-example (QBE) or music similarity uses detailed analysis of the object’s content. This is difficult and it is often impossible to capture how consumers react to the music. We argue that a large collection of user’s preferences is more accurate, at least in comparison to our benchmark system, at finding similar songs. We describe an algorithm based on the song’s rating data, and show how this approach works by measuring its performance using an objective metric based on whether the same artist performed both songs. Our similarity results are based on 1.5 million musical judgments by 380,000 users. We test our system by generating playlists using a content-based system, our rating-based system, and a random list of songs. Music listeners greatly preferred the ratings-based playlists over the content-based and random playlists.",USA,company,Developed economies,"[-41.486935, 30.993015]","[36.108498, 15.0707655]","[-4.4016466, 18.79728, -4.776305]","[14.054857, 3.7155972, 14.302265]","[13.352165, 9.356883]","[12.09634, 1.9559635]","[13.797833, 15.182164, -0.6719643]","[13.228067, 5.488044, 12.737767]"
112,Hiromasa Fujihara;Masataka Goto,A Music Information Retrieval System Based on Singing Voice Timbre.,2007,https://doi.org/10.5281/zenodo.1416228,Hiromasa Fujihara+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We developed a music information retrieval system based on singing voice timbre, i.e., a system that can search for songs in a database that have similar vocal timbres. To achieve this, we developed a method for extracting feature vectors that represent characteristics of singing voices and calculating the vocal-timbre similarity between two songs by using a mutual information content of their feature vectors. We operated the system using 75 songs and confirmed that the system worked appropriately. According to the results of a subjective experiment, 80% of subjects judged that compared with a conventional method using MFCC, our method finds more appropriate songs that have similar vocal timbres.",JPN,facility,Developed economies,"[-20.955135, 0.94538325]","[15.994101, 8.170248]","[12.369827, 13.36049, -10.202475]","[11.794911, 0.41649368, -0.11289212]","[10.630581, 11.220351]","[9.842582, 1.9992034]","[11.794338, 15.529064, 0.3470211]","[11.374294, 6.446942, 12.202725]"
111,Keiichiro Hoashi;Hiromi Ishizaki;Kazunori Matsumoto;Fumiaki Sugaya,Content-Based Music Retrieval Using Query Integration for Users with Diverse Preferences.,2007,https://doi.org/10.5281/zenodo.1416004,"Keiichiro Hoashi+KDDI R&D Laboratories, Inc.>JPN>company;Hiromi Ishizaki+KDDI R&D Laboratories, Inc.>JPN>company;Kazunori Matsumoto+KDDI R&D Laboratories, Inc.>JPN>company;Fumiaki Sugaya+KDDI R&D Laboratories, Inc.>JPN>company","This paper proposes content-based music information retrieval (MIR) methods based on user preferences, which aim to improve the accuracy of MIR for users with “diverse” preferences, i.e., users whose preferences range in songs with a wide variety of features. The proposed MIR method dynamically generates an optimal set of query vectors from the sample set of songs submitted by the user to express their preferences, based on the similarity of the songs in the sample set. Experiments conducted on a music collection with subjective user ratings verify that our proposal is effective to improve the accuracy of content-based MIR. Furthermore, by implementing a two-step MIR algorithm which utilizes song clustering results, the efficiency of the proposed MIR method is significantly improved.",JPN,company,Developed economies,"[-19.116304, 23.172375]","[34.25888, 15.0369625]","[-11.483404, 9.319103, -12.31282]","[12.648054, 1.8219274, 15.4977665]","[14.140822, 7.9857974]","[11.993611, 1.7414147]","[14.133507, 14.843686, -2.1456957]","[12.907903, 5.203199, 12.696211]"
110,Christian André Romming;Eleanor Selfridge-Field,Algorithms for Polyphonic Music Retrieval: The Hausdorff Metric and Geometric Hashing.,2007,https://doi.org/10.5281/zenodo.1417615,Christian André Romming+Stanford University>USA>education|Stanford University>USA>education;Eleanor Selfridge-Field+Stanford University>USA>education,"We consider two formulations of the computational problem of transposition-invariant, time-offset tolerant, meter-invariant, and time-scale invariant polyphonic music retrieval. We provide algorithms for both that are scalable in the sense that space requirements are asymptotically linear and queries are efficient for large databases of music. The focus is on cases where a query pattern M consisting of m events is to be matched against a database N consisting of n events, and m ≪ n. The database is assumed to be polyphonic, and the algorithms support polyphonic queries. We are interested in finding exact and proximate occurrences of the query pattern. The first problem considered is that of finding the minimum directed Hausdorff distance from M to N. We give a (2 + ǫ)-approximation algorithm that solves this problem in O (nm) query time and O (n) space. The second problem is that of finding all maximal subset matches of M in N, and we give an algorithm that solves this problem in O(m³ (k + 1)) query time and O(w²n) space, where w represents the maximum window size and k is the number of matches. Using the same method, the problem can be solved in O(m (k + 1)) query time and O(wn) space if we do not require the time-scale invariance property. The latter query time is asymptotically optimal for the given problem.",USA,education,Developed economies,"[-10.996595, 19.883814]","[10.102152, 17.865063]","[-2.1208532, 3.9062598, -9.413564]","[9.56904, -10.756975, 6.1304064]","[13.215709, 8.022166]","[9.055115, 0.76990813]","[13.082714, 14.4773035, -1.7848835]","[10.742067, 6.3730993, 13.347722]"
109,Jörg Garbers;Peter van Kranenburg;Anja Volk;Frans Wiering;Remco C. Veltkamp;Louis P. Grijp,Using Pitch Stability Among a Group of Aligned Query Melodies to Retrieve Unidentified Variant Melodies.,2007,https://doi.org/10.5281/zenodo.1418191,Jörg Garbers+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Peter van Kranenburg+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Anja Volk+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Frans Wiering+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Remco C. Veltkamp+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Louis P. Grijp+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown,"Melody identification is an important task in folk song variation research. In this paper we develop methods and tools that support researchers in finding melodies in a database that belong to the same variant group as a set of given melodies. The basic approach is to derive from the pitches of the known variants per onset a weighted pitch distribution, which quantifies pitch stability. We allow for partial matching and AND and OR queries. Technically we do so by defining a distance measure between weighted pitch distribution sequences. It is based on two applications of the Earth Mover’s Distance, which is a distribution distance. We set up a distance framework and discuss musically meaningful parameterizations for two tasks: a) Study the inner-group distances between the group as a whole and single members of the group. b) Use the group’s weighted pitch distribution sequence to query for variant melodies. The first experimental results seem very promising: a) The inner-group distances correlate to expert assigned subgroups. b) For variant retrieval our method works better than last year’s MIREX winner.",NLD,education,Developed economies,"[8.069156, -7.9921293]","[14.224708, 3.474093]","[7.783531, 8.32404, -0.30337298]","[4.4700456, 0.2374258, 2.8036203]","[10.651316, 9.876107]","[9.278245, 1.7775654]","[11.521387, 15.183467, -0.843102]","[11.088628, 7.0099163, 13.035798]"
108,Ioannis Karydis;Alexandros Nanopoulos;Apostolos N. Papadopoulos;Emilios Cambouropoulos,VISA: The Voice Integration/Segregation Algorithm.,2007,https://doi.org/10.5281/zenodo.1416552,Ioannis Karydis+Aristotle University of Thessaloniki>GRC>education;Alexandros Nanopoulos+Aristotle University of Thessaloniki>GRC>education;Apostolos N. Papadopoulos+Aristotle University of Thessaloniki>GRC>education;Emilios Cambouropoulos+Aristotle University of Thessaloniki>GRC>education,"Listeners are capable to perceive multiple voices in music. Adopting a perceptual view of musical ‘voice’ that corresponds to the notion of auditory stream, a computational model is developed that splits musical scores (symbolic musical data) into different voices. A single ‘voice’ may consist of more than one synchronous notes that are perceived as belonging to the same auditory stream; in this sense, the proposed algorithm, may separate a given musical work into fewer voices than the maximum number of notes in the greatest chord. This is paramount, among other, for developing MIR systems that enable pattern recognition and extraction within musically pertinent ‘voices’ (e.g. melodic lines). The algorithm is tested against a small dataset that acts as groundtruth.",GRC,education,Developed economies,"[-1.4657084, -46.232754]","[-9.967509, 26.658602]","[29.754992, 9.51503, -4.1122746]","[-8.802308, -11.524605, 7.287458]","[8.887928, 10.581474]","[8.569193, 2.4831536]","[10.992796, 14.321925, 1.4969246]","[10.203985, 6.313051, 10.870747]"
107,George Tzanetakis;Randy Jones;Kirk McNally,Stereo Panning Features for Classifying Recording Production Style.,2007,https://doi.org/10.5281/zenodo.1417537,George Tzanetakis+University of Victoria>CAN>education;Randy Jones+University of Victoria>CAN>education;Kirk McNally+University of Victoria>CAN>education,"Recording engineers, mixers and producers play important yet often overlooked roles in defining the sound of a particular record, artist or group. The placement of different sound sources in space using stereo panning information is an important component of the production process. Audio classification systems typically convert stereo signals to mono and to the best of our knowledge have not utilized information related to stereo panning. In this paper we propose a set of audio features that can be used to capture stereo information. These features are shown to provide statistically important information for non-trivial audio classification tasks and are compared with the traditional Mel-Frequency Cepstral Coefficients. The proposed features can be viewed as a first attempt to capture extra-musical information related to the production process through music information retrieval techniques.",CAN,education,Developed economies,"[20.925507, -35.00137]","[11.107635, -10.460455]","[5.5267005, -20.845446, -15.931934]","[10.789438, -0.82216144, -6.3273025]","[10.455557, 5.7548747]","[9.142807, 3.2892652]","[11.2783985, 12.473128, -1.6205738]","[10.858944, 7.321838, 10.661859]"
106,Pedro J. Ponce de León;David Rizo;José Manuel Iñesta Quereda,Towards a Human-Friendly Melody Characterization by Automatically Induced Rules.,2007,https://doi.org/10.5281/zenodo.1414902,Pedro J. Ponce de León+Universidad de Alicante>ESP>education;David Rizo+Universidad de Alicante>ESP>education;José M. Iñesta+Universidad de Alicante>ESP>education,"There is an increasing interest in music information retrieval for reference, motive, or thumbnail extraction from a piece in order to have a compact and representative representation of the information to be retrieved. One of the main references for music is its melody. In a practical environment of symbolic format collections the information can be found in standard MIDI file format, structured as a number of tracks, usually one of them containing the melodic line, while the others contain the accompaniment. The goal of this work is to analyse how statistical rules can be used to characterize a melody in such a way that one can understand the solution of an automatic system for selecting the track containing the melody in such files.",ESP,education,Developed economies,"[9.263722, -6.0885262]","[8.281946, 5.1695576]","[12.23542, 9.21427, -0.53686637]","[0.79392, -4.712148, 4.548891]","[10.484303, 9.725294]","[8.735558, 1.2763667]","[11.494115, 15.033097, -0.78678125]","[10.268653, 6.252955, 12.408162]"
105,Parag Chordia;Alex Rae,Raag Recognition Using Pitch-Class and Pitch-Class Dyad Distributions.,2007,https://doi.org/10.5281/zenodo.1416888,Parag Chordia+Georgia Institute of Technology>USA>education|Unknown>Unknown>Unknown;Alex Rae+Georgia Institute of Technology>USA>education|Unknown>Unknown>Unknown,"We describe the results of the first large-scale raag recognition experiment. Raags are the central structure of Indian classical music, each consisting of a unique set of complex melodic gestures. We construct a system to recognize raags based on pitch-class distributions (PCDs) and pitch-class dyad distributions (PCDDs) calculated directly from the audio signal. A large, diverse database consisting of 20 hours of recorded performances in 31 different raags by 19 different performers was assembled to train and test the system. Classification was performed using support vector machines, maximum a posteriori (MAP) rule using a multivariate likelihood model (MVN), and Random Forests. When classification was done on 60s segments, a maximum classification accuracy of 99.0% was attained in a cross-validation experiment. In a more difficult unseen generalization experiment, accuracy was 75%. The current work clearly demonstrates the effectiveness of PCDs and PCDDs in discriminating raags, even when musical differences are subtle.",USA,education,Developed economies,"[23.399666, -20.424393]","[8.042932, -16.691242]","[14.801526, -12.253165, -12.387211]","[15.092492, -7.6588583, -8.366987]","[10.08919, 5.8644753]","[7.496673, 1.4988867]","[11.140376, 13.250421, -0.75806975]","[9.283158, 7.258896, 12.464637]"
104,Jean-Julien Aucouturier;François Pachet;Pierre Roy;Anthony Beurivé,Signal + Context = Better Classification.,2007,https://doi.org/10.5281/zenodo.1416852,Jean-Julien Aucouturier+The University of Tokyo>JPN>education|SONY CSL Paris>FRA>company;François Pachet+SONY CSL Paris>FRA>company;Pierre Roy+SONY CSL Paris>FRA>company;Anthony Beurivé+SONY CSL Paris>FRA>company,"Typical signal-based approaches to extract musical descriptions from audio only have limited precision. A possible explanation is that they do not exploit context, which provides important cues in human cognitive processing of music: e.g. electric guitar is unlikely in 1930s music, children choirs rarely perform heavy metal, etc. We propose an architecture to train a large set of binary classifiers simultaneously, for many different musical metadata (genre, instrument, mood, etc.), in such a way that correlation between metadata is used to reinforce each individual classifier. The system is iterative: it uses classification decisions it made on some classification problems as new features for new, harder problems; and hybrid: it uses a signal classifier based on timbre similarity to bootstrap symbolic inference with decision trees. While further work is needed, the approach seems to outperform signal-only algorithms by 5% precision on average, and sometimes up to 15% for traditionally difficult problems such as cultural and subjective categories.",JPN,education,Developed economies,"[1.0128015, -18.342205]","[17.46512, -6.521462]","[1.9358755, -20.02647, 6.4028053]","[12.619249, 7.741272, -4.0135655]","[11.7322, 9.410535]","[9.470942, 3.5050886]","[12.144705, 13.3193245, 0.115537725]","[11.13587, 6.9872847, 10.671118]"
103,Yves Raimond;Samer A. Abdallah;Mark B. Sandler;Frederick Giasson,The Music Ontology.,2007,https://doi.org/10.5281/zenodo.1415966,"Yves Raimond+Queen Mary, University of London>GBR>education;Samer Abdallah+Queen Mary, University of London>GBR>education;Mark Sandler+Queen Mary, University of London>GBR>education;Frederick Giasson+Zitgist LLC>USA>company","In this paper, we overview some Semantic Web technologies and describe the Music Ontology: a formal framework for dealing with music-related information on the Semantic Web, including editorial, cultural and acoustic information. We detail how this ontology can act as a grounding for more domain-specific knowledge representation. In addition, we describe current projects involving the Music Ontology and interlinked repositories of music-related knowledge.",GBR,education,Developed economies,"[-26.289137, 37.61829]","[17.072042, 38.51434]","[-21.682196, -0.085369475, -3.6111073]","[-3.0157146, -2.1492374, 25.699263]","[14.188375, 9.002047]","[10.836426, -0.1658603]","[15.081657, 13.803406, -1.305218]","[12.156055, 5.1488395, 11.633792]"
102,Mark Levy;Mark B. Sandler,A Semantic Space for Music Derived from Social Tags.,2007,https://doi.org/10.5281/zenodo.1415628,"Mark Levy+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility;Mark Sandler+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility","In this paper we investigate social tags as a novel high-volume source of semantic metadata for music, using techniques from the fields of information retrieval and multivariate data analysis. We show that, despite the ad hoc and informal language of tagging, tags define a low-dimensional semantic space that is extremely well-behaved at the track level, in particular being highly organised by artist and musical genre. We introduce the use of Correspondence Analysis to visualise this semantic space, and show how it can be applied to create a browse-by-mood interface for a psychologically-motivated two-dimensional subspace representing musical emotion.",GBR,education,Developed economies,"[-43.089497, 3.1232822]","[43.181126, 2.6695075]","[-19.066519, 13.656968, 4.108187]","[22.23927, 11.414348, 9.396353]","[14.413741, 10.228815]","[12.366988, 3.2487981]","[15.454012, 14.314729, -0.10359719]","[13.600761, 5.7580576, 11.387382]"
101,David A. Torres;Douglas Turnbull;Luke Barrington;Gert R. G. Lanckriet,Identifying Words that are Musically Meaningful.,2007,https://doi.org/10.5281/zenodo.1417175,"David Torres+University of California, San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Douglas Turnbull+University of California, San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Luke Barrington+University of California, San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Gert Lanckriet+University of California, San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown","A musically meaningful vocabulary is one of the keystones in building a computer audition system that can model the semantics of audio content. If a word in the vocabulary is inconsistently used by human annotators, or the word is not clearly represented by the underlying acoustic representation, the word can be considered as noisy and should be removed from the vocabulary to denoise the modeling process. This paper proposes an approach to construct a vocabulary of predictive semantic concepts based on sparse canonical component analysis (sparse CCA). Experimental results illustrate that, by identifying musically meaningful words, we can improve the performance of a previously proposed computer audition system for music annotation and retrieval.",USA,education,Developed economies,"[0.6261988, 10.8052025]","[4.852842, -3.0641193]","[-9.622265, 3.001971, 4.2059526]","[19.155935, 9.682231, -12.399102]","[12.936077, 9.654018]","[6.7527947, 4.402119]","[13.370522, 15.2851515, -0.5522958]","[10.584535, 8.456686, 10.5725565]"
100,Jayme Garcia Arnal Barbedo;Amauri Lopes;Patrick J. Wolfe,High Time-Resolution Estimation of Multiple Fundamental Frequencies.,2007,https://doi.org/10.5281/zenodo.1415972,Jayme Garcia Arnal Barbedo+Harvard University>USA>education|State University of Campinas>BRA>education;Amauri Lopes+State University of Campinas>BRA>education;Patrick J. Wolfe+Harvard University>USA>education,"This paper presents a high time-resolution strategy to estimate multiple fundamental frequencies in musical signals. The signal is first divided into overlapping blocks, and a high-resolution estimate made of the short-term spectrum. The resulting spectrum is modified such that only the most relevant spectral components are considered, and an iterative algorithm based on earlier work by Klapuri is used to identify candidate fundamental frequencies. Finally, a context-based rule is used to improve the accuracy of fundamental frequency estimates. The performance of this technique is investigated under both noiseless and noisy conditions, and its accuracy is examined in cases where the polyphony is known and unknown a priori.",USA,education,Developed economies,"[37.053356, -18.6369]","[-46.32424, -16.356218]","[6.9423466, -24.132982, 8.494087]","[3.200234, -8.435927, -30.90196]","[8.895573, 8.801423]","[6.628935, 2.5906723]","[11.015293, 13.554646, -0.13013239]","[9.1173315, 8.152328, 10.977995]"
113,Hamish Allan;Daniel Müllensiefen;Geraint A. Wiggins,Methodological Considerations in Studies of Musical Similarity.,2007,https://doi.org/10.5281/zenodo.1416956,"Hamish Allan+Goldsmiths, University of London>GBR>education;Daniel Müllensiefen+Goldsmiths, University of London>GBR>education;Geraint Wiggins+Goldsmiths, University of London>GBR>education","There are many different aspects of musical similarity. Some relate to acoustic properties, such as melodic, rhythmic, harmonic and timbral. Others are bound up in cultural aspects: artists involved in creation, year of first release, subject matter of lyrics, demographics of listeners, etc. In judgments about musical similarity, the relative importance of each of these aspects will change, not only for different listeners, but also for the same listener in different contexts. Extra care must therefore be taken when designing studies in musical similarity to ensure that the context is an explicit variable. This paper describes the methodology behind our work in context-based musical similarity; introduces a novel system through which users can specify by example the context and focus of their retrieval needs; and details the design of a study to find parameters for our system which can also be adapted to test the system as a whole.",GBR,education,Developed economies,"[-1.5228561, 13.470403]","[17.390774, 7.978921]","[-5.7018375, 5.3273835, 2.9444044]","[6.7156887, 0.5294042, 8.7220335]","[12.881261, 9.530672]","[9.851406, 1.8920538]","[13.353448, 15.313571, -0.8062962]","[11.388872, 6.7536206, 12.655049]"
65,Tillman Weyde;Jens Wissmann;Kerstin Neubarth,An Experiment on the Role of Pitch Intervals in Melodic Segmentation.,2007,https://doi.org/10.5281/zenodo.1417591,Tillman Weyde+City University London>GBR>education;Jens Wissmann+City University London>GBR>education;Kerstin Neubarth+City University London>GBR>education,"This paper presents the results of an experiment to test the influence of IOI, dynamics, pitch change, and pitch direction change on melodic segmentation, extending an earlier experiment. The new results show little to no significant influence of pitch, when evaluated by a linear or log-linear statistical model with regression. This supports the earlier findings, which are in contrast to the commonly made assumption that greater pitch intervals lead to melodic segmentation.",GBR,education,Developed economies,"[4.7141643, -6.886363]","[2.411291, -7.8345647]","[9.815413, 7.2257905, -6.9390197]","[-4.080626, -8.666345, -0.15729076]","[10.734821, 9.8873825]","[7.4255347, 1.9882666]","[11.714161, 15.11742, -0.57462317]","[9.224729, 7.518708, 11.793289]"
86,Ajay Kapur;Graham Percival;Mathieu Lagrange;George Tzanetakis,Pedagogical Transcription for Multimodal Sitar Performance.,2007,https://doi.org/10.5281/zenodo.1417127,Ajay Kapur+University of Victoria>CAN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Graham Percival+University of Victoria>CAN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Mathieu Lagrange+University of Victoria>CAN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;George Tzanetakis+University of Victoria>CAN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques and the creation of annotated guidelines for students. We propose the use of modified traditional instruments enhanced with sensors which can obtain such data; as a case study we examine the sitar.",CAN,education,Developed economies,"[35.651497, -6.350321]","[-9.288792, -12.597509]","[18.37603, -5.8463197, 20.001982]","[-2.0511706, -9.897677, -9.638725]","[9.813039, 7.2262588]","[6.649058, 2.5357742]","[12.105251, 11.459367, -0.28175136]","[8.943288, 7.179229, 10.852585]"
63,Wei You;Roger B. Dannenberg,Polyphonic Music Note Onset Detection Using Semi-Supervised Learning.,2007,https://doi.org/10.5281/zenodo.1417385,Wei You+Carnegie Mellon University>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education,"Automatic note onset detection is particularly difficult in orchestral music (and polyphonic music in general). Machine learning offers one promising approach, but it is limited by the availability of labeled training data. Score-to-audio alignment, however, offers an economical way to locate onsets in recorded audio, and score data is freely available for many orchestral works in the form of standard MIDI files. Thus, large amounts of training data can be generated quickly, but it is limited by the accuracy of the alignment, which in turn is ultimately related to the problem of onset detection. Semi-supervised or bootstrapping techniques can be used to iteratively refine both onset detection functions and the data used to train the functions. We show that this approach can be used to improve and adapt a general purpose onset detection algorithm for use with orchestral music.",USA,education,Developed economies,"[28.928202, -28.184032]","[-20.245619, -8.545661]","[4.4725738, -21.866285, -6.9414563]","[-4.242814, 3.295717, -11.113895]","[10.316319, 5.059902]","[5.912525, 2.4660895]","[10.418823, 13.316154, -1.355057]","[8.2079525, 7.3250823, 10.8006935]"
29,Jérôme Lebossé;Luc Brun,Audio Fingerprint Identification by Approximate String Matching.,2007,https://doi.org/10.5281/zenodo.1417927,Jerome Lebosse+France Telecom R&D>FRA>company|GREYC UMR 6072>FRA>education;Luc Brun+GREYC UMR 6072>FRA>education,An audio fingerprint is a small digest of an audio file which allows to identify it among a database of candidates. This paper first presents a fingerprint extraction algorithm. The identification task is performed by a new identification scheme which combines string matching algorithms and q-grams filtration.,FRA,company,Developed economies,"[-15.181213, -25.584667]","[24.550406, -23.009163]","[6.163689, -10.433857, -23.125872]","[15.338143, -13.388323, 4.48497]","[9.051724, 4.484879]","[8.470752, -0.075611524]","[10.635782, 11.552759, -1.8832792]","[10.102426, 5.241191, 13.022235]"
28,Michaël Betser;Patrice Collen;Jean-Bernard Rault,Audio Identification Using Sinusoidal Modeling and Application to Jingle Detection.,2007,https://doi.org/10.5281/zenodo.1416890,Michaël Betser+France Télécom R&D>FRA>company;Patrice Collen+France Télécom R&D>FRA>company;Jean-Bernard Rault+France Télécom R&D>FRA>company,"This article presents a new descriptor dedicated to Audio Identification (audioID), based on sinusoidal modeling. The core idea is an appropriate selection of the sinusoidal components of the signal to be detected. This new descriptor is robust against usual distortions found in audioID tasks. It has several advantages compared to classical subband-based descriptors including an increased robustness to additive noise, especially non-random noise such as additional speech, and a robust detection of short audio events. This descriptor is compared to a classical subband-based feature for a jingle detection task on broadcast radio. It is shown that the new introduced descriptor greatly improves the performance in terms of recall/precision.",FRA,company,Developed economies,"[-13.480612, -29.2004]","[18.080833, -19.79194]","[9.611278, -9.951124, -25.81417]","[19.551598, -0.97220963, -10.877254]","[10.408118, 10.848569]","[9.313607, 2.895578]","[11.65773, 14.863417, 0.5908355]","[10.986825, 6.914345, 11.042937]"
27,Mehryar Mohri;Pedro J. Moreno;Eugene Weinstein,"Robust Music Identification, Detection, and Analysis.",2007,https://doi.org/10.5281/zenodo.1418043,Mehryar Mohri+Courant Institute of Mathematical Sciences>USA>education|Google Inc.>USA>company;Pedro Moreno+Courant Institute of Mathematical Sciences>USA>education|Google Inc.>USA>company;Eugene Weinstein+Courant Institute of Mathematical Sciences>USA>education|Google Inc.>USA>company,"In previous work, we presented a new approach to music identification based on finite-state transducers and Gaussian mixture models. Here, we expand this work and study the performance of our system in the presence of noise and distortions. We also evaluate a song detection method based on a universal background model in combination with a support vector machine classifier and provide some insight into why our transducer representation allows for accurate identification even when only a short song snippet is available.",USA,education,Developed economies,"[-4.37938, -7.881024]","[14.0582285, -19.3419]","[8.044715, -2.1380992, -4.015329]","[14.06804, -1.1942737, -13.851629]","[11.984946, 8.569381]","[8.358549, 3.4229472]","[12.647788, 14.207612, 0.31569117]","[10.447436, 8.004056, 10.80002]"
26,Neil J. Hurley;Félix Balado;Elizabeth P. McCarthy;Guenole C. M. Silvestre,Performance of Philips Audio Fingerprinting under Desynchronisation.,2007,https://doi.org/10.5281/zenodo.1416068,Neil J. Hurley+University College Dublin>IRL>education;Félix Balado+University College Dublin>IRL>education;Elizabeth P. McCarthy+University College Dublin>IRL>education;Guénolé C. M. Silvestre+University College Dublin>IRL>education,An audio fingerprint is a compact representation (robust hash) of an audio signal which is linked to its perceptual content. Perceptually equivalent instances of the signal must lead to the same hash value. Fingerprinting finds application in efficient indexing of music databases. We present a theoretical analysis of the Philips audio fingerprinting method under desynchronisation for correlated stationary Gaussian sources.,IRL,education,Developed economies,"[-15.663985, -27.000347]","[24.66353, -23.889826]","[5.093476, -13.969606, -21.871946]","[15.302293, -13.156327, 6.384556]","[9.047168, 4.444268]","[8.464277, -0.06794758]","[10.571577, 11.501571, -1.9313191]","[10.160214, 5.262384, 13.011802]"
25,Christian Fremerey;Frank Kurth;Meinard Müller;Michael Clausen,A Demonstration of the SyncPlayer System.,2007,https://doi.org/10.5281/zenodo.1415586,Christian Fremerey+Bonn University>DEU>education;Frank Kurth+Bonn University>DEU>education;Meinard Müller+Bonn University>DEU>education;Michael Clausen+Bonn University>DEU>education,"The SyncPlayer system is an advanced audio player for multimodal presentation, browsing, and retrieval of music data. The system has been extended significantly in the last few years. In this contribution, we describe the current state of the system and demonstrate the functionalities and interactions of the novel SyncPlayer components including combined inter- and intra-document music browsing.",DEU,education,Developed economies,"[25.3733, -31.923218]","[11.379302, 32.521603]","[-3.24346, -22.243496, -15.032592]","[-8.816808, 1.6299472, 20.94483]","[11.232337, 5.8004217]","[11.201308, 1.1038274]","[12.095485, 12.530693, -2.2242153]","[11.702973, 4.9433618, 12.577672]"
24,Olivier Lartillot;Petri Toiviainen,MIR in Matlab (II): A Toolbox for Musical Feature Extraction from Audio.,2007,https://doi.org/10.5281/zenodo.1417145,Olivier Lartillot+University of Jyvaskyla>FIN>education;Petri Toiviainen+University of Jyvaskyla>FIN>education,"We present the MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio files. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize. This paper offers an overview of the set of features, related, among others, to timbre, tonality, rhythm or form, that can be extracted with the MIRtoolbox. One particular analysis is provided as an example. The toolbox also includes functions for statistical analysis, segmentation and clustering. Particular attention has been paid to the design of a syntax that offers both simplicity of use and transparent adaptiveness to a multiplicity of possible input types. Each feature extraction method can accept as argument an audio file, or any preliminary result from intermediary stages of the chain of operations. Also the same syntax can be used for analyses of single audio files, batches of files, series of audio segments, multi-channel signals, etc. For that purpose, the data and methods of the toolbox are organised in an object-oriented architecture.",FIN,education,Developed economies,"[-8.618237, -16.150202]","[3.9079592, 29.886581]","[-7.472522, -12.137335, -2.936788]","[-7.715582, -4.6378727, 11.842743]","[13.060102, 5.633643]","[10.114874, 1.3406895]","[14.411919, 11.74062, -1.1726124]","[10.853638, 5.717009, 11.428718]"
23,Ian Knopke;Donald Byrd,Towards Musicdiff: A Foundation for Improved Optical Music Recognition Using Multiple Recognizers.,2007,https://doi.org/10.5281/zenodo.1416522,Ian Knopke+Indiana University>USA>education;Donald Byrd+Indiana University>USA>education,"This paper presents work towards a “musicdiff” program for comparing files representing different versions of the same piece, primarily in the context of comparing versions produced by different optical music recognition (OMR) programs. Previous work by the current authors and others strongly suggests that using multiple recognizers will make it possible to improve OMR accuracy substantially. The basic methodology requires several stages: documents must be scanned and submitted to several OMR programs, programs whose strengths and weaknesses have previously been evaluated in detail. We discuss techniques we have implemented for normalization, alignment and rudimentary error correction. We also describe a visualization tool for comparing multiple versions on a measure-by-measure basis.",USA,education,Developed economies,"[38.94892, 22.393959]","[-19.2285, 38.266506]","[18.361322, 15.018594, 8.575275]","[-9.497379, -21.885738, 4.5050797]","[8.612309, 6.141957]","[6.6247735, -0.41979688]","[10.662698, 11.112835, -0.12905796]","[8.052248, 4.23631, 10.723962]"
22,Bin Wei;Chengliang Zhang;Mitsunori Ogihara,Keyword Generation for Lyrics.,2007,https://doi.org/10.5281/zenodo.1418369,Bin Wei+University of Rochester>USA>education|University of Rochester>USA>education|University of Rochester>USA>education;Chengliang Zhang+University of Rochester>USA>education;Mitsunori Ogihara+University of Rochester>USA>education,This paper proposes a scheme for content based keyword generation of song lyrics. Syntactic as well semantic similarity is used for sentence level clustering to separate the topic from the background of a song. A method is proposed to search for a center in the semantic graph of WordNet for generating keywords not contained in original text.,USA,education,Developed economies,"[-31.759144, -32.923283]","[35.990646, -12.082989]","[7.5828943, 23.63995, -1.8442117]","[17.266014, -7.399027, 14.460641]","[11.510826, 11.734238]","[10.87948, 2.8597622]","[12.474288, 15.967567, 1.1039188]","[12.3537855, 6.4381514, 11.684332]"
21,Gijs Geleijnse;Jan H. M. Korst,Tool Play Live: Dealing with Ambiguity in Artist Similarity Mining from the Web.,2007,https://doi.org/10.5281/zenodo.1416430,Gijs Geleijnse+Philips Research>NLD>company;Jan Korst+Philips Research>NLD>company,"As methods in artist similarity identification using Web Music Information Retrieval perform well on known evaluation sets, we investigate the application of such a method to a more realistic data set. We notice that ambiguous artist names lead to unsatisfying results. We present a simple, efficient and unsupervised method to deal with ambiguous artist names.",NLD,company,Developed economies,"[-39.609795, 7.664446]","[36.96355, 6.686943]","[-23.003994, 10.064781, 7.249809]","[17.022102, 5.367436, 7.6468043]","[14.240844, 10.005587]","[11.714046, 2.6263332]","[15.018955, 14.795493, -0.07297614]","[13.025285, 6.036723, 12.03423]"
20,Markus Schedl;Gerhard Widmer;Tim Pohle;Klaus Seyerlehner,Web-Based Detection of Music Band Members and Line-Up.,2007,https://doi.org/10.5281/zenodo.1418325,Markus Schedl+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility;Tim Pohle+Johannes Kepler University>AUT>education;Klaus Seyerlehner+Johannes Kepler University>AUT>education,"We present first steps towards the automatic detection of music band members and instrumentation using web content mining techniques. To this end, we combine a named entity detection method with rule-based linguistic text analysis. We report on preliminary evaluation results and discuss limitations of the current method.",AUT,education,Developed economies,"[-30.496641, 30.07967]","[39.604645, 9.129064]","[-23.113409, 0.4818593, -16.875793]","[21.594156, 14.753478, 5.842026]","[14.311541, 7.6717224]","[11.217966, 2.9575348]","[14.413, 14.44798, -2.1133807]","[12.438439, 5.8360853, 11.478296]"
19,Tim Pohle;Peter Knees;Markus Schedl;Gerhard Widmer,Meaningfully Browsing Music Services.,2007,https://doi.org/10.5281/zenodo.1417777,Tim Pohle+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Peter Knees+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Markus Schedl+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"We present a browser application that offers the user an enhanced access to the content of music web services. Most importantly, the technique we apply aims at making it feasible to add to the automated suggestion of similar artists some intentional spin, or direction. At the heart of the algorithm, automatically derived artist descriptions are analyzed for common topics or aspects, and each artist is described by the extent to which it is associated with each of these topics. The browser application enables the user to formulate a query by means of these underlying topics by simply adjusting slider positions. The best matching artist is shown, and its web page found on the web music service is displayed.",AUT,education,Developed economies,"[-31.832287, 27.965391]","[37.711575, 7.816202]","[-19.388334, 13.278147, -15.162987]","[18.899012, 5.027512, 6.3170433]","[14.839575, 7.734399]","[11.688211, 2.145852]","[14.829899, 14.551534, -2.1669319]","[12.701737, 5.7242455, 12.399168]"
18,Cory McKay;Ichiro Fujinaga,jWebMiner: A Web-Based Feature Extractor.,2007,https://doi.org/10.5281/zenodo.1417679,Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"jWebMiner is a software package for extracting cultural features from the web. It is designed to be used for arbitrary types of MIR research, either as a stand-alone application or as part of the jMIR suite. It emphasizes extensibility, generality and an easy-to-use interface. At its most basic level, the software operates by using web services to extract hit counts from search engines. Functionality is available for calculating a variety of statistical features based on these counts, for variably weighting web sites or limiting searches only to particular sites, for excluding hits that do not contain particular filter terms, for defining synonym relationships between certain search strings, and for applying a number of additional search configurations.",CAN,education,Developed economies,"[15.523569, 38.392216]","[22.280306, 25.385082]","[-17.534716, -15.351761, -9.45484]","[-0.7409289, 1.0953727, 30.607105]","[12.78294, 7.253374]","[10.98013, 0.32069618]","[13.930016, 13.109705, -0.6391114]","[12.134713, 5.2115235, 12.186616]"
17,Alberto Novello;Martin F. McKinney,Assessment of Perceptual Music Similarity.,2007,https://doi.org/10.5281/zenodo.1415050,Alberto Novello+Philips Research Laboratories>NLD>company;Martin McKinney+Philips Research Laboratories>NLD>company,"This paper extends a study on music similarity perception presented at ISMIR last year, in which subjects ranked the similarity of excerpt-pairs presented in triads. The larger number of subjects and stimuli in the current study required a modification of the methodological strategy. We use here two nested incomplete block designs in order to cover the full set of song-excerpts comparisons (triads) while limiting the experimental time per subject. In addition to the two variable factors of the previous experiment, tempo and genre, we examine here the effect of prevalent instrument timbre. We found that 69 of 78 subjects were significantly consistent in their judgments of repeated triads. Furthermore, we found significant across-subject consistency on all 10 repeated triads. A significant difference was found in the distributions of inter- and intra-genre excerpt distances. The stress values in the Shepard’s plot show evidence of increased complexity in the present study compared to the previous smaller study.",NLD,company,Developed economies,"[-2.9524379, 13.296213]","[21.055368, 2.1922765]","[-5.741505, 7.278861, 2.0050452]","[7.1685896, 7.343171, 7.8761]","[12.887273, 9.407214]","[10.411602, 2.273248]","[13.45315, 15.307339, -0.7356917]","[11.778859, 6.94262, 12.792802]"
30,Yushen Han;Christopher Raphael,Desoloing Monaural Audio Using Mixture Models.,2007,https://doi.org/10.5281/zenodo.1417507,Yushen Han+Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education,"We describe a new approach to the “desoloing” problem, in which one tries to isolate the accompanying instruments from a monaural recording of a soloist with accompaniment. Our approach is based on explicit knowledge of the audio in the form of a score match – a correspondence between a symbolic score and the music audio, giving the times of all musical events. We employ the familiar idea of masking the short time Fourier transform to eliminate the solo part. The ideal mask is estimated by fitting a model to the data, whose note-based components are derived from the score match. The parameters for our probabilistic model are estimated using the EM algorithm.",USA,education,Developed economies,"[9.345425, -37.528744]","[-24.942371, -18.109955]","[25.589834, -1.6759942, -6.9096785]","[6.770263, -19.193968, -4.6891146]","[9.529231, 8.905747]","[6.610197, 4.7493706]","[12.089371, 13.173804, 0.9050321]","[9.631931, 8.055797, 10.144085]"
16,Carlos Gómez;Soraya Abad-Mota;Edna Ruckhaus,An Analysis of the Mongeau-Sankoff Algorithm for Music Information Retrieval.,2007,https://doi.org/10.5281/zenodo.1417931,Carlos Gómez+Universidad Simón Bolívar>VEN>education;Soraya Abad-Mota+Universidad Simón Bolívar>VEN>education;Edna Ruckhaus+Universidad Simón Bolívar>VEN>education,"""An essential problem in music information retrieval is to determine the similarity between two given melodies; there are several melodic similarity measures that have been proposed, among others, the Mongeau-Sankoff measure. In this work we implemented a modified version of the Mongeau-Sankoff measure. We conducted an experimental study to compare the implemented measure with other similarity measures; this evaluation was done in the context of the 2005 edition of the MIREX symbolic melodic similarity competition. The most relevant result of our work is an implementation of the Mongeau-Sankoff measure that presents greater effectiveness when compared to other current melodic similarity measures.""",VEN,education,Developing economies,"[-16.380413, 22.943245]","[17.416903, 5.80919]","[-9.325478, 7.659873, -14.34884]","[7.666676, 0.03901122, 6.481252]","[13.882968, 7.8711815]","[9.557957, 1.7415417]","[13.711105, 14.678751, -2.1202385]","[11.358843, 6.7734933, 12.914036]"
14,Elias Pampalk;Masataka Goto,MusicSun: A New Approach to Artist Recommendation.,2007,https://doi.org/10.5281/zenodo.1417487,Elias Pampalk+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"MusicSun is a graphical user interface to discover artists. Artists are recommended based on one or more artists selected by the user. The recommendations are computed by combining 3 different aspects of similarity. The users can change the impact of each of these aspects. In addition words are displayed which describe the artists selected by the user. The user can select one of these words to focus the search on a specific direction. In this paper we present the techniques used to compute the recommendations and the graphical user interface. Furthermore, we present the results of an evaluation with 33 users. We asked them, for example, to judge the usefulness of the different interface components and the quality of the recommendations.",JPN,facility,Developed economies,"[-45.869267, 26.719109]","[32.72438, 17.581158]","[-10.04622, 25.509846, -12.328683]","[14.438539, 1.3694426, 18.075687]","[15.920805, 9.23612]","[11.895999, 1.8972245]","[15.8300905, 15.735516, -1.4892104]","[12.921732, 5.540251, 12.774167]"
13,Amelie Anglade;Marco Tiemann;Fabio Vignoli,Virtual Communities for Creating Shared Music Channels.,2007,https://doi.org/10.5281/zenodo.1414710,Amélie Anglade+Philips Research Europe>NLD>company|Queen Mary University of London>GBR>education;Marco Tiemann+Philips Research Europe>NLD>company;Fabio Vignoli+Philips Research Europe>NLD>company,"We present an approach to automatically create virtual communities of users with similar music tastes. Our goal is to create personalized music channels for these communities in a distributed way, so that they can for example be used in peer-to-peer networks. To find suitable techniques for creating these communities we analyze graphs created from real-world recommender datasets and identify specific properties of these datasets. Based on these properties we select and evaluate different graph-based community-extraction techniques. We select a technique that exploits identified properties to create clusters of music listeners. We validate the suitability of this technique using a music dataset and a large movie dataset. On a graph of 6,040 peers, the selected technique assigns at least 85% of the peers to optimal communities, and obtains a mean classification error of less than 0.05 over the remaining peers that are not assigned to the best community.",NLD,company,Developed economies,"[-33.721638, 30.018587]","[42.724438, 10.929979]","[-24.60834, 13.210761, -13.59943]","[21.139889, 4.055118, 12.108255]","[14.734741, 8.866787]","[12.142693, 2.517574]","[15.054218, 14.468546, -1.4241027]","[13.258866, 5.6784053, 12.04823]"
12,Kazuyoshi Yoshii;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Improving Efficiency and Scalability of Model-Based Music Recommender System Based on Incremental Training.,2007,https://doi.org/10.5281/zenodo.1416880,Kazuyoshi Yoshii+Kyoto University>JPN>education|JSPS Research Fellow>JPN>Unknown;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazunori Komatani+Kyoto University>JPN>education;Tetsuya Ogata+Kyoto University>JPN>education;Hiroshi G. Okuno+Kyoto University>JPN>education,"We aimed at improving the efficiency and scalability of a hybrid music recommender system based on a probabilistic generative model that integrates both collaborative data (rating scores provided by users) and content-based data (acoustic features of musical pieces). Although the hybrid system was proved to make accurate recommendations, it lacks efficiency and scalability. In other words, the entire model needs to be re-trained from scratch whenever a new score, user, or piece is added. Furthermore, the system cannot deal with practical numbers of users and pieces on an enterprise scale. To improve efficiency, we propose an incremental method that partially updates the model at low computational cost. To enhance scalability, we propose a method that first constructs a small “core” model over fewer virtual representatives created from real users and pieces, and then adds the real users and pieces to the core model by using the incremental method. The experimental results revealed that the proposed system was not only efficient and scalable but also outperformed the original system in terms of accuracy.",JPN,education,Developed economies,"[-45.67674, 29.18963]","[38.30983, 15.323238]","[-7.863349, 26.503492, -15.499906]","[17.957542, 3.0883934, 15.482079]","[15.862288, 9.1598425]","[12.570995, 1.9985212]","[15.68811, 15.659266, -1.470269]","[13.507379, 5.352897, 12.730415]"
64,Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo,ATTA: Implementing GTTM on a Computer.,2007,https://doi.org/10.5281/zenodo.1418269,Masatoshi Hamanaka+University of Tsukuba>JPN>education;Keiji Hirata+University of Tsukuba>JPN>education;Satoshi Tojo+University of Tsukuba>JPN>education,"We have been discussing the design principle for the implementation of GTTM and presented the semi-automatic generation techniques of grouping structure, metrical structure, and time-span tree, and the searching method for the optimal parameter value assignments. In ISMIR2007, we organize a tutorial session on the techniques for implementing music theory GTTM for summarizing our work and report it to relevant participants of the conference. Since the time of the tutorial session is not enough, we demonstrate a working automatic time-span tree analyzer ATTA in a demo session. ATTA is an integration of our work done so far; by looking at the ATTA demonstration or using ATTA, people will be able to understand the techniques for implementing GTTM as well as GTTM itself in more detail.",JPN,education,Developed economies,"[-4.674658, 44.320335]","[-10.079615, 23.614634]","[-17.167332, -14.018447, -18.11495]","[-15.668887, 1.5512877, 9.70401]","[12.706095, 6.6700215]","[8.614769, 1.7607512]","[13.800815, 12.962609, -1.3731306]","[10.0348215, 6.253962, 11.953414]"
10,Christopher DeCoro;Zafer Barutçuoglu;Rebecca Fiebrink,Bayesian Aggregation for Hierarchical Genre Classification.,2007,https://doi.org/10.5281/zenodo.1416012,Christopher DeCoro+Princeton University>USA>education;Zafer Barutcuoglu+Princeton University>USA>education;Rebecca Fiebrink+Princeton University>USA>education,"Hierarchical taxonomies of classes arise in the analysis of many types of musical information, including genre, as a means of organizing overlapping categories at varying levels of generality. However, incorporating hierarchical structure into conventional machine learning systems presents a challenge: the use of independent binary classifiers for each class in the hierarchy can produce hierarchically inconsistent predictions. That is, an example may be assigned to a class, and not assigned to the parent of that class. This paper applies a Bayesian framework to combine, or aggregate, a hierarchy of multiple binary classifiers in a principled manner, and consequently improves performance over the hierarchy as a whole. Furthermore, such an approach allows for an arbitrarily complex hierarchy, and does not suffer from classes that are too broad or too refined. Experiments on the MIREX 2005 symbolic genre classification dataset show that our Bayesian Aggregation algorithm provides significant improvement over independent classifiers, and demonstrates superior performance compared to previous work. Our method also improves similarity search by ranking songs by similarity of hierarchical predictions to those of a query song.",USA,education,Developed economies,"[-31.46753, -12.06832]","[29.867945, -3.1677754]","[-20.284243, 6.0505133, 14.31212]","[15.583057, 7.6485224, -1.3930562]","[13.016057, 10.88399]","[10.49628, 3.4461558]","[13.96283, 14.258418, 1.4134014]","[12.165805, 6.6255975, 10.929919]"
9,Alastair J. D. Craft;Geraint A. Wiggins;Tim Crawford,How Many Beans Make Five? The Consensus Problem in Music-Genre Classification and a New Evaluation Method for Single-Genre Categorisation Systems.,2007,https://doi.org/10.5281/zenodo.1414982,"Alastair J. D. Craft+Goldsmiths, University of London>GBR>education;Geraint A. Wiggins+Goldsmiths, University of London>GBR>education;Tim Crawford+Goldsmiths, University of London>GBR>education","Genre definition and attribution is generally considered to be subjective. This makes evaluation of any genre-labelling system intrinsically difficult, as the ground-truth against which it is compared is based upon subjective responses, with little inter-participant consensus. This paper presents a novel method of analysing the results of a genre-labelling task, and demonstrates that there are groups of genre-labelling behaviour which are self-consistent. It is proposed that the evaluation of any genre classification system uses this modified analysis method.",GBR,education,Developed economies,"[-30.909893, -13.247055]","[28.90689, -1.6409358]","[-17.999586, 4.4737988, 12.168046]","[14.616818, 11.9715605, 3.4368594]","[13.078547, 10.891627]","[10.911806, 3.3300996]","[13.983676, 14.274126, 1.4249865]","[12.562059, 6.062961, 11.189993]"
8,Xiao Hu 0001;J. Stephen Downie,"Exploring Mood Metadata: Relationships with Genre, Artist and Usage Metadata.",2007,https://doi.org/10.5281/zenodo.1415126,Xiao Hu+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"There is a growing interest in developing and then evaluating Music Information Retrieval (MIR) systems that can provide automated access to the mood dimension of music. Mood as a music access feature, however, is not well understood in that the terms used to describe it are not standardized and their application can be highly idiosyncratic. To better understand how we might develop methods for comprehensively developing and formally evaluating useful automated mood access techniques, we explore the relationships that mood has with genre, artist and usage metadata. Statistical analyses of term interactions across three metadata collections (AllMusicGuide.com, epinions.com and Last.fm) reveal important consistencies within the genre-mood and artist-mood relationships. These consistencies lead us to recommend a cluster-based approach that overcomes specific term-related problems by creating a relatively small set of data-derived “mood spaces” that could form the ground-truth for a proposed MIREX “Automated Mood Classification” task.",USA,education,Developed economies,"[-50.315502, 4.2180104]","[51.232418, -2.9583683]","[-18.160181, 20.31767, 1.4753875]","[13.091297, 19.131414, 8.333845]","[13.50374, 12.2297125]","[13.078758, 3.5121787]","[15.924349, 14.864095, 1.1502658]","[14.222377, 5.082421, 10.854203]"
7,Thomas Lidy;Andreas Rauber;Antonio Pertusa;José Manuel Iñesta Quereda,Improving Genre Classification by Combination of Audio and Symbolic Descriptors Using a Transcription Systems.,2007,https://doi.org/10.5281/zenodo.1416344,Thomas Lidy+Vienna University of Technology>AUT>education;Andreas Rauber+Vienna University of Technology>AUT>education;Antonio Pertusa+University of Alicante>ESP>education;José Manuel Iñesta+University of Alicante>ESP>education,"Recent research in music genre classification hints at a glass ceiling being reached using timbral audio features. To overcome this, the combination of multiple different feature sets bearing diverse characteristics is needed. We propose a new approach to extend the scope of the features: We transcribe audio data into a symbolic form using a transcription system, extract symbolic descriptors from that representation and combine them with audio features. With this method, we are able to surpass the glass ceiling and to further improve music genre classification, as shown in the experiments through three reference music databases and comparison to previously published performance results.",AUT,education,Developed economies,"[-30.482527, -10.22819]","[13.771222, -7.0744596]","[-18.321554, 3.9182746, 19.96246]","[10.576794, 7.231123, -7.9651303]","[12.965682, 10.796886]","[9.357035, 3.2228155]","[13.798067, 14.168098, 1.2389795]","[11.071815, 7.307017, 10.816734]"
6,Alan Marsden,Automatic Derivation of Musical Structure: A Tool for Research on Schenkerian Analysis.,2007,https://doi.org/10.5281/zenodo.1415814,Alan Marsden+Lancaster University>GBR>education,"This paper describes software to facilitate research on the automatic derivation of hierarchical (Schenkerian) musical structures from a musical surface. Many MIR tasks require information about musical structure, or would perform better if such information were available. Automatic derivation of musical structure faces two significant obstacles. Firstly, the solution space of possible structural analyses of a piece is very large. Secondly, pieces can have more than one valid structural analysis, and there is little firm agreement among music theorists about how to distinguish a good analysis. To circumvent the first of these obstacles, software has been developed which derives a tractable ‘matrix’ of possibilities from a musical surface (i.e., MIDI-like note-time information). The matrix is somewhat like the intermediate results of a dynamic-programming algorithm, and in a similar way it is possible to extract a particular structural analysis from the matrix by following the appropriate path from the top level to the surface. It therefore provides a tool to facilitate research on the second obstacle by allowing candidate ‘goodness’ metrics to be incorporated into the software and tested on actual music.",GBR,education,Developed economies,"[22.72505, 28.455154]","[-11.690405, 21.12452]","[-8.232708, -10.259333, 6.7301254]","[-10.78982, -3.4149106, 9.053776]","[11.9083805, 7.96902]","[8.533492, 1.8263544]","[12.916435, 13.5526, -0.9145403]","[10.044224, 6.2111573, 11.793043]"
5,Douglas Turnbull;Gert R. G. Lanckriet;Elias Pampalk;Masataka Goto,A Supervised Approach for Detecting Boundaries in Music Using Difference Features and Boosting.,2007,https://doi.org/10.5281/zenodo.1415082,"Douglas Turnbull+University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education;Elias Pampalk+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility","A musical boundary is a transition between two musical segments such as a verse and a chorus. Our goal is to automatically detect musical boundaries using temporally-local audio features. We develop a set of difference features that indicate when there are changes in perceptual aspects (e.g., timbre, harmony, melody, rhythm) of the music. We show that many individual difference features are useful for detecting boundaries. By combining these features and formulating the problem as a supervised learning problem, we can further improve performance. This is an alternative to previous work on music segmentation which has focused on unsupervised approaches based on notions of self-similarity computed over an entire song. We evaluate performance using a publicly available data set of 100 copyright-cleared pop/rock songs, each of which has been segmented by a human expert.",USA,education,Developed economies,"[-5.5906453, -8.970092]","[-0.09112939, 3.491299]","[7.9291463, -0.39534217, -1.4273769]","[-0.7878627, 2.104273, -0.67748207]","[11.778907, 8.823524]","[8.434319, 2.9209137]","[12.576751, 14.167843, 0.4233156]","[10.559501, 7.270249, 11.466045]"
4,Meinard Müller;Michael Clausen,Transposition-Invariant Self-Similarity Matrices.,2007,https://doi.org/10.5281/zenodo.1414814,Meinard Müller+Bonn University>DEU>education|Unknown>Unknown>Unknown;Michael Clausen+Bonn University>DEU>education|Unknown>Unknown>Unknown,"Self-similarity matrices have become an important tool for visualizing the repetitive structure of a music recording. Transforming an audio data stream into a feature sequence, one obtains a self-similarity matrix by pairwise comparing all features of the sequence with respect to a local cost measure. The basic idea is that similar audio segments are revealed as paths of low cost along diagonals in the resulting self-similarity matrix. It is often the case, in particular for classical music, that certain musical parts are repeated in another key. In this paper, we introduce the concept of a transposition-invariant self-similarity matrix, which reveals the repetitive structure even in the presence of key transpositions. Furthermore, we introduce an associated transposition index matrix displaying harmonic relations within the music recording. As an application, we sketch how our concept can be used for the task of audio structure analysis.",DEU,education,Developed economies,"[13.142515, 31.095552]","[3.0335782, 1.8838425]","[-20.694145, -19.766153, 1.6833788]","[0.9218609, 4.484487, -5.006045]","[12.453452, 8.952227]","[7.8222384, 3.115659]","[13.168007, 14.451008, -0.47389603]","[10.688575, 7.928408, 11.587359]"
3,Christophe Rhodes;Michael A. Casey,Algorithms for Determining and Labelling Approximate Hierarchical Self-Similarity.,2007,https://doi.org/10.5281/zenodo.1418095,Christophe Rhodes+Goldsmiths University of London>GBR>education;Michael Casey+Goldsmiths University of London>GBR>education,"We describe an algorithm for finding approximate sequence similarity at all scales of interest, being explicit about our modelling assumptions and the parameters of the algorithm. We further present an algorithm for producing section labels based on the sequence similarity, and compare these labels with some expert-provided ground truth for a particular set of recordings.",GBR,education,Developed economies,"[14.174643, 31.421751]","[-10.702591, -18.58475]","[-20.081532, -18.609911, 3.18528]","[8.017276, 19.362957, -4.9993]","[12.518608, 9.024561]","[6.667202, 1.4427859]","[13.221206, 14.437232, -0.39507848]","[9.994913, 8.082976, 11.977727]"
2,Geoffroy Peeters,Sequence Representation of Music Structure Using Higher-Order Similarity Matrix and Maximum-Likelihood Approach.,2007,https://doi.org/10.5281/zenodo.1416748,Geoffroy Peeters+Ircam Sound Analysis/Synthesis Team - CNRS STMS>FRA>facility,"In this paper, we present a novel method for the automatic estimation of the structure of music tracks using a sequence representation. A set of timbre-related (MFCC and Spectral Contrast) and pitch-related (Pitch Class Profile) features are first extracted from the signal leading to three similarity matrices which are then combined. We then introduce the use of higher-order (2nd and 3rd order) similarity matrices in order to reinforce the diagonals corresponding to common repetitions and reduce the background noise. Segments are then detected and a maximum-likelihood approach is proposed in order to derive simultaneously the underlying sequence representation of the music track and the most representative segment of each sequence. The proposed method is evaluated positively on the MPEG-7 “melody repetition” test set.",FRA,facility,Developed economies,"[-1.5379356, 1.6962512]","[-2.9674964, 0.3556722]","[0.0534521, -4.5681343, 1.3165861]","[1.7196143, -1.3957881, -6.751874]","[11.918463, 8.224492]","[7.61729, 2.844584]","[12.694981, 13.819517, -0.43656558]","[10.246782, 7.830843, 11.44028]"
15,Jesper Højvang Jensen;Daniel P. W. Ellis;Mads Græsbøll Christensen;Søren Holdt Jensen,Evaluation of Distance Measures Between Gaussian Mixture Models of MFCCs.,2007,https://doi.org/10.5281/zenodo.1415752,Jesper Højvang Jensen+Aalborg University>DNK>education;Daniel P.W. Ellis+Columbia University>USA>education;Mads G. Christensen+Aalborg University>DNK>education;Søren Holdt Jensen+Aalborg University>DNK>education,"In music similarity and in the related task of genre classification, a distance measure between Gaussian mixture models is frequently needed. We present a comparison of the Kullback-Leibler distance, the earth movers distance and the normalized L2 distance for this application. Although the normalized L2 distance was slightly inferior to the Kullback-Leibler distance with respect to classification performance, it has the advantage of obeying the triangle inequality, which allows for efficient searching.",DNK,education,Developed economies,"[-7.5996594, 9.524248]","[27.025387, 5.2473664]","[-1.4951833, 12.486749, -3.2722898]","[21.431808, -3.4784076, 6.7920575]","[12.6855, 9.188942]","[10.68687, 2.4595962]","[13.383471, 14.659548, -0.29061002]","[12.215971, 6.6301894, 12.366407]"
31,Juan José Burred;Thomas Sikora,Monaural Source Separation from Musical Mixtures Based on Time-Frequency Timbre Models.,2007,https://doi.org/10.5281/zenodo.1416396,Juan José Burred+Technical University of Berlin>DEU>education;Thomas Sikora+Technical University of Berlin>DEU>education,"We present a system for source separation from monaural musical mixtures based on sinusoidal modeling and on a library of timbre models trained a priori. The models, which rely on Principal Component Analysis, serve as time-frequency probabilistic templates of the spectral envelope. They are used to match groups of sinusoidal tracks and assign them to a source, as well as to reconstruct overlapping partials. The proposed method does not make any assumptions on the harmonicity of the sources, and does not require a previous multipitch estimation stage. Since the timbre matching stage detects the instruments present on the mixture, the system can also be used for classification and segmentation.",DEU,education,Developed economies,"[7.7706323, -44.430843]","[-41.86882, -26.443974]","[27.10376, -1.9589999, -5.81109]","[-8.32463, -6.164791, -31.18517]","[8.456617, 9.853258]","[6.5801177, 5.245002]","[11.083773, 13.498749, 1.5146123]","[9.874048, 8.514095, 9.701623]"
11,Sally Jo Cunningham;David Bainbridge 0001;Dana McKay,Finding New Music: A Diary Study of Everyday Encounters with Novel Songs.,2007,https://doi.org/10.5281/zenodo.1417083,Sally Jo Cunningham+University of Waikato>NZL>education;David Bainbridge+University of Waikato>NZL>education;Dana McKay+Swinburne University of Technology>AUS>education,"This paper explores how we, as individuals, purposefully or serendipitously encounter “new music” (that is, music that we haven’t heard before) and relates these behaviours to music information retrieval activities such as music searching and music discovery via use of recommender systems. 41 participants participated in a three-day diary study, in which they recorded all incidents that brought them into contact with new music. The diaries were analyzed using a Grounded Theory approach. The results of this analysis are discussed with respect to location, time, and whether the music encounter was actively sought or occurred passively. Based on these results, we outline design implications for music information retrieval software, and suggest an extension of “laid back” searching.",NZL,education,Developed economies,"[-32.307346, 22.339546]","[33.611275, 31.962252]","[-17.20198, 15.232406, -5.874644]","[9.224483, 8.751709, 20.745436]","[14.965971, 8.511484]","[12.402463, 1.0169927]","[14.827283, 14.974333, -1.6408868]","[12.859118, 4.585186, 12.350763]"
33,Catherine Lai;Ichiro Fujinaga;David Descheneau;Michael Frishkopf;Jenn Riley;Joseph Hafner;Brian McMillan,Metadata Infrastructure for Sound Recordings.,2007,https://doi.org/10.5281/zenodo.1415224,Catherine Lai+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education;David Descheneau+University of Alberta>CAN>education;Michael Frishkopf+University of Alberta>CAN>education;Jenn Riley+Indiana University>USA>education;Joseph Hafner+McGill University>CAN>education;Brian McMillan+McGill University>CAN>education,"This paper describes the first iteration of a working model for searching heterogeneous distributed metadata repositories for sound recording collections, focusing on techniques used for real-time querying and harmonizing diverse metadata models. The initial model for a metadata infrastructure presented here is the first of its kind for sound recordings.",CAN,education,Developed economies,"[-7.554167, 29.71328]","[19.298304, 32.671307]","[-22.101954, -2.6468718, -11.530199]","[-1.6103506, -4.106976, 20.627691]","[14.126237, 8.8632555]","[10.835125, 0.265855]","[14.877779, 13.583608, -1.2998978]","[11.711945, 4.9047937, 12.260617]"
32,Alberto Pinto;Reinier H. van Leuken;M. Fatih Demirci;Frans Wiering;Remco C. Veltkamp,Indexing Music Collections Through Graph Spectra.,2007,https://doi.org/10.5281/zenodo.1416744,Alberto Pinto+Università degli Studi di Milano>ITA>education|Universiteit Utrecht>NLD>education;Reinier H. van Leuken+Universiteit Utrecht>NLD>education;M. Fatih Demirci+Universiteit Utrecht>NLD>education;Frans Wiering+Universiteit Utrecht>NLD>education;Remco C. Veltkamp+Universiteit Utrecht>NLD>education,"Content based music retrieval opens up large collections, both for the general public and music scholars. It basically enables the user to find (groups of) similar melodies, thus facilitating musicological research of many kinds. We present a graph spectral approach, new to the music retrieval field, in which melodies are represented as graphs, based on the intervals between the notes they are composed of. These graphs are then indexed into a database using their laplacian spectra as a feature vector. This laplacian spectrum is known to be very informative about the graph, and is therefore a good representative of the original melody. Consequently, range searching around the query spectrum returns similar melodies. We present an experimental evaluation of this approach, together with a comparison with two known retrieval techniques. On our test corpus, a subset of a well documented and annotated collection of Dutch folk songs, this evaluation demonstrates the effectiveness of the overall approach.",ITA,education,Developed economies,"[-16.020136, 25.378485]","[17.225155, 12.122022]","[-8.13773, 6.899426, -17.39521]","[7.0431347, -2.6761959, 8.578425]","[14.058719, 7.7604604]","[9.735847, 1.424264]","[13.993209, 14.447072, -2.3634024]","[11.295022, 6.3614774, 12.728494]"
62,Riccardo Miotto;Nicola Orio,A Methodology for the Segmentation and Identification of Music Works.,2007,https://doi.org/10.5281/zenodo.1415952,Riccardo Miotto+University of Padova>ITA>education;Nicola Orio+University of Padova>ITA>education,"The identification of unknown recordings is a challenging problem that has several applications. In this paper, we focus on the identification of alternative releases of a given music work. To this end, a statistical model of the possible performances of a given score is built from the recording of a single performance. The methodology is based on the automatic segmentation of audio recordings, exploiting a technique that has been proposed for text segmentation. The segmentation is followed by the automatic extraction of a set of relevant audio features from each segment. Identification is then carried out using an application of hidden Markov models. The approach has been tested with a collection of orchestral music, showing good results in the identification of acoustic performances.",ITA,education,Developed economies,"[-1.9763027, -5.3229117]","[-4.9366083, -0.8487623]","[7.3604107, -4.23875, -3.342905]","[2.4470596, -5.792542, -3.7844656]","[11.726797, 8.229447]","[8.088479, 2.9901524]","[12.425905, 14.146348, 0.101303965]","[10.366945, 7.722473, 11.363309]"
61,Paul H. Peeling;Ali Taylan Cemgil;Simon J. Godsill,A Probabilistic Framework for Matching Music Representations.,2007,https://doi.org/10.5281/zenodo.1417677,Paul Peeling+Cambridge University>GBR>education;A. Taylan Cemgil+Cambridge University>GBR>education;Simon Godsill+Cambridge University>GBR>education,"In this paper we introduce a probabilistic framework for matching different music representations (score, MIDI, audio) by incorporating models of how one musical representation might be rendered from another. We propose a dynamical hidden Markov model for the score pointer as a prior, and two observation models, the first based on matching spectrogram data to a trained template, the second detecting damped sinusoids within a frame of audio by subspace methods. The resulting Bayesian framework is robust to local variations in tempo, and can be used for a wide variety of applications. We evaluate both methods in a score alignment context by inferring the posterior distribution of the current position in the score exactly. The spectrogram method is shown to infer the score position reliably with minimal computation, and the damped sinusoid model is able to pinpoint the positions of score events in the audio with a high level of timing accuracy.",GBR,education,Developed economies,"[-8.656762, 9.795392]","[-17.29097, -11.386839]","[-1.6644309, 9.488282, -4.695052]","[2.8179328, -18.468395, -4.833183]","[12.525892, 8.716479]","[6.1093307, 1.2541474]","[13.399102, 13.935408, -0.46484402]","[8.227679, 6.3885007, 10.902968]"
60,Frank Kurth;Meinard Müller;Christian Fremerey;Yoon-ha Chang;Michael Clausen,Automated Synchronization of Scanned Sheet Music with Audio Recordings.,2007,https://doi.org/10.5281/zenodo.1416918,Frank Kurth+Bonn University>DEU>education;Meinard Müller+Bonn University>DEU>education;Christian Fremerey+Bonn University>DEU>education;Yoon-ha Chang+Bonn University>DEU>education;Michael Clausen+Bonn University>DEU>education,"In this paper, we present a procedure for automatically synchronizing scanned sheet music with a corresponding CD audio recording, where suitable regions (given in pixels) of the scanned digital images are linked to time positions of the audio file. In a first step, we extract note parameters and 2D position information from the scanned images using standard software for optical music recognition (OMR). We then use a chroma-based synchronization algorithm to align the note parameters to the given audio recording. Our experiments show that even though the output of current OMR software is often erroneous, the music parameters extracted from the digital images still suffice to derive a reasonable alignment with the audio data stream. The resulting link structure can be used to highlight the current position in the scanned score or to automatically turn pages during playback of an audio recording. Such functionalities have been realized as plug-in for the SyncPlayer, which is a free prototypical software framework for bringing together various MIR techniques and applications.",DEU,education,Developed economies,"[30.594278, 8.179174]","[-12.799603, -15.133527]","[6.9861364, -12.354986, -14.083593]","[-4.5367193, -22.50428, 0.34552258]","[10.83654, 6.3139396]","[6.3350964, 0.30574906]","[12.14262, 12.140213, -1.5349154]","[8.078577, 5.368826, 10.836834]"
59,Matthias Mauch;Simon Dixon;Christopher Harte;Michael A. Casey;Benjamin Fields,Discovering Chord Idioms Through Beatles and Real Book Songs.,2007,https://doi.org/10.5281/zenodo.1415564,"Matthias Mauch+Queen Mary, University of London>GBR>education;Simon Dixon+Queen Mary, University of London>GBR>education;Christopher Harte+Queen Mary, University of London>GBR>education;Michael Casey+Goldsmiths, University of London>GBR>education;Benjamin Fields+Goldsmiths, University of London>GBR>education","Modern collections of symbolic and audio music content provide unprecedented possibilities for musicological research, but traditional qualitative evaluation methods cannot realistically cope with such amounts of data. We are interested in harmonic analysis and propose key-independent chord idioms derived from a bottom-up analysis of musical data as a new subject of musicological interest. In order to motivate future research on audio chord idioms and on probabilistic models of harmony we perform a quantitative study of chord progressions in two popular music collections. In particular, we extract common subsequences of chord classes from symbolic data, independent of key and context, and order them by frequency of occurrence, thus enabling us to identify chord idioms. We make musicological observations on selected chord idioms from the collections.",GBR,education,Developed economies,"[57.44212, -3.586999]","[-21.92613, 18.990826]","[25.22007, -11.9181385, 22.312239]","[-20.237143, -3.2702355, 7.949773]","[6.696682, 8.663792]","[7.670297, 2.3988662]","[11.901594, 10.379109, 2.1132734]","[10.003876, 8.088746, 12.497095]"
57,Kyogu Lee;Malcolm Slaney,A Unified System for Chord Transcription and Key Extraction Using Hidden Markov Models.,2007,https://doi.org/10.5281/zenodo.1415208,Kyogu Lee+Stanford University>USA>education|Yahoo! Research>USA>company;Malcolm Slaney+Yahoo! Research>USA>company,"A new approach to acoustic chord transcription and key extraction is presented. As in an isolated word recognizer in automatic speech recognition systems, we treat a musical key as a word and build a separate hidden Markov model for each key in 24 major/minor keys. In order to acquire a large set of labeled training data for supervised training, we first perform harmonic analysis on symbolic data to extract the key information and the chord labels with precise segment boundaries. In parallel, we synthesize audio from the same symbolic data whose harmonic progression are in perfect alignment with the automatically generated annotations. We then estimate the model parameters directly from the labeled training data, and build 24 key-specific HMMs. The experimental results show that the proposed model not only successfully estimates the key, but also yields higher chord recognition accuracy than a universal, key-independent model.",USA,education,Developed economies,"[51.85746, -4.963172]","[-32.424744, 20.009062]","[24.135122, -11.323855, 16.571964]","[-25.132084, -6.0950747, 0.22556952]","[7.0170193, 8.763124]","[6.198147, 3.6875558]","[11.9848795, 10.587923, 1.8684576]","[9.701073, 8.910135, 12.161149]"
56,Juan Pablo Bello,"Audio-Based Cover Song Retrieval Using Approximate Chord Sequences: Testing Shifts, Gaps, Swaps and Beats.",2007,https://doi.org/10.5281/zenodo.1417911,Juan Pablo Bello+New York University>USA>education,"This paper presents a variation on the theme of using string alignment for MIR in the context of cover song identification in audio collections. Here, the strings are derived from audio by means of HMM-based chord estimation. The characteristics of the cover-song ID problem and the nature of common chord estimation errors are carefully considered. As a result strategies are proposed and systematically evaluated for key shifting, the cost of gap insertions and character swaps in string alignment, and the use of a beat-synchronous feature set. Results support the view that string alignment, as a mechanism for audio-based retrieval, cannot be oblivious to the problems of robustly estimating musically-meaningful data from audio.",USA,education,Developed economies,"[6.784016, 41.913837]","[5.606386, 9.638308]","[1.1918226, 12.001992, -20.482649]","[-8.836867, 15.977897, 14.006332]","[16.0473, 11.068169]","[10.036573, 2.1539474]","[12.890896, 17.279352, -0.38934353]","[11.511199, 6.307748, 11.8314495]"
55,Pierre Leveau;David Sodoyer;Laurent Daudet,Automatic Instrument Recognition in a Polyphonic Mixture Using Sparse Representations.,2007,https://doi.org/10.5281/zenodo.1414984,Pierre Leveau+GET-ENST (Télécom Paris)>FRA>education|University Pierre et Marie Curie>FRA>education;David Sodoyer+University Pierre et Marie Curie>FRA>education;Laurent Daudet+University Pierre et Marie Curie>FRA>education,"In this paper, we introduce a method to address automatic instrument recognition in polyphonic music. It is based on the decomposition of the music signal with instrument-specific harmonic atoms, yielding an approximate object representation of the signal. A post-processing is then applied to exhibit ensemble saliences that give clues about the number of instruments and their labels. The whole algorithm is then applied on artificial mixes of solo performances. The identification of the number of instrument reaches 73 % on 10-s segments and the fully blind problem of identification of the ensemble label without prior knowledge on the number of instruments is 17 %.",FRA,education,Developed economies,"[6.950078, -23.801876]","[-9.737391, 1.5427771]","[18.995754, -3.768359, -2.189823]","[9.232053, -1.574114, -10.327303]","[8.887209, 7.3534575]","[8.554913, 3.7267053]","[11.220301, 12.647913, 0.53513527]","[10.263145, 7.871511, 10.254792]"
54,Pierre Roy;François Pachet;Sergio Krakowski,Improving the Classification of Percussive Sounds with Analytical Features: A Case Study.,2007,https://doi.org/10.5281/zenodo.1417875,Pierre Roy+Sony CSL>FRA>company;François Pachet+Sony CSL>FRA>company;Sergio Krakowski+Sony CSL>FRA>company,"There is an increasing need for automatically classifying sounds for MIR and interactive music applications. In the context of supervised classification, we conducted experiments with so-called analytical features, an approach that improves the performance of the general bag-of-frame scheme without loosing its generality. These analytical features are better, in a sense we define precisely than standard, general features, or even than ad hoc features designed by hand for specific problems. Our method allows us to build a large number of these features, evaluate and select them automatically for arbitrary audio classification problems. We present here a specific study concerning the analysis of Pandeiro (Brazilian tambourine) sounds. Two problems are considered: the classification of entire sounds, for MIR applications, and the classification of attack portions of the sound only, for interactive music applications. We evaluate precisely the gain obtained by analytical features on these two problems, in comparison with standard approaches.",FRA,company,Developed economies,"[-22.412169, -6.4776893]","[12.762441, -11.708811]","[-13.879345, -3.7769396, 21.232376]","[10.842769, 5.9478188, -11.231182]","[12.406933, 10.402819]","[9.269736, 3.6866996]","[13.418699, 13.9942045, 1.0676792]","[10.739791, 6.846336, 10.423328]"
53,Jouni Paulus;Anssi Klapuri,Combining Temporal and Spectral Features in HMM-Based Drum Transcription.,2007,https://doi.org/10.5281/zenodo.1417257,Jouni Paulus+Tampere University of Technology>FIN>education;Anssi Klapuri+Tampere University of Technology>FIN>education,"To date several methods for transcribing drums from polyphonic music have been published. Majority of the features used in the transcription systems are “spectral”: parameterising some property of the signal spectrum in a relatively short time frames. It has been shown that utilising narrow-band features describing long-term temporal evolution in conjunction with the more traditional features can improve the overall performance in speech recognition. We investigate similar utilisation of temporal features in addition to the HMM baseline. The effect of the proposed extension is evaluated with simulations on acoustic data, and the results suggest that temporal features do improve the result slightly. Demonstrational signals of the transcription results are available at http://www.cs.tut.fi/sgn/arg/paulus/demo/.",FIN,education,Developed economies,"[28.875746, -45.567055]","[-16.023827, 0.043438796]","[20.465515, -20.350386, 3.739624]","[6.7771034, 7.722327, -16.74619]","[7.596598, 7.1685987]","[8.378778, 3.8831713]","[10.35655, 11.455704, 1.0510623]","[9.536758, 7.513333, 10.308615]"
52,Olivier Gillet;Gaël Richard,Supervised and Unsupervised Sequence Modelling for Drum Transcription.,2007,https://doi.org/10.5281/zenodo.1417237,Olivier Gillet+GET/Télécom Paris>FRA>education|CNRS LTCI>FRA>facility;Gaël Richard+GET/Télécom Paris>FRA>education|CNRS LTCI>FRA>facility,"We discuss in this paper two post-processings for drum transcription systems, which aim to model typical properties of drum sequences. Both methods operate on a symbolic representation of the sequence, which is obtained by quantizing the onsets of drum strokes on an optimal tatum grid, and by fusing the posterior probabilities produced by the drum transcription system. The first proposed method is a generalization of the N-gram model. We discuss several training and recognition strategies (style-dependent models, local models) in order to maximize the reliability and the specificity of the trained models. Alternatively, we introduce a novel unsupervised algorithm based on a complexity criterion, which finds the most regular and well-structured sequence compatible with the acoustic scores produced by the transcription system. Both approaches are evaluated on a subset of the ENST-drums corpus, and yield performance improvements.",FRA,education,Developed economies,"[28.633646, -44.993397]","[-35.855583, -13.420873]","[20.436789, -20.902916, 5.4564533]","[0.4255821, 13.303529, -18.257803]","[7.5790634, 7.165463]","[8.383869, 4.312256]","[10.358153, 11.467967, 1.0501531]","[9.2367935, 7.2121234, 9.856951]"
51,Luis Gustavo Martins;Juan José Burred;George Tzanetakis;Mathieu Lagrange,Polyphonic Instrument Recognition Using Spectral Clustering.,2007,https://doi.org/10.5281/zenodo.1415074,Luis Gustavo Martins+INESC Porto>PRT>facility;Juan José Burred+Technical University of Berlin>DEU>education;George Tzanetakis+University of Victoria>CAN>education;Mathieu Lagrange+University of Victoria>CAN>education,"The identification of the instruments playing in a polyphonic music signal is an important and unsolved problem in Music Information Retrieval. In this paper, we propose a framework for the sound source separation and timbre classification of polyphonic, multi-instrumental music signals. The sound source separation method is inspired by ideas from Computational Auditory Scene Analysis and formulated as a graph partitioning problem. It utilizes a sinusoidal analysis front-end and makes use of the normalized cut, applied as a global criterion for segmenting graphs. Timbre models for six musical instruments are used for the classification of the resulting sound sources. The proposed framework is evaluated on a dataset consisting of mixtures of a variable number of simultaneous pitches and instruments, up to a maximum of four concurrent notes.",PRT,facility,Developed economies,"[7.6002364, -23.179459]","[-41.99377, -26.934263]","[17.455906, -4.6883235, -1.9332849]","[-7.6939487, -5.091339, -30.629038]","[8.937492, 7.304418]","[6.7744236, 4.974082]","[11.128917, 12.599294, 0.47390175]","[9.979185, 8.406602, 9.899837]"
50,Gabriel Gatzsche;Markus Mehnert;David Gatzsche;Karlheinz Brandenburg,A Symmetry Based Approach for Musical Tonality Analysis.,2007,https://doi.org/10.5281/zenodo.1416214,G. Gatzsche+Fraunhofer IDMT>DEU>facility;M. Mehnert+Technische Universität Ilmenau>DEU>education;D. Gatzsche+Hochschule für Musik Franz Liszt Weimar>DEU>education;K. Brandenburg+Technische Universität Ilmenau>DEU>education,"We present a geometric approach for tonality analysis called symmetry model. To derive the symmetry model, Carol L. Krumhansl and E.J. Kessler’s toroidal Multi Dimensional Scaling (MDS) solution is separated into a key spanning and a key related component. While the key spanning component represents relationships between different keys, the key related component is suitable for the analysis of inner relationships of diatonic keys, for example tension or resolution tendencies, or functional relationships. These features are directly related to the symmetric organisation of tones around the tonal center, which is particularly visualized by the key related component.",DEU,facility,Developed economies,"[9.437418, 20.644033]","[-17.875725, 20.012154]","[-5.18931, 15.572613, 9.0562315]","[-18.084213, -8.901807, 9.783666]","[12.406403, 8.887012]","[7.776884, 2.195432]","[12.72666, 14.332888, -0.7019765]","[10.073522, 7.748883, 12.388937]"
49,Gabi Teodoru;Christopher Raphael,Pitch Spelling with Conditionally Independent Voices.,2007,https://doi.org/10.5281/zenodo.1414946,Gabi Teodoru+Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education,"We introduce a new approach for pitch spelling from MIDI data based on a probabilistic model. The model uses a hidden sequence of variables, one for each measure, describing the local key of the music. The spellings in the voices evolve as conditionally independent Markov chains, given the hidden keys. The model represents both vertical relations through the shared key and horizontal voice-leading relations through the explicit Markov models for the voices. This conditionally independent voice model leads to an efficient dynamic programming algorithm for finding the most likely configuration of hidden variables — spellings and harmonic sequence. The model is also straightforward to train from unlabeled data, though we have not been able to demonstrate any improvement in performance due to training. Our results compare favorably with others when tested on Meredith’s corpus, designed specifically for this problem.",USA,education,Developed economies,"[26.588924, -19.573687]","[-11.681125, -9.920673]","[19.491728, -17.362957, -11.113459]","[-1.0635147, -12.772368, -5.2490444]","[9.848773, 5.806906]","[6.8111653, 2.9843464]","[10.879851, 13.306561, -0.7141229]","[9.0176115, 7.2034345, 10.740048]"
58,John Ashley Burgoyne;Laurent Pugin;Corey Kereliuk;Ichiro Fujinaga,A Cross-Validated Study of Modelling Strategies for Automatic Chord Recognition in Audio.,2007,https://doi.org/10.5281/zenodo.1416816,John Ashley Burgoyne+McGill University>CAN>education;Laurent Pugin+McGill University>CAN>education;Corey Kereliuk+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Although automatic chord recognition has generated a number of recent papers in MIR, nobody to date has done a proper cross validation of their recognition results. Cross validation is the most common way to establish baseline standards and make comparisons, e.g., for MIREX competitions, but a lack of labelled aligned training data has rendered it impractical. In this paper, we present a comparison of several modelling strategies for chord recognition, hidden Markov models (HMMs) and conditional random fields (CRFs), on a new set of aligned ground truth for the Beatles data set of Sheh and Ellis (2003). Consistent with previous work, our models use pitch class profile (PCP) vectors for audio modelling. Our results show improvement over previous literature, provide precise estimates of the performance of both old and new approaches to the problem, and suggest several avenues for future work.",CAN,education,Developed economies,"[54.346336, -5.784602]","[-32.66807, 18.228508]","[26.717371, -12.381463, 15.95674]","[-22.791973, -5.0388665, -1.2309012]","[6.8047757, 8.596339]","[6.0951176, 3.7143822]","[11.894045, 10.400411, 2.0958645]","[9.709545, 8.919228, 12.136511]"
47,Arpi Mardirossian;Elaine Chew,Visualizing Music: Tonal Progressions and Distributions.,2007,https://doi.org/10.5281/zenodo.1417773,Arpi Mardirossian+University of Southern California>USA>education|University of Southern California>USA>education;Elaine Chew+University of Southern California>USA>education|University of Southern California>USA>education,"This paper presents a music visualization tool that shows the tonal progression in, and tonal distribution of, a piece of music on Lerdahl’s two-dimensional tonal pitch space. The method segments a piece into uniform time slices, and determines the most likely key in each slice. It then generates the visualization by dynamically showing the sequence of keys as translucent, growing discs on the two-dimensional plane. The frequency of a key is indicated by the size of its colored disc. Each color and position corresponds to a key, and related keys are shown in proximity with related colors. The visual result effectively presents the changing distribution of the keys employed. The proposed visualization is an improvement over more basic charting methods, such as histograms, and it maintains standards of information design in the form of added dimensionality, color, and animation. We show that the visualization is invariant under music transformations that preserve the piece’s identity. We conclude by illustrating how this method may be used to visually distinguish between tonal progression and distribution patterns in western classical versus Armenian folk music.",USA,education,Developed economies,"[-15.307637, 32.853996]","[-16.410667, 13.86326]","[-14.111271, 10.036595, -25.663631]","[-11.608642, 0.7451901, 3.2611098]","[13.753019, 6.852718]","[8.093149, 2.1764858]","[13.962592, 13.598381, -2.411835]","[10.000769, 6.973085, 12.092527]"
34,Christian Landone;Joseph Harrop;Josh Reiss,"Enabling Access to Sound Archives Through Integration, Enrichment and Retrieval: The EASAIER Project.",2007,https://doi.org/10.5281/zenodo.1415584,"Christian Landone+Centre for Digital Music, Queen Mary University of London>GBR>education;Joseph Harrop+Centre for Digital Music, Queen Mary University of London>GBR>education;Josh Reiss+Centre for Digital Music, Queen Mary University of London>GBR>education|Royal Scottish Academy of Music and Drama>GBR>education","Many digital sound archives suffer from problems concerning online access: sound materials are often held separately from other related media, they are not easily browsed and little opportunity to search the actual audio content of the material is provided. The EASAIER project aims to alleviate these problems, offering a number of solutions to support sound archive managers and users. EASAIER will enable enhanced access to sound archives, providing multiple methods of retrieval, integration with other media archives, content enrichment and enhanced access tools.",GBR,education,Developed economies,"[-7.393236, 31.073837]","[24.08035, 37.591206]","[-20.356533, -3.6220326, -15.619961]","[-3.013194, 4.67751, 25.248396]","[13.480539, 8.032919]","[11.234946, 0.17039546]","[14.260173, 13.906002, -1.5821129]","[11.841014, 4.594549, 12.327957]"
48,Özgür Izmirli,Localized Key Finding from Audio Using Nonnegative Matrix Factorization for Segmentation.,2007,https://doi.org/10.5281/zenodo.1417197,Özgür İzmirli+Connecticut College>USA>education,"A model for localized key finding from audio is proposed. Besides being able to estimate the key in which a piece starts, the model can also identify points of modulation and label multiple sections with their key names throughout a single piece. The front-end employs an adaptive tuning stage prior to spectral analysis and calculation of chroma features. The segmentation stage uses groups of contiguous chroma vectors as input and identifies sections that are candidates for unique local keys in relation to their neighboring key centers. Non-negative matrix factorization with additional sparsity constraints and additive updates is used for segmentation. The use of segmentation is demonstrated for single and multiple key estimation problems. A correlational model of key finding is applied to the candidate segments to estimate the local keys. Evaluation is given on three different data sets and a range of analysis parameters.",USA,education,Developed economies,"[32.38172, 15.519048]","[-5.1733766, -3.072886]","[8.15594, -11.704925, 7.634506]","[6.3289275, -3.288837, -6.895059]","[10.924259, 7.744582]","[7.3637123, 2.8450725]","[12.103424, 13.313074, 0.26270196]","[10.250235, 8.307219, 11.7969]"
35,Polina Proutskova,Musical Memory of the World - Data Infrastructure in Ethnomusicological Archives.,2007,https://doi.org/10.5281/zenodo.1416316,"Polina Proutskova+Goldsmiths, University of London>GBR>education","Ethnomusicological archives build the musical memory of the world, covering the geographical and the historical aspects of music worldwide. This article gives a brief description of the nature and the functionality of ethnomusicological archives. It reflects the current state of data infrastructure (policy and technology), addressing issues of access to archives’ holdings, of online visibility of music collections and of interoperability between archives. An outlook of a mutual involvement and a resulting influence at each others work between MIR community and ethnomusicological archives is given.",GBR,education,Developed economies,"[-26.443066, 21.005684]","[25.736221, 38.650093]","[-23.052004, -8.927609, -6.407329]","[0.04240807, 5.2891784, 24.069359]","[14.255186, 8.417948]","[11.450477, 0.13013394]","[14.487116, 14.562086, -1.558637]","[11.914065, 4.4114933, 12.156083]"
36,Rosana S. G. Lanzelotte;Adriana O. Ballesté;Martha Ulhoa,A Digital Collection of Brazilian Lundus.,2007,https://doi.org/10.5281/zenodo.1415634,Rosana S. G. Lanzelotte+Universidade Federal do Estado do Rio de Janeiro>BRA>education;Adriana O. Ballesté+Laboratório Nacional de Computação Científica>BRA>facility;Martha Ulhoa+Universidade Federal do Estado do Rio de Janeiro>BRA>education,"Lundu is a typical Brazilian popular musical form at the 19th century. The distinguished musicologist Mozart de Araújo devoted himself to studying lundus and other forms of that period. He collected 48 lundus, which are nowadays stored in a private library, unavailable to public access. The present work describes the implementation of a digital collection of those lundus, using Dspace as the repository. Dspace is chosen in order to guarantee interoperability through the OAI-PMH protocol. Metadata is generated using Dublin Core elements, fully compatible with Dspace. The digital collection provides access to the lundu score images, incipits and midi files, as well as metadata. It is the first time such a rare collection of 19th Brazilian popular music will be available on the web. As Dspace enables interoperation among repositories, a broad community may access the collection.  ",BRA,education,Developing economies,"[-17.233646, 38.841896]","[23.397966, 36.02933]","[-23.1027, -9.94812, -10.446494]","[-0.51707023, 3.4643214, 22.26945]","[14.102272, 8.001462]","[11.303965, 0.17523287]","[14.287907, 14.088652, -1.9772477]","[11.839862, 4.570827, 12.278403]"
38,Stefan Leitich;Martin Topf,Globe of Music - Music Library Visualization Using Geosom.,2007,https://doi.org/10.5281/zenodo.1416930,Stefan Leitich+University of Vienna>AUT>education;Martin Topf+University of Vienna>AUT>education,"Music collections are commonly represented as plain textual lists of artist, title, album etc. for each contained music track. The large volume of personal music libraries makes them difficult to browse and access for users. In respect to possible information visualization techniques, no established convenient user interfaces exist. By using a spherical self-organizing map algorithm on low level audio features and processing the resulting map data, a Geographic Information System is used to visualize a music collection. This results in an aspiring music library visualization, which can be handled intuitively by the user and even provides new possibilities for accessing a music collection in the digital domain.",AUT,education,Developed economies,"[-17.648914, 33.02515]","[27.369549, 19.360628]","[-16.998892, 6.874539, -25.061216]","[12.972413, -6.0090804, 23.384161]","[14.146781, 7.2738566]","[11.25277, 1.7252456]","[14.285757, 13.984298, -2.384997]","[12.371655, 5.6794505, 13.259932]"
39,Adam R. Tindale;David Sprague;George Tzanetakis,Strike-A-Tune: Fuzzy Music Navigation Using a Drum Interface.,2007,https://doi.org/10.5281/zenodo.1418243,Adam R. Tindale+University of Victoria>CAN>education;David Sprague+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"A traditional music library system controlled by a mouse and keyboard is precise, allowing users to select their desired song. Alternatively, randomized playlist or shuffles are used when users have no particular music in mind. We present a new interface and visualization system called Strike-A-Tune for fuzzy music navigation. Fuzzy navigation is an imprecise navigation approach allowing users to choose preference related items. We believe this will help users to play music they want to hear and re-discover infrequently played songs in their music library, thus combining the best aspects of precision navigation and shuffles. We have designed an interface using an electronic drum to communicate with a visualization and playback system.",CAN,education,Developed economies,"[22.42214, -43.110996]","[29.868296, 23.060326]","[22.178062, -15.103676, -1.2713678]","[11.994219, -1.2517745, 24.695654]","[7.9701724, 7.008592]","[11.4228, 1.3979777]","[10.185769, 11.842167, 0.77246934]","[12.303656, 5.1686482, 13.163309]"
37,Beinan Li;Simon de Leon;Ichiro Fujinaga,Alternative Digitization Approach for Stereo Phonograph Records Using Optical Audio Reconstruction.,2007,https://doi.org/10.5281/zenodo.1415746,Beinan Li+McGill University>CAN>education;Simon de Leon+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper presents the first Optical Audio Reconstruction (OAR) approach for the long-term digital preservation of stereo phonograph records. OAR uses precision metrology and digital image processing to obtain and convert groove contour data into digital audio for access and preservation. This contactless and imaging-based approach has considerable advantages over the traditional mechanical methods, such as being the only optical method with the potential to restore broken stereo records. Although past efforts on monophonic phonograph records have been successful, no attempts on 33rpm long-playing stereo records (LPs) have been reported. By using a white-light interferometry optical profiler, we are able to extract stereo audio information encoded in the 3D profile of the phonograph record grooves.",CAN,education,Developed economies,"[30.151848, -15.550738]","[13.666174, -37.206917]","[18.11635, 20.070639, 8.019693]","[-5.7571883, -26.759413, 3.6617293]","[8.613253, 6.1532674]","[6.4774804, -0.026359947]","[10.649292, 11.192286, -0.14317502]","[8.034707, 4.8027267, 10.718886]"
41,Stephen Hitchner;Jennifer Murdoch;George Tzanetakis,Music Browsing Using a Tabletop Display.,2007,https://doi.org/10.5281/zenodo.1415896,Stephen Hitchner+University of Victoria>CAN>education;Jennifer Murdoch+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"The majority of work in Music Information Retrieval (MIR) follows a search/retrieval paradigm. More recently, the importance of browsing as an interaction paradigm has been realized, and several novel interfaces have been proposed. In this paper, we describe two novel interaction schemes for content-aware browsing of music collections that use a graphical tabletop interface. We further present findings from qualitative user studies. We describe our work in the context of two primary themes: music collection browsing, and collaborative (multiple simultaneous users) interaction and involvement during the browsing/selection process.",CAN,education,Developed economies,"[-21.942352, 30.042906]","[29.072742, 26.287409]","[-18.404675, 12.010081, -17.053467]","[8.933418, -1.4759436, 21.739052]","[14.415748, 7.3835354]","[11.610954, 1.0719318]","[14.424679, 14.254211, -2.412356]","[12.264118, 4.9068675, 12.806425]"
42,Peter Knees,Search & Select - Intuitively Retrieving Music from Large Collections.,2007,https://doi.org/10.5281/zenodo.1416456,Peter Knees+Johannes Kepler University Linz>AUT>education,"A retrieval system for large-scale collections that allows users to search for music using natural language queries and relevance feedback is presented. In contrast to existing music search engines that are either restricted to manually annotated meta-data or based on a query-by-example variant, the presented approach describes audio pieces via a traditional term vector model and allows therefore to retrieve relevant music pieces by issuing simple free-form text queries. Term vector descriptors for music pieces are derived by applying Web-based and audio-based similarity measures. Additionally, as the user selects music pieces that he/she likes, the subsequent results are adapted to accommodate to the user’s preferences. Real-world performance of the system is indicated by a small user study.",AUT,education,Developed economies,"[-16.821074, 26.957806]","[19.873281, 19.069956]","[-10.073418, 5.3179593, -18.351446]","[8.6390705, -4.347639, 15.69283]","[14.172419, 7.6339903]","[10.621927, 0.8786008]","[14.0925665, 14.569939, -2.387337]","[11.772493, 5.5236034, 12.748582]"
43,Marco Tiemann;Steffen Pauws;Fabio Vignoli,Ensemble Learning for Hybrid Music Recommendation.,2007,https://doi.org/10.5281/zenodo.1417781,Marco Tiemann+Philips Research Europe>NLD>company;Steffen Pauws+Philips Research Europe>NLD>company;Fabio Vignoli+Philips Research Europe>NLD>company,"We investigate ensemble learning methods for hybrid music recommenders, combining a social and a content-based recommender algorithm in an initial experiment by applying a simple combination rule to merge recommender results. A first experiment suggests that such a combination can reduce the mean absolute prediction error compared to the used recommenders’ individual errors.",NLD,company,Developed economies,"[-48.567204, 26.190304]","[40.48649, 15.002956]","[-7.5247126, 27.636482, -11.539893]","[19.015936, 3.261254, 17.578938]","[15.966976, 9.257926]","[12.588485, 1.9936428]","[15.83382, 15.722652, -1.4682616]","[13.560968, 5.347577, 12.708965]"
44,Justin Donaldson;Ian Knopke,Music Recommendation Mapping and Interface Based on Structural Network Entropy.,2007,https://doi.org/10.5281/zenodo.1417329,Justin Donaldson+Indiana University>USA>education|Indiana University>USA>education;Ian Knopke+Indiana University>USA>education|Indiana University>USA>education,"Recommendation systems generally produce the results of their output to their users in the form of an ordinal list. In the interest of simplicity, these lists are often obscure, abstract, or omit many relevant metrics pertaining to the measured strength of the recommendations or the relationships the recommended items share with each other. This information is often useful for coming to a better understanding of the nature of how the items are structured according to the recommendation data. This paper describes the ZMDS algorithm, a novel way of analyzing the fundamental network structure of recommendation results. Furthermore, it also describes a dynamic plot interaction method as a recommendation browsing utility. A novel “Recommendation Map” web application implements both the ZMDS algorithm and the plot interface and are offered as an example of both components working together.",USA,education,Developed economies,"[-46.4314, 27.243097]","[32.774178, 16.999657]","[-8.124945, 22.337942, -11.988973]","[16.20369, 1.6821295, 18.47397]","[15.9262, 9.280767]","[12.174022, 1.9288324]","[15.793084, 15.726897, -1.5077]","[13.150126, 5.4346366, 12.785605]"
46,Daniel McEnnis;Sally Jo Cunningham,Sociology and Music Recommendation Systems.,2007,https://doi.org/10.5281/zenodo.1414854,Daniel McEnnis+Waikato University>NZL>education|Waikato University>NZL>education;Sally Jo Cunningham+Waikato University>NZL>education,"Music recommendation systems have centred on two different approaches: content based analysis and collaborative filtering. Little attention has been paid to the reasons why these techniques have been effective. Fortunately, the social sciences have asked these questions. One of the findings of this research is that social context is much more important than previously thought. This paper introduces this body of research from sociology and its relevance to music recommendation algorithms.",NZL,education,Developed economies,"[-44.352013, 25.005953]","[43.425106, 29.947525]","[-12.578504, 23.260525, -11.057293]","[11.833325, 12.891961, 20.514317]","[15.761356, 9.133729]","[12.839716, 1.5113553]","[15.6794195, 15.566128, -1.4250528]","[13.481727, 4.670831, 12.23322]"
40,Paul Lamere;Douglas Eck,Using 3D Visualizations to Explore and Discover Music.,2007,https://doi.org/10.5281/zenodo.1415022,Paul Lamere+Sun Microsystems>USA>company;Douglas Eck+Sun Microsystems>USA>company,"This paper presents Search Inside the Music an application for exploring and discovering new music. Search Inside the Music uses a music similarity model and 3D visualizations to provide a user with new tools for exploring and interacting with a music collection. With Search Inside the Music, a music listener can find new music, generate interesting playlists, and interact with their music collection.",USA,company,Developed economies,"[-16.445023, 32.08187]","[28.97321, 21.714]","[-14.877036, 8.40611, -23.730225]","[10.983163, -2.937795, 21.932217]","[14.056436, 7.016997]","[11.46834, 1.3742231]","[14.152097, 13.818972, -2.50714]","[12.349199, 5.1641946, 13.12777]"
80,Arturo Camacho,Detection of Pitched/Unpitched Sound using Pitch Strength Clustering.,2008,https://doi.org/10.5281/zenodo.1418169,Arturo Camacho+University of Florida>USA>education,"A method for detecting pitched/unpitched sound is presented. The method tracks the pitch strength trace of the signal, determining clusters of pitch and unpitched sound. The criterion used to determine the clusters is the local maximization of the distance between the centroids. The method makes no assumption about the data except that the pitched and unpitched clusters have different centroids. This allows the method to dispense with free parameters. The method is shown to be more reliable than using fixed thresholds when the SNR is unknown.",USA,education,Developed economies,"[28.213673, -24.374245]","[-3.624413, -7.8287854]","[11.90689, -17.314745, -6.9709597]","[7.3351326, -8.703417, -9.736049]","[10.05055, 5.4092956]","[6.6930804, 2.709106]","[10.61889, 13.191352, -1.0888779]","[9.065387, 8.050402, 11.009034]"
73,Mitsuyo Hashida;Toshie Matsui;Haruhiro Katayose,A New Music Database Describing Deviation Information of Performance Expressions.,2008,https://doi.org/10.5281/zenodo.1416418,"Mitsuyo Hashida+Kwansei Gakuin University>JPN>education|CrestMuse Project, JST>JPN>facility;Toshie Matsui+Kwansei Gakuin University>JPN>education|CrestMuse Project, JST>JPN>facility;Haruhiro Katayose+Kwansei Gakuin University>JPN>education|CrestMuse Project, JST>JPN>facility","We introduce the CrestMuse Performance Expression Database (CrestMusePEDB), a music database that describes music performance expression and is available for academic research. While music databases are being provided as MIR technologies continue to progress, few databases deal with performance expression. We constructed a music expression database, CrestMusePEDB. It may be utilized in the research fields of music informatics, music perception and cognition, and musicology. It will contain music expression information on virtuosis’ expressive performances, including those of 3 to 10 players at a time, on about 100 pieces of classical Western music. The latest version of the database, CrestMusePEDB Ver. 2.0, is available. The paper gives an overview of CrestMusePEDB.",JPN,education,Developed economies,"[-15.983652, 7.5544424]","[13.956276, 33.341984]","[-10.470453, -8.408779, -9.375937]","[-0.9170294, 10.39263, 13.364386]","[12.822759, 7.4029303]","[11.132417, 0.79098576]","[13.651266, 13.448743, -1.3777895]","[11.488559, 4.7869134, 11.517634]"
81,John Woodruff;Yipeng Li;DeLiang Wang,Resolving Overlapping Harmonics for Monaural Musical Sound Separation using Fundamental Frequency and Common Amplitude Modulation.,2008,https://doi.org/10.5281/zenodo.1417279,John Woodruff+The Ohio State University>USA>education;Yipeng Li+The Ohio State University>USA>education;DeLiang Wang+The Ohio State University>USA>education,"In mixtures of pitched sounds, the problem of overlapping harmonics poses a significant challenge to monaural musical sound separation systems. In this paper we present a new algorithm for sinusoidal parameter estimation of overlapping harmonics for pitched instruments. Our algorithm is based on the assumptions that harmonics of the same source have correlated amplitude envelopes and the phase change of harmonics can be accurately predicted from an instrument’s pitch. We exploit these two assumptions in a least-squares estimation framework to resolve overlapping harmonics. This new algorithm is incorporated into a separation system and quantitative evaluation shows that the resulting system performs significantly better than an existing monaural music separation system for mixtures of harmonic instruments.",USA,education,Developed economies,"[10.39119, -43.12184]","[-41.570225, -25.990334]","[26.362335, -5.6504827, -6.750792]","[-8.302077, -5.8270903, -32.781322]","[8.630569, 9.681722]","[6.5011783, 5.2897167]","[11.057648, 13.459796, 1.2685213]","[9.799272, 8.50024, 9.703791]"
79,Emmanuel Ravelli;Gaël Richard;Laurent Daudet,Fast MIR in a Sparse Transform Domain.,2008,https://doi.org/10.5281/zenodo.1417002,Emmanuel Ravelli+Université Paris 6>FRA>education|TELECOM ParisTech>FRA>education;Gaël Richard+TELECOM ParisTech>FRA>education;Laurent Daudet+Université Paris 6>FRA>education,"We consider in this paper sparse audio coding as an alternative to transform audio coding for efficient MIR in the transform domain. We use an existing audio coder based on a sparse representation in a union of MDCT bases, and propose a fast algorithm to compute mid-level representations for beat tracking and chord recognition, respectively an onset detection function and a chromagram. The resulting transform domain system is significantly faster than a comparable state-of-the-art system while obtaining close performance above 8 kbps.",FRA,education,Developed economies,"[-8.641505, 59.52773]","[-52.661377, -18.777834]","[-40.462494, -2.486392, -5.1345854]","[-7.8644633, 13.251111, -23.808157]","[13.630502, 4.6718936]","[6.0047207, 4.4807677]","[15.035522, 11.060558, -1.4857544]","[10.190693, 6.0237346, 10.592036]"
78,Olivier Lartillot;Tuomas Eerola;Petri Toiviainen;José Fornari,"Multi-Feature Modeling of Pulse Clarity: Design, Validation and Optimization.",2008,https://doi.org/10.5281/zenodo.1415514,"Olivier Lartillot+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Tuomas Eerola+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Petri Toiviainen+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Jose Fornari+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education","Pulse clarity is considered as a high-level musical dimension that conveys how easily in a given musical piece, or a particular moment during that piece, listeners can perceive the underlying rhythmic or metrical pulsation. The objective of this study is to establish a composite model explaining pulse clarity judgments from the analysis of audio recordings. A dozen of descriptors have been designed, some of them dedicated to low-level characterizations of the onset detection curve, whereas the major part concentrates on descriptions of the periodicities developed throughout the temporal evolution of music. A high number of variants have been derived from the systematic exploration of alternative methods proposed in the literature on onset detection curve estimation. To evaluate the pulse clarity model and select the best predictors, 25 participants have rated the pulse clarity of one hundred excerpts from movie soundtracks. The mapping between the model predictions and the ratings was carried out via regressions. Nearly a half of listeners’ rating variance can be explained via a combination of periodicity-based factors.",FIN,education,Developed economies,"[32.274418, -31.708828]","[-24.323132, -5.959841]","[9.678683, -33.070587, -0.15116338]","[-1.430387, 5.999549, -9.228791]","[10.608823, 4.6072216]","[5.5498443, 2.265341]","[10.350718, 13.081439, -2.1495183]","[7.8264556, 7.3145566, 10.936653]"
77,Chee-Chuan Toh;Bingjun Zhang;Ye Wang,Multiple-Feature Fusion Based Onset Detection for Solo Singing Voice.,2008,https://doi.org/10.5281/zenodo.1414756,Chee Chuan Toh+National University of Singapore>SGP>education;Bingjun Zhang+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"Onset detection is a challenging problem in automatic singing transcription. In this paper, we address singing onset detection with three main contributions. First, we outline the nature of a singing voice and present a new singing onset detection approach based on supervised machine learning. In this approach, two Gaussian Mixture Models (GMMs) are used to classify audio features of onset frames and non-onset frames. Second, existing audio features are thoroughly evaluated for this approach to singing onset detection. Third, feature-level and decision-level fusion are employed to fuse different features for a higher level of performance. Evaluated on a recorded singing database, the proposed approach outperforms state-of-the-art onset detection algorithms significantly.",SGP,education,Developing economies,"[5.249998, -33.53661]","[-22.687807, -8.654181]","[5.674373, -21.944998, -10.610525]","[-3.5757692, 5.9564004, -13.75349]","[10.26653, 4.975728]","[5.8072977, 2.6102881]","[10.280242, 13.317045, -1.4524236]","[8.114655, 7.4815907, 10.768625]"
75,Craig Sapp,Hybrid Numeric/Rank Similarity Metrics for Musical Performance Analysis.,2008,https://doi.org/10.5281/zenodo.1417561,"Craig Stuart Sapp+CHARM, Royal Holloway, University of London>GBR>education","This paper describes a numerical method for examining similarities among tempo and loudness features extracted from recordings of the same musical work and evaluates its effectiveness compared to Pearson correlation. Starting with correlation at multiple timescales, other concepts such as a performance “noise-floor” are used to generate measurements which are more refined than correlation alone. The measurements are evaluated and compared to plain correlation in their ability to identify performances of the same Chopin mazurka played by the same pianist out of a collection of recordings by various pianists.",GBR,education,Developed economies,"[-4.557631, 13.043352]","[-26.07535, 5.2061677]","[-6.066687, 4.2052655, -1.0829964]","[-5.758121, 9.089344, -2.750149]","[13.110523, 9.267823]","[6.0521307, 1.4695393]","[13.607376, 15.174459, -0.775062]","[8.186868, 6.5100446, 11.782498]"
74,Miguel Molina-Solana;Josep Lluís Arcos;Emilia Gómez,Using Expressive Trends for Identifying Violin P erformers.,2008,https://doi.org/10.5281/zenodo.1417907,"Miguel Molina-Solana+University of Granada>ESP>education;Josep Lluís Arcos+CSIC, Spanish National Research Council>ESP>facility;Emilia Gomez+Universitat Pompeu Fabra>ESP>education","This paper presents a new approach for identifying professional performers in commercial recordings. We propose a Trend-based model that, analyzing the way Narmour’s Implication-Realization patterns are played, is able to characterize performers. Concretely, starting from automatically extracted descriptors provided by state-of-the-art extraction tools, the system performs a mapping to a set of qualitative behavior shapes and constructs a collection of frequency distributions for each descriptor. Experiments were conducted in a data-set of violin recordings from 23 different performers. Reported results show that our approach is able to achieve high identification rates.",ESP,education,Developed economies,"[16.044462, -19.33135]","[-30.500988, 9.190169]","[5.638256, -15.867989, -3.5070338]","[-12.866695, 6.665299, 0.02502304]","[9.471488, 6.7490635]","[8.464708, 3.2824657]","[11.35185, 12.3474455, -0.5098269]","[10.317415, 7.2022586, 10.74239]"
72,Rafael Ramírez 0001;Alfonso Pérez;Stefan Kersten,Performer Identification in Celtic Violin Recordings.,2008,https://doi.org/10.5281/zenodo.1416988,Rafael Ramirez+Universitat Pompeu Fabra>ESP>education;Alfonso Perez+Universitat Pompeu Fabra>ESP>education;Stefan Kersten+Universitat Pompeu Fabra>ESP>education,We present an approach to the task of identifying performers from their playing styles. We investigate how violinists express and communicate their view of the musical content of Celtic popular pieces and how to use this information in order to automatically identify performers. We study note-level deviations of parameters such as timing and amplitude. Our approach to performer identification consists of inducing an expressive performance model for each of the interpreters (essentially establishing a performer dependent mapping of inter-note features to a timing and amplitude expressive transformations). We present a successful performer identification case study.,ESP,education,Developed economies,"[16.161583, -19.467718]","[-30.567, 9.011084]","[5.2578435, -16.505281, -2.1980066]","[-12.4578285, 6.2745543, -0.28036606]","[9.422846, 6.748222]","[7.822333, 3.4686558]","[11.249598, 12.334792, -0.459303]","[9.076479, 6.349501, 10.887811]"
66,Travis M. Doll;Raymond Migneco;Youngmoo E. Kim,Online Activities for Music Information and Acoustics Education and Psychoacoustic Data Collection.,2008,https://doi.org/10.5281/zenodo.1418341,Travis M. Doll+Drexel University>USA>education;Ray V. Migneco+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"Online collaborative activities provide a powerful platform for the collection of psychoacoustic data on the perception of audio and music from a very large numbers of subjects. Furthermore, these activities can be designed to simultaneously educate users about aspects of music information and acoustics, particularly for younger students in grades K-12. We have created prototype interactive activities illustrating aspects of two different sound and acoustics concepts: musical instrument timbre and the cocktail party problem (sound source isolation within mixtures) that also provide a method of collecting perceptual data related to these problems with a range of parameter variation that is difficult to achieve for large subject populations using traditional psychoacoustic evaluation. We present preliminary data from a pilot study where middle school students were engaged with the two activities to demonstrate the potential benefits as an education and data collection platform.",USA,education,Developed economies,"[-31.826904, 23.744709]","[35.164192, 36.647453]","[-17.585945, 14.063313, -9.505034]","[2.026449, 15.667246, 19.020376]","[14.674824, 8.148943]","[12.2110615, 0.76316303]","[14.62492, 14.784069, -1.9261864]","[12.384552, 4.448411, 11.524973]"
70,J. Stephen Downie;Mert Bay;Andreas F. Ehmann;M. Cameron Jones,Audio Cover Song Identification: MIREX 2006-2007 Results and Analyses.,2008,https://doi.org/10.5281/zenodo.1417133,J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Mert Bay+University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education;M. Cameron Jones+University of Illinois at Urbana-Champaign>USA>education,"This paper presents analyses of the 2006 and 2007 results of the Music Information Retrieval Evaluation eXchange (MIREX) Audio Cover Song Identification (ACS) tasks. The Music Information Retrieval Evaluation eXchange (MIREX) is a community-based endeavor to scientifically evaluate music information retrieval (MIR) algorithms and techniques. The ACS task was created to motivate MIR researchers to expand their notions of similarity beyond acoustic similarity to include the important idea that musical works retain their identity notwithstanding variations in style, genre, orchestration, rhythm or melodic ornamentation, etc. A series of statistical analyses were performed that indicate significant improvements in this domain have been made over the course of 2006-2007. Post-hoc analyses reveal distinct differences between individual systems and the effects of certain classes of queries on performance. This paper discusses some of the techniques that show promise in this research domain.",USA,education,Developed economies,"[6.7236137, 44.443344]","[21.235025, 21.16184]","[4.4356585, 15.28814, -22.743603]","[6.292469, 2.3378265, 14.137449]","[16.089453, 11.1142]","[11.420516, 1.0235656]","[12.827153, 17.377686, -0.30747017]","[11.981686, 5.1429925, 12.25582]"
69,Xiao Hu 0001;J. Stephen Downie;Cyril Laurier;Mert Bay;Andreas F. Ehmann,The 2007 MIREX Audio Mood Classification Task: Lessons Learned.,2008,https://doi.org/10.5281/zenodo.1416380,"Xiao Hu+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education;Cyril Laurier+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Mert Bay+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education","Recent music information retrieval (MIR) research pays increasing attention to music classification based on moods expressed by music pieces. The first Audio Mood Classification (AMC) evaluation task was held in the 2007 running of the Music Information Retrieval Evaluation eXchange (MIREX). This paper describes important issues in setting up the task, including dataset construction and ground-truth labeling, and analyzes human assessments on the audio dataset, as well as system performances from various angles. Interesting findings include system performance differences with regard to mood clusters and the levels of agreement amongst human judgments regarding mood labeling. Based on these analyses, we summarize experiences learned from the first community scale evaluation of the AMC task and propose recommendations for future AMC and similar evaluation tasks.",USA,education,Developed economies,"[-55.22118, 2.0520475]","[51.672607, -2.6758487]","[-20.639057, 22.586494, 6.584053]","[13.605174, 19.226862, 9.580141]","[13.503975, 12.626917]","[13.136547, 3.6328943]","[16.143494, 14.905089, 1.5085968]","[14.252349, 5.1376905, 10.765915]"
68,Megan A. Winget,"The Liner Notes Digitization Project: Providing Users with Cultural, Historical, and Critical Music Information.",2008,https://doi.org/10.5281/zenodo.1417933,Megan A. Winget+University of Texas at Austin>USA>education,"Digitizing cultural information is a complex endeavor. Not only do users expect to have access to primary information like digital music files; it is also becoming more important for digital systems to provide contextual information for the primary artifacts contained within. The Liner Notes Markup Language (LNML) was developed to provide an XML vocabulary for encoding complex contextual documents that include an album’s packaging, informational notes and inserts, liners, and album labels. This paper describes the development of the LNML framework, its major structural elements and functions, and some of the more pressing problems related to usability and purpose. The current LNML model is based on the examination and encoding of fifty albums from the 80s Rock genre. We are currently encoding fifty additional Jazz albums, which will provide data to augment and strengthen the model. Development of the LNML is ongoing, with plans to examine Classical and World Music examples to further augment the model.",USA,education,Developed economies,"[1.3872867, 28.76366]","[3.7786522, 39.988914]","[-10.440801, 1.2999693, -26.05262]","[-8.500528, -5.9285192, 21.87731]","[12.339357, 7.22853]","[10.250717, 0.1056897]","[12.745357, 12.350741, -1.3695307]","[10.818049, 4.754805, 12.036539]"
67,Carlos Nascimento Silla Jr.;Alessandro L. Koerich;Celso A. A. Kaestner,The Latin Music Database.,2008,https://doi.org/10.5281/zenodo.1416282,Carlos N. Silla Jr.+University of Kent>GBR>education;Alessandro L. Koerich+Pontifical Catholic University of Paraná>BRA>education;Celso A. A. Kaestner+Federal University of Technology of Paraná>BRA>education,"In this paper we present the Latin Music Database, a novel database of Latin musical recordings which has been developed for automatic music genre classification, but can also be used in other music information retrieval tasks. The method for assigning genres to the musical recordings is based on human expert perception and therefore capture their tacit knowledge in the genre labeling process. We also present the ethnomusicology of the genres available in the database as it might provide important information for the analysis of the results of any experiment that employs the database.",GBR,education,Developed economies,"[-0.70525634, 25.563147]","[20.677204, -2.7500424]","[-17.096603, -7.8380933, -6.658832]","[13.301836, 9.863435, -0.1549183]","[13.168253, 7.6011267]","[10.034603, 3.1492267]","[13.846907, 13.827904, -1.786969]","[11.702747, 6.839097, 10.986715]"
65,Jenn Riley,Application of the Functional Requirements for Bibliographic Records (FRBR) to Music.,2008,https://doi.org/10.5281/zenodo.1416446,Jenn Riley+Indiana University>USA>education,"This paper describes work applying the Functional Requirements for Bibliographic Records (FRBR) model to music, as the basis for implementing a fully FRBR-compliant music digital library system. A detailed analysis of the FRBR and Functional Requirements for Authority Data (FRAD) entities and attributes is presented. The paper closes with a discussion of the ways in which FRBR is gaining adoption outside of the library environment in which it was born. This work benefits the MIR community by demonstrating a model that can be used in MIR systems for the storage of descriptive information in support of metadata-based searching, and by positioning the Variations system to be a source of robust descriptive information for use by third-party MIR systems.",USA,education,Developed economies,"[-26.54781, 24.479074]","[21.090248, 33.479187]","[-19.671825, 4.7231116, -9.259568]","[0.74302363, -1.394502, 20.389782]","[14.358265, 7.9966216]","[11.077759, 0.16604954]","[14.430613, 14.763803, -2.1861293]","[11.715979, 4.6817656, 12.220165]"
8,James B. Maxwell;Arne Eigenfeldt,A Music Database and Query System for Recombinant Composition.,2008,https://doi.org/10.5281/zenodo.1415794,James B. Maxwell+Simon Fraser University>CAN>education;Arne Eigenfeldt+Simon Fraser University>CAN>education,"""We propose a design and implementation for a music information database and query system, the MusicDB, which can be used for Music Information Retrieval (MIR). The MusicDB is implemented as a Java package, and is loaded in MaxMSP using the mxj external. The MusicDB contains a music analysis module, capable of extracting musical information from standard MIDI files, and a search engine. The search engine accepts queries in the form of a simple six-part syntax, and can return a variety of different types of musical information, drawing on the encoded knowledge of musical form stored in the database.""",CAN,education,Developed economies,"[-17.870367, 11.69863]","[18.699612, 29.58873]","[-13.171669, -4.727331, -8.162975]","[0.64462477, -1.9868729, 16.784716]","[13.332078, 7.5465064]","[10.725074, 0.52319384]","[13.89254, 14.003101, -1.6457268]","[11.33322, 5.048086, 12.291092]"
63,Juan José Burred;Carmine-Emanuele Cella;Geoffroy Peeters;Axel Röbel;Diemo Schwarz,Using the SDIF Sound Description Interchange Format for Audio Features.,2008,https://doi.org/10.5281/zenodo.1415762,Juan José Burred+IRCAM - CNRS STMS>FRA>facility;Carmine Emanuele Cella+IRCAM - CNRS STMS>FRA>facility;Geoffroy Peeters+IRCAM - CNRS STMS>FRA>facility;Axel Röbel+IRCAM - CNRS STMS>FRA>facility;Diemo Schwarz+IRCAM - CNRS STMS>FRA>facility,"We present a set of extensions to the Sound Description Interchange Format (SDIF) for the purpose of storage and/or transmission of general audio descriptors. The aim is to allow portability and interoperability between the feature extraction module of an audio information retrieval application and the remaining modules, such as training, classification or clustering. A set of techniques addressing the needs of short-time features and temporal modeling over longer windows are proposed, together with the mechanisms that allow further extensions or adaptations by the user. The paper is completed by an overview of the general aspects of SDIF and its practical use by means of a set of existing programming interfaces for, among others, C, C++ and Matlab.",FRA,facility,Developed economies,"[-9.4377985, -20.004538]","[7.352282, 28.416975]","[-1.2511386, -7.4399633, -19.131023]","[-5.221049, -4.4067197, 12.802007]","[12.4093275, 7.5371375]","[10.322215, 1.3803599]","[13.612889, 13.438744, -0.34664777]","[11.201603, 5.669153, 11.504159]"
82,Bernhard Niedermayer,Non-Negative Matrix Division for the Automatic Transcription of Polyphonic Music.,2008,https://doi.org/10.5281/zenodo.1415040,Bernhard Niedermayer+Johannes Kepler University Linz>AUT>education,"In this paper we present a new method in the style of non-negative matrix factorization for automatic transcription of polyphonic music played by a single instrument (e.g., a piano). We suggest using a fixed repository of base vectors corresponding to tone models of single pitches played on a certain instrument. This assumption turns the blind factorization into a kind of non-negative matrix division for which an algorithm is presented. The same algorithm can be applied for learning the model dictionary from sample tones as well. This method is biased towards the instrument used during the training phase. But this is admissible in applications like performance analysis of solo music. The proposed approach is tested on a Mozart sonata where a symbolic representation is available as well as the recording on a computer controlled grand piano.",AUT,education,Developed economies,"[29.289999, -11.399184]","[-49.71653, -23.73007]","[11.496862, -6.3402987, 7.127783]","[-5.825312, -15.321478, -26.307562]","[9.3622875, 7.808981]","[6.148529, 4.771081]","[11.712522, 12.358396, 0.22143617]","[9.401429, 8.667334, 9.871715]"
62,Laurent Pugin;Jason Hockman;John Ashley Burgoyne;Ichiro Fujinaga,Gamera Versus Aruspix: Two Optical Music Recognition Approaches.,2008,https://doi.org/10.5281/zenodo.1417683,Laurent Pugin+Schulich School of Music of McGill University>CAN>education;Jason Hockman+Schulich School of Music of McGill University>CAN>education;John Ashley Burgoyne+Schulich School of Music of McGill University>CAN>education;Ichiro Fujinaga+Schulich School of Music of McGill University>CAN>education,"Optical music recognition (OMR) applications are predominantly designed for common music notation and as such, are inherently incapable of adapting to specialized notation forms within early music. Two OMR systems, namely Gamut (a Gamera application) and Aruspix, have been proposed for early music. In this paper, we present a novel comparison of the two systems, which use markedly different approaches to solve the same problem, and pay close attention to the performance and learning rates of both applications. In order to obtain a complete comparison of Gamut and Aruspix, we evaluated the core recognition systems and the pitch determination processes separately. With our experiments, we were able to highlight the advantages of both approaches as well as causes of problems and possibilities for future improvements.",CAN,education,Developed economies,"[39.992966, 22.65691]","[-24.799023, 37.270924]","[21.113543, 15.159269, 8.278128]","[-13.467442, -20.894928, 4.152779]","[8.616426, 6.15097]","[6.6910343, -0.58778304]","[10.669655, 11.130782, -0.11066824]","[7.936561, 4.147226, 10.600952]"
71,Charlie Inskip;Andy MacFarlane;Pauline Rafferty,"Music, Movies and Meaning: Communication in Film-Makers' Search for Pre-Existing Music, and the Implications for Music Information Retrieval.",2008,https://doi.org/10.5281/zenodo.1418101,Charlie Inskip+City University London>GBR>education;Andy Macfarlane+City University London>GBR>education;Pauline Rafferty+University of Aberystwyth>GBR>education,"""While the use of music to accompany moving images is widespread, the information behaviour, communicative practice and decision making by creative professionals within this area of the music industry is an under-researched area. This investigation discusses the use of music in films and advertising focusing on communication and meaning of the music and introduces a reflexive communication model. The model is discussed in relation to interviews with a sample of music professionals who search for and use music for their work. Key factors in this process include stakeholders, briefs, product knowledge and relevance. Searching by both content and context is important, although the final decision when matching music to picture is partly intuitive and determined by a range of stakeholders.""",GBR,education,Developed economies,"[-24.328693, 20.442392]","[33.02769, 38.008923]","[-13.450526, 6.848891, -5.4601917]","[-2.0334463, 13.850147, 19.92869]","[14.3493595, 8.219158]","[12.184452, 0.4788245]","[14.444756, 14.794431, -1.884643]","[12.427681, 4.354472, 11.716313]"
83,Adrien Daniel;Valentin Emiya;Bertrand David,Perceptually-Based Evaluation of the Errors Usually Made When Automatically Transcribing Music.,2008,https://doi.org/10.5281/zenodo.1417155,Adrien Daniel+TELECOM ParisTech (ENST)>FRA>education|CNRS LTCI>FRA>facility;Valentin Emiya+TELECOM ParisTech (ENST)>FRA>education|CNRS LTCI>FRA>facility;Bertrand David+TELECOM ParisTech (ENST)>FRA>education|CNRS LTCI>FRA>facility,"This paper investigates the perceptual importance of typical errors occurring when transcribing polyphonic music excerpts into a symbolic form. The case of the automatic transcription of piano music is taken as the target application and two subjective tests are designed. The main test aims at understanding how human subjects rank typical transcription errors such as note insertion, deletion or replacement, note doubling, incorrect note onset or duration, and so forth. The Bradley-Terry-Luce (BTL) analysis framework is used and the results show that pitch errors are more clearly perceived than incorrect loudness estimations or temporal deviations from the original recording. A second test presents a first attempt to include this information in more perceptually motivated measures for evaluating transcription systems.",FRA,education,Developed economies,"[-8.281325, -23.195559]","[-6.594246, -15.681062]","[14.968246, -8.628412, -22.443747]","[-4.9109683, -9.345234, -6.012225]","[9.95127, 6.9442964]","[7.276684, 2.3002715]","[11.788677, 11.9140215, -0.69409937]","[9.227501, 6.7406797, 10.920482]"
98,Matthew Wright;W. Andrew Schloss;George Tzanetakis,Analyzing Afro-Cuban Rhythms using Rotation-Aware Clave Template Matching with Dynamic Programming.,2008,https://doi.org/10.5281/zenodo.1416356,Matthew Wright+University of Victoria>CAN>education;W. Andrew Schloss+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"The majority of existing research in Music Information Retrieval (MIR) has focused on either popular or classical music and frequently makes assumptions that do not generalize to other music cultures. We use the term Computational Ethnomusicology (CE) to describe the use of computer tools to assist the analysis and understanding of musics from around the world. Although existing MIR techniques can serve as a good starting point for CE, the design of effective tools can benefit from incorporating domain-specific knowledge about the musical style and culture of interest. In this paper we describe our realization of this approach in the context of studying Afro-Cuban rhythm. More specifically we show how computer analysis can help us characterize and appreciate the complexities of tracking tempo and analyzing micro-timing in these particular music styles. A novel template-based method for tempo tracking in rhythmically complex Afro-Cuban music is proposed. Although our approach is domain-specific, we believe that the concepts and ideas used could also be used for studying other music cultures after some adaptation.",CAN,education,Developed economies,"[9.174156, 8.20734]","[-19.704353, 9.357448]","[-7.3648405, -17.553278, -1.4041468]","[-4.44758, 13.3659725, -0.034176704]","[11.824323, 5.8217673]","[6.1705, 1.7052245]","[11.625445, 13.981959, -1.9180514]","[8.4332, 6.702359, 11.757544]"
85,François Deliège;Bee Yong Chua;Torben Bach Pedersen,High-Level Audio Features: Distributed Extraction and Similarity Search.,2008,https://doi.org/10.5281/zenodo.1416514,François Deliège+Aalborg University>DNK>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Bee Yong Chua+Aalborg University>DNK>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Torben Bach Pedersen+Aalborg University>DNK>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Today, automatic extraction of high-level audio features suffers from two main scalability issues. First, the extraction algorithms are very demanding in terms of memory and computation resources. Second, copyright laws prevent the audio files to be shared among computers, limiting the use of existing distributed computation frameworks and reducing the transparency of the methods evaluation process. The iSound Music Warehouse (iSoundMW), presented in this paper, is a framework to collect and query high-level audio features. It performs the feature extraction in a two-step process that allows distributed computations while respecting copyright laws. Using public computers, the extraction can be performed on large scale music collections. However, to be truly valuable, data management tools to search among the extracted features are needed. The iSoundMW enables similarity search among the collected high-level features and demonstrates its flexibility and efficiency by using a weighted combination of high-level features and constraints while showing good search performance results.",DNK,education,Developed economies,"[-11.682751, -16.390293]","[10.079039, 28.272957]","[1.2394315, -5.5287657, -14.093125]","[-4.473575, -0.76759475, 14.879607]","[12.599727, 8.196726]","[10.405849, 1.3806899]","[13.036732, 13.69964, 0.47348303]","[11.429446, 5.6993046, 11.695864]"
61,Christian Fremerey;Meinard Müller;Frank Kurth;Michael Clausen,Automatic Mapping of Scanned Sheet Music to Audio Recordings.,2008,https://doi.org/10.5281/zenodo.1416034,Christian Fremerey+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility;Meinard Müller+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility;Frank Kurth+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility;Michael Clausen+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility,"Signiﬁcant digitization efforts have resulted in large multi-modal music collections comprising visual (scanned sheet music) as well as acoustic material (audio recordings). In this paper, we present a novel procedure for mapping scanned pages of sheet music to a given collection of audio recordings by identifying musically corresponding audio clips. To this end, both the scanned images as well as the audio recordings are ﬁrst transformed into a common feature representation using optical music recognition (OMR) and methods from digital signal processing, respectively. Based on this common representation, a direct comparison of the two different types of data is facilitated. This allows for a search of scan-based queries in the audio collection. We report on systematic experiments conducted on the corpus of Beethoven’s piano sonatas showing that our mapping procedure works with high precision across the two types of music data in the case that there are no severe OMR errors. The proposed mapping procedure is relevant in a real-world application scenario at the Bavarian State Library for automatically identifying and annotating scanned sheet music by means of already available annotated audio material.",DEU,education,Developed economies,"[30.646664, 7.9994097]","[-21.055296, 38.677017]","[7.32533, -11.591306, -13.703525]","[-10.133392, -22.815145, 0.9615915]","[10.634328, 6.771455]","[6.7116866, -0.47781837]","[12.296257, 12.084639, -1.3433822]","[7.985464, 4.304093, 10.682139]"
104,Rainer Typke;Agatha Walczak-Typke,A Tunneling-Vantage Indexing Method for Non-Metrics.,2008,https://doi.org/10.5281/zenodo.1417307,R. Typke+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|University of Vienna>AUT>education;A. C. Walczak-Typke+University of Vienna>AUT>education,"We consider an instance of the Earth Mover’s Distance (EMD) useful for comparing rhythmical patterns. To make searches for r-near neighbours efficient, we decompose our search space into disjoint metric subspaces, in each of which the EMD reduces to the l1 norm. We then use a combined approach of two methods, one for searching within the subspaces, the other for searching between them. For the former, we show how one can use vantage indexing without false positives nor false negatives for solving the exact r-near neighbour problem, and find an optimum number and placement of vantage objects for this result. For searching between subspaces, where the EMD is not a metric, we show how one can guarantee that still no false negatives occur, and the percentage of false positives is reduced as the search radius is increased.",AUT,facility,Developed economies,"[0.8805251, 36.566463]","[9.66859, 19.204914]","[-7.2284565, -8.311715, -22.89078]","[10.636763, -9.269897, 5.104723]","[14.497365, 6.2631574]","[9.20758, 0.8649611]","[13.192628, 15.174013, -2.6346927]","[10.941391, 6.427799, 13.289297]"
103,Kjell Lemström;Niko Mikkilä;Veli Mäkinen,Fast Index Based Filters for Music Retrieval.,2008,https://doi.org/10.5281/zenodo.1416774,Kjell Lemström+University of Helsinki>FIN>education;Niko Mikkilä+University of Helsinki>FIN>education;Veli Mäkinen+University of Helsinki>FIN>education,"We consider two content-based music retrieval problems where the music is modeled as sets of points in the Euclidean plane, formed by the (on-set time, pitch) pairs. We introduce fast filtering methods based on indexing the underlying database. The filters run in a sublinear time in the length of the database, and they are lossless if a quadratic space may be used. By taking into account the application, the search space can be narrowed down, obtaining practically lossless filters using linear size index structures. For the checking phase, which dominates the overall running time, we exploit previously designed algorithms suitable for local checking. In our experiments on a music database, our best filter-based methods performed several orders of a magnitude faster than previous solutions.",FIN,education,Developed economies,"[-14.764616, 20.566721]","[11.252735, 16.987328]","[-6.4782624, 6.9952955, -13.311221]","[10.29854, -9.737491, 8.312989]","[13.679115, 7.9639754]","[9.228352, 0.7230219]","[13.571072, 14.62647, -2.020553]","[10.862948, 6.2563424, 13.334037]"
102,Mitsunori Ogihara;Tao Li 0001,N-Gram Chord Profiles for Composer Style Representation.,2008,https://doi.org/10.5281/zenodo.1415956,Mitsunori Ogihara+University of Miami>USA>education;Tao Li+Florida International University>USA>education,"This paper studies the problem of using weighted N-grams of chord sequences to construct the profile of a composer. The N-gram profile of a chord sequence is the collection of all N-grams appearing in a sequence where each N-gram is given a weight proportional to its beat count. The N-gram profile of a collection of chord sequences is the simple average of the N-gram profile of all the chord sequences in the collection. Similarity of two composers is measured by the cosine of their respective profiles, which has a value in the range [0, 1]. Using the cosine-based similarity, a group of composers is clustered into a hierarchy, which appears to be explicable. Also, the composition style can be identified using N-gram signatures.",USA,education,Developed economies,"[53.067207, 0.24820057]","[14.121608, 5.41139]","[21.81393, -17.56515, 19.214401]","[3.3080716, 2.682846, 6.64646]","[7.3643117, 8.5326805]","[9.385061, 1.7752538]","[12.178797, 10.767834, 1.5162256]","[11.204331, 6.969326, 12.917761]"
101,Eric Thul;Godfried T. Toussaint,Rhythm Complexity Measures: A Comparison of Mathematical Models of Human Perception and Performance.,2008,https://doi.org/10.5281/zenodo.1416218,Eric Thul+McGill University>CAN>education;Godfried T. Toussaint+McGill University>CAN>education,"Thirty two measures of rhythm complexity are compared using three widely different rhythm data sets. Twenty-two of these measures have been investigated in a limited context in the past, and ten new measures are explored here. Some of these measures are mathematically inspired, some were designed to measure syncopation, some were intended to predict various measures of human performance, some are based on constructs from music theory, such as Pressing’s cognitive complexity, and others are direct measures of different aspects of human performance, such as perceptual complexity, meter complexity, and performance complexity. In each data set the rhythms are ranked either according to increasing complexity using the judgements of human subjects, or using calculations with the computational models. Spearman rank correlation coefficients are computed between all pairs of rhythm rankings. Then phylogenetic trees are used to visualize and cluster the correlation coefficients. Among the many conclusions evident from the results, there are several observations common to all three data sets that are worthy of note. The syncopation measures form a tight cluster far from other clusters. The human performance measures fall in the same cluster as the syncopation measures. The complexity measures based on statistical properties of the inter-onset-interval histograms are poor predictors of syncopation or human performance complexity. Finally, this research suggests several open problems.",CAN,education,Developed economies,"[45.9463, 7.258977]","[-23.49005, 3.9851918]","[-7.201924, -25.330774, 2.43137]","[-1.4532921, 17.223541, -4.525798]","[12.01488, 5.3463407]","[6.041444, 1.6570398]","[11.301718, 14.160336, -2.2343483]","[8.037882, 6.755797, 11.875911]"
100,Linxing Xiao;Aibo Tian;Wen Li;Jie Zhou,Using Statistic Model to Capture the Association between Timbre and Perceived Tempo.,2008,https://doi.org/10.5281/zenodo.1414922,Linxing Xiao+Tsinghua University>CHN>education;Aibo Tian+Tsinghua University>CHN>education;Wen Li+Tsinghua University>CHN>education;Jie Zhou+Tsinghua University>CHN>education,"The estimation of the perceived tempo is required in many MIR applications. However, automatic tempo estimation itself is still an open problem due to the insufficient understanding of the inherent mechanisms of the tempo perception. Published methods only use the information of rhythm pattern, so they may meet the half/double tempo error problem. To solve this problem, we propose to use a statistic model to investigate the association between timbre and tempo and use timbre information to improve the performance of tempo estimation. Experiment results show that this approach performs at least comparably to existing tempo extraction algorithms.",CHN,education,Developing economies,"[42.345478, -24.927652]","[-29.70329, -5.479534]","[-4.7051334, -28.491255, 0.19727679]","[-6.5818405, 12.839459, -10.054931]","[11.572257, 4.566054]","[5.1620803, 1.7859836]","[10.920841, 13.519759, -2.7299113]","[7.4004955, 6.9633975, 11.013534]"
99,Andre Holzapfel;Yannis Stylianou,Beat Tracking using Group Delay Based Onset Detection.,2008,https://doi.org/10.5281/zenodo.1415516,"Andre Holzapfel+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education;Yannis Stylianou+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education","""This paper introduces a novel approach to estimate onsets in musical signals based on the phase spectrum and specifically using the average of the group delay function. A frame-by-frame analysis of a music signal provides the evolution of group delay over time, referred to as phase slope function. Onsets are then detected simply by locating the positive zero-crossings of the phase slope function. The proposed approach is compared to an amplitude-based onset detection approach in the framework of a state-of-the-art system for beat tracking. On a data set of music with less percussive content, the beat tracking accuracy achieved by the system is improved by 82% when the suggested phase-based onset detection approach is used instead of the amplitude-based approach, while on a set of music with stronger percussive characteristics both onset detection approaches provide comparable results of accuracy.""",GRC,facility,Developed economies,"[35.12576, -32.976295]","[-24.422123, -4.406231]","[8.20019, -26.58139, -6.5859013]","[-1.8851973, 8.046476, -9.09002]","[10.462762, 4.484278]","[5.466236, 2.0952117]","[10.235743, 13.028474, -2.042954]","[7.707892, 7.261884, 11.066418]"
97,Ernesto Trajano de Lima;Geber Ramalho,On Rhythmic Pattern Extraction in Bossa Nova Music.,2008,https://doi.org/10.5281/zenodo.1417959,Ernesto Trajano de Lima+Centro de Informática (CIn)—Univ. Federal de Pernambuco>BRA>education;Geber Ramalho+Centro de Informática (CIn)—Univ. Federal de Pernambuco>BRA>education,"The analysis of expressive performance, an important research topic in Computer Music, is almost exclusively devoted to the study of Western Classical piano music. Instruments like the acoustic guitar and styles like Bossa Nova and Samba have been little studied, despite their harmonic and rhythmic richness. This paper describes some experimental results obtained with the extraction of rhythmic patterns from the guitar accompaniment of Bossa Nova songs. The songs, played by two different performers and recorded with the help of a MIDI guitar, were represented as strings and processed by FlExPat, a string matching algorithm. The results obtained were then compared to a previously acquired catalogue of “good” patterns.",BRA,education,Developing economies,"[8.882725, 8.410533]","[-32.948868, 10.647376]","[-6.793215, -18.388348, -0.0029879538]","[-12.190164, -9.465042, 2.2129037]","[11.857162, 5.858917]","[8.037359, 1.5657766]","[11.601777, 14.115003, -1.8667232]","[9.16, 6.243573, 11.150773]"
96,Patrick Flanagan,Quantifying Metrical Ambiguity.,2008,https://doi.org/10.5281/zenodo.1417117,Patrick Flanagan+Unknown>Unknown>Unknown,"This paper explores how data generated by meter induction models may be recycled to quantify metrical ambiguity, which is calculated by measuring the dispersion of metrical induction strengths across a population of possible meters. A measure of dispersion commonly used in economics to measure income inequality, the Gini coefficient, is introduced for this purpose. The value of this metric as a rhythmic descriptor is explored by quantifying the ambiguity of several common clave patterns and comparing the results to other metrics of rhythmic complexity and syncopation.",Unknown,Unknown,Unknown,"[-1.80163, 22.405363]","[-23.574911, 4.9084773]","[-11.518549, 1.3916761, -0.022415387]","[-1.8581939, 18.519882, -5.7884893]","[12.967129, 9.487599]","[6.066458, 1.6630657]","[13.368757, 15.183072, -0.6301136]","[8.060262, 6.7819657, 11.852641]"
95,Nicolas Scaringella,Timbre and Rhythmic TRAP-TANDEM Features for Music Information Retrieval.,2008,https://doi.org/10.5281/zenodo.1418113,Nicolas Scaringella+Idiap Research Institute>CHE>facility|Ecole Polytechnique Fédérale de Lausanne (EPFL)>CHE>education,"The enormous growth of digital music databases has led to a comparable growth in the need for methods that help users organize and access such information. One area in particular that has seen much recent research activity is the use of automated techniques to describe audio content and to allow for its identification, browsing and retrieval. Conventional approaches to music content description rely on features characterizing the shape of the signal spectrum in relatively short-term frames. In the context of Automatic Speech Recognition (ASR), Hermansky described an interesting alternative to short-term spectrum features, the TRAP-TANDEM approach which uses long-term band-limited features trained in a supervised fashion. We adapt this idea to the specific case of music signals and propose a generic system for the description of temporal patterns. The same system with different settings is able to extract features describing either timbre or rhythmic content. The quality of the generated features is demonstrated in a set of music retrieval experiments and compared to other state-of-the-art models.",CHE,facility,Developed economies,"[-14.06185, 22.610456]","[14.105568, -9.854993]","[-6.8528256, 1.4821016, -7.866854]","[7.982183, 5.06755, -7.129046]","[13.35492, 8.008409]","[9.137207, 3.3518243]","[13.162765, 14.602818, -1.910681]","[11.094088, 7.356395, 10.713769]"
94,Yusuke Tsuchihashi;Tetsuro Kitahara;Haruhiro Katayose,Using Bass-line Features for Content-Based MIR.,2008,https://doi.org/10.5281/zenodo.1417895,"Yusuke Tsuchihashi+Kwansei Gakuin University>JPN>education;Tetsuro Kitahara+Kwansei Gakuin University>JPN>education|CrestMuse Project, CREST, JST>JPN>facility;Haruhiro Katayose+Kwansei Gakuin University>JPN>education|CrestMuse Project, CREST, JST>JPN>facility","We propose new audio features that can be extracted from bass lines. Most previous studies on content-based music information retrieval (MIR) used low-level features such as the mel-frequency cepstral coefficients and spectral centroid. Musical similarity based on these features works well to some extent but has a limit to capture fine musical characteristics. Because bass lines play important roles in both harmonic and rhythmic aspects and have a different style for each music genre, our bass-line features are expected to improve the similarity measure and classification accuracy. Furthermore, it is possible to achieve a similarity measure that enhances the bass-line characteristics by weighting the bass-line and other features. Results for applying our features to automatic genre classification and music collection visualization showed that our features improved genre classification accuracy and did achieve a similarity measure that enhances bass-line characteristics.",JPN,education,Developed economies,"[-12.457774, 56.7908]","[16.192646, -10.316474]","[-38.1979, 3.4612021, -0.45310277]","[4.9008203, 6.351669, -5.95351]","[13.366225, 5.034896]","[9.342957, 3.1375535]","[14.762755, 11.39594, -1.3758289]","[11.192984, 7.3998146, 10.998389]"
93,Luke Barrington;Mehrdad Yazdani;Douglas Turnbull;Gert R. G. Lanckriet,Combining Feature Kernels for Semantic Music Retrieval.,2008,https://doi.org/10.5281/zenodo.1415070,"Luke Barrington+University of California, San Diego>USA>education;Mehrdad Yazdani+University of California, San Diego>USA>education;Douglas Turnbull+University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education","We apply a new machine learning tool, kernel combination, to the task of semantic music retrieval. We use 4 different types of acoustic content and social context feature sets to describe a large music corpus and derive 4 individual kernel matrices from these feature sets. Each kernel is used to train a support vector machine (SVM) classifier for each semantic tag (e.g., ‘aggressive’, ‘classic rock’, ‘distorted electric guitar’) in a large tag vocabulary. We examine the individual performance of each feature kernel and then show how to learn an optimal linear combination of these kernels using convex optimization. We find that the retrieval performance of the SVMs trained using the combined kernel is superior to SVMs trained using the best individual kernel for a large number of tags. In addition, the weights placed on individual kernels in the linear combination reflect the relative importance of each feature set when predicting a tag.",USA,education,Developed economies,"[-12.847587, 15.560856]","[38.02848, -2.8923674]","[-6.036871, 10.923236, -9.005433]","[23.798162, 8.684689, -0.44839925]","[13.362177, 8.673927]","[11.435532, 3.428966]","[13.741778, 14.680182, -1.0985984]","[12.905061, 6.435901, 11.153156]"
92,Tomonori Izumitani;Kunio Kashino,A Robust Musical Audio Search Method Based on Diagonal Dynamic Programming Matching of Self-Similarity Matrices.,2008,https://doi.org/10.5281/zenodo.1417399,Tomonori Izumitani+NTT Communication Science Laboratories>JPN>facility;Kunio Kashino+NTT Communication Science Laboratories>JPN>facility,"We propose a new musical audio search method based on audio signal matching that can cope with key and tempo variations. The method employs the self-similarity matrix of an audio signal to represent a key-invariant structure of musical audio. And, we use dynamic programming (DP) matching of self-similarity matrices to deal with time variations. However, conventional DP-based sequence matching methods cannot be directly applied for self-similarity matrices because they cannot treat gaps independently of other time frames. We resolve this problem by introducing “matched element indices,” which reflect the history of matching, to a DP-based sequence matching method. We performed experiments using musical audio signals. The results indicate that the proposed method improves the detection accuracy in comparison to that that obtained by two conventional methods, namely, DP matching with chroma-based vector rotations and a simple matching of self-similarity feature vectors.",JPN,facility,Developed economies,"[-4.0627737, 6.970566]","[-2.9248025, -4.7098284]","[-4.4043283, -2.0901303, -4.5427794]","[5.9846215, -17.86688, -0.17112167]","[12.815836, 8.801014]","[7.3275566, 1.6108363]","[13.129141, 14.418014, -0.8185983]","[10.528193, 7.5513945, 11.820669]"
91,Pierre-Antoine Manzagol;Thierry Bertin-Mahieux;Douglas Eck,On the Use of Sparce Time Relative Auditory Codes for Music.,2008,https://doi.org/10.5281/zenodo.1415034,Pierre-Antoine Manzagol+Université de Montréal>CAN>education;Thierry Bertin-Mahieux+Université de Montréal>CAN>education;Douglas Eck+Université de Montréal>CAN>education,"Many if not most audio features used in MIR research are inspired by work done in speech recognition and are variations on the spectrogram. Recently, much attention has been given to new representations of audio that are sparse and time-relative. These representations are efficient and able to avoid the time-frequency trade-off of a spectrogram. Yet little work with music streams has been conducted and these features remain mostly unused in the MIR community. In this paper we further explore the use of these features for musical signals. In particular, we investigate their use on realistic music examples (i.e. released commercial music) and their use as input features for supervised learning. Furthermore, we identify three specific issues related to these features which will need to be further addressed in order to obtain the full benefit for MIR applications.",CAN,education,Developed economies,"[22.25153, -26.118761]","[-9.482505, 27.750837]","[-1.2921764, -19.62022, -1.5398396]","[-6.5751863, -12.771475, 7.616019]","[11.540336, 5.711579]","[9.445687, 3.883239]","[11.4028, 13.496252, -1.8575343]","[10.39829, 6.114805, 10.572556]"
90,Cory McKay;Ichiro Fujinaga,"Combining Features Extracted from Audio, Symbolic and Cultural Sources.",2008,https://doi.org/10.5281/zenodo.1415576,Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper experimentally investigates the classification utility of combining features extracted from separate audio, symbolic and cultural sources of musical information. This was done via a series of genre classification experiments performed using all seven possible combinations and subsets of the three corresponding types of features. These experiments were performed using jMIR, a software suite designed for use both as a toolset for performing MIR research and as a platform for developing and sharing new algorithms. The experimental results indicate that combining feature types can indeed substantively improve classification accuracy. Accuracies of 96.8% and 78.8% were attained respectively on 5 and 10-class genre taxonomies when all three feature types were combined, compared to average respective accuracies of 85.5% and 65.1% when features extracted from only one of the three sources of data were used. It was also found that combining feature types decreased the seriousness of those misclassifications that were made, on average, particularly when cultural features were included.",CAN,education,Developed economies,"[-26.069757, -0.89406365]","[21.23898, -5.581702]","[-22.880575, -5.9811935, -3.385886]","[10.635865, 11.359094, -1.1897801]","[13.884577, 8.781017]","[9.996558, 3.1623273]","[14.33472, 13.275435, -0.98492765]","[11.826751, 6.7515454, 10.976585]"
89,Patrick Rabbat;François Pachet,Direct and Inverse Inference in Music Databases: How to Make at Song Funk?,2008,https://doi.org/10.5281/zenodo.1416486,Patrick Rabbat+Sony CSL>FRA>company;François Pachet+Sony CSL>FRA>company,"We propose an algorithm for exploiting statistical properties of large-scale metadata databases about music titles to answer musicological queries. We introduce two inference schemes called “direct” and “inverse” inference, based on an efficient implementation of a kernel regression approach. We describe an evaluation experiment conducted on a large-scale database of fine-grained musical metadata. We use this database to train the direct inference algorithm, test it, and also to identify the optimal parameters of the algorithm. The inverse inference algorithm is based on the direct inference algorithm. We illustrate it with some examples.",FRA,company,Developed economies,"[-15.937356, 11.775038]","[35.820442, -0.3069809]","[-10.886366, 0.4470038, -5.302777]","[21.701372, 5.5943193, -0.034834698]","[13.346953, 8.291362]","[11.210154, 3.2947764]","[13.785966, 14.335068, -1.2815902]","[12.660201, 6.217119, 11.324913]"
88,Ioannis Panagakis;Emmanouil Benetos;Constantine Kotropoulos,Music Genre Classification: A Multilinear Approach.,2008,https://doi.org/10.5281/zenodo.1414714,Ioannis Panagakis+Aristotle University of Thessaloniki>GRC>education;Emmanouil Benetos+Aristotle University of Thessaloniki>GRC>education;Constantine Kotropoulos+Aristotle University of Thessaloniki>GRC>education,"In this paper, music genre classification is addressed in a multilinear perspective. Inspired by a model of auditory cortical processing, multiscale spectro-temporal modulation features are extracted. Such spectro-temporal modulation features have been successfully used in various content-based audio classification tasks recently, but not yet in music genre classification. Each recording is represented by a third-order feature tensor generated by the auditory model. Thus, the ensemble of recordings is represented by a fourth-order data tensor created by stacking the third-order feature tensors associated to the recordings. To handle large data tensors and derive compact feature vectors suitable for classification, three multilinear subspace techniques are examined, namely the Non-Negative Tensor Factorization (NTF), the High-Order Singular Value Decomposition (HOSVD), and the Multilinear Principal Component Analysis (MPCA). Classification is performed by a Support Vector Machine. Stratified cross-validation tests on the GTZAN dataset and the ISMIR 2004 Genre one demonstrate the advantages of NTF and HOSVD versus MPCA. The best accuracies obtained by the proposed multilinear approach is comparable with those achieved by state-of-the-art music genre classification algorithms.",GRC,education,Developed economies,"[-29.311825, -11.909463]","[17.845572, -23.73492]","[-16.180042, 6.12509, 13.718009]","[19.38015, 5.7743244, -13.628383]","[13.030094, 10.934045]","[9.353266, 3.6947207]","[14.0709505, 14.198784, 1.3910818]","[11.031428, 7.9745092, 10.507174]"
87,Michael I. Mandel;Daniel P. W. Ellis,Multiple-Instance Learning for Music Information Retrieval.,2008,https://doi.org/10.5281/zenodo.1418299,Michael I. Mandel+Columbia University>USA>education|LabROSA>USA>facility;Daniel P. W. Ellis+Columbia University>USA>education|LabROSA>USA>facility,"Multiple-instance learning algorithms train classifiers from lightly supervised data, i.e. labeled collections of items, rather than labeled items. We compare the multiple-instance learners mi-SVM and MILES on the task of classifying 10-second song clips. These classifiers are trained on tags at the track, album, and artist levels, or granularities, that have been derived from tags at the clip granularity, allowing us to test the effectiveness of the learners at recovering the clip labeling in the training set and predicting the clip labeling for a held-out test set. We find that mi-SVM is better than a control at the recovery task on training clips, with an average classification accuracy as high as 87% over 43 tags; on test clips, it is comparable to the control with an average classification accuracy of up to 68%. MILES performed adequately on the recovery task, but poorly on the test clips.",USA,education,Developed economies,"[-13.950444, 16.660028]","[33.96831, -3.657147]","[-8.9537115, 12.188193, -10.61827]","[16.762253, 14.749916, -1.9326537]","[13.506633, 8.509735]","[10.515774, 3.6954846]","[13.848005, 14.6624155, -1.419318]","[12.284522, 6.410205, 10.731363]"
86,Parag Chordia;Mark Godfrey;Alex Rae,Extending Content-Based Recommendation: The Case of Indian Classical Music.,2008,https://doi.org/10.5281/zenodo.1415132,Parag Chordia+Georgia Tech>USA>education;Mark Godfrey+Georgia Tech>USA>education;Alex Rae+Georgia Tech>USA>education,"We describe a series of experiments that attempt to create a content-based similarity model suitable for making recommendations about North Indian classical music (NICM). We introduce a dataset (nicm2008) consisting of 897 tracks of NICM along with substantial ground-truth annotations, including artist, predominant instrument, tonic pitch, raag, and parent scale (thaat). Using a timbre-based similarity model derived from short-time MFCCs we find that artist R-precision is 32.69% and that the predominant instrument is correctly classified 90.30% of the time. Consistent with previous work, we find that certain tracks (“hubs”) appear falsely similar to many other tracks. We find that this problem can be attenuated by model homogenization. We also introduce the use of pitch-class distribution (PCD) features to measure melodic similarity. Its effectiveness is evaluated by raag R-precision (16.97%), thaat classification accuracy (75.83%), and comparison to reference similarity metrics. We propose that a hybrid timbral-melodic similarity model may be effective for Indian classical music recommendation. Further, this work suggests that “hubs” are a general features of such similarity modeling that may be partially alleviated by model homogenization.",USA,education,Developed economies,"[-43.61537, 25.697134]","[31.184029, 5.835136]","[-14.136137, 24.112429, -12.824262]","[12.521826, 3.7192197, 7.739585]","[15.891325, 9.226662]","[11.034528, 2.4412556]","[15.774815, 15.618984, -1.4143108]","[12.532815, 6.4872937, 12.395841]"
84,Benjamin Fields;Christophe Rhodes;Michael A. Casey;Kurt Jacobson,Social Playlists and Bottleneck Measurements: Exploiting Musician Social Graphs Using Content-Based Dissimilarity and Pairwise Maximum Flow Values.,2008,https://doi.org/10.5281/zenodo.1417939,"Ben Fields+Goldsmiths, University of London>GBR>education;Christophe Rhodes+Goldsmiths, University of London>GBR>education;Michael Casey+Goldsmiths, University of London>GBR>education;Kurt Jacobson+Queen Mary, University of London>GBR>education","We have sampled the artist social network of Myspace and to it applied the pairwise relational connectivity measure Minimum cut/Maximum flow. These values are then compared to a pairwise acoustic Earth Mover’s Distance measure and the relationship is discussed. Further, a means of constructing playlists using the maximum flow value to exploit both the social and acoustic distances is realized.",GBR,education,Developed economies,"[-41.93136, 35.194103]","[42.403255, 9.4987]","[-8.944554, 28.75777, -3.267278]","[21.789684, 3.201381, 9.411632]","[15.561621, 8.688522]","[12.142285, 2.6345057]","[15.857559, 14.8794565, -1.35237]","[13.277258, 5.6965013, 11.9555645]"
60,John Ashley Burgoyne;Johanna Devaney;Laurent Pugin;Ichiro Fujinaga,Enhanced Bleedthrough Correction for Early Music Documents with Recto-Verso Registration.,2008,https://doi.org/10.5281/zenodo.1417235,John Ashley Burgoyne+McGill University>CAN>education;Johanna Devaney+McGill University>CAN>education;Laurent Pugin+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Ink bleedthrough is common problem in early music documents. Even when such bleedthrough does not pose problems for human perception, it can inhibit the performance of optical music recognition (OMR). One way to reduce the amount of bleedthrough is to take into account what is printed on the reverse of the page. In order to do so, the reverse of the page must be registered to match the front of the page on a pixel-by-pixel basis. This paper describes our approach to registering scanned early music scores as well as our modifications to two robust binarization approaches to take into account bleedthrough and the information available from the registration process. We determined that although the information from registration itself often makes little difference in recognition performance, other modifications to binarization algorithms for correcting bleedthrough can yield dramatic increases in OMR results.",CAN,education,Developed economies,"[33.81334, 24.227604]","[-20.953861, 40.597786]","[20.739143, -8.171479, -20.852722]","[-10.40055, -24.577213, 3.1421802]","[10.851388, 6.6582966]","[6.658088, -0.61207926]","[12.157093, 12.4919, -1.1884655]","[7.875165, 4.142182, 10.565292]"
7,Mathieu Bergeron;Darrell Conklin,Structured Polyphonic Patterns.,2008,https://doi.org/10.5281/zenodo.1415638,Mathieu Bergeron+City University London>GBR>education;Darrell Conklin+City University London>GBR>education,"This paper presents a new approach to polyphonic music retrieval, based on a structured pattern representation. Polyphonic patterns are formed by joining and layering pattern components into sequences and simultaneities. Pattern components are conjunctions of features which encode event properties or relations with other events. Relations between events that overlap in time but are not simultaneous are supported, enabling patterns to express many of the temporal relations encountered in polyphonic music. The approach also provides a mechanism for defining new features. It is illustrated and evaluated by querying for three musicological patterns in a corpus of 185 chorale harmonizations by J.S.Bach.",GBR,education,Developed economies,"[11.806823, 11.25425]","[6.681018, 15.026835]","[1.0530183, -14.115152, 6.5669317]","[3.416961, -9.576467, 8.671249]","[11.3792095, 7.4292064]","[8.944749, 0.8916993]","[12.395759, 13.228449, -0.8145076]","[10.574221, 6.4199147, 12.891742]"
57,Meinard Müller;Sebastian Ewert,Joint Structure Analysis with Applications to Music Annotation and Synchronization.,2008,https://doi.org/10.5281/zenodo.1415236,Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Sebastian Ewert+Bonn University>DEU>education,"The general goal of music synchronization is to automatically align different versions and interpretations related to a given musical work. In computing such alignments, recent approaches assume that the versions to be aligned correspond to each other with respect to their overall global structure. However, in real-world scenarios, this assumption is often violated. For example, for a popular song there often exist various structurally different album, radio, or extended versions. Or, in classical music, different recordings of the same piece may exhibit omissions of repetitions or significant differences in parts such as solo cadenzas. In this paper, we introduce a novel approach for automatically detecting structural similarities and differences between two given versions of the same piece. The key idea is to perform a single structural analysis for both versions simultaneously instead of performing two separate analyses for each of the two versions. Such a joint structure analysis reveals the repetitions within and across the two versions. As a further contribution, we show how this information can be used for deriving musically meaningful partial alignments and annotations in the presence of structural variations.",DEU,education,Developed economies,"[14.722987, -12.737891]","[-15.531393, -14.647498]","[-1.4668672, -8.446691, -4.6989856]","[-0.4988267, -21.557419, 0.3408334]","[11.127941, 6.5929976]","[6.8862934, 1.3486124]","[12.196032, 12.84612, -1.4735844]","[9.658021, 6.8113074, 11.443638]"
26,Masahiro Niitsuma;Hiroshi Takaesu;Hazuki Demachi;Masaki Oono;Hiroaki Saito,Development of an Automatic Music Selection System Based on Runner's Step Frequency.,2008,https://doi.org/10.5281/zenodo.1415900,Niitsuma Masahiro+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Hiroshi Takaesu+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Hazuki Demachi+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Masaki Oono+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Hiroaki Saito+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown,"This paper presents an automatic music selection system based on runner’s step frequency. Recent development of portable music players like iPod has increased the number of those who listen to music while exercising. However, few systems which connect exercises with music selection have been developed. We propose a system that automatically selects music suitable for user’s running exercises. Although many parameters can be taken into account, as a first step we focus on runner’s step frequency. This system selects music with tempo suitable for runner’s step frequency and when runner’s step frequency changes, it executes another music selection. The system consists of three modules: step frequency estimation, music selection, and music playing. In the first module, runner’s step frequency is estimated from data derived from an acceleration sensor. In the second module, appropriate music is selected based on the estimated step frequency. In the third module, the selected music is played until runner’s step frequency changes. In the experiment, subjects ran on a running machine at different paces listening to the music selected by the proposed system. Experimental results show that the system can estimate runner’s SPM accurately and on the basis of the estimated SPM it can select music appropriate for users’ exercises with more than 85.0% accuracy, and makes running exercises more pleasing.",JPN,education,Developed economies,"[-21.877924, -22.445871]","[38.513077, 24.89733]","[-4.119653, -22.52778, -7.192188]","[-3.7180185, 20.90138, 18.722294]","[10.759029, 4.955093]","[5.3152566, 1.6470226]","[10.711349, 13.286653, -1.9076529]","[7.5809536, 6.683584, 11.091211]"
25,Sally Jo Cunningham;Edmond Zhang,Development of a Music Organizer for Children.,2008,https://doi.org/10.5281/zenodo.1418329,Sally Jo Cunningham+University of Waikato>NZL>education;Yiwen (Edmond) Zhang+University of Waikato>NZL>education,"Software development for children is challenging; children have their own needs, which often are not met by ‘grown up’ software. We focus on software for playing songs and managing a music collection—tasks that children take great interest in, but for which they have few or inappropriate tools. We address this situation with the design of a new music management system, created with children as design partners: the Kids Music Box.",NZL,education,Developed economies,"[-28.045486, 29.300245]","[38.104008, 37.58968]","[-7.261755, 22.539053, 1.2132854]","[1.9618545, 14.1350565, 22.835024]","[15.017384, 8.007417]","[11.800004, 0.67775357]","[15.144269, 14.530648, -1.9681233]","[12.150673, 4.22189, 11.734732]"
24,Markus Dopler;Markus Schedl;Tim Pohle;Peter Knees,Accessing Music Collections Via Representative Cluster Prototypes in a Hierarchical Organization Scheme.,2008,https://doi.org/10.5281/zenodo.1417305,Markus Dopler+Johannes Kepler University>AUT>education;Markus Schedl+Johannes Kepler University>AUT>education;Tim Pohle+Johannes Kepler University>AUT>education;Peter Knees+Johannes Kepler University>AUT>education,"This paper addresses the issue of automatically organizing a possibly large music collection for intuitive access. We present an approach to cluster tracks in a hierarchical manner and to automatically find representative pieces of music for each cluster on each hierarchy level. To this end, audio signal-based features are complemented with features derived via Web content mining in a novel way. Automatic hierarchical clustering is performed using a variant of the Self-Organizing Map, which we further modified in order to create playlists containing similar tracks. The proposed approaches for playlist generation on a hierarchically structured music collection and finding prototypical tracks for each cluster are then integrated into the Traveller’s Sound Player, a mobile audio player application that organizes music in a playlist such that the distances between consecutive tracks are minimal. We extended this player to deal with the hierarchical nature of the playlists generated by the proposed structuring approach. As for evaluation, we first assess the quality of the clustering method using the measure of entropy on a genre-annotated test set. Second, the goodness of the method to find prototypical tracks for each cluster is investigated in a user study.",AUT,education,Developed economies,"[-26.853964, 28.193804]","[28.14351, 17.50817]","[-17.606607, 15.947062, -20.625551]","[16.60646, -5.944412, 22.62761]","[14.453817, 7.8874507]","[11.052953, 1.9580007]","[14.364694, 14.39619, -2.0939858]","[12.373233, 5.9077296, 13.006846]"
23,Arthur Flexer;Dominik Schnitzer;Martin Gasser;Gerhard Widmer,Playlist Generation using Start and End Songs.,2008,https://doi.org/10.5281/zenodo.1418273,Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Dominik Schnitzer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education,A new algorithm for automatic generation of playlists with an inherent sequential order is presented. Based on a start and end song it creates a smooth transition allowing users to discover new songs in a music collection. The approach is based on audio similarity and does not require any kind of meta data. It is evaluated using both objective genre labels and subjective listening tests. Our approach allows users of the website of a public radio station to create their own digital “mixtapes” online.,AUT,facility,Developed economies,"[-39.00607, 39.425102]","[36.17751, 21.850845]","[-1.2700733, 27.936049, -2.320323]","[15.375312, 4.416575, 22.687035]","[16.178356, 8.177726]","[12.043108, 1.5480056]","[16.567495, 14.82502, -1.7439429]","[13.121879, 5.137969, 13.192387]"
22,Yvonne Moh;Joachim M. Buhmann,Kernel Expansion for Online Preference Tracking.,2008,https://doi.org/10.5281/zenodo.1415962,Yvonne Moh+Swiss Federal Institute of Technology (ETH) Zurich>CHE>education|Institute of Computational Science>CHE>education;Joachim M. Buhmann+Swiss Federal Institute of Technology (ETH) Zurich>CHE>education|Institute of Computational Science>CHE>education,"User preferences of music genres can significantly change over time depending on fashions and the personal situation of music consumers. We propose a model to learn user preferences and their changes in an adaptive way. Our approach refines a model for user preferences by explicitly considering two plausible constraints of computational costs and limited storage space. The model is required to adapt itself to changing data distributions, and yet be able to compress “historical” data. We exploit the success of kernel SVM, and we consider an online expansion of the induced space as a preprocessing step to a simple linear online learner that updates with maximal agreement to previously seen data.",CHE,education,Developed economies,"[-44.859886, 21.575327]","[43.737278, 18.369833]","[-9.250496, 19.978626, -7.548681]","[20.890257, 10.234307, 17.623755]","[15.5940275, 9.184381]","[12.201791, 2.2134824]","[15.571932, 15.528087, -1.3915771]","[13.334213, 5.710759, 12.428791]"
21,Terence Magno;Carl Sable,"A Comparison of Signal Based Music Recommendation to Genre Labels, Collaborative Filtering, Musicological Analysis, Human Recommendation and Random Baseline.",2008,https://doi.org/10.5281/zenodo.1418139,Terence Magno+Cooper Union>USA>education|Cooper Union>USA>education;Carl Sable+Cooper Union>USA>education|Cooper Union>USA>education,"The emergence of the Internet as today’s primary medium of music distribution has brought about demands for fast and reliable ways to organize, access, and discover music online. To date, many applications designed to perform such tasks have risen to popularity; each relies on a specific form of music metadata to help consumers discover songs and artists that appeal to their tastes. Very few of these applications, however, analyze the signal waveforms of songs directly. This low-level representation can provide dimensions of information that are inaccessible by metadata alone. To address this issue, we have implemented signal-based measures of musical similarity that have been optimized based on their correlations with human judgments. Furthermore, multiple recommendation engines relying on these measures have been implemented. These systems recommend songs to volunteers based on other songs they find appealing. Blind experiments have been conducted in which volunteers rate the systems’ recommendations along with recommendations of leading online music discovery tools (Allmusic which uses genre labels, Pandora which uses musicological analysis, and Last.fm which uses collaborative filtering), random baseline recommendations, and personal recommendations by the first author. This paper shows that the signal-based engines perform about as well as popular, commercial, state-of-the-art systems.",USA,education,Developed economies,"[-45.679447, 25.703175]","[36.494537, 15.838451]","[-9.728936, 22.809973, -9.696982]","[13.893331, 6.023573, 14.311824]","[15.890683, 9.219109]","[12.259601, 1.9614741]","[15.697262, 15.611643, -1.4212817]","[13.322199, 5.281902, 12.593492]"
20,Rebecca Fiebrink;Ge Wang 0002;Perry R. Cook,Support for MIR Prototyping and Real-Time Applications in the ChucK Programming Language.,2008,https://doi.org/10.5281/zenodo.1415268,Rebecca Fiebrink+Princeton University>USA>education;Ge Wang+Stanford University>USA>education;Perry Cook+Princeton University>USA>education,"In this paper, we discuss our recent additions of audio analysis and machine learning infrastructure to the ChucK music programming language, wherein we provide a complementary system prototyping framework for MIR researchers and lower the barriers to applying many MIR algorithms in live music performance. The new language capabilities preserve ChucK’s breadth of control—from high-level control using building block components to sample-level manipulation—and on-the-fly reprogrammability, allowing the programmer to experiment with new features, signal processing techniques, and learning algorithms with ease and flexibility. Furthermore, our additions integrate tightly with ChucK’s synthesis system, allowing the programmer to apply the results of analysis and learning to drive real-time music creation and interaction within a single framework. In this paper, we motivate and describe our recent additions to the language, outline a ChucK-based approach to rapid MIR prototyping, present three case studies in which we have applied ChucK to audio analysis and MIR tasks, and introduce our new toolkit to facilitate experimentation with analysis and learning in the language.",USA,education,Developed economies,"[-10.12607, 59.425392]","[-8.013804, 33.50365]","[-38.24075, 2.9952838, -7.00906]","[-16.348164, 1.2231609, 16.846506]","[13.503874, 4.789896]","[9.293699, 5.708341]","[14.946784, 11.140116, -1.4589401]","[10.134002, 5.5505543, 10.142092]"
19,Ian Knopke,The PerlHumdrum and PerlLilypond Toolkits for Symbolic Music Information Retrieval.,2008,https://doi.org/10.5281/zenodo.1416894,Ian Knopke+Goldsmiths Digital Studios>GBR>facility,"PerlHumdrum is an alternative toolkit for working with large numbers of Humdrum scores. While based on the original Humdrum toolkit, it is a completely new, self-contained implementation that can serve as a replacement, and may be a better choice for some computing systems. PerlHumdrum is fully object-oriented, is designed to easily facilitate analysis and processing of multiple humdrum files, and to answer common musicological questions across entire sets, collections of music, or even the entire output of single or multiple composers. Several extended capabilities that are not available in the original toolkit are also provided, such as translation of MIDI scores to Humdrum, provisions for constructing graphs, a graphical user interface for non-programmers, and the ability to generate complete scores or partial musical examples as standard musical notation using PerlLilypond. These tools are intended primarily for use by music theorists, computational musicologists, and Music Information Retrieval (MIR) researchers.",GBR,facility,Developed economies,"[15.709038, 16.055588]","[-2.0428238, 30.889645]","[-4.470137, -6.9722295, 16.837152]","[-11.246463, -1.8980074, 14.258579]","[11.997452, 7.1277895]","[9.726178, 0.7028403]","[13.588913, 12.473779, -1.1069924]","[10.406558, 5.5145555, 11.783923]"
18,Nobutaka Ono;Kenichi Miyamoto;Hirokazu Kameoka;Shigeki Sagayama,A Real-time Equalizer of Harmonic and Percussive Components in Music Signals.,2008,https://doi.org/10.5281/zenodo.1415044,Nobutaka Ono+The University of Tokyo>JPN>education;Kenichi Miyamoto+The University of Tokyo>JPN>education;Hirokazu Kameoka+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"""In this paper, we present a real-time equalizer to control a volume balance of harmonic and percussive components in music signals without a priori knowledge of scores or included instruments. The harmonic and percussive components of music signals have much different structures in the power spectrogram domain, the former is horizontal, while the latter is vertical. Exploiting the anisotropy, our methods separate input music signals into them based on the MAP estimation framework. We derive two kind of algorithm based on a I-divergence-based mixing model and a hard mixing model. Although they include iterative update equations, we realized the real-time processing by a sliding analysis technique. The separated harmonic and percussive components are finally remixed in an arbitrary volume balance and played. We show the prototype system implemented on Windows environment.""",JPN,education,Developed economies,"[32.799786, -20.130085]","[-37.01418, -21.908915]","[23.8373, -6.7502394, -11.536151]","[-20.430788, 0.24158923, -27.547258]","[9.103001, 9.350699]","[7.340883, 6.1771526]","[11.285531, 13.540186, 0.7387901]","[9.478667, 7.751414, 9.541886]"
17,Katsutoshi Itoyama;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Instrument Equalizer for Query-by-Example Retrieval: Improving Sound Source Separation Based on Integrated Harmonic and Inharmonic Models.,2008,https://doi.org/10.5281/zenodo.1417407,"Katsutoshi Itoyama+Graduate School of Infomatics, Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazunori Komatani+Graduate School of Infomatics, Kyoto University>JPN>education;Tetsuya Ogata+Graduate School of Infomatics, Kyoto University>JPN>education;Hiroshi G. Okuno+Graduate School of Infomatics, Kyoto University>JPN>education","This paper describes a music remixing interface, called Instrument Equalizer, that allows users to control the volume of each instrument part within existing audio recordings in real time. Although query-by-example retrieval systems need a user to prepare favorite examples (songs) in general, our interface gives a user to generate examples from existing ones by cutting or boosting some instrument/vocal parts, resulting in a variety of retrieved results. To change the volume, all instrument parts are separated from the input sound mixture using the corresponding standard MIDI file. For the separation, we used an integrated tone (timbre) model consisting of harmonic and inharmonic models that are initialized with template sounds recorded from a MIDI sound generator. The remaining but critical problem here is to deal with various performance styles and instrument bodies that are not given in the template sounds. To solve this problem, we train probabilistic distributions of timbre features by using various sounds. By adding a new constraint of maximizing the likelihood of timbre features extracted from each tone model, we succeeded in estimating model parameters that better express actual timbre.",JPN,education,Developed economies,"[-8.661194, -3.8553839]","[-35.49515, -24.64562]","[-6.420464, -2.1190367, -20.963291]","[-9.093835, -0.77572984, -6.8286614]","[9.4437, 9.257379]","[8.111917, 6.0553126]","[12.829581, 14.370152, -1.4829822]","[9.937775, 5.890141, 9.92934]"
16,David Little;Bryan Pardo,Learning Musical Instruments from Mixtures of Audio with Weak Labels.,2008,https://doi.org/10.5281/zenodo.1417815,David Little+Northwestern University>USA>education|Unknown>Unknown>Unknown;Bryan Pardo+Northwestern University>USA>education|Unknown>Unknown>Unknown,"""We are interested in developing a system that learns to recognize individual sound sources in an auditory scene where multiple sources may be occurring simultaneously. We focus here on sound source recognition in music audio mixtures. Many researchers have made progress by using isolated training examples or very strongly labeled training data. We consider an alternative approach: the learner is presented with a variety of weakly-labeled mixtures. Positive examples include the target instrument at some point in a mixture of sounds, and negative examples are mixtures that do not contain the target. We show that it not only possible to learn from weakly-labeled mixtures of instruments, but that it works significantly better (78% correct labeling compared to 55%) than learning from isolated examples when the task is identification of an instrument in novel mixtures.""",USA,education,Developed economies,"[6.524326, -24.801659]","[-36.922432, -26.976742]","[20.152945, -2.4405563, -1.3303686]","[-9.70312, -1.7908527, -29.776459]","[8.90271, 7.691787]","[6.8541055, 5.349155]","[11.528781, 12.675355, 0.75119317]","[9.77469, 8.2135315, 9.429377]"
15,Ling Feng;Andreas Brinch Nielsen;Lars Kai Hansen,Vocal Segment Classification in Popular Music.,2008,https://doi.org/10.5281/zenodo.1416932,Ling Feng+Technical University of Denmark>DNK>education;Andreas Brinch Nielsen+Technical University of Denmark>DNK>education;Lars Kai Hansen+Technical University of Denmark>DNK>education,"This paper explores the vocal and non-vocal music classification problem within popular songs. A newly built labeled database covering 147 popular songs is announced. It is designed for classifying signals from 1sec time windows. Features are selected for this particular task, in order to capture both the temporal correlations and the dependencies among the feature dimensions. We systematically study the performance of a set of classifiers, including linear regression, generalized linear model, Gaussian mixture model, reduced kernel orthonormalized partial least squares and K-means on cross-validated training and test setup. The database is divided in two different ways: with/without artist overlap between training and test sets, so as to study the so called ‘artist effect’. The performance and results are analyzed in depth: from error rates to sample-to-sample error correlation. A voting scheme is proposed to enhance the performance under certain conditions.",DNK,education,Developed economies,"[-10.786289, -33.350143]","[15.937117, -14.508516]","[16.4863, 15.848125, -14.273471]","[14.532887, 3.0976944, -9.07351]","[10.200066, 11.180946]","[9.188104, 3.6360557]","[11.478236, 15.149252, 0.74443483]","[11.023284, 7.60155, 10.357704]"
27,Kazumasa Murata;Kazuhiro Nakadai;Kazuyoshi Yoshii;Ryu Takeda;Toyotaka Torii;Hiroshi G. Okuno;Yuji Hasegawa;Hiroshi Tsujino,A Robot Singer with Music Recognition Based on Real-Time Beat Tracking.,2008,https://doi.org/10.5281/zenodo.1415108,"Kazumasa Murata+Tokyo Institute of Technology>JPN>education;Kazuhiro Nakadai+Honda Research Institute Japan Co., Ltd.>JPN>company;Kazuyoshi Yoshii+Kyoto University>JPN>education;Ryu Takeda+Kyoto University>JPN>education;Toyotaka Torii+Honda Research Institute Japan Co., Ltd.>JPN>company;Hiroshi G. Okuno+Kyoto University>JPN>education;Yuji Hasegawa+Honda Research Institute Japan Co., Ltd.>JPN>company;Hiroshi Tsujino+Honda Research Institute Japan Co., Ltd.>JPN>company","A robot that can provide an active and enjoyable user interface is one of the most challenging applications for music information processing, because the robot should cope with high-power noises including self voices and motor noises. This paper proposes noise-robust musical beat tracking by using a robot-embedded microphone, and describes its application to a robot singer with music recognition. The proposed beat tracking introduces two key techniques, that is, spectro-temporal pattern matching and echo cancellation. The former realizes robust tempo estimation with a shorter window length, thus, it can quickly adapt to tempo changes. The latter is effective to cancel self periodic noises such as stepping, scatting, and singing. We constructed a robot singer based on the proposed beat tracking for Honda ASIMO. The robot detects a musical beat with its own microphone in a noisy environment. It tries to recognize music based on the detected musical beat. When it successfully recognizes music, it sings while stepping according to the beat. Otherwise, it performs scatting instead of singing because the lyrics are unavailable. Experimental results showed fast adaptation to tempo changes and high robustness in beat tracking even when stepping, scatting and singing.",JPN,education,Developed economies,"[31.172255, -35.70122]","[-29.01498, -3.5617733]","[12.328044, -33.52301, -7.6418767]","[-8.100757, 6.5203595, -10.252144]","[10.336578, 4.2657895]","[5.3018017, 1.9047889]","[10.107209, 12.912769, -2.13237]","[7.5553885, 6.766575, 10.784951]"
14,Wei-Ho Tsai;Shih-Jie Liao;Catherine Lai,Automatic Identification of Simultaneous Singers in Duet Recordings.,2008,https://doi.org/10.5281/zenodo.1416358,Wei-Ho Tsai+National Taipei University of Technology>TWN>education;Shih-Jie Liao+National Taipei University of Technology>TWN>education;Catherine Lai+Open Text Corporation>CAN>company,"The problem of identifying singers in music recordings has received considerable attention with the explosive growth of the Internet and digital media. Although a number of studies on automatic singer identification from acoustic features have been reported, most systems to date, however, reliably establish the identity of singers in solo recordings only. The research presented in this paper attempts to automatically identify singers in music recordings that contain overlapping singing voices. Two approaches to overlapping singer identification are proposed and evaluated. Results obtained demonstrate the feasibility of the systems.",TWN,education,Developing economies,"[-9.396628, -35.16292]","[11.846563, -20.936563]","[17.581644, 12.435864, -16.073664]","[11.439637, -3.508295, -16.224928]","[10.111351, 11.240016]","[8.464179, 3.6069329]","[11.288913, 15.382287, 0.75627357]","[10.592422, 7.9645944, 10.072999]"
12,Anja Volk;Peter van Kranenburg;Jörg Garbers;Frans Wiering;Remco C. Veltkamp;Louis P. Grijp,A Manual Annotation Method for Melodic Similarity and the Study of Melody Feature Sets.,2008,https://doi.org/10.5281/zenodo.1416428,Anja Volk+Utrecht University>NLD>education;Peter van Kranenburg+Utrecht University>NLD>education;Jörg Garbers+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education|Meertens Institute>NLD>facility;Louis P. Grijp+Meertens Institute>NLD>facility,This paper describes both a newly developed method for manual annotation for aspects of melodic similarity and its use for evaluating melody features concerning their contribution to perceived similarity. The second issue is also addressed with a computational evaluation method. These approaches are applied to a corpus of folk song melodies. We show that classification of melodies could not be based on single features and that the feature sets from the literature are not sufficient to classify melodies into groups of related melodies. The manual annotations enable us to evaluate various models for melodic similarity.,NLD,education,Developed economies,"[1.9145442, 16.773703]","[16.189964, 3.349495]","[3.402293, 7.056482, -0.05880012]","[6.7168374, 2.3273816, 4.3356023]","[12.288964, 9.776664]","[9.442573, 1.955909]","[12.768106, 15.43123, -0.7218516]","[11.28528, 7.055395, 13.068223]"
11,Michael Skalak;Jinyu Han;Bryan Pardo,Speeding Melody Search With Vantage Point Trees.,2008,https://doi.org/10.5281/zenodo.1415714,Michael Skalak+Northwestern University>USA>education;Jinyu Han+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"Melodic search engines let people find music in online collections by specifying the desired melody. Comparing the query melody to every item in a large database is prohibitively slow. If melodies can be placed in a metric space, search can be sped by comparing the query to a limited number of vantage melodies, rather than the entire database. We describe a simple melody metric that is customizable using a small number of example queries. This metric allows use of a generalized vantage point tree to organize the database. We show on a standard melodic database that the general vantage tree approach achieves superior search results for query-by-humming compared to an existing vantage point tree method. We then show this method can be used as a preprocessor to speed search for non-metric melodic comparison.",USA,education,Developed economies,"[4.2123175, 37.86303]","[13.056703, 15.025777]","[1.7860272, 7.8943286, -17.051552]","[8.272552, -8.378672, 14.185949]","[11.253337, 9.860354]","[9.207368, 0.6468548]","[12.102102, 15.644111, -0.9515737]","[10.936773, 5.936014, 13.092621]"
10,Marcus Pearce;Daniel Müllensiefen;Geraint A. Wiggins,A Comparison of Statistical and Rule-Based Models of Melodic Segmentation.,2008,https://doi.org/10.5281/zenodo.1417115,"M. T. Pearce+Goldsmiths, University of London>GBR>education;D. Müllensiefen+Goldsmiths, University of London>GBR>education;G. A. Wiggins+Goldsmiths, University of London>GBR>education","""We introduce a new model for melodic segmentation based on information-dynamic analysis of melodic structure. The performance of the model is compared to several existing algorithms in predicting the annotated phrase boundaries in a large corpus of folk music.""",GBR,education,Developed economies,"[4.4306545, -7.7069373]","[0.79368836, 5.32828]","[10.28967, 8.301703, -5.454787]","[-4.706635, -4.574161, 0.23068143]","[10.732089, 9.843936]","[8.252854, 2.5956948]","[11.754669, 15.111298, -0.6047984]","[10.073607, 7.249147, 11.6711855]"
9,Dimitrios Rafailidis;Alexandros Nanopoulos;Yannis Manolopoulos;Emilios Cambouropoulos,Detection of Stream Segments in Symbolic Musical Data.,2008,https://doi.org/10.5281/zenodo.1416002,Dimitris Rafailidis+Aristotle University of Thessaloniki>GRC>education;Alexandros Nanopoulos+Aristotle University of Thessaloniki>GRC>education;Emilios Cambouropoulos+Aristotle University of Thessaloniki>GRC>education;Yannis Manolopoulos+Aristotle University of Thessaloniki>GRC>education,"A listener is thought to be able to organise musical notes into groups within musical streams/voices. A stream segment is a relatively short coherent sequence of tones that is separated horizontally from co-sounding streams and, vertically from neighbouring musical sequences. This paper presents a novel algorithm that discovers musical stream segments in symbolic musical data. The proposed algorithm makes use of a single set of fundamental auditory principles for the concurrent horizontal and vertical segregation of a given musical texture into stream segments. The algorithm is tested against a small manually-annotated dataset of musical excerpts, and results are analysed; it is shown that the technique is promising.",GRC,education,Developed economies,"[16.637245, 16.681807]","[-6.6138077, 20.95491]","[-2.9781837, -9.98027, 16.036573]","[-10.139613, -10.280394, 7.346822]","[11.837061, 6.9755306]","[8.602931, 2.413722]","[13.1972, 12.596643, -1.0399573]","[10.265204, 6.49495, 11.144982]"
58,Kyogu Lee;Markus Cremer,Segmentation-Based Lyrics-Audio Alignment using Dynamic Programming.,2008,https://doi.org/10.5281/zenodo.1416934,Kyogu Lee+Gracenote>USA>company|Media Technology Lab>USA>company;Markus Cremer+Gracenote>USA>company|Media Technology Lab>USA>company,"In this paper, we present a system for automatic alignment of textual lyrics with musical audio. Given an input audio signal, structural segmentation is first performed and similar segments are assigned a label by computing the distance between the segment pairs. Using the results of segmentation and hand-labeled paragraphs in lyrics as a pair of input strings, we apply a dynamic programming (DP) algorithm to find the best alignment path between the two strings, achieving segment-to-paragraph synchronization. We demonstrate that the proposed algorithm performs well for various kinds of musical audio.",USA,company,Developed economies,"[-27.734034, -33.94731]","[-14.997196, -13.176642]","[10.463875, 20.432076, -2.4396386]","[-1.3512776, -18.413351, -3.2573311]","[11.187891, 11.775055]","[6.2813497, 0.8139742]","[12.16224, 15.782244, 1.1086423]","[8.254867, 5.9386067, 10.878563]"
0,Ricardo Scholz;Geber Ramalho,COCHONUT: Recognizing Complex Chords from MIDI Guitar Sequences.,2008,https://doi.org/10.5281/zenodo.1416986,Ricardo Scholz+Federal University of Pernambuco>BRA>education;Geber Ramalho+Federal University of Pernambuco>BRA>education,"""Chord recognition from symbolic data is a complex task, due to its strong context dependency and the large number of possible combinations of the intervals which the chords are made of, specially when dealing with dissonances, such as 7ths, 9ths, 13ths and suspended chords. None of the current approaches deal with such complexity. Most of them consider only simple chord patterns, in the best cases, including sevenths. In addition, when considering symbolic data captured from a MIDI guitar, we need to deal with non quantized and noisy data, which increases the difficulty of the task. The current symbolic approaches deal only with quantized data, with no automatic technique to reduce noise. This paper proposes a new approach to recognize chords, from symbolic MIDI guitar data, called COCHONUT (Complex Chords Nutting). The system uses contextual harmonic information to solve ambiguous cases, integrated with other techniques, such as decision theory, optimization, pattern matching and rule-based recognition. The results are encouraging and provide strong indications that the use of harmonic contextual information, integrated with other techniques, can actually improve the results currently found in literature.""",BRA,education,Developing economies,"[50.943604, -6.9671097]","[-28.1832, 18.188334]","[24.917196, -10.495937, 11.753325]","[-24.76688, -9.86094, 2.1970532]","[7.2976604, 8.444667]","[6.3957343, 3.5344262]","[11.950634, 10.791466, 1.6963899]","[9.731831, 8.717273, 12.188734]"
1,Xinglin Zhang;David Gerhard,Chord Recognition using Instrument Voicing Constraints.,2008,https://doi.org/10.5281/zenodo.1414826,Xinglin Zhang+University of Regina>CAN>education;David Gerhard+University of Regina>CAN>education,"This paper presents a technique of disambiguation for chord recognition based on a-priori knowledge of probabilities of chord voicings in the specific musical medium. The main motivating example is guitar chord recognition, where the physical layout and structure of the instrument, along with human physical and temporal constraints, make certain chord voicings and chord sequences more likely than others. Pitch classes are first extracted using the Pitch Class Profile (PCP) technique, and chords are then recognized using Artificial Neural Networks. The chord information is then analyzed using an array of voicing vectors (VV) indicating likelihood for chord voicings based on constraints of the instrument. Chord sequence analysis is used to reinforce accuracy of individual chord estimations. The specific notes of the chord are then inferred by combining the chord information and the best estimated voicing of the chord.",CAN,education,Developed economies,"[53.745632, -6.651813]","[-34.3596, 18.707355]","[26.524132, -10.897234, 14.210054]","[-27.421589, -7.89587, -0.03497516]","[6.8928814, 8.570444]","[6.1710243, 3.6717467]","[11.953805, 10.420608, 2.0499513]","[9.727282, 8.918894, 12.204217]"
2,Kouhei Sumi;Katsutoshi Itoyama;Kazuyoshi Yoshii;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Automatic Chord Recognition Based on Probabilistic Integration of Chord Transition and Bass Pitch Estimation.,2008,https://doi.org/10.5281/zenodo.1417221,Kouhei Sumi+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Katsutoshi Itoyama+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazunori Komatani+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Tetsuya Ogata+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroshi G. Okuno+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a method that identifies musical chords in polyphonic musical signals. As musical chords mainly represent the harmony of music and are related to other musical elements such as melody and rhythm, the performance of chord recognition should improve if this interrelationship is taken into consideration. Nevertheless, this interrelationship has not been utilized in the literature as far as the authors are aware. In this paper, bass lines are utilized as clues for improving chord recognition because they can be regarded as an element of the melody. A probabilistic framework is devised to uniformly integrate bass lines extracted by using bass pitch estimation into a hypothesis-search-based chord recognition. To prune the hypothesis space of the search, the hypothesis reliability is defined as the weighted sum of three reliabilities: the likelihood of Gaussian Mixture Models for the observed features, the joint probability of chord and bass pitch, and the chord transition N-gram probability. Experimental results show that our method recognized the chord sequences of 150 songs in twelve Beatles albums; the average frame-rate accuracy of the results was 73.4%.",JPN,education,Developed economies,"[54.559734, -6.11755]","[-29.34368, 17.895115]","[28.789211, -12.575421, 16.07802]","[-26.719408, -8.701859, 2.2091234]","[6.8250175, 8.546996]","[6.3912983, 3.5186126]","[11.862932, 10.472249, 2.0300162]","[9.75481, 8.6208, 12.034193]"
3,Matthias Mauch;Simon Dixon,A Discrete Mixture Model for Chord Labelling.,2008,https://doi.org/10.5281/zenodo.1416982,"Matthias Mauch+Queen Mary, University of London>GBR>education;Simon Dixon+Queen Mary, University of London>GBR>education","Chord labels for recorded audio are in high demand both as an end product used by musicologists and hobby musicians and as an input feature for music similarity applications. Many past algorithms for chord labelling are based on chromagrams, but distribution of energy in chroma frames is not well understood. Furthermore, non-chord notes complicate chord estimation. We present a new approach which uses as a basis a relatively simple chroma model to represent short-time sonorities derived from melody range and bass range chromagrams. A chord is then modelled as a mixture of these sonorities, or subchords. We prove the practicability of the model by implementing a hidden Markov model (HMM) for chord labelling, in which we use the discrete subchord features as observations. We model gamma-distributed chord durations by duplicate states in the HMM, a technique that had not been applied to chord labelling. We test the algorithm by five-fold cross-validation on a set of 175 hand-labelled songs performed by the Beatles. Accuracy figures compare very well with other state of the art approaches. We include accuracy specified by chord type as well as a measure of temporal coherence.",GBR,education,Developed economies,"[53.62845, -3.2203155]","[-31.29315, 18.542435]","[25.975212, -16.890203, 13.6605015]","[-22.919168, -4.5205555, 1.8499618]","[6.850786, 8.643918]","[6.329665, 3.481527]","[11.972035, 10.409178, 2.0239336]","[9.779526, 8.644574, 12.073351]"
4,W. Bas de Haas;Remco C. Veltkamp;Frans Wiering,Tonal Pitch Step Distance: a Similarity Measure for Chord Progressions.,2008,https://doi.org/10.5281/zenodo.1418069,W. Bas de Haas+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education,"The computational analysis of musical harmony has received a lot of attention the last decades. Although it is widely recognized that extracting symbolic chord labels from music yields useful abstractions, and the number of chord labeling algorithms for symbolic and audio data is steadily growing, surprisingly little effort has been put into comparing sequences of chord labels. This study presents and tests a new distance function that measures the difference between chord progressions. The presented distance function is based on Lerdahl’s Tonal Pitch Space. It compares the harmonic changes of two sequences of chord labels over time. This distance, named the Tonal Pitch Step Distance (TPSD), is shown to be effective for retrieving similar jazz standards found in the Real Book. The TPSD matches the human intuitions about harmonic similarity which is demonstrated on a set of blues variations.",NLD,education,Developed economies,"[58.635292, -0.24963716]","[-23.156404, 18.334799]","[28.86455, -19.04754, 17.651184]","[-22.037947, -4.1575093, 7.7856708]","[7.102664, 8.780782]","[7.645106, 2.406206]","[11.803204, 10.666205, 2.0720806]","[10.051161, 8.116689, 12.516642]"
5,Ching-Hua Chuan;Elaine Chew,Evaluating and Visualizing Effectiveness of Style Emulation in Musical Accompaniment.,2008,https://doi.org/10.5281/zenodo.1417095,Ching-Hua Chuan+University of Southern California>USA>education|Radcliffe Institute for Advanced Study at Harvard University>USA>facility;Elaine Chew+University of Southern California>USA>education|Radcliffe Institute for Advanced Study at Harvard University>USA>facility,"We propose general quantitative methods for evaluating and visualizing the results of machine-generated style-specific accompaniment. The evaluation of automated accompaniment systems, and the degree to which they emulate a style, has been based primarily on subjective opinion. To quantify style similarity between machine-generated and original accompaniments, we propose two types of measures: one based on transformations in the neo-Riemannian chord space, and another based on the distribution of melody-chord intervals. The first set of experiments demonstrate the methods on an automatic style-specific accompaniment (ASSA) system. They test the effect of training data choice on style emulation effectiveness, and challenge the assumption that more data is better. The second set of experiments compare the output of the ASSA system with those of a rule-based system, and random chord generator. While the examples focus primarily on machine emulation of Pop/Rock accompaniment, the methods generalize to music of other genres.",USA,education,Developed economies,"[-10.6726265, 5.222884]","[-0.24002348, -39.490463]","[3.9020734, 12.238241, 26.478464]","[-23.396004, 5.470648, 0.19213162]","[10.689355, 8.568453]","[9.53681, 6.1282754]","[13.593724, 12.263529, -0.28847393]","[9.888925, 5.2148166, 9.476777]"
6,Amelie Anglade;Simon Dixon,Characterisation of Harmony With Inductive Logic Programming.,2008,https://doi.org/10.5281/zenodo.1416550,Amélie Anglade+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"We present an approach for the automatic characterisation of the harmony of song sets making use of relational induction of logical rules. We analyse manually annotated chord data available in RDF and interlinked with web identifiers for chords which themselves give access to the root, bass, component intervals of the chords. We pre-process these data to obtain high-level information such as chord category, degree and intervals between chords before passing them to an Inductive Logic Programming software which extracts the harmony rules underlying them. This framework is tested over the Beatles songs and the Real Book songs. It generates a total over several experiments of 12,450 harmony rules characterising and differentiating the Real Book (jazz) songs and the Beatles’ (pop) music. Encouragingly, a preliminary analysis of the most common rules reveals a list of well-known pop and jazz patterns that could be completed by a more in depth analysis of the other rules.",GBR,education,Developed economies,"[26.575861, 25.252565]","[-23.273956, 21.55108]","[-1.6905382, -9.230247, 35.68666]","[-21.925508, 0.8056276, 6.2908545]","[9.8703985, 9.258994]","[7.1896996, 3.1988795]","[11.776782, 14.031962, -0.9047963]","[9.766927, 8.155387, 12.355306]"
13,Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo,Melody Expectation Method Based on GTTM and TPS.,2008,https://doi.org/10.5281/zenodo.1416822,Masatoshi Hamanaka+University of Tsukuba>JPN>education;Keiji Hirata+NTT Communication Science Laboratories>JPN>company;Satoshi Tojo+Japan Advanced Institute of Science and Technology>JPN>education,"A method that predicts the next notes is described for assisting musical novices to play improvisations. Melody prediction is one of the most difficult problems in musical information retrieval because composers and players may or may not create melodies that conform to our expectation. The development of a melody expectation method is thus important for building a system that supports musical novices because melody expectation is one of the most basic skills for a musician. Unlike most previous prediction methods, which use statistical learning, our method evaluates the appropriateness of each candidate note from the view point of musical theory. In particular, it uses the concept of melody stability based on the generative theory of tonal music (GTTM) and the tonal pitch space (TPS) to evaluate the appropriateness of the melody. It can thus predict the candidate next notes not only from the surface structure of the melody but also from the deeper structure of the melody acquired by GTTM and TPS analysis. Experimental results showed that the method can evaluate the appropriateness of the melody sufficiently well.",JPN,education,Developed economies,"[3.6870837, -10.273375]","[7.986876, -31.43863]","[13.279678, 10.960776, -3.351387]","[-6.616937, -12.549271, 1.6402771]","[10.272086, 9.8912115]","[7.2231026, 2.9755163]","[11.261872, 14.997422, -0.58251745]","[9.737875, 7.638194, 12.139494]"
28,Martin Gasser;Arthur Flexer;Gerhard Widmer,Streamcatcher: Integrated Visualization of Music Clips and Online Audio Streams.,2008,https://doi.org/10.5281/zenodo.1416362,Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education,"We propose a content-based approach to explorative visualization of online audio streams (e.g., web radio streams). The visualization space is defined by prototypical instances of musical concepts taken from personal music collections. Our system shows the relation of prototypes to each other and generates an animated visualization that places representations of audio streams in the vicinity of their most similar prototypes. Both computation of music similarity and visualization are formulated for online real time performance. A software implementation of these ideas is presented and evaluated.",AUT,facility,Developed economies,"[-13.328479, 33.34465]","[26.226665, 24.566809]","[-11.263467, 11.275911, -24.190657]","[9.00196, -7.3369656, 24.358868]","[13.635343, 6.918897]","[10.961511, 1.368891]","[13.935796, 13.575062, -2.3185065]","[12.036586, 5.425591, 12.9543295]"
64,Joachim Ganseman;Paul Scheunders;Wim D'haes,Using XQuery on MusicXML Databases for Musicological Analysis.,2008,https://doi.org/10.5281/zenodo.1414836,Joachim Ganseman+IBBT - Visionlab>BEL>facility|University of Antwerp>BEL>education;Paul Scheunders+IBBT - Visionlab>BEL>facility|University of Antwerp>BEL>education;Wim D’haes+Mu Technologies NV>BEL>company,"MusicXML is a fairly recent XML-based file format for music scores, now supported by many score and audio editing software applications. Several online score library projects exist or are emerging, some of them using MusicXML as main format. When storing a large set of XML-encoded scores in an XML database, XQuery can be used to retrieve information from this database. We present some small practical examples of such large scale analysis, using the Wikifonia lead sheet database and the eXist XQuery engine. This shows the feasibility of automated musicological analysis on digital score libraries using the latest software tools. Bottom line: it’s easy.",BEL,facility,Developed economies,"[-2.611548, 31.052942]","[6.315419, 39.811012]","[-14.208379, 3.4100525, -6.7230773]","[-6.0604863, -9.4585705, 21.320465]","[14.095922, 7.438264]","[10.2325, 0.25899383]","[14.111736, 14.527493, -2.2768466]","[10.848571, 4.935904, 12.10381]"
30,Panagiotis Symeonidis;Maria M. Ruxanda;Alexandros Nanopoulos;Yannis Manolopoulos,Ternary Semantic Analysis of Social Tags for Personalized Music Recommendation.,2008,https://doi.org/10.5281/zenodo.1416672,Panagiotis Symeonidis+Aristotle University of Thessaloniki>GRC>education|Aalborg University>DNK>education;Maria Ruxanda+Aalborg University>DNK>education;Alexandros Nanopoulos+Aristotle University of Thessaloniki>GRC>education;Yannis Manolopoulos+Aristotle University of Thessaloniki>GRC>education,"""Social tagging is the process by which many users add metadata in the form of keywords, to annotate information items. In case of music, the annotated items can be songs, artists, albums. Current music recommenders which employ social tagging to improve the music recommendation, fail to always provide appropriate item recommendations, because: (i) users may have different interests for a musical item, and (ii) musical items may have multiple facets. In this paper, we propose an approach that tackles the problem of the multimodal use of music. We develop a unified framework, represented by a 3-order tensor, to model altogether users, tags, and items. Then, we recommend musical items according to users multimodal perception of music, by performing latent semantic analysis and dimensionality reduction using the Higher Order Singular Value Decomposition technique. We experimentally evaluate the proposed method against two state-of-the-art recommendations algorithms using real Last.fm data. Our results show significant improvements in terms of effectiveness measured through recall/precision.""",GRC,education,Developed economies,"[-43.484306, 2.253235]","[42.769722, 4.6039624]","[-17.97385, 15.416666, 4.327287]","[20.901604, 8.583556, 11.9085245]","[14.444457, 10.269582]","[12.437398, 2.7129788]","[15.632813, 14.356278, -0.039917216]","[13.523985, 5.7189617, 11.999481]"
29,Kazuyoshi Yoshii;Masataka Goto,Music Thumbnailer: Visualizing Musical Pieces in Thumbnail Images Based on Acoustic Features.,2008,https://doi.org/10.5281/zenodo.1415936,Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a principled method called MusicThumbnailer to transform musical pieces into visual thumbnail images based on acoustic features extracted from their audio signals. These thumbnails can help users immediately guess the musical contents of audio signals without trial listening. This method is consistent in ways that optimize thumbnails according to the characteristics of a target music collection. This means the appropriateness of transformation should be defined to eliminate ad hoc transformation rules. In this paper, we introduce three top-down criteria to improve memorability of thumbnails (generate gradations), deliver information more completely, and distinguish thumbnails more clearly. These criteria are mathematically implemented as minimization of brightness differences of adjacent pixels and maximization of brightness variances within and between thumbnails. The optimized parameters of a modified linear mapping model we assumed are obtained by minimizing a unified cost function based on the three criteria with a steepest descent method. Experimental results indicate that generated thumbnails can provide users with useful hints as to the musical contents of musical pieces.",JPN,facility,Developed economies,"[-12.146496, 32.084084]","[32.08255, 25.313553]","[-9.519936, 12.785767, -23.18467]","[4.495705, 10.219395, 9.820362]","[13.543203, 7.0438476]","[11.138942, 1.5792266]","[13.788963, 13.60177, -2.2196963]","[12.304073, 5.342052, 12.68187]"
48,Shyamala Doraisamy;Shahram Golzari;Noris Mohd. Norowi;Md Nasir Sulaiman;Nur Izura Udzir,A Study on Feature Selection and Classification Techniques for Automatic Genre Classification of Traditional Malay Music.,2008,https://doi.org/10.5281/zenodo.1415124,Shyamala Doraisamy+University Putra Malaysia>MYS>education;Shahram Golzari+University Putra Malaysia>MYS>education;Noris Mohd. Norowi+University Putra Malaysia>MYS>education;Md. Nasir B Sulaiman+University Putra Malaysia>MYS>education;Nur Izura Udzir+University Putra Malaysia>MYS>education,"Machine learning techniques for automated musical genre classification is currently widely studied. With large collections of digital musical files, one approach to classification is to classify by musical genres such as pop, rock and classical in Western music. Beat, pitch and temporal related features are extracted from audio signals and various machine learning algorithms are applied for classification. Features that resulted in better classification accuracies for Traditional Malay Music (TMM), in comparison to western music, in a previous study were beat related features. However, only the J48 classifier was used and in this study we perform a more comprehensive investigation on improving the classification of TMM. In addition, feature selection was performed for dimensionality reduction. Classification accuracies using classifiers of varying paradigms on a dataset comprising ten TMM genres were obtained. Results identify potentially useful classifiers and show the impact of adding a feature selection phase for TMM genre classification.",MYS,education,Developing economies,"[-27.66829, -14.356783]","[20.236553, -6.288548]","[-15.881984, 0.8761532, 15.813959]","[10.701655, 11.4644, -3.1756136]","[12.834952, 10.7890005]","[9.553985, 3.3193696]","[13.873557, 14.187371, 1.3638543]","[11.453007, 7.1246767, 10.893025]"
47,Konstantinos Trohidis;Grigorios Tsoumakas;George Kalliris;Ioannis P. Vlahavas,Multi-Label Classification of Music into Emotions.,2008,https://doi.org/10.5281/zenodo.1414900,Konstantinos Trohidis+Aristotle University of Thessaloniki>GRC>education;Grigorios Tsoumakas+Aristotle University of Thessaloniki>GRC>education;George Kalliris+Aristotle University of Thessaloniki>GRC>education;Ioannis Vlahavas+Aristotle University of Thessaloniki>GRC>education,"In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features.",GRC,education,Developed economies,"[-56.27211, -0.42176348]","[53.058876, -8.157571]","[-23.525389, 25.23457, 8.425327]","[9.667167, 25.660723, 8.062336]","[13.854434, 12.678252]","[13.071784, 4.0932837]","[15.995283, 14.576589, 1.5976242]","[14.294506, 5.164217, 10.357478]"
46,Yuxiang Liu;Ye Wang;Arun Shenoy;Wei-Ho Tsai;Lianhong Cai,Clustering Music Recordings by Their Keys.,2008,https://doi.org/10.5281/zenodo.1417375,Yuxiang Liu+Tsinghua University>CHN>education|National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education;Arun Shenoy+Unknown>Unknown>Unknown;Wei-Ho Tsai+National Taipei University of Technology>TWN>education;Lianhong Cai+Tsinghua University>CHN>education,"Music key, a high level feature of musical audio, is an effective tool for structural analysis of musical works. This paper presents a novel unsupervised approach for clustering music recordings by their keys. Based on chroma-based features extracted from acoustic signals, an inter-recording distance metric which characterizes diversity of pitch distribution together with harmonic center of music pieces, is introduced to measure dissimilarities among musical features. Then, recordings are divided into categories via unsupervised clustering, where the best number of clusters can be determined automatically by minimizing estimated Rand Index. Any existing technique for key detection can then be employed to identify key assignment for each cluster. Empirical evaluation on a dataset of 91 pop songs illustrates an average cluster purity of 57.3% and a Rand Index of close to 50%, thus highlighting the possibility of integration with existing key identification techniques to improve accuracy, based on strong cross-correlation data available from this framework for input dataset.",CHN,education,Developing economies,"[-6.759208, 2.4959693]","[-4.097322, -2.0303686]","[-0.7410736, -2.9867766, -6.713955]","[5.4515347, -2.3143632, -4.987646]","[11.968932, 7.9559712]","[7.7396245, 2.876521]","[12.458351, 13.601393, -0.09452911]","[10.42023, 8.106338, 11.666915]"
44,Mark Godfrey;Parag Chordia,Hubs and Homogeneity: Improving Content-Based Music Modeling.,2008,https://doi.org/10.5281/zenodo.1415154,Mark T. Godfrey+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education;Parag Chordia+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education,"We explore the origins of hubs in timbre-based song modeling in the context of content-based music recommendation and propose several remedies. Specifically, we find that a process of model homogenization, in which certain components of a mixture model are systematically removed, improves performance as measured against several ground-truth similarity metrics. Extending the work of Aucouturier, we introduce several new methods of homogenization. On a subset of the uspop data set, model homogenization improves artist R-precision by a maximum of 3.5% and agreement to user collection co-occurrence data by 7.4%. We also explore differences in the effectiveness of the various homogenization methods for hub reduction. Further, we extend the modeling of frame-based MFCC features by using a kernel density estimation approach to non-parametric modeling. We find that such an approach significantly reduces the number of hubs (by 2.6% of the dataset) while improving agreement to ground-truth by 5% and slightly improving artist R-precision as compared with the standard parametric model.",USA,education,Developed economies,"[-14.490593, 13.276002]","[30.10182, 5.0219035]","[-11.484664, 14.074736, -0.57667583]","[22.170135, -0.6232674, 5.3715544]","[13.724706, 8.878387]","[10.998386, 2.5681088]","[14.373351, 14.731989, -1.1067815]","[12.551932, 6.5133843, 12.3479]"
49,Rudolf Mayer;Robert Neumayer;Andreas Rauber,Rhyme and Style Features for Musical Genre Classification by Song Lyrics.,2008,https://doi.org/10.5281/zenodo.1416758,Rudolf Mayer+Vienna University of Technology>AUT>education;Robert Neumayer+Vienna University of Technology>AUT>education|Norwegian University of Science and Technology>NOR>education;Andreas Rauber+Vienna University of Technology>AUT>education,"How individuals perceive music is influenced by many different factors. The audible part of a piece of music, its sound, does for sure contribute, but is only one aspect to be taken into account. Cultural information influences how we experience music, as does the songs’ text and its sound. Next to symbolic and audio based music information retrieval, which focus on the sound of music, song lyrics, may thus be used to improve classification or similarity ranking of music. Song lyrics exhibit specific properties different from traditional text documents – many lyrics are for example composed in rhyming verses, and may have different frequencies for certain parts-of-speech when compared to other text documents. Further, lyrics may use ‘slang’ language or differ greatly in the length and complexity of the language used, which can be measured by some statistical features such as word / verse length, and the amount of repetative text. In this paper, we present a novel set of features developed for textual analysis of song lyrics, and combine them with and compare them to classical bag-of-words indexing approaches. We present results for musical genre classification on a test collection in order to demonstrate our analysis.",AUT,education,Developed economies,"[-31.822605, -18.27916]","[23.42636, -3.0894585]","[-13.094413, 7.976407, 17.164072]","[8.314754, 8.677351, 1.453608]","[12.801384, 11.1585455]","[9.657734, 2.5107043]","[14.031159, 14.513112, 1.3787833]","[11.317433, 6.653667, 11.686788]"
50,Bill Z. Manaris;Dwight Krehbiel;Patrick Roos;Thomas Zalonis,Armonique: Experiments in Content-Based Similarity Retrieval using Power-Law Melodic and Timbre Metrics.,2008,https://doi.org/10.5281/zenodo.1416778,Bill Manaris+College of Charleston>USA>education;Dwight Krehbiel+Bethel College>USA>education;Patrick Roos+College of Charleston>USA>education;Thomas Zalonis+College of Charleston>USA>education,"This paper presents results from an on-going MIR study utilizing hundreds of melodic and timbre features based on power laws for content-based similarity retrieval. These metrics are incorporated into a music search engine prototype, called Armonique. This prototype is used with a corpus of 9153 songs encoded in both MIDI and MP3 to identify pieces similar to and dissimilar from selected songs. The MIDI format is used to extract various power-law features measuring proportions of music-theoretic and other attributes, such as pitch, duration, melodic intervals, and chords. The MP3 format is used to extract power-law features measuring proportions within FFT power spectra related to timbre. Several assessment experiments have been conducted to evaluate the effectiveness of the similarity model. The results suggest that power-law metrics are very promising for content-based music querying and retrieval, as they seem to correlate with aspects of human emotion and aesthetics.",USA,education,Developed economies,"[-1.4877223, 17.61797]","[18.093271, 6.902091]","[-0.6870837, 7.9322534, -1.7138643]","[8.913711, 0.5214065, 7.7445316]","[12.517228, 9.651521]","[9.817823, 1.8131182]","[12.911639, 15.41952, -0.8472611]","[11.4434, 6.599222, 12.602793]"
51,Matthew D. Hoffman;David M. Blei;Perry R. Cook,Content-Based Musical Similarity Computation using the Hierarchical Dirichlet Process.,2008,https://doi.org/10.5281/zenodo.1417223,Matthew Hoffman+Princeton University>USA>education;David Blei+Princeton University>USA>education;Perry Cook+Princeton University>USA>education,"We develop a method for discovering the latent structure in MFCC feature data using the Hierarchical Dirichlet Process (HDP). Based on this structure, we compute timbral similarity between recorded songs. The HDP is a nonparametric Bayesian model. Like the Gaussian Mixture Model (GMM), it represents each song as a mixture of some number of multivariate Gaussian distributions However, the number of mixture components is not fixed in the HDP, but is determined as part of the posterior inference process. Moreover, in the HDP the same set of Gaussians is used to model all songs, with only the mixture weights varying from song to song. We compute the similarity of songs based on these weights, which is faster than previous approaches that compare single Gaussian distributions directly. Experimental results on a genre-based retrieval task illustrate that our HDP-based method is both faster and produces better retrieval quality than such previous approaches.",USA,education,Developed economies,"[-10.052056, 13.284647]","[28.937624, 3.580994]","[-4.310129, 11.821538, -5.277597]","[23.693924, -1.0289829, 3.861706]","[12.914934, 8.967147]","[10.873568, 2.7351408]","[13.443971, 14.7181, -0.71314013]","[12.389582, 6.513603, 12.262076]"
53,Phillip B. Kirlin;Paul E. Utgoff,A Framework for Automated Schenkerian Analysis.,2008,https://doi.org/10.5281/zenodo.1415892,Phillip B. Kirlin+University of Massachusetts Amherst>USA>education;Paul E. Utgoff+University of Massachusetts Amherst>USA>education,"In Schenkerian analysis, one seeks to find structural dependences among the notes of a composition and organize these dependences into a coherent hierarchy that illustrates the function of every note. This type of analysis reveals multiple levels of structure in a composition by constructing a series of simplifications of a piece showing various elaborations and prolongations. We present a framework for solving this problem, called IVI, that uses a state-space search formalism. IVI includes multiple interacting components, including modules for various preliminary analyses (harmonic, melodic, rhythmic, and cadential), identifying and performing reductions, and locating pieces of the Ursatz. We describe a number of the algorithms by which IVI forms, stores, and updates its hierarchy of notes, along with details of the Ursatz-finding algorithm. We illustrate IVI’s functionality on an excerpt from a Schubert piano composition, and also discuss the issues of subproblem interactions and the multiple parsings problem.",USA,education,Developed economies,"[22.02242, 30.504324]","[-12.450571, 19.971783]","[-14.602175, -16.58153, 10.132879]","[-13.063365, -2.8178442, 6.703838]","[11.94647, 6.7440505]","[8.421411, 1.9011202]","[13.100556, 12.542648, -1.4708257]","[9.925165, 6.64879, 12.05936]"
54,Jouni Paulus;Anssi Klapuri,Music Structure Analysis Using a Probabilistic Fitness Measure and an Integrated Musicological Model.,2008,https://doi.org/10.5281/zenodo.1415826,Jouni Paulus+Tampere University of Technology>FIN>education;Anssi Klapuri+Tampere University of Technology>FIN>education,"This paper presents a system for recovering the sectional form of a musical piece: segmentation and labelling of musical parts such as chorus or verse. The system uses three types of acoustic features: mel-frequency cepstral coefficients, chroma, and rhythmogram. An analysed piece is first subdivided into a large amount of potential segments. The distance between each two segments is then calculated and the value is transformed to a probability that the two segments are occurrences of a same musical part. Different features are combined in the probability space and are used to define a fitness measure for a candidate structure description. Musicological knowledge of the temporal dependencies between the parts is integrated into the fitness measure. A novel search algorithm is presented for finding the description that maximises the fitness measure. The system is evaluated with a data set of 557 manually annotated popular music pieces. The results suggest that integrating the musicological model to the fitness measure leads to a more reliable labelling of the parts than performing the labelling as a post-processing step.",FIN,education,Developed economies,"[-0.9102227, 4.1642456]","[-4.922677, 4.404936]","[-3.7130227, -6.114068, 3.5906742]","[1.7453951, -2.5290358, 0.4725569]","[12.194799, 8.203599]","[8.339857, 2.6936831]","[13.027695, 13.794634, -0.7287273]","[10.313926, 7.303445, 11.520531]"
55,Hanna M. Lukashevich,Towards Quantitative Measures of Evaluating Song Segmentation.,2008,https://doi.org/10.5281/zenodo.1417623,Hanna Lukashevich+Fraunhofer IDMT>DEU>facility,"Automatic music structure analysis or song segmentation has immediate applications in the field of music information retrieval. Among these applications is active music navigation, automatic generation of audio summaries, automatic music analysis, etc. One of the important aspects of a song segmentation task is its evaluation. Commonly, that implies comparing the automatically estimated segmentation with a ground-truth, annotated by human experts. The automatic evaluation of segmentation algorithms provides the quantitative measure that reflects how well the estimated segmentation matches the annotated ground-truth. In this paper we present a novel evaluation measure based on information-theoretic conditional entropy. The principal advantage of the proposed approach lies in the applied normalization, which enables the comparison of the automatic evaluation results, obtained for songs with a different amount of states. We discuss and compare the evaluation scores commonly used for evaluating song segmentation at present. We provide several examples illustrating the behavior of different evaluation measures and weigh the benefits of the presented metric against the others.",DEU,facility,Developed economies,"[-3.6641948, -5.7892995]","[-1.4500334, 4.452295]","[4.692283, -3.0598059, -4.997424]","[-1.2282263, -0.84560716, -0.89576256]","[11.7937765, 8.447835]","[8.385611, 2.8579116]","[12.46288, 14.341104, 0.025264986]","[10.429015, 7.327215, 11.435987]"
56,Jörg Garbers;Frans Wiering,Towards Structural Alignment of Folk Songs.,2008,https://doi.org/10.5281/zenodo.1415838,Jörg Garbers+Utrecht University>NLD>education|Unknown>Unknown>Unknown;Frans Wiering+Utrecht University>NLD>education|Unknown>Unknown>Unknown,"We describe an alignment-based similarity framework for folk song variation research. The framework makes use of phrase and meter information encoded in Humdrum scores. Local similarity measures are used to compute match scores, which are combined with gap scores to form increasingly larger alignments and higher-level similarity values. We discuss the effects of some similarity measures on the alignment of four groups of melodies that are variants of each other.",NLD,education,Developed economies,"[-28.15555, -4.505012]","[14.528735, 2.9874122]","[3.387554, 12.017028, -2.1604733]","[4.668152, 1.6994414, 2.73026]","[12.633907, 9.8915415]","[9.402375, 1.8380651]","[12.866274, 15.15206, -0.25035203]","[11.153736, 7.0013156, 13.0689945]"
45,Malcolm Slaney;Kilian Q. Weinberger;William White,Learning a Metric for Music Similarity.,2008,https://doi.org/10.5281/zenodo.1415554,Malcolm Slaney+Yahoo! Research>USA>company;Kilian Weinberger+Yahoo! Research>USA>company;William White+Yahoo! Media Innovation>USA>company,"This paper describe five different principled ways to embed songs into a Euclidean metric space. In particular, we learn embeddings so that the pairwise Euclidean distance between two songs reflects semantic dissimilarity. This allows distance-based analysis, such as for example straightforward nearest-neighbor classification, to detect and potentially suggest similar songs within a collection. Each of the six approaches (baseline, whitening, LDA, NCA, LMNN and RCA) rotate and scale the raw feature space with a linear transform. We tune the parameters of these models using a song-classification task with content-based features.",USA,company,Developed economies,"[-6.8115263, 13.366287]","[27.88261, -10.484588]","[-3.7181413, 11.366683, 0.57987475]","[18.765532, -2.2976894, 2.69428]","[13.152899, 9.364629]","[10.504703, 2.7876098]","[13.6555805, 15.063749, -0.5448365]","[12.051658, 6.695566, 11.830022]"
42,Matthew Riley;Eric Heinen;Joydeep Ghosh,A Text Retrieval Approach to Content-Based Audio Hashing.,2008,https://doi.org/10.5281/zenodo.1417985,Matthew Riley+University of Texas at Austin>USA>education;Eric Heinen+University of Texas at Austin>USA>education;Joydeep Ghosh+University of Texas at Austin>USA>education,"This paper presents a novel approach to robust, content-based retrieval of digital music. We formulate the hashing and retrieval problems analogously to that of text retrieval and leverage established results for this unique application. Accordingly, songs are represented as a ""Bag-of-Audio-Words"" and similarity calculations follow directly from the well-known Vector Space model [12]. We evaluate our system on a 4000 song data set to demonstrate its practical applicability, and evaluation shows our technique to be robust to a variety of signal distortions. Most interestingly, the system is capable of matching studio recordings to live recordings of the same song with high accuracy.",USA,education,Developed economies,"[-9.367822, 27.43313]","[20.171618, 13.605803]","[-4.619702, 3.2947154, -17.054415]","[14.996073, -9.719939, 6.7730546]","[13.388051, 7.9519496]","[10.183135, 1.6614276]","[13.326303, 14.479312, -1.8427335]","[11.524505, 6.1782956, 12.60404]"
41,Florian Kleedorfer;Peter Knees;Tim Pohle,Oh Oh Oh Whoah! Towards Automatic Topic Detection In Song Lyrics.,2008,https://doi.org/10.5281/zenodo.1416154,Florian Kleedorfer+Studio Smart Agent Technologies>AUT>company|Research Studios Austria>AUT>facility;Peter Knees+Johannes Kepler University Linz>AUT>education;Tim Pohle+Johannes Kepler University Linz>AUT>education,We present an algorithm that allows for indexing music by topic. The application scenario is an information retrieval system into which any song with known lyrics can be inserted and indexed so as to make a music collection browsable by topic. We use text mining techniques for creating a vector space model of our lyrics collection and non-negative matrix factorization (NMF) to identify topic clusters which are then labeled manually. We include a discussion of the decisions regarding the parametrization of the applied methods. The suitability of our approach is assessed by measuring the agreement of test subjects who provide the labels for the topic clusters.,AUT,company,Developed economies,"[-29.828442, -29.667631]","[30.06909, 13.336321]","[8.365666, 22.289064, -6.5033565]","[15.31797, -3.2833207, 13.213812]","[11.53171, 11.735431]","[11.328061, 2.3268273]","[12.487403, 15.936908, 0.98031306]","[12.538227, 6.0877213, 12.352772]"
40,Hiromasa Fujihara;Masataka Goto;Jun Ogata,Hyperlinking Lyrics: A Method for Creating Hyperlinks Between Phrases in Song Lyrics.,2008,https://doi.org/10.5281/zenodo.1418251,Hiromasa Fujihara+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Jun Ogata+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We describe a novel method for creating a hyperlink from a phrase in the lyrics of a song to the same phrase in the lyrics of another song. This method can be applied to various applications, such as song clustering based on the meaning of the lyrics and a music playback interface that will enable a user to browse and discover songs on the basis of lyrics. Given a song database consisting of songs with their text lyrics and songs without their text lyrics, our method first extracts appropriate keywords (phrases) from the text lyrics without using audio signals. It then finds these keywords in audio signals by estimating the keywords’ start and end times. Although the performance obtained in our experiments has room for improvement, the potential of this new approach is shown.",JPN,facility,Developed economies,"[-32.85156, -33.06304]","[35.948437, -12.076193]","[6.455998, 24.258253, -4.2008395]","[16.715273, -7.3307056, 14.641107]","[11.466694, 11.76161]","[10.94266, 2.8653128]","[12.474569, 16.022001, 1.0820491]","[12.390577, 6.409916, 11.704232]"
39,Claudio Baccigalupo;Enric Plaza;Justin Donaldson,Uncovering Affinity of Artists to Multiple Genres from Social Behaviour Data.,2008,https://doi.org/10.5281/zenodo.1417823,"Claudio Baccigalupo+IIIA, Artificial Intelligence Research Institute>ESP>facility|Indiana University>USA>education;Enric Plaza+IIIA, Artificial Intelligence Research Institute>ESP>facility|Indiana University>USA>education;Justin Donaldson+Indiana University>USA>education","In organisation schemes, musical artists are commonly identified with a unique ‘genre’ label attached, even when they have affinity to multiple genres. To uncover this hidden cultural awareness about multi-genre affinity, we present a new model based on the analysis of the way in which a community of users organise artists and genres in playlists. Our work is based on a novel dataset that we have elaborated identifying the co-occurrences of artists in the playlists shared by the members of a popular Web-based community, and that is made publicly available. The analysis defines an automatic social-based method to uncover relationships between artists and genres, and introduces a series of novel concepts that characterises artists and genres in a richer way than a unique ‘genre’ label would do.",ESP,facility,Developed economies,"[-39.613304, 9.213293]","[42.223473, 7.4736724]","[-24.832548, 8.426679, 6.3664217]","[17.069286, 11.456717, 8.947092]","[14.402804, 9.893751]","[12.091236, 2.7685466]","[15.121314, 14.781802, -0.19482985]","[13.212095, 5.7411633, 11.778993]"
38,Kurt Jacobson;Mark B. Sandler;Benjamin Fields,Using Audio Analysis and Network Structure to Identify Communities in On-Line Social Networks of Artists.,2008,https://doi.org/10.5281/zenodo.1417867,"Kurt Jacobson+Queen Mary, University of London>GBR>education;Mark Sandler+Queen Mary, University of London>GBR>education;Ben Fields+Goldsmiths, University of London>GBR>education","Community detection methods from complex network theory are applied to a subset of the Myspace artist network to identify groups of similar artists. Methods based on the greedy optimization of modularity and random walks are used. In a second iteration, inter-artist audio-based similarity scores are used as input to enhance these community detection methods. The resulting community structures are evaluated using a collection of artist-assigned genre tags. Evidence suggesting the Myspace artist network structure is closely related to musical genre is presented and a Semantic Web service for accessing this structure is described.",GBR,education,Developed economies,"[-38.934715, 10.892288]","[42.56728, 8.5170965]","[-24.807827, 8.877203, 3.6618314]","[20.611828, 4.8968773, 9.156956]","[14.628013, 9.6098995]","[12.065237, 2.7152987]","[15.133106, 14.70164, -0.5669998]","[13.307692, 5.7803926, 11.932118]"
37,Yves Raimond;Mark B. Sandler,A Web of Musical Information.,2008,https://doi.org/10.5281/zenodo.1414840,"Yves Raimond+Queen Mary, University of London>GBR>education;Mark Sandler+Queen Mary, University of London>GBR>education","We describe our recent achievements in interlinking several music-related data sources on the Semantic Web. In particular, we describe interlinked datasets dealing with Creative Commons content, editorial, encyclopedic, geographic and statistical data, along with queries they can answer and tools using their data. We describe our web services, providing an on-demand access to content-based features linked with such data sources and information pertaining to their creation (including processing steps, applied algorithms, inputs, parameters or associated developers). We also provide a tool allowing such music analysis services to be set up and scripted in a simple way.",GBR,education,Developed economies,"[-22.899042, 26.65936]","[18.269478, 36.721783]","[-17.992676, 3.2230103, -14.171462]","[-4.0022626, -1.2765461, 24.146107]","[14.086211, 8.277738]","[10.899887, -0.022025017]","[14.276348, 14.364719, -1.8750749]","[12.104245, 5.112073, 11.817201]"
36,Mohamed Sordo;Òscar Celma;Martin Blech;Enric Guaus,The Quest for Musical Genres: Do the Experts and the Wisdom of Crowds Agree?,2008,https://doi.org/10.5281/zenodo.1415262,Mohamed Sordo+Universitat Pompeu Fabra>ESP>education;Oscar Celma+Universitat Pompeu Fabra>ESP>education;Martín Blech+Universitat Pompeu Fabra>ESP>education;Enric Guaus+Universitat Pompeu Fabra>ESP>education,"This paper presents some findings around musical genres. The main goal is to analyse whether there is any agreement between a group of experts and a community, when defining a set of genres and their relationships. For this purpose, three different experiments are conducted using two datasets: the MP3.com expert taxonomy, and last.fm tags at artist level. The experimental results show a clear agreement for some components of the taxonomy (Blues, Hip-Hop), whilst in other cases (e.g. Rock) there is no correlations. Interestingly enough, the same results are found in the MIREX2007 results for audio genre classification task. Therefore, a multi-faceted approach for musical genre using expert based classifications, dynamic associations derived from the wisdom of crowds, and content-based analysis can improve genre classification, as well as other relevant MIR tasks such as music similarity or music recommendation.",ESP,education,Developed economies,"[-29.435476, 5.322099]","[29.364012, -0.8389367]","[-20.150894, 1.6005698, 12.279132]","[14.321033, 10.468418, 2.0903049]","[14.068991, 9.187603]","[10.940954, 3.242459]","[14.611617, 14.613986, -0.90636927]","[12.481351, 6.0338044, 11.234085]"
33,Zhiyao Duan;Lie Lu;Changshui Zhang,Collective Annotation of Music from Multiple Semantic Categories.,2008,https://doi.org/10.5281/zenodo.1416694,Zhiyao Duan+Microsoft Research Asia (MSRA)>CHN>company;Lie Lu+Microsoft Research Asia (MSRA)>CHN>company;Changshui Zhang+Tsinghua University>CHN>education,"Music semantic annotation aims to automatically annotate a music signal with a set of semantic labels (words or tags). Existing methods on music semantic annotation usually take it as a multi-label binary classification problem, and model each semantic label individually while ignoring their relationships. However, there are usually strong correlations between some labels. Intuitively, investigating this correlation can be helpful to improve the overall annotation performance. In this paper, we report our attempts to collective music semantic annotation, which not only builds a model for each semantic label, but also builds models for the pairs of labels that have significant correlations. Two methods are exploited in this paper, one based on a generative model (Gaussian Mixture Model), and another based on a discriminative model (Conditional Random Field). Experiments show slight but consistent improvement in terms of precision and recall, compared with the individual-label modeling methods.",CHN,company,Developing economies,"[-31.58711, 8.940137]","[37.155636, -1.0213491]","[-16.289223, 5.9984746, 3.4031692]","[20.955738, 7.7201447, 2.0577736]","[14.003259, 9.553761]","[11.321389, 3.4266074]","[14.860336, 13.877751, -0.56766224]","[12.887512, 6.4118257, 11.202396]"
32,Youngmoo E. Kim;Erik M. Schmidt;Lloyd Emelle,MoodSwings: A Collaborative Game for Music Mood Label Collection.,2008,https://doi.org/10.5281/zenodo.1416586,Youngmoo E. Kim+Drexel University>USA>education;Erik Schmidt+Drexel University>USA>education;Lloyd Emelle+Drexel University>USA>education,"There are many problems in the field of music information retrieval that are not only difficult for machines to solve, but that do not have well-defined answers. In labeling and detecting emotions within music, this lack of specificity makes it difficult to train systems that rely on quantified labels for supervised machine learning. The collection of such “ground truth” data for these subjectively perceived features necessarily requires human subjects. Traditional methods of data collection, such as the hiring of subjects, can be flawed, since labeling tasks are time-consuming, tedious, and expensive. Recently, there have been many initiatives to use customized online games to harness so-called “Human Computation” for the collection of label data, and several such games have been proposed to collect labels spanning an excerpt of music. We present a new game, MoodSwings (http://schubert.ece.drexel.edu/moodswings), which differs in that it records dynamic (per-second) labels of players’ mood ratings of music, in keeping with the unique time-varying quality of musical mood. As in prior collaborative game approaches, players are partnered to verify each others’ results, and the game is designed to maximize consensus-building between users. We present preliminary results from an initial set of game play data.",USA,education,Developed economies,"[-54.79501, 6.2535663]","[46.01206, -5.301492]","[-17.405333, 25.498285, 1.1895695]","[14.033065, 20.249023, 1.3840536]","[13.58272, 12.392971]","[12.585518, 3.843756]","[16.109463, 14.858636, 1.3268665]","[13.660529, 5.4365764, 10.552634]"
34,Geoffroy Peeters;David Fenech;Xavier Rodet,MCIpa: A Music Content Information Player and Annotator for Discovering Music.,2008,https://doi.org/10.5281/zenodo.1415796,Geoffroy Peeters+Ircam - CNRS STMS>FRA>facility;David Fenech+Ircam>FRA>facility;Xavier Rodet+Ircam - CNRS STMS>FRA>facility,"In this paper, we present a new tool for intra-document browsing of musical pieces. This tool is a multimedia player which represents the content of a musical piece visually. Each type of musical content (structure, chords, downbeats/beats, notes, events) is associated with a distinct visual representation. The user sees what he/she is listening to. He can also browse inside the music according to the visual content. For this, each type of visual object has a dedicated feedback, either as an audio-feedback or as a playhead feedback. Content information can be extracted automatically from audio (using signal processing algorithms) or annotated by hand by the user. This multimedia player can also be used as an annotator tool guided by the content.",FRA,facility,Developed economies,"[-30.587278, 11.940548]","[10.695025, 34.7083]","[-18.278423, 7.3476777, -0.3259696]","[5.9543324, -6.8709435, 23.844555]","[14.250772, 8.745474]","[10.847383, 0.9505598]","[14.810053, 14.057631, -1.0577924]","[11.373311, 5.143208, 12.458881]"
43,Riccardo Miotto;Nicola Orio,A Music Identification System Based on Chroma Indexing and Statistical Modeling.,2008,https://doi.org/10.5281/zenodo.1415254,Riccardo Miotto+University of Padova>ITA>education;Nicola Orio+University of Padova>ITA>education,"A methodology is described for the automatic identification of classical music works. It can be considered an extension of fingerprinting techniques because the identification is carried out also when the query is a different performance of the work stored in the database, possibly played by different instruments and with background noise. The proposed methodology integrates an already existing approach based on hidden Markov models with an additional component that aims at improving scalability. The general idea is to carry out a clustering of the collection to highlight a limited number of candidates to be used for the HMM-based identification. Clustering is computed using the chroma features of the music works, hashed in a single value and retrieved using a bag of terms approach. Evaluation results are provided to show the validity of the combined approaches.",ITA,education,Developed economies,"[-12.188969, 13.020211]","[-4.794559, -0.85222363]","[0.7583105, 1.8311898, -7.747194]","[2.9091294, -6.2530274, -3.1235833]","[12.279817, 7.907253]","[8.27972, 2.922985]","[12.8768, 13.9600315, 0.26456806]","[10.645571, 7.698872, 11.491276]"
35,Nik Corthaut;Sten Govaerts;Katrien Verbert;Erik Duval,"Connecting the Dots: Music Metadata Generation, Schemas and Applications.",2008,https://doi.org/10.5281/zenodo.1415244,Nik Corthaut+Katholieke Universiteit Leuven>BEL>education;Sten Govaerts+Katholieke Universiteit Leuven>BEL>education;Katrien Verbert+Katholieke Universiteit Leuven>BEL>education;Erik Duval+Katholieke Universiteit Leuven>BEL>education,"With the ever-increasing amount of digitized music becoming available, metadata is a key driver for different music related application domains. A service that combines different metadata sources should be aware of the existence of different schemas to store and exchange music metadata. The user of a metadata provider could benefit from knowledge about the metadata needs for different music application domains. In this paper, we present how we can compare the expressiveness and richness of a metadata schema for an application. To cope with different levels of granularity in metadata fields we defined clusters of semantically related metadata fields. Similarly, application domains were defined to tackle the fine-grained functionality space in music applications. Next is shown to what extent music application domains and metadata schemas make use of the metadata field clusters. Finally, we link the metadata schemas with the application domains. A decision table is presented that assists the user of a metadata provider in choosing the right metadata schema for his application.",BEL,education,Developed economies,"[-24.916454, 35.471436]","[18.416521, 34.635338]","[-23.468754, 0.7185487, -7.2212963]","[-1.7898451, -0.7771649, 20.442238]","[14.267855, 9.02078]","[10.977353, 0.048918393]","[15.106868, 13.633502, -1.2067392]","[11.986144, 4.963628, 11.948968]"
31,Douglas Turnbull;Luke Barrington;Gert R. G. Lanckriet,Five Approaches to Collecting Tags for Music.,2008,https://doi.org/10.5281/zenodo.1416804,Douglas Turnbull+UC San Diego>USA>education|UC San Diego>USA>education|UC San Diego>USA>education;Luke Barrington+UC San Diego>USA>education;Gert Lanckriet+UC San Diego>USA>education,"We compare five approaches to collecting tags for music: conducting a survey, harvesting social tags, deploying annotation games, mining web documents, and autotagging audio content. The comparison includes a discussion of both scalability (financial cost, human involvement, and computational resources) and quality (the cold start problem & popularity bias, strong vs. weak labeling, vocabulary structure & size, and annotation accuracy). We then describe one state-of-the-art system for each approach. The performance of each system is evaluated using a tag-based music information retrieval task. Using this task, we are able to quantify the effect of popularity bias on each approach by making use of a subset of more popular (short-head) songs and a set of less popular (long-tail) songs. Lastly, we propose a simple hybrid context-content system that combines our individual approaches and produces superior retrieval results.",USA,education,Developed economies,"[-41.175167, -0.09476589]","[39.64112, 2.3414283]","[-14.925886, 13.642332, 6.4712906]","[22.378258, 7.885024, 6.232442]","[14.529356, 10.452077]","[11.7629795, 2.9957552]","[15.550814, 14.213925, -0.0034250345]","[13.1457205, 6.0814123, 11.6388645]"
32,Heng-Yi Lin;Yin-Tzu Lin;Ming-Chun Tien;Ja-Ling Wu,Music Paste: Concatenating Music Clips based on Chroma and Rhythm Features.,2009,https://doi.org/10.5281/zenodo.1415758,Heng-Yi Lin+National Taiwan University>TWN>education;Yin-Tzu Lin+National Taiwan University>TWN>education;Ming-Chun Tien+National Taiwan University>TWN>education;Ja-Ling Wu+National Taiwan University>TWN>education,"In this paper, we provide a tool for automatically choosing appropriate music clips from a given audio collection and properly combining the chosen clips. To seamlessly concatenate two different music clips without causing any audible defect is really a hard nut to crack. Borrowing the idea from the musical dice game and the DJ’s strategy and considering psychoacoustics, we employ the currently available audio analysis and editing techniques to paste music sounded as pleasant as possible. Besides, we conduct subjective evaluations on the correlation between pasting methods and the auditory quality of combined clips. The experimental results show that the automatically generated music pastes are acceptable to most of the evaluators. The proposed system can be used to generate lengthened or shortened background music and dancing suite, which is useful for some audio-assisted multimedia applications.",TWN,education,Developing economies,"[-10.660614, 7.5531216]","[-7.5480227, 25.571568]","[10.367219, 15.557579, 24.009207]","[-15.652409, 6.8094707, 10.87614]","[11.952714, 7.752338]","[10.757684, 1.8090019]","[13.331212, 13.013675, -0.670222]","[10.219208, 5.5221305, 10.301711]"
33,Emiru Tsunoo;Nobutaka Ono;Shigeki Sagayama,Musical Bass-Line Pattern Clustering and Its Application to Audio Genre Classification.,2009,https://doi.org/10.5281/zenodo.1417141,Emiru Tsunoo+The University of Tokyo>JPN>education;Nobutaka Ono+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"""This paper discusses a new approach for clustering musical bass-line patterns representing particular genres and its application to audio genre classification. Many musical genres are characterized not only by timbral information but also by distinct representative bass-line patterns. So far this kind of temporal features have not so effectively been utilized. In particular, modern music songs mostly have certain fixed bar-long bass-line patterns per genre. For instance, while frequently bass-lines in rock music have constant pitch and a uniform rhythm, in jazz music there are many characteristic movements such as walking bass. We propose a representative bass-line pattern template extraction method based on k-means clustering handling a pitch-shift problem. After extracting the fundamental bass-line pattern templates for each genre, distances from each template are calculated and used as a feature vector for supervised learning. Experimental result shows that the automatically calculated bass-line pattern information can be used for genre classification effectively and improve upon current approaches based on timbral features.""",JPN,education,Developed economies,"[-25.94377, -17.003174]","[16.131773, -10.4152]","[-13.027311, 2.704787, 21.592428]","[4.570144, 5.4180083, -6.5268173]","[12.540857, 10.73228]","[9.229102, 3.1663058]","[13.414661, 14.096476, 1.2357242]","[11.1503315, 7.592042, 11.068579]"
31,Keiichiro Hoashi;Shuhei Hamawaki;Hiromi Ishizaki;Yasuhiro Takishima;Jiro Katto,Usability Evaluation of Visualization Interfaces for Content-based Music Retrieval Systems.,2009,https://doi.org/10.5281/zenodo.1414834,Keiichiro Hoashi+KDDI R&D Laboratories Inc.>JPN>company;Shuhei Hamawaki+Waseda University>JPN>education;Hiromi Ishizaki+KDDI R&D Laboratories Inc.>JPN>company;Yasuhiro Takishima+KDDI R&D Laboratories Inc.>JPN>company;Jiro Katto+Waseda University>JPN>education,"This research presents a formal user evaluation of a typical visualization method for content-based music information retrieval (MIR) systems, and also proposes a novel interface to improve MIR usability. Numerous interfaces to visualize content-based MIR systems have been proposed, but reports on user evaluations of such proposed GUIs are scarce. This research aims to evaluate the effectiveness of a typical 2-D visualization method for content-based MIR systems, by conducting comparative user evaluations against the traditional list-based format to present MIR results to the user. Based on the observations of the experimental results, we next propose a 3-D visualization system, which features a function to specify sub-regions of the feature space based on genre classification results, and a function which allows users to select features that are assigned to the axes of the 3-D space. Evaluation of this GUI conclude that the functions of the 3-D system can significantly improve both the efficiency and usability of MIR systems.",JPN,company,Developed economies,"[-17.79525, 31.166662]","[29.628853, 30.064058]","[-15.406383, 9.125534, -21.470472]","[6.2677407, 3.703012, 17.436779]","[14.227587, 7.312048]","[11.59386, 1.1271269]","[14.331793, 14.05027, -2.3550682]","[12.393643, 4.7647767, 12.574166]"
30,Chao-Ling Hsu;Liang-Yu Chen;Jyh-Shing Roger Jang;Hsing-Ji Li,Singing Pitch Extraction from Monaural Polyphonic Songs by Contextual Audio Modeling and Singing Harmonic Enhancement.,2009,https://doi.org/10.5281/zenodo.1418009,Chao-Ling Hsu+MediaTek-NTHU Joint Lab>TWN>education;Liang-Yu Chen+MediaTek-NTHU Joint Lab>TWN>education;Jyh-Shing Roger Jang+MediaTek-NTHU Joint Lab>TWN>education;Hsing-Ji Li+Innovative Digitech-Enabled Applications & Services Institute (IDEAS)>TWN>facility,"This paper proposes a novel approach to extract the pitches of singing voices from monaural polyphonic songs. The hidden Markov model (HMM) is adopted to model the transition between adjacent singing pitches in time, and the relationships between melody and its chord, which is implicitly represented by features extracted from the spectrum. Moreover, another set of features which represents the energy distribution of the enhanced singing harmonic structure is proposed by applying a normalized sub-harmonic summation technique. By using these two feature sets with complementary characteristics, a 2-stream HMM is constructed for singing pitch extraction. Quantitative evaluation shows that the proposed system outperforms the compared approaches for singing pitch extraction from polyphonic songs.",TWN,education,Developing economies,"[-1.0914903, -31.879719]","[-2.1682591, -10.305672]","[19.313745, 4.9606695, -7.121449]","[5.3729143, -8.380232, -11.614773]","[9.64222, 10.364537]","[6.7226763, 2.9302316]","[10.922132, 14.773123, 0.30326825]","[9.248596, 8.196176, 11.075491]"
28,Peter Grosche;Meinard Müller,A Mid-Level Representation for Capturing Dominant Tempo and Pulse Information in Music Recordings.,2009,https://doi.org/10.5281/zenodo.1416016,Peter Grosche+Saarland University>DEU>education|MPI Informatik>DEU>facility;Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility,"""Automated beat tracking and tempo estimation from music recordings become challenging tasks in the case of non-percussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation which captures predominant local pulse information. To this end, we first derive a tempogram by performing a local spectral analysis on a previously extracted, possibly very noisy onset representation. From this, we derive for each time position the predominant tempo as well as a sinusoidal kernel that best explains the local periodic nature of the onset representation. Then, our main idea is to accumulate the local kernels over time yielding a single function that reveals the predominant local pulse (PLP). We show that this function constitutes a robust mid-level representation from which one can derive musically meaningful tempo and beat information for non-percussive music even in the presence of significant tempo fluctuations. Furthermore, our representation allows for incorporating prior knowledge on the expected tempo range to exhibit information on different pulse levels.""",DEU,education,Developed economies,"[37.831333, -27.955034]","[-25.991213, -3.9654274]","[-0.8005315, -27.92089, -5.106365]","[-3.6669955, 9.8309765, -8.377095]","[11.339449, 4.4796762]","[5.483223, 1.9102185]","[10.759871, 13.362399, -2.5685694]","[7.6645756, 7.073052, 11.173289]"
34,Joan Serrà;Massimiliano Zanin;Cyril Laurier;Mohamed Sordo,Unsupervised Detection of Cover Song Sets: Accuracy Improvement and Original Identification.,2009,https://doi.org/10.5281/zenodo.1418063,"Joan Serrà+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Massimiliano Zanin+Universidad Autónoma de Madrid>ESP>education;Cyril Laurier+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Mohamed Sordo+Music Technology Group, Universitat Pompeu Fabra>ESP>education","The task of identifying cover songs has formerly been studied in terms of a prototypical query retrieval framework. However, this framework is not the only one the task allows. In this article, we revise the task of identifying cover songs to include the notion of sets (or groups) of covers. In particular, we study the application of unsupervised clustering and community detection algorithms to detect cover sets. We consider current state-of-the-art algorithms and propose new methods to achieve this goal. Our experiments show that the detection of cover sets is feasible, that it can be performed in a reasonable amount of time, that it does not require extensive parameter tuning, and that it presents certain robustness to inaccurate measurements. Furthermore, we highlight two direct outcomes that naturally arise from the proposed framework revision: increasing the accuracy of query retrieval-based systems and detecting the original song within a set of covers.",ESP,education,Developed economies,"[8.586631, 44.077194]","[24.71988, 12.972162]","[3.4268668, 12.225853, -24.944525]","[17.350544, -1.14846, 5.4414234]","[16.08856, 11.11698]","[10.423441, 2.3967185]","[12.858258, 17.33918, -0.3395022]","[11.869638, 6.4032893, 12.082676]"
27,Parag Chordia;Alex Rae,Using Source Separation to Improve Tempo Detection.,2009,https://doi.org/10.5281/zenodo.1416536,Parag Chordia+Georgia Institute of Technology>USA>education|Georgia Tech Center for Music Technology>USA>education;Alex Rae+Georgia Institute of Technology>USA>education|Georgia Tech Center for Music Technology>USA>education,"We describe a novel tempo estimation method based on decomposing musical audio into sources using principal latent component analysis (PLCA). The approach is motivated by the observation that in rhythmically complex music, some layers may be more rhythmically regular than the overall mix, thus facilitating tempo detection. Each excerpt was analyzed using PLCA and the resulting components were each tempo tracked using a standard autocorrelation-based algorithm. We describe several techniques for aggregating or choosing among the multiple estimates that result from this process to extract a global tempo estimate. The system was evaluated on the MIREX 2006 training database as well as a newly constructed database of rhythmically complex electronic music consisting of 27 examples (IDM DB). For these databases the algorithms improved accuracy by 10% (60% vs 50%) and 22.3% (48.2% vs. 25.9%) respectively. These preliminary results suggest that for some types of music, source-separation may lead to better tempo detection.",USA,education,Developed economies,"[39.90929, -28.118591]","[-28.227169, -5.400878]","[1.3567064, -30.549332, -1.833103]","[-5.245258, 10.736706, -10.136493]","[11.423172, 4.3831744]","[5.174887, 1.8631107]","[10.795789, 13.36292, -2.720061]","[7.445029, 7.011619, 11.101337]"
26,Matthias Gruhne;Christian Dittmar;Daniel Gärtner,Improving Rhythmic Similarity Computation by Beat Histogram Transformations.,2009,https://doi.org/10.5281/zenodo.1417635,Matthias Gruhne+Bach Technology AS>NOR>company;Christian Dittmar+Fraunhofer IDMT>DEU>facility;Daniel Gaertner+Fraunhofer IDMT>DEU>facility,"Rhythmic descriptors are often utilized for semantic music classification, such as genre recognition or tempo detection. Several algorithms dealing with the extraction of rhythmic information from music signals were proposed in literature. Most of them derive a so-called beat histogram by auto-correlating a representation of the temporal envelope of the music signal. To circumvent the problem of tempo dependency, post-processing via higher-order statistics has been reported. Tests concluded, that these statistics are still tempo dependent to a certain extent. This paper describes a method, which transforms the original auto-correlated envelope into a tempo-independent rhythmic feature vector by multiplying the lag-axis with a stretch factor. This factor is computed with a new correlation technique which works in the logarithmic domain. The proposed method is evaluated for rhythmic similarity, consisting of two tasks: One test with manually created rhythms as proof of concept and another test using a large real-world music archive.",NOR,company,Developed economies,"[44.11716, 7.254486]","[-22.305668, -0.89600706]","[-8.323239, -22.000605, 5.125214]","[0.7816051, 11.221855, -7.6783295]","[12.009525, 5.434602]","[5.9498186, 1.8953948]","[11.374959, 14.171653, -2.1219227]","[8.167877, 6.9598393, 11.670714]"
25,Zoltán Juhász,Motive Identification in 22 Folksong Corpora Using Dynamic Time Warping and Self Organizing Maps.,2009,https://doi.org/10.5281/zenodo.1416216,Zoltán Juhász+Research Institute for Technical Physics and Materials Science>HUN>facility,"A system for automatic motive identification of large folksong corpora is described in this article. The method is based on a dynamic time warping algorithm determining inherent repeating elements of the melodies and a self-organizing map that learns the most typical motive contours. Using this system, the typical motive collections of 22 cultures in Eurasia have been determined, and another great common self organising map has been trained by the unified collection of the national/areal motive collections. The analysis of the overlaps of the national-areal excitations on the common map allowed us to draw a graph of connections, which shows two main distinct groups, according to the geographical distribution.",HUN,facility,Developed economies,"[-30.976477, -4.1494293]","[24.714952, 2.8348126]","[1.5848395, -0.79852366, -32.66175]","[17.300453, -10.357291, 23.222504]","[12.870962, 9.953207]","[10.973949, 1.9814812]","[13.183366, 14.955236, -0.07299622]","[12.037612, 6.2265487, 13.256385]"
24,Christopher Santoro;Corey Cheng,Multiple F0 Estimation in the Transform Domain.,2009,https://doi.org/10.5281/zenodo.1416242,Christopher A. Santoro+LSB Audio>USA>company;Corey I. Cheng+University of Miami>USA>education,"A novel algorithm is proposed to estimate the fundamental frequencies present in polyphonic acoustic mixtures expressed in a transform domain. As an example, the algorithm operates on Modified Discrete Cosine Transform (MDCT) coefficients in order to demonstrate the utility of the method in commercially available perceptual audio codecs which use the MDCT. An auditory model is developed along with several optimizations that deal with the constraints of processing in the transform-domain, including an interpolation method, a transform-domain half-wave rectification model, tonal component estimation, and sparse convolution. Test results are separated by instrument and analyzed in detail. The proposed algorithm is shown to perform comparably to state of the art time-domain methods.",USA,company,Developed economies,"[41.369514, -17.612911]","[-45.52788, -18.4888]","[2.999523, -24.115715, 12.160235]","[-0.062300377, -8.503664, -31.437462]","[8.904862, 8.661072]","[6.547979, 2.9023492]","[11.155156, 13.417854, -0.2776283]","[9.266728, 8.274169, 10.811942]"
29,Dominik Lübbers;Matthias Jarke,Adaptive Multimodal Exploration of Music Collections.,2009,https://doi.org/10.5281/zenodo.1415518,Dominik Lübbers+RWTH Aachen University>DEU>education;Matthias Jarke+German University of Technology>OMN>education,"Discovering music that we like rarely happens as a result of a directed search. Except for the case where we have exact meta data at hand it is hard to articulate what song is attractive to us. Therefore it is essential to develop and evaluate systems that support guided exploratory browsing of the music space. While a number of algorithms for organizing music collections according to a given similarity measure have been applied successfully, the generated structure is usually only presented visually and listening requires cumbersome skipping through the individual pieces. To close this media gap we describe an immersive multimodal exploration environment which extends the presentation of a song collection in a video-game-like virtual 3-D landscape by carefully adjusted spatialized playback of songs. The user can freely navigate through the virtual world guided by the acoustic clues surrounding him. Observing his interaction with the environment the system furthermore learns the user’s way of structuring his collection by adapting a weighted combination of a wide range of integrated content-based, meta-data-based and collaborative similarity measures. Our evaluation proves the importance of auditory feedback for music exploration and shows that our system is capable of adjusting to different notions of similarity.",DEU,education,Developed economies,"[-16.512453, 29.130573]","[30.40636, 20.693365]","[-10.956463, 6.8665004, -21.523722]","[14.942327, -2.6894114, 23.331348]","[14.201761, 7.360235]","[11.50848, 1.5705966]","[14.088664, 14.204208, -2.488096]","[12.4899235, 5.4412293, 13.178427]"
35,Matthias Mauch;Katy C. Noland;Simon Dixon,Using Musical Structure to Enhance Automatic Chord Transcription.,2009,https://doi.org/10.5281/zenodo.1414844,Matthias Mauch+Queen Mary University of London>GBR>education;Katy Noland+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Chord extraction from audio is a well-established music computing task, and many valid approaches have been presented in recent years that use different chord templates, smoothing techniques and musical context models. The present work shows that additional exploitation of the repetitive structure of songs can enhance chord extraction, by combining chroma information from multiple occurrences of the same segment type. To justify this claim we modify an existing chord labelling method, providing it with manual or automatic segment labels, and compare chord extraction results on a collection of 125 songs to baseline methods without segmentation information. Our method results in consistent and more readily readable chord labels and provides a statistically significant boost in label accuracy.",GBR,education,Developed economies,"[50.259884, -4.45136]","[-30.304134, 19.72312]","[21.391237, -11.537444, 14.590844]","[-26.414776, -6.0719776, 3.6824162]","[7.246958, 8.709629]","[6.2110505, 3.6181395]","[11.981034, 10.742361, 1.7036265]","[9.809961, 8.806517, 12.163427]"
42,Erik M. Schmidt;Kris West;Youngmoo E. Kim,Efficient Acoustic Feature Extraction for Music Information Retrieval Using Programmable Gate Arrays.,2009,https://doi.org/10.5281/zenodo.1416698,Erik M. Schmidt+Drexel University>USA>education;Kris West+University of Illinois>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"Many of the recent advances in music information retrieval from audio signals have been data-driven, i.e., resulting from the analysis of very large data sets. Widespread performance evaluations on common data sets, such as the annual MIREX events, have also been instrumental in advancing the field. These endeavors incur a large computational cost, and could potentially benefit greatly from more rapid calculation of acoustic features. Traditional, cluster-based solutions for large-scale feature extraction are expensive and space- and power-inefficient. Using the massively parallel architecture of the field programmable gate array (FPGA), it is possible to design an application specific chip rivaling the speed of a cluster for large-scale acoustic feature computation at lower cost. Recent advances in development tools, such as the Xilinx Blockset in Simulink, allow rapid prototyping, simulation, and implementation on actual hardware. Such devices also show potential for the implementation of MIR systems on embedded devices such as cell phones and PDAs where hardware acceleration would be an absolute necessity. We present a prototype library for acoustic feature calculation for implementation on Xilinx FPGA hardware. Furthermore, using a genre classification task we compare the performance of simulated hardware features to those computed using standard methods, demonstrating a nearly negligible drop in classification performance with the potential for large reductions in computation time.",USA,education,Developed economies,"[-9.403693, -14.822491]","[10.611063, 27.04907]","[-0.70621765, -2.0632277, -15.148765]","[-3.8145423, -1.761558, 12.272446]","[13.057793, 8.068743]","[10.302671, 1.5044074]","[13.170495, 13.82386, 0.32958436]","[11.287073, 5.736436, 11.413266]"
37,Martín Haro;Perfecto Herrera,From Low-Level to Song-Level Percussion Descriptors of Polyphonic Music.,2009,https://doi.org/10.5281/zenodo.1417201,Martín Haro+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>facility;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>facility,"We address here the automatic description of percussive events in real-world polyphonic music. By taking a pattern recognition approach we evaluate more than 2,450 object-level features. Three binary instrument-wise support vector machines (SVM) are built from a training set of more that 100 songs and 10 genres. Then, we use these binary models to build a drum transcription system achieving comparable results with state of the art algorithms. Finally, we present 17 song-level percussion descriptors computed from the imperfect output of the transcription algorithm. We evaluate the usefulness of the proposed descriptors in music information retrieval (MIR) tasks like genre classification, danceability estimation and Western vs. non-Western music discrimination. We conclude that the presented song-level percussion descriptors provide complementary information to “classic” descriptors, that can help in the previously mentioned MIR tasks.",ESP,education,Developed economies,"[21.898508, -45.528698]","[-13.678988, 3.4549475]","[15.700507, -17.253212, -1.8581563]","[7.4981284, 8.465964, -11.897735]","[8.122756, 7.145969]","[9.05441, 3.7000966]","[10.491972, 12.029805, 0.75892144]","[10.297597, 7.255922, 10.32218]"
38,Yannis Panagakis;Constantine Kotropoulos;Gonzalo R. Arce,Music Genre Classification Using Locality Preserving Non-Negative Tensor Factorization and Sparse Representations.,2009,https://doi.org/10.5281/zenodo.1416414,Yannis Panagakis+Aristotle University of Thessaloniki>GRC>education;Constantine Kotropoulos+Aristotle University of Thessaloniki>GRC>education;Gonzalo R. Arce+University of Delaware>USA>education,"A robust music genre classification framework is proposed that combines the rich, psycho-physiologically grounded properties of auditory cortical representations of music recordings and the power of sparse representation-based classifiers. A novel multilinear subspace analysis method that incorporates the underlying geometrical structure of the cortical representations space into non-negative tensor factorization is proposed for dimensionality reduction compatible to the working principle of sparse representation-based classification. The proposed method is referred to as Locality Preserving Non-Negative Tensor Factorization (LPNTF). Dimensionality reduction is shown to play a crucial role within the classification framework under study. Music genre classification accuracy of 92.4% and 94.38% on the GTZAN and the ISMIR2004 Genre datasets is reported, respectively. Both accuracies outperform any accuracy ever reported for state of the art music genre classification algorithms applied to the aforementioned datasets.",GRC,education,Developed economies,"[-35.35337, -10.713754]","[17.79201, -23.853815]","[-17.936531, 11.966694, 17.66337]","[19.458977, 6.3343115, -13.596573]","[13.0123005, 10.4865465]","[9.400383, 3.6672847]","[13.912562, 14.0515, 0.8962521]","[10.982648, 8.07446, 10.539694]"
39,Justin Salamon;Martin Rohrmeier,A Quantitative Evaluation of a Two Stage Retrieval Approach for a Melodic Query by Example System.,2009,https://doi.org/10.5281/zenodo.1416324,Justin Salamon+Universitat Pompeu Fabra>ESP>education;Martin Rohrmeier+University of Cambridge>GBR>education,"We present a two-stage approach for retrieval in a melodic Query by Example system inspired by the BLAST algorithm used in bioinformatics for DNA matching. The first stage involves an indexing method using n-grams and reduces the number of targets to consider in the second stage. In the second stage we use a matching algorithm based on local alignment with modified cost functions which take into account musical considerations. We evaluate our system using queries made by real users utilising both short-term and long-term memory, and present a detailed study of the system’s parameters and how they affect retrieval performance and efficiency. We show that whilst similar approaches were shown to be unsuccessful for Query by Humming (where singing and transcription errors result in queries with higher error rates), in the case of our system the approach is successful in reducing the database size without decreasing retrieval performance.",ESP,education,Developed economies,"[2.9263089, 20.47166]","[11.283677, 12.914301]","[2.5842998, 6.4448814, -5.449528]","[9.300984, -12.204838, 12.891078]","[11.8342495, 9.872905]","[8.90716, 0.5187471]","[12.496708, 15.529478, -0.9967008]","[10.494775, 5.9631233, 13.226791]"
40,Sten Govaerts;Erik Duval,A Web-based Approach to Determine the Origin of an Artist..,2009,https://doi.org/10.5281/zenodo.1415974,Sten Govaerts+K.U. Leuven>BEL>education;Erik Duval+K.U. Leuven>BEL>education,"One can define the origin of an artist as the geographical location where he started his career. The origin is an important metadata element, because it can help to specify subgenres, be an indicator of regional popularity and improve recommendations. In this paper, we present six methods to determine the origin, based on Web data sources: one extracts data from Last.fm, two query Freebase and three analyze biographies. We evaluate the different methods with 11275 artists. Circa 55% of the artists can be classified using biographies. The best Freebase method can classify 26% and the Last.fm based method 7%. When comparing on accuracy, the Last.fm and Freebase methods perform similarly with around 90% accuracy. For the biography-based methods we achieve 71%. To improve coverage, a final, hybrid method achieves 77% accuracy and 60% coverage. The accuracy of the continent classification is 87%. As a showcase for our classifier, we developed a mashup application that displays, among others, information about the origin of artists from radio station playlists on a map.",BEL,education,Developed economies,"[-40.38405, 6.295591]","[39.5427, 6.948402]","[-24.201485, 9.693707, 11.050187]","[18.658066, 9.170029, 8.69726]","[14.215142, 10.032372]","[11.181249, 0.20104173]","[15.030886, 14.719162, 0.03413893]","[12.491477, 5.3649898, 12.030984]"
41,Wijnand Schepens,Chronicle: Representation of Complex Time Structures.,2009,https://doi.org/10.5281/zenodo.1417353,Wijnand Schepens+University College Ghent>BEL>education,"Chronicle is a novel open source system for representing structured data involving time, such as music. It offers an XML-based file format, object models for internal representation in various programming languages, and software libraries and tools for reading and writing XML and for data transformations. Chronicle defines basic blocks for representing time-based information using events, a hierarchy of groups and instantiable templates. It supports two modes of timing: local timing within a group and association with other elements. The built-in mechanism for resolving time references can be used to implement both timescale mappings and tagging of information. Chronicle aims to be a powerful and flexible foundation on which new file formats and software can be built. Chronicle focuses on structure and timing, but leaves the actual content free to choose. Thus format- or software-developers can specify their own domain-model. This makes it possible to make representations for different types of musical information (scores, performance data, ...) in different styles or cultures (CMN, non-western, contemporary, ...), but also for other domains like choreography, scheduling, task management, and so on. It is also ideal for structured tagging of audio and multimedia (movie subtitles, karaoke, synchronisation, ...) and for representing internal data used in music algorithms. The system is organized in four levels of increasing complexity. Software developed for a specific level and domain will also accept lower level data, while users can choose to represent data in a higher level and use Chronicle tools to reduce the level.",BEL,education,Developed economies,"[47.596798, 10.390741]","[13.699329, 41.51828]","[-13.405744, -23.299232, 0.9266597]","[-0.2920557, -5.4632306, 28.973326]","[12.004114, 5.4578524]","[10.368607, 0.11252219]","[11.351157, 14.19393, -2.107668]","[11.714284, 5.1608157, 11.654796]"
43,Pascal Ferraro;Pierre Hanna;Laurent Imbert;Thomas Izard,Accelerating Query-by-Humming on GPU.,2009,https://doi.org/10.5281/zenodo.1415798,Pascal Ferraro+LaBRI - U. Bordeaux 1>FRA>education|PIMS/CNRS - U. Calgary>CAN>education;Pierre Hanna+LaBRI - U. Bordeaux 1>FRA>education;Laurent Imbert+Lirmm - CNRS>FRA>education|PIMS/CNRS - U. Calgary>CAN>education;Thomas Izard+Lirmm - U. Montpellier 2>FRA>education,"Searching for similarities in large musical databases has become a common procedure. Local alignment methods, based on dynamic programming, explore all the possible matchings between two musical pieces; and as a result return the optimal local alignment. Unfortunately these very powerful methods have a very high computational cost. The exponential growth of musical databases makes exact alignment algorithm unrealistic for searching similarities. Alternatives have been proposed in bioinformatics either by using heuristics or by developing faster implementation of exact algorithm. The main motivation of this work is to exploit the huge computational power of commonly available graphic cards to develop high performance solutions for Query-by-Humming applications. In this paper, we present a fast implementation of a local alignment method, which allows to retrieve a hummed query in a database of MIDI files, with good accuracy, in a time up to 160 times faster than other comparable systems.",FRA,education,Developed economies,"[-1.3628511, 37.359356]","[6.326996, 7.8675776]","[-10.603273, -8.85655, -25.647572]","[-6.915942, 21.795013, 4.997199]","[14.791455, 6.1617355]","[8.433001, 0.7196135]","[13.210472, 15.32207, -2.9194596]","[10.254685, 6.3132925, 13.203659]"
44,Zafar Rafii;Bryan Pardo,Learning to Control a Reverberator Using Subjective Perceptual Descriptors.,2009,https://doi.org/10.5281/zenodo.1418035,Zafar Rafii+Northwestern University>USA>education|Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education|Northwestern University>USA>education,"The complexity of existing tools for mastering audio can be daunting. Moreover, many people think about sound in individualistic terms (such as “boomy”) that may not have clear mappings onto the controls of existing audio tools. We propose learning to map subjective audio descriptors, such as “boomy”, onto measures of signal properties in order to build a simple controller that manipulates an audio reverberator in terms of a chosen descriptor. For example, “make the sound less boomy”. In the learning process, a user is presented with a series of sounds altered in different ways by a reverberator and asked to rate how well each sound represents the audio concept. The system correlates these ratings with reverberator parameters to build a controller that manipulates reverberation in the user’s terms. In this paper, we focus on developing the mapping between reverberator controls, measures of qualities of reverberation and user ratings. Results on 22 subjects show the system learns quickly (under 3 minutes of training per concept), predicts users responses well (mean correlation coefficient of system predictiveness 0.75) and meets users’ expectations (average human rating of 7.4 out of 10).",USA,education,Developed economies,"[17.220873, -29.74254]","[1.7625057, 26.669573]","[24.03231, -15.610443, -17.951014]","[-14.76991, 18.882439, 10.620187]","[8.991058, 6.079937]","[9.44456, 5.623575]","[11.173792, 11.566546, -0.27773497]","[10.540023, 5.56378, 10.045085]"
23,Ioannis Karydis;Alexandros Nanopoulos;Hans-Henning Gabriel;Myra Spiliopoulou,Tag-Aware Spectral Clustering of Music Items.,2009,https://doi.org/10.5281/zenodo.1417483,Ioannis Karydis+Ionian University>GRC>education;Alexandros Nanopoulos+Hildesheim University>DEU>education;Hans-Henning Gabriel+Otto-von-Guericke-University Magdeburg>DEU>education;Myra Spiliopoulou+Otto-von-Guericke-University Magdeburg>DEU>education,"""Social tagging is an increasingly popular phenomenon with substantial impact on Music Information Retrieval (MIR). Tags express the personal perspectives of the user on the music items (such as songs, artists, or albums) they tagged. These personal perspectives should be taken into account in MIR tasks that assess the similarity between music items. In this paper, we propose an novel approach for clustering music items represented in social tagging systems. Its characteristic is that it determines similarity between items by preserving the 3-way relationships among the inherent dimensions of the data, i.e., users, items, and tags. Conversely to existing approaches that use reductions to 2-way relationships (between items-users or items-tags), this characteristic allows the proposed algorithm to consider the personal perspectives of tags and to improve the clustering quality. Due to the complexity of social tagging data, we focus on spectral clustering that has been proven effective in addressing complex data. However, existing spectral clustering algorithms work with 2-way relationships. To overcome this problem, we develop a novel data-modeling scheme and a tag-aware spectral clustering procedure that uses tensors (high-dimensional arrays) to store the multi-graph structures that capture the personalised aspects of similarity. Experimental results with data from Last.fm indicate the superiority of the proposed method in terms of clustering quality over conventional spectral clustering approaches that consider only 2-way relationships.""",GRC,education,Developed economies,"[-40.291954, 1.5698965]","[40.770622, 3.9249487]","[-17.729364, 13.941974, 8.831773]","[22.023016, 7.859575, 9.4000845]","[14.407565, 10.331198]","[12.016104, 2.9627926]","[15.384401, 14.228256, 0.0062233214]","[13.329255, 5.955148, 11.74137]"
45,Masatoshi Hamanaka;Satoshi Tojo,Interactive Gttm Analyzer.,2009,https://doi.org/10.5281/zenodo.1415094,Masatoshi Hamanaka+University of Tsukuba>JPN>education;Satoshi Tojo+Japan Advanced Institute of Science and Technology>JPN>education,"We describe an interactive analyzer for the generative theory of tonal music (GTTM). Generally, a piece of music has more than one interpretation, and dealing with such ambiguity is one of the major problems when constructing a music analysis system. To solve this problem, we propose an interactive GTTM analyzer, called an automatic time-span tree analyzer (ATTA), with a GTTM manual editor. The ATTA has adjustable parameters that enable the analyzer to generate multiple analysis results. As the ATTA cannot output all the analysis results that correspond to all the interpretations of a piece of music, we designed a GTTM manual editor, which generates all the analysis results. Experimental results showed that our interactive GTTM analyzer outperformed the GTTM manual editor without an ATTA. Since we hope to contribute to the research of music analysis, we publicize our interactive GTTM analyzer and a dataset of three hundred pairs of a score and analysis results by musicologist on our website http://music.iit.tsukuba.ac.jp/hamanaka/gttm.htm, which is the largest database of analyzed results from the GTTM to date.",JPN,education,Developed economies,"[-4.672893, 44.31059]","[-9.854632, 23.441816]","[-17.75449, -14.278871, -17.628347]","[-14.754306, 1.8634503, 9.939857]","[12.6613865, 6.6250196]","[8.736173, 1.6547083]","[13.841824, 12.970233, -1.3486094]","[10.035965, 6.193336, 11.922655]"
46,Roger B. Dannenberg;Larry A. Wasserman,Estimating the Error Distribution of a Single Tap Sequence without Ground Truth.,2009,https://doi.org/10.5281/zenodo.1417265,Roger B. Dannenberg+Carnegie Mellon University>USA>education;Larry Wasserman+Carnegie Mellon University>USA>education,"""Detecting beats, estimating tempo, aligning scores to audio, and detecting onsets are all interesting problems in the field of music information retrieval. In much of this research, it is convenient to think of beats as occurring at precise time points. However, anyone who has attempted to label beats by hand soon realizes that precise annotation of music audio is not possible. A common method of beat annotation is simply to tap along with audio and record the tap times. This raises the question: How accurate are the taps? It may seem that an answer to this question would require knowledge of “true” beat times. However, tap times can be characterized as a random distribution around true beat times. Multiple independent taps can be used to estimate not only the location of the true beat time, but also the statistical distribution of measured tap times around the true beat time. Thus, without knowledge of true beat times, and without even requiring the existence of precise beat times, we can estimate the uncertainty of tap times. This characterization of tapping can be useful for estimating tempo variation and evaluating alternative annotation methods.""",USA,education,Developed economies,"[42.73784, -16.435492]","[-31.370106, -1.8356826]","[1.6393732, -20.325558, 11.619985]","[-9.599984, 12.704791, -6.1780868]","[8.891513, 8.383233]","[5.344372, 1.5471385]","[11.4713125, 13.194809, -0.40761635]","[7.4181237, 6.675117, 11.15107]"
47,Cory McKay;John Ashley Burgoyne;Jessica Thompson 0001;Ichiro Fujinaga,"Using ACE XML 2.0 to Store and Share Feature, Instance and Class Data for Musical Classification.",2009,https://doi.org/10.5281/zenodo.1418173,Cory McKay+McGill University>CAN>education;John Ashley Burgoyne+McGill University>CAN>education;Jessica Thompson+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper introduces ACE XML 2.0, a set of file formats that are designed to meet the special representational needs of research in automatic music classification. Such standardized formats are needed to facilitate the sharing and long-term storage of valuable research data. ACE XML 2.0 is designed to represent a broad range of musical information clearly using a flexible, extensible, self-contained and formally structured framework. An emphasis is placed on representing extracted feature values, feature descriptions, instance annotations, class ontologies and related metadata.",CAN,education,Developed economies,"[-20.876665, -10.960243]","[4.0677614, 40.252213]","[-14.187193, -5.2284875, 12.384818]","[-8.314534, -7.014042, 22.676287]","[12.736901, 10.152597]","[10.179327, 0.08981901]","[13.982085, 12.858378, -1.4172908]","[10.784372, 4.8107357, 12.006534]"
48,Pablo Cancela;Martín Rocamora;Ernesto López,An Efficient Multi-Resolution Spectral Transform for Music Analysis.,2009,https://doi.org/10.5281/zenodo.1416788,Pablo Cancela+Universidad de la República>URY>education;Martín Rocamora+Universidad de la República>URY>education;Ernesto López+Universidad de la República>URY>education,"In this paper we focus on multi-resolution spectral anal- ysis algorithms for music signals based on the FFT. Two previously devised efﬁcient algorithms (efﬁcient constant- Q transform [1] and multiresolution FFT [2]) are reviewed and compared with a new proposal based on the IIR ﬁl- tering of the FFT. Apart from its simplicity, the proposed method shows to be a good compromise between design ﬂexibility and reduced computational effort. Additionally, it was used as a part of an effective melody extraction al- gorithm.",URY,education,Developing economies,"[-1.3675445, -11.189861]","[-47.24046, -16.42173]","[16.688807, -6.2644515, -15.612728]","[4.293096, -9.43474, -29.989737]","[12.097678, 8.392167]","[6.6289854, 2.6388466]","[12.731792, 13.9054365, -0.17703971]","[9.190108, 8.183886, 10.962699]"
36,Jennifer MacRitchie;Bryony Buck;Nicholas J. Bailey,Visualising Musical Structure Through Performance Gesture.,2009,https://doi.org/10.5281/zenodo.1418205,J. MacRitchie+University of Glasgow>GBR>education|Centre for Music Technology>GBR>facility;B. Buck+University of Glasgow>GBR>education|Centre for Music Technology>GBR>facility;N. J. Bailey+University of Glasgow>GBR>education|Centre for Music Technology>GBR>facility,"A musical performance is seen as the performer’s interpretation of a musical score, illuminating the interaction between the musical structure and implied emotive character. It has been demonstrated that performers’ physical gestures correlate with structural and emotional aspects of the piece they are performing and that this information can be decoded by an audience when presented with a visual-only performance. This paper investigates the relationship between direction of physical movement and underlying musical structures. The Vicon motion capture system is used to record 3D movements made by nine university-level pianists performing Chopin preludes op.28 Nos 6 and 7. The examination of several pianists provides insight into the similarity and differences in gestures between performers and how these relate to structure. Principal Component Analysis (PCA) of these performances and consequent analysis of variance reveals a relationship between extrema of the first six significant components and timing of phrasing structure in Prelude 7 where motion troughs consistently lag behind the occurrence of phrase boundaries in the audio. This relationship is then examined for Prelude 6 which encompasses longer, expanded phrases and changes in rhythm. These expanded phrases are associated with elongated or split gestures, and variations of the motif with changes in movement.",GBR,education,Developed economies,"[-11.432069, 34.291874]","[-30.170942, 7.1732306]","[-5.3980546, -11.106862, -15.848423]","[-9.562851, 6.164888, 0.21872176]","[13.012397, 6.816034]","[7.0004625, 1.5585953]","[13.48455, 13.225539, -2.0272176]","[8.894533, 6.315279, 11.26859]"
22,Laurent Oudre;Yves Grenier;Cédric Févotte,Template-based Chord Recognition : Influence of the Chord Types.,2009,https://doi.org/10.5281/zenodo.1414884,Laurent Oudre+Institut TELECOM>FRA>education;Yves Grenier+Institut TELECOM>FRA>education;Cédric Févotte+Institut TELECOM>FRA>education,"This paper describes a fast and efficient template-based chord recognition method. We introduce three chord models taking into account one or more harmonics for the notes of the chord. The use of pre-determined chord models enables to consider several types of chords (major, minor, dominant seventh, minor seventh, augmented, diminished...). After extracting a chromagram from the signal, the detected chord over a frame is the one minimizing a measure of fit between the chromagram frame and the chord templates. Several popular measures in the probability and signal processing field are considered for our task. In order to take into account the time persistence, we perform a post-processing filtering over the recognition criteria. The transcription tool is evaluated on the 13 Beatles albums with different chord types and compared to state-of-the-art chord recognition methods. We particularly focus on the influence of the chord types considered over the performances of the system. Experimental results show that our method outperforms the state-of-the-art and more importantly is less computationally demanding than the other evaluated systems.",FRA,education,Developed economies,"[54.236652, -4.27412]","[-30.23605, 20.035065]","[25.16077, -14.825102, 17.290749]","[-25.089132, -5.6351767, 3.404229]","[6.944614, 8.673468]","[6.322424, 3.548902]","[12.020254, 10.485807, 1.925444]","[9.769728, 8.8134985, 12.174204]"
9,Matija Marolt,Probabilistic Segmentation and Labeling of Ethnomusicological Field Recordings.,2009,https://doi.org/10.5281/zenodo.1415532,Matija Marolt+University of Ljubljana>SVN>education,"The paper presents a method for segmentation and labeling of ethnomusicological field recordings. Field recordings are integral documents of folk music performances and typically contain interviews with performers intertwined with actual performances. As these are live recordings of amateur folk musicians, they may contain interruptions, false starts, environmental noises or other interfering factors. Our goal was to design a robust algorithm that would approximate manual segmentation of field recordings. First, short audio fragments are classified into one of the following categories: speech, solo singing, choir singing, instrumental or bell chiming performance. Then, a set of candidate segment boundaries is obtained by observing how the energy of the signal and its content change, and finally the recording is segmented with a probabilistic model that maximizes the posterior probability of segments given a set of candidate segment boundaries with their probabilities and prior knowledge of lengths of segments belonging to different categories. Evaluation of the algorithm on a set of field recordings from the Ethnomuse archive is presented.",SVN,education,Developed economies,"[-28.24998, -1.0672071]","[0.26349917, -3.0197117]","[-23.814863, -11.747104, -3.2340333]","[-3.0432088, -3.03744, -2.9633832]","[13.890359, 8.756458]","[8.1608515, 2.928063]","[14.151092, 14.5125475, -1.0665156]","[10.320796, 7.515378, 11.383304]"
20,David Gross-Amblard;Philippe Rigaux;Lylia Abrouk;Nadine Cullot,Fingering Watermarking in Symbolic Digital Scores.,2009,https://doi.org/10.5281/zenodo.1416454,David Gross-Amblard+Université de Bourgogne>FRA>education;Philippe Rigaux+Université de Dauphine>FRA>education;Lylia Abrouk+Université de Bourgogne>FRA>education;Nadine Cullot+Université de Bourgogne>FRA>education,"""We propose a new watermarking method that hides the writer’s identity into symbolic musical scores featuring fingering annotations. These annotations constitute a valuable part of the symbolic representation, yet they can be slightly modified without altering the quality of the musical information. The method applies a controlled distortion of the existing fingerings so that unauthorized copies can be identified. The proposed watermarking method is robust against attacks like random fingering alterations and score cropping, and its detection does not require the original fingering, but only the suspect one. The method is general and applies to various fingering contexts and instruments.""",FRA,education,Developed economies,"[37.37852, 11.420128]","[7.1901383, 36.44398]","[-4.1794105, -38.593334, -1.7035556]","[-3.9513655, -13.835608, 18.270218]","[11.398885, 6.769152]","[8.277399, 0.2541129]","[12.697724, 12.369299, -1.217744]","[10.184919, 5.4294887, 12.555243]"
123,Polina Proutskova;Michael A. Casey,You Call That Singing? Ensemble Classification for Multi-Cultural Collections of Music Recordings.,2009,https://doi.org/10.5281/zenodo.1418183,Polina Proutskova+Goldsmiths>GBR>education;Michael Casey+Dartmouth College>USA>education,"The wide range of vocal styles, musical textures and recording techniques found in ethnomusicological field recordings leads us to consider the problem of automatically labeling the content to know whether a recording is a song or instrumental work. Furthermore, if it is a song, we are interested in labeling aspects of the vocal texture: e.g. solo, choral, acapella or singing with instruments. We present evidence to suggest that automatic annotation is feasible for recorded collections exhibiting a wide range of recording techniques and representing musical cultures from around the world. Our experiments used the Alan Lomax Cantometrics training tapes data set, to encourage future comparative evaluations. Experiments were conducted with a labeled subset consisting of several hundred tracks, annotated at the track and frame levels, as acapella singing, singing plus instruments or instruments only. We trained frame-by-frame SVM classifiers using MFCC features on positive and negative exemplars for two tasks: per-frame labeling of singing and acapella singing. In a further experiment, the frame-by-frame classifier outputs were integrated to estimate the predominant content of whole tracks. Our results show that frame-by-frame classifiers achieved 71% frame accuracy and whole track classifier integration achieved 88% accuracy. We conclude with an analysis of classifier errors suggesting avenues for developing more robust features and classifier strategies for large ethnographically diverse collections.",GBR,education,Developed economies,"[-11.950867, -33.621902]","[10.1971, -23.065178]","[17.612514, 18.627934, -14.698977]","[10.812905, 0.014436814, -19.709373]","[10.475033, 11.253439]","[8.9913645, 3.4346225]","[11.79756, 14.996458, 0.90628266]","[10.944649, 7.3165507, 10.20365]"
122,Emilia Gómez;Martín Haro;Perfecto Herrera,Music and Geography: Content Description of Musical Audio from Different Parts of the World.,2009,https://doi.org/10.5281/zenodo.1416010,Emilia Gómez+Universitat Pompeu Fabra>ESP>education;Martín Haro+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"This paper analyses how audio features related to different musical facets can be useful for the comparative analysis and classification of music from diverse parts of the world. The music collection under study gathers around 6,000 pieces, including traditional music from different geographical zones and countries, as well as a varied set of Western musical styles. We achieve promising results when trying to automatically distinguish music from Western and non-Western traditions. A 86.68% of accuracy is obtained using only 23 audio features, which are representative of distinct musical facets (timbre, tonality, rhythm), indicating their complementarity for music description. We also analyze the relative performance of the different facets and the capability of various descriptors to identify certain types of music. We finally present some results on the relationship between geographical location and musical features in terms of extracted descriptors. All the reported outcomes demonstrate that automatic description of audio signals together with data mining techniques provide means to characterize huge music collections from different traditions, complementing ethnomusicological manual analysis and providing a link between music and geography.",ESP,education,Developed economies,"[-29.303656, 17.6128]","[22.9532, -1.3510617]","[-20.411352, -6.585783, 2.4224484]","[9.335857, 6.513531, 1.0022918]","[14.233092, 8.521413]","[9.392568, 2.5108032]","[14.49542, 14.721918, -1.2774367]","[11.067628, 6.843443, 11.668854]"
121,Sally Jo Cunningham;David M. Nichols,Exploring Social Music Behavior: An Investigation of Music Selection at Parties.,2009,https://doi.org/10.5281/zenodo.1416410,Sally Jo Cunningham+University of Waikato>NZL>education;David M. Nichols+University of Waikato>NZL>education,"This paper builds an understanding how music is currently listened to by small (fewer than 10 individuals) to medium-sized (10 to 40 individuals) gatherings of people—how songs are chosen for playing, how the music fits in with other activities of group members, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of participant observations and interviews focusing on the selection of songs to play at social gatherings. We suggest features for software to support music playing at parties.",NZL,education,Developed economies,"[-37.6802, 22.413824]","[41.591496, 34.01128]","[-19.327402, 20.496582, -4.5833335]","[6.1605277, 16.151947, 19.509287]","[15.181501, 8.886387]","[12.97865, 0.9636947]","[15.233053, 15.202452, -1.3093292]","[13.365375, 4.215822, 11.879289]"
120,Korinna Bade;Andreas Nürnberger;Sebastian Stober;Jörg Garbers;Frans Wiering,Supporting Folk-Song Research by Automatic Metric Learning and Ranking.,2009,https://doi.org/10.5281/zenodo.1418165,Korinna Bade+Otto-von-Guericke University Magdeburg>DEU>education;Andreas Nü rnberger+Otto-von-Guericke University Magdeburg>DEU>education;Sebastian Stober+Otto-von-Guericke University Magdeburg>DEU>education;Jörg Garbers+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education,"In folk song research, appropriate similarity measures can be of great help, e.g. for classification of new tunes. Several measures have been developed so far. However, a particular musicological way of classifying songs is usually not directly reflected by just a single one of these measures. We show how a weighted linear combination of different basic similarity measures can be automatically adapted to a specific retrieval task by learning this metric based on a special type of constraints. Further, we describe how these constraints are derived from information provided by experts. In experiments on a folk song database, we show that the proposed approach outperforms the underlying basic similarity measures and study the effect of different levels of adaptation on the performance of the retrieval system.",DEU,education,Developed economies,"[-28.338577, -5.7404137]","[18.010433, 8.72142]","[2.0155756, 12.438971, -0.6336144]","[8.657176, -0.6621918, 10.044727]","[12.75322, 10.011904]","[10.106406, 1.9956648]","[13.119302, 15.120043, -0.095138334]","[11.6998005, 6.500135, 12.563485]"
119,Meinard Müller;Peter Grosche;Frans Wiering,Robust Segmentation and Annotation of Folk Song Recordings.,2009,https://doi.org/10.5281/zenodo.1417099,Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Peter Grosche+Saarland University>DEU>education|MPI Informatik>DEU>facility;Frans Wiering+Utrecht University>NLD>education,"Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of score-based transcriptions. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer studied in the actual folk song research though they still may contain valuable information. In this paper, we introduce an automated approach for segmenting folk song recordings into its constituent stanzas, which can then be made accessible to folk song researchers by means of suitable visualization, searching, and navigation interfaces. Performed by elderly non-professional singers, the main challenge with the recordings is that most singers have serious problems with the intonation, fluctuating with their voices even over several semitones throughout a song. Using a combination of robust audio features along with various cleaning and audio matching strategies, our approach yields accurate segmentations even in the presence of strong deviations.",DEU,education,Developed economies,"[-3.6534617, -6.5031357]","[1.333725, -4.184992]","[5.651579, -1.7022395, -5.58791]","[-5.4436584, -3.7347245, -3.21132]","[11.770262, 8.446999]","[8.236946, 2.7469642]","[12.534448, 14.466703, 0.054356646]","[10.185976, 7.2672257, 11.6057]"
118,Ruben Hillewaere;Bernard Manderick;Darrell Conklin,Global Feature Versus Event Models for Folk Song Classification.,2009,https://doi.org/10.5281/zenodo.1417829,Ruben Hillewaere+Vrije Universiteit Brussel>BEL>education;Bernard Manderick+Vrije Universiteit Brussel>BEL>education;Darrell Conklin+City University London>GBR>education,"Music classification has been widely investigated in the past few years using a variety of machine learning approaches. In this study, a corpus of 3367 folk songs, divided into six geographic regions, has been created and is used to evaluate two popular yet contrasting methods for symbolic melody classification. For the task of folk song classification, a global feature approach, which summarizes a melody as a feature vector, is outperformed by an event model of abstract event features. The best accuracy obtained on the folk song corpus was achieved with an ensemble of event models. These results indicate that the event model should be the default model of choice for folk song classification.",BEL,education,Developed economies,"[-28.553432, -8.055141]","[18.420433, -5.241588]","[-18.632912, -0.6518773, 18.903395]","[8.540768, 9.517399, -3.8888767]","[12.760864, 10.492117]","[9.435278, 3.0984857]","[13.616969, 14.3540945, 0.9806461]","[11.240637, 7.175687, 11.06013]"
0,J. Stephen Downie;Donald Byrd;Tim Crawford,Ten Years of ISMIR: Reflections on Challenges and Opportunities.,2009,https://doi.org/10.5281/zenodo.1415170,J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Donald Byrd+Indiana University at Bloomington>USA>education;Tim Crawford+Goldsmiths College University of London>GBR>education,"The International Symposium on Music Information Retrieval (ISMIR) was born on 13 August 1999. This paper expresses the opinions of three of ISMIR’s founders as they reflect upon what has happened during its first decade. The paper provides the background context for the events that led to the establishment of ISMIR. We highlight the first ISMIR, held in Plymouth, MA in October of 2000, and use it to elucidate key trends that have influenced subsequent ISMIRs. Indicators of growth and success drawn from ISMIR publication data are presented. The role that the Music Information Retrieval Evaluation eXchange (MIREX) has played at ISMIR is examined. The factors contributing to ISMIR's growth and success are also enumerated. The paper concludes with a set of challenges and opportunities that the newly formed International Society for Music Information Retrieval should embrace to ensure the future vitality of the conference series and the ISMIR community.",USA,education,Developed economies,"[-13.920903, 46.861088]","[27.20198, 34.2702]","[-29.671795, -7.317945, -0.46171528]","[2.0476477, 6.306302, 20.31989]","[13.789633, 5.713817]","[11.878809, 0.41174322]","[15.137833, 12.023497, -1.7174088]","[12.1857605, 4.283248, 12.221075]"
1,David Bretherton;Daniel A. Smith;Monica M. C. Schraefel;Richard Polfreman;Mark Everist;Jeanice Brooks;Joe Lambert,Integrating Musicology's Heterogeneous Data Sources for Better Exploration.,2009,https://doi.org/10.5281/zenodo.1415168,David Bretherton+University of Southampton>GBR>education;Daniel Alexander Smith+University of Southampton>GBR>education;mc schraefel+University of Southampton>GBR>education;Richard Polfreman+University of Southampton>GBR>education;Mark Everist+University of Southampton>GBR>education;Jeanice Brooks+University of Southampton>GBR>education;Joe Lambert+University of Southampton>GBR>education,"Musicologists have to consult an extraordinarily heterogeneous body of primary and secondary sources during all stages of their research. Many of these sources are now available online, but the historical dispersal of material across libraries and archives has now been replaced by segregation of data and metadata into a plethora of online repositories. This segregation hinders the intelligent manipulation of metadata, and means that extracting large tranches of basic factual information or running multi-part search queries is still enormously and needlessly time consuming. To counter this barrier to research, the “musicSpace” project is experimenting with integrating access to many of musicology’s leading data sources via a modern faceted browsing interface that utilises Semantic Web and Web2.0 technologies such as RDF and AJAX. This will make previously intractable search queries tractable, enable musicologists to use their time more efficiently, and aid the discovery of potentially significant information that users did not think to look for. This paper outlines our work to date.",GBR,education,Developed economies,"[-25.44412, 16.594229]","[19.453516, 36.101345]","[-17.363176, 3.3438952, -5.078837]","[-3.3305879, -0.5056714, 23.03913]","[14.129298, 8.364152]","[11.06666, 0.1454105]","[14.493991, 14.426988, -1.7858038]","[11.944542, 4.941843, 12.113405]"
2,Kurt Jacobson;Yves Raimond;Mark B. Sandler,An Ecosystem for Transparent Music Similarity in an Open World.,2009,https://doi.org/10.5281/zenodo.1417151,Kurt Jacobson+Queen Mary University of London>GBR>education;Yves Raimond+BBC>GBR>company;Mark Sandler+Queen Mary University of London>GBR>education,"There exist many methods for deriving music similarity associations and additional variations are likely to be seen in the future. In this work we introduce the Similarity Ontology for describing associations between items. Using a combination of RDF/OWL and N3, our ontology allows for transparency and provenance tracking in a distributed and open system. We describe a similarity ecosystem where agents assert and aggregate similarity statements on the Web of Data allowing a client application to make queries for recommendation, playlisting, or other tasks. In this ecosystem any number of similarity derivation methods can exist side-by-side, specifying similarity relationships as well as the processes used to derive these statements. The data consumer can then select which similarity statements to trust based on knowledge of the similarity derivation processes or a list of trusted assertion agents.",GBR,education,Developed economies,"[-7.0101185, 16.816195]","[16.953323, 36.34275]","[-8.884448, 11.208769, -2.2420216]","[-6.200788, -0.5874221, 25.932682]","[13.538246, 9.271455]","[10.772387, -0.0607252]","[13.857634, 15.047535, -0.73193634]","[12.304194, 5.3795233, 11.900476]"
3,Andrew Hankinson;Laurent Pugin;Ichiro Fujinaga,Interfaces for Document Representation in Digital Music Libraries.,2009,https://doi.org/10.5281/zenodo.1414780,Andrew Hankinson+Schulich School of Music of McGill University>CAN>education|RISM>CHE>Unknown;Laurent Pugin+RISM>CHE>Unknown;Ichiro Fujinaga+Schulich School of Music of McGill University>CAN>education,"Musical documents, that is, documents whose primary content is printed music, introduce interesting design challenges for presentation in an online environment. Considerations for the unique properties of printed music, as well as users’ expected levels of comfort with these materials, present opportunities for developing a viewer specifically tailored to displaying musical documents. This paper outlines five design considerations for a music document viewer, drawing examples from existing digital music libraries. We then present our work towards incorporating these considerations in a new digital music library system currently under development.",CAN,education,Developed economies,"[-22.2333, 32.289715]","[24.017025, 28.132977]","[-21.864037, 5.678975, -19.154505]","[5.5937324, -4.1103888, 23.070158]","[14.455099, 7.3558702]","[10.852334, 0.7688407]","[14.502805, 13.995194, -2.3740168]","[11.494142, 4.91465, 12.653337]"
4,Rolf Inge Godøy;Alexander Refsum Jensenius,Body Movement in Music Information Retrieval.,2009,https://doi.org/10.5281/zenodo.1416668,Rolf Inge Godøy+University of Oslo>NOR>education|FourMs>Unknown>Unknown;Alexander Refsum Jensenius+University of Oslo>NOR>education|FourMs>Unknown>Unknown,"We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations. There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions. The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR.",NOR,education,Developed economies,"[-18.305706, 19.1643]","[9.316734, 45.11958]","[-10.11747, 5.590457, -6.8756127]","[-3.6132743, 15.773268, 14.560318]","[13.884834, 7.970435]","[11.198972, 0.85549325]","[13.829868, 14.555347, -2.031053]","[11.494058, 4.686837, 11.36582]"
5,Maarten Grachten;Gerhard Widmer,Who Is Who in the End? Recognizing Pianists by Their Final Ritardandi.,2009,https://doi.org/10.5281/zenodo.1417109,Maarten Grachten+Johannes Kepler University>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence>AUT>facility|Johannes Kepler University>AUT>education,"The performance of music usually involves a great deal of interpretation by the musician. In classical music, final ritardandi are emblematic for the expressive aspect of music performance. In this paper we investigate to what degree individual performance style has an effect on the form of final ritardandi. To this end we look at interonset-interval deviations from a performance norm. We define a criterion for filtering out deviations that are likely to be due to measurement error. Using a machine-learning classifier, we evaluate an automatic pairwise pianist identification task as an initial assessment of the suitability of the filtered data for characterizing the individual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, pianists can often be identified with accuracy significantly above baseline.",AUT,education,Developed economies,"[38.504215, 15.557155]","[-32.60068, 7.7481403]","[16.839022, -4.4576697, 25.810703]","[-12.872708, 3.6995902, -1.7164924]","[9.979721, 6.804311]","[7.7307463, 3.6460629]","[12.207069, 11.5048275, -0.65497226]","[9.172935, 6.201497, 10.738374]"
21,Yi-Hsuan Yang;Yu-Ching Lin;Ann Lee;Homer H. Chen,Improving Musical Concept Detection by Ordinal Regression and Context Fusion.,2009,https://doi.org/10.5281/zenodo.1416650,Yi-Hsuan Yang+National Taiwan University>TWN>education;Yu-Ching Lin+National Taiwan University>TWN>education;Ann Lee+National Taiwan University>TWN>education;Homer Chen+National Taiwan University>TWN>education,"To facilitate information retrieval of large-scale music databases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept detection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label classification methods. To further improve the detection accuracy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selection to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924.",TWN,education,Developing economies,"[-24.192322, -7.5483627]","[35.407284, -2.382335]","[-7.9646373, 4.641166, 14.287054]","[19.000647, 7.155915, -0.33514637]","[12.6529455, 10.127625]","[11.009205, 3.4475842]","[13.465924, 14.301245, 0.37271267]","[12.606989, 6.5388055, 11.112303]"
6,Jin Ha Lee;M. Cameron Jones;J. Stephen Downie,"An Analysis of ISMIR Proceedings: Patterns of Authorship, Topic, and Citation.",2009,https://doi.org/10.5281/zenodo.1416618,Jin Ha Lee+University of Washington>USA>education;M. Cameron Jones+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"This paper presents analyses of peer-reviewed papers and posters published in the past nine years of ISMIR proceedings: examining publication and authorship practices, topics and titles of research, as well as the citation patterns among the ISMIR proceedings. The main objective is to provide an overview of the progress made over the past nine years in the ISMIR community and to obtain some insights into where the community should be heading in the coming years. Overall, the ISMIR community has grown considerably over the past nine years, both in the number of papers and posters published each year, as well as the number of authors contributing. Furthermore, the amount of collaboration among authors, as reflected in co-authorship, has increased. Main areas of research are revealed by an analysis of most commonly used title terms. Also, major authors and research groups are identified by analyzing the co-authorship and citation patterns in ISMIR proceedings.",USA,education,Developed economies,"[-14.095808, 46.347668]","[29.087719, 40.91764]","[-29.963903, -7.43061, -2.6818314]","[3.5962086, 8.223439, 22.898405]","[13.794822, 5.6767497]","[11.969711, 0.3435778]","[15.160449, 11.9873295, -1.7243516]","[12.28581, 4.125453, 12.192]"
8,Meinard Müller;Verena Konz;Andi Scharfstein;Sebastian Ewert;Michael Clausen,Towards Automated Extraction of Tempo Parameters from Expressive Music Recordings.,2009,https://doi.org/10.5281/zenodo.1416024,Meinard Müller+Saarland University>DEU>education;Verena Konz+Saarland University>DEU>education;Andi Scharfstein+Saarland University>DEU>education;Sebastian Ewert+Bonn University>DEU>education;Michael Clausen+Bonn University>DEU>education,"A performance of a piece of music heavily depends on the musician’s or conductor’s individual vision and personal interpretation of the given musical score. As basis for the analysis of artistic idiosyncrasies, one requires accurate annotations that reveal the exact timing and intensity of the various note events occurring in the performances. In the case of audio recordings, this annotation is often done manually, which is prohibitive in view of large music collections. In this paper, we present a fully automatic approach for extracting temporal information from a music recording using score-audio synchronization techniques. This information is given in the form of a tempo curve that reveals the relative tempo difference between an actual performance and some reference representation of the underlying musical piece. As shown by our experiments on harmony-based Western music, our approach allows for capturing the overall tempo flow and for certain classes of music even finer expressive tempo nuances.",DEU,education,Developed economies,"[38.759354, -26.710829]","[-27.20836, -0.1292721]","[0.62533706, -27.007895, -3.2182052]","[-7.7496057, 7.4034314, -5.059901]","[11.394609, 4.460745]","[5.767942, 1.2944796]","[10.872573, 13.343874, -2.7455084]","[8.035755, 6.228869, 11.258988]"
49,Mert Bay;Andreas F. Ehmann;J. Stephen Downie,Evaluation of Multiple-F0 Estimation and Tracking Systems.,2009,https://doi.org/10.5281/zenodo.1418241,Mert Bay+University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"""Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music information retrieval systems. This paper presents the systematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music. The evaluations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008. The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed.""",USA,education,Developed economies,"[41.40592, -17.63953]","[-4.6336813, -8.471784]","[2.550143, -23.967283, 10.904152]","[6.1479325, -11.381314, -8.734691]","[8.913937, 8.611336]","[6.9606576, 2.4561658]","[11.138324, 13.42593, -0.29940465]","[9.241192, 7.81476, 11.260508]"
10,Thibault Langlois;Gonçalo Marques,A Music Classification Method based on Timbral Features.,2009,https://doi.org/10.5281/zenodo.1417863,Thibault Langlois+Faculdade de Ciências da Universidade de Lisboa>PRT>education;Gonçalo Marques+Instituto Superior de Engenharia de Lisboa>PRT>education,"This paper describes a method for music classification based solely on the audio contents of the music signal. More specifically, the audio signal is converted into a compact symbolic representation that retains timbral characteristics and accounts for the temporal structure of a music piece. Models that capture the temporal dependencies observed in the symbolic sequences of a set of music pieces are built using a statistical language modeling approach. The proposed method is evaluated on two classification tasks (Music Genre classification and Artist Identification) using publicly available datasets. Finally, a distance measure between music pieces is derived from the method and examples of playlists generated using this distance are given. The proposed method is compared with two alternative approaches which include the use of Hidden Markov Models and a classification scheme that ignores the temporal structure of the sequences of symbols. In both cases the proposed approach outperforms the alternatives.",PRT,education,Developed economies,"[-23.22159, -13.503441]","[8.478119, -12.5428915]","[-10.102228, 1.114483, 15.477626]","[8.927043, 2.7230194, -14.740889]","[12.496648, 10.468268]","[8.692674, 3.0998223]","[13.507928, 14.028606, 1.0297575]","[10.775393, 7.483453, 11.030634]"
11,Frieder Stolzenburg,A Periodicity-based Theory for Harmony Perception and Scales.,2009,https://doi.org/10.5281/zenodo.1414904,Frieder Stolzenburg+Hochschule Harz>DEU>education,"Empirical results demonstrate, that human subjects rate harmonies, e.g. major and minor triads, differently with respect to their sonority. These judgements of listeners have a strong psychophysical basis. Therefore, harmony perception often is explained by the notions of dissonance and tension, computing the consonance of one or two intervals. In this paper, a theory on harmony perception based on the notion of periodicity is introduced. Mathematically, periodicity is derivable from the frequency ratios of the tones in the chord with respect to its lowest tone. The used ratios can be computed by continued fraction expansion and are psychophysically motivated by the just noticeable differences in pitch perception. The theoretical results presented here correlate well to experimental results and also explain the origin of complex chords and common musical scales.",DEU,education,Developed economies,"[28.300112, 22.45088]","[-23.042107, 16.181105]","[-0.557979, -12.708252, 32.980328]","[-21.954021, -7.314242, 8.281986]","[9.823767, 9.228866]","[7.683838, 2.3811033]","[11.665153, 13.9929905, -0.9912572]","[10.099649, 7.9827995, 12.499945]"
12,Yoshiyuki Kobayashi,Automatic Generation of Musical Instrument Detector by Using Evolutionary Learning Method.,2009,https://doi.org/10.5281/zenodo.1417233,Yoshiyuki Kobayashi+SONY Corporation>JPN>company,"This paper presents a novel way of generating information extractors that obtain high-level information from recorded music such as the presence of a certain musical instrument. Our information extractor is comprised of a feature set and a discrimination or regression formula. We introduce a scheme to generate the entire information extractor given only a large amount of labeled dataset. For example, data could be waveform, and label could be the presence of musical instruments in them. We propose a very flexible description of features that allows various kinds of data other than waveform. Our proposal also includes a modified evolutionary learning method to optimize the feature set. We applied our scheme to automatically generate musical instrument detectors for mixed-down music in stereo. The experiment showed that our scheme could find a suitable set of features for the objective and could generate good detectors.",JPN,company,Developed economies,"[8.834115, -20.936478]","[10.072858, -7.2992573]","[14.225259, -6.7905684, -4.423551]","[8.508862, 1.7954102, -5.720911]","[8.92518, 7.1710167]","[9.04632, 2.9942136]","[11.1522, 12.541747, 0.48228922]","[10.748512, 6.924438, 10.792536]"
13,Andre Holzapfel;Yannis Stylianou,Rhythmic Similarity in Traditional Turkish Music.,2009,https://doi.org/10.5281/zenodo.1417861,"Andre Holzapfel+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education;Yannis Stylianou+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education","In this paper, the problem of automatically assigning a piece of traditional Turkish music into a class of rhythm referred to as usul is addressed. For this, an approach for rhythmic similarity measurement based on scale transforms has been evaluated on a set of MIDI data. Because this task is related to time signature estimation, the accuracy of the proposed method is evaluated and compared with a state of the art time signature estimation approach. The results indicate that the proposed method can be successfully applied to audio signals of Turkish music and that it captures relevant properties of the individual usul.",GRC,facility,Developed economies,"[3.7791102, 12.13932]","[-20.220861, 0.65055877]","[-12.217532, -21.07306, -10.870574]","[2.0703723, 12.770402, -3.1308494]","[11.954957, 9.82937]","[6.2847195, 1.6603823]","[12.280107, 15.021953, -1.4215481]","[8.354512, 6.8078127, 11.815623]"
14,Emmanouil Benetos;Andre Holzapfel;Yannis Stylianou,Pitched Instrument Onset Detection based on Auditory Spectra.,2009,https://doi.org/10.5281/zenodo.1416174,"Emmanouil Benetos+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education;André Holzapfel+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education;Yannis Stylianou+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education","In this paper, a novel method for onset detection of music signals using auditory spectra is proposed. The auditory spectrogram provides a time-frequency representation that employs a sound processing model resembling the human auditory system. Recent work on onset detection employs DFT-based features, such as the spectral flux and group delay function. The spectral flux and group delay are introduced in the auditory framework and an onset detection algorithm is proposed. Experiments are conducted on a dataset covering 11 pitched instrument types, consisting of 1829 onsets in total. Results indicate the superiority of the auditory representations over the DFT-based ones, with the auditory spectral flux exhibiting an onset detection improvement by 2% in terms of F-measure when compared to the DFT-based feature.",GRC,facility,Developed economies,"[28.632074, -24.942438]","[-23.001719, -5.2087145]","[10.18442, -18.058516, -6.5852375]","[-0.43483675, 7.9379416, -11.404345]","[10.196723, 5.2632394]","[5.649781, 2.4492428]","[10.520801, 13.199751, -1.2126493]","[7.9445305, 7.573661, 10.894601]"
15,Dominikus Baur;Tim Langer;Andreas Butz,Shades of Music: Letting Users Discover Sub-Song Similarities.,2009,https://doi.org/10.5281/zenodo.1417844,Dominikus Baur+University of Munich (LMU)>DEU>education;Tim Langer+University of Munich (LMU)>DEU>education;Andreas Butz+University of Munich (LMU)>DEU>education,"Many interesting pieces of music violate established structures or rules of their genre on purpose. These songs can be very atypical in their interior structure and their different parts might actually allude to entirely different other songs or genres. We present a query-by-example-based user interface that shows songs related to the one currently playing. This relation is not based on overall similarity, but on the similarity between the part currently playing and parts of other songs in the collection along different dimensions (pitch, timbre, bars, beats, loudness). The similarity is initially computed automatically, but can be corrected by the user. Once a sufficient number of corrections has been made, we expect the similarity measure to reach an even higher precision. Our system thereby allows users to discover hidden similarities on the level of song sections instead of whole songs.",DEU,education,Developed economies,"[-8.102437, 14.839233]","[24.663277, 11.605089]","[-6.9676523, 13.482118, -2.9939046]","[13.826806, -1.3161365, 7.066324]","[13.550444, 9.322954]","[10.655385, 2.01972]","[13.875149, 15.155723, -0.78444856]","[12.009279, 6.17649, 12.574399]"
16,Norberto Degara-Quintela;Antonio S. Pena;Soledad Torres-Guijarro,A Comparison of Score-Level Fusion Rules for Onset Detection in Music Signals.,2009,https://doi.org/10.5281/zenodo.1415692,Norberto Degara-Quintela+University of Vigo>ESP>education;Antonio Pena+University of Vigo>ESP>education;Soledad Torres-Guijarro+Laboratorio Oficial de Metroloxía de Galicia (LOMG)>ESP>facility,"Finding automatically the starting time of audio events is a difficult process. A promising approach for onset detection lies in the combination of multiple algorithms. The goal of this paper is to compare score-level fusion rules that combine signal processing algorithms in a problem of automatic detection of onsets. Previous approaches usually combine detection functions by adding these functions in the time domain. The combination methods explored in this work fuse, at score-level, the peak score information (peak time and onset probability) in order to obtain a better estimate of the probability of having an onset given the probability estimates of multiple experts. Three state-of-the-art spectral-based onset detection functions are used: a spectral flux detection function, a weighted phase deviation function, and a complex domain detection function. Both untrained and trained fusion rules will be compared using a standard data set of music excerpts.",ESP,education,Developed economies,"[29.83171, -27.580856]","[-23.286804, -6.758448]","[6.408696, -20.679848, -8.298093]","[-2.7728364, 6.271123, -11.689736]","[10.303115, 5.0046816]","[5.633326, 2.4304647]","[10.364661, 13.288907, -1.5056434]","[7.930849, 7.5042434, 10.864369]"
17,Yajie Hu;Xiaoou Chen;Deshun Yang,Lyric-based Song Emotion Detection with Affective Lexicon and Fuzzy Clustering Method.,2009,https://doi.org/10.5281/zenodo.1417983,Yajie Hu+Peking University>CHN>education;Xiaoou Chen+Peking University>CHN>education;Deshun Yang+Peking University>CHN>education,"A method is proposed for detecting the emotions of Chinese song lyrics based on an affective lexicon. The lexicon is composed of words translated from ANEW and words selected by other means. For each lyric sentence, emotion units, each based on an emotion word in the lexicon, are found out, and the influences of modifiers and tenses on emotion units are taken into consideration. The emotion of a sentence is calculated from its emotion units. To figure out the prominent emotions of a lyric, a fuzzy clustering method is used to group the lyric’s sentences according to their emotions. The emotion of a cluster is worked out from that of its sentences considering the individual weight of each sentence. Clusters are weighted according to the weights and confidences of their sentences and singing speeds of sentences are considered as the adjustment of the weights of clusters. Finally, the emotion of the cluster with the highest weight is selected from the prominent emotions as the main emotion of the lyric. The performance of our approach is evaluated through an experiment of emotion classification of 500 Chinese song lyrics.",CHN,education,Developing economies,"[-53.181805, -0.116752]","[57.45177, -5.1054335]","[-17.650803, 25.338072, 10.475391]","[15.7644005, 26.122028, 10.255805]","[13.471523, 12.640004]","[12.940275, 3.7565901]","[16.00866, 14.821323, 1.5755501]","[14.181096, 5.307436, 10.854931]"
18,Klaus Seyerlehner;Peter Knees;Dominik Schnitzer;Gerhard Widmer,Browsing Music Recommendation Networks.,2009,https://doi.org/10.5281/zenodo.1416972,Klaus Seyerlehner+Johannes Kepler University>AUT>education;Peter Knees+Johannes Kepler University>AUT>education;Dominik Schnitzer+Austrian Research Institute for AI>AUT>facility;Gerhard Widmer+Austrian Research Institute for AI>AUT>facility,Many music portals offer the possibility to explore music collections via browsing automatically generated music recommendations. In this paper we argue that such music recommender systems can be transformed into an equivalent recommendation graph. We then analyze the recommendation graph of a real-world content-based music recommender systems to find out if users can really explore the underlying song database by following those recommendations. We find that some songs are not recommended at all and are consequently not reachable via browsing. We then take a first attempt to modify a recommendation network in such a way that the resulting network is better suited to explore the respective music space.,AUT,education,Developed economies,"[-45.74902, 26.882753]","[36.465313, 16.819263]","[-10.2751, 23.442057, -12.479261]","[19.485472, 3.5112875, 12.439752]","[15.870146, 9.177204]","[12.207547, 2.0624108]","[15.758541, 15.679532, -1.469059]","[13.185268, 5.3566055, 12.51032]"
19,Hiromi Ishizaki;Keiichiro Hoashi;Yasuhiro Takishima,Full-Automatic DJ Mixing System with Optimal Tempo Adjustment based on Measurement Function of User Discomfort.,2009,https://doi.org/10.5281/zenodo.1418231,Hiromi Ishizaki+KDDI R&D Laboratories Inc.>JPN>company;Keiichiro Hoashi+KDDI R&D Laboratories Inc.>JPN>company;Yasuhiro Takishima+KDDI R&D Laboratories Inc.>JPN>company,"This paper proposes an automatic DJ mixing method that can automate the processes of real world DJs and describes a prototype for a fully automatic DJ mix-like playing system. Our goal is to achieve a fully automatic DJ mixing system that can preserve overall user comfort level during DJ mixing. In this paper, we assume that the difference between the original and adjusted songs is the main cause of user discomfort in the mixed song. In order to preserve user comfort, we define the measurement function of user discomfort based on the results of a subjective experiment. Furthermore, this paper proposes a unique tempo adjustment technique called “optimal tempo adjustment”, which is robust for any combination of tempi of songs to be mixed. In the subjective experiment, the proposed method obtained higher averages of user ratings on three evaluation items compared to the conventional method. These results indicate that our system is able to preserve user comfort.",JPN,company,Developed economies,"[18.585375, -39.886154]","[-12.383216, 47.74684]","[9.073754, -28.068058, -14.679647]","[-5.506828, 21.128567, 17.68829]","[9.9554205, 4.6194086]","[7.240234, 6.2498813]","[10.631697, 12.276097, -1.9238452]","[7.972571, 6.8229976, 10.787396]"
7,Maarten Grachten;Markus Schedl;Tim Pohle;Gerhard Widmer,The ISMIR Cloud: A Decade of ISMIR Conferences at Your Fingertips.,2009,https://doi.org/10.5281/zenodo.1416434,Maarten Grachten+Johannes Kepler University>AUT>education;Markus Schedl+Johannes Kepler University>AUT>education;Tim Pohle+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"In this paper, we analyze the proceedings of the past International Symposia on Music Information Retrieval (ISMIR). We extract meaningful term sets from the accepted submissions and apply term weighting and Web-based filtering techniques to distill information about the topics covered by the papers. This enables us to visualize and interpret the change of hot ISMIR topics in the course of time. Furthermore, the performed analysis allows for assessing the cumulative ISMIR proceedings by semantic content (rather than by literal text search). To illustrate this, we introduce two prototype applications that are publicly accessible online. The first allows the user to search for ISMIR publications by selecting subsets of ISMIR topics. The second provides interactive visual access to the joint content of ISMIR publications in the form of a tag cloud – the ISMIR Cloud.",AUT,education,Developed economies,"[-13.41399, 46.361313]","[22.74732, 19.150965]","[-30.548443, -8.923865, -0.8413472]","[9.607807, -4.6390047, 17.707993]","[13.800525, 5.719072]","[10.83185, 1.0267204]","[15.147471, 12.009501, -1.7308141]","[11.929799, 5.4199233, 12.750783]"
50,Ferdinand Fuhrmann;Martín Haro;Perfecto Herrera,"Scalability, Generality and Temporal Aspects in Automatic Recognition of Predominant Musical Instruments in Polyphonic Music.",2009,https://doi.org/10.5281/zenodo.1416394,"Ferdinand Fuhrmann+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Martín Haro+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education","In this paper we present an approach towards the classification of pitched and unpitched instruments in polyphonic audio. In particular, the presented study accounts for three aspects currently lacking in literature: model scalability to polyphonic data, model generalisation in respect to the number of instruments, and incorporation of perceptual information. Therefore, our goal is a unifying recognition framework which enables the extraction of the main instruments’ information. The applied methodology consists of training classifiers with audio descriptors, using extensive datasets to model the instruments sufficiently. All data consist of real world music, including categories of 11 pitched and 3 percussive instruments. We designed our descriptors by temporal integration of the raw feature values, which are directly extracted from the polyphonic data. Moreover, to evaluate the applicability of modelling temporal aspects in polyphonic audio, we studied the performance of different encodings of the temporal information. Along with accuracies of 63% and 78% for the pitched and percussive classification task, results show both the importance of temporal encoding as well as strong limitations of modelling it accurately.",ESP,education,Developed economies,"[7.7072086, -21.435085]","[-10.416381, -1.3959216]","[16.400595, -4.232126, -3.919254]","[8.559946, 2.3964164, -9.935541]","[8.907181, 7.263318]","[8.797015, 3.552542]","[11.100434, 12.570023, 0.4204549]","[10.482862, 7.44336, 10.406087]"
60,Cyril Laurier;Mohamed Sordo;Joan Serrà;Perfecto Herrera,Music Mood Representations from Social Tags.,2009,https://doi.org/10.5281/zenodo.1415600,"Cyril Laurier+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Mohamed Sordo+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Joan Serrà+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education","This paper presents findings about mood representations. We aim to analyze how do people tag music by mood, to create representations based on this data and to study the agreement between experts and a large community. For this purpose, we create a semantic mood space from last.fm tags using Latent Semantic Analysis. With an unsupervised clustering approach, we derive from this space an ideal categorical representation. We compare our community based semantic space with expert representations from Hevner and the clusters from the MIREX Audio Mood Classification task. Using dimensional reduction with a Self-Organizing Map, we obtain a 2D representation that we compare with the dimensional model from Russell. We present as well a tree diagram of the mood tags obtained with a hierarchical clustering approach. All these results show a consistency between the community and the experts as well as some limitations of current expert models. This study demonstrates a particular relevancy of the basic emotions model with four mood clusters that can be summarized as: happy, sad, angry and tender. This outcome can help to create better ground truth and to provide more realistic mood classification algorithms. Furthermore, this method can be applied to other types of representations to build better computational models.",ESP,education,Developed economies,"[-50.571007, 3.3023975]","[49.778126, -2.9901428]","[-19.93516, 19.849789, 2.5010712]","[16.289381, 20.62408, 7.8334026]","[13.645044, 12.344531]","[13.032373, 3.7224724]","[16.034334, 14.80454, 1.3072284]","[14.181016, 5.2258277, 10.755324]"
52,Zhiyao Duan;Jinyu Han;Bryan Pardo,Harmonically Informed Multi-Pitch Tracking.,2009,https://doi.org/10.5281/zenodo.1418045,Zhiyao Duan+Northwestern University>USA>education;Jinyu Han+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"""This paper presents a novel system for multi-pitch tracking, i.e. estimate the pitch trajectory of each monophonic source in a mixture of harmonic sounds. The system consists of two stages: multi-pitch estimation and pitch trajectory formation. In the first stage, we propose a new approach based on modeling spectral peaks and non-peak regions to estimate pitches and polyphony in each single frame. In the second stage, we view the pitch trajectory formation problem as a constrained clustering problem of pitch estimates in all the frames. Constraints are imposed on some pairs of pitch estimates, according to time and frequency proximity. In clustering, harmonic structure is employed as the feature. The proposed system is tested on 10 recorded four-part J. S. Bach chorales. Both multi-pitch estimation and tracking results are very promising. In addition, for multi-pitch estimation, the proposed system is shown to outperform a state-of-the-art multi-pitch estimation approach.""",USA,education,Developed economies,"[26.47034, -22.926353]","[-7.0051804, -9.93166]","[11.942682, -16.323656, -10.715896]","[1.1816331, -11.531024, -13.770849]","[10.013373, 5.555519]","[6.732712, 2.8076518]","[10.775606, 13.167991, -0.9200771]","[9.03743, 7.9880342, 10.824444]"
88,W. Bas de Haas;Martin Rohrmeier;Remco C. Veltkamp;Frans Wiering,Modeling Harmonic Similarity Using a Generative Grammar of Tonal Harmony.,2009,https://doi.org/10.5281/zenodo.1416212,W. Bas de Haas+Utrecht University>NLD>education;Martin Rohrmeier+University of Cambridge>GBR>education;Remco C. Veltkamp+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education,"In this paper we investigate a new approach to the similarity of tonal harmony. We create a fully functional remodeling of an earlier version of Rohrmeier’s grammar of harmony. With this grammar an automatic harmonic analysis of a sequence of symbolic chord labels is obtained in the form of a parse tree. The harmonic similarity is determined by finding and examining the largest labeled common embeddable subtree (LLCES) of two parse trees. For the calculation of the LLCES a new O(min(n, m)nm) time algorithm is presented, where n and m are the sizes of the trees. For the analysis of the LLCES we propose six distance measures that exploit several structural characteristics of the Combined LLCES. We demonstrate in a retrieval experiment that at least one of these new methods significantly outperforms a baseline string matching approach and thereby show that using additional musical knowledge from music cognitive and music theoretic models actually helps improving retrieval performance.",NLD,education,Developed economies,"[27.145737, 23.198282]","[-23.275362, 18.87292]","[-2.823818, -11.170304, 32.179977]","[-22.68203, -2.5129716, 8.283977]","[9.828862, 9.251947]","[7.779394, 2.2910218]","[11.70718, 14.062434, -0.93064183]","[10.236044, 7.968742, 12.708202]"
89,Christopher Raphael,Symbolic and Structural Representation of Melodic Expression.,2009,https://doi.org/10.5281/zenodo.1417817,"Christopher Raphael+Indiana University, Bloomington>USA>education","A method for expressive melody synthesis is presented seeking to capture the structural and prosodic (stress, direction, and grouping) elements of musical interpretation. The interpretation of melody is represented through a hierarchical structural decomposition and a note-level prosodic annotation. An audio performance of the melody is constructed using the time-evolving frequency and intensity functions. A method is presented that transforms the expressive annotation into the frequency and intensity functions, thus giving the audio performance. In this framework, the problem of expressive rendering is cast as estimation of structural decomposition and the prosodic annotation. Examples are presented on a dataset of around 50 folk-like melodies, realized both from hand-marked and estimated annotations.",USA,education,Developed economies,"[5.4127755, 19.461382]","[-9.853113, 9.591778]","[3.7968042, 2.6738641, 0.40499142]","[-10.104341, -1.6924919, -4.0187974]","[12.097723, 9.743286]","[8.602475, 2.5163639]","[12.591606, 15.403028, -0.93622386]","[10.217414, 6.6214495, 11.296401]"
90,Maksim Khadkevich;Maurizio Omologo,Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.,2009,https://doi.org/10.5281/zenodo.1415930,"Maksim Khadkevich+FBK-irst, Università degli studi di Trento>ITA>education;Maurizio Omologo+Fondazione Bruno Kessler-irst>ITA>facility","""This paper focuses on automatic extraction of acoustic chord sequences from a musical piece. Standard and factored language models are analyzed in terms of applicability to the chord recognition task. Pitch class profile vectors that represent harmonic information are extracted from the given audio signal. The resulting chord sequence is obtained by running a Viterbi decoder on trained hidden Markov models and subsequent lattice rescoring, applying the language model weight. We performed several experiments using the proposed technique. Results obtained on 175 manually-labeled songs provided an increase in accuracy of about 2%.""",ITA,education,Developed economies,"[56.033012, -5.2911763]","[-32.34603, 19.751387]","[27.662714, -12.069306, 19.60513]","[-24.393019, -6.879191, 0.19308384]","[6.6469183, 8.735792]","[6.1452913, 3.6815033]","[11.897074, 10.396598, 2.0627792]","[9.7157545, 8.83289, 12.103211]"
91,Sam Ferguson;Densil Cabrera,Auditory Spectral Summarisation for Audio Signals with Musical Applications.,2009,https://doi.org/10.5281/zenodo.1417393,"Sam Ferguson+University of Technology, Sydney>AUS>education;Densil Cabrera+The University of Sydney>AUS>education","Methods for spectral analysis of audio signals and their graphical display are widespread. However, assessing music and audio in the visual domain involves a number of challenges in the translation between auditory images into mental or symbolically represented concepts. This paper presents a spectral analysis method that exists entirely in the auditory domain, and results in an auditory presentation of a spectrum. It aims to strip a segment of audio signal of its temporal content, resulting in a quasi-stationary signal that possesses a similar spectrum to the original signal. The method is extended and applied for the purpose of music summarisation.",AUS,education,Developed economies,"[-2.1354098, -12.260135]","[-26.892843, 11.291186]","[14.019867, -6.445044, -15.940912]","[-8.614516, -8.243506, -0.22970021]","[12.260564, 8.186671]","[7.8925924, 2.8896575]","[12.779658, 13.859632, -0.441265]","[10.045274, 7.6103177, 11.355799]"
92,Cynthia C. S. Liem;Alan Hanjalic,Cover Song Retrieval: A Comparative Study of System Component Choices.,2009,https://doi.org/10.5281/zenodo.1414896,Cynthia C. S. Liem+Delft University of Technology>NLD>education;Alan Hanjalic+Delft University of Technology>NLD>education,"The Cover Song Retrieval (CSR) problem has received considerable attention in the MIREX 2006-2008 evaluation sessions. While the reported performance figures provide a general idea about the strengths of the submitted systems, it is not clear what actually causes the reported performance of a certain system. In other words, the question arises whether some system component design choices are more critical for a system’s performance results than others. In order to obtain a better understanding of the performance of current CSR approaches and to give recommendations for future research in the field of CSR, we designed and performed a comparative study involving system component design approaches from the best-performing systems in MIREX 2006 and 2007. The datasets used for evaluation were carefully chosen to cover the broad spectrum of the cover song domain, while still providing designated test cases. While the choice of the dissimilarity assessment method was found to cause the largest CSR performance boost and very good retrieval results were obtained on classical opus retrieval cases, results obtained on a new test case, involving recordings originating from different microphone sets, point out new challenges in optimizing the feature representation step.",NLD,education,Developed economies,"[7.2213793, 39.866062]","[20.686434, 21.604408]","[-0.5694638, 8.374988, -22.098434]","[5.620549, 2.7070858, 12.772619]","[15.834181, 10.736053]","[11.125205, 1.1703712]","[13.738557, 15.151195, -1.6423087]","[11.880541, 5.348552, 12.216638]"
93,Peter Knees;Tim Pohle;Markus Schedl;Dominik Schnitzer;Klaus Seyerlehner;Gerhard Widmer,Augmenting Text-based Music Retrieval with Audio Similarity: Advantages and Limitations.,2009,https://doi.org/10.5281/zenodo.1418361,P. Knees+Johannes Kepler University Linz>AUT>education;T. Pohle+Johannes Kepler University Linz>AUT>education;M. Schedl+Johannes Kepler University Linz>AUT>education;D. Schnitzer+Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;K. Seyerlehner+Johannes Kepler University Linz>AUT>education;G. Widmer+Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"We investigate an approach to a music search engine that indexes music pieces based on related Web documents. This allows for searching for relevant music pieces by issuing descriptive textual queries. In this paper, we examine the effects of incorporating audio-based similarity into the text-based ranking process – either by directly modifying the retrieval process or by performing post-hoc audio-based re-ranking of the search results. The aim of this combination is to improve ranking quality by including relevant tracks that are left out by text-based retrieval approaches. Our evaluations show overall improvements but also expose limitations of these unsupervised approaches to combining sources. Evaluations are carried out on two collections, one large real-world collection containing about 35,000 tracks and on the CAL500 set.",AUT,education,Developed economies,"[-11.166463, 16.768084]","[20.086432, 18.873156]","[-6.1082354, 7.4459248, -7.910342]","[9.325213, -4.2942896, 15.218735]","[13.475608, 8.470568]","[10.420855, 0.99634844]","[13.582531, 14.797423, -1.5620687]","[11.72173, 5.6267715, 12.747355]"
94,Bernhard Niedermayer,Improving Accuracy of Polyphonic Music-to-Score Alignment.,2009,https://doi.org/10.5281/zenodo.1415220,Bernhard Niedermayer+Johannes Kepler University Linz>AUT>education,"This paper presents a new method to refine music-to-score alignments. The proposed system works offline in two passes, where in the first step a state-of-the art alignment based on chroma vectors and dynamic time warping is performed. In the second step a non-negative matrix factorization is calculated within a small search window around each predicted note onset, using pretrained tone models of only those pitches which are expected to be played within that window. Note onsets are then reset according to the pitch activation patterns yielded by the matrix factorization. In doing so, we are able to resolve individual notes within a chord. We show that this method is feasible of increasing the accuracy of aligned note’s onsets which are already aligned relatively near to the real note attack. However it is so far not suitable for the detection and correction of outliers which are displaced by a large timespan. We also compared our system to a reference method showing that it outperforms bandpass filtering based onset detection in the refinement step.",AUT,education,Developed economies,"[17.812746, -12.538927]","[-18.844505, -14.7855215]","[0.53732955, -13.527585, -7.3895636]","[-1.3941793, -20.775785, -6.0040293]","[10.891698, 6.4553905]","[6.126044, 0.65743476]","[11.9867115, 12.621512, -1.4928235]","[8.184177, 5.780349, 10.825468]"
95,Kjell Lemström;Geraint A. Wiggins,Formalizing Invariances for Content-based Music Retrieval.,2009,https://doi.org/10.5281/zenodo.1418363,"Kjell Lemström+University of Helsinki>FIN>education;Geraint A. Wiggins+Goldsmiths, University of London>GBR>education","Invariances are central concepts in content-based music retrieval. Musical representations and similarity measures are designed to capture musically relevant invariances, such as transposition invariance. Though regularly used, their explicit definition is usually omitted because of the heavy formalism required. The lack of explicit definition, however, can result in misuse or misunderstanding of the terms. We discuss the musical relevance of various musical invariances and develop a set-theoretic formalism, for defining and classifying them. Using it, we define the most common invariances, and give a taxonomy which they inhabit. The taxonomy serves as a useful tool for identifying where work is needed to address real world problems in content-based music retrieval.",FIN,education,Developed economies,"[-16.06548, 18.056147]","[17.710812, 9.679879]","[-9.975444, 8.102482, -9.18875]","[6.2325726, 0.0016856598, 10.35]","[13.742106, 8.39298]","[9.921492, 1.6892781]","[13.980896, 14.669691, -1.7334615]","[11.370856, 6.2541404, 12.326893]"
96,Ciril Bohak;Matija Marolt,Calculating Similarity of Folk Song Variants with Melody-based Features.,2009,https://doi.org/10.5281/zenodo.1416516,Ciril Bohak+University of Ljubljana>SVN>education;Matija Marolt+University of Ljubljana>SVN>education,"As folk songs live largely through oral transmission, there usually is no standard form of a song - each performance of a folk song may be unique. Different interpretations of the same song are called song variants, all variants of a song belong to the same variant type. In the paper, we explore how various melody-based features relate to folk song variants. Specifically, we explore whether we can derive a melodic similarity measure that would correlate to variant types in the sense that it would measure songs belonging to the same variant type as more similar, in contrast to songs from different variant types. The measure would be useful for folk song retrieval based on variant types, classification of unknown tunes, as well as a measure of similarity between variant types. We experimented with a number of melodic features calculated from symbolic representations of folk song melodies and combined them into a melody-based folk song similarity measure. We evaluated the measure on the task of classifying an unknown melody into a set of existing variant types. We show that the proposed measure gives the correct variant type in the top 10 list for 68% of queries in our data set.",SVN,education,Developed economies,"[-0.8419162, 16.382109]","[15.029371, 3.9706278]","[3.2296154, 9.987216, -0.32702827]","[4.492831, -0.14018187, 4.58584]","[12.478932, 9.812785]","[9.481521, 1.9015121]","[12.774628, 15.387171, -0.5872956]","[11.157977, 7.014483, 12.959649]"
97,Jan Weil;Thomas Sikora;Jean-Louis Durrieu;Gaël Richard,Automatic Generation of Lead Sheets from Polyphonic Music Signals.,2009,https://doi.org/10.5281/zenodo.1414758,Jan Weil+Technische Universität Berlin>DEU>education;Thomas Sikora+Technische Universität Berlin>DEU>education;J.-L. Durrieu+Telecom ParisTech>FRA>education|CNRS LTCI>FRA>facility;Gaël Richard+Telecom ParisTech>FRA>education|CNRS LTCI>FRA>facility,"A lead sheet is a type of music notation which summarizes the content of a song. The usual elements that are reproduced are the melody, chords, tempo, time signature, style and the lyrics, if any. In this paper we propose a system that aims at transcribing both the melody and the associated chords in a beat-synchronous framework. A beat tracker identifies the pulse positions and thus defines a beat grid on which the chord sequence and the melody notes are mapped. The harmonic changes are used to estimate the time signature and the down beats as well as the key of the piece. The different modules perform very well on each of the different tasks, and the lead sheets that were rendered show the potential of the approaches adopted in this paper.",DEU,education,Developed economies,"[30.195168, 6.126531]","[-27.465048, 0.3338888]","[12.8934555, -1.8692932, -5.9177957]","[-8.523534, 6.315606, -6.043256]","[10.281664, 6.995251]","[5.9643188, 1.138797]","[12.168755, 12.14228, -0.97195596]","[7.9652524, 6.277459, 11.182183]"
98,Jeremy Reed;Yushi Ueda;Sabato Marco Siniscalchi;Yuuki Uchiyama;Shigeki Sagayama;Chin-Hui Lee,Minimum Classification Error Training to Improve Isolated Chord Recognition.,2009,https://doi.org/10.5281/zenodo.1417163,J.T. Reed+Georgia Institute of Technology>USA>education;Yushi Ueda+The University of Tokyo>JPN>education;S. Siniscalchi+Norwegian University of Science and Technology>NOR>education;Yuki Uchiyama+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education;C.-H. Lee+Georgia Institute of Technology>USA>education,"Audio chord detection is the combination of two separate tasks: recognizing what chords are played and determining when chords are played. Most current audio chord detection algorithms use hidden Markov model (HMM) classifiers because of the task similarity with automatic speech recognition. For most speech recognition algorithms, the performance is measured by word error rate; i.e., only the identity of recognized segments is considered because word boundaries in continuous speech are often ambiguous. In contrast, audio chord detection performance is typically measured in terms of frame error rate, which considers both timing and classification. This paper treats these two tasks separately and focuses on the first problem; i.e., classifying the correct chords given boundary information. The best performing chroma/HMM chord detection algorithm, as measured in the 2008 MIREX Audio Chord Detection Contest, is used as the baseline in this paper. Further improvements are made to reduce feature correlation, account for differences in tuning, and incorporate minimum classification error (MCE) training in obtaining chord HMMs. Experiments demonstrate that classification rates can be improved with tuning compensation and MCE discriminative training.",USA,education,Developed economies,"[55.9463, -7.438958]","[-32.61454, 18.676922]","[31.149635, -10.050003, 18.435406]","[-23.907328, -4.051349, -0.39269215]","[6.5813026, 8.7267685]","[6.0590453, 3.725664]","[11.929831, 10.280675, 2.1735282]","[9.704167, 8.986163, 12.177136]"
99,Anssi Klapuri,A Method for Visualizing the Pitch Content of Polyphonic Music Signals.,2009,https://doi.org/10.5281/zenodo.1415036,Anssi Klapuri+Tampere University of Technology>FIN>education,"This paper proposes a method for visualizing the pitch content of polyphonic music signals. More specifically, a model is proposed for calculating the salience of pitch candidates within a given pitch range, and an optimization technique is proposed to find the parameters of the model. The aim is to produce a continuous function which shows peaks at the positions of true pitches and where spurious peaks at multiples and submultiples of the true pitches are suppressed. The proposed method was evaluated using synthesized MIDI signals, for which it outperformed a baseline method in terms of precision and recall. A straightforward visualization technique is proposed to render the pitch salience function on the traditional staves when the musical key and barline information is available.",FIN,education,Developed economies,"[-10.33368, 34.228905]","[-3.7634344, -14.418806]","[-4.810586, -9.16509, -13.897939]","[6.301102, -14.408685, -7.8476157]","[12.9947405, 6.728489]","[6.9562964, 2.4350653]","[13.400756, 13.358453, -2.076152]","[9.240179, 7.723409, 11.2606535]"
100,Tuomas Eerola;Olivier Lartillot;Petri Toiviainen,Prediction of Multidimensional Emotional Ratings in Music from Audio Using Multivariate Regression Models.,2009,https://doi.org/10.5281/zenodo.1416730,Tuomas Eerola+University of Jyvaskyla>FIN>education;Olivier Lartillot+University of Jyvaskyla>FIN>education;Petri Toiviainen+University of Jyvaskyla>FIN>education,"Content-based prediction of musical emotions and moods has a large number of exciting applications in Music Information Retrieval. However, what should be predicted, and precisely how, remain a challenge in the field. We provide an empirical comparison of two common paradigms of emotion representation in music, opposing a multidimensional space to a set of basic emotions. New ground-truth data consisting of film soundtracks was used to assess the compatibility of these models. The findings suggest that the two are highly compatible and a quantitative mapping between the two is provided. Next we propose a model predicting perceived emotions based on a set of features extracted from the audio. The feature selection and transformation is given special emphasis and three separate data reduction techniques are compared (stepwise regression, principal component analysis, and partial least squares regression). Best linear models consisting of 2-5 predictors from the data reduction process were able to account for between 58 and 85% of the variance. In general, partial least squares models performed the best and the data transformation has a significant role in building linear models.",FIN,education,Developed economies,"[-58.313145, 3.3852332]","[50.766624, -8.967894]","[-23.64528, 22.763594, 3.117022]","[10.740383, 23.652328, 5.125934]","[13.848073, 12.814761]","[13.083438, 4.2112494]","[16.251286, 14.585998, 1.7329416]","[14.197238, 5.072865, 10.318091]"
101,Beinan Li;Jordan B. L. Smith;Ichiro Fujinaga,Optical Audio Reconstruction for Stereo Phonograph Records Using White Light Interferometry.,2009,https://doi.org/10.5281/zenodo.1416230,Beinan Li+McGill University>CAN>education;Jordan B. L. Smith+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Our work focuses on optically reconstructing the stereo audio signal of a 33 rpm long-playing (LP) record using a white-light interferometry-based approach. Previously, a theoretical framework was presented, alongside the primitive reconstruction result from a few cycles of a stereo sinusoidal test signal. To reconstruct an audible duration of a longer stereo signal requires tackling new problems, such as disc warping, image alignment, and eliminating the effects of noise and broken grooves. This paper proposes solutions to these problems, and presents the complete workflow of our Optical Audio Reconstruction (OAR) system.",CAN,education,Developed economies,"[30.146147, -15.5560875]","[13.66428, -37.207382]","[18.924673, 20.156857, 7.8948374]","[-5.6510277, -26.77273, 3.6413596]","[8.632026, 6.202076]","[6.4318213, -0.037436824]","[10.645498, 11.201249, -0.07718399]","[8.032928, 4.9042363, 10.743412]"
102,Noam Koenigstein;Yuval Shavitt,Song Ranking based on Piracy in Peer-to-Peer Networks.,2009,https://doi.org/10.5281/zenodo.1417038,Noam Koenigstein+Tel Aviv University>ISR>education;Yuval Shavitt+Tel Aviv University>ISR>education,"Music sales are loosing their role as a means for music dissemination but are still used by the music industry for ranking artist success, e.g., in the Billboard Magazine chart. Thus, it was suggested recently to use social networks as an alternative ranking system; a suggestion which is problematic due to the ease of manipulating the list and the difficulty of implementation. In this work we suggest to use logs of queries from peer-to-peer file-sharing systems for ranking song success. We show that the trend and fluctuations of the popularity of a song in the Billboard list have strong correlation (0.89) to the ones in a list built from the P2P network, and that the P2P list has a week advantage over the Billboard list. Namely, music sales are strongly correlated with music piracy.",ISR,education,Developing economies,"[-19.880203, 25.978758]","[48.388096, 17.984024]","[-13.778208, 11.951352, -16.650137]","[23.381031, 1.8881423, 13.354567]","[14.457098, 7.907831]","[12.201227, 2.096899]","[14.362039, 14.99753, -2.2467878]","[13.205921, 5.3803496, 12.356698]"
103,Matthias Robine;Pierre Hanna;Mathieu Lagrange,Meter Class Profiles for Music Similarity and Retrieval.,2009,https://doi.org/10.5281/zenodo.1415788,Matthias Robine+LaBRI - University of Bordeaux>FRA>education;Pierre Hanna+LaBRI - University of Bordeaux>FRA>education;Mathieu Lagrange+Telecom ParisTech>FRA>education,"Rhythm is one of the main properties of Western tonal music. Existing content-based retrieval systems generally deal with melody or style. A few existing ones based on meter or rhythm characteristics have been recently proposed but they require a precise analysis, or they rely on a low-level descriptor. In this paper, we propose a mid-level descriptor: the Meter Class Profile (MCP). The MCP is centered on the tempo and represents the strength of beat multiples, including the measure rate, and the beat subdivisions. The MCP coefficients are estimated by means of the autocorrelation and the Fourier transform of the onset detection curve. Experiments on synthetic and real databases are presented, and the results demonstrate the efficacy of the MCP descriptor in clustering and retrieval of songs according to their metric properties.",FRA,education,Developed economies,"[-6.122269, 14.998722]","[-22.56172, -1.3977311]","[-5.1033883, 8.659491, -3.8260264]","[-0.41843808, 11.168861, -7.9223404]","[13.066281, 9.1361475]","[6.0880685, 1.9672648]","[13.502845, 14.960636, -0.86443275]","[8.291987, 7.0244856, 11.56951]"
104,Christian Fremerey;Michael Clausen;Sebastian Ewert;Meinard Müller,Sheet Music-Audio Identification.,2009,https://doi.org/10.5281/zenodo.1416742,Christian Fremerey+Bonn University>DEU>education;Michael Clausen+Bonn University>DEU>education;Sebastian Ewert+Bonn University>DEU>education;Meinard Müller+Saarland University and MPI Informatik>DEU>education,"In this paper, we introduce and discuss the task of sheet music-audio identification. Given a query consisting of a sequence of bars from a sheet music representation, the task is to find corresponding sections within an audio interpretation of the same piece. Two approaches are proposed: a semi-automatic approach using synchronization and a fully automatic approach using matching techniques. A workflow is described that allows for evaluating the matching approach using the results of the more reliable synchronization approach. This workflow makes it possible to handle even complex queries from orchestral scores. Furthermore, we present an evaluation procedure, where we investigate several matching parameters and tempo estimation strategies. Our experiments have been conducted on a dataset comprising pieces of various instrumentations and complexity.",DEU,education,Developed economies,"[33.3386, 7.7034674]","[-2.4911625, -3.9050143]","[23.464815, 6.265358, 18.480259]","[3.5317473, -18.298922, 0.20278247]","[10.553492, 6.790005]","[7.1110706, 1.4480792]","[12.280666, 11.983711, -1.2347109]","[9.91839, 7.0370016, 11.44615]"
105,Byeong-jun Han;Seungmin Rho;Roger B. Dannenberg;Eenjun Hwang,SMERS: Music Emotion Recognition Using Support Vector Regression.,2009,https://doi.org/10.5281/zenodo.1415674,Byeong-jun Han+Korea University>KOR>education;Seungmin Rho+Korea University>KOR>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education;Eenjun Hwang+Korea University>KOR>education,"Music emotion plays an important role in music retrieval, mood detection and other music-related applications. Many issues for music emotion recognition have been addressed by different disciplines such as physiology, psychology, cognitive science and musicology. We present a support vector regression (SVR) based music emotion recognition system. The recognition process consists of three steps: (i) seven distinct features are extracted from music; (ii) those features are mapped into eleven emotion categories on Thayer’s two-dimensional emotion model; (iii) two regression functions are trained using SVR and then arousal and valence values are predicted. We have tested our SVR-based emotion classifier in both Cartesian and polar coordinate system empirically. The result indicates the SVR classifier in the polar representation produces satisfactory result which reaches 94.55% accuracy superior to the SVR (in Cartesian) and other machine learning classification algorithms such as SVM and GMM.",KOR,education,Developing economies,"[-58.529697, -1.6908264]","[51.848866, -8.165575]","[-26.507473, 19.685448, 6.3293185]","[12.509578, 25.21525, 6.023739]","[14.0872965, 13.005387]","[13.090991, 4.121717]","[15.975221, 14.338973, 1.8519604]","[14.27273, 5.0912924, 10.341922]"
107,Joachim Ganseman;Paul Scheunders;Wim D'haes,Using XML-Formatted Scores in Real-Time Applications.,2009,https://doi.org/10.5281/zenodo.1417893,Joachim Ganseman+IBBT - Visionlab>BEL>facility|University of Antwerp>BEL>education;Paul Scheunders+IBBT - Visionlab>BEL>facility|University of Antwerp>BEL>education;Wim D’haes+Mu Technologies NV>BEL>company,"In this paper we present fast and scalable methods to access relevant data from music scores stored in an XML based notation format, with the explicit goal of using scores in real-time audio processing frameworks. Quick and easy access is important when accessing or traversing a score, for instance for real-time playback. Any time complexity improvement in these contexts is valuable, while memory constraints are usually less important. We show that with some well chosen design choices and precomputation of the necessary data, runtime time complexity of several key score manipulation operations can be reduced to a level that allows use in a real-time context.",BEL,facility,Developed economies,"[-0.9373412, 31.32949]","[6.7824388, 39.23732]","[-10.275249, -8.587818, -17.107151]","[-6.026924, -10.667241, 20.587822]","[13.255571, 6.876277]","[10.181057, 0.31125766]","[13.566437, 12.9675455, -1.8296651]","[10.833262, 5.028628, 12.167335]"
108,Amelie Anglade;Rafael Ramírez 0001;Simon Dixon,Genre Classification Using Harmony Rules Induced from Automatic Chord Transcriptions.,2009,https://doi.org/10.5281/zenodo.1414944,Amélie Anglade+Queen Mary University of London>GBR>education|Centre for Digital Music>GBR>facility;Rafael Ramirez+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>facility;Simon Dixon+Queen Mary University of London>GBR>education|Centre for Digital Music>GBR>facility,"We present an automatic genre classification technique making use of frequent chord sequences that can be applied on symbolic as well as audio data. We adopt a first-order logic representation of harmony and musical genres: pieces of music are represented as lists of chords and musical genres are seen as context-free definite clause grammars using subsequences of these chord lists. To induce the context-free definite clause grammars characterising the genres we use a first-order logic decision tree induction algorithm. We report on the adaptation of this classification framework to audio data using an automatic chord transcription algorithm. We also introduce a high-level harmony representation scheme which describes the chords in term of both their degrees and chord categories. When compared to another high-level harmony representation scheme used in a previous study, it obtains better classification accuracies and shorter run times. We test this framework on 856 audio files synthesized from Band in a Box files and covering 3 main genres, and 9 subgenres. We perform 3-way and 2-way classification tasks on these audio files and obtain good classification results: between 67% and 79% accuracy for the 2-way classification tasks and between 58% and 72% accuracy for the 3-way classification tasks.",GBR,education,Developed economies,"[48.44111, -4.630494]","[-23.491772, 21.67232]","[21.681866, -10.158621, 19.346924]","[-22.204838, 0.9618464, 5.7108045]","[7.253562, 8.811505]","[6.977933, 3.3045478]","[11.986044, 10.736172, 1.8350652]","[9.681612, 8.294765, 12.303026]"
109,Carles Fernandes Julià;Sergi Jordà,SongExplorer: A Tabletop Application for Exploring Large Collections of Songs.,2009,https://doi.org/10.5281/zenodo.1418177,Carles F. Julià+Universitat Pompeu Fabra>ESP>education;Sergi Jordà+Universitat Pompeu Fabra>ESP>education,"This paper presents SongExplorer, a system for the exploration of large music collections on tabletop interfaces. SongExplorer addresses the problem of finding new interesting songs on large music databases, from an interaction design perspective. Using high level descriptors of musical songs, SongExplorer creates a coherent 2D map based on similarity, in which neighboring songs tend to be more similar. All songs are represented as throbbing circles that highlight their more relevant high-level properties, and the resulting music map is browseable and zoomable by the users who can use their fingers as well as specially designed tangible pucks, for helping them to find interesting music, independently of their previous knowledge of the collection. SongExplorer also offers basic player capabilities, allowing the users to organize the songs they have just discovered into playlists which can be manipulated as well as played and displayed. In this paper, the system hardware, software and interaction design are explained, and the usability tests carried are presented. Finally, conclusions and future work are discussed.",ESP,education,Developed economies,"[-18.587343, 28.478031]","[29.105333, 22.249243]","[-13.211181, 5.492817, -19.462212]","[11.656066, -2.3587348, 23.30091]","[14.197008, 7.5195823]","[11.400429, 1.3187809]","[14.181418, 14.26862, -2.4162683]","[12.328572, 5.1795754, 13.11469]"
110,Woojay Jeon;Changxue Ma;Yan Ming Cheng,An Efficient Signal-Matching Approach to Melody Indexing and Search Using Continuous Pitch Contours and Wavelets.,2009,https://doi.org/10.5281/zenodo.1415054,"Woojay Jeon+Motorola, Inc.>USA>company;Changxue Ma+Motorola, Inc.>USA>company;Yan Ming Cheng+Motorola, Inc.>USA>company","""We describe a method of indexing and efﬁciently searching music melodies based on their continuous dominant fundamental frequency (f0) contours without obtaining note-level transcriptions. Each f0 contour is encoded by a redundant set of wavelet coefﬁcients that represent its shape in level-normalized form at various locations and time scales. This allows a query melody to be exhaustively compared with variable-length portions of a target melody at arbitrary locations while accounting for differences in key and tempo. The method is applied in a Query-by-Humming (QBH) system where users may search a database of recorded pop songs by humming or singing an arbitrary part of the melody of an intended song. The system has fast retrieval times because the wavelet coefﬁcients can be efﬁciently indexed in a binary tree and a vector distance measure instead of dynamic programming is used for comparisons. Using automatic pitch extraction to obtain all f0 contours from acoustic data, the method demonstrates practical performance in an experiment with an existing monophonic data set and in a preliminary experiment with real-world polyphonic music.""",USA,company,Developed economies,"[6.4379926, -9.9079485]","[8.41371, 10.956039]","[11.113871, 4.1973915, -2.8282278]","[6.6361475, -14.877439, 9.962753]","[10.44478, 9.963107]","[8.625278, 0.6615002]","[11.360303, 15.170424, -0.699509]","[10.201503, 6.104743, 13.194232]"
111,Özgür Izmirli,Tonal-Atonal Classification of Music Audio Using Diffusion Maps.,2009,https://doi.org/10.5281/zenodo.1416936,Özgür İzmirli+Connecticut College>USA>education,"In this paper we look at the problem of classifying music audio as tonal or atonal by learning a low-dimensional structure representing tonal relationships among keys. We use a training set composed of tonal pieces which includes all major and minor keys. A kernel eigenmap based method is used for structure learning and discovery. Specifically, a Diffusion Maps (DM) framework is used and its parameter tuning is discussed. Since these methods do not scale well with increasing data size, it becomes infeasible to use these methods in online applications. In order to facilitate on-line classification an out-of-sample extension to the DM framework is given. The learned structure of tonal relationships is presented and a simple scheme for classification of tonal-atonal pieces is proposed. Evaluation results show that the method is able to perform at an accuracy above 90% with the current data set.",USA,education,Developed economies,"[-21.94633, -14.002611]","[12.400061, -8.25585]","[-7.839714, -0.39405447, 16.82551]","[14.058519, 10.582714, -9.490682]","[12.354908, 10.337155]","[9.729428, 3.2834353]","[13.3018055, 13.973335, 0.9355796]","[11.224389, 7.58298, 11.0956335]"
112,Tae Hong Park;Zhiye Li;Wen Wu,Easy Does It: The Electro-Acoustic Music Analysis Toolbox.,2009,https://doi.org/10.5281/zenodo.1416674,Tae Hong Park+Tulane University>USA>education;Zhiye Li+Tulane University>USA>education;Wen Wu+Tulane University>USA>education,"""In this paper we present the EASY (Electro-Acoustic muSic analYsis) Toolbox software system for assisting electro-acoustic music analysis. The primary aims of the system are to present perceptually relevant features and audio descriptors via visual designs to gain more insight into electro-acoustic music works and provide easy-to-use “click-and-go” software interface paradigms for practical use of the system by non-experts and experts alike. The development of the EASY system exploits MIR techniques with particular emphasis on the electro-acoustic music repertoire – musical pieces that concentrate on timbral dimensions rather than traditional elements such as pitch, melody, harmony, and rhythm. The project was mainly inspired by the lack of software tools available for aiding electro-acoustic music analysis. The system’s frameworks, feature analysis algorithms, along with the initial analyses of pieces are presented here.""",USA,education,Developed economies,"[-3.8573847, 27.96154]","[3.7339935, 29.865643]","[-18.519678, -14.64938, -3.3825827]","[-8.394389, -5.243403, 11.792575]","[13.588895, 7.5910482]","[10.004133, 1.2505263]","[13.926924, 14.000956, -1.8444456]","[10.79331, 5.592428, 11.643232]"
113,Mika Kuuskankare;Mikael Laurson,MIR in ENP - Rule-based Music Information Retrieval from Symbolic Music Notation.,2009,https://doi.org/10.5281/zenodo.1417277,Mika Kuuskankare+Sibelius Academy>FIN>education;Mikael Laurson+Sibelius Academy>FIN>education,"""Symbolic music information retrieval is one of the most underrepresented areas in the field of MIR. Here, symbolic music means common practice music notation–the musician readable format. In this paper we introduce a novel rule-based symbolic music retrieval mechanism. The Scripting system–ENP-Script–is augmented with MIR functionality. It allows us to perform sophisticated retrieval operations on symbolic musical scores prepared with the help of the music notation system ENP. We will also give a special attention to visualization of the query results. All the statistical queries, such as histograms, are visualized with the help of common music notation where appropriate. N-grams and more complex queries–the ones dealing with voice leading, for example–are visualized directly in the score. Our aim is to demonstrate the power and expressivity of the combination of common music notation and a rule-based scripting language through several challenging examples.""",FIN,education,Developed economies,"[14.661012, 17.756744]","[0.371041, 33.388157]","[-7.4129887, -8.782768, 14.901424]","[-2.0217853, -9.873517, 11.424745]","[12.869942, 6.3425565]","[9.845686, 0.72608554]","[13.9847555, 12.240641, -1.2783827]","[10.638679, 5.467403, 12.041238]"
114,Min-Yian Su;Yi-Hsuan Yang;Yu-Ching Lin;Homer H. Chen,An Integrated Approach to Music Boundary Detection.,2009,https://doi.org/10.5281/zenodo.1417585,Min-Yian Su+National Taiwan University>TWN>education;Yi-Hsuan Yang+National Taiwan University>TWN>education;Yu-Ching Lin+National Taiwan University>TWN>education;Homer Chen+National Taiwan University>TWN>education,"Music boundary detection is a fundamental step of music analysis and summarization. Existing works use either unsupervised or supervised methodologies to detect boundary. In this paper, we propose an integrated approach that takes advantage of both methodologies. In particular, a graph-theoretic approach is proposed to fuse the results of an unsupervised model and a supervised one by the knowledge of the typical length of a music section. To further improve accuracy, a number of novel mid-level features are developed and incorporated to the boundary detection framework. Evaluation result on the RWC dataset shows the effectiveness of the proposed approach.",TWN,education,Developing economies,"[-5.633385, -8.989542]","[0.83548915, 3.21004]","[8.838037, -1.2385674, -1.7190783]","[-0.050243616, 3.8215146, -0.95794827]","[11.706203, 8.752514]","[8.419177, 3.0212755]","[12.531838, 14.149674, 0.35642558]","[10.627558, 7.4229136, 11.392403]"
115,Hussein Hirjee;Daniel G. Brown 0001,Automatic Detection of Internal and Imperfect Rhymes in Rap Lyrics.,2009,https://doi.org/10.5281/zenodo.1416160,Hussein Hirjee+University of Waterloo>CAN>education;Daniel G. Brown+University of Waterloo>CAN>education,"Imperfect and internal rhymes are two important features in rap music often ignored in the music information retrieval community. We develop a method of scoring potential rhymes using a probabilistic model based on phoneme frequencies in rap lyrics. We use this scoring scheme to automatically identify internal and line-final rhymes in song lyrics and demonstrate the performance of this method compared to rules-based models. Higher level rhyme features are produced and used to compare rhyming styles in song lyrics from different genres, and for different rap artists.",CAN,education,Developed economies,"[-31.3718, -29.560923]","[25.341593, -4.0191836]","[7.202632, 25.362673, 1.1378769]","[6.7231855, 10.091756, 2.9230573]","[11.556598, 11.699985]","[10.193638, 3.0298107]","[12.56122, 15.819356, 0.97889143]","[11.906285, 6.74829, 11.288485]"
116,Verena Thomas;Christian Fremerey;David Damm;Michael Clausen,Slave: A Score-Lyrics-Audio-Video-Explorer.,2009,https://doi.org/10.5281/zenodo.1418029,Verena Thomas+University of Bonn>DEU>education;Christian Fremerey+University of Bonn>DEU>education;David Damm+University of Bonn>DEU>education;Michael Clausen+University of Bonn>DEU>education,"We introduce the music exploration system SLAVE, which is based upon previous developments of our group. SLAVE manages multimedia music collections and allows for multimodal navigation, playback, and visualization in an efficient and user-friendly manner. While previously the focus of our system development has been the simultaneous exploration of digitized sheet music and audio, with SLAVE we enhance the functionalities by video and lyrics to achieve a more comprehensive music interaction. In this paper, we concentrate on two aspects. Firstly, we integrate video documents into our framework. Secondly, we introduce a graphical user interface for semi-automatic feature extraction, indexing, and synchronization of heterogeneous music collections. The output of this GUI is used by SLAVE to offer both high quality audio and video playback with time-synchronous display of digitized sheet music and content-based search.",DEU,education,Developed economies,"[-30.275179, -32.858883]","[27.65803, 21.879124]","[5.184837, 20.260454, -2.0155268]","[11.760825, -2.0918176, 20.198648]","[11.3598995, 11.715239]","[11.376401, 1.3242027]","[12.340091, 15.88445, 1.1168337]","[12.221476, 5.1817484, 13.040099]"
117,Eric Nichols;Donald Byrd,Lyric Extraction and Recognition on Digital Images of Early Music Sources.,2009,https://doi.org/10.5281/zenodo.1417046,John Ashley Burgoyne+McGill University>CAN>education;Johanna Devaney+McGill University>CAN>education;Yue Ouyang+McGill University>CAN>education;Laurent Pugin+McGill University>CAN>education;Tristan Himmelman+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Optical music recognition (OMR) is one of the most promising tools for generating large-scale, distributable libraries of musical data. Much OMR work has focussed on instrumental music, avoiding a special challenge vocal music poses for OMR: lyric recognition. Lyrics complicate the page layout, making it more difﬁcult to identify the regions of the page that carry musical notation. Furthermore, users expect a complete OMR process for vocal music to include recognition of the lyrics, reuniﬁcation of syllables when they have been separated, and alignment of these lyrics with the recognised music. Unusual layouts and inconsistent practises for syllabiﬁcation, however, make lyric recognition more challenging than traditional optical character recognition (OCR). This paper surveys historical approaches to lyric recognition, outlines open challenges, and presents a new approach to extracting text lines in medieval manuscripts, one of the frontiers of OMR research today.",CAN,education,Developed economies,"[-28.001225, -30.598175]","[-23.806347, 38.469917]","[11.845061, 24.030306, -4.8737206]","[-14.139234, -22.566519, 2.6370027]","[11.342461, 11.735012]","[6.756497, -0.652663]","[12.327728, 15.910896, 1.00889]","[7.835554, 4.0478663, 10.5668]"
87,Nicola Orio;Antonio Rodà,A Measure of Melodic Similarity based on a Graph Representation of the Music Structure.,2009,https://doi.org/10.5281/zenodo.1415010,Nicola Orio+University of Padova>ITA>education;Antonio Rodà+University of Udine>ITA>education,"Content-based music retrieval requires to define a similarity measure between music documents. In this paper, we propose a novel similarity measure between melodic content, as represented in symbolic notation, that takes into account musicological aspects on the structural function of the melodic elements. The approach is based on the representation of a collection of music scores with a graph structure, where terminal nodes directly describe the music content, internal nodes represent its incremental generalization, and arcs denote the relationships among them. The similarity between two melodies can be computed by analyzing the graph structure and finding the shortest path between the corresponding nodes inside the graph. Preliminary results in terms of music similarity are presented using a small test collection.",ITA,education,Developed economies,"[0.49859756, 15.911882]","[16.915699, 6.6455984]","[-0.6510376, 4.680708, -0.1935637]","[7.278835, -1.4911289, 6.955492]","[12.359671, 9.621118]","[9.54584, 1.6126869]","[12.819587, 15.387478, -0.83853185]","[11.228928, 6.7013803, 12.798825]"
86,Dominik Schnitzer;Arthur Flexer;Gerhard Widmer,A Filter-and-Refine Indexing Method for Fast Similarity Search in Millions of Music Tracks.,2009,https://doi.org/10.5281/zenodo.1417831,Dominik Schnitzer+Austrian Research Institute for Artificial Intelligence>AUT>facility;Arthur Flexer+Austrian Research Institute for Artificial Intelligence>AUT>facility;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,We present a filter-and-refine method to speed up acoustic audio similarity queries which use the Kullback-Leibler divergence as similarity measure. The proposed method rescales the divergence and uses a modified FastMap implementation to accelerate nearest-neighbor queries. The search for similar music pieces is accelerated by a factor of 10−30 compared to a linear scan but still offers high recall values (relative to a linear scan) of 95 − 99%. We show how the proposed method can be used to query several million songs for their acoustic neighbors very fast while producing almost the same results that a linear scan over the whole database would return. We present a working prototype implementation which is able to process similarity queries on a 2.5 million songs collection in about half a second on a standard CPU.,AUT,facility,Developed economies,"[-8.947074, 17.113148]","[22.806105, 13.66035]","[-2.836533, 8.47665, -10.5807495]","[14.938473, -5.278444, 7.467115]","[13.355445, 8.821522]","[10.373356, 1.930759]","[13.55961, 15.077082, -1.3198861]","[11.859978, 6.2605224, 12.646039]"
85,Juan Pablo Bello,Grouping Recorded Music by Structural Similarity.,2009,https://doi.org/10.5281/zenodo.1414876,"Juan Pablo Bello+Music and Audio Research Lab (MARL), New York University>USA>education","This paper introduces a method for the organization of recorded music according to structural similarity. It uses the Normalized Compression Distance (NCD) to measure the pairwise similarity between songs, represented using beat-synchronous self-similarity matrices. The approach is evaluated on its ability to cluster a collection into groups of performances of the same musical work. Tests are aimed at finding the combination of system parameters that improve clustering, and at highlighting the benefits and shortcomings of the proposed method. Results show that structural similarities can be well characterized by this approach, given consistency in beat tracking and overall song structure.",USA,education,Developed economies,"[-6.491545, 2.6474712]","[20.923792, 8.176591]","[-0.22500263, -4.1836343, -6.455468]","[13.208309, 7.3873105, -17.820923]","[12.04974, 8.032828]","[9.779907, 2.159604]","[12.768397, 14.119706, -0.4237781]","[11.278322, 7.1367364, 12.31062]"
84,Tim Pohle;Dominik Schnitzer;Markus Schedl;Peter Knees;Gerhard Widmer,On Rhythm and General Music Similarity.,2009,https://doi.org/10.5281/zenodo.1418229,Tim Pohle+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Dominik Schnitzer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Markus Schedl+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Peter Knees+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"The contribution of this paper is threefold: First, we propose modifications to Fluctuation Patterns. The resulting descriptors are evaluated in the task of rhythm similarity computation on the “Ballroom Dancers” collection. Second, we show that by combining these rhythmic descriptors with a timbral component, results for rhythm similarity computation are improved beyond the level obtained when using the rhythm descriptor component alone. Third, we present one “unified” algorithm with fixed parameter set. This algorithm is evaluated on three different music collections. We conclude from these evaluations that the computed similarities reflect relevant aspects both of rhythm similarity and of general music similarity. The performance can be improved by tuning parameters of the “unified” algorithm to the specific task (rhythm similarity / general music similarity) and the specific collection, respectively.",AUT,education,Developed economies,"[0.8269064, 14.120357]","[12.221087, 4.887382]","[-2.5602279, 5.880563, 1.4744139]","[2.4912572, 13.70908, -5.4084787]","[12.62949, 9.4613285]","[6.4679947, 1.5867159]","[12.710043, 15.099344, -1.188885]","[8.434505, 6.607299, 11.990945]"
53,Kazuyoshi Yoshii;Masataka Goto,Continuous pLSI and Smoothing Techniques for Hybrid Music Recommendation.,2009,https://doi.org/10.5281/zenodo.1415204,Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents an extended probabilistic latent semantic indexing (pLSI) for hybrid music recommendation that deals with rating data provided by users and with content-based data extracted from audio signals. The original pLSI can be applied to collaborative filtering by treating users and items as discrete random variables that follow multinomial distributions. In hybrid recommendation, it is necessary to deal with musical contents that are usually represented as continuous vectorial values. To do this, we propose a continuous pLSI that incorporates Gaussian mixture models. This extension, however, causes a severe local optima problem because it increases the number of parameters drastically. This is considered to be a major factor generating “hubs,” which are items that are inappropriately recommended to almost all users. To solve this problem, we tested three smoothing techniques: multinomial smoothing, Gaussian parameter tying, and artist-based item clustering. The experimental results revealed that although the first method improved nothing, the others significantly improved the recommendation accuracy and reduced the hubness. This indicates that it is important to appropriately limit the model complexity to use the pLSI in practical.",JPN,facility,Developed economies,"[-48.424843, 26.822378]","[39.139233, 14.630438]","[-6.2898784, 26.231165, -12.023149]","[19.21632, 5.0213275, 15.561601]","[15.997857, 9.2702675]","[12.551977, 2.0702288]","[15.845127, 15.759665, -1.5002719]","[13.55309, 5.3618703, 12.675491]"
54,François Maillet;Douglas Eck;Guillaume Desjardins;Paul Lamere,Steerable Playlist Generation by Learning Song Similarity from Radio Station Playlists.,2009,https://doi.org/10.5281/zenodo.1416280,François Maillet+Université de Montréal>CAN>education|CIRMMT>CAN>facility;Douglas Eck+Université de Montréal>CAN>education|CIRMMT>CAN>facility;Guillaume Desjardins+Université de Montréal>CAN>education|CIRMMT>CAN>facility;Paul Lamere+The Echo Nest>USA>company,"This paper presents an approach to generating steerable playlists. We first demonstrate a method for learning song transition probabilities from audio features extracted from songs played in professional radio station playlists. We then show that by using this learnt similarity function as a prior, we are able to generate steerable playlists by choosing the next song to play not simply based on that prior, but on a tag cloud that the user is able to manipulate to express the high-level characteristics of the music he wishes to listen to.",CAN,education,Developed economies,"[-38.45124, 39.12267]","[35.652576, 21.11264]","[-1.7345672, 26.530432, -3.686643]","[16.75515, 2.5105085, 22.16284]","[16.092367, 8.159374]","[12.010334, 1.5948952]","[16.5281, 14.84457, -1.7483462]","[13.06303, 5.1754527, 13.160843]"
55,Klaas Bosteels;Elias Pampalk;Etienne E. Kerre,Evaluating and Analysing Dynamic Playlist Generation Heuristics Using Radio Logs and Fuzzy Set Theory.,2009,https://doi.org/10.5281/zenodo.1417675,Klaas Bosteels+Ghent University>BEL>education|Last.fm Ltd.>GBR>company;Elias Pampalk+Last.fm Ltd.>GBR>company;Etienne E. Kerre+Ghent University>BEL>education,"In this paper, we analyse and evaluate several heuristics for adding songs to a dynamically generated playlist. We explain how radio logs can be used for evaluating such heuristics, and show that formalizing the heuristics using fuzzy set theory simplifies the analysis. More concretely, we verify previous results by means of a large scale evaluation based on 1.26 million listening patterns extracted from radio logs, and explain why some heuristics perform better than others by analysing their formal definitions and conducting additional evaluations.",BEL,education,Developed economies,"[-39.49485, 40.728523]","[34.855095, 22.329445]","[-1.9600375, 29.937262, -6.1228113]","[15.195739, 3.1005633, 24.511023]","[16.209747, 8.164569]","[11.983111, 1.5350857]","[16.567286, 14.834486, -1.7468581]","[13.062448, 5.156378, 13.231583]"
56,Luke Barrington;Reid Oda;Gert R. G. Lanckriet,Smarter than Genius? Human Evaluation of Music Recommender Systems.,2009,https://doi.org/10.5281/zenodo.1417803,"Luke Barrington+University of California, San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Reid Oda+University of California, San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Gert Lanckriet+University of California, San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown","""Genius is a popular commercial music recommender system that is based on collaborative filtering of huge amounts of user data. To understand the aspects of music similarity that collaborative filtering can capture, we compare Genius to two canonical music recommender systems: one based purely on artist similarity, the other purely on similarity of acoustic content. We evaluate this comparison with a user study of 185 subjects. Overall, Genius produces the best recommendations. We demonstrate that collaborative filtering can actually capture similarities between the acoustic content of songs. However, when evaluators can see the names of the recommended songs and artists, we find that artist similarity can account for the performance of Genius. A system that combines these musical cues could generate music recommendations that are as good as Genius, even when collaborative filtering data is unavailable.""",USA,education,Developed economies,"[-45.30926, 24.373762]","[37.32464, 15.38753]","[-12.676414, 25.708878, -9.2872715]","[15.821484, 5.212242, 15.14845]","[15.796052, 9.051974]","[12.424718, 2.013699]","[15.710315, 15.568726, -1.4018306]","[13.498731, 5.3853765, 12.667918]"
57,Fei Wang 0001;Xin Wang 0013;Bo Shao;Tao Li 0001;Mitsunori Ogihara,Tag Integrated Multi-Label Music Style Classification with Hypergraph.,2009,https://doi.org/10.5281/zenodo.1416962,Fei Wang+Florida International University>USA>education;Xin Wang+Florida International University>USA>education;Bo Shao+Florida International University>USA>education;Tao Li+Florida International University>USA>education;Mitsunori Ogihara+University of Miami>USA>education,"Automatic music style classification is an important, but challenging problem in music information retrieval. It has a number of applications, such as indexing of and searching in musical databases. Traditional music style classification approaches usually assume that each piece of music has a unique style and they make use of the music contents to construct a classifier for classifying each piece into its unique style. However, in reality, a piece may match more than one, even several different styles. Also, in this modern Web 2.0 era, it is easy to get a hold of additional, indirect information (e.g., music tags) about music. This paper proposes a multi-label music style classification approach, called Hypergraph integrated Support Vector Machine (HiSVM), which can integrate both music contents and music tags for automatic music style classification. Experimental results based on a real world data set are presented to demonstrate the effectiveness of the method.",USA,education,Developed economies,"[-38.064594, -1.3860209]","[37.64414, 4.6675453]","[-16.120342, 9.76284, 11.265581]","[18.114485, 8.18743, 4.4895935]","[14.126315, 10.494061]","[11.113361, 3.2852018]","[15.312012, 14.153172, 0.22073267]","[12.636519, 6.5284305, 11.186727]"
58,Matthew D. Hoffman;David M. Blei;Perry R. Cook,Easy As CBA: A Simple Probabilistic Model for Tagging Music.,2009,https://doi.org/10.5281/zenodo.1417347,Matthew D. Hoffman+Princeton University>USA>education;David M. Blei+Princeton University>USA>education;Perry R. Cook+Princeton University>USA>education,"Many songs in large music databases are not labeled with semantic tags that could help users sort out the songs they want to listen to from those they do not. If the words that apply to a song can be predicted from audio, then those predictions can be used both to automatically annotate a song with tags, allowing users to get a sense of what qualities characterize a song at a glance. Automatic tag prediction can also drive retrieval by allowing users to search for the songs most strongly characterized by a particular word. We present a probabilistic model that learns to predict the probability that a word applies to a song from audio. Our model is simple to implement, fast to train, predicts tags for new songs quickly, and achieves state-of-the-art performance on annotation and retrieval tasks.",USA,education,Developed economies,"[-41.38804, -1.5914866]","[38.176926, -1.0414846]","[-14.115333, 15.71405, 9.076129]","[22.58015, 6.9140167, 2.8679652]","[14.466359, 10.582503]","[11.458425, 3.4073656]","[15.588032, 14.128456, 0.095801905]","[12.98391, 6.377858, 11.255327]"
59,Joon Hee Kim;Brian Tomasik;Douglas Turnbull,Using Artist Similarity to Propagate Semantic Information.,2009,https://doi.org/10.5281/zenodo.1416510,Joon Hee Kim+Swarthmore College>USA>education;Brian Tomasik+Swarthmore College>USA>education;Douglas Turnbull+Swarthmore College>USA>education,"Tags are useful text-based labels that encode semantic information about music (instrumentation, genres, emotions, geographic origins). While there are a number of ways to collect and generate tags, there is generally a data sparsity problem in which very few songs and artists have been accurately annotated with a sufficiently large set of relevant tags. We explore the idea of tag propagation to help alleviate the data sparsity problem. Tag propagation, originally proposed by Sordo et al., involves annotating a novel artist with tags that have been frequently associated with other similar artists. In this paper, we explore four approaches for computing artists similarity based on different sources of music information (user preference data, social tags, web documents, and audio content). We compare these approaches in terms of their ability to accurately propagate three different types of tags (genres, acoustic descriptors, social tags). We find that the approach based on collaborative filtering performs best. This is somewhat surprising considering that it is the only approach that is not explicitly based on notions of semantic similarity. We also find that tag propagation based on content-based music analysis results in relatively poor performance.",USA,education,Developed economies,"[-41.39034, 6.9337063]","[39.94373, 3.298594]","[-24.486843, 12.413732, 10.091557]","[21.685123, 8.119927, 7.572853]","[14.202548, 10.036653]","[11.827058, 2.91774]","[15.006909, 14.807014, -0.03026031]","[13.157004, 6.047569, 11.756137]"
61,Edith Law;Kris West;Michael I. Mandel;Mert Bay;J. Stephen Downie,Evaluation of Algorithms Using Games: The Case of Music Tagging.,2009,https://doi.org/10.5281/zenodo.1417647,Edith Law+Carnegie Mellon University>USA>education;Kris West+University of Illinois at Urbana-Champaign>USA>education;Michael Mandel+Columbia University>USA>education;Mert Bay+University of Illinois at Urbana-Champaign>USA>education|IMIRSEL>USA>facility;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education|IMIRSEL>USA>facility,"Search by keyword is an extremely popular method for retrieving music. To support this, novel algorithms that automatically tag music are being developed. The conventional way to evaluate audio tagging algorithms is to compute measures of agreement between the output and the ground truth set. In this work, we introduce a new method for evaluating audio tagging algorithms on a large scale by collecting set-level judgments from players of a human computation game called TagATune. We present the design and preliminary results of an experiment comparing five algorithms using this new evaluation metric, and contrast the results with those obtained by applying several conventional agreement-based evaluation metrics.",USA,education,Developed economies,"[-33.190586, 11.065468]","[42.34464, -2.6580157]","[-14.625146, 10.405197, 2.8918555]","[25.68235, 10.252006, 1.4198182]","[14.193289, 9.384352]","[11.5211935, 3.4340801]","[15.0544405, 13.950747, -0.75725836]","[13.017265, 6.266551, 11.193774]"
62,Tatsuya Kako;Yasunori Ohishi;Hirokazu Kameoka;Kunio Kashino;Kazuya Takeda,Automatic Identification for Singing Style based on Sung Melodic Contour Characterized in Phase Plane.,2009,https://doi.org/10.5281/zenodo.1417415,Tatsuya Kako+Nagoya University>JPN>education;Yasunori Ohishi+NTT Corporation>JPN>company;Hirokazu Kameoka+NTT Corporation>JPN>company;Kunio Kashino+NTT Corporation>JPN>company;Kazuya Takeda+Nagoya University>JPN>education,"A stochastic representation of singing styles is proposed. The dynamic property of melodic contour, i.e., fundamental frequency (F0) sequence, is assumed to be the main cue for singing styles because it can characterize such typical ornamentations as vibrato. F0 signal trajectories in the phase plane are used as the basic representation. By fitting Gaussian mixture models to the observed F0 trajectories in the phase plane, a parametric representation is obtained by a set of GMM parameters. The effectiveness of our proposed method is confirmed through experimental evaluation where 94.1% accuracy for singer-class discrimination was obtained.",JPN,education,Developed economies,"[-8.603339, -31.887928]","[2.9952216, -10.464796]","[15.248691, 10.890762, -15.545334]","[11.967195, -11.0482025, -13.936179]","[10.246566, 11.028371]","[7.076158, 2.1723385]","[11.361462, 15.295832, 0.40698272]","[9.076538, 7.683575, 11.492824]"
63,Philippe Hamel;Sean Wood;Douglas Eck,Automatic Identification of Instrument Classes in Polyphonic and Poly-Instrument Audio.,2009,https://doi.org/10.5281/zenodo.1415092,Philippe Hamel+Universit´e de Montr´eal>CAN>education;Sean Wood+Universit´e de Montr´eal>CAN>education;Douglas Eck+Universit´e de Montr´eal>CAN>education,"We present and compare several models for automatic identification of instrument classes in polyphonic and poly-instrument audio. The goal is to be able to identify which categories of instrument (Strings, Woodwind, Guitar, Piano, etc.) are present in a given audio example. We use a machine learning approach to solve this task. We constructed a system to generate a large database of musically relevant poly-instrument audio. Our database is generated from hundreds of instruments classified in 7 categories. Musical audio examples are generated by mixing multi-track MIDI files with thousands of instrument combinations. We compare three different classifiers: a Support Vector Machine (SVM), a Multilayer Perceptron (MLP) and a Deep Belief Network (DBN). We show that the DBN tends to outperform both the SVM and the MLP in most cases.",CAN,education,Developed economies,"[7.9301677, -22.071617]","[-10.831351, 2.9063525]","[16.172277, -5.7616463, -3.0068579]","[7.552286, -0.06951817, -12.500508]","[8.894085, 7.2240305]","[8.15629, 4.5589137]","[11.147583, 12.570331, 0.45139867]","[10.125485, 7.2407866, 9.868558]"
64,Brian Tomasik;Joon Hee Kim;Margaret Ladlow;Malcolm Augat;Derek Tingle;Rich Wicentowski;Douglas Turnbull,Using Regression to Combine Data Sources for Semantic Music Discovery.,2009,https://doi.org/10.5281/zenodo.1415558,Brian Tomasik+Swarthmore College>USA>education;Joon Hee Kim+Swarthmore College>USA>education;Margaret Ladlow+Swarthmore College>USA>education;Malcolm Augat+Swarthmore College>USA>education;Derek Tingle+Swarthmore College>USA>education;Richard Wicentowski+Swarthmore College>USA>education;Douglas Turnbull+Swarthmore College>USA>education,"In the process of automatically annotating songs with descriptive labels, multiple types of input information can be used. These include keyword appearances in web documents, acoustic features of the song’s audio content, and similarity with other tagged songs. Given these individual data sources, we explore the question of how to aggregate them. We find that fixed-combination approaches like sum and max perform well but that trained linear regression models work better. Retrieval performance improves with more data sources. On the other hand, for large numbers of training songs, Bayesian hierarchical models that aim to share information across individual tag regressions offer no advantage.",USA,education,Developed economies,"[-25.613031, 16.351562]","[37.419353, -1.4387904]","[-17.296196, 3.5146692, -3.7924314]","[22.186222, 6.718871, 1.3588573]","[13.909591, 8.771727]","[11.318911, 3.4564455]","[14.516712, 14.172476, -1.2387378]","[12.879944, 6.437881, 11.207801]"
65,Xiao Hu 0001;J. Stephen Downie;Andreas F. Ehmann,Lyric Text Mining in Music Mood Classification.,2009,https://doi.org/10.5281/zenodo.1416790,Xiao Hu+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education,"This research examines the role lyric text can play in improving audio music mood classification. A new method is proposed to build a large ground truth set of 5,585 songs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relatively complete set of lyric features and representation models were investigated. The best performing lyric feature set was also compared to a leading audio-based system. In combining lyric and audio sources, hybrid feature sets built with three different feature selection methods were also examined. The results show patterns at odds with findings in previous studies: audio features do not always outperform lyrics features, and combining lyrics and audio features can improve performance in many mood categories, but not all of them.",USA,education,Developed economies,"[-52.912254, 1.1353105]","[52.740227, -3.841845]","[-17.104435, 24.596378, 8.4763565]","[15.16004, 22.436169, 9.580784]","[13.380118, 12.540011]","[13.0848875, 3.738376]","[16.022396, 14.934691, 1.4973358]","[14.176545, 5.133764, 10.712057]"
66,Xin Xu;Masaki Naito;Tsuneo Kato;Hisashi Kawai,Robust and Fast Lyric Search based on Phonetic Confusion Matrix.,2009,https://doi.org/10.5281/zenodo.1418227,"Xin Xu+KDDI R&D Laboratories, Inc.>JPN>company;Masaki Naito+KDDI R&D Laboratories, Inc.>JPN>company;Tsuneo Kato+KDDI R&D Laboratories, Inc.>JPN>company;Hisashi Kawai+National Institute of Information and Communications Technology>JPN>facility","This paper proposes a robust and fast lyric search method for music information retrieval. Current lyric search systems by normal text retrieval techniques are severely deteriorated in the case that the queries of lyric phrases contain incorrect parts due to mishearing and misremembering. To solve this problem, the authors apply acoustic distance, which is computed based on a confusion matrix of an ASR experiment, into DP-based phonetic string matching. The experimental results show that the search accuracy is increased by more than 40% compared with the normal text retrieval method; and by 2% ∼4% compared with the conventional phonetic string matching method. Considering the high computation complexity of DP matching, the authors propose a novel two-pass search strategy to shorten the processing time. By pre-selecting the probable candidates by a rapid index-based search for the first pass and executing a DP-based search among these candidates during the second pass, the proposed method reduces processing time by 85.8% and keeps search accuracy at the same level as that of a complete search by DP matching with all lyrics.",JPN,company,Developed economies,"[-25.816017, -29.390865]","[12.022342, 12.786085]","[10.292364, 17.674997, -6.33404]","[11.251678, -12.167423, 13.670096]","[11.143768, 11.612855]","[9.156249, 0.64422226]","[12.176779, 15.834568, 0.8445226]","[10.65951, 5.952598, 13.122426]"
67,Phillip B. Kirlin,Using Harmonic and Melodic Analyses to Automate the Initial Stages of Schenkerian Analysis.,2009,https://doi.org/10.5281/zenodo.1416654,Phillip B. Kirlin+University of Massachusetts Amherst>USA>education,"Structural music analysis is used to reveal the inner workings of a musical composition by recursively applying reductions to the music, resulting in a series of successively more abstract views of the composition. Schenkerian analysis is the most well-developed type of structural analysis, and while there is a wide body of research on the theory, there is no well-defined algorithm to perform such an analysis. A automated algorithm for Schenkerian analysis would be extremely useful to music scholars and researchers studying music from a computational standpoint. The first major step in producing a Schenkerian analysis involves selecting notes from the composition in question for the primary soprano and bass parts of the analysis. We present an algorithm for this that uses harmonic and melodic analyses to accomplish this task.",USA,education,Developed economies,"[22.729046, 28.438055]","[-11.743245, 20.152842]","[-8.778668, -10.85496, 7.137259]","[-12.085247, -2.3336363, 7.4813414]","[11.741913, 7.826871]","[8.529412, 1.9353806]","[12.67866, 13.603411, -1.0955237]","[10.013419, 6.549328, 11.956978]"
51,Toni Heittola;Anssi Klapuri;Tuomas Virtanen,Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation.,2009,https://doi.org/10.5281/zenodo.1417377,Toni Heittola+Tampere University of Technology>FIN>education;Anssi Klapuri+Tampere University of Technology>FIN>education;Tuomas Virtanen+Tampere University of Technology>FIN>education,"This paper proposes a novel approach to musical instrument recognition in polyphonic audio signals by using a source-filter model and an augmented non-negative matrix factorization algorithm for sound separation. The mixture signal is decomposed into a sum of spectral bases modeled as a product of excitations and filters. The excitations are restricted to harmonic spectra and their fundamental frequencies are estimated in advance using a multipitch estimator, whereas the filters are restricted to have smooth frequency responses by modeling them as a sum of elementary functions on the Mel-frequency scale. The pitch and timbre information are used in organizing individual notes into sound sources. In the recognition, Mel-frequency cepstral coefficients are used to represent the coarse shape of the power spectrum of sound sources and Gaussian mixture models are used to model instrument-conditional densities of the extracted features. The method is evaluated with polyphonic signals, randomly generated from 19 instrument classes. The recognition rate for signals having six note polyphony reaches 59%.",FIN,education,Developed economies,"[8.208403, -40.839214]","[-44.759796, -23.091232]","[22.824, -5.391966, -3.4656253]","[-2.6634016, -11.104575, -31.691414]","[8.528282, 9.532526]","[6.6131606, 4.6566906]","[11.059158, 13.080821, 1.0255032]","[9.900399, 8.480147, 10.159095]"
68,James B. Maxwell;Philippe Pasquier;Arne Eigenfeldt,Hierarchical Sequential Memory for Music: A Cognitive Model.,2009,https://doi.org/10.5281/zenodo.1414850,James B. Maxwell+Simon Fraser University>CAN>education;Philippe Pasquier+Simon Fraser University>CAN>education;Arne Eigenfeldt+Simon Fraser University>CAN>education,"We propose a new machine-learning framework called the Hierarchical Sequential Memory for Music, or HSMM. The HSMM is an adaptation of the Hierarchical Temporal Memory (HTM) framework, designed to make it better suited to musical applications. The HSMM is an online learner, capable of recognition, generation, continuation, and completion of musical structures.",CAN,education,Developed economies,"[-10.988452, 0.6242041]","[-8.773624, -36.163418]","[2.8143706, 5.1208844, 16.122068]","[-17.599388, 4.6887894, -7.138169]","[11.731228, 8.680716]","[8.818409, 5.7030783]","[13.273013, 13.550676, -0.32057202]","[9.69765, 6.009522, 9.224805]"
70,Diane Hu;Lawrence K. Saul,A Probabilistic Topic Model for Unsupervised Learning of Musical Key-Profiles.,2009,https://doi.org/10.5281/zenodo.1415160,"Diane J. Hu+University of California, San Diego>USA>education;Lawrence K. Saul+University of California, San Diego>USA>education","We describe a probabilistic model for learning musical key-profiles from symbolic files of polyphonic, classical music. Our model is based on Latent Dirichlet Allocation (LDA), a statistical approach for discovering hidden topics in large corpora of text. In our adaptation of LDA, symbolic music files play the role of text documents, groups of musical notes play the role of words, and musical key-profiles play the role of topics. The topics are discovered as significant, recurring distributions over twelve neutral pitch-classes. Though discovered automatically, these distributions closely resemble the traditional key-profiles used to indicate the stability and importance of neutral pitch-classes in the major and minor keys of western music. Unlike earlier approaches based on human judgement, our model learns key-profiles in an unsupervised manner, inferring them automatically from a large musical corpus that contains no key annotations. We show how these learned key-profiles can be used to determine the key of a musical piece and track its harmonic modulations. We also show how the model’s inferences can be used to compare musical pieces based on their harmonic structure.",USA,education,Developed economies,"[-10.492762, 12.954842]","[-13.142185, 13.297605]","[-3.8985605, 13.416085, -5.7442355]","[-8.057582, -2.4424021, 1.8433684]","[12.826572, 8.834211]","[7.568696, 2.7353437]","[13.441151, 14.6204815, -0.7015265]","[10.097504, 7.982707, 11.902408]"
71,Dan Tidhar;György Fazekas;Sefki Kolozali;Mark B. Sandler,Publishing Music Similarity Features on the Semantic Web.,2009,https://doi.org/10.5281/zenodo.1417071,"Dan Tidhar+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility;György Fazekas+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility;Sefki Kolozali+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility;Mark Sandler+Queen Mary, University of London>GBR>education|Centre for Digital Music>GBR>facility","We describe the process of collecting, organising and publishing a large set of music similarity features produced by the SoundBite playlist generator tool. These data can be a valuable asset in the development and evaluation of new Music Information Retrieval algorithms. They can also be used in Web-based music search and retrieval applications. For this reason, we make a database of features available on the Semantic Web via a SPARQL end-point, which can be used in Linked Data services. We provide examples of using the data in a research tool, as well as in a simple web application which responds to audio queries and finds a set of similar tracks in our database.",GBR,education,Developed economies,"[-28.299585, 13.396055]","[18.094442, 36.378143]","[-20.9041, 4.7558856, -2.641188]","[-5.167485, -1.161345, 24.042593]","[13.9279, 9.154277]","[10.757838, 0.28498808]","[14.767364, 14.242796, -1.0875528]","[12.014823, 5.3067546, 12.156662]"
72,Jakob Abeßer;Hanna M. Lukashevich;Christian Dittmar;Gerald Schuller,Genre Classification Using Bass-Related High-Level Features and Playing Styles.,2009,https://doi.org/10.5281/zenodo.1417697,Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Hanna Lukashevich+Fraunhofer IDMT>DEU>facility;Christian Dittmar+Fraunhofer IDMT>DEU>facility;Gerald Schuller+Fraunhofer IDMT>DEU>facility,"Considering its mediation role between the poles of rhythm, harmony, and melody, the bass plays a crucial role in most music genres. This paper introduces a novel set of transcription-based high-level features that characterize the bass and its interaction with other participating instruments. Furthermore, a new method to model and automatically retrieve different genre-specific bass playing styles is presented. A genre classification task is used as a benchmark to compare common machine learning algorithms based on the presented high-level features with a classification algorithm solely based on detected bass playing styles.",DEU,facility,Developed economies,"[-26.638836, -18.24084]","[17.711952, -10.410517]","[-13.126548, 2.7839372, 24.161272]","[8.751729, 12.957287, -6.3754826]","[12.616539, 10.8007345]","[9.368986, 3.305348]","[13.627738, 14.110197, 1.3668625]","[11.267515, 7.4350033, 10.88716]"
73,Hanna M. Lukashevich;Jakob Abeßer;Christian Dittmar;Holger Großmann,From Multi-Labeling to Multi-Domain-Labeling: A Novel Two-Dimensional Approach to Music Genre Classification.,2009,https://doi.org/10.5281/zenodo.1417535,Hanna Lukashevich+Fraunhofer Institute for Digital Media Technologies>DEU>facility;Jakob Abeßer+Fraunhofer Institute for Digital Media Technologies>DEU>facility;Christian Dittmar+Fraunhofer Institute for Digital Media Technologies>DEU>facility;Holger Grossmann+Fraunhofer Institute for Digital Media Technologies>DEU>facility,"In this publication we describe a novel two-dimensional approach for automatic music genre classification. Although the subject poses a well studied task in Music Information Retrieval, some fundamental issues of genre classification have not been covered so far. Especially many modern genres are influenced by manifold musical styles. Most of all, this holds true for the broad category “World Music”, which comprises many different regional styles and a mutual mix up thereof. A common approach to tackle this issue in manual categorization is to assign multiple genre labels to a single recording. However, for commonly used automatic classification algorithms, multi-labeling poses a problem due to its ambiguities. Thus, we propose to break down multi-label genre annotations into single-label annotations within given time segments and musical domains. A corresponding multi-stage evaluation based on a representative set of items from a global music taxonomy is performed and discussed accordingly. Therefore, we conduct 3 different experiments that cover multi-labeling, multi-labeling with time segmentation and the proposed multi-domain labeling.",DEU,facility,Developed economies,"[-29.248587, -11.273364]","[31.299189, -1.8873823]","[-16.273937, 6.956014, 12.30904]","[16.760723, 9.804146, 0.5422086]","[13.15656, 10.928765]","[10.832601, 3.5667102]","[14.21415, 14.24648, 1.3377638]","[12.404615, 6.348993, 10.969201]"
74,Dimitri Diakopoulos;Owen Vallis;Jordan Hochenbaum;Jim W. Murphy;Ajay Kapur,21st Century Electronica: MIR Techniques for Classification and Performance.,2009,https://doi.org/10.5281/zenodo.1416278,Dimitri Diakopoulos+California Institute of the Arts>USA>education;Owen Vallis+New Zealand School of Music>NZL>education;Jordan Hochenbaum+New Zealand School of Music>NZL>education;Jim Murphy+California Institute of the Arts>USA>education;Ajay Kapur+New Zealand School of Music>NZL>education,"The performance of electronica by Disc Jockys (DJs) presents a unique opportunity to develop interactions between performer and music. Through recent research in the MIR field, new tools for expanding DJ performance are emerging. The use of spectral, loudness, and temporal descriptors for the classification of electronica is explored. Our research also introduces the use of a multi-touch interface to drive a performance-oriented DJ application utilizing the feature set. Furthermore, we present that a multi-touch surface provides an extensible and collaborative interface for browsing and manipulating MIR-related data in real time.",USA,education,Developed economies,"[-10.262433, 57.887966]","[8.617161, 33.060406]","[-37.43681, 0.66043806, -3.6511402]","[-12.085527, 2.9482293, 20.049196]","[13.6192045, 4.7156157]","[11.087823, 1.1236541]","[15.053315, 11.117553, -1.5063679]","[11.275096, 4.989109, 11.894617]"
106,Kerstin Bischoff;Claudiu S. Firan;Raluca Paiu;Wolfgang Nejdl;Cyril Laurier;Mohamed Sordo,Music Mood and Theme Classification - a Hybrid Approach.,2009,https://doi.org/10.5281/zenodo.1417317,"Kerstin Bischoff+L3S Research Center>DEU>facility;Claudiu S. Firan+L3S Research Center>DEU>facility;Raluca Paiu+L3S Research Center>DEU>facility;Wolfgang Nejdl+L3S Research Center>DEU>facility;Cyril Laurier+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Mohamed Sordo+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Music perception is highly intertwined with both emotions and context. Not surprisingly, many of the users’ information seeking actions aim at retrieving music songs based on these perceptual dimensions – moods and themes, expressing how people feel about music or which situations they associate it with. In order to successfully support music retrieval along these dimensions, powerful methods are needed. Still, most existing approaches aiming at inferring some of the songs’ latent characteristics focus on identifying musical genres. In this paper we aim at bridging this gap between users’ information needs and indexed music features by developing algorithms for classifying music songs by moods and themes. We extend existing approaches by also considering the songs’ thematic dimensions and by using social data from the Last.fm music portal, as support for the classification tasks. Our methods exploit both audio features and collaborative user annotations, fusing them to improve overall performance. Evaluation performed against the AllMusic.com ground truth shows that both kinds of information are complementary and should be merged for enhanced classification accuracy.",DEU,facility,Developed economies,"[-54.031265, 3.6778443]","[50.536537, -4.042618]","[-18.452362, 26.189981, 5.241675]","[12.670863, 19.750069, 6.6590276]","[13.490966, 12.542356]","[12.902363, 3.5307333]","[16.070696, 14.838854, 1.4070457]","[13.996733, 5.186542, 10.8985615]"
76,Makoto P. Kato,RhythMiXearch: Searching for Unknown Music by Mixing Known Music.,2009,https://doi.org/10.5281/zenodo.1416542,Makoto P. Kato+Kyoto University>JPN>education,"We present a novel method for searching for unknown music. RhythMiXearch is a music search system we developed that can accept two music inputs and mix those inputs to search for music that could reasonably be a result of the mixture. This approach expands the ability of Query-by-Example and allows greater flexibility for users in finding unknown music. Each music piece stored by our system is characterized by text data written by users, i.e., review data. We used Latent Dirichlet Allocation (LDA) to capture semantics from the reviews that were then used to characterize the music by Hevner’s eight impression categories. RhythMiXearch mixes two music inputs in accordance with a probabilistic mixture model and finds music that is the most likely product of the mixture. Our experimental results indicate that the proposed method is comparable to human in searching for music by multiple examples.",JPN,education,Developed economies,"[-24.606628, 16.592705]","[31.19325, 14.034703]","[-6.2757974, -1.1252778, -11.051265]","[16.45924, -1.1956451, 13.861349]","[13.779808, 7.960419]","[11.39108, 2.2921436]","[13.773372, 14.4820385, -2.0001383]","[12.475138, 6.021778, 12.443631]"
77,Benjamin Martin 0001;Matthias Robine;Pierre Hanna,Musical Structure Retrieval by Aligning Self-Similarity Matrices.,2009,https://doi.org/10.5281/zenodo.1416502,Benjamin Martin+LaBRI - University of Bordeaux>FRA>education;Matthias Robine+LaBRI - University of Bordeaux>FRA>education;Pierre Hanna+LaBRI - University of Bordeaux>FRA>education,"We propose a new retrieval system based on musical structure using symbolic structural queries. The aim is to compare musical form in audio files without extracting explicitly the underlying audio structure. From a given or arbitrary segmentation, an audio file is segmented. Irrespective of the audio feature choice, we then compute a self-similarity matrix whose coefficients correspond to the estimation of the similarity between entire parts, obtained by local alignment. Finally, we compute a binary matrix from the symbolic structural query and compare it to the audio segmented matrix, which provides a structural similarity score. We perform experiments using large databases of audio files, and prove robustness to possible imprecisions in the structural query.",FRA,education,Developed economies,"[-4.1768823, 6.3399777]","[19.291298, 9.330854]","[-3.9519503, -2.6522498, -3.0186822]","[9.114853, -3.8934894, 5.709375]","[12.432779, 8.724998]","[9.690529, 1.7288848]","[13.055779, 14.23165, -0.571374]","[11.204511, 6.6568384, 12.526858]"
78,Dirk Moelants;Olmo Cornelis;Marc Leman,Exploring African Tone Scales.,2009,https://doi.org/10.5281/zenodo.1416338,Dirk Moelants+Ghent University>BEL>education;Olmo Cornelis+University College Ghent>BEL>education;Marc Leman+Ghent University>BEL>education,"Key-finding is a central topic in Western music analysis and development of MIR tools. However, most approaches rely on the Western 12-tone scale, which is not universally used. African music does not follow a fixed tone scale. In order to classify and study African tone scales, we developed a system in which the pitch is first analyzed on a continuous scale. Peak analysis is then applied on these data to extract the actual scale used. This system has been applied to a selection of African music, it allows us to look for similarities using cross-correlation. Thus it provides an interesting tool for query-by-example and database management in collections of ethnic music which can not be simply classified according to keys. Next to this the data can be used for ethnomusicological research. The study of the intervals used in this collection, e.g., gives us evidence for Western influence, with recent recordings having a tendency to use more regular intervals.",BEL,education,Developed economies,"[-27.612974, 1.9403533]","[14.933374, 21.34273]","[-11.828021, 15.926376, 24.634184]","[-2.931407, 15.697902, 4.19288]","[11.992013, 9.595611]","[9.054839, 2.1411464]","[12.596046, 14.846851, -1.1309437]","[10.714195, 6.3425236, 11.999358]"
79,Nicola Montecchio;Nicola Orio,A Discrete Filter Bank Approach to Audio to Score Matching for Polyphonic Music.,2009,https://doi.org/10.5281/zenodo.1418093,Nicola Montecchio+University of Padova>ITA>education|Unknown>Unknown>Unknown;Nicola Orio+University of Padova>ITA>education|Unknown>Unknown>Unknown,"""This paper presents a system for tracking the position of a polyphonic music performance in a symbolic score, possibly in real time. The system, based on Hidden Markov Models, is briefly presented, focusing on specific aspects such as observation modeling based on discrete filterbanks, in contrast with traditional FFT-based approaches, and describing the approaches to decoding. Experimental results are provided to assess the validity of the presented model. Proof-of-concept applications are shown, which effectively employ the described approach beyond the traditional automatic accompaniment system.""",ITA,education,Developed economies,"[-7.1941514, -5.5369406]","[-15.394945, -7.891488]","[12.837493, -3.6503143, -10.274721]","[2.8287578, -14.046772, -2.077738]","[12.6910515, 7.759888]","[6.480267, 2.1599598]","[12.608664, 13.952384, -1.4124395]","[8.717151, 6.9251285, 10.880345]"
80,Eric Battenberg;David Wessel,Accelerating Non-Negative Matrix Factorization for Audio Source Separation on Multi-Core and Many-Core Architectures.,2009,https://doi.org/10.5281/zenodo.1417020,"Eric Battenberg+University of California, Berkeley>USA>education;David Wessel+University of California, Berkeley>USA>education","Non-negative matrix factorization (NMF) has been successfully used in audio source separation and parts-based analysis; however, iterative NMF algorithms are computationally intensive, and therefore, time to convergence is very slow on typical personal computers. In this paper, we describe high performance parallel implementations of NMF developed using OpenMP for shared-memory multi-core systems and CUDA for many-core graphics processors. For 20 seconds of audio, we decrease running time from 18.5 seconds to 2.6 seconds using OpenMP and 0.6 seconds using CUDA. These performance increases allow source separation to be carried out on entire songs in a number of seconds, a process which was previously impractical with respect to time. We give insight into how such significant speed gains were made and encourage the development and use of parallel music information retrieval software.",USA,education,Developed economies,"[11.858746, -46.6793]","[-47.008556, -28.026218]","[30.276459, 0.26275656, -10.512147]","[-9.78495, -12.968206, -28.651493]","[8.574728, 9.928141]","[6.324196, 5.241092]","[11.1511135, 13.767177, 1.4191176]","[9.655566, 8.741443, 9.569587]"
81,Peter van Kranenburg;Anja Volk;Frans Wiering;Remco C. Veltkamp,Musical Models for Melody Alignment.,2009,https://doi.org/10.5281/zenodo.1415608,Peter van Kranenburg+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"In this paper we show that the modeling of musical knowledge within alignment algorithms results in a successful similarity approach to melodies. The score of the alignment of two melodies is taken as a measure of similarity. We introduce a number of scoring functions that model the influence of different musical parameters. The evaluation of their retrieval performance on a well-annotated set of 360 folk-song melodies with various kinds of melodic variation, shows that a combination of pitch, rhythm and segmentation-based scoring functions performs best, with a mean average precision of 0.83.",NLD,education,Developed economies,"[14.776964, -10.963658]","[14.835482, 3.0226603]","[0.72877485, -11.774473, -3.8921304]","[4.8023224, 2.0644045, 3.7386935]","[10.982659, 6.710736]","[9.393485, 1.7832803]","[11.991421, 12.839567, -1.4190398]","[11.176676, 6.973502, 13.122165]"
82,Brian McFee;Gert R. G. Lanckriet,Heterogeneous Embedding for Subjective Artist Similarity.,2009,https://doi.org/10.5281/zenodo.1416284,"Brian McFee+University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education","""We describe an artist recommendation system which integrates several heterogeneous data sources to form a holistic similarity space. Using social, semantic, and acoustic features, we learn a low-dimensional feature transformation which is optimized to reproduce human-derived measurements of subjective similarity between artists. By producing low-dimensional representations of artists, our system is suitable for visualization and recommendation tasks.""",USA,education,Developed economies,"[-41.997192, 7.380759]","[35.510674, 7.864406]","[-26.857872, 12.238438, 9.890764]","[16.509281, 6.572985, 10.475558]","[14.179219, 10.002277]","[11.807867, 2.5367906]","[14.987543, 14.830464, -0.041496545]","[13.186764, 6.0064445, 12.264356]"
83,Masahiro Niitsuma;Tsutomu Fujinami;Yo Tomita,The Intersection of Computational Analysis and Music Manuscripts: A New Model for Bach Source Studies of the 21st Century.,2009,https://doi.org/10.5281/zenodo.1417935,"Masahiro Niitsuma+Queen's University, Belfast>GBR>education;Tsutomu Fujinami+Japan Advanced Institute of Science and Technology (JAIST)>JPN>education;Yo Tomita+Queen's University, Belfast>GBR>education","This paper addresses the intersection of computational analysis and musicological source studies. In musicology, scholars often find themselves in the situation where their methodologies are inadequate to achieve their goals. Their problems appear to be twofold: (1) the lack of scientific objectivity and (2) the over-reliance on new source discoveries. We propose three stages to resolve these problems, a preliminary result of which is shown. The successful outcome of this work will have a huge impact not only on musicology but also on a wide range of subjects.",GBR,education,Developed economies,"[3.5211477, 6.122981]","[27.13748, 37.473072]","[-9.101654, -5.52441, 7.0073605]","[-1.2869723, 6.7243648, 20.689682]","[12.205691, 7.845543]","[11.553223, 0.20424105]","[12.896735, 13.480313, -0.9942195]","[11.961065, 4.384563, 12.006449]"
69,Jessica Thompson 0001;Cory McKay;John Ashley Burgoyne;Ichiro Fujinaga,Additions and Improvements in the ACE 2.0 Music Classifier.,2009,https://doi.org/10.5281/zenodo.1416048,Jessica Thompson+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;John Ashley Burgoyne+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper presents additions and improvements to the Autonomous Classification Engine (ACE), a framework for using and optimizing classifiers. Given a set of feature values, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality-reduction techniques in order to arrive at a configuration that is well-suited to a given problem. Changes and additions have been made to ACE in order to increase its functionality as well as to make it easier to use and incorporate into other software frameworks. Details are provided on ACE’s remodeled class structure and associated API, the improved command line and graphical user interfaces, a new ACE XML 2.0 ZIP file format and expanded statistical reporting associated with cross validation. The resulting improved processing and methods of operation are also discussed.",CAN,education,Developed economies,"[-21.42025, -11.372607]","[16.514973, -4.2129974]","[-13.501766, -3.6115396, 11.946862]","[10.904822, 5.526805, -5.137407]","[12.5738325, 10.3705635]","[9.547616, 3.486766]","[13.536157, 13.934544, 0.9354716]","[11.14471, 6.984058, 10.544335]"
75,Eric Nichols;Dan Morris;Sumit Basu;Christopher Raphael,Relationships Between Lyrics and Melody in Popular Music.,2009,https://doi.org/10.5281/zenodo.1417321,Eric Nichols+Indiana University>USA>education|Microsoft Research>USA>company;Dan Morris+Microsoft Research>USA>company;Sumit Basu+Microsoft Research>USA>company;Christopher Raphael+Indiana University>USA>education,"Composers of popular music weave lyrics, melody, and instrumentation together to create a consistent and compelling emotional scene. The relationships among these elements are critical to musical communication, and understanding the statistics behind these relationships can contribute to numerous problems in music information retrieval and creativity support. In this paper, we present the results of an observational study on a large symbolic database of popular music; our results identify several patterns in the relationship between lyrics and melody.",USA,education,Developed economies,"[-32.726818, -25.382595]","[15.962995, 9.734678]","[8.452175, 22.50646, 4.917991]","[4.648091, -1.6713498, 7.4712486]","[12.29759, 11.748303]","[9.55268, 1.5422304]","[12.947701, 15.760419, 0.98761624]","[11.211673, 6.422516, 12.551033]"
67,Özgür Izmirli;Roger B. Dannenberg,Understanding Features and Distance Functions for Music Sequence Alignment.,2010,https://doi.org/10.5281/zenodo.1418353,Özgür İzmirli+Connecticut College>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education,"We investigate the problem of matching symbolic representations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representations that optimize the classification of “matching” vs. “non-matching” frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representation but they also reveal interesting projection structures that differ distinctly from the traditional chromagram.",USA,education,Developed economies,"[16.07594, -10.313246]","[-14.235638, -9.962958]","[-1.2922734, -14.113687, -4.559363]","[11.4645, -22.222385, -13.769959]","[11.109376, 6.485807]","[7.088095, 1.1769824]","[11.99422, 12.823792, -1.535169]","[9.141272, 6.4700327, 10.938764]"
97,Florian Eyben;Sebastian Böck;Björn W. Schuller;Alex Graves,Universal Onset Detection with Bidirectional Long Short-Term Memory Neural Networks.,2010,https://doi.org/10.5281/zenodo.1417131,Florian Eyben+Technische Universität München>DEU>education;Sebastian Böck+Technische Universität München>DEU>education;Björn Schuller+Technische Universität München>DEU>education;Alex Graves+Technische Universität München>DEU>education,"Many different onset detection methods have been proposed in recent years. However those that perform well tend to be highly specialised for certain types of music, while those that are more widely applicable give only moderate performance. In this paper we present a new onset detector with superior performance and temporal precision for all kinds of music, including complex music mixes. It is based on auditory spectral features and relative spectral differences processed by a bidirectional Long Short-Term Memory recurrent neural network, which acts as reduction function. The network is trained with a large database of onset data covering various genres and onset types. Due to the data driven nature, our approach does not require the onset detection method and its parameters to be tuned to a particular type of music. We compare results on the Bello onset data set and can conclude that our approach is on par with related results on the same set and outperforms them in most cases in terms of F1-measure. For complex music with mixed onset types, an absolute improvement of 3.6% is reported.",DEU,education,Developed economies,"[31.483322, -26.653517]","[-24.252983, -8.080603]","[8.052202, -23.622578, -8.292558]","[-2.4636424, 8.471178, -12.886571]","[10.264613, 4.9951363]","[5.6031594, 2.4938707]","[10.324284, 13.231628, -1.5727991]","[7.9049096, 7.434668, 10.781036]"
79,Michael O. Jewell;Christophe Rhodes;Mark d'Inverno,Querying Improvised Music: Do You Sound Like Yourself?.,2010,https://doi.org/10.5281/zenodo.1417227,"Michael O. Jewell+Goldsmiths, University of London>GBR>education;Christophe Rhodes+Goldsmiths, University of London>GBR>education;Mark d’Inverno+Goldsmiths, University of London>GBR>education","Improvisers are often keen to assess how their performance practice stands up to an ideal: whether that ideal is of technical accuracy or instant composition of material meeting complex harmonic constraints at speed. This paper reports on the development of an interface for querying and navigating a collection of recorded material for the purpose of presenting information on musical similarity, and the application of this interface to the investigation of a set of recordings by jazz performers. We investigate the retrieval performance of our tool, and in analysing the ‘hits’ and particularly the ‘misses’, provide information suggesting a change in one of the authors’ improvisation style.",GBR,education,Developed economies,"[-31.392546, 19.612713]","[7.204443, 20.911964]","[-15.642556, 13.336566, -2.751553]","[-12.246377, 10.965666, 9.466156]","[14.052151, 8.315777]","[9.524446, 1.790563]","[14.601525, 14.685579, -1.5669497]","[9.5304985, 6.141153, 11.90997]"
78,Akira Maezawa;Masataka Goto;Hiroshi G. Okuno,Query-by-conducting: An Interface to Retrieve Classical-music Interpretations by Real-time Tempo Input.,2010,https://doi.org/10.5281/zenodo.1416614,Akira Maezawa+Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroshi G. Okuno+Kyoto University>JPN>education,"This paper presents an interface for finding interpretations of a user-specified music, Query-by-Conducting. In classical music, there are many interpretations to a particular piece, and finding “the” interpretation that matches the listener’s taste allows a listener to further enjoy the piece. The critical issue in finding such an interpretation is the way or interface to allow the listener to listen through different interpretations. Our interface allows a user, by swinging a conducting hardware interface, to conduct the desired global tempo along the playback of a piece, at any time in the piece. The real-time conducting input by the user dynamically switches the interpretation being played back to the one closest to how the user is currently conducting. At the end of the piece, our interface ranks each interpretation according to how close the tempo of each interpretation was to the user input. At the core of our interface is an automated tempo estimation method based on audio-score alignment. We improve tempo estimation by requiring the audio-score alignment of different interpretations to be consistent with each other. We evaluate the tempo estimation method using a solo, chamber, and orchestral repertoire. The proposed tempo estimation decreases the error by as much as 0.94 times the original error.",JPN,education,Developed economies,"[-14.508218, 23.896303]","[-29.97051, -0.55616254]","[-7.736919, -2.5129607, -8.305602]","[-7.559938, 4.035044, -7.170202]","[13.117994, 7.192975]","[5.5826473, 1.4193105]","[13.268218, 13.875356, -2.0371397]","[7.86518, 6.3522477, 11.121645]"
77,Ching-Hua Chuan;Elaine Chew,Quantifying the Benefits of Using an Interactive Decision Support Tool for Creating Musical Accompaniment in a Particular Style.,2010,https://doi.org/10.5281/zenodo.1417337,Ching-Hua Chuan+University of North Florida>USA>education;Elaine Chew+University of Southern California>USA>education,"We present a human-centered experiment designed to measure the degree of support for creating musical accompaniment provided by an interactive composition decision-support system. We create an interactive system with visual and audio cues to assist users in the choosing of chords to craft an accompaniment in a desired style. We propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quantitative measures of musical distance – percentage correct and closely related chords, and average neo-Riemannian distance – compare the user-created accompaniment with the original, with and without decision support. Numbers of backward edits, unique chords explored, and repeated chord choices during composition help quantify composition behavior. We present experimental data from musicians and non-musicians. We observe that decision support reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as repeated chord choices, and the gap between musicians’ and non-musicians’ work, without significantly limiting the range of users’ choices.",USA,education,Developed economies,"[-10.705226, 5.221996]","[0.023733994, -39.530483]","[3.7271755, 12.966362, 26.986202]","[-23.409143, 6.2341986, -0.36699182]","[10.820403, 8.671933]","[9.570774, 6.106244]","[13.657453, 12.337412, -0.29817057]","[9.980678, 5.2374215, 9.595531]"
76,Erik M. Schmidt;Youngmoo E. Kim,Prediction of Time-varying Musical Mood Distributions from Audio.,2010,https://doi.org/10.5281/zenodo.1416238,Erik M. Schmidt+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"The appeal of music lies in its ability to express emotions, and it is natural for us to organize music in terms of emotional associations. But the ambiguities of emotions make the determination of a single, unequivocal response label for the mood of a piece of music unrealistic. We address this lack of specificity by modeling human response labels to music in the arousal-valence (A-V) representation of affect as a stochastic distribution. Based upon our collected data, we present and evaluate methods using multiple sets of acoustic features to estimate these mood distributions parametrically using multivariate regression. Furthermore, since the emotional content of music often varies within a song, we explore the estimation of these A-V distributions in a time-varying context, demonstrating the ability of our system to track changes on a short-time basis.",USA,education,Developed economies,"[-57.758686, 3.2848263]","[48.666035, -9.610521]","[-23.293076, 24.917477, 3.375663]","[9.49234, 23.805067, 2.2073371]","[13.792551, 12.77233]","[13.0089655, 4.213391]","[16.23688, 14.639636, 1.6798961]","[14.088776, 5.036534, 10.251891]"
75,Aline K. Honingh;Rens Bod,Pitch Class Set Categories as Analysis Tools for Degrees of Tonality.,2010,https://doi.org/10.5281/zenodo.1417533,Aline Honingh+University of Amsterdam>NLD>education;Rens Bod+University of Amsterdam>NLD>education,"This is an explorative paper in which we present a new method for music analysis based on pitch class set categories. It has been shown before that pitch class sets can be divided into six different categories. Each category inherits a typical character which can “tell” something about the music in which it appears. In this paper we explore the possibilities of using pitch class set categories for 1) classification in major/minor mode, 2) classification in tonal/atonal music, 3) determination of a degree of tonality, and 4) determination of a composer’s period.",NLD,education,Developed economies,"[22.679007, -20.36308]","[13.016084, -1.7548028]","[16.648209, -12.585547, -13.307042]","[4.1140175, 6.111715, -0.124086566]","[10.18042, 6.0129824]","[8.964701, 2.6665163]","[11.440182, 13.3832445, -0.67511106]","[10.644233, 7.242187, 11.670306]"
74,Andre Holzapfel;Yannis Stylianou,Parataxis: Morphological Similarity in Traditional Music.,2010,https://doi.org/10.5281/zenodo.1416896,"Andre Holzapfel+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education;Yannis Stylianou+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education","In this paper an automatic system for the detection of similar phrases in music of the Eastern Mediterranean is proposed. This music follows a specific structure, which is referred to as parataxis. The proposed system can be applied to audio signals of complex mixtures that contain the lead melody together with instrumental accompaniment. It is shown that including a lead melody estimation into a state-of-the-art system for cover song detection leads to promising results on a dataset of transcribed traditional dances from the island of Crete in Greece. Furthermore, a general framework that includes also rhythmic aspects is proposed. The proposed method represents a simple framework for the support of ethnomusicological studies on related forms of traditional music.",GRC,facility,Developed economies,"[2.9080698, 11.96726]","[9.152197, 0.10372639]","[-12.196634, -18.613573, -10.7508335]","[1.835765, 9.24545, -2.1487703]","[12.255645, 9.747935]","[6.554577, 1.5689435]","[12.485183, 15.154335, -1.2462229]","[8.657789, 6.7708573, 12.036513]"
73,Markus Schedl,On the Use of Microblogging Posts for Similarity Estimation and Artist Labeling.,2010,https://doi.org/10.5281/zenodo.1418215,Markus Schedl+Johannes Kepler University>AUT>education,"Microblogging services, such as Twitter, have risen enormously in popularity during the past years. Despite their popularity, such services have never been analyzed for MIR purposes, to the best of our knowledge. We hence present first investigations of the usability of music artist-related microblogging posts to perform artist labeling and similarity estimation tasks. To this end, we look into different text-based indexing models and term weighting measures. Two artist collections are used for evaluation, and the different methods are evaluated against data from last.fm. We show that microblogging posts are a valuable source for musical meta-data.",AUT,education,Developed economies,"[-40.97353, 7.876822]","[50.666374, 11.821333]","[-23.782957, 12.461494, 7.7031198]","[22.284727, 12.563904, 13.426951]","[14.218439, 10.014991]","[12.18905, 2.4553034]","[14.995314, 14.849979, -0.0597842]","[13.326382, 5.7904434, 12.08279]"
72,Benoît Mathieu;Slim Essid;Thomas Fillon;Jacques Prado;Gaël Richard,"YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software.",2010,https://doi.org/10.5281/zenodo.1418321,Benoit Mathieu+Institut Telecom>FRA>education;Slim Essid+Institut Telecom>FRA>education;Thomas Fillon+Institut Telecom>FRA>education;Jacques Prado+Institut Telecom>FRA>education;Gaël Richard+Institut Telecom>FRA>education,"Music Information Retrieval systems are commonly built on a feature extraction stage. For applications involving automatic classification (e.g. speech/music discrimination, music genre or mood recognition, ...), traditional approaches will consider a large set of audio features to be extracted on a large dataset. In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efficient feature extraction. In this paper, a new audio feature extraction software, YAAFE, is presented and compared to widely used libraries. The main advantage of YAAFE is a significantly lower complexity due to the appropriate exploitation of redundancy in the feature calculation. YAAFE remains easy to configure and each feature can be parameterized independently. Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License.",FRA,education,Developed economies,"[-10.746276, -17.066507]","[9.757486, 28.463778]","[0.56139517, -5.223836, -16.501093]","[-5.3476634, -1.5534196, 14.861739]","[12.4927025, 7.8960214]","[10.315342, 1.4901117]","[13.220738, 13.5848, 0.26735944]","[11.248553, 5.760994, 11.5545025]"
71,Steven K. Tjoa;K. J. Ray Liu,Musical Instrument Recognition using Biologically Inspired Filtering of Temporal Dictionary Atoms.,2010,https://doi.org/10.5281/zenodo.1416166,Steven K. Tjoa+University of Maryland>USA>education;K. J. Ray Liu+University of Maryland>USA>education,"Most musical instrument recognition systems rely entirely upon spectral information instead of temporal information. In this paper, we test the hypothesis that temporal information can improve upon the accuracy achievable by the state of the art in instrument recognition. Unlike existing temporal classification methods which use traditional features such as temporal moments, we extract novel features from temporal atoms generated by nonnegative matrix factorization by using a multiresolution gamma filterbank. Among isolated sounds taken from twenty-four instrument classes, the proposed system can achieve 92.3% accuracy, thus improving upon the state of the art.",USA,education,Developed economies,"[9.722126, -24.469267]","[-47.212708, -21.034206]","[20.8203, -7.726353, -0.60281837]","[-3.8619635, -12.4532585, -33.60668]","[8.807277, 6.9739623]","[6.7291327, 4.4801297]","[10.980109, 12.421882, 0.32585427]","[10.061452, 8.174834, 10.25786]"
70,Florian Kaiser;Thomas Sikora,Music Structure Discovery in Popular Music using Non-negative Matrix Factorization.,2010,https://doi.org/10.5281/zenodo.1418085,Florian Kaiser+Technische Universität Berlin>DEU>education|Unknown>Unknown>Unknown;Thomas Sikora+Technische Universität Berlin>DEU>education|Unknown>Unknown>Unknown,"We introduce a method for the automatic extraction of musical structures in popular music. The proposed algorithm uses non-negative matrix factorization to segment regions of acoustically similar frames in a self-similarity matrix of the audio data. We show that over the dimensions of the NMF decomposition, structural parts can easily be modeled. Based on that observation, we introduce a clustering algorithm that can explain the structure of the whole music piece. The preliminary evaluation we report in the paper shows very encouraging results.",DEU,education,Developed economies,"[-5.0046115, -1.0727468]","[2.536178, 0.2470139]","[5.5750146, -7.443513, 3.4289975]","[-0.09982978, 1.4647068, -6.268871]","[11.657056, 8.193619]","[7.0714173, 3.9858303]","[12.493705, 13.779913, -0.029667778]","[10.423574, 8.361549, 10.993343]"
69,Robert Macrae;Simon Dixon,Accurate Real-time Windowed Time Warping.,2010,https://doi.org/10.5281/zenodo.1416156,Robert Macrae+Queen Mary University of London>GBR>education|Centre for Digital Music>GBR>facility;Simon Dixon+Queen Mary University of London>GBR>education|Centre for Digital Music>GBR>facility,"Dynamic Time Warping (DTW) is used to find alignments between two related streams of information and can be used to link data, recognise patterns or find similarities. Typically, DTW requires the complete series of both input streams in advance and has quadratic time and space requirements. As such DTW is unsuitable for real-time applications and is inefficient for aligning long sequences. We present Windowed Time Warping (WTW), a variation on DTW that, by dividing the path into a series of DTW windows and making use of path cost estimation, achieves alignments with an accuracy and efficiency superior to other leading modifications and with the capability of synchronising in real-time. We demonstrate this method in a score following application. Evaluation of the WTW score following system found 97.0% of audio note onsets were correctly aligned within 2000 ms of the known time. Results also show reductions in execution times over state-of-the-art efficient DTW modifications.",GBR,education,Developed economies,"[52.541355, 10.282981]","[-17.529776, -16.664715]","[10.412977, -19.50299, -17.816916]","[1.6850733, -24.061262, -5.2172832]","[10.605505, 5.5889854]","[6.029774, 0.6417805]","[11.185859, 12.922496, -1.6452851]","[8.001607, 5.7433825, 10.893159]"
0,Tom Collins;Jeremy Thurlow;Robin C. Laney;Alistair Willis;Paul H. Garthwaite,A Comparative Evaluation of Algorithms for Discovering Translational Patterns in Baroque Keyboard Works.,2010,https://doi.org/10.5281/zenodo.1416070,Tom Collins+The Open University>GBR>education;Jeremy Thurlow+University of Cambridge>GBR>education;Robin Laney+The Open University>GBR>education;Alistair Willis+The Open University>GBR>education;Paul H. Garthwaite+The Open University>GBR>education,"We consider the problem of intra-opus pattern discovery, that is, the task of discovering patterns of a specified type within a piece of music. A music analyst undertook this task for works by Domenico Scarlattti and Johann Sebastian Bach, forming a benchmark of ‘target’ patterns. The performance of two existing algorithms and one of our own creation, called SIACT, is evaluated by comparison with this benchmark. SIACT out-performs the existing algorithms with regard to recall and, more often than not, precision. It is demonstrated that in all but the most carefully selected excerpts of music, the two existing algorithms can be affected by what is termed the ‘problem of isolated membership’. Central to the relative success of SIACT is our intention that it should address this particular problem. The paper contrasts string-based and geometric approaches to pattern discovery, with an introduction to the latter. Suggestions for future work are given.",GBR,education,Developed economies,"[27.687855, 13.443979]","[4.7147737, 17.27604]","[5.506987, -15.829752, 15.974102]","[1.0880163, -11.975059, 5.4899254]","[10.870185, 7.256385]","[8.797276, 1.245316]","[12.339343, 12.586117, -0.55234826]","[10.464985, 6.731773, 12.722166]"
1,Verena Konz;Meinard Müller;Sebastian Ewert,A Multi-Perspective Evaluation Framework for Chord Recognition.,2010,https://doi.org/10.5281/zenodo.1415686,Verena Konz+Saarland University>DEU>education|MPI Informatik>DEU>facility;Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Sebastian Ewert+University of Bonn>DEU>education,"The automated extraction of chord labels from audio recordings constitutes a major task in music information retrieval. To evaluate computer-based chord labeling procedures, one requires ground truth annotations for the underlying audio material. However, the manual generation of such annotations on the basis of audio recordings is tedious and time-consuming. On the other hand, trained musicians can easily derive chord labels from symbolic score data. In this paper, we bridge this gap by describing a procedure that allows for transferring annotations and chord labels from the score domain to the audio domain and vice versa. Using music synchronization techniques, the general idea is to locally warp the annotations of all given data streams onto a common time axis, which then allows for a cross-domain evaluation of the various types of chord labels. As a further contribution of this paper, we extend this principle by introducing a multi-perspective evaluation framework for simultaneously comparing chord recognition results over multiple performances of the same piece of music. The revealed inconsistencies in the results do not only indicate limitations of the employed chord labeling strategies but also deepen the understanding of the underlying music material.",DEU,education,Developed economies,"[54.382374, -5.0178156]","[-26.71903, 20.57857]","[27.075092, -14.637271, 16.179934]","[-26.539661, -2.0031502, 6.898368]","[6.77579, 8.649357]","[6.5513253, 3.4227]","[11.8716, 10.419907, 2.0702748]","[9.84317, 8.463261, 12.343288]"
2,Riccardo Miotto;Nicola Orio,A Probabilistic Approach to Merge Context and Content Information for Music Retrieval.,2010,https://doi.org/10.5281/zenodo.1415256,Riccardo Miotto+University of Padova>ITA>education|University of Padova>ITA>education;Nicola Orio+University of Padova>ITA>education,"An interesting problem in music information retrieval is how to combine the information from different sources in order to improve retrieval effectiveness. This paper introduces an approach to represent a collection of tagged songs through a hidden Markov model with the purpose to develop a system that merges in the same framework both acoustic similarity and semantic descriptions. The former provides content-based information on song similarity, the latter provides context-aware information about individual songs. Experimental results show how the proposed model leads to better performances than approaches that rank songs using both a single information source and their linear combination.",ITA,education,Developed economies,"[-15.326959, 18.20794]","[26.021435, 12.428885]","[-8.381472, 9.12921, -10.449393]","[12.600415, -2.0509388, 12.282297]","[13.730483, 8.327125]","[10.885962, 2.55975]","[13.790105, 14.735481, -1.7207663]","[12.173111, 6.1856475, 12.296234]"
3,Graham Grindlay;Daniel P. W. Ellis,A Probabilistic Subspace Model for Multi-instrument Polyphonic Transcription.,2010,https://doi.org/10.5281/zenodo.1416842,Graham Grindlay+Columbia University>USA>education|LabROSA>USA>facility;Daniel P. W. Ellis+Columbia University>USA>education|LabROSA>USA>facility,"In this paper we present a general probabilistic model suitable for transcribing single-channel audio recordings containing multiple polyphonic sources. Our system requires no prior knowledge of the instruments in the mixture, although it can benefit from this information if available. In contrast to many existing polyphonic transcription systems, our approach explicitly models the individual instruments and is thereby able to assign detected notes to their respective sources. We use a set of training instruments to learn a model space which is then used during transcription to constrain the properties of models fit to the target mixture. In addition, we encourage model sparsity using a simple approach related to tempering. We evaluate our method on both recorded and synthesized two-instrument mixtures, obtaining average frame-level F-measures of up to 0.60 for synthesized audio and 0.53 for recorded audio. If knowledge of the instrument types in the mixture is available, we can increase these measures to 0.68 and 0.58, respectively, by initializing the model with parameters from similar instruments.",USA,education,Developed economies,"[29.450615, -9.005194]","[-36.834988, -24.984856]","[10.849436, -7.54436, 10.961426]","[-8.666352, -0.30400398, -31.863844]","[9.489412, 7.789028]","[6.871083, 4.9842525]","[11.764124, 12.301885, 0.15261847]","[9.344357, 7.7495284, 9.720177]"
4,Maxime Le Coz;Hélène Lachambre;Lionel Koenig;Régine André-Obrecht,A Segmentation-based Tempo Induction Method.,2010,https://doi.org/10.5281/zenodo.1416766,"Maxime Le Coz+IRIT, Universite Paul Sabatier>FRA>education;Helene Lachambre+IRIT, Universite Paul Sabatier>FRA>education;Lionel Koenig+IRIT, Universite Paul Sabatier>FRA>education;Regine Andre-Obrecht+IRIT, Universite Paul Sabatier>FRA>education","The automatized beat detection and localization have been the subject of multiple research in the field of music information retrieval. Most of the methods are based on onset detection. We propose an alternative approach: Our method is based on the “Forward-Backward segmentation”: the segments may be interpreted as attacks, decays, sustains and releases of notes. We process the segment boundaries as a weighted Dirac signal. Three methods derived from its spectral analysis are proposed to find a periodicity which corresponds to the tempo. The experiments are carried out on a corpus of 100 songs of the RWC database. The performances of our system on this base demonstrate a potential in the use of a “Forward-Backward Segmentation” for temporal information retrieval in musical signals.",FRA,education,Developed economies,"[40.859, -27.458132]","[-25.129274, -4.6482797]","[-1.73032, -31.522818, -1.4691093]","[-3.3821254, 8.468842, -10.402683]","[11.480172, 4.4949045]","[5.5595794, 2.2389784]","[10.957066, 13.308433, -2.85756]","[7.84147, 7.3322678, 11.062802]"
5,Igor Vatolkin;Wolfgang M. Theimer;Martin Botteck,AMUSE (Advanced MUSic Explorer) - A Multitool Framework for Music Data Analysis.,2010,https://doi.org/10.5281/zenodo.1414918,Igor Vatolkin+TU Dortmund>DEU>education;Wolfgang Theimer+Research in Motion>DEU>company;Martin Botteck+Unknown>Unknown>Unknown,"A large variety of research tools is available now for music information retrieval tasks. In this paper we present a further framework which aims to facilitate the interaction between these applications. Since the available tools are very different in target domain, range of available methods, learning efforts, installation and runtime characteristics etc., it is not easy to find software which is optimal for certain research goals. Another problematic issue is that many incompatible data formats exist, so it is not always possible to use output from one tool just as input for another one. At first we describe some of the available projects and outline our motivation starting the development of AMUSE framework for audio data analysis. Requirements and application purposes are given. The structure of our framework is introduced in detail and the information for efficient application is provided. Finally we discuss several ideas for further work.",DEU,education,Developed economies,"[-18.33887, 14.774257]","[13.099035, 27.607115]","[-11.775459, -0.9840809, -11.919295]","[-1.0602257, -2.6850874, 15.456758]","[13.590621, 7.6655126]","[10.503712, 1.056744]","[14.000642, 14.128749, -1.7696414]","[11.31167, 5.4117904, 11.972608]"
6,Cyril Joder;Slim Essid;Gaël Richard,An Improved Hierarchical Approach for Music-to-symbolic Score Alignment.,2010,https://doi.org/10.5281/zenodo.1417883,"Cyril Joder+Institut TELECOM, TELECOM ParisTech>FRA>education;Slim Essid+Institut TELECOM, TELECOM ParisTech>FRA>education;Gaël Richard+Institut TELECOM, TELECOM ParisTech>FRA>education","We present an efficient approach for an off-line alignment of a symbolic score to a recording of the same piece, using a statistical model. A hidden state model is built from the score, which allows for the use of two different kinds of features, namely chroma vectors and an onset detection function (spectral flux) with specific production models, in a simple manner. We propose a hierarchical pruning method for an approximate decoding of this statistical model. This strategy reduces the search space in an adaptive way, yielding a better overall efficiency than the tested state-of-the art method. Experiments run on a large database of 94 pop songs show that the resulting system obtains higher recognition rates than the dynamic programming algorithm (DTW), with a significantly lower complexity, even though the rhythmic information is not used for the alignment.",FRA,education,Developed economies,"[17.95553, -11.0767]","[-14.605582, -10.896537]","[-0.96705407, -14.7226095, -7.8519115]","[0.38615075, -17.364239, -7.592051]","[10.936557, 6.4765267]","[6.526402, 1.0698469]","[12.066165, 12.532636, -1.5126301]","[8.520571, 6.2735925, 10.863868]"
68,Bernhard Niedermayer;Gerhard Widmer,A Multi-pass Algorithm for Accurate Audio-to-Score Alignment.,2010,https://doi.org/10.5281/zenodo.1415910,Bernhard Niedermayer+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence>AUT>facility,"Most current audio-to-score alignment algorithms work on the level of score time frames; i.e., they cannot differentiate between several notes occurring at the same discrete time within the score. This level of accuracy is sufficient for a variety of applications. However, for those that deal with, for example, musical expression analysis such micro-timings might also be of interest. Therefore, we propose a method that estimates the onset times of individual notes in a post-processing step. Based on the initial alignment and a feature obtained by matrix factorization, those notes for which the confidence in the alignment is high are chosen as anchor notes. The remaining notes in between are revised, taking into account the additional information about these anchors and the temporal relations given by the score. We show that this method clearly outperforms a reference method that uses the same features but does not differentiate between anchor and non-anchor notes.",AUT,education,Developed economies,"[20.02059, -15.435127]","[-18.812336, -14.302476]","[0.4439721, -15.855218, -12.596]","[-1.1556914, -21.00646, -4.919209]","[10.846241, 6.053962]","[6.0745378, 0.83530486]","[11.73768, 12.482655, -1.7842053]","[8.173865, 5.918888, 10.848762]"
7,Chung-Che Wang;Jyh-Shing Roger Jang;Wennen Wang,An Improved Query by Singing/Humming System Using Melody and Lyrics Information.,2010,https://doi.org/10.5281/zenodo.1414802,Chung-Che Wang+Tsing Hua University>TWN>education;Jyh-Shing Roger Jang+Tsing Hua University>TWN>education|Institute for Information Industry>Unknown>Unknown;Wennen Wang+Institute for Information Industry>Unknown>Unknown,"This paper proposes an improved query by singing/humming (QBSH) system using both melody and lyrics information for achieving better performance. Singing/humming discrimination (SHD) is first performed to distinguish singing from humming queries. For a humming query, we apply a pitch-only melody recognition method that has been used for QBSH task at MIREX with rank-1 performance. For a singing query, we combine the scores from melody recognition and lyrics recognition to take advantage of the extra lyrics information. Lyrics recognition is based on a modified tree lexicon that is commonly used in speech recognition. The performance of the overall QBSH system achieves 39.01% and 23.53% error reduction rates, respectively, for top-20 recognition under two experimental settings, indicating the feasibility of the proposed method.",TWN,education,Developing economies,"[-4.5048165, 36.20904]","[9.852092, 9.245273]","[-15.080242, -4.265886, -23.715101]","[4.5140705, -15.9369, 13.750483]","[14.923272, 6.1510863]","[8.564917, 0.6513318]","[13.167448, 15.321775, -2.8643632]","[10.306723, 6.0155883, 13.055871]"
8,Andrew Hankinson;Laurent Pugin;Ichiro Fujinaga,An Interchange Format for Optical Music Recognition Applications.,2010,https://doi.org/10.5281/zenodo.1417633,Andrew Hankinson+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility|McGill University>CAN>education;Laurent Pugin+RISM Switzerland>CHE>Unknown|Geneva University>CHE>education;Ichiro Fujinaga+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility|McGill University>CAN>education,"Page appearance and layout for music notation is a critical component of the overall musical information contained in a document. To capture and transfer this information, we outline an interchange format for OMR applications, the OMR Interchange Package (OIP) format, which is designed to allow layout information and page images to be preserved and transferred along with semantic musical content. We identify a number of uses for this format that can enhance digital representations of music, and introduce a novel idea for distributed optical music recognition system based on this format.",CAN,facility,Developed economies,"[39.347694, 21.408047]","[-21.877048, 39.045765]","[20.129988, 14.472973, 10.27281]","[-13.045003, -23.434702, -0.23353489]","[8.610425, 6.122464]","[6.667113, -0.6950213]","[10.658629, 11.129419, -0.15148167]","[7.864411, 4.090849, 10.57766]"
9,Dingding Wang 0001;Tao Li 0001;Mitsunori Ogihara,Are Tags Better Than Audio? The Effect of Joint Use of Tags and Audio Content Features for Artistic Style Clustering.,2010,https://doi.org/10.5281/zenodo.1417543,Dingding Wang+Florida International University>USA>education;Tao Li+Florida International University>USA>education;Mitsunori Ogihara+University of Miami>USA>education,"Social tags are receiving growing interests in information retrieval. In music information retrieval previous research has demonstrated that tags can assist in music classification and clustering. This paper studies the problem of combining tags and audio contents for artistic style clustering. After studying the effectiveness of using tags and audio contents separately for clustering, this paper proposes a novel language model that makes use of both data sources. Experiments with various methods for combining feature sets demonstrate that tag features are more useful than audio content features for style clustering and that the proposed model can marginally improve clustering performance by combining tags and audio contents.",USA,education,Developed economies,"[-39.816357, 2.6513839]","[41.753086, 2.965784]","[-17.814398, 12.074786, 6.8982077]","[23.736773, 9.6064415, 8.850015]","[14.362064, 10.287635]","[12.043344, 3.0487795]","[15.332969, 14.321001, -0.0070732054]","[13.325667, 6.010868, 11.579032]"
10,Shintaro Funasawa;Hiromi Ishizaki;Keiichiro Hoashi;Yasuhiro Takishima;Jiro Katto,Automated Music Slideshow Generation Using Web Images Based on Lyrics.,2010,https://doi.org/10.5281/zenodo.1415996,Shintaro Funasawa+Waseda University>JPN>education|KDDI R&D Laboratories Inc.>JPN>company;Hiromi Ishizaki+KDDI R&D Laboratories Inc.>JPN>company;Keiichiro Hoashi+KDDI R&D Laboratories Inc.>JPN>company;Yasuhiro Takishima+KDDI R&D Laboratories Inc.>JPN>company;Jiro Katto+Waseda University>JPN>education,"In this paper, we propose a system which automatically generates slideshows for music, by utilizing images retrieved from photo sharing web sites, based on query words extracted from song lyrics. The proposed system consists of two major steps: (1) query extraction from song lyrics, (2) image selection from web image search results. Moreover, in order to improve the display duration of each image in the slideshow, we adjust image transition timing by analyzing the duration of each lyric line in the input song. We have conducted subjective evaluation experiments, which prove that the proposal can generate impressive music slideshows for any input song.",JPN,education,Developed economies,"[-32.53243, -34.10921]","[23.2011, 20.659645]","[9.2494135, 25.678074, -3.769776]","[12.588407, -3.435497, 16.576378]","[11.468605, 11.85314]","[11.069655, 1.5979431]","[12.385198, 15.991392, 1.0870453]","[12.121269, 5.5959888, 12.668711]"
80,Arnaud Dessein;Arshia Cont;Guillaume Lemaitre,Real-time Polyphonic Music Transcription with Non-negative Matrix Factorization and Beta-divergence.,2010,https://doi.org/10.5281/zenodo.1414824,Arnaud Dessein+IRCAM – CNRS UMR 9912>FRA>facility;Arshia Cont+IRCAM – CNRS UMR 9912>FRA>facility;Guillaume Lemaitre+IRCAM – CNRS UMR 9912>FRA>facility,"In this paper, we investigate the problem of real-time polyphonic music transcription by employing non-negative matrix factorization techniques and the β-divergence as a cost function. We consider real-world setups where the music signal arrives incrementally to the system and is transcribed as it unfolds in time. The proposed transcription system is addressed with a modified non-negative matrix factorization scheme, called non-negative decomposition, where the incoming signal is projected onto a fixed basis of templates learned off-line prior to the decomposition. We discuss the use of non-negative matrix factorization with the β-divergence to achieve the real-time decomposition. The proposed system is evaluated on the specific task of piano music transcription and the results show that it can outperform several state-of-the-art off-line approaches.",FRA,facility,Developed economies,"[29.613186, -11.349127]","[-49.971157, -24.101389]","[10.179762, -6.877125, 7.449301]","[-6.379209, -15.977352, -27.0842]","[9.331093, 7.8195634]","[6.0665793, 4.814017]","[11.693289, 12.316757, 0.25291047]","[9.380395, 8.733594, 9.860679]"
81,Tim Crawford;Matthias Mauch;Christophe Rhodes,Recognising Classical Works in Historical Recordings.,2010,https://doi.org/10.5281/zenodo.1417429,"Tim Crawford+Goldsmiths, University of London>GBR>education;Matthias Mauch+Queen Mary, University of London>GBR>education;Christophe Rhodes+Goldsmiths, University of London>GBR>education","In collections of recordings of classical music, it is normal to find multiple performances, usually by different artists, of the same pieces of music. While there may be differences in many dimensions of musical similarity, such as timbre, pitch or structural detail, the underlying musical content is essentially and recognizably the same. The degree of divergence is generally less than that found between ‘cover songs’ in the domain of popular music, and much less than in typical performances of jazz standards. MIR methods, based around variants of the chroma representation, can be useful in tasks such as work identification especially where disco/bibliographical metadata is absent or incomplete as well as for access, curation and management of collections. We describe some initial experiments in work-recognition on a test-collection comprising c. 2000 digital transfers of historical recordings, and show that the use of NNLS chroma, a new, musically-informed chroma feature, dramatically improves recognition.",GBR,education,Developed economies,"[4.4036126, 26.050058]","[18.118122, 22.588036]","[-12.447587, -10.168304, -1.9678842]","[-4.2841873, 8.794146, 13.893932]","[12.358022, 7.299337]","[11.340794, 0.8503721]","[13.103844, 13.47385, -1.6065099]","[11.737535, 5.010932, 11.848174]"
82,Alan Marsden,Recognition of Variations Using Automatic Schenkerian Reduction.,2010,https://doi.org/10.5281/zenodo.1418179,Alan Marsden+Lancaster University>GBR>education,"Experiments on techniques to automatically recognise whether or not an extract of music is a variation of a given theme are reported, using a test corpus derived from ten of Mozart‘s sets of variations for piano. Methods which examine the notes of the ‘surface’ are compared with methods which make use of an automatically derived quasi-Schenkerian reduction of the theme and the extract in question. The maximum average F-measure achieved was 0.87. Unexpectedly, this was for a method of matching based on the surface alone, and in general the results for matches based on the surface were marginally better than those based on reduction, though the small number of possible test queries means that this result cannot be regarded as conclusive. Other inferences on which factors seem to be important in recognising variations are discussed. Possibilities for improved recognition of matching using reduction are outlined.",GBR,education,Developed economies,"[22.602222, 30.359903]","[13.868674, 8.441018]","[-13.692088, -17.929192, 11.097164]","[1.0410563, -4.153036, 7.915488]","[11.900191, 6.780053]","[8.724756, 1.8278955]","[13.068097, 12.553677, -1.4509254]","[10.550115, 6.5989404, 12.264568]"
83,James Bergstra;Michael I. Mandel;Douglas Eck,Scalable Genre and Tag Prediction with Spectral Covariance.,2010,https://doi.org/10.5281/zenodo.1416942,James Bergstra+University of Montreal>CAN>education;Michael Mandel+University of Montreal>CAN>education;Douglas Eck+University of Montreal>CAN>education,"Cepstral analysis is effective in separating source from filter in vocal and monophonic [pitched] recordings, but is it a good general-purpose framework for working with music audio? We evaluate covariance in spectral features as an alternative to means and variances in cepstral features (particularly MFCCs) as summaries of frame-level features. We find that spectral covariance is more effective than mean, variance, and covariance statistics of MFCCs for genre and social tag prediction. Support for our model comes from strong and state-of-the-art performance on the GTZAN genre dataset, MajorMiner, and MagnaTagatune. Our classification strategy based on linear classifiers is easy to implement, exhibits very little sensitivity to hyper-parameters, trains quickly (even for web-scale datasets), is fast to apply, and offers competitive performance in genre and tag prediction.",CAN,education,Developed economies,"[-42.60651, -1.6474115]","[14.865766, -14.587378]","[-15.618209, 17.364292, 9.556921]","[15.783394, 1.4299293, -7.402726]","[14.469463, 10.571966]","[9.696729, 3.5634015]","[15.613358, 14.133585, 0.10988933]","[11.327813, 7.4020762, 10.667172]"
96,Bryan Duggan;Brendan O'Shea,Tunepal - Disseminating a Music Information Retrieval System to the Traditional Irish Music Community.,2010,https://doi.org/10.5281/zenodo.1416312,Bryan Duggan+Dublin Institute of Technology>IRL>education|Dublin Institute of Technology>IRL>education;Brendan O’ Shea+Dublin Institute of Technology>IRL>education|Dublin Institute of Technology>IRL>education,"In this paper we present two new query-by-playing (QBP) music information retrieval (MIR) systems aimed at musicians playing traditional Irish dance music. Firstly, a browser hosted system - tunepal.org is presented. Secondly, we present Tunepal for iPhone/iPod touch devices - a QBP system that can be used in situ in traditional music sessions. Both of these systems use a backend corpus of 13,290 tunes drawn from community sources and “standard” references. These systems have evolved from academic research to become popular tools used by musicians around the world. 16,064 queries have been logged since the systems were launched on 31 July, 2009 and 11 February, 2010 respectively to 18 May 2010. As we log data on every query made, including geocoding queries made on the iPhone, we propose that these tools may be used to follow trends in the playing of traditional music. We also present an analysis of the data we have collected on the usage of these systems.",IRL,education,Developed economies,"[-26.37993, 18.926405]","[32.429523, 28.213661]","[-14.641958, -0.44709393, -5.0301704]","[7.420527, 3.5936882, 22.753296]","[14.176717, 7.939021]","[11.99315, 1.092585]","[14.068133, 14.659922, -2.1793563]","[12.44174, 4.947298, 12.40609]"
98,Mathieu Lagrange;Joan Serrà,Unsupervised Accuracy Improvement for Cover Song Detection Using Spectral Connectivity Network.,2010,https://doi.org/10.5281/zenodo.1416998,Mathieu Lagrange+IRCAM-CNRS UMR 9912>FRA>facility;Joan Serrà+Universitat Pompeu Fabra>ESP>education,"This paper introduces a new method for improving the accuracy in medium scale music similarity problems. Recently, it has been shown that the raw accuracy of query by example systems can be enhanced by considering priors about the distribution of its output or the structure of the music collection being considered. The proposed approach focuses on reducing the dependency to those priors by considering an eigenvalue decomposition of the aforementioned system’s output. Experiments carried out in the framework of cover song detection show that the proposed approach has good performance for enhancing a high accuracy system. Furthermore, it maintains the accuracy level for lower performing systems.",FRA,facility,Developed economies,"[9.299613, 44.551968]","[21.14575, 11.518865]","[3.9596467, 10.662015, -25.952961]","[15.265624, -5.533715, 3.3334246]","[16.104494, 11.127923]","[10.232956, 2.19111]","[12.870429, 17.331614, -0.3514055]","[11.775161, 6.45072, 12.337447]"
109,Jacek Wolkowicz;Vlado Keselj,Predicting Development of Research in Music Based on Parallels with Natural Language Processing.,2010,https://doi.org/10.5281/zenodo.1416808,Jacek Wołkowicz+Dalhousie University>CAN>education;Vlado Keˇselj+Dalhousie University>CAN>education,"The hypothesis of the paper is that the domain of Natural Languages Processing (NLP) resembles current research in music so one could benefit from this by employing NLP techniques to music. In this paper the similarity between both domains is described. The levels of NLP are listed with pointers to respective tasks within the research of computational music. A brief introduction to history of NLP enables locating music research in this history. Possible directions of research in music, assuming its affinity to NLP, are introduced. Current research in generational and statistical music modeling is compared to similar NLP theories. The paper is concluded with guidelines for music research and information retrieval.",CAN,education,Developed economies,"[-34.571716, 16.769659]","[-4.634424, 14.069198]","[-15.537134, -1.316483, 4.301985]","[-6.601431, 3.8536031, 7.8613334]","[14.400026, 8.313962]","[9.578053, 1.0792344]","[14.4891615, 14.441702, -1.5023918]","[10.890381, 5.9603014, 12.391378]"
107,Charlie Inskip;Andy MacFarlane;Pauline Rafferty,"Upbeat and Quirky, With a Bit of a Build: Interpretive Repertoires in Creative Music Search.",2010,https://doi.org/10.5281/zenodo.1417617,Charlie Inskip+City University London>GBR>education;Andy MacFarlane+City University London>GBR>education;Pauline Rafferty+University of Aberystwyth>GBR>education,"Pre-existing commercial music is widely used to accompany moving images in films, TV commercials and computer games. This process is known as music synchronisation. Professionals are employed by rights holders and film makers to perform creative music searches on large catalogues to find appropriate pieces of music for synchronisation. This paper discusses a Discourse Analysis of thirty interview texts related to the process. Coded examples are presented and discussed. Four interpretive repertoires are identified: the Musical Repertoire, the Soundtrack Repertoire, the Business Repertoire and the Cultural Repertoire. These ways of talking about music are adopted by all of the community regardless of their interest as Music Owner or Music User. Music is shown to have multi-variate and sometimes conflicting meanings within this community which are dynamic and negotiated. This is related to a theoretical feedback model of communication and meaning making which proposes that Owners and Users employ their own and shared ways of talking and thinking about music and its context to determine musical meaning. The value to the music information retrieval community is to inform system design from a user information needs perspective.",GBR,education,Developed economies,"[-31.457073, 19.976828]","[32.998978, 37.938046]","[-16.958612, 12.632921, -3.8034196]","[-1.6352394, 13.67858, 19.475796]","[14.301514, 8.5623]","[12.108978, 0.51964283]","[14.561257, 14.833835, -1.4813529]","[12.412484, 4.389991, 11.67613]"
106,Peter Grosche;Meinard Müller;Craig Stuart Sapp,What Makes Beat Tracking Difficult? A Case Study on Chopin Mazurkas.,2010,https://doi.org/10.5281/zenodo.1415852,Peter Grosche+Saarland University>DEU>education|MPI Informatik>DEU>facility;Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Craig Stuart Sapp+Stanford University>USA>education,"The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat tracking approaches still have significant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for detecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musical properties of certain beats that frequently evoke tracking errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimental results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material.",DEU,education,Developed economies,"[36.52811, -31.778357]","[-28.397469, -1.8547014]","[4.9283533, -30.786745, -8.549036]","[-7.251339, 10.165994, -6.465966]","[10.797048, 4.4937825]","[5.351342, 1.6522924]","[10.529931, 12.960357, -2.4177122]","[7.4801373, 6.699975, 11.117868]"
105,Jeffrey J. Scott;Raymond Migneco;Brandon G. Morton;Christian M. Hahn;Paul J. Diefenbach;Youngmoo E. Kim,An Audio Processing Library for MIR Application Development in Flash.,2010,https://doi.org/10.5281/zenodo.1414740,Jeffrey Scott+Drexel University>USA>education;Raymond Migneco+Drexel University>USA>education;Brandon Morton+Drexel University>USA>education;Christian M. Hahn+Drexel University>USA>education;Paul Diefenbach+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"In recent years, the Adobe Flash platform has risen as a credible and universal platform for rapid development and deployment of interactive web-based applications. It is also the accepted standard for delivery of streaming media, and many web applications related to music information retrieval, such as Pandora, Last.fm and Musicovery, are built using Flash. The limitations of Flash, however, have made it difficult for music-IR researchers and developers to utilize complex sound and music signal processing within their web applications. Furthermore, the real-time audio processing and synchronization required for some music-IR-related activities demands significant computational power and specialized audio algorithms, far beyond what is possible to implement using Flash scripting. By taking advantage of features recently added to the platform, including dynamic audio control and C cross-compilation for near-native performance, we have developed the Audio-processing Library for Flash (ALF), providing developers with a library of common audio processing routines and affording Flash developers a degree of sound interaction previously unavailable through web-based platforms. We present several music-IR-driven applications that incorporate ALF to demonstrate its utility.",USA,education,Developed economies,"[-12.423027, 57.92679]","[9.725533, 30.100605]","[-40.14826, 3.2323732, -2.5454853]","[-6.3422976, -1.8958118, 17.537428]","[13.378757, 5.035647]","[10.449356, 1.2833571]","[14.795531, 11.347724, -1.4064672]","[11.278802, 5.452529, 11.756354]"
104,Michael Scott Cuthbert;Christopher Ariza,Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data.,2010,https://doi.org/10.5281/zenodo.1416114,Michael Scott Cuthbert+Massachusetts Institute of Technology>USA>education;Christopher Ariza+Massachusetts Institute of Technology>USA>education,"Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (score-based) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demonstrating how to use it and the types of problems it is well-suited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores.",USA,education,Developed economies,"[16.205423, 15.530341]","[-1.3567382, 32.01282]","[-3.131236, -7.118743, 17.818718]","[-11.706543, -3.66382, 15.425722]","[12.0062685, 7.1244698]","[9.854186, 0.6894215]","[13.510565, 12.391125, -1.0698248]","[10.420615, 5.378285, 11.760231]"
103,Jouni Paulus;Meinard Müller;Anssi Klapuri,State of the Art Report: Audio-Based Music Structure Analysis.,2010,https://doi.org/10.5281/zenodo.1417289,Jouni Paulus+Fraunhofer Institute for Integrated Circuits IIS>DEU>facility;Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Anssi Klapuri+Queen Mary University of London>GBR>education,"Humans tend to organize perceived information into hierarchies and structures, a principle that also applies to music. Even musically untrained listeners unconsciously analyze and segment music with regard to various musical aspects, for example, identifying recurrent themes or detecting temporal boundaries between contrasting musical parts. This paper gives an overview of state-of-the-art methods for computational music structure analysis, where the general goal is to divide an audio recording into temporal segments corresponding to musical parts and to group these segments into musically meaningful categories. There are many different criteria for segmenting and structuring music audio. In particular, one can identify three conceptually different approaches, which we refer to as repetition-based, novelty-based, and homogeneity-based approaches. Furthermore, one has to account for different musical dimensions such as melody, harmony, rhythm, and timbre. In our state-of-the-art report, we address these different issues in the context of music structure analysis, while discussing and categorizing the most relevant and recent articles in this field.",DEU,facility,Developed economies,"[-2.677083, 1.5320386]","[-2.7735803, 4.7039557]","[-1.3989062, -6.6195, -0.27527282]","[-3.9196205, 1.6120658, -2.0813773]","[11.989288, 8.269971]","[8.374621, 2.5932717]","[12.792765, 13.839321, -0.47857788]","[10.379013, 7.168309, 11.58446]"
102,Xiao Hu 0001;J. Stephen Downie,When Lyrics Outperform Audio for Music Mood Classification: A Feature Analysis.,2010,https://doi.org/10.5281/zenodo.1415540,Xiao Hu+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"This paper builds upon and extends previous work on multi-modal mood classification (i.e., combining audio and lyrics) by analyzing in-depth those feature types that have shown to provide statistically significant improvements in the classification of individual mood categories. The dataset used in this study comprises 5,296 songs (with lyrics and audio for each) divided into 18 mood categories derived from user-generated tags taken from last.fm. These 18 categories show remarkable consistency with the popular Russell’s mood model. In seven categories, lyric features significantly outperformed audio spectral features. In one category only, audio outperformed all lyric feature types. A fine grained analysis of the significant lyric feature types indicates a strong and obvious semantic association between extracted terms and the categories. No such obvious semantic linkages were evident in the case where audio spectral features proved superior.",USA,education,Developed economies,"[-53.11551, 1.6061367]","[52.576206, -3.79926]","[-17.314781, 24.232473, 7.0192018]","[14.49125, 22.11767, 9.06538]","[13.422673, 12.545005]","[13.088097, 3.7077634]","[16.05442, 14.947856, 1.476836]","[14.1950865, 5.151564, 10.767907]"
101,Björn W. Schuller;Christoph Kozielski;Felix Weninger;Florian Eyben;Gerhard Rigoll,Vocalist Gender Recognition in Recorded Popular Music.,2010,https://doi.org/10.5281/zenodo.1415984,Björn Schuller+Technische Universität München>DEU>education;Christoph Kozielski+Technische Universität München>DEU>education;Felix Weninger+Technische Universität München>DEU>education;Florian Eyben+Technische Universität München>DEU>education;Gerhard Rigoll+Technische Universität München>DEU>education,"We introduce the task of vocalist gender recognition in popular music and evaluate the benefit of Non-Negative Matrix Factorization based enhancement of melodic components to this aim. The underlying automatic separation of drum beats is described in detail, and the obtained significant gain by its use is verified in extensive test-runs on a novel database of 1.5 days of MP3 coded popular songs based on transcriptions of the Karaoke-game UltraStar. As classifiers serve Support Vector Machines and Hidden Naive Bayes. Overall, the suggested methods lead to fully automatic recognition of the pre-dominant vocalist gender at 87.31 % accuracy on song level for artists unknown to the system in originally recorded music.",DEU,education,Developed economies,"[-13.580668, -35.875885]","[10.6748705, -19.527403]","[14.926709, 14.105752, -19.216452]","[13.822458, -5.0075855, -17.961445]","[10.206753, 11.555263]","[8.6880865, 3.646732]","[11.488494, 15.633224, 0.7069792]","[10.531198, 7.9910436, 10.127678]"
100,Gabriel Vigliensoni;Cory McKay;Ichiro Fujinaga,Using jWebMiner 2.0 to Improve Music Classification Performance by Combining Different Types of Features Mined from the Web.,2010,https://doi.org/10.5281/zenodo.1416590,Gabriel Vigliensoni+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper presents the jWebMiner 2.0 cultural feature extraction software and describes the results of several musical genre classification experiments performed with it. jWebMiner 2.0 is an easy-to-use and open-source tool that allows users to mine the Internet in order to extract features based on both Last.fm social tags and general web search string co-occurrences extracted using the Yahoo! API. The experiments performed found that the features based on social tags were more effective at classifying music into a small (5-genre) genre ontology, but the features based on general web co-occurrences were more effective at classifying a moderate (10-genre) ontology. It was also found that combining the two types of features resulted in improved performance overall.",CAN,education,Developed economies,"[-26.346697, 10.599747]","[22.394344, 25.395187]","[-19.520918, -5.9705353, 15.0682535]","[-1.4781636, 0.6880143, 29.710878]","[13.97387, 7.743887]","[10.805332, 0.065349564]","[13.836375, 14.10779, 0.56783104]","[12.343681, 5.542234, 11.575721]"
11,Eric Humphrey,Automatic Characterization of Digital Music for Rhythmic Auditory Stimulation.,2010,https://doi.org/10.5281/zenodo.1418135,Eric Humphrey+University of Miami>USA>education,"A computational rhythm analysis system is proposed to characterize the suitability of musical recordings for rhythmic auditory stimulation, a neurologic music therapy technique that uses rhythm to entrain periodic physical motion. Current applications of RAS are limited by the general inability to take advantage of the enormous amount of digital music that exists today. The system aims to identify motor-rhythmic music for the entrainment of neuromuscular activity for rehabilitation and exercise, motivating the concept of musical “use-genres.” This work builds upon prior research in meter and tempo analysis to establish a representation of rhythm chroma and alternatively describe beat spectra.",USA,education,Developed economies,"[22.515207, -25.614681]","[-23.974817, -0.004134273]","[-2.9578671, -19.34262, -0.17893288]","[-1.3660206, 11.703011, -5.382206]","[11.61682, 5.8125763]","[5.7749295, 1.835977]","[11.435274, 13.572201, -1.7899996]","[8.020593, 6.9102325, 11.62493]"
99,Audrey Laplante,Users' Relevance Criteria in Music Retrieval in Everyday Life: An Exploratory Study.,2010,https://doi.org/10.5281/zenodo.1415578,Audrey Laplante+Université de Montréal>CAN>education,"The paper presents the findings of a qualitative study on the way young adults make relevance inferences about music items when searching for music for recreational purposes. Data were collected through in-depth interviews and analyzed following the constant comparative method. Content analysis revealed that participants used four types of clues to make relevance inferences: bibliographic metadata (e.g., names of contributors, labels), relational metadata (e.g., genres, similar artists), associative metadata (e.g., cover arts), and recommendations/reviews. Relevance judgments were also found to be influenced by the external context (i.e., the functions music plays in one’s life) and the internal context (i.e., individual tastes and beliefs, state of mind).",CAN,education,Developed economies,"[-23.105595, 23.438292]","[38.926014, 33.042267]","[-16.265263, 10.9299555, -10.032726]","[8.612491, 11.574442, 21.748924]","[14.669351, 8.1692505]","[12.73864, 0.9782868]","[14.566852, 14.884255, -2.0648952]","[13.09456, 4.3417854, 12.081252]"
94,Rafael Ferrer;Tuomas Eerola,Timbral Qualities of Semantic Structures of Music.,2010,https://doi.org/10.5281/zenodo.1416484,Rafael Ferrer+Finnish Centre of Excellence in Interdisciplinary Music Research>FIN>facility|Unknown>Unknown>Unknown;Tuomas Eerola+Finnish Centre of Excellence in Interdisciplinary Music Research>FIN>facility|Unknown>Unknown>Unknown,"The rapid expansion of social media in music has provided the field with impressive datasets that offer insights into the semantic structures underlying everyday uses and classification of music. We hypothesize that the organization of these structures are rather directly linked with the “qualia” of the music as sound. To explore the ways in which these structures are connected with the qualities of sounds, a semantic space was extracted from a large collection of musical tags with latent semantic and cluster analysis. The perceptual and musical properties of 19 clusters were investigated by a similarity rating task that used spliced musical excerpts representing each cluster. The resulting perceptual space denoting the clusters correlated high with selected acoustical features extracted from the stimuli. The first dimension related to the high-frequency energy content, the second to the regularity of the spectrum, and the third to the fluctuations within the spectrum. These findings imply that meaningful organization of music may be derived from low-level descriptions of the excerpts. Novel links with the functions of music embedded into the tagging information included within the social media are proposed.",FIN,facility,Developed economies,"[1.5265988, 10.146093]","[42.832565, 2.6424766]","[-10.881974, 0.7470724, 4.198307]","[23.730852, 11.1768265, 9.463835]","[12.961701, 9.525015]","[12.171521, 3.1644194]","[13.714629, 14.610389, -0.4631196]","[13.470174, 5.839211, 11.467779]"
93,Ya-Xi Chen;René Klüber,ThumbnailDJ: Visual Thumbnails of Music Content.,2010,https://doi.org/10.5281/zenodo.1418271,Ya-Xi Chen+University of Munich>DEU>education;René Klüber+University of Munich>DEU>education,"Musical perception is non-visual and people cannot describe what a song sounds like without listening to it. To facilitate music browsing and searching, we explore the automatic generation of visual thumbnails for music. Targeting an expert user groups, DJs, we developed a concept named ThumbnailDJ: Based on a metaphor of music notation, a visual thumbnail can be automatically generated for an audio file, including information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, and our concept was preferred most. Based on the results of this interview, we refined ThumbnailDJ and conducted an evaluation with DJs. The results confirmed that ThumbnailDJ can facilitate expert users browsing and searching within their music collection.",DEU,education,Developed economies,"[-12.168768, 31.617598]","[32.23713, 25.117018]","[-9.956469, 12.950797, -21.724176]","[5.1579337, 10.846552, 11.14983]","[13.877298, 7.170093]","[11.449114, 1.5879484]","[13.965098, 13.906378, -2.2912982]","[12.395485, 5.3106465, 12.676522]"
92,Simone Sammartino;Lorenzo J. Tardón;Cristina de la Bandera;Isabel Barbancho;Ana M. Barbancho,The Standardized Variogram as a Novel Tool for Music Similarity Evaluation.,2010,https://doi.org/10.5281/zenodo.1417195,Simone Sammartino+Universidad de Málaga>ESP>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Cristina de la Bandera+Universidad de Málaga>ESP>education;Isabel Barbancho+Universidad de Málaga>ESP>education;Ana M. Barbancho+Universidad de Málaga>ESP>education,"Most of methods for audio similarity evaluation are based on the Mel frequency cepstral coefficients, employed as main tool for the characterization of audio contents. Such approach needs some way of data compression aimed to optimize the information retrieval task and to reduce the computational costs derived from the usage of cluster analysis tools and probabilistic models. A novel approach is presented in this paper, based on the standardized variogram. This tool, inherited from Geostatistics, is applied to MFCCs matrices to reduce their size and compute compact representations of the audio contents (song signatures), aimed to evaluate audio similarity. The performance of the proposed approach is analyzed in comparison with other alternative methods and on the base of human responses.",ESP,education,Developed economies,"[-3.9477553, 13.544833]","[21.20954, 6.471836]","[-4.6246347, 6.494721, -0.75346506]","[11.5039835, -0.8347083, 3.930392]","[13.00511, 9.319759]","[10.015766, 2.2022634]","[13.562338, 15.169506, -0.70675564]","[11.526028, 6.868427, 12.273706]"
91,Aggelos Gkiokas;Vassilios Katsouros;George Carayannis,Tempo Induction Using Filterbank Analysis and Tonal Features.,2010,https://doi.org/10.5281/zenodo.1415210,Aggelos Gkiokas+Institute for Language and Speech Processing>GRC>facility|National Technical University of Athens>GRC>education;Vassilis Katsouros+Institute for Language and Speech Processing>GRC>facility|National Technical University of Athens>GRC>education;George Carayannis+Institute for Language and Speech Processing>GRC>facility|National Technical University of Athens>GRC>education,"This paper presents an algorithm that extracts the tempo of a musical excerpt. The proposed system assumes a constant tempo and deals directly with the audio signal. A sliding window is applied to the signal and two feature classes are extracted. The first class is the log-energy of each band of a mel-scale triangular filterbank, a common feature vector used in various MIR applications. For the second class, a novel feature for the tempo induction task is presented; the strengths of the twelve western musical tones at all octaves are calculated for each audio frame, in a similar fashion with Pitch Class Profile. The time-evolving feature vectors are convolved with a bank of resonators, each resonator corresponding to a target tempo. Then the results of each feature class are combined to give the final output. The algorithm was evaluated on the popular ISMIR 2004 Tempo Induction Evaluation Exchange Dataset. Results demonstrate that the superposition of the different types of features enhance the performance of the algorithm, which is in the current state-of-the-art algorithms of the tempo induction task.",GRC,facility,Developed economies,"[40.994915, -27.54377]","[-31.198711, -7.7250037]","[-1.3804363, -32.046593, -2.491935]","[-6.832973, 8.789162, -12.815349]","[11.43368, 4.4286633]","[5.160209, 1.9018853]","[10.908487, 13.308005, -2.8987265]","[7.430089, 7.009197, 10.963591]"
90,Carolina Ramirez;Jun Ohya,Symbol Classification Approach for OMR of Square Notation Manuscripts.,2010,https://doi.org/10.5281/zenodo.1415122,Carolina Ramirez+Waseda University>JPN>education;Jun Ohya+Waseda University>JPN>education,"Researchers in the field of OMR (Optical Music Recognition) have acknowledged that the automatic transcription of medieval musical manuscripts is still an open problem, mainly due to lack of standards in notation and the physical quality of the documents. Nonetheless, the amount of medieval musical manuscripts is so vast that the consensus seems to be that OMR can be a vital tool to help in the preserving and sharing of this information in digital format. In this paper we report our results on a preliminary approach to OMR of medieval plainchant manuscripts in square notation, at the symbol classification level, which produced good results in the recognition of eight basic symbols. Our preliminary approach consists of the preprocessing, segmentation, and classification stages.",JPN,education,Developed economies,"[31.616089, 29.509428]","[-23.75551, 38.689617]","[3.5666947, -19.032688, 19.404993]","[-14.565201, -22.949675, 1.8022336]","[9.858432, 6.2390103]","[6.74668, -0.731599]","[12.832006, 11.513945, -1.5068282]","[7.823165, 4.03621, 10.562687]"
89,Peter Knees;Markus Schedl;Tim Pohle;Klaus Seyerlehner;Gerhard Widmer,Supervised and Unsupervised Web Document Filtering Techniques to Improve Text-Based Music Retrieval.,2010,https://doi.org/10.5281/zenodo.1417173,Peter Knees+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Markus Schedl+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Tim Pohle+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Klaus Seyerlehner+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"We aim at improving a text-based music search engine by applying different techniques to exclude misleading information from the indexing process. The idea of the original approach is to index music pieces by “contextual” information, more precisely, by all texts to be found on Web pages retrieved via a common Web search engine. This representation allows for issuing arbitrary textual queries to retrieve relevant music pieces. The goal of this work is to improve precision of the retrieved set of music pieces by filtering out Web pages that lead to irrelevant tracks. To this end we present two unsupervised and two supervised filtering approaches. Evaluation is carried out on two collections previously used in the literature. The obtained results suggest that the proposed filtering techniques can improve results significantly but are only effective when applied to large and diverse music collections with millions of Web pages associated.",AUT,education,Developed economies,"[-15.027216, 19.884357]","[20.605095, 18.650045]","[-6.2014074, 7.407572, -11.228685]","[10.538326, -4.637055, 14.906914]","[13.791113, 8.070075]","[10.44913, 1.0224409]","[13.727774, 14.703776, -1.9713297]","[11.817299, 5.737726, 12.707758]"
88,Ruben Hillewaere;Bernard Manderick;Darrell Conklin,String Quartet Classification with Monophonic Models.,2010,https://doi.org/10.5281/zenodo.1417619,"Ruben Hillewaere+Vrije Universiteit Brussel>BEL>education;Bernard Manderick+Vrije Universiteit Brussel>BEL>education;Darrell Conklin+IKERBASQUE, Basque Foundation or Science>ESP>facility","Polyphonic music classification remains a very challenging area in the field of music information retrieval. In this study, we explore the performance of monophonic models on single parts that are extracted from the polyphony. The presented method is specifically designed for the case of voiced polyphony, but can be extended to any type of music with multiple parts. On a dataset of 207 Haydn and Mozart string quartet movements, global feature models with standard machine learning classifiers are compared with a monophonic n-gram model for the task of composer recognition. Global features emerging from feature selection are presented, and future guidelines for the research of polyphonic music are outlined.",BEL,education,Developed economies,"[-30.520884, -8.051403]","[9.152967, -8.396542]","[-20.957413, 0.6517069, 21.353376]","[10.217238, 2.9082983, -7.8948455]","[11.185973, 7.9272175]","[9.0542, 3.3394322]","[12.847992, 12.819041, -0.25775534]","[10.724677, 7.259459, 10.597065]"
87,Dominikus Baur;Bartholomäus Steinmayr;Andreas Butz,SongWords: Exploring Music Collections Through Lyrics.,2010,https://doi.org/10.5281/zenodo.1416682,Dominikus Baur+University of Munich (LMU)>DEU>education;Bartholomäus Steinmayr+University of Munich (LMU)>DEU>education;Andreas Butz+University of Munich (LMU)>DEU>education,"The lyrics of a song are an interesting, yet underused type of symbolic music data. We present SongWords, an application for tabletop computers that allows browsing and exploring a music collection based on its lyrics. SongWords can present the collection in a self-organizing map or sorted along different dimensions. Songs can be ordered by lyrics, user-generated tags or alphabetically by name, which allows exploring simple correlations, e.g., between genres (such as gospel) and words (such as lord). In this paper, we discuss the design rationale and implementation of SongWords as well as a user study with personal music collections. We found that lyrics indeed enable a different access to music collections and identified some challenges for future lyrics-based interfaces.",DEU,education,Developed economies,"[-24.202452, 18.522545]","[28.831938, 20.086645]","[3.9873683, 22.153032, -4.1590147]","[14.144806, -3.967243, 20.35629]","[14.064477, 8.274866]","[11.406191, 1.6759957]","[14.095372, 14.913889, -2.0884123]","[12.446686, 5.482336, 12.9748]"
86,Chao-Ling Hsu;Jyh-Shing Roger Jang,Singing Pitch Extraction by Voice Vibrato / Tremolo Estimation and Instrument Partial Deletion.,2010,https://doi.org/10.5281/zenodo.1417357,Chao-Ling Hsu+National Tsing Hua University>TWN>education;Jyh-Shing Roger Jang+National Tsing Hua University>TWN>education,"This paper proposes a novel and effective approach to extract the pitches of the singing voice from monaural polyphonic songs. The sinusoidal partials of the musical audio signals are first extracted. The Fourier transform is then applied to extract the vibrato/tremolo information of each partial. Some criteria based on this vibrato/tremolo information are employed to discriminate the vocal partials from the music accompaniment partials. Besides, a singing pitch trend estimation algorithm which is able to find the global singing progressing tunnel is also proposed. The singing pitches can then be extracted more robustly via these two processes. Quantitative evaluation shows that the proposed algorithms significantly improve the raw pitch accuracy of our previous approach and are comparable with other state of the art approaches submitted to MIREX.",TWN,education,Developing economies,"[-1.7818611, -32.332058]","[-2.7260497, -10.771186]","[20.519407, 6.068973, -7.803124]","[6.1839094, -9.360303, -11.840602]","[9.627178, 10.509425]","[6.706219, 2.729515]","[10.968007, 14.855617, 0.496506]","[9.0817995, 8.077412, 11.090617]"
85,Daniel Gärtner,Singing / Rap Classification of Isolated Vocal Tracks.,2010,https://doi.org/10.5281/zenodo.1417089,Daniel Gärtner+Fraunhofer Institute for Digital Media Technology IDMT>DEU>facility,"In this paper, a system for the classification of the vocal characteristics in HipHop / R&B music is presented. Isolated vocal track segments, taken from acapella versions of commercial recordings, are classified into classes singing and rap. A feature-set motivated by work from song / speech classification, speech emotion recognition, and from differences that humans perceive and utilize, is presented. An SVM is used as classifier, accuracies of about 90% are achieved. In addition, the features are analyzed according to their contribution, using the IRMFSP feature selection algorithm. In another experiment, it is shown that the features are robust against utterance-specific characteristics.",DEU,facility,Developed economies,"[-11.540434, -33.601746]","[11.766527, -16.765026]","[17.18682, 17.28045, -14.3475275]","[11.66721, 0.90067744, -16.380072]","[10.228977, 11.18165]","[8.838301, 3.5913212]","[11.558391, 15.0464945, 0.85622346]","[10.837991, 7.650913, 10.314863]"
84,Chun-Man Mak;Tan Lee;Suman Senapati;Yu Ting Yeung;Wang-Kong Lam,Similarity Measures for Chinese Pop Music Based on Low-level Audio Signal Attributes.,2010,https://doi.org/10.5281/zenodo.1415636,Chun-Man Mak+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Tan Lee+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Suman Senapati+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Yu-Ting Yeung+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Wang-Kong Lam+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"In this article a method of computing similarity of two Chinese pop songs is presented. It is based on five attributes extracted from the audio signal. They include music instrument, singing voice style, singer gender, tempo, and degree of noisiness. We compare the computed similarity measures with similarity scores obtained with subjective listening by over 200 human subjects. The results show that rhythm and mood related attributes like tempo and degree of noisiness are most correlated to human perception of Chinese pop songs. Instrument and singing style are relatively less relevant. The results of subjective evaluation also indicate that the proposed method of similarity computation is fairly correlated with human perception.",HKG,education,Developing economies,"[-4.051053, 15.190147]","[17.865738, 2.5900464]","[-7.762097, 5.7745667, -2.7951357]","[6.8556056, 4.60521, 6.078018]","[13.054117, 9.334098]","[9.587632, 1.9236106]","[13.569842, 15.148579, -0.63442963]","[11.4667425, 6.9935474, 12.972056]"
95,Kjell Lemström,Towards More Robust Geometric Content-Based Music Retrieval.,2010,https://doi.org/10.5281/zenodo.1415150,Kjell Lemström+University of Helsinki>FIN>education,"This paper studies the problem of transposition and time-scale invariant (ttsi) polyphonic music retrieval in symbolically encoded music. In the setting, music is represented by sets of points in plane. We give two new algorithms. Applying a search window of size w and given a query point set, of size m, to be searched for in a database point set, of size n, our algorithm for exact ttsi occurrences runs in O(mwn log n) time; for partial occurrences we have an O(mnw2 log n) algorithm. The framework used is flexible allowing development towards even more robust geometric retrieval.",FIN,education,Developed economies,"[-12.5057, 21.555784]","[10.163949, 17.033054]","[-4.267869, 5.2183228, -9.470776]","[8.221142, -10.648134, 7.327857]","[13.538767, 8.074501]","[9.087608, 0.79679865]","[13.4524145, 14.611161, -1.9452112]","[10.73532, 6.344868, 13.347518]"
12,Menno van Zaanen;Pieter Kanters,Automatic Mood Classification Using TF*IDF Based on Lyrics.,2010,https://doi.org/10.5281/zenodo.1417287,Menno van Zaanen+Tilburg University>NLD>education;Pieter Kanters+Tilburg University>NLD>education,"This paper presents the outcomes of research into using lingual parts of music in an automatic mood classification system. Using a collection of lyrics and corresponding user-tagged moods, we build classifiers that classify lyrics of songs into moods. By comparing the performance of different mood frameworks (or dimensions), we examine to what extent the linguistic part of music reveals adequate information for assigning a mood category and which aspects of mood can be classified best. Our results show that word oriented metrics provide a valuable source of information for automatic mood classification of music, based on lyrics only. Metrics such as term frequencies and tf*idf values are used to measure relevance of words to the different mood classes. These metrics are incorporated in a machine learning classifier setup. Different partitions of the mood plane are investigated and we show that there is no large difference in mood prediction based on the mood division. Predictions on the valence, tension and combinations of aspects lead to similar performance.",NLD,education,Developed economies,"[-53.303383, 0.80532664]","[52.7121, -4.1138515]","[-17.945953, 26.090498, 8.302318]","[13.514978, 23.175076, 8.966148]","[13.36253, 12.560302]","[13.136107, 3.74471]","[16.077942, 14.952969, 1.5042168]","[14.237923, 5.1721864, 10.717803]"
108,Emmanuel Vincent;Stanislaw Andrzej Raczynski;Nobutaka Ono;Shigeki Sagayama,A Roadmap Towards Versatile MIR.,2010,https://doi.org/10.5281/zenodo.1418141,Emmanuel Vincent+INRIA>FRA>facility;Stanisław A. Raczyński+University of Tokyo>JPN>education;Nobutaka Ono+University of Tokyo>JPN>education;Shigeki Sagayama+University of Tokyo>JPN>education,"Most MIR systems are specifically designed for one application and one cultural context and suffer from the semantic gap between the data and the application. Advances in the theory of Bayesian language and information processing enable the vision of a versatile, meaningful and accurate MIR system integrating all levels of information. We propose a roadmap to collectively achieve this vision.",FRA,facility,Developed economies,"[-9.139129, 58.045624]","[5.5972977, 33.805935]","[-36.51255, 0.8046241, -5.564927]","[-8.929802, 8.7136545, 21.564438]","[13.619592, 4.7441077]","[10.921471, 0.6275131]","[15.014875, 11.144425, -1.4753779]","[11.520824, 5.0696564, 11.574859]"
14,Halfdan Rump;Shigeki Miyabe;Emiru Tsunoo;Nobutaka Ono;Shigeki Sagayama,Autoregressive MFCC Models for Genre Classification Improved by Harmonic-percussion Separation.,2010,https://doi.org/10.5281/zenodo.1418239,Halfdan Rump+The University of Tokyo>JPN>education;Shigeki Miyabe+The University of Tokyo>JPN>education;Emiru Tsunoo+The University of Tokyo>JPN>education;Nobukata Ono+The University of Tokyo>JPN>education;Shigeki Sagama+The University of Tokyo>JPN>education,"In this work we improve accuracy of MFCC-based genre classification by using the Harmonic-Percussion Signal Separation (HPSS) algorithm on the music signal, and then calculate the MFCCs on the separated signals. The choice of the HPSS algorithm was mainly based on the observation that the presence of harmonics causes the high MFCCs to be noisy. A multivariate autoregressive (MAR) model was trained on the improved MFCCs, and performance in the task of genre classification was evaluated. By combining features calculated on the separated signals, relative error rate reductions of 20% and 16.2% were obtained when an SVM classifier was trained on the MFCCs and MAR features respectively. Next, by analyzing the MAR features calculated on the separated signals, it was concluded that the original signal contained some information which the MAR model was capable of handling, and that the best performance was obtained when all three signals were used. Finally, by choosing the number of MFCCs from each signal type to be used in the autoregressive modelling, it was verified that the best performance was reached when the high MFCCs calculated on the harmonic signal were discarded.",JPN,education,Developed economies,"[-28.209633, -16.436005]","[12.838365, -14.380296]","[-14.351472, 5.6792016, 20.760548]","[9.732911, 11.079977, -10.432146]","[12.6954775, 10.861311]","[9.026705, 3.7877846]","[13.6037035, 14.145995, 1.4205287]","[10.742558, 7.4511375, 10.464232]"
43,Ioannis Karydis;Milos Radovanovic;Alexandros Nanopoulos;Mirjana Ivanovic,"Looking Through the ""Glass Ceiling"": A Conceptual Framework for the Problems of Spectral Similarity.",2010,https://doi.org/10.5281/zenodo.1417283,Ioannis Karydis+Ionian University>GRC>education;Miloš Radovanović+University of Novi Sad>SRB>education;Alexandros Nanopoulos+University of Hildesheim>DEU>education;Mirjana Ivanović+University of Novi Sad>SRB>education,"Spectral similarity measures have been shown to exhibit good performance in several Music Information Retrieval (MIR) applications. They are also known, however, to possess several undesirable properties, namely allowing the existence of hub songs (songs which frequently appear in nearest neighbor lists of other songs), “orphans” (songs which practically never appear), and difficulties in distinguishing the farthest from the nearest neighbor due to the concentration effect caused by high dimensionality of data space. In this paper we develop a conceptual framework that allows connecting all three undesired properties. We show that hubs and “orphans” are expected to appear in high-dimensional data spaces, and relate the cause of their appearance with the concentration property of distance / similarity measures. We verify our conclusions on real music data, examining groups of frames generated by Gaussian Mixture Models (GMMs), considering two similarity measures: Earth Mover’s Distance (EMD) in combination with Kullback-Leibler (KL) divergence, and Monte Carlo (MC) sampling. The proposed framework can be useful to MIR researchers to address problems of spectral similarity, understand their fundamental origins, and thus be able to develop more robust methods for their remedy.",GRC,education,Developed economies,"[-0.77904975, 19.672705]","[26.959692, 5.5032845]","[-7.144262, 1.7348113, 1.2550107]","[20.620167, -3.9832232, 7.0495596]","[12.858333, 9.4888525]","[10.720516, 2.3750417]","[13.238051, 15.244845, -0.7016339]","[12.186758, 6.53716, 12.5132475]"
44,Noam Koenigstein;Yuval Shavitt;Ela Weinsberg;Udi Weinsberg,On the Applicability of Peer-to-peer Data in Music Information Retrieval Research.,2010,https://doi.org/10.5281/zenodo.1414784,Noam Koenigstein+Tel-Aviv University>ISR>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Yuval Shavitt+Tel-Aviv University>ISR>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Ela Weinsberg+Tel-Aviv University>ISR>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Udi Weinsberg+Tel-Aviv University>ISR>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Peer-to-Peer (p2p) networks are being increasingly adopted as an invaluable resource for various music information retrieval (MIR) tasks, including music similarity, recommendation and trend prediction. However, these networks are usually extremely large and noisy, which raises doubts regarding the ability to actually extract sufficiently accurate information. This paper evaluates the applicability of using data originating from p2p networks for MIR research, focusing on partial crawling, inherent noise and localization of songs and search queries. These aspects are quantified using songs collected from the Gnutella p2p network. We show that the power-law nature of the network makes it relatively easy to capture an accurate view of the main-streams using relatively little effort. However, some applications, like trend prediction, mandate collection of the data from the “long tail”, hence a much more exhaustive crawl is needed. Furthermore, we present techniques for overcoming noise originating from user generated content and for filtering non informative data, while minimizing information loss.",ISR,education,Developing economies,"[-19.890293, 25.151922]","[48.547684, 18.819138]","[-13.859959, 10.710458, -14.990662]","[22.902243, 0.7165155, 13.998973]","[14.444606, 8.005457]","[12.06098, 1.9782989]","[14.390276, 14.957488, -2.249517]","[13.069311, 5.3850555, 12.461992]"
45,Thomas Lidy;Rudolf Mayer;Andreas Rauber;Pedro J. Ponce de León;Antonio Pertusa;José Manuel Iñesta Quereda,A Cartesian Ensemble of Feature Subspace Classifiers for Music Categorization.,2010,https://doi.org/10.5281/zenodo.1415598,T. Lidy+Vienna University of Technology>AUT>education;R. Mayer+Vienna University of Technology>AUT>education;A. Rauber+Vienna University of Technology>AUT>education;P. J. Ponce de León+University of Alicante>ESP>education;A. Pertusa+University of Alicante>ESP>education;J. M. Iñesta+University of Alicante>ESP>education,"We present a cartesian ensemble classification system that is based on the principle of late fusion and feature subspaces. These feature subspaces describe different aspects of the same data set. The framework is built on the Weka machine learning toolkit and able to combine arbitrary feature sets and learning schemes. In our scenario, we use it for the ensemble classification of multiple feature sets from the audio and symbolic domains. We present an extensive set of experiments in the context of music genre classification, based on numerous Music IR benchmark datasets, and evaluate a set of combination/voting rules. The results show that the approach is superior to the best choice of a single algorithm on a single feature set. Moreover, it also releases the user from making this choice explicitly.",AUT,education,Developed economies,"[-25.082994, -11.74292]","[22.941658, -7.750111]","[-13.1481, 1.2971145, 12.896444]","[16.459955, 5.2440424, -3.0364718]","[12.685746, 10.551503]","[9.762075, 3.3398504]","[13.72305, 14.057702, 1.1522242]","[11.50023, 7.116685, 10.853455]"
46,Julián Urbano;Mónica Marrero;Diego Martín 0001;Juan Lloréns,Improving the Generation of Ground Truths Based on Partially Ordered Lists.,2010,https://doi.org/10.5281/zenodo.1417525,Julián Urbano+University Carlos III of Madrid>ESP>education;Mónica Marrero+University Carlos III of Madrid>ESP>education;Diego Martín+University Carlos III of Madrid>ESP>education;Juan Lloréns+University Carlos III of Madrid>ESP>education,"Ground truths based on partially ordered lists have been used for some years now to evaluate the effectiveness of Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity. However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations. In this paper we revise the methodology used to generate these ground truths and disclose some issues that need to be addressed. In particular, we focus on the arrangement and aggregation of the relevant results, and show that it is not possible to ensure lists completely consistent. We develop a measure of consistency based on Average Dynamic Recall and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method. The results of the MIREX 2005 evaluation are revisited using these alternative ground truths.",ESP,education,Developed economies,"[26.302906, 25.86834]","[18.307085, 5.762176]","[-1.2598349, -8.538366, 37.063763]","[5.8682165, 4.0564995, 9.964466]","[9.9606695, 9.190428]","[9.869408, 1.7275901]","[12.074774, 13.974181, -0.7841372]","[11.505701, 6.545157, 12.78397]"
47,João Lobato Oliveira;Fabien Gouyon;Luis Gustavo Martins;Luís Paulo Reis,IBT: A Real-time Tempo and Beat Tracking System.,2010,https://doi.org/10.5281/zenodo.1416470,João Lobato Oliveira+Institute for Systems and Computer Engineering of Porto (INESC Porto)>PRT>education;Fabien Gouyon+Institute for Systems and Computer Engineering of Porto (INESC Porto)>PRT>education;Luis Gustavo Martins+Research Center for Science and Technology in Art (CITAR)>PRT>education;Luis Paulo Reis+Artificial Intelligence and Computer Science Laboratory (LIACC)>PRT>education,"This paper describes a tempo induction and beat tracking system based on the efficient strategy (initially introduced in the BeatRoot system [Dixon S., “Automatic extraction of tempo and beat from expressive performances.” Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main reasons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consideration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform). The system is implemented in C++, permitting faster than real-time processing of audio data. It is integrated in the MARSYAS framework, and is therefore available under GPL for users and/or researchers. Detailed evaluation of the causal and non-causal versions of the system on common benchmark datasets show performances reaching those of state-of-the-art beat trackers. We propose a series of lines for future work based on careful analysis of the results.",PRT,education,Developed economies,"[37.66815, -32.626736]","[-28.445465, -3.2784576]","[4.9421725, -31.770594, -4.6944165]","[-7.4454184, 7.454114, -9.243153]","[10.631485, 4.348018]","[5.254004, 1.764354]","[10.358121, 12.997386, -2.341505]","[7.4959536, 6.7600245, 10.884859]"
48,Riccardo Miotto;Luke Barrington;Gert R. G. Lanckriet,Improving Auto-tagging by Modeling Semantic Co-occurrences.,2010,https://doi.org/10.5281/zenodo.1415590,Riccardo Miotto+University of Padova>ITA>education;Luke Barrington+UC San Diego>USA>education;Gert Lanckriet+UC San Diego>USA>education,Automatic taggers describe music in terms of a multinomial distribution over relevant semantic concepts. This paper presents a framework for improving automatic tagging of music content by modeling contextual relationships between these semantic concepts. The framework extends existing auto-tagging methods by adding a Dirichlet mixture to model the contextual co-occurrences between semantic multinomials. Experimental results show that adding context improves automatic annotation and retrieval of music and demonstrate that the Dirichlet mixture is an appropriate model for capturing co-occurrences between semantics.,ITA,education,Developed economies,"[-44.306458, -3.8366632]","[37.442436, -0.84552]","[-13.535501, 19.141846, 13.559508]","[21.735722, 8.773732, 2.3015435]","[14.49751, 10.621949]","[11.330337, 3.4269402]","[15.641816, 14.061093, 0.14090729]","[12.946657, 6.4257636, 11.243942]"
49,Jouni Paulus,Improving Markov Model Based Music Piece Structure Labelling with Acoustic Information.,2010,https://doi.org/10.5281/zenodo.1416732,Jouni Paulus+Fraunhofer Institute for Integrated Circuits IIS>DEU>facility,"This paper proposes using acoustic information in the labelling of music piece structure descriptions. Here, music piece structure means the sectional form of the piece: temporal segmentation and grouping to parts such as chorus or verse. The structure analysis methods rarely provide the parts with musically meaningful names. The proposed method labels the parts in a description. The baseline method models the sequential dependencies between musical parts with N-grams and uses them for the labelling. The acoustic model proposed in this paper is based on the assumption that the parts with the same label even in different pieces share some acoustic properties compared to other parts in the same pieces. The proposed method uses mean and standard deviation of relative loudness in a part as the feature which is then modelled with a single multivariate Gaussian distribution. The method is evaluated on three data sets of popular music pieces, and in all of them the inclusion of the acoustic model improves the labelling accuracy over the baseline method.",DEU,facility,Developed economies,"[-0.7545693, 0.027182493]","[-4.883087, 3.2153907]","[1.8192555, -3.9489913, 2.683437]","[0.14602804, -3.6137385, -1.1207808]","[11.746961, 8.116009]","[8.240063, 2.9225848]","[12.48544, 13.800467, -0.2870726]","[10.493108, 7.6256437, 11.325344]"
50,Kazuyoshi Yoshii;Masataka Goto,Infinite Latent Harmonic Allocation: A Nonparametric Bayesian Approach to Multipitch Analysis.,2010,https://doi.org/10.5281/zenodo.1414912,Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a statistical method called Infinite Latent Harmonic Allocation (iLHA) for detecting multiple fundamental frequencies in polyphonic audio signals. Conventional methods face a crucial problem known as model selection because they assume that the observed spectra are superpositions of a certain fixed number of bases (sound sources and/or finer parts). iLHA avoids this problem by assuming that the observed spectra are superpositions of a stochastically-distributed unbounded (theoretically infinite) number of bases. Such uncertainty can be treated in a principled way by leveraging the state-of-the-art paradigm of machine learning called Bayesian nonparametrics. To represent a set of time-sliced spectral strips, we formulated nested infinite Gaussian mixture models (GMMs) based on hierarchical and generalized Dirichlet processes. Each strip is allowed to contain an unbounded number of sound sources (GMMs), each of which is allowed to contain an unbounded number of harmonic partials (Gaussians). To train the nested infinite GMMs efficiently, we used a modern inference technique called collapsed variational Bayes (CVB). Our experiments using audio recordings of real piano and guitar performances showed that fully automated iLHA based on noninformative priors performed as well as optimally tuned conventional methods.",JPN,facility,Developed economies,"[36.80671, -14.1308775]","[-44.340878, -21.175556]","[12.128234, -22.515537, 11.700559]","[-0.78519285, -12.044497, -28.297043]","[8.976286, 9.014269]","[6.46291, 4.6325583]","[11.37674, 13.395808, 0.20331207]","[9.721819, 8.614374, 10.254083]"
51,Yushen Han;Christopher Raphael,Informed Source Separation of Orchestra and Soloist.,2010,https://doi.org/10.5281/zenodo.1416750,Yushen Han+Indiana University Bloomington>USA>education;Christopher Raphael+Indiana University Bloomington>USA>education,"A novel technique of unmasking to repair the degradation in sources separated by spectrogram masking is proposed. Our approach is based on explicit knowledge of the musical audio at note level from a score-audio alignment, which we termed Informed Source Separation (ISS). Such knowledge allows the spectrogram energy to be decomposed into note-based models. We assume that a spectrogram mask for the solo is obtained and focus on the problem of repairing audio resulting from applying the mask. We evaluate the spectrogram as well as the harmonic structure of the music. We either search for unmasked (orchestra) partials of the orchestra to be transposed onto a masked (solo) region or reshape a solo partial with phase and amplitude imputed from unmasked regions. We describe a Kalman smoothing technique to decouple the phase and amplitude of a musical partial that enables the modification to the spectrogram. Audio examples from a piano concerto are available for evaluation.",USA,education,Developed economies,"[5.175032, -47.56639]","[-39.74622, -25.607079]","[32.912716, 2.9763303, -1.7063429]","[-12.691733, -7.0728703, -31.380651]","[8.342652, 10.122733]","[6.586552, 5.306003]","[10.923257, 13.702143, 1.7648227]","[9.666585, 8.381413, 9.669913]"
52,Gabriele Barbieri;François Pachet;Mirko Degli Esposti;Pierre Roy,Is There a Relation Between the Syntax and the Fitness of an Audio Feature?.,2010,https://doi.org/10.5281/zenodo.1416014,Gabriele Barbieri+Università di Bologna>ITA>education|Sony CSL>FRA>company;François Pachet+Sony CSL>FRA>company;Mirko Degli Esposti+Università di Bologna>ITA>education;Pierre Roy+Sony CSL>FRA>company,"Feature generation has been proposed recently to generate feature sets automatically, as opposed to human-designed feature sets. This technique has shown promising results in many areas of supervised classification, in particular in the audio domain. However, feature generation is usually performed blindly, with genetic algorithms. As a result search performance is poor, thereby limiting its practical use. We propose a method to increase the search performance of feature generation systems. We focus on analytical features, i.e. features determined by their syntax. Our method consists in first extracting statistical properties of the feature space called spin patterns, by analogy with statistical physics. We show that spin patterns carry information about the topology of the feature space. We exploit these spin patterns to guide a simulated annealing algorithm specifically designed for feature generation. We evaluate our approach on three audio classification problems, and show that it increases performance by an order of magnitude. More generally this work is a first step in using tools from statistical physics for the supervised classification of complex audio signals.",ITA,education,Developed economies,"[5.469764, 6.951465]","[-0.46719146, 20.882318]","[-14.136702, -6.589082, 6.4211063]","[-7.9822297, -15.398001, 14.3040495]","[11.172969, 6.6164412]","[9.099274, 2.9026334]","[12.199735, 13.33862, -1.2577329]","[10.7603, 6.6955976, 10.920102]"
53,Dominik Schnitzer;Arthur Flexer;Gerhard Widmer;Martin Gasser,Islands of Gaussians: The Self Organizing Map and Gaussian Music Similarity Features.,2010,https://doi.org/10.5281/zenodo.1415248,Dominik Schnitzer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education,"Multivariate Gaussians are of special interest in the MIR field of automatic music recommendation. They are used as the de facto standard representation of music timbre to compute music similarity. However, standard algorithms for clustering and visualization are usually not designed to handle Gaussian distributions and their attached metrics (e.g. the Kullback-Leibler divergence). Hence to use these features the algorithms generally handle them indirectly by first mapping them to a vector space, for example by deriving a feature vector representation from a similarity matrix. This paper uses the symmetrized Kullback-Leibler centroid of Gaussians to show how to avoid the vectorization detour for the Self Organizing Maps (SOM) data visualization algorithm. We propose an approach so that the algorithm can directly and naturally work on Gaussian music similarity features to compute maps of music collections. We show that by using our approach we can create SOMs which (1) better preserve the original similarity topology and (2) are far less complex to compute, as the often costly vectorization step is eliminated.",AUT,facility,Developed economies,"[-6.4747963, 8.510964]","[26.505266, 16.983421]","[-7.7494583, -0.91825956, -2.1758866]","[15.289895, -8.142609, 21.62024]","[12.59358, 8.943057]","[11.030061, 1.956173]","[13.333607, 14.458754, -0.5421223]","[12.314853, 5.9740334, 13.165582]"
42,Youngmoo E. Kim;Erik M. Schmidt;Raymond Migneco;Brandon G. Morton;Patrick Richardson;Jeffrey J. Scott;Jacquelin A. Speck;Douglas Turnbull,State of the Art Report: Music Emotion Recognition: A State of the Art Review.,2010,https://doi.org/10.5281/zenodo.1417945,Youngmoo E. Kim+Drexel University>USA>education;Erik M. Schmidt+Drexel University>USA>education;Raymond Migneco+Drexel University>USA>education;Brandon G. Morton+Drexel University>USA>education;Patrick Richardson+Ithaca College>USA>education;Jeffrey Scott+Ithaca College>USA>education;Jacquelin A. Speck+Ithaca College>USA>education;Douglas Turnbull+Ithaca College>USA>education,"This paper surveys the state of the art in automatic emotion recognition in music. Music is oftentimes referred to as a “language of emotion” [1], and it is natural for us to categorize music in terms of its emotional associations. Myriad features, such as harmony, timbre, interpretation, and lyrics affect emotion, and the mood of a piece may also change over its duration. But in developing automated systems to organize music in terms of emotional content, we are faced with a problem that oftentimes lacks a well-defined answer; there may be considerable disagreement regarding the perception and interpretation of the emotions of a song or ambiguity within the piece itself. When compared to other music information retrieval tasks (e.g., genre identification), the identification of musical mood is still in its early stages, though it has received increasing attention in recent years. In this paper we explore a wide range of research in music emotion recognition, particularly focusing on methods that use contextual text information (e.g., websites, tags, and lyrics) and content-based approaches, as well as systems combining multiple feature domains.",USA,education,Developed economies,"[-58.887302, -1.2416333]","[51.93505, -6.543807]","[-25.732365, 21.411781, 6.0744057]","[11.42362, 21.00352, 6.1695547]","[14.059759, 12.959425]","[13.09217, 3.9865916]","[16.027466, 14.392923, 1.7890979]","[14.178004, 5.09367, 10.4886465]"
54,Matija Marolt;Marieke Lefeber,It's Time for a Song - Transcribing Recordings of Bell-playing Clocks.,2010,https://doi.org/10.5281/zenodo.1416704,Matija Marolt+University of Ljubljana>SVN>education;Marieke Lefeber+Meertens Instituut>NLD>facility,"The paper presents an algorithm for automatic transcription of recordings of bell-playing clocks. Bell-playing clocks are clocks containing a hidden bell-playing mechanism that is periodically activated to play a melody. Clocks from the eighteenth century give us unique insight into the musical taste of their owners, so we are interested in studying their repertoire and performances - thus the need for automatic transcription. In the paper, we first present an analysis of acoustical properties of bells found in bell-playing clocks. We propose a model that describes positions of bell partials and an algorithm that discovers the number of bells and positions of their partials in a given recording. To transcribe a recording, we developed a probabilistic method that maximizes the joint probability of a note sequence given the recording and positions of bell partials. Finally, we evaluate our algorithms on a set of recordings of bell-playing clocks.",SVN,education,Developed economies,"[20.568773, -27.978207]","[-13.386403, -5.0891304]","[-8.14428, -31.144154, -6.683512]","[2.8709185, -0.7023021, -15.542602]","[11.449852, 4.8345246]","[6.4597373, 2.2703776]","[11.2503605, 13.478826, -2.2974422]","[8.540584, 6.9505095, 11.024914]"
56,Brian McFee;Luke Barrington;Gert R. G. Lanckriet,Learning Similarity from Collaborative Filters.,2010,https://doi.org/10.5281/zenodo.1416198,"Brian McFee+University of California, San Diego>USA>education;Luke Barrington+University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education","Collaborative filtering methods (CF) exploit the wisdom of crowds to capture deeply structured similarities in musical objects, such as songs, artists or albums. When CF is available, it frequently outperforms content-based methods in recommendation tasks. However, songs in the so-called “long tail” cannot reap the benefits of collaborative filtering, and practitioners must rely on content-based methods. We propose a method for improving content-based recommendation in the long tail by learning an optimized similarity function from a sample of collaborative filtering data. Our experimental results demonstrate substantial improvements in accuracy by learning optimal similarity functions.",USA,education,Developed economies,"[-42.550835, 30.468962]","[37.520706, 13.948938]","[-5.350396, 19.476828, -7.5873876]","[16.821556, 4.6738358, 13.741701]","[13.4068575, 9.346418]","[12.456015, 2.086523]","[14.062932, 15.253552, -0.6708191]","[13.420657, 5.53831, 12.755514]"
57,Joaquín Mora;Francisco Gómez 0001;Emilia Gómez;Francisco Escobar-Borrego;José Miguel Díaz-Báñez,Characterization and Similarity in A Cappella Flamenco Cantes.,2010,https://doi.org/10.5281/zenodo.1415242,Joaquín Mora+University of Seville>ESP>education;Francisco Gómez+Polytechnic University of Madrid>ESP>education;Emilia Gómez+Universitat Pompeu Fabra>ESP>education;Francisco Escobar-Borrego+University of Seville>ESP>education;José Miguel Díaz-Báñez+University of Seville>ESP>education,"This paper intends to research on the link between musical similarity and style and sub-style (variant) classification in the context of flamenco a cappella singing styles. Given the limitation of standard computational models for melodic characterization and similarity computation in this particular context, we have proposed a specific set of melodic features adapted to flamenco singing styles. In order to evaluate them, we have gathered a collection of music recordings from the most representative singers and have manually extracted those proposed features. Based on those features, we have defined a similarity measure between two performances and have validated their usefulness in differentiating several styles and variants. The main conclusion of this work is the need to incorporate specific musical features to the design of similarity measures for flamenco music so that flamenco-adapted MIR systems can be developed.",ESP,education,Developed economies,"[-0.3232044, -15.000294]","[16.759768, 1.4370894]","[14.939794, 5.180147, -23.862415]","[4.782935, 5.312403, 4.3924704]","[11.62676, 9.976603]","[9.401882, 1.9309199]","[12.323478, 15.229366, -0.42229882]","[11.199144, 7.0032, 12.89325]"
58,Seokhwan Jo;Chang D. Yoo,Melody Extraction from Polyphonic Audio Based on Particle Filter.,2010,https://doi.org/10.5281/zenodo.1414978,Seokhwan Jo+Korea Advanced Institute of Science Technology>KOR>education;Chang D. Yoo+Korea Advanced Institute of Science Technology>KOR>education,"This paper considers a particle filter based algorithm to extract melody from a polyphonic audio in the short-time Fourier transforms (STFT) domain. The extraction is focused on overcoming the difficulties due to harmonic / percussive sound interferences, possibility of octave mismatch, and dynamic variation in melody. The main idea of the algorithm is to consider probabilistic relations between melody and polyphonic audio. Melody is assumed to follow a Markov process, and the framed segments of polyphonic audio are assumed to be conditionally independent given the parameters that represent the melody. The melody parameters are estimated using sequential importance sampling (SIS) which is a conventional particle filter method. In this paper, the likelihood and state transition are defined to overcome the aforementioned difficulties. The SIS algorithm relies on sequential importance density, and this density is designed using multiple pitches which are estimated by a simple multi-pitch extraction algorithm. Experimental results show that the considered algorithm outperforms other famous melody extraction algorithms in terms of the raw pitch accuracy (RPA) and the raw chroma accuracy (RCA).",KOR,education,Developing economies,"[5.7099905, -13.688605]","[-2.3375425, -9.37394]","[16.14805, 4.7106314, -5.452848]","[4.526475, -7.61233, -13.261358]","[9.985641, 9.989782]","[6.711264, 2.6972034]","[10.959086, 14.803749, -0.29873648]","[9.157641, 8.153873, 11.107996]"
59,Stanislaw Andrzej Raczynski;Emmanuel Vincent;Frédéric Bimbot;Shigeki Sagayama,Multiple Pitch Transcription using DBN-based Musicological Models.,2010,https://doi.org/10.5281/zenodo.1415198,Stanisław A. Raczyński+The University of Tokyo>JPN>education|INRIA Rennes>FRA>company;Emmanuel Vincent+INRIA Rennes>FRA>company;Frédéric Bimbot+INRIA Rennes>FRA>company;Shigeki Sagayama+The University of Tokyo>JPN>education,"We propose a novel approach to solve the problem of estimating pitches of notes present in an audio signal. We have developed a probabilistically rigorous model that takes into account temporal dependencies between musical notes and between the underlying chords, as well as the instantaneous dependencies between chords, notes and the observed note saliences. We investigated its modeling ability by measuring the cross-entropy with symbolic (MIDI) data and then proceed to observe the model's performance in multiple pitch estimation of audio data.",JPN,education,Developed economies,"[27.688177, -6.4812613]","[-12.532641, -8.001864]","[11.678053, -2.1319861, 15.30886]","[1.7606798, -12.640584, -5.996519]","[9.433498, 7.918208]","[6.8005557, 3.0209637]","[11.919539, 12.015301, 0.15858209]","[9.036482, 7.451134, 10.993603]"
60,Noor Azilah Draman;Campbell Wilson;Sea Ling,Modified Ais-based Classifier for Music Genre Classification.,2010,https://doi.org/10.5281/zenodo.1417437,Noor Azilah Draman+Monash University>AUS>education|Caulfield School of IT>AUS>education;Campbell Wilson+Monash University>AUS>education|Caulfield School of IT>AUS>education;Sea Ling Caulfield+Monash University>AUS>education|Caulfield School of IT>AUS>education,"Automating human capabilities for classifying different genre of songs is a difficult task. This has led to various studies that focused on finding solutions to solve this problem. Analyzing music contents (often referred as content-based analysis) is one of many ways to identify and group similar songs together. Various music contents, for example beat, pitch, timbral and many others were used and analyzed to represent the music. To be able to manipulate these content representations for recognition: feature extraction and classification are two major focuses of investigation in this area. Though various classification techniques proposed so far, we are introducing yet another one. The objective of this paper is to introduce a possible new technique in the Artificial Immune System (AIS) domain called a modified immune classifier (MIC) for music genre classification. MIC is the newest version of Negative Selection Algorithm (NSA) where it stresses the self and non-self cells recognition and a complementary process for generating detectors. The discussion will detail out the MIC procedures applied and the modified part in solving the classification problem. At the end, the results of proposed framework will be presented, discussed and directions for future work are given.",AUS,education,Developed economies,"[-28.297518, -13.319353]","[17.397732, -12.449885]","[-14.348672, 4.190702, 16.452742]","[13.447159, 4.943152, -5.5862]","[12.9284115, 10.839123]","[9.394196, 3.3578203]","[13.887715, 14.207704, 1.3908801]","[11.071828, 7.128234, 10.611639]"
61,Kazuma Murao;Masahiro Nakano;Yu Kitano;Nobutaka Ono;Shigeki Sagayama,Monophonic Instrument Sound Segregation by Clustering NMF Components Based on Basis Similarity and Gain Disjointness.,2010,https://doi.org/10.5281/zenodo.1417113,Kazuma Murao+The University of Tokyo>JPN>education;Masahiro Nakano+The University of Tokyo>JPN>education;Yu Kitano+The University of Tokyo>JPN>education;Nobutaka Ono+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"This paper discusses a method for monophonic instrument sound separation based on nonnegative matrix factorization (NMF). In general, it is not easy to classify NMF components into each instrument. By contrast, monophonic instrument sound gives us an important clue to classify them, because no more than one sound would be activated simultaneously. Our approach is to classify NMF components into each instrument based on basis spectrum vector similarity and temporal activity disjointness. Our clustering employs a hierarchical clustering algorithm: group average method (GAM). The efficiency of our approach is evaluated by some experiments.",JPN,education,Developed economies,"[7.91025, -40.361397]","[-45.007965, -24.524332]","[23.095186, -2.9125917, -4.252395]","[-4.34129, -10.861889, -31.506325]","[8.485345, 9.5641165]","[6.4168315, 5.0086603]","[11.082593, 13.1655855, 1.1502948]","[9.809036, 8.742425, 9.892984]"
62,Parag Chordia;Avinash Sastry;Trishul Mallikarjuna;Aaron Albin,Multiple Viewpoints Modeling of Tabla Sequences.,2010,https://doi.org/10.5281/zenodo.1416540,Parag Chordia+Georgia Tech>USA>education;Avinash Sastry+Georgia Tech>USA>education;Trishul Malikarjuna+Georgia Tech>USA>education;Aaron Albin+Georgia Tech>USA>education,"We describe a system that attempts to predict the continuation of a symbolically encoded tabla composition at each time step using a variable-length n-gram model. Using cross-entropy as a measure of model fit, the best model attained an entropy rate of 0.780 in a cross-validation experiment, showing that symbolic tabla compositions can be effectively encoded using such a model. The choice of smoothing algorithm, which determines how information from different-order models is combined, is found to be an important factor in the models performance. We extend the basic n-gram model by adding viewpoints, other streams of information that can be used to improve predictive performance. First, we show that adding a short-term model, built on the current composition and not the entire corpus, leads to substantial improvements. Additional experiments were conducted with derived types, representations derived from the basic data type (stroke names), and cross-types, which model dependencies between parameters, such as duration and stroke name. For this database, such extensions improved performance only marginally, although this may have been due to the low entropy rate attained by the basic model.",USA,education,Developed economies,"[34.572796, -46.930725]","[-7.9319997, -30.144873]","[29.834833, -17.452139, 4.228084]","[-18.973204, -2.970387, -3.2293756]","[7.739836, 7.838867]","[8.686648, 5.562153]","[11.05533, 11.466623, 1.1545326]","[9.325979, 6.0422173, 9.450893]"
63,Kaichun K. Chang;Jyh-Shing Roger Jang;Costas S. Iliopoulos,Music Genre Classification via Compressive Sampling.,2010,https://doi.org/10.5281/zenodo.1418289,Kaichun K. Chang+King's College London>GBR>education;Jyh-Shing Roger Jang+National Tsing Hua University>TWN>education;Costas S. Iliopoulos+King's College London>GBR>education,"Compressive sampling (CS) is a new research topic in signal processing that has piqued the interest of a wide range of researchers in different fields recently. In this paper, we present a CS-based classifier for music genre classification, with two sets of features, including short-time and long-time features of audio music. The proposed classifier generates a compact signature to achieve a significant reduction in the dimensionality of the audio music signals. The experimental results demonstrate that the computation time of the CS-based classifier is only about 20% of SVM on GTZAN dataset, with an accuracy of 92.7%. Several experiments were conducted in this study to illustrate the feasibility and robustness of the proposed methods as compared to other approaches.",GBR,education,Developed economies,"[-33.577408, -14.214599]","[18.21791, -14.404056]","[-17.858965, 9.68217, 19.450699]","[16.465158, 5.8072453, -9.595957]","[12.923304, 10.788803]","[9.512872, 3.622659]","[13.838938, 14.109982, 1.2617981]","[11.255926, 7.526195, 10.564687]"
66,Jun Wang;Xiaoou Chen;Yajie Hu;Tao Feng,Predicting High-level Music Semantics Using Social Tags via Ontology-based Reasoning.,2010,https://doi.org/10.5281/zenodo.1417785,Jun Wang+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education;Xiaoou Chen+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education;Yajie Hu+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education;Tao Feng+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education,"High-level semantics such as “mood” and “usage” are very useful in music retrieval and recommendation but they are normally hard to acquire. Can we predict them from a cloud of social tags? We propose a semantic identification and reasoning method: Given a music taxonomy system, we map it to an ontology’s terminology, map its finite set of terms to the ontology’s assertional axioms, and then map tags to the closest conceptual level of the referenced terms in WordNet to enrich the knowledge base, then we predict richer high-level semantic information with a set of reasoning rules. We find this method predicts mood annotations for music with higher accuracy, as well as giving richer semantic association information, than alternative SVM-based methods do.",CHN,education,Developing economies,"[-43.162712, 3.1096869]","[48.42736, -3.8585608]","[-18.493929, 13.0919, 3.2383173]","[16.733786, 20.548586, 5.3439245]","[14.30383, 9.386361]","[12.665691, 3.610917]","[15.204887, 13.9347725, -0.8379829]","[13.850834, 5.4201465, 10.813141]"
13,Emanuele Coviello;Luke Barrington;Antoni B. Chan;Gert R. G. Lanckriet,Automatic Music Tagging With Time Series Models.,2010,https://doi.org/10.5281/zenodo.1415274,"Emanuele Coviello+University of California, San Diego>USA>education;Luke Barrington+University of California, San Diego>USA>education;Antoni B. Chan+City University of Hong Kong>HKG>education;Gert. R. G. Lanckriet+University of California, San Diego>USA>education","State-of-the-art systems for automatic music tagging model music based on bag-of-feature representations which give little or no account of temporal dynamics, a key characteristic of the audio signal. We describe a novel approach to automatic music annotation and retrieval that captures temporal (e.g., rhythmical) aspects as well as timbral content. The proposed approach leverages a recently proposed song model that is based on a generative time series model of the musical content — the dynamic texture mixture (DTM) model — that treats fragments of audio as the output of a linear dynamical system. To model characteristic temporal dynamics and timbral content at the tag level, a novel, efficient hierarchical EM algorithm for DTM (HEM-DTM) is used to summarize the common information shared by DTMs modeling individual songs associated with a tag. Experiments show learning the semantics of music benefits from modeling temporal dynamics.",USA,education,Developed economies,"[-41.64928, -2.5777025]","[40.054653, -3.788295]","[-13.975897, 14.163872, 10.949863]","[26.16899, 4.9462814, 1.1144438]","[14.481237, 10.598055]","[11.421631, 3.5525424]","[15.573097, 14.126128, 0.08332828]","[13.030999, 6.471385, 11.139585]"
65,Michael I. Mandel;Douglas Eck;Yoshua Bengio,Learning Tags that Vary Within a Song.,2010,https://doi.org/10.5281/zenodo.1416130,"Michael I Mandel+LISA Lab, Université de Montréal>CAN>education;Douglas Eck+LISA Lab, Université de Montréal>CAN>education;Yoshua Bengio+LISA Lab, Université de Montréal>CAN>education","This paper examines the relationship between human generated tags describing different parts of the same song. These tags were collected using Amazon’s Mechanical Turk service. We find that the agreement between different people’s tags decreases as the distance between the parts of a song that they heard increases. To model these tags and these relationships, we describe a conditional restricted Boltzmann machine. Using this model to fill in tags that should probably be present given a context of other tags, we train automatic tag classifiers (autotaggers) that outperform those trained on the original data.",CAN,education,Developed economies,"[-40.568447, -1.5052626]","[40.182255, -0.3323414]","[-12.355647, 16.63393, 8.261408]","[26.044552, 8.406947, 4.320067]","[14.533683, 10.585083]","[11.60181, 3.3157856]","[15.614286, 14.116161, 0.10952344]","[13.088658, 6.3225985, 11.303791]"
55,Philippe Hamel;Douglas Eck,Learning Features from Music Audio with Deep Belief Networks.,2010,https://doi.org/10.5281/zenodo.1414970,Philippe Hamel+Université de Montréal>CAN>education;Douglas Eck+Université de Montréal>CAN>education,"Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically extract relevant features from audio for a given task. The feature extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the audio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. In particular, we learned the features to solve the task of genre recognition. The learned features perform significantly better than MFCCs. Moreover, we obtain a classification accuracy of 84.3% on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classifiers using frame-based features. We also applied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features.",CAN,education,Developed economies,"[-11.308048, -10.317721]","[21.211922, -15.209509]","[9.13664, 8.299228, 10.535083]","[21.459194, 4.703034, -8.898454]","[11.622984, 9.304918]","[9.90498, 3.9771135]","[13.294672, 13.316197, 0.7229077]","[11.447473, 6.7303014, 10.274662]"
41,Jingxuan Li;Tao Li 0001;Mitsunori Ogihara,Hierarchical Co-Clustering of Artists and Tags.,2010,https://doi.org/10.5281/zenodo.1416718,Jingxuan Li+Florida International University>USA>education;Tao Li+Florida International University>USA>education;Mitsunori Ogihara+University of Miami>USA>education,"The user-assigned tag is a growingly important research topic in MIR. Noticing that some tags are more specific versions of others, this paper studies the problem of organizing tags into a hierarchical structure by taking into account the fact that the corresponding artists are organized into a hierarchy based on genre and style. A novel clustering algorithm, Hierarchical Co-clustering Algorithm (HCC), is proposed as a solution. Unlike traditional hierarchical clustering algorithms that deal with homogeneous data only, the proposed algorithm simultaneously organizes two distinct data types into hierarchies. HCC is additionally able to receive constraints that state certain objects “must-be-together” or “should-be-together” and build clusters so as to satisfying the constraints. HCC may lead to better and deeper understandings of relationship between artists and tags assigned to them. An experiment finds that by trying to hierarchically cluster the two types of data better clusters are obtained for both. It is also shown that HCC is able to incorporate instance-level constraints on artists and/or tags to improve the clustering process.",USA,education,Developed economies,"[-40.31546, 4.1347837]","[41.194614, 4.6124787]","[-20.735401, 11.861922, 8.831036]","[23.681564, 7.4805565, 10.119132]","[14.309927, 10.131245]","[11.982129, 2.8997233]","[15.189165, 14.591525, 0.018407369]","[13.268143, 5.953201, 11.747122]"
64,Yannis Panagakis;Constantine Kotropoulos;Gonzalo R. Arce,Sparse Multi-label Linear Embedding Within Nonnegative Tensor Factorization Applied to Music Tagging.,2010,https://doi.org/10.5281/zenodo.1417036,Yannis Panagakis+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown;Constantine Kotropoulos+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown;Gonzalo R. Arce+University of Delaware>USA>education,"A novel framework for music tagging is proposed. First, each music recording is represented by bio-inspired auditory temporal modulations. Then, a multilinear subspace learning algorithm based on sparse label coding is developed to effectively harness the multi-label information for dimensionality reduction. The proposed algorithm is referred to as Sparse Multi-label Linear Embedding Nonnegative Tensor Factorization, whose convergence to a stationary point is guaranteed. Finally, a recently proposed method is employed to propagate the multiple labels of training auditory temporal modulations to auditory temporal modulations extracted from a test music recording by means of the sparse ℓ1 reconstruction coefficients. The overall framework, that is described here, outperforms both humans and state-of-the-art computer audition systems in the music tagging task, when applied to the CAL500 dataset.",GRC,education,Developed economies,"[-35.97921, -10.211247]","[17.571627, -24.321703]","[-17.919943, 13.392987, 16.514006]","[19.677092, 7.74294, -13.480723]","[13.951891, 10.506894]","[6.5736547, 4.6402736]","[14.952386, 14.068909, 0.36808747]","[10.618291, 8.439089, 10.515025]"
39,Scott Miller;Paul Reimer;Steven R. Ness;George Tzanetakis,"Geoshuffle: Location-Aware, Content-based Music Browsing Using Self-organizing Tag Clouds.",2010,https://doi.org/10.5281/zenodo.1417703,Scott Miller+University of Victoria>CAN>education;Paul Reimer+University of Victoria>CAN>education;Steven Ness+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"In the past few years the computational capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShuffle – a prototype system for content-based music browsing and exploration that targets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning capabilities based on GPS. GeoShuffle adds location-based and time-based context to a user’s listening preferences. Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of interaction is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of textual information that can be displayed. We propose self-organizing tag clouds, a 2D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evaluate the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can improve the quality of music recommendation and that self-organizing tag clouds provide faster browsing and are more engaging than text-based tag clouds.",CAN,education,Developed economies,"[-39.794792, 1.3479896]","[29.268154, 19.688688]","[-18.714323, 15.248116, 10.186768]","[15.265965, -2.949617, 20.001678]","[14.512307, 10.127199]","[11.581219, 1.816856]","[15.406134, 14.188722, -0.23088951]","[12.594997, 5.4882917, 12.9323845]"
40,Christian Fremerey;Meinard Müller;Michael Clausen,Handling Repeats and Jumps in Score-performance Synchronization.,2010,https://doi.org/10.5281/zenodo.1415942,Christian Fremerey+Bonn University>DEU>education;Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Michael Clausen+Bonn University>DEU>education,"Given a score representation and a recorded performance of the same piece of music, the task of score-performance synchronization is to temporally align musical sections such as bars specified by the score to temporal sections in the performance. Most of the previous approaches assume that the score and the performance to be synchronized globally agree with regard to the overall musical structure. In practice, however, this assumption is often violated. For example, a performer may deviate from the score by ignoring a repeat or introducing an additional repeat that is not written in the score. In this paper, we introduce a synchronization approach that can cope with such structural differences. As main technical contribution, we describe a novel variant of dynamic time warping (DTW), referred to as JumpDTW, which allows for handling jumps and repeats in the alignment. Our approach is evaluated for the practically relevant case of synchronizing score data obtained from scanned sheet music via optical music recognition to corresponding audio recordings. Our experiments based on Beethoven piano sonatas show that JumpDTW can robustly identify and handle most of the occurring jumps and repeats leading to an overall alignment accuracy of over 99% on the bar-level.",DEU,education,Developed economies,"[24.45418, -29.308357]","[-16.830662, -15.529828]","[-3.5300856, -17.529232, -14.61873]","[1.6705278, -23.27657, -3.2050524]","[11.140186, 5.714695]","[6.0065336, 0.74521446]","[12.049855, 12.565972, -2.124784]","[8.03887, 5.9118752, 10.967946]"
15,Jakob Abeßer;Paul Bräuer;Hanna M. Lukashevich;Gerald Schuller,Bass Playing Style Detection Based on High-level Features and Pattern Similarity.,2010,https://doi.org/10.5281/zenodo.1418213,Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Paul Bräuer+Piranha Musik & IT>DEU>company;Hanna Lukashevich+Fraunhofer IDMT>DEU>facility;Gerald Schuller+Fraunhofer IDMT>DEU>facility,"In this paper, we compare two approaches for automatic classification of bass playing styles, one based on high-level features and another one based on similarity measures between bass patterns. For both approaches, we compare two different strategies: classification of patterns as a whole and classification of all measures of a pattern with a subsequent accumulation of the classification results. Furthermore, we investigate the influence of potential transcription errors on the classification accuracy, which tend to occur when real audio data is analyzed. We achieve best classification accuracy values of 60.8% for the feature-based classification and 68.5% for the classification based on pattern similarity based on a taxonomy consisting of 8 different bass playing styles.",DEU,facility,Developed economies,"[-26.527279, -18.4079]","[17.73268, -10.467451]","[-12.147255, 2.887297, 24.639114]","[8.620598, 13.369505, -6.6098547]","[12.458561, 10.721444]","[9.30021, 3.330199]","[13.415762, 14.04585, 1.3216265]","[11.274154, 7.44849, 10.912957]"
17,Qi Lu;Xiaoou Chen;Deshun Yang;Jun Wang,Boosting for Multi-Modal Music Emotion Classification.,2010,https://doi.org/10.5281/zenodo.1417085,Qi Lu+Peking University>CHN>education;Xiaoou Chen+Peking University>CHN>education;Deshun Yang+Peking University>CHN>education;Jun Wang+Peking University>CHN>education,"With the explosive growth of music recordings, automatic classification of music emotion becomes one of the hot spots on research and engineering. Typical music emotion classification (MEC) approaches apply machine learning methods to train a classifier based on audio features. In addition to audio features, the MIDI and lyrics features of music also contain useful semantic information for predicting the emotion of music. In this paper we apply AdaBoost algorithm to integrate MIDI, audio and lyrics information and propose a two-layer classifying strategy called Fusion by Subtask Merging for 4-class music emotion classification. We evaluate each modality respectively using SVM, and then combine any two of the three modalities, using AdaBoost algorithm (MIDI+audio, MIDI+lyrics, audio+lyrics). Moreover, integrating this in a multimodal system (MIDI+audio+lyrics) allows an improvement in the overall performance. The experimental results show that MIDI, audio and lyrics information are complementary, and can be combined to improve a classification system.",CHN,education,Developing economies,"[-57.249966, -0.6055007]","[53.49636, -9.28315]","[-24.789501, 23.455816, 8.4101715]","[11.20541, 27.07157, 7.023204]","[13.936069, 12.861748]","[12.921503, 4.218898]","[16.031975, 14.49189, 1.7141975]","[14.276111, 5.211318, 10.366512]"
18,Thierry Bertin-Mahieux;Ron J. Weiss;Daniel P. W. Ellis,Clustering Beat-Chroma Patterns in a Large Music Database.,2010,https://doi.org/10.5281/zenodo.1416720,Thierry Bertin-Mahieux+Columbia University>USA>education;Ron J. Weiss+New York University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"A musical style or genre implies a set of common conventions and patterns combined and deployed in different ways to make individual musical pieces; for instance, most would agree that contemporary pop music is assembled from a relatively small palette of harmonic and melodic patterns. The purpose of this paper is to use a database of tens of thousands of songs in combination with a compact representation of melodic-harmonic content (the beat-synchronous chromagram) and data-mining tools (clustering) to attempt to explicitly catalog this palette – at least within the limitations of the beat-chroma representation. We use online k-means clustering to summarize 3.7 million 4-beat bars in a codebook of a few hundred prototypes. By measuring how accurately such a quantized codebook can reconstruct the original data, we can quantify the degree of diversity (distortion as a function of codebook size) and temporal structure (i.e. the advantage gained by joint quantizing multiple frames) in this music. The most popular codewords themselves reveal the common chords used in the music. Finally, the quantized representation of music can be used for music retrieval tasks such as artist and genre classification, and identifying songs that are similar in terms of their melodic-harmonic content.",USA,education,Developed economies,"[-7.8213096, 1.9651363]","[11.049083, 2.2122445]","[0.9370411, -0.9670547, -7.2871776]","[5.2429495, 0.15574874, -1.6095347]","[12.010513, 8.128246]","[9.823488, 2.0124598]","[12.896852, 14.09229, 0.047127403]","[11.1595955, 7.0204425, 12.207514]"
19,Markus Schedl;Tim Pohle;Noam Koenigstein;Peter Knees,What's Hot? Estimating Country-specific Artist Popularity.,2010,https://doi.org/10.5281/zenodo.1415870,Markus Schedl+Johannes Kepler University>AUT>education;Tim Pohle+Johannes Kepler University>AUT>education;Noam Koenigstein+Tel Aviv University>ISR>education;Peter Knees+Johannes Kepler University>AUT>education,"Predicting artists that are popular in certain regions of the world is a well desired task, especially for the music industry. Also the cosmopolitan and cultural-aware music aficionado is likely be interested in which music is currently “hot” in other parts of the world. We therefore propose four approaches to determine artist popularity rankings on the country-level. To this end, we mine the following data sources: page counts from Web search engines, user posts on Twitter, shared folders on the Gnutella file sharing network, and playcount data from last.fm. We propose methods to derive artist rankings based on these four sources and perform cross-comparison of the resulting rankings via overlap scores. We further elaborate on the advantages and disadvantages of all approaches as they yield interestingly diverse results.",AUT,education,Developed economies,"[-41.34798, 11.755883]","[44.236412, 13.092344]","[-27.847128, 7.3540525, 6.5042663]","[17.051348, 11.468563, 13.5882]","[14.549457, 9.780755]","[12.364351, 2.266935]","[15.101214, 14.823779, -0.40394717]","[13.35694, 5.3135457, 12.15414]"
20,Ron J. Weiss;Juan Pablo Bello,Identifying Repeated Patterns in Music Using Sparse Convolutive Non-negative Matrix Factorization.,2010,https://doi.org/10.5281/zenodo.1415934,"Ron J. Weiss+Music and Audio Research Lab (MARL), New York University>USA>education;Juan Pablo Bello+Music and Audio Research Lab (MARL), New York University>USA>education","We describe an unsupervised, data-driven, method for automatically identifying repeated patterns in music by analyzing a feature matrix using a variant of sparse convolutive non-negative matrix factorization. We utilize sparsity constraints to automatically identify the number of patterns and their lengths, parameters that would normally need to be fixed in advance. The proposed analysis is applied to beat-synchronous chromagrams in order to concurrently extract repeated harmonic motifs and their locations within a song. Finally, we show how this analysis can be used for long-term structure segmentation, resulting in an algorithm that is competitive with other state-of-the-art segmentation algorithms based on hidden Markov models and self similarity matrices.",USA,education,Developed economies,"[-5.492698, -1.2106016]","[2.0390978, 0.7084433]","[5.984164, -7.7701063, 4.7028384]","[0.8536398, 1.4084274, -5.3029146]","[11.255716, 8.189466]","[7.681733, 3.2265005]","[12.4068985, 13.565518, 0.14849976]","[10.488934, 8.104101, 11.2906475]"
21,Cillian Kelly;Mikel Gainza;David Dorran;Eugene Coyle,Locating Tune Changes and Providing a Semantic Labelling of Sets of Irish Traditional Tunes.,2010,https://doi.org/10.5281/zenodo.1418303,Cillian Kelly+Dublin Institute of Technology>IRL>education|Audio Research Group>IRL>facility;Mikel Gainza+Dublin Institute of Technology>IRL>education|Audio Research Group>IRL>facility;David Dorran+Dublin Institute of Technology>IRL>education|Audio Research Group>IRL>facility;Eugene Coyle+Dublin Institute of Technology>IRL>education|Audio Research Group>IRL>facility,"An approach is presented which provides the tune change locations within a set of Irish Traditional tunes. Also provided are semantic labels for each part of each tune within the set. A set in Irish Traditional music is a number of individual tunes played segue. Each of the tunes in the set are made up of structural segments called parts. Musical variation is a prominent characteristic of this genre. However, a certain set of notes known as ‘set accented tones’ are considered impervious to musical variation. Chroma information is extracted at ‘set accented tone’ locations within the music. The resulting chroma vectors are grouped to represent the parts of the music. The parts are then compared with one another to form a part similarity matrix. Unit kernels which represent the possible structures of an Irish Traditional tune are matched with the part similarity matrix to determine the tune change locations and semantic part labels.",IRL,education,Developed economies,"[-26.602436, 18.730997]","[13.627414, 2.1591115]","[-14.290827, -1.4022604, -4.0936227]","[3.851518, 2.640728, 1.655037]","[13.383609, 8.657755]","[9.3288765, 1.9809192]","[12.168411, 14.059616, -1.9611746]","[11.13219, 6.955298, 12.773021]"
22,Matthias Mauch;Simon Dixon,Approximate Note Transcription for the Improved Identification of Difficult Chords.,2010,https://doi.org/10.5281/zenodo.1416598,Matthias Mauch+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord profiles and higher-level time-series modelling have received a lot of attention, resulting in methods with an overall performance of more than 70% in the MIREX Chord Detection task 2009. Research on the front end of chord transcription algorithms has often concentrated on finding good chord templates to fit the chroma features. In this paper we reverse this approach and seek to find chroma features that are more suitable for usage in a musically-motivated model. We do so by performing a prior approximate transcription using an existing technique to solve non-negative least squares problems (NNLS). The resulting NNLS chroma features are tested by using them as an input to an existing state-of-the-art high-level model for chord transcription. We achieve very good results of 80% accuracy using the song collection and metric of the 2009 MIREX Chord Detection tasks. This is a significant increase over the top result (74%) in MIREX 2009. The nature of some chords makes their identification particularly susceptible to confusion between fundamental frequency and partials. We show that the recognition of these difficult chords in particular is substantially improved by the prior approximate transcription using NNLS.",GBR,education,Developed economies,"[51.276657, -5.588426]","[-30.528738, 20.603476]","[23.81002, -11.28461, 13.660426]","[-25.379246, -4.6615515, 2.6708431]","[7.078077, 8.661957]","[6.157156, 3.6564622]","[11.957558, 10.670346, 1.7964479]","[9.736525, 8.904032, 12.167323]"
23,Thomas Rocher;Matthias Robine;Pierre Hanna;Laurent Oudre,Concurrent Estimation of Chords and Keys from Audio.,2010,https://doi.org/10.5281/zenodo.1417485,"Thomas Rocher+LaBRI, University of Bordeaux>FRA>education;Matthias Robine+LaBRI, University of Bordeaux>FRA>education;Pierre Hanna+LaBRI, University of Bordeaux>FRA>education;Laurent Oudre+Institut TELECOM, TELECOM ParisTech>FRA>education","This paper proposes a new method for local key and chord estimation from audio signals. A harmonic content of the musical piece is first extracted by computing a set of chroma vectors. Correlation with fixed chord and key templates then selects a set of key/chord pairs for every frame. A weighted acyclic harmonic graph is then built with these pairs as vertices, and the use of a musical distance to weigh its edges. Finally, the output sequences of chords and keys are obtained by finding the best path in the graph. The proposed system allows a mutual and beneficial chord and key estimation. It is evaluated on a corpus composed of Beatles songs for both the local key estimation and chord recognition tasks. Results show that it performs better than state-of-the art chord analysis algorithms while providing a more complete harmonic analysis.",FRA,education,Developed economies,"[52.932156, -6.017288]","[-30.210812, 19.752668]","[29.234991, -14.352965, 13.59916]","[-24.943747, -6.8755827, 3.5443864]","[7.0835447, 8.416439]","[6.38162, 3.4567354]","[11.916677, 10.688362, 1.8378907]","[9.852876, 8.682243, 12.071907]"
24,Hussein Hirjee;Daniel G. Brown 0001,Solving Misheard Lyric Search Queries Using a Probabilistic Model of Speech Sounds.,2010,https://doi.org/10.5281/zenodo.1414942,Hussein Hirjee+University of Waterloo>CAN>education;Daniel G. Brown+University of Waterloo>CAN>education,"Music listeners often mishear the lyrics to unfamiliar songs heard from public sources, such as the radio. Since standard text search engines will find few relevant results when they are entered as a query, these misheard lyrics require phonetic pattern matching techniques to identify the song. We introduce a probabilistic model of mishearing trained on examples of actual misheard lyrics, and develop a phoneme similarity scoring matrix based on this model. We compare this scoring method to simpler pattern matching algorithms on the task of finding the correct lyric from a collection given a misheard query. The probabilistic method significantly outperforms all other methods, finding 5-8% more correct lyrics within the first five hits than the previous best method.",CAN,education,Developed economies,"[-25.80092, -29.32044]","[32.756462, -11.180723]","[10.111711, 16.95522, -6.9515686]","[12.21998, -12.051949, 14.103866]","[11.059392, 11.437833]","[10.618546, 2.7967885]","[12.112758, 15.758616, 0.59642583]","[12.195175, 6.475925, 11.692502]"
25,Noam Koenigstein;Gert R. G. Lanckriet;Brian McFee;Yuval Shavitt,Collaborative Filtering Based on P2P Networks.,2010,https://doi.org/10.5281/zenodo.1415620,"Noam Koenigstein+Tel Aviv University>ISR>education|University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education;Brian McFee+University of California, San Diego>USA>education;Yuval Shavitt+Tel Aviv University>ISR>education","Peer-to-Peer (P2P) networks are used by millions of people for sharing music files. As these networks become ever more popular, they also serve as an excellent source for Music Information Retrieval (MIR) tasks. This paper reviews the latest MIR studies based on P2P data-sets, and presents a new file sharing data collection system over the Gnutella. We discuss several advantages of P2P based data-sets over some of the more “traditional” data sources, and evaluate the information quality of our data-set in comparison to other data sources (Last.fm, social tags, biography data, and MFCCs). The evaluation is based on an artists similarity task using Partial Order Embedding (POE). We show that a P2P based Collaborative Filtering data-set performs at least as well as “traditional” data-sets, yet maintains some inherent advantages such as scale, availability and additional information features such as ID3 tags and geographical location.",ISR,education,Developing economies,"[-43.23058, 29.983486]","[48.43978, 18.804285]","[-5.9289317, 20.276617, -8.950827]","[21.778746, 0.7178101, 14.118965]","[15.954724, 9.256962]","[12.108029, 2.0745344]","[15.733073, 15.781628, -1.5288355]","[13.091557, 5.4631186, 12.48485]"
26,Alex Hrybyk;Youngmoo E. Kim,Combined Audio and Video Analysis for Guitar Chord Identification.,2010,https://doi.org/10.5281/zenodo.1417465,Alex Hrybyk+Drexel University>USA>education;Youngmoo Kim+Drexel University>USA>education,"This paper presents a multi-modal approach to automatically identifying guitar chords using audio and video of the performer. Chord identification is typically performed by analyzing the audio, using a chroma based feature to extract pitch class information, then identifying the chord with the appropriate label. Even if this method proves perfectly accurate, stringed instruments add extra ambiguity as a single chord or melody may be played in different positions on the fretboard. Preserving this information is important, because it signifies the original fingering, and implied “easiest” way to perform the selection. This chord identification system combines analysis of audio to determine the general chord scale (i.e. A major, G minor), and video of the guitarist to determine chord voicing (i.e. open, barred, inversion), to accurately identify the guitar chord.",USA,education,Developed economies,"[52.621258, -7.4552927]","[-34.756958, 17.874702]","[27.45351, -11.220369, 11.937171]","[-27.451735, -9.211788, -0.26254842]","[7.195398, 8.403987]","[6.385971, 3.634153]","[11.840035, 10.785118, 1.8873347]","[9.705462, 8.754998, 12.045204]"
16,Leigh M. Smith,Beat Critic: Beat Tracking Octave Error Identification By Metrical Profile Analysis.,2010,https://doi.org/10.5281/zenodo.1417891,Leigh M. Smith+IRCAM>FRA>facility,"Computational models of beat tracking of musical audio have been well explored, however, such systems often make “octave errors”, identifying the beat period at double or half the beat rate than that actually recorded in the music. A method is described to detect if octave errors have occurred in beat tracking. Following an initial beat tracking estimation, a feature vector of metrical profile separated by spectral subbands is computed. A measure of subbeat quaver (1/8th note) alternation is used to compare half time and double time measures against the initial beat track estimation and indicate a likely octave error. This error estimate can then be used to re-estimate the beat rate. The performance of the approach is evaluated against the RWC database, showing successful identification of octave errors for an existing beat tracker. Using the octave error detector together with the existing beat tracking model improved beat tracking by reducing octave errors to 43% of the previous error rate.",FRA,facility,Developed economies,"[34.428394, -31.987047]","[-27.14616, -2.2306056]","[6.5880933, -28.299183, -8.827838]","[-5.7564836, 11.400838, -6.9435625]","[10.494412, 4.379551]","[5.373536, 1.8008039]","[10.30722, 12.875538, -2.1720233]","[7.5135503, 6.8962817, 11.183319]"
28,Arthur Flexer;Dominik Schnitzer;Martin Gasser;Tim Pohle,Combining Features Reduces Hubness in Audio Similarity.,2010,https://doi.org/10.5281/zenodo.1416360,Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Dominik Schnitzer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University Linz>AUT>education;Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Tim Pohle+Johannes Kepler University Linz>AUT>education,"In audio based music similarity, a well known effect is the existence of hubs, i.e. songs which appear similar to many other songs without showing any meaningful perceptual similarity. We verify that this effect also exists in very large databases (> 250000 songs) and that it even gets worse with growing size of databases. By combining different aspects of audio similarity we are able to reduce the hub problem while at the same time maintaining a high overall quality of audio similarity.",AUT,facility,Developed economies,"[-6.6439133, -19.940443]","[26.62736, 9.93556]","[0.6044943, -5.6190696, -23.233608]","[17.387285, -3.5815983, 8.389014]","[12.782577, 8.99795]","[10.596485, 2.1195946]","[12.2830515, 13.625701, 0.69905317]","[12.09656, 6.3996296, 12.740701]"
29,Nick Collins,Computational Analysis of Musical Influence: A Musicological Case Study Using MIR Tools.,2010,https://doi.org/10.5281/zenodo.1416756,Nick Collins+University of Sussex>GBR>education,"Are there new insights through computational methods to the thorny problem of plotting the flow of musical influence? This project, motivated by a musicological study of early synth pop, applies MIR tools as an aid to the investigator. Web scraping and web services provide one angle, sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz. Charts of influence are constructed in GraphViz combining artist similarity and dates. Content based music similarity is the second approach, based around a core collection of synth pop albums. The prospect for new musical analyses are discussed with respect to these techniques.",GBR,education,Developed economies,"[-14.393301, 52.6366]","[47.50025, 8.465407]","[-33.76203, 1.8632452, 3.7247014]","[11.443035, 12.373825, 8.3488865]","[13.262585, 5.514465]","[12.11856, 2.6619036]","[14.583751, 11.886127, -1.3185058]","[13.111327, 5.476969, 11.708804]"
30,Jin Ha Lee,Crowdsourcing Music Similarity Judgments using Mechanical Turk.,2010,https://doi.org/10.5281/zenodo.1416478,Jin Ha Lee+University of Washington>USA>education,"Collecting human judgments for music similarity evaluation has always been a difficult and time consuming task. This paper explores the viability of Amazon Mechanical Turk (MTurk) for collecting human judgments for audio music similarity evaluation tasks. We compared the similarity judgments collected from Evalutron6000 (E6K) and MTurk using the Music Information Retrieval Evaluation eXchange 2009 Audio Music Similarity and Retrieval task dataset. Our data show that the results are highly comparable, and MTurk may be a useful method for collecting subjective ground truth data. Furthermore, there are several benefits to using MTurk over the traditional E6K infrastructure. We conclude that using MTurk is a practical alternative of music similarity when it is used with some precautions.",USA,education,Developed economies,"[-4.508134, 11.733911]","[31.775768, 8.962432]","[-7.586019, 15.509693, 3.050737]","[10.67901, 4.766132, 11.331446]","[13.030757, 9.296913]","[11.27382, 2.2596095]","[13.442833, 15.096908, -0.39720342]","[12.736667, 6.394801, 12.580492]"
31,Frédéric Bimbot;Olivier Le Blouch;Gabriel Sargent;Emmanuel Vincent,Decomposition Into Autonomous and Comparable Blocks: A Structural Description of Music Pieces.,2010,https://doi.org/10.5281/zenodo.1414734,"Frédéric Bimbot+IRISA, CNRS - UMR 6074>FRA>facility;Olivier Le Blouch+INRIA, Rennes Bretagne Atlantique>FRA>facility;Gabriel Sargent+IRISA, CNRS - UMR 6074>FRA>facility;Emmanuel Vincent+INRIA, Rennes Bretagne Atlantique>FRA>facility","The structure of a music piece is a concept which is often referred to in various areas of music sciences and technologies, but for which there is no commonly agreed definition. This raises a methodological issue in MIR, when designing and evaluating automatic structure inference algorithms. It also strongly limits the possibility to produce consistent large-scale annotation datasets in a cooperative manner. This article proposes an approach called decomposition into autonomous and comparable blocks, based on principles inspired from structuralism and generativism. It specifies a methodology for producing music structure annotation by human listeners based on simple criteria and resorting solely to the listening experience of the annotator. We show on a development set that the proposed approach can provide a reasonable level of concordance across annotators and we introduce a set of annotations on the RWC database, intended to be released to the MIR community.",FRA,facility,Developed economies,"[-0.5284387, -3.1175146]","[-2.4348793, 8.63343]","[3.5695834, -8.601251, -2.353159]","[-3.8175042, 0.9556778, 3.4225776]","[11.93019, 8.357985]","[8.750444, 2.409446]","[12.646355, 14.04222, -0.41069037]","[10.518121, 6.6403055, 11.404726]"
32,Bruno Angeles;Cory McKay;Ichiro Fujinaga,Discovering Metadata Inconsistencies.,2010,https://doi.org/10.5281/zenodo.1415550,Bruno Angeles+McGill University>CAN>education;Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper describes the use of fingerprinting-based querying in identifying metadata inconsistencies in music libraries, as well as the updates to the jMusicMeta-Manager software in order to perform the analysis. Test results are presented for both the Codaich database and a generic library of unprocessed metadata. Statistics were computed in order to evaluate the differences between a manually-maintained library and an unprocessed collection when comparing metadata with values on a MusicBrainz server queried by fingerprinting.",CAN,education,Developed economies,"[-23.71928, 42.96631]","[18.86758, 30.934835]","[-26.412327, -2.5721054, -9.596573]","[-1.2200968, -1.6214097, 18.227188]","[14.435857, 9.006798]","[10.930303, 0.3161871]","[15.236293, 13.475304, -1.233331]","[11.740037, 4.9616265, 12.166629]"
33,Darrell Conklin;Mathieu Bergeron,Discovery of Contrapuntal Patterns.,2010,https://doi.org/10.5281/zenodo.1417413,"Darrell Conklin+Universidad del País Vasco>ESP>education|IKERBASQUE, Basque Foundation for Science>ESP>facility;Mathieu Bergeron+McGill University>CAN>education","This paper develops and applies a new method for the discovery of polyphonic patterns. The method supports the representation of abstract relations that are formed between notes that overlap in time without being simultaneous. Such relations are central to understanding species counterpoint. The method consists of an application of the vertical viewpoint technique, which relies on a vertical slicing of the musical score. It is applied to two-voice contrapuntal textures extracted from the Bach chorale harmonizations. Results show that the new method is powerful enough to represent and discover distinctive modules of species counterpoint, including remarkably the suspension principle of fourth species counterpoint. In addition, by focusing on two voices in particular and setting them against all other possible voice pairs, the method can elicit patterns that illustrate well the unique treatment of the voices under investigation, e.g. the inner and outer voices. The results are promising and indicate that the method is suitable for computational musicology research.",ESP,education,Developed economies,"[16.941088, 25.964409]","[-11.711722, 16.457912]","[-4.1201653, -18.384552, 8.596673]","[-9.007558, -6.724557, 4.774425]","[11.518832, 7.3304424]","[8.106659, 1.8490268]","[12.373183, 13.178404, -0.7839016]","[9.870284, 7.07814, 12.244705]"
34,Alberto Pinto,Eigenvector-based Relational Motif Discovery.,2010,https://doi.org/10.5281/zenodo.1415906,Alberto Pinto+Università degli Studi di Milano>ITA>education,"The development of novel analytical tools to investigate the structure of music works is central in current music information retrieval research. In particular, music summarization aims at finding the most representative parts of a music piece (motifs) that can be exploited for an efficient music database indexing system. Here we present a novel approach for motif discovery in music pieces based on an eigenvector method. Scores are segmented into a network of bars and then ranked depending on their centrality. Bars with higher centrality are more likely to be relevant for music summarization. Results on the corpus of J.S.Bach’s 2-part Inventions demonstrate the effectiveness of the method and suggest that different musical metrics might be more suitable than others for different applications.",ITA,education,Developed economies,"[15.71415, 25.742855]","[17.226171, 12.611558]","[-4.8557253, -15.660088, 9.030856]","[7.0006847, -3.959451, 8.684451]","[11.500277, 7.489831]","[9.715734, 1.429494]","[12.519373, 13.146751, -0.69269705]","[11.230502, 6.327903, 12.715542]"
35,Cory McKay;John Ashley Burgoyne;Jason Hockman;Jordan B. L. Smith;Gabriel Vigliensoni;Ichiro Fujinaga,"Evaluating the Genre Classification Performance of Lyrical Features Relative to Audio, Symbolic and Cultural Features.",2010,https://doi.org/10.5281/zenodo.1415706,Cory McKay+McGill University>CAN>education;John Ashley Burgoyne+McGill University>CAN>education;Jason Hockman+McGill University>CAN>education;Jordan B. L. Smith+McGill University>CAN>education;Gabriel Vigliensoni+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper describes experimental research investigating the genre classification utility of combining features extracted from lyrical, audio, symbolic and cultural sources of musical information. It was found that cultural features consisting of information extracted from both web searches and mined listener tags were particularly effective, with the result that classification accuracies were achieved that compare favorably with the current state of the art of musical genre classification. It was also found that features extracted from lyrics were less effective than the other feature types. Finally, it was found that, with some exceptions, combining feature types does improve classification performance. The new lyricFetcher and jLyrics software are also presented as tools that can be used as a framework for developing more effective classification methodologies based on lyrics in the future.",CAN,education,Developed economies,"[-32.14438, -18.771559]","[22.70165, -4.8799353]","[-12.738225, 9.51343, 17.208567]","[8.808864, 11.053544, -0.35430577]","[12.712872, 11.649021]","[10.11292, 3.149824]","[14.21437, 14.762881, 1.4137055]","[11.868057, 6.769298, 11.093817]"
37,Miguel Molina-Solana;Maarten Grachten;Gerhard Widmer,Evidence for Pianist-specific Rubato Style in Chopin Nocturnes.,2010,https://doi.org/10.5281/zenodo.1417793,Miguel Molina-Solana+University of Granada>ESP>education;Maarten Grachten+Ghent University>BEL>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"The performance of music usually involves a great deal of interpretation by the musician. In classical music, the final ritardando is a good example of the expressive aspect of music performance. Even though expressive timing data is expected to have a strong component that is determined by the piece itself, in this paper we investigate to what degree individual performance style has an effect on the timing of final ritardandi. The particular approach taken here uses Friberg and Sundberg’s kinematic rubato model in order to characterize performed ritardandi. Using a machine-learning classifier, we carry out a pianist identification task to assess the suitability of the data for characterizing the individual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, when cancelling the piece-specific aspects, pianists can often be identified with accuracy above baseline. This fact suggests the existence of a performer-specific style of playing ritardandi.",ESP,education,Developed economies,"[4.976269, 29.39626]","[-32.5713, 7.643102]","[-5.973537, -0.404136, -30.809502]","[-12.632105, 3.3488815, -1.5742933]","[11.335665, 5.018839]","[7.731208, 3.5585172]","[11.186918, 13.042134, -2.52881]","[9.106098, 6.2772546, 10.8413515]"
38,Jason Hockman;Ichiro Fujinaga,Fast vs Slow: Learning Tempo Octaves from User Data.,2010,https://doi.org/10.5281/zenodo.1416110,Jason A. Hockman+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"The widespread use of beat- and tempo-tracking methods in music information retrieval tasks has been marginalized due to undesirable sporadic results from these algorithms. While sensorimotor and listening studies have demonstrated the subjectivity and variability inherent to human performance of this task, MIR applications such as recommendation require more reliable output than available from present tempo estimation models. In this paper, we present a initial investigation of tempo assessment based on the simple classification of whether the music is fast or slow. Through three experiments, we provide performance results of our method across two datasets, and demonstrate its usefulness in the pursuit of a reliable global tempo estimation.",CAN,education,Developed economies,"[44.206715, -27.126637]","[-30.714727, -4.7585063]","[1.4332771, -30.787256, 4.6743402]","[-8.463013, 12.090636, -9.759278]","[11.447006, 4.348498]","[5.1109977, 1.7468377]","[10.864171, 13.363298, -2.8295422]","[7.3648567, 6.8122168, 11.054305]"
27,Teppo E. Ahonen,Combining Chroma Features For Cover Version Identification.,2010,https://doi.org/10.5281/zenodo.1414766,Teppo E. Ahonen+University of Helsinki>FIN>education,"We present an approach for cover version identification which is based on combining different discretized features derived from the chromagram vectors extracted from the audio data. For measuring similarity between features, we use a parameter-free quasi-universal similarity metric which utilizes data compression. Evaluation proves that combined feature distances increase the accuracy in cover version identification.",FIN,education,Developed economies,"[6.4927764, 45.90894]","[25.606548, -12.528315]","[1.2937591, 16.15333, -23.543531]","[13.763395, -2.5451677, 0.18433705]","[16.091291, 11.123366]","[10.190526, 2.6327205]","[12.862608, 17.33835, -0.3373099]","[11.680997, 6.796894, 11.830551]"
36,Joachim Ganseman;Paul Scheunders;Gautham J. Mysore;Jonathan S. Abel,Evaluation of a Score-informed Source Separation System.,2010,https://doi.org/10.5281/zenodo.1416062,Joachim Ganseman+IBBT - Visielab>BEL>facility|University of Antwerp>BEL>education;Paul Scheunders+IBBT - Visielab>BEL>facility|University of Antwerp>BEL>education;Gautham J. Mysore+CCRMA>USA>facility|Stanford University>USA>education;Jonathan S. Abel+CCRMA>USA>facility|Stanford University>USA>education,"In this work, we investigate a method for score-informed source separation using Probabilistic Latent Component Analysis (PLCA). We present extensive test results that give an indication of the performance of the method, its strengths and weaknesses. For this purpose, we created a test database that has been made available to the public, in order to encourage comparisons with alternative methods.",BEL,facility,Developed economies,"[6.102779, -49.25902]","[-42.986183, -24.836529]","[34.20684, 0.3696916, -2.5229568]","[-8.804699, -8.862984, -32.987038]","[8.351191, 10.160981]","[6.373294, 5.0980167]","[10.902149, 13.707449, 1.8115147]","[9.909868, 8.720107, 9.772427]"
66,Verena Thomas;Christian Wagner 0003;Michael Clausen,OCR based post processing of OMR for the recovery of transposing instruments in complex orchestral scores.,2011,https://doi.org/10.5281/zenodo.1415790,Verena Thomas+University of Bonn>DEU>education;Christian Wagner+University of Bonn>DEU>education;Michael Clausen+University of Bonn>DEU>education,"Given a scanned score page, Optical Music Recognition (OMR) attempts to reconstruct all contained music information. However, the available OMR systems lack the ability to recognize transposition information contained in complex orchestral scores. An additional unsolved OMR problem is the handling of orchestral scores using compressed notation. Here, the information of which instrument has to play which staff is crucial for a correct interpretation of the score. But this mapping is lost along the pages of the score during the OMR process. In this paper, we present a method for retrieving the instrumentation and transposition information of orchestral scores. In our approach, we combine the results of Optical Character Recognition (OCR) and OMR to regain the information available through text annotations of the score. In addition, a method to reconstruct the instrument and transposition information for staves where text annotations were omitted or not recognized is presented. In an evaluation we analyze the impact of transposition information on the quality of score-audio synchronizations of orchestral music. The results show that the knowledge of transposing instruments improves the synchronization accuracy and that our method helps in regaining this knowledge.",DEU,education,Developed economies,"[33.273987, 29.704082]","[-22.48637, 36.38361]","[6.727466, -21.011639, 19.636467]","[-10.6150465, -21.996675, -1.1260505]","[9.675977, 6.27967]","[6.7273664, -0.5697302]","[12.643577, 11.3541355, -1.4872924]","[7.921653, 4.1376257, 10.625429]"
65,Esben Paul Bugge;Kim Lundsteen Juncher;Brian Søborg Mathiasen;Jakob Grue Simonsen,Using Sequence Alignment and Voting to Improve Optical Music Recognition from Multiple Recognizers.,2011,https://doi.org/10.5281/zenodo.1418175,Esben Paul Bugge+University of Copenhagen>DNK>education;Kim Lundsteen Juncher+University of Copenhagen>DNK>education;Brian Søborg Mathiasen+University of Copenhagen>DNK>education;Jakob Grue Simonsen+University of Copenhagen>DNK>education,"Digitalizing sheet music using Optical Music Recognition (OMR) is error-prone, especially when using noisy images created from scanned prints. Inspired by DNA-sequence alignment, we devise a method to use multiple sequence alignment to automatically compare output from multiple third party OMR tools and perform automatic error-correction of pitch and duration of notes. We perform tests on a corpus of 49 one-page scores of varying quality. Our method on average reduces the amount of errors from an ensemble of 4 commercial OMR tools. The method achieves, on average, fewer errors than each recognizer by itself, but statistical tests show that it is significantly better than only 2 of the 4 commercial recognizers. The results suggest that recognizers may be improved somewhat by sequence alignment and voting, but that more elaborate methods may be needed to obtain substantial improvements. All software, scanned music data used for testing, and experiment protocols are open source and available at: http://code.google.com/p/omr-errorcorrection/",DNK,education,Developed economies,"[38.879353, 22.317017]","[-19.985115, 37.956486]","[17.137062, 14.783275, 9.5289755]","[-9.085685, -21.543463, 2.7951126]","[8.583785, 6.1282434]","[6.570581, -0.4287519]","[10.656026, 11.107792, -0.12823391]","[7.9657903, 4.3042836, 10.740561]"
61,George Sioros;Carlos Guedes,Complexity Driven Recombination of MIDI Loops.,2011,https://doi.org/10.5281/zenodo.1415804,George Sioros+University of Porto>PRT>education;Carlos Guedes+University of Porto>PRT>education|INESC - Porto>PRT>facility,"An algorithm and a software application for recombining in real time MIDI drum loops that makes use of a novel analysis of rhythmic patterns that sorts them in order of their complexity is presented. We measure rhythmic complexity by comparing each rhythmic pattern found in the loops to a metrical template characteristic of its time signature. The complexity measure is used to sort the MIDI loops prior to utilizing them in the recombination algorithm. This way, the user can effectively control the complexity and variation in the generated rhythm during performance.",PRT,education,Developed economies,"[39.67643, 2.1712763]","[-20.618944, 3.2630572]","[7.15604, -7.5825224, 26.987234]","[3.2199397, 17.078115, -6.1542325]","[10.254429, 7.3919773]","[6.130719, 1.5417858]","[12.748888, 11.432838, -0.36450177]","[8.089413, 6.5042844, 11.649555]"
63,Phillip B. Kirlin;David D. Jensen,Probabilistic Modeling of Hierarchical Music Analysis.,2011,https://doi.org/10.5281/zenodo.1417119,Phillip B. Kirlin+University of Massachusetts Amherst>USA>education;David D. Jensen+University of Massachusetts Amherst>USA>education,"Hierarchical music analysis, as exemplified by Schenkerian analysis, describes the structure of a musical composition by a hierarchy among its notes. Each analysis defines a set of prolongations, where musical objects persist in time even though others are present. We present a formal model for representing hierarchical music analysis, probabilistic interpretations of that model, and an efficient algorithm for computing the most probable analysis under these interpretations. We represent Schenkerian analyses as maximal outerplanar graphs (MOPs). We use this representation to encode the largest known data set of computer-processable Schenkerian analyses, and we use these data to identify statistical regularities in the human-generated analyses. We show that a dynamic programming algorithm can be applied to these regularities to identify the maximum likelihood analysis for a given piece of music.",USA,education,Developed economies,"[-0.11055114, 3.663901]","[-10.875785, 20.206964]","[-2.4279573, -4.1028514, 4.9195657]","[-12.310772, -0.42420447, 7.1640635]","[12.104868, 8.273034]","[8.587437, 2.0052974]","[13.066393, 13.730817, -0.53990483]","[9.945761, 6.476689, 11.878417]"
62,Michael Scott Cuthbert;Christopher Ariza;Lisa Friedland,Feature Extraction and Machine Learning on Symbolic Music using the music21 Toolkit.,2011,https://doi.org/10.5281/zenodo.1416288,Michael Scott Cuthbert+M.I.T.>USA>education;Christopher Ariza+M.I.T.>USA>education;Lisa Friedland+University of Massachusetts Amherst>USA>education,"Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the “feature” capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard feature-extraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system’s built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper’s demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.",USA,education,Developed economies,"[15.265033, 14.717893]","[-0.8589276, 28.714804]","[-2.1386843, -6.6998453, 15.615553]","[-8.656886, -0.8756285, 13.590086]","[11.758961, 7.1029077]","[9.635157, 1.570043]","[13.500599, 12.3371315, -1.0583314]","[10.588719, 6.0358047, 11.596721]"
60,Florence Levé;Richard Groult;Guillaume Arnaud;Cyril Séguin;Rémi Gaymay;Mathieu Giraud,Rhythm Extraction from Polyphony Symbolic Music.,2011,https://doi.org/10.5281/zenodo.1416368,Florence Levé+Université de Picardie Jules Verne>FRA>education;Richard Groult+Université de Picardie Jules Verne>FRA>education;Guillaume Arnaud+Université de Picardie Jules Verne>FRA>education;Cyril Séguin+Université de Picardie Jules Verne>FRA>education;Rémi Gaymay+Université Lille 1>FRA>education;Mathieu Giraud+Université Lille 1>FRA>education,"In this paper, we focus on the rhythmic component of symbolic music similarity, proposing several ways to extract a monophonic rhythmic signature from a symbolic polyphonic score. To go beyond the simple extraction of all time intervals between onsets (notes on extraction), we select notes according to their length (short and long extractions) or their intensities (intensity+/− extractions). Once the rhythm is extracted, we use dynamic programming to compare several sequences. We report results of analysis on the size of rhythm patterns that are specific to a unique piece, as well as experiments on similarity queries (ragtime music and Bach chorale variations). These results show that long and intensity+ extractions are often good choices for rhythm extraction. Our conclusions are that, even from polyphonic symbolic music, rhythm alone can be enough to identify a piece or to perform pertinent music similarity queries, especially when using wise rhythm extractions.",FRA,education,Developed economies,"[13.342023, 13.496206]","[5.280713, 12.095134]","[-1.0477132, -7.648304, 12.235531]","[5.102418, -5.8510756, 3.8864758]","[11.627817, 6.977072]","[8.164843, 1.1577628]","[13.1938, 12.463485, -0.9991361]","[9.674709, 6.5537353, 12.650654]"
59,Mika Laitinen;Kjell Lemström,Dynamic Programming in Transposition and Time-Warp Invariant Polyphonic Content-Based Music Retrieval.,2011,https://doi.org/10.5281/zenodo.1416094,Mika Laitinen+University of Helsinki>FIN>education;Kjell Lemström+University of Helsinki>FIN>education,"We consider the problem of transposition and time-warp invariant (TTWI) polyphonic content-based music retrieval (CBMR) in symbolically encoded music. For this setting, we introduce two new algorithms based on dynamic programming. Given a query point set, of size m, to be searched for in a database point set, of size n, and applying a search window of width w, our algorithms run in time O(mnw) for finding exact TTWI occurrences, and O(mnw2) for partial occurrences. Our new algorithms are computationally more efficient as their counterparts in the worst case scenario. More importantly, the elegance of our algorithms lies in their simplicity: they are much easier to implement and to understand than the rivalling sweepline-based algorithms. Our solution bears also theoretical interest. Dynamic programming has been used in very basic content-based retrieval problems, but generalizing them to more complex cases has proven to be challenging. In this special, seemingly more complex case, however, dynamic programming seems to be a viable option.",FIN,education,Developed economies,"[-11.676225, 21.050014]","[9.916454, 16.998093]","[-5.334172, 2.3092248, -10.323852]","[7.86595, -11.638427, 7.2943077]","[13.332124, 8.03153]","[9.039321, 0.8012476]","[13.190141, 14.538013, -1.8977723]","[10.709836, 6.295806, 13.324187]"
64,Jonathan Bragg;Elaine Chew;Stuart M. Shieber,Neo-Riemannian Cycle Detection with Weighted Finite-State Transducers.,2011,https://doi.org/10.5281/zenodo.1416152,"Jonathan Bragg+Harvard University>USA>education;Elaine Chew+Queen Mary, University of London>GBR>education;Stuart Shieber+Harvard University>USA>education","This paper proposes a finite-state model for detecting harmonic cycles as described by neo-Riemannian theorists. Given a string of triads representing a harmonic analysis of a piece, the task is to identify and label all substrings corresponding to these cycles with high accuracy. The solution method uses a noisy channel model implemented with weighted finite-state transducers. On a dataset of four works by Franz Schubert, our model predicted cycles in the same regions as cycles in the ground truth with a precision of 0.18 and a recall of 1.0. The recalled cycles had an average edit distance of 3.2 insertions or deletions from the ground truth cycles, which average 6.4 labeled triads in length. We suggest ways in which our model could be used to contribute to current work in music theory, and be generalized to other music pattern-finding applications.",USA,education,Developed economies,"[41.645798, -30.63773]","[-12.333964, 8.172547]","[1.2070189, -34.934048, -4.510071]","[-0.7808869, -6.322342, -7.15257]","[11.39484, 4.4424863]","[7.1974573, 3.0276334]","[10.812651, 13.29247, -2.7836797]","[9.726166, 8.037668, 11.8343115]"
67,Masahiro Niitsuma;Yo Tomita,Classifying Bach's Handwritten C-Clefs.,2011,https://doi.org/10.5281/zenodo.1415822,"Masahiro Niitsuma+Queen's University, Belfast>GBR>education;Yo Tomita+Queen's University, Belfast>GBR>education","The aim of this study is to explore how we could use computational technology to help determination of the chronology of music manuscripts. Applying a battery of techniques to Bach’s manuscripts reveals the limitation in current image processing techniques, thereby clarifying future tasks. Analysis of C-clefs, the chosen musical symbol for this study, extracted from Bach’s manuscripts dating from 1708–1748, is also carried out. Random forest using 15 features produces significant accuracy for chronological classification.",GBR,education,Developed economies,"[26.90388, 13.764168]","[16.075207, -29.633537]","[3.810142, -16.417358, 17.102175]","[-1.1159773, -6.2428837, 9.991129]","[11.213756, 7.3329806]","[9.33975, 1.1861972]","[12.658448, 12.93004, -0.9116724]","[10.636586, 6.1684427, 12.424064]"
72,Débora C. Corrêa;Alexandre L. M. Levada;Luciano da F. Costa,Finding Community Structure in Music Genres Networks.,2011,https://doi.org/10.5281/zenodo.1418147,D´ebora C. Corrˆea+Instituto de Física de São Carlos>BRA>education;Luciano da F. Costa+Instituto de Física de São Carlos>BRA>education;Alexandre L. M. Levada+Universidade Federal de São Carlos>BRA>education,"Complex networks have shown to be promising mechanisms to represent several aspects of nature, since their topological and structural features help in the understanding of relations, properties and intrinsic characteristics of the data. In this context, we propose to build music networks in order to find community structures of music genres. Our main contributions are twofold: 1) Define a totally unsupervised approach for music genres discrimination; 2) Incorporate topological features in music data analysis. We compared different distance metrics and clustering algorithms. Each song is represented by a vector of conditional probabilities for the note values in its percussion track. Initial results indicate the effectiveness of the proposed methodology.",BRA,education,Developing economies,"[-38.802216, 11.4956]","[42.984753, 8.770462]","[-25.416523, 8.300045, 2.5088935]","[19.820871, 4.4072723, 9.518923]","[14.709452, 9.47431]","[12.087401, 2.6103356]","[15.097939, 14.706202, -0.68692106]","[13.250043, 5.7089534, 11.924553]"
69,Kerstin Neubarth;Mathieu Bergeron;Darrell Conklin,Associations between Musicology and Music Information Retrieval.,2011,https://doi.org/10.5281/zenodo.1416564,"Kerstin Neubarth+Canterbury Christ Church University>GBR>education;Mathieu Bergeron+McGill University>CAN>education;Darrell Conklin+Universidad del País Vasco>ESP>education|IKERBASQUE, Basque Foundation for Science>ESP>facility","A higher level of interdisciplinary collaboration between music information retrieval (MIR) and musicology has been proposed both in terms of MIR tools for musicology, and musicological motivation and interpretation of MIR research. Applying association mining and content citation analysis methods to musicology references in ISMIR papers, this paper explores which musicological subject areas are of interest to MIR, whether references to specific musicology areas are significantly over-represented in specific MIR areas, and precisely why musicology is cited in MIR.",GBR,education,Developed economies,"[-23.818436, 21.198698]","[28.867971, 34.025867]","[-14.023704, 6.9266977, -7.6671104]","[4.2170687, 5.476453, 19.585966]","[14.363628, 8.1794]","[11.948218, 0.57761115]","[14.45987, 14.727909, -1.9018162]","[12.2478285, 4.3149605, 12.157799]"
70,Dingding Wang 0001;Mitsunori Ogihara,Potential Relationship Discovery in Tag-Aware Music Style Clustering and Artist Social Networks.,2011,https://doi.org/10.5281/zenodo.1418221,Dingding Wang+University of Miami>USA>education|Center for Computational Science>USA>facility;Mitsunori Ogihara+University of Miami>USA>education|Center for Computational Science>USA>facility,"With the rapid growth of music information and data in today’s ever changing world, exploring and analyzing music style has become more and more difficult. Traditional content-based methods for music style analysis and newly emerged tag-based methods usually assume music items are independent of each other. However, in real world applications, do there exist some relationships among them. In this paper, we construct the social relation graph among different music artists by extracting the friendship information from social media such as Twitter, and incorporate the generated social networking graph into tag-based music style clustering. Experiments on real data show the effectiveness of this novel integration of different information sources.",USA,education,Developed economies,"[-40.754948, 2.643417]","[41.532627, 3.2017314]","[-19.277899, 13.104338, 8.539805]","[22.64235, 9.395134, 8.472122]","[14.424169, 10.20731]","[12.101613, 2.9930496]","[15.364661, 14.413895, -0.059995312]","[13.332753, 5.913179, 11.669627]"
71,Charith Gunaratna;Evan Stoner;Ronaldo Menezes,Using Network Sciences to Rank Musicians and Composers in Brazilian Popular Music.,2011,https://doi.org/10.5281/zenodo.1415818,Charith Gunaratna+Florida Tech>USA>education;Evan Stoner+Florida Tech>USA>education;Ronaldo Menezes+Florida Tech>USA>education,Music fascinates and touches most people. This fascination leads to opinions about the music pieces that reflects people’s exposure and personal experience. This inherent bias of people towards music indicates that personal opinion is inappropriate for defining the quality of music and musicians. This paper takes a holistic view of the problem and delves into the understanding of the structure of Brazilian music rooted in Network Sciences. In this paper we work with a large database of albums of Brazilian music and study the structure of collaborations between all the musicians and composers. The collaboration is modelled as a social network of musicians and then analyzed from different perspectives with the goal of describing what we call the structure of that musical genre as well as provide a ranking of musicians and composers.,USA,education,Developed economies,"[-40.306293, 13.689318]","[44.33286, 9.187788]","[-29.23396, 6.138139, 2.204784]","[15.189396, 10.454104, 10.389266]","[14.602246, 9.380347]","[12.232641, 2.6706707]","[14.910575, 14.756173, -0.8336532]","[13.236372, 5.6017356, 11.877171]"
73,Robert Macrae;Simon Dixon,"Guitar Tab Mining, Analysis and Ranking.",2011,https://doi.org/10.5281/zenodo.1416476,Robert Macrae+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"With over 4.5 million tablatures and chord sequences (collectively known as tabs), the web holds vast quantities of hand annotated scores in non-standardised text files. These scores are typically error-prone and incomplete, and tab collections contain many duplicates, making retrieval of high quality tabs difficult. Despite this, tabs are by far the most popular means of sharing musical instructions on the internet. We have developed tools that use text analysis and alignment for the automatic retrieval, interpretation and analysis of such tabs in order to filter and estimate the most accurate tabs from the multitude available. We show that the standard means of ranking tabs, such as search engine ranks or user ratings, have little correlation with the accuracy of a tab and that a better ranking method is to use features such as the concurrency between tabs of the same song. We also compare the quality of top-ranked tabs with state-of-the-art chord transcription output and find that the latter provides a more reliable source of chord symbols with an accuracy rate 10% higher than the ranked hand annotations.",GBR,education,Developed economies,"[46.610176, -9.578195]","[13.497807, 19.066668]","[26.377195, -11.674054, 6.4943624]","[11.3870325, -8.159639, 15.03296]","[7.6827393, 8.182867]","[9.461616, 0.85106915]","[11.583892, 11.234019, 1.3786513]","[10.868675, 6.0278645, 12.917052]"
74,Cory McKay;David Bainbridge 0001,A Musical Web Mining and Audio Feature Extraction Extension to The Greenstone Digital Library Software.,2011,https://doi.org/10.5281/zenodo.1415006,Cory McKay+Marianopolis College>CAN>education;David Bainbridge+University of Waikato>NZL>education,"This paper describes updates to the Greenstone open source digital library software that significantly expand its functionality with respect to music. The first of the two major improvements now allows Greenstone to extract and store classification-oriented features from audio files using a newly updated version of the jAudio software. The second major improvement involves the implementation and integration of the new jSongMiner software, which provides Greenstone with a framework for automatically identifying audio recordings using audio fingerprinting and then extracting extensive metadata about them from a variety of resources available on the Internet. Several illustrative use cases and case studies are discussed.",CAN,education,Developed economies,"[-20.37327, 35.768364]","[12.141833, 29.5666]","[-21.861479, 2.6310227, -14.900473]","[-2.9334366, -1.7872517, 17.132032]","[14.146052, 7.661184]","[10.592352, 0.9843337]","[14.227811, 14.41376, -2.1663046]","[11.428606, 5.341428, 11.892743]"
75,Sefki Kolozali;Mathieu Barthet;György Fazekas;Mark B. Sandler,Knowledge Representation Issues in Musical Instrument Ontology Design.,2011,https://doi.org/10.5281/zenodo.1416142,"Sefki Kolozali+Centre for Digital Music, Queen Mary University of London>GBR>education;Mathieu Barthet+Centre for Digital Music, Queen Mary University of London>GBR>education;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","This paper presents preliminary work on musical instruments ontology design, and investigates heterogeneity and limitations in existing instrument classification schemes. Numerous research to date aims at representing information about musical instruments. The works we examined are based on the well known Hornbostel and Sach’s classification scheme. We developed representations using the Ontology Web Language (OWL), and compared terminological and conceptual heterogeneity using SPARQL queries. We found evidence to support that traditional designs based on taxonomy trees lead to ill-defined knowledge representation, especially in the context of an ontology for the Semantic Web. In order to overcome this issue, it is desirable to have an instrument ontology that exhibits a semantically rich structure.",GBR,education,Developed economies,"[-25.632965, 38.383335]","[16.466694, 40.005974]","[-22.480188, -1.3255944, -1.771708]","[-2.1270776, -4.7968497, 24.988914]","[14.283653, 8.987578]","[10.851977, -0.19989914]","[15.076137, 13.729884, -1.2778504]","[12.155057, 5.092558, 11.575032]"
76,György Fazekas;Mark B. Sandler,The Studio Ontology Framework.,2011,https://doi.org/10.5281/zenodo.1417735,György Fazekas+Queen Mary University of London>GBR>education;Mark B. Sandler+Queen Mary University of London>GBR>education,"This paper introduces the Studio Ontology Framework for describing and sharing detailed information about music production. The primary aim of this ontology is to capture the nuances of record production by providing an explicit, application and situation independent conceptualisation of the studio environment. We may use the ontology to describe real-world recording scenarios involving physical hardware, or (post) production on a personal computer. It builds on Semantic Web technologies and previously published ontologies for knowledge representation and knowledge sharing.",GBR,education,Developed economies,"[-26.38955, 40.295975]","[17.664576, 39.99428]","[-24.586956, -1.9454116, -3.7813778]","[-3.8312607, -4.9233823, 26.602665]","[14.281037, 8.904675]","[10.768682, -0.23626003]","[15.11671, 13.591813, -1.210633]","[12.191735, 5.1351967, 11.601396]"
77,Ruofeng Chen;Ming Li,Music Structural Segmentation by Combining Harmonic and Timbral Information.,2011,https://doi.org/10.5281/zenodo.1414774,"Ruofeng Chen+Georgia Institute of Technology>USA>education;Ming Li+Institute of Acoustics, Chinese Academy of Sciences>CHN>facility","We propose a novel model for music structural segmentation aiming at combining harmonic and timbral information. We use two-level clustering with splitting initialization and random turbulence to produce segment labels using chroma and MFCC separately as feature. We construct a score matrix to combine segment labels from both aspects. Finally Non-negative Matrix Factorization and Maximum Likelihood are applied to extract the final segment labels. By comparing sparseness, our method is capable of automatically determining the number of segment types in a given song. The pairwise F-measure of our algorithm can reach 0.63 without rules of music knowledge, running on 180 Beatles songs. We show our model can be easily associated with more sophisticated structural segmentation algorithms and extended to probabilistic models.",USA,education,Developed economies,"[-2.0118117, -3.6551526]","[-2.259892, 2.393585]","[5.658101, -6.103614, -2.069983]","[-0.5250419, 0.4077283, -4.65075]","[11.611103, 8.288955]","[7.88591, 2.9402902]","[12.372788, 14.129141, 0.088923335]","[10.334415, 7.8027906, 11.438353]"
58,Camélia Constantin;Cédric du Mouza;Zoé Faget;Philippe Rigaux,The Melodic Signature Index for Fast Content-based Retrieval of Symbolic Scores Camelia Constantin.,2011,https://doi.org/10.5281/zenodo.1416860,"Camelia Constantin+LIP6, Univ. Paris 6>FRA>education;Zoé Faget+Armadillo & Univ. Paris-Dauphine>FRA>education;Cédric du Mouza+CEDRIC, CNAM>FRA>education;Philippe Rigaux+CEDRIC, CNAM>FRA>education","NEUMA is an on-line library that stores collections of symbolic scores and proposes a public interface to search for melodic pieces based on several kinds of patterns: pitches-based, with or without rhythms, transposed or not. In addition, searches can be either exact or approximate. We describe an index structure apt at supporting all these searches in a consistent setting. Its distinctive feature is an encoding of the various information that might be involved in the pattern-matching process with algebraic signatures. The properties of these signatures are suitable to represent in a compact and expressive way the sequences of complex features that constitute a melodic description.",FRA,education,Developed economies,"[3.8882802, 19.065403]","[9.832557, 4.205618]","[1.474106, 4.712835, -3.85787]","[2.9287322, -3.0379171, 4.316843]","[12.065136, 9.778438]","[8.662731, 1.389655]","[12.575962, 15.443879, -0.989219]","[10.379207, 6.654173, 12.593826]"
79,Matthias Mauch;Mark Levy,Structural Change on Multiple Time Scales as a Correlate of Musical Complexity.,2011,https://doi.org/10.5281/zenodo.1416240,Matthias Mauch+Last.fm>GBR>company;Mark Levy+Last.fm>GBR>company,"We propose the novel audio feature structural change for the analysis and visualisation of recorded music, and argue that it is related to a particular notion of musical complexity. Structural change is a meta feature that can be calculated from an arbitrary frame-wise basis feature, with each element in the structural change feature vector representing the change of the basis feature at a different time scale. We describe an efficient implementation of the feature and discuss its properties based on three basis features pertaining to harmony, rhythm and timbre. We present a novel flower-like visualisation that allows us to illustrate the overall structural change characteristics of a piece of audio in a compact way. Several examples of real-world music and synthesised audio exemplify the characteristics of the structural change feature. We present the results of a web-based listening experiment with 197 participants to show the validity of the proposed feature.",GBR,company,Developed economies,"[-1.1542886, 11.266165]","[-8.697039, 8.250768]","[-5.5332203, 2.1458514, 5.171304]","[-5.280442, 2.6240523, -3.044661]","[12.823146, 9.350572]","[8.742602, 2.3954487]","[13.303177, 15.177728, -0.83364064]","[10.568035, 7.0516343, 11.694517]"
68,Gabriel Vigliensoni;John Ashley Burgoyne;Andrew Hankinson;Ichiro Fujinaga,Automatic Pitch Detection in Printed Square Notation.,2011,https://doi.org/10.5281/zenodo.1415580,Gabriel Vigliensoni+McGill University>CAN>education;John Ashley Burgoyne+McGill University>CAN>education;Andrew Hankinson+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"In this paper we present our research in the development of a pitch-finding system to extract the pitches of neumes—some of the oldest representations of pitch in Western music—from the Liber Usualis, a well-known compendium of plainchant as used in the Roman Catholic church. Considerations regarding the staff position, staff removal, space- and line-zones, as well as how we treat specific neume classes and modifiers are covered. This type of notation presents a challenge for traditional optical music recognition (OMR) systems because individual note pitches are indivisible from the larger ligature group that forms the neume. We have created a dataset of correctly-notated transcribed chant for comparing the performance of different variants of our pitch-finding system. The best result showed a recognition rate of 97% tested with more than 2000 neumes.",CAN,education,Developed economies,"[27.463684, -20.904629]","[-24.893286, 37.421635]","[15.103837, -17.428135, -9.33422]","[-14.542833, -21.359371, 4.0646687]","[9.927296, 5.671056]","[6.8127112, -0.5567595]","[10.837395, 13.293638, -0.80542845]","[7.9212003, 4.1313567, 10.595685]"
78,Gabriel Sargent;Frédéric Bimbot;Emmanuel Vincent,A Regularity-Constrained Viterbi Algorithm and Its Application to The Structural Segmentation of Songs.,2011,https://doi.org/10.5281/zenodo.1415950,Gabriel Sargent+Université de Rennes 1>FRA>education;Frédéric Bimbot+CNRS>FRA>facility;Emmanuel Vincent+INRIA Rennes>FRA>facility,"""This paper presents a general approach for the structural segmentation of songs. It is formalized as a cost optimization problem that combines properties of the musical content and prior regularity assumption on the segment length. A versatile implementation of this approach is proposed by means of a Viterbi algorithm, and the design of the costs are discussed. We then present two systems derived from this approach, based on acoustic and symbolic features respectively. The advantages of the regularity constraint are evaluated on a database of 100 popular songs by showing a significant improvement of the segmentation performance in terms of F-measure.""",FRA,education,Developed economies,"[-2.855447, -4.348014]","[-3.0224748, 3.1916678]","[2.772617, -5.5173635, -3.993648]","[-2.534445, 0.8309854, -4.2284756]","[11.597392, 8.334526]","[8.2492485, 2.9306486]","[12.435343, 14.09934, 0.056280203]","[10.386746, 7.6466446, 11.504187]"
46,Andrew Hankinson;Perry Roland;Ichiro Fujinaga,The Music Encoding Initiative as a Document-Encoding Framework.,2011,https://doi.org/10.5281/zenodo.1417609,"Andrew Hankinson+CIRMMT / Schulich School of Music, McGill University>CAN>education;Perry Roland+University of Virginia>USA>education;Ichiro Fujinaga+CIRMMT / Schulich School of Music, McGill University>CAN>education","Recent changes in the Music Encoding Initiative (MEI) have transformed it into an extensible platform from which new notation encoding schemes can be produced. This paper introduces MEI as a document-encoding framework, and illustrates how it can be extended to encode new types of notation, eliminating the need for creating specialized and potentially incompatible notation encoding standards.",CAN,education,Developed economies,"[-22.314098, 32.27783]","[0.68314964, 39.074482]","[-21.622335, 4.2605944, -19.963923]","[-12.411911, -7.125551, 20.05047]","[14.013519, 7.30758]","[9.866734, 0.12291329]","[14.338605, 13.840785, -2.2193851]","[10.373603, 4.908891, 11.822145]"
56,Mathieu Barthet;Simon Dixon,Ethnographic Observations of Musicologists at the British Library: Implications for Music Information Retrieval.,2011,https://doi.org/10.5281/zenodo.1415670,Mathieu Barthet+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Without a rich understanding of user behaviours and needs, music information retrieval (MIR) systems might not be ideally suited to their potential users. In this study, we followed an ethnographic methodology to elicit some of the strategies used by musicologists to explore and document musical performances, in order to investigate if and how technologies could enhance such a process. Observations of musicologists studying historical recordings of classical music were conducted at the British Library. The observations show that the musicologists alternate between a closed listening practice, relying exclusively on aural observations, and a multimodal listening practice, where they interact with various music representations and information sources using different media (e.g. metadata about the recordings and performers, sound visualisations, scores, lyrics and performance videos). The spoken parts of broadcast recordings brought historical/extra-musical clues helping to understand music performance practices. Sound visualisation and computational methods fostered the analysis of specific musical expression patterns. We suggest that software designed for musicologists should facilitate switching between closed and multimodal listening modes, interaction with scores and lyrics, and analysis and annotation of speech and music performance using content-based MIR techniques.",GBR,education,Developed economies,"[-24.855455, 23.560438]","[30.169342, 37.12758]","[-19.238874, 8.706026, -9.685937]","[-0.9551537, 10.78104, 16.081875]","[14.489999, 8.168111]","[11.647752, 0.6406153]","[14.652515, 14.653446, -1.9707577]","[12.109674, 4.4576573, 11.655844]"
32,Jakob Abeßer;Olivier Lartillot,Modeling Musical Attributes to Characterize Two-Track Recordings with Bass and Drums.,2011,https://doi.org/10.5281/zenodo.1417529,Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Olivier Lartillot+University of Jyvaskyla>FIN>education,"In this publication, we present a method to characterize two-track audio recordings (bass and drum instruments) based on musical attributes. These attributes are modelled using different regression algorithms. All regression models are trained based on score-based audio features computed from given scores and human annotations of the attributes. We compare five regression model configurations that predict values of different attributes. The regression models are trained based on manual annotations from 11 participants for a data-set of 70 double-track recordings. The average estimation errors within a cross-validation scenario are computed as evaluation measure. Models based on Partial Least Squares Regression (PLSR) with preceding Principal Component Analysis (PCA) and on Support Vector Regression (SVR) performed best.",DEU,facility,Developed economies,"[32.827984, -37.37453]","[-39.643307, -0.8122617]","[12.606085, -28.157423, -9.431101]","[6.990363, 18.08921, -15.390169]","[10.31097, 4.6559973]","[8.808803, 3.930248]","[10.499371, 12.652516, -1.9219612]","[10.356157, 6.944128, 10.221767]"
80,Yannis Panagakis;Constantine Kotropoulos;Gonzalo R. Arce,l1-Graph Based Music Structure Analysis.,2011,https://doi.org/10.5281/zenodo.1417335,Yannis Panagakis+Aristotle University of Thessaloniki>GRC>education;Constantine Kotropoulos+Aristotle University of Thessaloniki>GRC>education;Gonzalo R. Arce+University of Delaware>USA>education,"An unsupervised approach for automatic music structure analysis is proposed resorting to the following assumption: If the feature vectors extracted from a specific music segment are drawn from a single subspace, then the sequence of feature vectors extracted from a music recording will lie in a union of as many subspaces as the music segments in this recording are. It is well known that each feature vector stemming from a union of independent linear subspaces admits a sparse representation with respect to a dictionary formed by all other feature vectors with nonzero coefficients associated only to feature vectors that stem from its own subspace. Such sparse representation reveals the relationships among the feature vectors and it is used to construct a similarity graph, the so-called ℓ1-graph. Accordingly, the segmentation of audio features is obtained by applying spectral clustering to the ℓ1-graph. The performance of the just described approach is assessed by conducting experiments on the Pop-Music and the UPF Beatles benchmark datasets. Promising results are reported.",GRC,education,Developed economies,"[-3.1697426, 1.9625152]","[2.1167045, 0.09522795]","[-0.7641967, -5.2501845, -0.6352107]","[1.7403162, 1.5918623, -6.328417]","[11.940701, 8.383011]","[7.553166, 3.5471163]","[12.826768, 13.831182, -0.36927864]","[10.570666, 8.190189, 11.111243]"
33,Meinard Müller;Sebastian Ewert,Chroma Toolbox: Matlab Implementations for Extracting Variants of Chroma-Based Audio Features.,2011,https://doi.org/10.5281/zenodo.1416032,Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Sebastian Ewert+University of Bonn>DEU>education,"Chroma-based audio features, which closely correlate to the aspect of harmony, are a well-established tool in processing and analyzing music data. There are many ways of computing and enhancing chroma features, which results in a large number of chroma variants with different properties. In this paper, we present a chroma toolbox [13], which contains MATLAB implementations for extracting various types of recently proposed pitch-based and chroma-based audio features. Providing the MATLAB implementations on a well-documented website under a GNU-GPL license, our aim is to foster research in music information retrieval. As another goal, we want to raise awareness that there is no single chroma variant that works best in all applications. To this end, we discuss two example applications showing that the final music analysis result may crucially depend on the initial feature design step.",DEU,education,Developed economies,"[-11.644309, -19.955488]","[6.8185916, 26.171528]","[2.4453285, -6.5644526, -18.694302]","[15.711954, -1.4489253, -6.1693826]","[12.166706, 7.749939]","[9.917645, 2.4199498]","[12.931653, 13.657213, 0.49999446]","[11.2778425, 6.716855, 11.534667]"
34,Ching-Hua Chuan,A Comparison of Statistical and Rule-Based Models for Style-Specific Harmonization.,2011,https://doi.org/10.5281/zenodo.1417365,Ching-Hua Chuan+University of North Florida>USA>education,"The process of generating chords for harmonizing a melody with the goal of mimicking an artist’s style is investigated in this paper. We compared and tested three different approaches, including a rule-based model, a statistical model, and a hybrid system of the two, for such tasks. Experiments were conducted using songs from seven stylistically identifiable pop/rock bands, and the chords generated by the systems were compared to the ones in the artists’ original work. Evaluations were performed on multiple aspects, including calculating the average percentage of chords that were the same and those that were related, studying the manner in which the size of the training set affects the output harmonization, and examining a system’s behaviors in terms of the ability of generating unseen chords and the number of unique chords produced per song. We observed that the rule-based system performs comparably well while the result of the system with learning capability varies as the training set grows.",USA,education,Developed economies,"[12.615176, -3.094355]","[-0.46016827, -39.448444]","[-0.41712692, -3.7606273, 33.380215]","[-23.33868, 4.801798, 0.92429]","[10.34749, 9.522188]","[9.518694, 6.13162]","[11.598441, 14.90217, -0.75982535]","[9.869744, 5.1993318, 9.465658]"
36,Matthias Mauch;Hiromasa Fujihara;Kazuyoshi Yoshii;Masataka Goto,Timbre and Melody Features for the Recognition of Vocal Activity and Instrumental Solos in Polyphonic Music.,2011,https://doi.org/10.5281/zenodo.1415716,Matthias Mauch+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiromasa Fujihara+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We propose the task of detecting instrumental solos in polyphonic music recordings, and the usage of a set of four audio features for vocal and instrumental activity detection. Three of the features are based on the prior extraction of the predominant melody line, and have not been used in the context of vocal/instrumental activity detection. Using a support vector machine hidden Markov model we conduct 14 experiments to validate several combinations of our proposed features. Our results clearly demonstrate the benefit of combining the features: the best performance was always achieved by combining all four features. The top accuracy for vocal activity detection is 87.2%. The more difficult task of detecting instrumental solos equally benefits from the combination of all features and achieves an accuracy of 89.8% and a satisfactory precision of 61.1%. With this paper we also release to the public the 102 annotations we used for training and testing. The annotations offer not only vocal/non-vocal labels, but also distinguish between female and male singers, and different solo instruments.",JPN,facility,Developed economies,"[5.848984, -18.238722]","[8.906583, -22.823566]","[17.579319, -0.50965196, -5.983051]","[8.91258, -0.66705954, -17.451332]","[8.890141, 7.5270386]","[8.630122, 3.5865283]","[11.041428, 14.268303, -0.057571482]","[10.704624, 7.516885, 10.020305]"
37,Ferdinand Fuhrmann;Perfecto Herrera,Quantifying the Relevance of Locally Extracted Information for Musical Instrument Recognition from Entire Pieces of Music.,2011,https://doi.org/10.5281/zenodo.1416350,Ferdinand Fuhrmann+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"In this work we study the problem of automatic musical instrument recognition from entire pieces of music. In particular, we present and evaluate 4 different methods to select, from an unknown piece of music, relevant excerpts in terms of instrumentation, on top of which instrument recognition techniques are applied to infer the labels. Since the desired information is assumed to be redundant (we may extract just a few labels from a thousands of audio frames) we examine the recognition performance, the amount of data used for processing, and their possible correlation. Experimental results on a collection of Western music pieces reveal state-of-the-art performance in instrument recognition together with a great reduction of the required input data. However, we also observe a performance ceiling with the currently applied instrument recognition method.",ESP,education,Developed economies,"[9.702283, -22.573454]","[-9.716469, 1.614457]","[14.697398, -7.7181473, -1.3462101]","[9.087681, -2.1268914, -10.65103]","[8.90561, 7.101236]","[8.723074, 3.4481874]","[11.120795, 12.484626, 0.3240089]","[10.266255, 7.7003813, 10.393126]"
38,Sebastian Ewert;Meinard Müller,Score-Informed Voice Separation For Piano Recordings.,2011,https://doi.org/10.5281/zenodo.1417581,Sebastian Ewert+University of Bonn>DEU>education;Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility,"The decomposition of a monaural audio recording into musically meaningful sound sources or voices constitutes a fundamental problem in music information retrieval. In this paper, we consider the task of separating a monaural piano recording into two sound sources (or voices) that correspond to the left hand and the right hand. Since in this scenario the two sources share many physical properties, sound separation approaches identifying sources based on their spectral envelope are hardly applicable. Instead, we propose a score-informed approach, where explicit note events specified by the score are used to parameterize the spectrogram of a given piano recording. This parameterization then allows for constructing two spectrograms considering only the notes of the left hand and the right hand, respectively. Finally, inversion of the two spectrograms yields the separation result. First experiments show that our approach, which involves high-resolution music synchronization and parametric modeling techniques, yields good results for real-world non-synthetic piano recordings.",DEU,education,Developed economies,"[2.0869417, -44.820072]","[-39.838364, -25.738043]","[28.761497, 3.754615, -2.764491]","[-11.613344, -6.8298464, -32.151546]","[8.685924, 10.380106]","[6.5670657, 5.3025846]","[10.842291, 14.107306, 1.5477041]","[9.65876, 8.425916, 9.691955]"
39,Balaji Thoshkahna;Ramakrishnan R. Kalpathi,A Postprocessing Technique for Improved Harmonic/Percussion Separation for Polyphonic Music.,2011,https://doi.org/10.5281/zenodo.1417056,Balaji Thoshkahna+Indian Institute of Science>IND>education;K.R. Ramakrishnan+Indian Institute of Science>IND>education,"In this paper we propose a postprocessing technique for a spectrogram diffusion based harmonic/percussion decomposition algorithm. The proposed technique removes harmonic instrument leakages in the percussion enhanced outputs of the baseline algorithm. The technique uses median filtering and an adaptive detection of percussive segments in subbands followed by piecewise signal reconstruction using envelope properties to ensure that percussion is enhanced while harmonic leakages are suppressed. A new binary mask is created for the percussion signal which upon applying on the original signal improves harmonic versus percussion separation. We compare our algorithm with two recent techniques and show that on a database of polyphonic Indian music, the postprocessing algorithm improves the harmonic versus percussion decomposition significantly.",IND,education,Developing economies,"[9.51892, -42.562798]","[-40.076923, -24.723017]","[25.81557, -5.7678328, -4.769871]","[-14.051595, -7.4882793, -32.15036]","[8.493162, 9.7434225]","[6.5543976, 5.463437]","[11.005777, 13.367632, 1.329065]","[9.7242985, 8.4501295, 9.625244]"
40,Steven K. Tjoa;K. J. Ray Liu,Factorization of Overlapping Harmonic Sounds Using Approximate Matching Pursuit.,2011,https://doi.org/10.5281/zenodo.1414962,Steven K. Tjoa+Imagine Research>USA>company;K. J. Ray Liu+University of Maryland>USA>education,"Factorization of polyphonic musical signals remains a difficult problem due to the presence of overlapping harmonics. Existing dictionary learning methods cannot guarantee that the learned dictionary atoms are semantically meaningful. In this paper, we explore the factorization of harmonic musical signals when a fixed dictionary of harmonic sounds is already present. We propose a method called approximate matching pursuit (AMP) that can efficiently decompose harmonic sounds by using a known predetermined dictionary. We illustrate the effectiveness of AMP by decomposing polyphonic musical spectra with respect to a large dictionary of instrumental sounds. AMP executes faster than orthogonal matching pursuit yet performs comparably based upon recall and precision.",USA,company,Developed economies,"[11.957759, -42.535107]","[4.19326, -2.457339]","[26.180653, -7.3415294, -9.2962675]","[19.015432, 11.637599, -13.353615]","[8.942589, 9.436656]","[6.711152, 4.3664017]","[11.331296, 13.538052, 0.79099]","[10.355243, 8.58397, 10.460497]"
41,Gopala K. Koduri;Marius Miron;Joan Serrà;Xavier Serra,Computational Approaches for the Understanding of Melody in Carnatic Music.,2011,https://doi.org/10.5281/zenodo.1415238,Gopala K. Koduri+Universitat Pompeu Fabra>ESP>education;Marius Miron+Universitat Pompeu Fabra>ESP>education;Joan Serrà+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"The classical music traditions of the Indian subcontinent, Hindustani and Carnatic, offer an excellent ground on which to test the limitations of current music information research approaches. At the same time, studies based on these music traditions can shed light on how to solve new and complex music modeling problems. Both traditions have very distinct characteristics, specially compared with western ones: they have developed unique instruments, musical forms, performance practices, social uses and context. In this article, we focus on the Carnatic music tradition of south India, especially on its melodic characteristics. We overview the theoretical aspects that are relevant for music information research and discuss the scarce computational approaches developed so far. We put emphasis on the limitations of the current methodologies and we present open issues that have not yet been addressed and that we believe are important to be worked on.",ESP,education,Developed economies,"[7.1941423, -1.3784455]","[2.869678, -17.501957]","[8.527456, 1.6360148, -13.246625]","[14.63883, -13.51058, -7.9233565]","[11.02829, 10.130561]","[7.512454, 1.2076483]","[11.646785, 15.152929, -1.1528066]","[8.948956, 7.0985975, 12.580672]"
42,Sertan Sentürk;Parag Chordia,Modeling Melodic Improvisation in Turkish Folk Music Using Variable-Length Markov Models.,2011,https://doi.org/10.5281/zenodo.1415570,Sertan Şentürk+Georgia Tech Center for Music Technology>USA>education;Parag Chordia+Georgia Tech Center for Music Technology>USA>education,"The paper describes a new database, which currently consists of 64 songs encompassing approximately 6600 notes, and a system, which uses Variable-Length Markov Models (VLMM) to predict the melodies in the uzun hava (long tune) form, a melodic structure in Turkish folk music. The work shows VLMMs are highly predictive. This suggests that variable-length Markov models (VLMMs) may be applied to makam-based and non-metered musical forms, in addition to Western musical traditions. To the best of our knowledge, the work presents the first symbolic, machine readable database of uzun havas and the first application of predictive modeling in Turkish folk music.",USA,education,Developed economies,"[10.179897, -2.6268418]","[-4.237338, 22.953575]","[5.5304656, 10.4394045, 3.1745722]","[6.26028, -11.920892, -3.4225376]","[10.814421, 9.618696]","[7.8325834, 1.7633054]","[11.908838, 14.887487, -0.7125806]","[9.502493, 6.6280756, 11.699287]"
43,Sajjad Abdoli,Iranian Traditional Music Dastgah Classification.,2011,https://doi.org/10.5281/zenodo.1417425,Sajjad Abdoli+Islamic Azad University>IRN>education,"In this study, a system for Iranian traditional music Dastgah classification is presented. Persian music is based upon a set of seven major Dastgahs. The Dastgah in Persian music is similar to western musical scales and also Maqams in Turkish and Arabic music. Fuzzy logic type 2 as the basic part of our system has been used for modeling the uncertainty of tuning the scale steps of each Dastgah. The method assumes each performed note as a Fuzzy Set (FS), so each musical piece is a set of FSs. The maximum similarity between this set and theoretical data indicates the desirable Dastgah. In this study, a collection of small-sized dataset for Persian music is also given. The results indicate that the system works accurately on the dataset.",IRN,education,Developing economies,"[7.7235603, 5.1246843]","[-0.81379074, 12.729655]","[-11.333567, -20.290995, -16.366375]","[-2.916821, 12.238236, 5.7763376]","[11.402068, 10.342813]","[7.841535, 1.646033]","[12.1003475, 15.143935, -1.3020412]","[9.540747, 6.724227, 12.108611]"
44,Simon Dixon;Dan Tidhar;Emmanouil Benetos,"The Temperament Police: The Truth, the Ground Truth, and Nothing but the Truth.",2011,https://doi.org/10.5281/zenodo.1418197,"Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education;Dan Tidhar+AHRC Research Centre for Musical Performance as Creative Practice, King's College London>GBR>education;Emmanouil Benetos+Centre for Digital Music, Queen Mary University of London>GBR>education","The tuning system of a keyboard instrument is chosen so that frequently used musical intervals sound as consonant as possible. Temperament refers to the compromise arising from the fact that not all intervals can be maximally consonant simultaneously. Recent work showed that it is possible to estimate temperament from audio recordings with no prior knowledge of the musical score, using a conservative (high precision, low recall) automatic transcription algorithm followed by frequency estimation using quadratic interpolation and bias correction from the log magnitude spectrum. In this paper we develop a harpsichord-specific transcription system to analyse over 500 recordings of solo harpsichord music for which the temperament is specified on the CD sleeve notes. We compare the measured temperaments with the annotations and discuss the differences between temperament as a theoretical construct and as a practical issue for professional performers and tuners. The implications are that ground truth is not always scientific truth, and that content-based analysis has an important role in the study of historical performance practice.",GBR,education,Developed economies,"[-51.74462, 10.938158]","[-6.371367, -16.510595]","[-19.787092, 24.88019, -3.2095382]","[-3.2766774, -8.88893, -7.754219]","[13.545764, 12.271959]","[6.9997687, 2.2740657]","[16.063381, 14.884308, 1.2758594]","[9.062574, 7.1572623, 11.147228]"
45,Frédéric Bimbot;Emmanuel Deruty;Gabriel Sargent;Emmanuel Vincent,Methodology and Resources for The Structural Segmentation of Music Pieces into Autonomous and Comparable Blocks.,2011,https://doi.org/10.5281/zenodo.1417611,"Frédéric Bimbot+IRISA, CNRS - UMR 6074>FRA>facility;Emmanuel Deruty+INRIA, Rennes Bretagne Atlantique>FRA>facility;Gabriel Sargent+Université de Rennes 1>FRA>education;Emmanuel Vincent+INRIA, Rennes Bretagne Atlantique>FRA>facility","The approach called decomposition into autonomous and comparable blocks specifies a methodology for producing music structure annotation by human listeners based on a set of criteria relying on the listening experience of the human annotator [12]. The present article develops further a number of fundamental notions and practical issues, so as to facilitate the usability and the reproducibility of the approach. We formalize the general methodology as an iterative process which aims at estimating both a structural metric pattern and its realization, by searching empirically for an optimal compromise describing the organization of the content of the music piece in the most economical way, around a typical time-scale. Based on experimental observations, we detail some practical considerations and we illustrate the method by an extensive case study. We introduce a set of 500 songs for which we are releasing freely the structural annotations to the research community, for examination, discussion and utilization.",FRA,facility,Developed economies,"[-1.0497093, -3.4013743]","[-2.3242176, 8.692238]","[4.340378, -7.5411015, -2.4561124]","[-3.2039683, 0.95641047, 3.8404043]","[11.666564, 8.261725]","[8.718839, 2.3975284]","[12.371764, 14.133146, 0.078249544]","[10.422988, 6.648652, 11.512229]"
47,Zoltán Juhász,Low Dimensional Visualization of Folk Music Systems Using the Self Organizing Cloud.,2011,https://doi.org/10.5281/zenodo.1417433,Zoltán Juhász+Research Institute for Technical Physics and Materials Sciences>HUN>facility,"We describe a computational method derived from self organizing mapping and multidimensional scaling algorithms for automatic classification and visual clustering of large vector databases. Testing the method on a large corpus of folksongs we have found that the performance of the classification and topological clustering was significantly improved compared to current techniques. Applying the method to an analysis of the connections of 31 Eurasian and North-American folk music cultures, a clearly interpretable system of musical connections was revealed. The results show the relevance of the musical language groups in the oral tradition of the humanity.",HUN,facility,Developed economies,"[-14.237915, 30.716805]","[24.74284, 2.8700418]","[-9.3927765, -2.695467, -2.7489645]","[16.923697, -9.755339, 23.195911]","[12.552385, 8.705426]","[11.055539, 1.9994993]","[13.353135, 14.3418665, -0.6512899]","[12.243307, 6.0323124, 13.270896]"
48,Christopher Raphael;Jingya Wang,New Approaches to Optical Music Recognition.,2011,https://doi.org/10.5281/zenodo.1414856,Christopher Raphael+Indiana University>USA>education|Indiana University>USA>education;Jingya Wang+Indiana University>USA>education|Indiana University>USA>education,"We present the beginnings of a new system for optical music recognition (OMR), aimed toward the score images of the International Music Score Library Project (IMSLP). Our system focuses on measures as the basic unit of recognition. We identify candidate composite symbols (chords and beamed groups) using grammatically-formulated top-down model-based methods, while employing template matching to find isolated rigid symbols. We reconcile these overlapping symbols by seeking non-overlapping variants of the composite symbols that best account for the pixel data. We present results on a representative score from the IMSLP.",USA,education,Developed economies,"[38.80902, 21.208769]","[-21.534725, 34.80647]","[19.481634, 13.095124, 9.336327]","[-9.156233, -19.719694, -0.93510395]","[8.663313, 6.1869016]","[6.6970224, -0.44594932]","[10.700645, 11.1382, -0.1042415]","[8.022219, 4.3525405, 10.683657]"
49,Masataka Goto;Kazuyoshi Yoshii;Hiromasa Fujihara;Matthias Mauch;Tomoyasu Nakano,Songle: A Web Service for Active Music Listening Improved by User Contributions.,2011,https://doi.org/10.5281/zenodo.1416256,Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiromasa Fujihara+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Matthias Mauch+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper describes a public web service for active music listening, Songle, that enriches music listening experiences by using music-understanding technologies based on signal processing. Although various research-level interfaces and technologies have been developed, it has not been easy to get people to use them in everyday life. Songle serves as a showcase to demonstrate how people can benefit from music-understanding technologies by enabling people to experience active music listening interfaces on the web. Songle facilitates deeper understanding of music by visualizing music scene descriptions estimated automatically, such as music structure, hierarchical beat structure, melody line, and chords. When using music-understanding technologies, however, estimation errors are inevitable. Songle therefore features an efficient error correction interface that encourages people to contribute by correcting those errors to improve the web service. We also propose a mechanism of collaborative training for music-understanding technologies, in which corrected errors will be used to improve the music-understanding performance through machine learning techniques. We hope Songle will serve as a research platform where other researchers can exhibit results of their music-understanding technologies to jointly promote the popularization of the field of music information research.",JPN,facility,Developed economies,"[-31.545427, 29.252275]","[-4.56635, 19.57406]","[-21.748512, 12.962893, -16.222124]","[-9.059915, 16.232399, 8.735291]","[14.662575, 7.6776843]","[10.75226, 1.9122318]","[14.705838, 14.40532, -2.1372716]","[10.836979, 5.6022677, 10.959124]"
50,Mark Levy,Improving Perceptual Tempo Estimation with Crowd-Sourced Annotations.,2011,https://doi.org/10.5281/zenodo.1417583,Mark Levy+Last.fm Ltd.>GBR>company,"We report the design and results of a web-based experiment intended to support the development and evaluation of tempo estimation algorithms, in which users tap to music and select descriptive labels. Analysis of the tapping data and labels chosen shows that, while different listeners frequently entrain to different metrical levels for some pieces, they rarely disagree about which pieces are fast and which are slow. We show how this result can be used to improve both the evaluation metrics used for automatic tempo estimation and the estimation algorithms themselves. We also report the relative performance of two recent tempo estimation methods according to a further controlled experiment that does not depend on groundtruth values of any kind.",GBR,company,Developed economies,"[41.473373, -26.5053]","[-31.471325, -3.2852151]","[-0.24910443, -31.663326, 0.953297]","[-9.811191, 11.512869, -8.036425]","[11.515413, 4.4716864]","[5.167784, 1.722814]","[10.901997, 13.374998, -2.8562412]","[7.3904214, 6.7026496, 11.114374]"
51,Markus Schedl;Peter Knees;Sebastian Böck,Investigating the Similarity Space of Music Artists on the Micro-Blogosphere.,2011,https://doi.org/10.5281/zenodo.1414808,Markus Schedl+Johannes Kepler University Linz>AUT>education;Peter Knees+Johannes Kepler University Linz>AUT>education;Sebastian Böck+Johannes Kepler University Linz>AUT>education,"Microblogging services such as Twitter have become an important means to share information. In this paper, we thoroughly analyze their potential for a key challenge in the field of MIR, namely the elaboration of perceptually meaningful similarity measures. To this end, comprehensive evaluation experiments were conducted using Twitter posts gathered during a period of several months. We investigated 23,100 combinations of different term weighting strategies, normalization methods, index term sets, Twitter query schemes, and similarity measurement techniques, aiming at determining in which way they influence the similarity estimates’ quality. Evaluation was performed on the task of similar artist retrieval. Two data sets were used: one of 224 well-known artists with a uniform genre distribution, the other constituting a collection of 3,000 artists extracted from last.fm and allmusic.com.",AUT,education,Developed economies,"[-40.765648, 8.981947]","[50.71716, 11.713289]","[-23.855656, 11.963386, 5.1424932]","[22.717123, 12.373307, 13.530279]","[14.231924, 9.89089]","[11.88003, 2.4729738]","[15.073158, 14.727566, -0.17849493]","[13.209331, 5.9468985, 12.12673]"
52,Nicholas J. Bryan;Ge Wang 0002,Musical Influence Network Analysis and Rank of Sample-Based Music.,2011,https://doi.org/10.5281/zenodo.1415768,Nicholas J. Bryan+Stanford University>USA>education;Ge Wang+Stanford University>USA>education,"Computational analysis of musical influence networks and rank of sample-based music is presented with a unique outside examination of the WhoSampled.com dataset. The exemplary dataset maintains a large collection of artist-to-artist relationships of sample-based music, specifying the origins of borrowed or sampled material on a song-by-song basis. Directed song, artist, and musical genre networks are created from the data, allowing the application of social network metrics to quantify various trends and characteristics. In addition, a method of influence rank is proposed, unifying song-level networks to higher-level artist and genre networks via a collapse-and-sum approach. Such metrics are used to help interpret and describe interesting patterns of musical influence in sample-based music suitable for musicological analysis. Empirical results and visualizations are also presented, suggesting that sampled-based influence networks follow a power-law degree distribution; heavy influence of funk, soul, and disco music on modern hip-hop, R&B, and electronic music; and other musicological results.",USA,education,Developed economies,"[-39.758854, 13.781412]","[46.738834, 9.076876]","[-29.261152, 8.239531, 2.355707]","[12.487306, 12.937181, 9.523992]","[14.596169, 9.392471]","[12.240348, 2.7048068]","[14.919579, 14.729378, -0.7435428]","[13.231474, 5.4945183, 11.697984]"
53,David M. Weigl;Catherine Guastavino,User studies in the Music Information Retrieval Literature.,2011,https://doi.org/10.5281/zenodo.1417811,David M. Weigl+McGill University>CAN>education;Catherine Guastavino+McGill University>CAN>education,"This paper presents an overview of user studies in the Music Information Retrieval (MIR) literature. A focus on the user has repeatedly been identified as a key requirement for future MIR research; yet empirical user studies have been relatively sparse in the literature, the overwhelming research attention in MIR remaining systems-focused. We present research topics, methodologies, and design implications covered in the user studies conducted thus far.",CAN,education,Developed economies,"[-23.41392, 22.662926]","[28.756119, 32.50425]","[-15.697463, 8.459298, -9.249005]","[4.7203355, 5.5192623, 17.545282]","[14.475394, 8.059356]","[11.863222, 0.79215264]","[14.525798, 14.807928, -1.9718332]","[12.37243, 4.4494333, 12.22922]"
54,Audrey Laplante,Social Capital and Music Discovery: An Examination of the Ties through Which Late Adolescents Discover New Music.,2011,https://doi.org/10.5281/zenodo.1417253,Audrey Laplante+Université de Montréal>CAN>education,"Research on everyday life information seeking has demonstrated that people often relied on other people to obtain the information they need. Weak ties (i.e., acquaintances) were found to be particularly instrumental to get new information. This study employed social network analysis to examine the characteristics of the ties through which late adolescents (15-17 years old) discover new music. In-depth interviews with 19 adolescents were conducted, which generated a sample of 334 ties. A statistical analysis of the ties showed that these adolescents relied mostly on strong ties to expand their music repertoire, that is, on people to which they felt very close and with whom they had frequent contacts. These ties were predominantly homophilous in terms of age, gender and musical taste. It was also found that parents were more likely than friends or other types of kins to be instrumental for music discovery. These findings suggest that a better knowledge of the characteristics of the ties through which people discover new music could provide useful insights for the design of recommender systems that include social networking features.",CAN,education,Developed economies,"[-36.595146, 23.139753]","[39.89984, 32.18129]","[-19.922392, 19.406359, -6.28985]","[9.358946, 13.592421, 20.805664]","[15.240434, 8.79311]","[12.953614, 1.0299212]","[15.17432, 15.155378, -1.4116566]","[13.361463, 4.2702055, 12.074702]"
55,Dan Stowell;Simon Dixon,MIR in School? Lessons from Ethnographic Observation of Secondary School Music Classes.,2011,https://doi.org/10.5281/zenodo.1416980,Dan Stowell+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"To help maximise the usefulness of MIR technologies in the wider community, we conducted an ethnographic study of music lessons in secondary schools in London, UK. The purpose is to understand better how musical concepts are negotiated with and without technology, so we can understand when and how MIR tools might be useful. We report on some of the themes uncovered, both about the range of technologies deployed in schools and about the ways different musical concepts are discussed. Importantly, this rich observation elicits some of the nuances between various high- and low-technologies. In particular, we discuss issues of multimodality and the role of technologies such as Youtube, as well as specific issues around musical concepts such as genre and rhythm.",GBR,education,Developed economies,"[-15.645498, 51.939644]","[30.796965, 38.165543]","[-35.434628, 0.96816874, 6.112314]","[0.30742058, 11.438649, 21.805334]","[13.331256, 5.5659237]","[11.837491, 0.37820974]","[14.635317, 11.94235, -1.3608425]","[12.125366, 4.2529154, 11.736313]"
57,Vladimir Viro,Peachnote: Music Score Search and Analysis Platform.,2011,https://doi.org/10.5281/zenodo.1417261,Vladimir Viro+Ludwig-Maximilians-University Munich>DEU>education,"Hundreds of thousands of music scores are being digitized by libraries all over the world. In contrast to books, they generally remain inaccessible for content-based retrieval and algorithmic analysis. There is no analogue to Google Books for music scores, and there exist no large corpora of symbolic music data that would empower musicology in the way large text corpora are empowering computational linguistics, sociology, history, and other humanities that have printed word as their major source of evidence about their research subjects. We want to help change that. In this paper we present the first result of our work in this direction - the Music Ngram Viewer and search engine, an analog of Google Books Ngram Viewer and Google Books search for music scores.",DEU,education,Developed economies,"[-20.537592, 15.19996]","[17.425997, 20.11467]","[-13.515432, 0.9487553, -10.457932]","[5.747898, -6.9771776, 15.878265]","[13.564074, 7.659881]","[10.123841, 0.82402766]","[14.094706, 14.026675, -1.7009196]","[11.298406, 5.567625, 12.765775]"
81,Peter Foster;Anssi Klapuri;Mark D. Plumbley,Causal Prediction of Continuous-Valued Music Features.,2011,https://doi.org/10.5281/zenodo.1418337,Peter Foster+Queen Mary University of London>GBR>education;Anssi Klapuri+Queen Mary University of London>GBR>education;Mark D. Plumbley+Queen Mary University of London>GBR>education,"""This paper investigates techniques for predicting sequences of continuous-valued feature vectors extracted from musical audio. In particular, we consider prediction of beat-synchronous Mel-frequency cepstral coefficients and chroma features in a causal setting, where features are predicted as they unfold in time. The methods studied comprise autoregressive models, N-gram models incorporating a smoothing scheme, and a novel technique based on repetition detection using a self-distance matrix. Furthermore, we propose a method for combining predictors, which relies on a running estimate of the error variance of the predictors to inform a linear weighting of the predictor outputs. Results indicate that incorporating information on long-term structure improves the prediction performance for continuous-valued, sequential musical data. For the Beatles data set, combining the proposed self-distance based predictor with both N-gram and autoregressive methods results in an average of 13% improvement compared to a linear predictive baseline.""",GBR,education,Developed economies,"[-11.07748, -1.4780738]","[-39.3231, -0.88457155]","[-3.991731, 0.15556212, 9.778858]","[7.706356, 17.530304, -14.622683]","[11.908887, 8.7384615]","[9.090831, 4.019939]","[13.245854, 13.568589, -0.069392815]","[10.485493, 6.923628, 10.203098]"
107,Joakim Andén;Stéphane Mallat,Multiscale Scattering for Audio Classification.,2011,https://doi.org/10.5281/zenodo.1415750,Joakim Andén+Ecole Polytechnique>FRA>education|CMAP>FRA>facility;Stéphane Mallat+Ecole Polytechnique>FRA>education|CMAP>FRA>facility,"Mel-frequency cepstral coefficients (MFCCs) are efficient audio descriptors providing spectral energy measurements over short time windows of length 23 ms. These measurements, however, lose non-stationary spectral information such as transients or time-varying structures. It is shown that this information can be recovered as spectral co-occurrence coefficients. Scattering operators compute these coefficients with a cascade of wavelet filter banks and modulus rectifiers. The signal can be reconstructed from scattering coefficients by inverting these wavelet modulus operators. An application to genre classification shows that second-order co-occurrence coefficients improve results obtained by MFCC and Delta-MFCC descriptors.",FRA,education,Developed economies,"[-20.55145, -14.569008]","[19.443422, -16.968922]","[-9.255491, -2.1088142, 18.640558]","[18.324549, 1.4277418, -8.201744]","[12.2848425, 10.262758]","[9.589045, 3.1133337]","[13.442344, 13.864733, 1.0052345]","[11.266304, 7.398219, 10.8087845]"
83,Zhiyao Duan;Bryan Pardo,Aligning Semi-Improvised Music Audio with Its Lead Sheet.,2011,https://doi.org/10.5281/zenodo.1417325,Zhiyao Duan+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"Existing audio-score alignment methods assume that the audio performance is faithful to a fully-notated MIDI score. For semi-improvised music (e.g. jazz), this assumption is strongly violated. In this paper, we address the problem of aligning semi-improvised music audio with a lead sheet. Our approach does not require prior training on performances of the lead sheet to be aligned. We start by analyzing the problem and propose to represent the lead sheet as a MIDI file together with a structural information file. Then we propose a dynamic-programming-based system to align the chromagram representations of the audio performance and the MIDI score. Techniques are proposed to address the chromagram scaling, key transposition and structural change (e.g. a performer unexpectedly repeats a section) problems. We test our system on 3 jazz lead sheets. For each sheet we align a set of solo piano performances and a set of full-band commercial recordings with different instrumentation and styles. Results show that our system achieves promising results on some highly improvised music.",USA,education,Developed economies,"[19.457008, -12.6300125]","[-14.553517, -14.544961]","[1.7205896, -9.849461, -8.6477995]","[-3.7299643, -19.959871, -3.206495]","[10.856303, 6.4811997]","[6.2868686, 0.6439471]","[11.968855, 12.53096, -1.5448649]","[8.204389, 5.716353, 10.845163]"
112,Hélène Papadopoulos;Matthieu Kowalski,Sparse Signal Decomposition on Hybrid Dictionaries Using Musical Priors.,2011,https://doi.org/10.5281/zenodo.1416264,"Hélène Papadopoulos+Laboratoire des Signaux et Systèmes>FRA>facility|UMR 8506, CNRS-SUPELEC-Univ Paris-Sud>FRA>facility;Matthieu Kowalski+Laboratoire des Signaux et Systèmes>FRA>facility|UMR 8506, CNRS-SUPELEC-Univ Paris-Sud>FRA>facility","""This paper investigates the use of musical priors for sparse expansion of audio signals of music on overcomplete dictionaries taken from the union of two orthonormal bases. More specifically, chord information is used to build a structured model that takes into account dependencies between coefficients of the decomposition. Evaluation on various music signals shows that our approach provides results whose quality measured by the signal-to-noise ratio corresponds to state-of-the-art approaches, and shows that our model is relevant to represent audio signals of Western tonal music and opens new perspectives.""",FRA,facility,Developed economies,"[3.847614, -26.2903]","[4.027927, -2.2548263]","[11.900745, 15.029758, 17.510548]","[17.350523, 11.498829, -13.2803135]","[11.107041, 8.059013]","[6.967158, 4.163474]","[12.492083, 13.255477, 0.11159168]","[10.551389, 8.438325, 10.700164]"
113,Yoko Anan;Kohei Hatano;Hideo Bannai;Masayuki Takeda,Music Genre Classification using Similarity Functions.,2011,https://doi.org/10.5281/zenodo.1418019,Yoko Anan+Kyushu University>JPN>education;Kohei Hatano+Kyushu University>JPN>education;Hideo Bannai+Kyushu University>JPN>education;Masayuki Takeda+Kyushu University>JPN>education,"We consider music classification problems. A typical machine learning approach is to use support vector machines with some kernels. This approach, however, does not seem to be successful enough for classifying music data in our experiments. In this paper, we follow an alternative approach. We employ a (dis)similarity-based learning framework proposed by Wang et al. This (dis)similarity-based approach has a theoretical guarantee that one can obtain accurate classifiers using (dis)similarity measures under a natural assumption. We demonstrate the effectiveness of our approach in computational experiments using Japanese MIDI data.",JPN,education,Developed economies,"[-29.175255, -12.745559]","[15.450836, -7.3050656]","[-15.983192, 4.942486, 15.362948]","[13.471452, 8.703895, -7.90644]","[13.020013, 10.794052]","[9.658553, 3.27979]","[14.008203, 14.303672, 1.2804139]","[11.381155, 7.3331523, 10.955115]"
114,Caio Miguel Marques;Ivan Rizzo Guilherme;Rodrigo Y. M. Nakamura;João P. Papa,New Trends in Musical Genre Classification Using Optimum-Path Forest.,2011,https://doi.org/10.5281/zenodo.1417657,C. Marques+UNESP - Univ Estadual Paulista>BRA>education;I. R. Guilherme+UNESP - Univ Estadual Paulista>BRA>education;R. Y. M. Nakamura+UNESP - Univ Estadual Paulista>BRA>education;J. P. Papa+UNESP - Univ Estadual Paulista>BRA>education,"""Musical genre classification has been paramount in the last years, mainly in large multimedia datasets, in which new songs and genres can be added at every moment by anyone. In this context, we have seen the growing of musical recommendation systems, which can improve the benefits for several applications, such as social networks and collective musical libraries. In this work, we have introduced a recent machine learning technique named Optimum-Path Forest (OPF) for musical genre classification, which has been demonstrated to be similar to the state-of-the-art pattern recognition techniques, but much faster for some applications. Experiments in two public datasets were conducted against Support Vector Machines and a Bayesian classifier to show the validity of our work. In addition, we have executed an experiment using very recent hybrid feature selection techniques based on OPF to speed up feature extraction process.""",BRA,education,Developing economies,"[-29.979187, -15.007768]","[19.489092, -7.3902946]","[-16.934433, 1.8374112, 12.607712]","[14.025431, 8.937297, -5.0175476]","[12.9174795, 10.83171]","[9.68211, 3.298755]","[13.8641405, 14.215926, 1.3613281]","[11.4873085, 7.1338882, 10.869192]"
115,Emanuele Coviello;Riccardo Miotto;Gert R. G. Lanckriet,Combining Content-Based Auto-Taggers with Decision-Fusion.,2011,https://doi.org/10.5281/zenodo.1415666,"Emanuele Coviello+University of California, San Diego>USA>education;Riccardo Miotto+University of Padova>ITA>education;Gert R. G. Lanckriet+University of California, San Diego>USA>education","To automatically annotate songs with descriptive keywords, a variety of content-based auto-tagging strategies have been proposed in recent years. Different approaches may capture different aspects of a song’s musical content, such as timbre, temporal dynamics, rhythmic qualities, etc. As a result, some auto-taggers may be better suited to model the acoustic characteristics commonly associated with one set of tags, while being less predictive for other tags. This paper proposes decision-fusion, a principled approach to combining the predictions of a diverse collection of content-based auto-taggers that focus on various aspects of the musical signal. By modeling the correlations between tag predictions of different auto-taggers, decision-fusion leverages the benefits of each of the original auto-taggers, and achieves superior annotation and retrieval performance.",USA,education,Developed economies,"[-44.389046, -3.5974414]","[38.9804, -1.3250937]","[-15.095911, 19.204168, 13.176064]","[24.274075, 7.27156, 2.4269395]","[14.489238, 10.616812]","[11.4639225, 3.4708538]","[15.631409, 14.048086, 0.13019978]","[12.981738, 6.4385333, 11.189106]"
116,Bo Xie 0002;Wei Bian;Dacheng Tao;Parag Chordia,Music Tagging with Regularized Logistic Regression.,2011,https://doi.org/10.5281/zenodo.1415258,Bo Xie+Georgia Tech>USA>education|GTCMT>USA>Unknown;Wei Bian+University of Technology Sydney>AUS>education|QCIS>Unknown>Unknown;Dacheng Tao+University of Technology Sydney>AUS>education|QCIS>Unknown>Unknown;Parag Chordia+Georgia Tech>USA>education|GTCMT>USA>Unknown,"""In this paper, we present a set of simple and efficient regularized logistic regression algorithms to predict tags of music. We first vector-quantize the delta MFCC features using k-means and construct “bag-of-words” representation for each song. We then learn the parameters of these logistic regression algorithms from the “bag-of-words” vectors and ground truth labels in the training set. At test time, the prediction confidence by the linear classifiers can be used to rank the songs for music annotation and retrieval tasks. Thanks to the convex property of the objective functions, we adopt an efficient and scalable generalized gradient method to learn the parameters, with global optimum guaranteed. And we show that these efficient algorithms achieve state-of-the-art performance in annotation and retrieval tasks evaluated on CAL-500.""",USA,education,Developed economies,"[-41.460438, -1.731489]","[37.76133, -2.4489388]","[-13.73925, 14.206449, 8.829303]","[22.777367, 7.7987733, 0.025445402]","[14.497894, 10.589867]","[11.328847, 3.5238593]","[15.583204, 14.132017, 0.094395205]","[12.878996, 6.4605036, 11.229381]"
117,Chris Sanden;John Z. Zhang,An Empirical Study of Multi-Label Classifiers for Music Tag Annotation.,2011,https://doi.org/10.5281/zenodo.1416432,Chris Sanden+University of Lethbridge>CAN>education;John Z. Zhang+University of Lethbridge>CAN>education,"""In this paper we study the problem of automatic music tag annotation. Treating tag annotation as a computational classification process, we attempt to explore the relationship between acoustic features and music tags. Toward this end, we conduct a series of empirical experiments to evaluate a set of multi-label classifiers and demonstrate which ones are more suitable for music tag annotation. Furthermore, we discuss various factors in the classification process, such as feature sets, frame sizes, etc. Experiments on two publicly available datasets show that the Calibrated Label Ranking (CLR) algorithm outperforms the other classifiers for a selection of evaluation measures.""",CAN,education,Developed economies,"[-38.286034, -1.2388144]","[38.970047, -1.9690919]","[-15.588888, 10.349693, 10.2852125]","[24.571083, 8.5805025, 1.4643265]","[14.208582, 10.425839]","[11.441907, 3.45865]","[15.315581, 14.094205, 0.08529519]","[12.963683, 6.440878, 11.138635]"
118,Katherine Ellis;Emanuele Coviello;Gert R. G. Lanckriet,Semantic Annotation and Retrieval of Music using a Bag of Systems Representation.,2011,https://doi.org/10.5281/zenodo.1416390,Katherine Ellis+University of California>USA>education|University of California>USA>education|University of California>USA>education;Emanuele Coviello+University of California>USA>education|University of California>USA>education|University of California>USA>education;Gert R.G. Lanckriet+University of California>USA>education|University of California>USA>education|University of California>USA>education,"We present a content-based auto-tagger that leverages a rich dictionary of musical codewords, where each codeword is a generative model that captures timbral and temporal characteristics of music. This leads to a higher-level, concise “Bag of Systems” (BoS) representation of the characteristics of a musical piece. Once songs are represented as a BoS histogram over codewords, traditional algorithms for text document retrieval can be leveraged for music auto-tagging. Compared to estimating a single generative model to directly capture the musical characteristics of songs associated with a tag, the BoS approach offers the flexibility to combine different classes of generative models at various time resolutions through the selection of the BoS codewords. Experiments show that this enriches the audio representation and leads to superior auto-tagging performance.",USA,education,Developed economies,"[-31.357332, 10.142697]","[39.920116, -3.4091582]","[-15.012819, 6.428051, 1.0936903]","[25.40575, 5.483747, 1.4892188]","[13.784417, 9.183378]","[11.452609, 3.508864]","[14.605566, 13.950246, -0.6817901]","[13.018301, 6.4359126, 11.199364]"
119,Philippe Hamel;Simon Lemieux;Yoshua Bengio;Douglas Eck,Temporal Pooling and Multiscale Learning for Automatic Annotation and Ranking of Music Audio.,2011,https://doi.org/10.5281/zenodo.1418237,Philippe Hamel+Université de Montréal>CAN>education;Simon Lemieux+Université de Montréal>CAN>education;Yoshua Bengio+Université de Montréal>CAN>education;Douglas Eck+Google Inc.>USA>company,"This paper analyzes some of the challenges in performing automatic annotation and ranking of music audio, and proposes a few improvements. First, we motivate the use of principal component analysis on the mel-scaled spectrum. Secondly, we present an analysis of the impact of the selection of pooling functions for summarization of the features over time. We show that combining several pooling functions improves the performance of the system. Finally, we introduce the idea of multiscale learning. By incorporating these ideas in our model, we obtained state-of-the-art performance on the Magnatagatune dataset.",CAN,education,Developed economies,"[-17.96805, -7.43773]","[27.307499, -7.806356]","[-7.8481045, 9.832867, 9.494087]","[20.051363, 1.8330503, -0.9872776]","[12.52814, 9.234766]","[10.271939, 3.540003]","[13.929484, 13.708183, 0.012841625]","[11.996321, 6.8807945, 10.963025]"
120,Mark Mann;Trevor J. Cox;Francis F. Li,Music Mood Classification of Television Theme Tunes.,2011,https://doi.org/10.5281/zenodo.1415884,M Mann+BBC R&D>GBR>company;T J Cox+University of Salford>GBR>education;F F Li+University of Salford>GBR>education,"This paper introduces methods used for Music Mood Classification to assist in the automated tagging of television programme theme tunes for the first time. The methods employed use a knowledge driven approach with tailored parameters extractable from the Matlab MIR Toolbox. Four new features were developed, three based on tonality and one on tempo, to enable a degree of quantified tagging, using support vector machines, employing various kernels, optimised along six mood axes. Using a “nearest neighbour” method of optimisation, a success rate in the range of 80-94% was achieved in being able to classify musical audio on a five point mood scale.",GBR,company,Developed economies,"[-54.041664, 3.8448005]","[51.559414, -4.1196704]","[-17.29129, 26.268194, 4.8081985]","[15.059578, 22.919453, 6.999986]","[13.509197, 12.55325]","[13.127456, 3.771676]","[16.144056, 14.886482, 1.4706923]","[14.231432, 5.180619, 10.697813]"
121,Sam Davies;Penelope Allen;Mark Mann;Trevor J. Cox,Musical Moods: A Mass Participation Experiment for Affective Classification of Music.,2011,https://doi.org/10.5281/zenodo.1415920,Sam Davies+BBC Research & Development>GBR>company;Penelope Allen+BBC Research & Development>GBR>company;Mark Mann+BBC Research & Development>GBR>company;Trevor Cox+University of Salford>GBR>education,"In this paper we present our mass participation experiment, Musical Moods. This experiment placed 144 theme tunes online, taken from TV and radio programmes from the last 60 years of the British Broadcasting Corporations (BBC) output. Members of the public were then invited to audition then rate these according to a set of semantic differentials based on the affective categories of evaluation, potency and activity. Participants were also asked to rate their familiarity of the theme tune and how much they liked the theme tune. A final question asked participants to identify the genre of the TV programme with which they associated the tune. The purpose of this is to aid in the affective classification of large-scale TV archives, such as those possessed by the BBC. We find correlations between evaluation and potency, potency and activity but none between activity and evaluation but no clear correlation between affect and genre. This paper presents our key findings from an analysis of the results along with our plans for further analysis. The initial results from this experiment are based on an analyses of over 51,000 answers from over 13,000 participants.",GBR,company,Developed economies,"[-54.541084, 4.909666]","[56.175537, -2.568493]","[-19.186043, 26.225037, 2.8432884]","[10.528187, 18.51151, 10.955631]","[13.5411, 12.477328]","[13.189334, 3.4945118]","[16.123077, 14.865553, 1.3806341]","[14.165548, 5.0152526, 10.874161]"
111,Mikael Henaff;Kevin Jarrett;Koray Kavukcuoglu;Yann LeCun,Unsupervised Learning of Sparse Features for Scalable Audio Classification.,2011,https://doi.org/10.5281/zenodo.1416086,Mikael Henaff+New York University>USA>education|Courant Institute of Mathematical Sciences>USA>education;Kevin Jarrett+New York University>USA>education|Courant Institute of Mathematical Sciences>USA>education;Koray Kavukcuoglu+New York University>USA>education|Courant Institute of Mathematical Sciences>USA>education;Yann LeCun+New York University>USA>education|Courant Institute of Mathematical Sciences>USA>education,"In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4% accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets.",USA,education,Developed economies,"[-19.24959, -15.561707]","[21.219597, -13.743319]","[-10.891673, -2.9853814, 15.991979]","[20.032076, 6.388431, -7.9739]","[12.232209, 10.174679]","[9.9083185, 3.9030724]","[13.415769, 13.777175, 0.9793937]","[11.527093, 6.9045177, 10.330004]"
122,Yonatan Vaizman;Roni Y. Granot;Gert R. G. Lanckriet,Modeling Dynamic Patterns for Emotional Content in Music.,2011,https://doi.org/10.5281/zenodo.1416580,Yonatan Vaizman+Hebrew University>ISR>education;Roni Y. Granot+Hebrew University>ISR>education;Gert Lanckriet+University of California San Diego>USA>education,"Emotional content is a major component in music. It has long been a research topic of interest to discover the acoustic patterns in the music that carry that emotional information, and enable performers to communicate emotional messages to listeners. Previous works looked in the audio signal for local cues, most of which assume monophonic music, and their statistics over time. Here, we used generic audio features, that can be calculated for any audio signal, and focused on the progression of these features through time, investigating how informative the dynamics of the audio is for emotional content. Our data is comprised of piano and vocal improvisations of musically trained performers, instructed to convey 4 categorical emotions. We applied Dynamic Texture Mixture (DTM), that models both the instantaneous sound qualities and their dynamics, and demonstrated the strength of the model. We further showed that once taking the dynamics into account even highly reduced versions of the generic audio features carry a substantial amount of information about the emotional content. Finally, we demonstrate how interpreting the parameters of the trained models can yield interesting cognitive suggestions.",ISR,education,Developing economies,"[-60.05463, 1.6784626]","[50.60962, -11.8897705]","[-27.375448, 23.914091, 1.5617647]","[6.8867373, 22.850708, 3.8785918]","[13.964852, 12.889681]","[12.873794, 4.352733]","[16.170887, 14.449908, 1.835874]","[13.966577, 4.832265, 10.265269]"
124,Björn W. Schuller;Felix Weninger;Johannes Dorfner,Multi-Modal Non-Prototypical Music Mood Analysis in Continuous Space: Reliability and Performances.,2011,https://doi.org/10.5281/zenodo.1417097,Björn Schuller+Technische Universität München>DEU>education|Institute for Human-Machine Communication>DEU>facility;Felix Weninger+Technische Universität München>DEU>education|Institute for Human-Machine Communication>DEU>facility;Johannes Dorfner+Technische Universität München>DEU>education|Institute for Energy Economy and Application Technology>DEU>facility,"Music Mood Classification is frequently turned into ‘Music Mood Regression’ by using a continuous dimensional model rather than discrete mood classes. In this paper we report on automatic analysis of performances in a mood space spanned by arousal and valence on the 2.6 k songs NTWICM corpus of popular UK chart music in full realism, i.e., by automatic web-based retrieval of lyrics and diverse acoustic features without pre-selection of prototypical cases. We discuss optimal modeling of the gold standard by introducing the evaluator weighted estimator principle, group-wise feature relevance, ‘tuning’ of the regressor, and compare early and late fusion strategies. In the result, correlation coefficients of .736 (valence) and .601 (arousal) are reached on previously unseen test data.",DEU,education,Developed economies,"[-55.80438, 4.289111]","[51.723927, -5.9155884]","[-19.993547, 24.123703, 1.7740058]","[12.186905, 22.888535, 6.8998666]","[13.596581, 12.500863]","[13.143914, 3.9180243]","[16.155018, 14.859708, 1.4293536]","[14.255771, 5.0894628, 10.584321]"
125,Xing Wang;Xiaoou Chen;Deshun Yang;Yuqian Wu,Music Emotion Classification of Chinese Songs based on Lyrics Using TF*IDF and Rhyme.,2011,https://doi.org/10.5281/zenodo.1416458,Xing Wang+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education;Xiaoou Chen+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education;Deshun Yang+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education;Yuqian Wu+Peking University>CHN>education|Institute of Computer Science and Technology>CHN>education,This paper presents the outcomes of research into an automatic classification system based on the lingual part of music. Two novel kinds of short features are extracted from lyrics using tf*idf and rhyme. Meta-learning algorithm is adapted to combine these two sets of features. Results show that our features promote the accuracy of classification and meta-learning algorithm is effective in fusing the two features.,CHN,education,Developing economies,"[-53.530487, -0.16716687]","[22.422642, -4.8743796]","[-18.602463, 27.327555, 9.51106]","[8.170656, 11.445303, -1.2270979]","[13.419532, 12.57798]","[9.930433, 3.0706997]","[16.08518, 14.873652, 1.610518]","[11.754448, 6.885427, 11.0821]"
126,Christopher Bennett;Richard McNeer;Colby Leider,Urgency Analysis of Audible Alarms in The Operating Room.,2011,https://doi.org/10.5281/zenodo.1416602,Christopher Bennett+University of Miami>USA>education;Richard McNeer+University of Miami>USA>education;Colby Leider+University of Miami>USA>education,"Recent studies by researchers, governmental agencies, and safety organizations have recognized a deficiency in the performance of medically related audible alarms. In the clinical setting, care providers can suffer from alarm fatigue, a condition in which audible alarms in an operating room are perceived as a nuisance. In this study, we explore the auditory features associated with current audible alarms using tools from the music information retrieval community, and then we examine how those auditory features correlate to listeners’ perception of urgency. The results show that aperiodic changes in the auditory spectrum over time are the most salient contributor to the perception of urgency in sound. These results could inform the development of a novel standard regarding the composition of medical audible alarms.",USA,education,Developed economies,"[30.327593, -24.764933]","[48.15241, -17.368853]","[12.300912, -21.046478, -7.121948]","[2.0674984, 22.807735, 8.270934]","[10.255602, 5.1603246]","[12.309239, 4.169688]","[10.400211, 13.205158, -1.4863273]","[13.513754, 4.838693, 10.441879]"
127,Erik M. Schmidt;Youngmoo E. Kim,Modeling Musical Emotion Dynamics with Conditional Random Fields.,2011,https://doi.org/10.5281/zenodo.1416606,Erik M. Schmidt+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"Human emotion responses to music are dynamic processes that evolve naturally over time in synchrony with the music. It is because of this dynamic nature that systems which seek to predict emotion in music must necessarily analyze such processes on short-time intervals, modeling not just the relationships between acoustic data and emotion parameters, but how those relationships evolve over time. In this work we seek to model such relationships using a conditional random field (CRF), a powerful graphical model which is trained to predict the conditional probability p(y|x) for a sequence of labels y given a sequence of features x. Treating our features as deterministic, we retain the rich local subtleties present in the data, which is especially applicable to content-based audio analysis, given the abundance of data in these problems. We train our graphical model on the emotional responses of individual annotators in an 11×11 quantized representation of the arousal-valence (A-V) space. Our model is fully connected, and can produce estimates of the conditional probability for each A-V bin, allowing us to easily model complex emotion-space distributions (e.g. multimodal) as an A-V heatmap.",USA,education,Developed economies,"[-59.37085, 2.1109467]","[47.88142, -10.253905]","[-26.809557, 25.082869, 2.7547002]","[8.868887, 24.690723, 1.2691798]","[13.998149, 12.881068]","[12.877977, 4.4255633]","[16.152617, 14.405284, 1.8485626]","[14.037894, 5.107997, 10.125137]"
128,Matt McVicar;Tim Freeman;Tijl De Bie,Mining the Correlation between Lyrical and Audio Features and the Emergence of Mood.,2011,https://doi.org/10.5281/zenodo.1415888,Matt McVicar+University of Bristol>GBR>education;Tim Freeman+University of Bristol>GBR>education;Tijl De Bie+University of Bristol>GBR>education,"""Understanding the mood of music holds great potential for recommendation and genre identification problems. Unfortunately, hand-annotating music with mood tags is usually an expensive, time-consuming and subjective process, to such an extent that automatic mood recognition methods are required. In this paper we present a new unsupervised learning approach for mood recognition, based on the lyrics and the audio of a song. Our system thus eliminates the need for ground truth mood annotations, even for training the system. We hypothesize that lyrics and audio are both partially determined by the mood, and that there are no other strong common effects affecting these aspects of music. Based on this assumption, mood can be detected by performing a multi-modal analysis, identifying what lyrics and audio have in common. We demonstrate the effectiveness of this using Canonical Correlation Analysis, and confirm our hypothesis in a subsequent analysis of the results.""",GBR,education,Developed economies,"[-51.754974, 0.79986]","[51.30235, -4.7013154]","[-15.304601, 24.309597, 7.829809]","[13.958091, 21.458807, 7.244944]","[13.175683, 12.289162]","[13.079695, 3.7645812]","[15.79966, 15.039916, 1.4247499]","[14.192652, 5.1789675, 10.708898]"
129,Xiao Hu 0001;Bei Yu,Exploring The Relationship Between Mood and Creativity in Rock Lyrics.,2011,https://doi.org/10.5281/zenodo.1415690,Xiao Hu+University of Denver>USA>education;Bei Yu+Syracuse University>USA>education,"The relationship between mood and creativity has been widely studied in psychology, however, no conclusion is reached in terms of which mood triggers high creativity, positive or negative. This paper provides new insights to this on-going argument by examining the relationship between lyrics creativity and music mood. We use three computational measures to gauge lyrics creativity: Type-to-Token Ratio, word norms fraction, and WordNet similarity. We then test three hypotheses regarding differences in lyrics creativity between music with different moods on 2715 U.S. rock songs. The three measures led to consistent findings that lyrics of negative and sad songs demonstrate higher linguistic creativity than those of positive and happy songs. Our findings support previous studies in psycholinguistics that people write more creatively when the text conveys sad or negative sentiment, and contradict previous research that positive mood triggers more unusual word associations. The result also indicates that different measures capture different aspects of lyrics creativity.",USA,education,Developed economies,"[-51.25556, 1.1952147]","[52.725063, -0.8368059]","[-14.198039, 23.97045, 6.175164]","[12.974296, 24.194813, 10.600208]","[13.21492, 12.275772]","[13.036706, 3.624568]","[15.861463, 14.995131, 1.3549157]","[14.081064, 5.159333, 10.822353]"
130,Gonçalo Marques;Marcos Aurélio Domingues;Thibault Langlois;Fabien Gouyon,Three Current Issues In Music Autotagging.,2011,https://doi.org/10.5281/zenodo.1417757,Gonc¸alo Marques+DEETC-ISEL>PRT>education;Marcos Aur´elio Domingues+INESC Porto>PRT>Unknown;Thibault Langlois+DI-FCUL>PRT>education;Fabien Gouyon+INESC Porto>PRT>Unknown,"The purpose of this paper is to address several aspects of music autotagging. We start by presenting autotagging experiments conducted with two different systems and show performances on a par with a method representative of the state-of-the-art. Beyond that, we illustrate via systematic experiments the importance of a number of issues relevant to autotagging, yet seldom reported in the literature. First, we show that the evaluation of autotagging techniques is fragile in the sense that small alterations to the set of tags to be learned, or in the set of music pieces may lead to dramatically different results. Hence we stress a set of methodological recommendations regarding data and evaluation metrics. Second, we conduct experiments on the generality of autotagging models, showing that a number of different methods at a similar performance level to the state-of-the-art fail to learn tag models able to generalize to datasets from different origins. Third we show that current performance level of a direct mapping between audio features and tags still appears insufficient to enable the possibility of exploiting natural tag correlations as a second stage to improve performance.",PRT,education,Developed economies,"[-41.91081, -3.7899556]","[39.42693, -0.4633182]","[-13.538107, 15.353658, 13.188512]","[24.517263, 8.331245, 3.6451983]","[14.546625, 10.591357]","[11.546955, 3.3915546]","[15.658094, 14.04155, 0.1865582]","[13.070544, 6.346347, 11.267617]"
131,Vijay Chandrasekhar;Matt Sharifi;David A. Ross,Survey and Evaluation of Audio Fingerprinting Schemes for Mobile Query-by-Example Applications.,2011,https://doi.org/10.5281/zenodo.1415260,Vijay Chandrasekhar+Stanford University>USA>education;Matt Shariﬁ+Google>USA>company;David A. Ross+Google>USA>company,"We survey and evaluate popular audio fingerprinting schemes in a common framework with short query probes captured from cell phones. We report and discuss results important for mobile applications: Receiver Operating Characteristic (ROC) performance, size of fingerprints generated compared to size of audio probe, and transmission delay if the fingerprint data were to be transmitted over a wireless link. We hope that the evaluation in this work will guide work towards reducing latency in practical mobile audio retrieval applications.",USA,education,Developed economies,"[-16.557177, -25.266743]","[25.717472, -23.566969]","[5.6477165, -10.870287, -25.357075]","[17.463898, -12.583142, 6.6483145]","[9.068709, 4.4429517]","[8.514655, -0.09328187]","[10.58755, 11.514534, -1.9394085]","[10.147439, 5.244104, 13.0181675]"
132,Asteris I. Zacharakis;Konstantinos Pastiadis;Georgios Papadelis;Joshua D. Reiss,An Investigation of Musical Timbre: Uncovering Salient Semantic Descriptors and Perceptual Dimensions.,2011,https://doi.org/10.5281/zenodo.1414964,Asteris Zacharakis+Queen Mary University of London>GBR>education;Kostantinos Pastiadis+Aristotle University of Thessaloniki>GRC>education;Georgios Papadelis+Aristotle University of Thessaloniki>GRC>education;Joshua D. Reiss+Queen Mary University of London>GBR>education,A study on the verbal attributes of musical timbre was conducted in an effort to identify the most significant semantic descriptors and to quantify the association between prominent timbral aspects and several categorical properties of environmental entities. A verbal attribute magnitude estimation (VAME) type of listening test in which participants were asked to describe 23 musical sounds using 30 Greek adjectives together with verbal terms of their own choice was designed and conducted for this purpose. Factor and Cluster Analysis were performed on the subjective evaluation data in order to shed some light on the relationships between the adjectives that were proposed and to conclude to the number and quality of the salient perceptual dimensions required for the description of this set of sounds.,GBR,education,Developed economies,"[-17.411406, -0.58829397]","[50.489674, -15.326715]","[-2.111864, 18.916565, 10.990807]","[2.8435512, 19.900723, 5.423891]","[12.673287, 9.104081]","[12.4367075, 4.178268]","[13.60733, 14.181633, -0.76741797]","[13.687077, 4.8118467, 10.369132]"
31,Balaji Thoshkahna;François Xavier Nsabimana;Ramakrishnan R. Kalpathi,A Transient Detection Algorithm for Audio Using Iterative Analysis of STFT.,2011,https://doi.org/10.5281/zenodo.1415630,Balaji Thoshkahna+Indian Institute of Science>IND>education;Francois Xavier Nsabimana+Fraunhofer Institute of Digital Media Technology>DEU>facility;K.R. Ramakrishnan+Indian Institute of Science>IND>education,"""We propose an iterative algorithm to detect transient segments in audio signals. Short time Fourier transform (STFT) is used to detect rapid local changes in the audio signal. The algorithm has two steps that iteratively - (a) calculate a function of the STFT and (b) build a transient signal. A dynamic thresholding scheme is used to locate the potential positions of transients in the signal. The iterative procedure ensures that genuine transients are built up while the localised spectral noise are suppressed by using an energy criterion. The extracted transient signal is later compared to a ground truth dataset. The algorithm performed well on two databases. On the EBU-SQAM database of monophonic sounds, the algorithm achieved an F-measure of 90% while on our database of polyphonic audio an F-measure of 91% was achieved. This technique is being used as a preprocessing step for a tempo analysis algorithm and a TSR (Transients + Sines + Residue) decomposition scheme.""",IND,education,Developing economies,"[26.763512, -26.58024]","[-22.500145, -4.954508]","[7.889243, -17.611118, -10.615371]","[0.92109394, 7.1259823, -11.5826845]","[10.379077, 5.4063272]","[5.677961, 2.4160519]","[10.665098, 13.200718, -1.2342496]","[8.012899, 7.641741, 10.941287]"
123,Rafael Cabredo;Roberto S. Legaspi;Masayuki Numao,Identifying Emotion Segments in Music by Discovering Motifs in Physiological Data.,2011,https://doi.org/10.5281/zenodo.1416872,Rafael Cabredo+Osaka University>JPN>education;Roberto Legaspi+Osaka University>JPN>education;Masayuki Numao+Osaka University>JPN>education,"Music can induce different emotions in people. We propose a system that can identify music segments which induce specific emotions from the listener. The work involves building a knowledge base with mappings between affective states (happiness, sadness, etc.) and music features (rhythm, chord progression, etc.). Building this knowledge base requires background knowledge from music and emotions psychology. Psychophysiological responses of a user, particularly, the blood volume pulse, are taken while he listens to music. These signals are analyzed and mapped to various musical features of the songs he listened to. A motif discovery algorithm used in data mining is adapted to analyze signals of physiological data. Motif discovery finds patterns in the data that indicate points of interest in the music. The different motifs are stored in a library of patterns and used to identify other songs that have similar musical content. Results show that motifs selected have similar chord progressions. Some of which include frequently used chords in western pop music.",JPN,education,Developed economies,"[-59.880096, 1.104424]","[55.788464, -8.2368765]","[-27.249512, 22.357132, 2.2436397]","[9.274742, 17.25963, 4.87978]","[14.00963, 12.887748]","[12.986581, 3.906468]","[16.110579, 14.403275, 1.8662953]","[14.061319, 5.0468955, 10.470816]"
110,Rudolf Mayer;Andreas Rauber,Music Genre Classification by Ensembles of Audio and Lyrics Features.,2011,https://doi.org/10.5281/zenodo.1416844,Rudolf Mayer+Vienna University of Technology>AUT>education;Andreas Rauber+Vienna University of Technology>AUT>education,"Algorithms that can understand and interpret characteristics of music, and organise them for and recommend them to their users can be of great assistance in handling the ever growing size of both private and commercial collections. Music is an inherently multi-modal type of data, and the lyrics associated with the music are as essential to the reception and the message of a song as is the audio. In this paper, we present advanced methods on how the lyrics domain of music can be combined with the acoustic domain. We evaluate our approach by means of a common task in music information retrieval, musical genre classification. Advancing over previous work that showed improvements with simple feature fusion, we apply the more sophisticated approach of result (or late) fusion. We achieve results superior to the best choice of a single algorithm on a single feature set.",AUT,education,Developed economies,"[-29.327002, -13.246437]","[23.855974, -7.6855435]","[-14.655139, 6.7626333, 15.830796]","[16.519375, 3.852182, -2.2982354]","[12.933376, 10.968777]","[10.251182, 3.1753802]","[13.961866, 14.254839, 1.4295119]","[11.96359, 6.7818336, 11.186011]"
109,Sander Dieleman;Philemon Brakel;Benjamin Schrauwen,Audio-based Music Classification with a Pretrained Convolutional Network.,2011,https://doi.org/10.5281/zenodo.1415188,Sander Dieleman+Ghent University>BEL>education;Philémon Brakel+Ghent University>BEL>education;Benjamin Schrauwen+Ghent University>BEL>education,"Recently the ‘Million Song Dataset’, containing audio features and metadata for one million songs, was made available. In this paper, we build a convolutional network that is then trained to perform artist recognition, genre recognition and key detection. The network is tailored to summarize the audio features over musically significant timescales. It is infeasible to train the network on all available data in a supervised fashion, so we use unsupervised pretraining to be able to harness the entire dataset: we train a convolutional deep belief network on all data, and then use the learnt parameters to initialize a convolutional multilayer perceptron with the same architecture. The MLP is then trained on a labeled subset of the data for each task. We also train the same MLP with randomly initialized weights. We find that our convolutional approach improves accuracy for the genre recognition and artist recognition tasks. Unsupervised pretraining improves convergence speed in all cases. For artist recognition it improves accuracy as well.",BEL,education,Developed economies,"[-18.257555, -11.913383]","[-23.008268, -36.492558]","[1.2779809, 2.9558964, 9.048374]","[-5.7694697, -1.6428182, -19.361795]","[11.796899, 9.726989]","[9.607731, 4.744006]","[13.3596, 13.500799, 0.9557697]","[10.757606, 6.5094857, 8.98143]"
108,Rémi Foucard;Slim Essid;Mathieu Lagrange;Gaël Richard,Multi-scale temporal fusion by boosting for music classification.,2011,https://doi.org/10.5281/zenodo.1416904,Rémi Foucard+TELECOM ParisTech>FRA>education|CNRS-LTCI>FRA>facility;Slim Essid+TELECOM ParisTech>FRA>education|CNRS-LTCI>FRA>facility;Mathieu Lagrange+Ircam>FRA>facility|CNRS-STMS>FRA>facility;Gaël Richard+TELECOM ParisTech>FRA>education|CNRS-LTCI>FRA>facility,"Short-term and long-term descriptors constitute complementary pieces of information in the analysis of audio signals. However, because they are extracted over different time horizons, it is difficult to exploit them concurrently in a fully effective manner. In this paper we propose a novel temporal fusion method that leverages the effectiveness of a given set of features by efficiently combining multi-scale versions of them. This fusion is achieved using a boosting technique exploiting trees as weak classifiers, which has the advantage of performing an embedded feature selection. We apply our algorithm to two standard classification tasks, namely musical instrument recognition and multi-tag classification. Our experiments indicate that the multi-scale approach is able to select different features at different scales and significantly outperforms the mono-scale systems in terms of classification performance.",FRA,education,Developed economies,"[-24.087719, -9.067965]","[24.712038, -8.623556]","[-10.386671, 6.7774434, 12.347331]","[17.17859, 1.9659433, -2.0212355]","[12.392019, 10.374627]","[10.057921, 3.3224213]","[13.64602, 13.803009, 0.9406192]","[11.751518, 7.0165462, 11.017182]"
84,Cynthia C. S. Liem;Alan Hanjalic,Expressive Timing from Cross-Performance and Audio-based Alignment Patterns: An Extended Case Study.,2011,https://doi.org/10.5281/zenodo.1416992,Cynthia C.S. Liem+Delft University of Technology>NLD>education;Alan Hanjalic+Delft University of Technology>NLD>education,"Audio recordings of classical music pieces reflect the artistic interpretation of the piece as seen by the recorded performing musician. With many recordings being typically available for the same music piece, multiple expressive rendition variations of this piece are obtained, many of which are induced by the underlying musical content. In earlier work, we focused on timing as a means of expressivity, and proposed a light-weight, unsupervised and audio-based method to study timing deviations among different performances through alignment patterns. By using the standard deviation of alignment patterns as a measure for the display of individuality in a recording, structural and interpretational aspects of a music piece turned out to be highlighted in a qualitative case study on five Chopin mazurkas. In this paper, we propose an entropy-based deviation measure as an alternative to the existing standard deviation measure. The obtained results for multiple short-time window resolutions, both from a quantitative and qualitative perspective, strengthen our earlier finding that the found patterns are musically informative and confirm that entropy is a good alternative measure for highlighting expressive timing deviations in recordings.",NLD,education,Developed economies,"[19.114803, -16.956524]","[-28.233324, 5.652547]","[2.8797417, -17.957184, -12.483992]","[-8.131514, 7.249624, -2.8422792]","[11.129619, 6.13629]","[5.9446, 1.2565463]","[11.898955, 12.68076, -1.7602386]","[8.280145, 6.265858, 11.328128]"
85,Takuma Otsuka;Kazuhiro Nakadai;Tetsuya Ogata;Hiroshi G. Okuno,Incremental Bayesian Audio-to-Score Alignment with Flexible Harmonic Structure Models.,2011,https://doi.org/10.5281/zenodo.1418159,"Takuma Otsuka+Kyoto University>JPN>education;Kazuhiro Nakadai+Honda Research Institute Japan, Co., Ltd.>JPN>company;Tetsuya Ogata+Kyoto University>JPN>education;Hiroshi G. Okuno+Kyoto University>JPN>education","Music information retrieval, especially the audio-to-score alignment problem, often involves a matching problem between the audio and symbolic representations. We must cope with uncertainty in the audio signal generated from the score in a symbolic representation such as the variation in the timbre or temporal fluctuations. Existing audio-to-score alignment methods are sometimes vulnerable to the uncertainty in which multiple notes are simultaneously played with a variety of timbres because these methods rely on static observation models. For example, a chroma vector or a fixed harmonic structure template is used under the assumption that musical notes in a chord are all in the same volume and timbre. This paper presents a particle filter-based audio-to-score alignment method with a flexible observation model based on latent harmonic allocation. Our method adapts to the harmonic structure for the audio-to-score matching based on the observation of the audio signal through Bayesian inference. Experimental results with 20 polyphonic songs reveal that our method is effective when more number of instruments are involved in the ensemble.",JPN,education,Developed economies,"[19.719833, -14.804401]","[-16.280207, -11.135061]","[0.7869817, -15.460247, -10.779355]","[3.9574625, -18.849714, -2.8773484]","[10.8596735, 6.026049]","[6.385002, 1.0439277]","[11.690641, 12.522594, -1.7165748]","[8.357589, 6.1822433, 10.929967]"
86,Kenta Okumura;Shinji Sako;Tadashi Kitamura,Stochastic Modeling of a Musical Performance with Expressive Representations from the Musical Score.,2011,https://doi.org/10.5281/zenodo.1417093,Kenta Okumura+Nagoya Institute of Technology>JPN>education;Shinji Sako+Nagoya Institute of Technology>JPN>education;Tadashi Kitamura+Nagoya Institute of Technology>JPN>education,"This paper presents a method for describing the characteristics of human musical performance. We consider the problem of building models that express the ways in which deviations from a strict interpretations of the score occurs in the performance, and that cluster these deviations automatically. The clustering process is performed using expressive representations unambiguously notated on the musical score, without any arbitrariness by the human observer. The result of clustering is obtained as hierarchical tree structures for each deviational factor that occurred during the operation of the instrument. This structure represents an approximation of the performer’s interpretation with information notated on the score they used during the performance. This model represents the conditions that generate the difference in the fluctuation of performance expression and the amounts of deviational factors directly from the data of real performance. Through validations of applying the method to the data measured from real performances, we show that the use of information regarding expressive representation on the musical score enables the efficient estimation of generative-model for the musical performance.",JPN,education,Developed economies,"[-14.026134, 2.053461]","[-29.762793, 5.449965]","[-4.180681, 6.356853, 18.593056]","[-9.756806, 4.959384, -2.886459]","[11.07297, 8.28823]","[7.6193023, 3.5018754]","[13.28755, 13.066008, -0.6758879]","[8.781119, 6.2059345, 11.033906]"
87,Brian McFee;Gert R. G. Lanckriet,The Natural Language of Playlists.,2011,https://doi.org/10.5281/zenodo.1418119,"Brian McFee+University of California, San Diego>USA>education|University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education|University of California, San Diego>USA>education","We propose a simple, scalable, and objective evaluation procedure for playlist generation algorithms. Drawing on standard techniques for statistical natural language processing, we characterize playlist algorithms as generative models of strings of songs belonging to some unknown language. To demonstrate the procedure, we compare several playlist algorithms derived from content, semantics, and meta-data. We then develop an efficient algorithm to learn an optimal combination of simple playlist algorithms. Experiments on a large collection of naturally occurring playlists demonstrate the efficacy of the evaluation procedure and learning algorithm.",USA,education,Developed economies,"[-41.982014, 37.9718]","[38.262917, 22.291912]","[-6.101159, 30.61897, -2.115967]","[18.011599, 5.323989, 22.281008]","[16.108877, 8.240957]","[12.126026, 1.67194]","[16.485191, 14.837941, -1.7118623]","[13.234325, 5.136867, 13.079628]"
88,Bernhard Niedermayer;Sebastian Böck;Gerhard Widmer,"On the Importance of ""Real"" Audio Data for MIR Algorithm Evaluation at the Note-Level - A Comparative Study.",2011,https://doi.org/10.5281/zenodo.1417923,Bernhard Niedermayer+Johannes Kepler University Linz>AUT>education;Sebastian Böck+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence>AUT>facility,"A considerable number of MIR tasks requires annotations at the note-level for the purpose of in-depth evaluation. A common means of obtaining accurately annotated data corpora is to start with a symbolic representation of a piece and generate corresponding audio data. This study investigates the effect of audio quality and source on the performance of two representative MIR algorithms – Onset Detection and Audio Alignment. Three kinds of audio material are compared: piano pieces generated using a freely available software synthesizer with its default instrument patches; a commercial high-quality sample library; and audio recordings made on a real (computer-controlled) grand piano. Also, the effect of varying richness of artistic changes in tempo and dynamics or natural asynchronies is examined. We show that the algorithms’ performance on the different datasets varies considerably, but synthesized audio does not necessarily yield better results.",AUT,education,Developed economies,"[-12.025673, 57.336357]","[-20.176357, -12.447127]","[-39.22048, 1.3151708, -1.5335821]","[-4.370489, -15.84935, -2.2618268]","[13.381781, 5.0548477]","[7.4782577, 1.700532]","[14.755564, 11.414337, -1.3661082]","[9.799392, 5.845297, 10.789832]"
89,Jacquelin A. Speck;Erik M. Schmidt;Brandon G. Morton;Youngmoo E. Kim,A Comparative Study of Collaborative vs. Traditional Musical Mood Annotation.,2011,https://doi.org/10.5281/zenodo.1417075,Jacquelin A. Speck+Drexel University>USA>education;Erik M. Schmidt+Drexel University>USA>education;Brandon G. Morton+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"Organizing music by emotional association is a natural process for humans, but the ambiguous nature of emotion makes it a difficult task for machines. Automatic systems for music emotion recognition rely on ground truth data collected from humans, and more effective methods for collecting such data are being continuously developed. In previous work, we developed MoodSwings, an online collaborative game for crowdsourcing dynamic (per-second) mood ratings from multiple players within the two-dimensional arousal-valence (A-V) representation of emotion. MoodSwings has proven effective for data collection, but potential data effects caused by collaborative labeling have not yet been analyzed. In this work, we compare the effectiveness of MoodSwings to that of a more traditional data collection method, where annotation is performed by single, paid annotators. We implement a simplified labeling task to run on Amazon’s crowdsourcing engine, Mechanical Turk (MTurk), and analyze the labels collected with each method. A statistical comparison shows consistencies between MoodSwings and MTurk data, and we produce similar results using each as training data for automatic emotion production via supervised machine learning. Furthermore the new dataset collected via MTurk has been made available to the Music Information Retrieval community.",USA,education,Developed economies,"[-53.84953, 5.821824]","[46.495438, -5.5443444]","[-18.85415, 27.829342, 1.6684881]","[13.94875, 20.960274, 1.9895555]","[13.492474, 12.372792]","[12.732313, 3.9602094]","[16.084938, 14.841285, 1.3119276]","[13.879092, 5.3574667, 10.463187]"
90,Jordan Bennett Louis Smith;John Ashley Burgoyne;Ichiro Fujinaga;David De Roure;J. Stephen Downie,Design and creation of a large-scale database of structural annotations.,2011,https://doi.org/10.5281/zenodo.1416884,Jordan B. L. Smith+University of Southern California>USA>education;J. Ashley Burgoyne+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education;David De Roure+University of Oxford>GBR>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"This paper describes the design and creation of an unprecedentedly large database of over 2400 structural annotations of nearly 1400 musical recordings. The database is intended to be a test set for algorithms that will be used to analyze a much larger corpus of hundreds of thousands of recordings, as part of the Structural Analysis of Large Amounts of Musical Information (SALAMI) project. This paper describes the design goals of the database and the practical issues that were encountered during its creation. In particular, we discuss the selection of the recordings, the development of an annotation format and procedure that adapts work by Peeters and Deruty [10], and the management and execution of the project. We also summarize some of the properties of the resulting corpus of annotations, including average inter-annotator agreement.",USA,education,Developed economies,"[16.955547, 33.869724]","[-2.016126, 9.461859]","[-19.754345, -14.67997, 5.9727254]","[-4.2318277, 2.534312, 4.230955]","[12.513624, 6.517396]","[8.751273, 2.2551317]","[14.29672, 12.909757, -0.75520146]","[10.484193, 6.4310813, 11.537865]"
91,Andreas F. Ehmann;Mert Bay;J. Stephen Downie;Ichiro Fujinaga;David De Roure,Music Structure Segmentation Algorithm Evaluation: Expanding on MIREX 2010 Analyses and Datasets.,2011,https://doi.org/10.5281/zenodo.1418151,Andreas F. Ehmann+University of Illinois Urbana-Champaign>USA>education;Mert Bay+University of Illinois Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois Urbana-Champaign>USA>education;Ichiro Fujinaga+McGill University>CAN>education;David De Roure+University of Oxford>GBR>education,"Music audio structure segmentation has been a task in the Music Information Retrieval Evaluation eXchange (MIREX) since 2009. In 2010, five algorithms were evaluated against two datasets (297 and 100 songs) with an almost exclusive focus on western popular music. A new annotated dataset significantly larger in size and with a more diverse range of musical styles became available in 2011. This new dataset comprises over 1,300 songs spanning pop, jazz, classical, and world music styles. The algorithms from the 2010 iteration of MIREX are re-evaluated against this new dataset. This paper presents a detailed analysis of these evaluation results in order to gain a better understanding of the current state-of-the-art in automatic structure segmentation. These expanded analyses focus on the interaction of algorithm performance and rankings with datasets, musical styles, and annotation level. Because the new dataset contains multiple annotations for each song, we also introduce a baseline for expected human performance for this task.",USA,education,Developed economies,"[-3.4776082, -3.2210417]","[-2.6707366, 6.2789326]","[5.287538, -4.267594, 0.23288469]","[-2.5135527, 0.17967781, 0.6192052]","[11.628016, 8.274356]","[8.4942665, 2.690428]","[12.51639, 14.110385, 0.07107645]","[10.478044, 6.950771, 11.351668]"
92,Steven R. Ness;Shawn Trail;Peter F. Driessen;W. Andrew Schloss;George Tzanetakis,Music Information Robotics: Coping Strategies for Musically Challenged Robots.,2011,https://doi.org/10.5281/zenodo.1415868,Steven Ness+University of Victoria>CAN>education;Shawn Trail+University of Victoria>CAN>education;Peter Driessen+University of Victoria>CAN>education;Andrew Schloss+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"""In the past few years there has been a growing interest in music robotics. Robotic instruments that generate sound acoustically using actuators have been increasingly developed and used in performances and compositions over the past 10 years. Although such devices can be very sophisticated mechanically, in most cases they are passive devices that directly respond to control messages from a computer. In the few cases where more sophisticated control and feedback is employed it is in the form of simple mappings with little musical understanding. Several techniques for extracting musical information have been proposed in the field of music information retrieval. In most cases the focus has been the batch processing of large audio collections rather than real time performance understanding. In this paper we describe how such techniques can be adapted to deal with some of the practical problems we have experienced in our own work with music robotics. Of particular importance is the idea of self-awareness or proprioception in which the robot(s) adapt their behavior based on understanding the connection between their actions and sound generation through listening. More specifically we describe techniques for solving the following problems: 1) controller mapping 2) velocity calibration, and 3) gesture recognition.""",CAN,education,Developed economies,"[-32.039246, 15.749869]","[-6.648637, 35.304295]","[-6.9116354, 27.738611, 14.392445]","[-15.357841, 5.246346, 16.12642]","[14.142133, 8.206713]","[10.460326, 1.2359554]","[14.584594, 14.477096, -1.6217049]","[10.674683, 5.1797514, 11.493689]"
93,Trevor Knight;Finn Upham;Ichiro Fujinaga,The potential for automatic assessment of trumpet tone quality.,2011,https://doi.org/10.5281/zenodo.1418123,"Trevor Knight+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University>CAN>education;Finn Upham+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University>CAN>education;Ichiro Fujinaga+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University>CAN>education","The goal of this study was to examine the possibility of training machine learning algorithms to differentiate between the performance of good notes and bad notes. Four trumpet players recorded a total of 239 notes from which audio features were extracted. The notes were subjectively graded by five brass players. The resulting dataset was used to train support vector machines with different groupings of ratings. Splitting the data set into two classes (“good” and “bad”) at the median rating, the classifier showed an average success rate of 72% when training and testing using cross-validation. Splitting the data into three roughly-equal classes (“good,” “medium,” and “bad”), the classifier correctly identified the class an average of 54% of the time. Even using seven classes, the classifier identified the correct class 46% of the time, which is better than the result expected from chance or from the strategy of picking the most populous class (36%).",CAN,education,Developed economies,"[6.961093, -29.044981]","[-43.230297, 1.7987036]","[14.614589, -12.557149, -5.169741]","[-9.165743, 22.849371, -4.322791]","[9.062128, 7.4403214]","[8.972442, 3.7577045]","[11.066843, 12.832852, 0.4004035]","[10.813444, 7.270858, 10.475755]"
94,Spencer S. Topel;Michael A. Casey,Elementary Sources: Latent Component Analysis for Music Composition.,2011,https://doi.org/10.5281/zenodo.1417351,Spencer S. Topel+Dartmouth College>USA>education;Michael A. Casey+Dartmouth College>USA>education,"Complexity of music audio signals creates an access problem to specific musical objects or structures within the source samples. Instead of employing more commonly used audio analysis or production techniques to access features, we describe extraction of sub-mixtures from real-world audio using a Probabilistic Latent Component Analysis-based decomposition tool for music composition. This is highlighted with the presentation of a prior relevant compositional approach named Spectral Music along with a discussion of five compositions extending these principles using methods more commonly associated with source separation research.",USA,education,Developed economies,"[-10.468979, 9.860446]","[-42.939903, -25.294817]","[1.4728913, 12.003191, 18.23815]","[-8.396918, -8.124778, -31.580078]","[12.647639, 8.717399]","[6.511133, 5.034496]","[13.523004, 13.944869, -0.4639513]","[9.911083, 8.653394, 9.878091]"
95,Alison Mattek;Michael A. Casey,Cross-Modal Aesthetics from A Feature Extraction Perspective: A Pilot Study.,2011,https://doi.org/10.5281/zenodo.1414906,Alison Mattek+Dartmouth College>USA>education;Michael Casey+Dartmouth College>USA>education,"This paper investigates perceptual relationships between art in the auditory and visual domains. First, we conducted a behavioral experiment asking subjects to assess similarity between 10 musical recordings and 10 works of abstract art. We found a significant degree of agreement across subjects as to which images correspond to which audio, even though neither the audio nor the images possessed semantic content. Secondly, we sought to find the relationship between audio and images within a defined feature space that correlated with the subjective similarity judgments. We trained two regression models using leave-one-subject-out and leave-one-audio-out cross-validation respectively, and exhaustively evaluated each model's ability to predict features of subject-ranked similar images using only a given audio clip's features. A retrieval task used the predicted image features to retrieve likely related images from the data set. The task was evaluated using the ground truth of subjects' actual similarity judgments. Our results show a mean cross-validated prediction accuracy of 0.61 with p<0.0001 for the first model, and a mean prediction accuracy of 0.51 with p<0.03 for the second model.",USA,education,Developed economies,"[-7.294362, 25.52863]","[31.961626, 8.9525795]","[-29.656595, 12.774833, 13.514951]","[11.178799, 5.907381, 11.64594]","[13.190132, 8.415112]","[11.30707, 2.2926526]","[14.045696, 13.958139, -0.9401675]","[12.766724, 6.3674345, 12.536942]"
96,Thierry Bertin-Mahieux;Daniel P. W. Ellis;Brian Whitman;Paul Lamere,The Million Song Dataset.,2011,https://doi.org/10.5281/zenodo.1415820,Thierry Bertin-Mahieux+Columbia University>USA>education;Daniel P.W. Ellis+Columbia University>USA>education;Brian Whitman+The Echo Nest>USA>company;Paul Lamere+The Echo Nest>USA>company,"We introduce the Million Song Dataset, a freely-available collection of audio features and metadata for a million contemporary popular music tracks. We describe its creation process, its content, and its possible uses. Attractive features of the Million Song Database include the range of existing resources to which it is linked, and the fact that it is the largest current research dataset in our field. As an illustration, we present year prediction as an example application, a task that has, until now, been difficult to study owing to the absence of a large set of suitable data. We show positive results on year prediction, and discuss more generally the future development of the dataset.",USA,education,Developed economies,"[-20.683779, 6.840327]","[10.12706, 23.904623]","[-22.743763, 1.6803776, 4.456594]","[-4.4252577, 3.7788813, 12.989725]","[12.928801, 7.779613]","[10.282851, 3.8128994]","[14.294762, 13.576586, -0.92782193]","[11.730484, 5.9160233, 10.902003]"
97,Julián Urbano;Diego Martín 0001;Mónica Marrero;Jorge Morato,Audio Music Similarity and Retrieval: Evaluation Power and Stability.,2011,https://doi.org/10.5281/zenodo.1417269,Julián Urbano+University Carlos III of Madrid>ESP>education;Diego Martín+University Carlos III of Madrid>ESP>education;Mónica Marrero+University Carlos III of Madrid>ESP>education;Jorge Morato+University Carlos III of Madrid>ESP>education,"In this paper we analyze the reliability of the results in the evaluation of Audio Music Similarity and Retrieval systems. We focus on the power and stability of the evaluation, that is, how often a significant difference is found between systems and how often these significant differences are incorrect. We study the effect of using different effectiveness measures with different sets of relevance judgments, for varying number of queries and alternative statistical procedures. Different measures are shown to behave similarly overall, though some are much more sensitive and stable than others. The use of different statistical procedures does improve the reliability of the results, and it allows using as little as half the number of queries currently used in MIREX evaluations while still offering very similar reliability levels. We also conclude that experimenters can be very confident that if a significant difference is found between two systems, the difference is indeed real.",ESP,education,Developed economies,"[-6.2957163, 15.275174]","[27.315672, 29.995626]","[-6.280483, 7.36102, -4.7906175]","[6.4255257, 5.021805, 12.750089]","[13.122312, 9.122243]","[11.692676, 1.0399154]","[13.573905, 15.049973, -0.85422695]","[12.300165, 4.9638033, 12.323163]"
98,Nicola Orio;David Rizo;Riccardo Miotto;Markus Schedl;Nicola Montecchio;Olivier Lartillot,MusiCLEF: a Benchmark Activity in Multimodal Music Information Retrieval.,2011,https://doi.org/10.5281/zenodo.1416506,Nicola Orio+University of Padova>ITA>education;David Rizo+University of Alicante>ESP>education;Riccardo Miotto+University of Padova>ITA>education;Nicola Montecchio+University of Padova>ITA>education;Markus Schedl+Johannes Kepler University>AUT>education;Olivier Lartillot+Academy of Finland>FIN>education,"This work presents the rationale, tasks and procedures of MusiCLEF, a novel benchmarking activity that has been developed along with the Cross-Language Evaluation Forum (CLEF). The main goal of MusiCLEF is to promote the development of new methodologies for music access and retrieval on real public music collections, which can combine content-based information, automatically extracted from music files, with contextual information, provided by users via tags, comments, or reviews. Moreover, MusiCLEF aims at maintaining a tight connection with real application scenarios, focusing on issues on music access and retrieval that are faced by professional users. To this end, this year’s evaluation campaign focused on two main tasks: automatic categorization of music to be used as soundtrack of TV shows and automatic identification of the digitized material of a music digital library.",ITA,education,Developed economies,"[-9.723452, 23.933979]","[23.41508, 21.83438]","[-6.9854074, 12.747423, -14.989531]","[10.378337, -1.7367783, 17.559511]","[13.537398, 7.91399]","[11.068484, 1.0385778]","[13.755209, 14.302809, -1.7289999]","[11.852084, 5.202223, 12.593705]"
99,Julián Urbano,Information Retrieval Meta-Evaluation: Challenges and Opportunities in the Music Domain.,2011,https://doi.org/10.5281/zenodo.1417897,Julián Urbano+University Carlos III of Madrid>ESP>education,"The Music Information Retrieval field has acknowledged the need for rigorous scientific evaluations for some time now. Several efforts were set out to develop and provide the necessary infrastructure, technology and methodologies to carry out these evaluations, out of which the annual Music Information Retrieval Evaluation eXchange emerged. The community as a whole has enormously gained from this evaluation forum, but very little attention has been paid to reliability and correctness issues. From the standpoint of the analysis of experimental validity, this paper presents a survey of past meta-evaluation work in the context of Text Information Retrieval, arguing that the music community still needs to address various issues concerning the evaluation of music systems and the IR cycle, pointing out directions for further research and proposals in this line.",ESP,education,Developed economies,"[-21.312838, 21.168247]","[27.198772, 32.10828]","[-16.015486, 3.6803482, -9.867951]","[4.2648478, 3.8015282, 15.017053]","[14.288897, 7.9612613]","[11.743832, 0.7545877]","[14.476284, 14.639167, -2.0325122]","[12.180974, 4.5636873, 12.3275175]"
100,Meinard Müller;Peter Grosche;Nanzhu Jiang,A Segment-Based Fitness Measure for Capturing Repetitive Structures of Music Recordings.,2011,https://doi.org/10.5281/zenodo.1416342,Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>education;Peter Grosche+Saarland University>DEU>education|MPI Informatik>DEU>education;Nanzhu Jiang+Saarland University>DEU>education|MPI Informatik>DEU>education,"In this paper, we deal with the task of determining the audio segment that best represents a given music recording (similar to audio thumbnailing). Typically, such a segment has many (approximate) repetitions covering large parts of the music recording. As main contribution, we introduce a novel fitness measure that assigns to each segment a fitness value that expresses how much and how well the segment “explains” the repetitive structure of the recording. In combination with enhanced feature representations, we show that our fitness measure can cope even with strong variations in tempo, instrumentation, and modulations that may occur within and across related segments. We demonstrate the practicability of our approach by means of several challenging examples including field recordings of folk music and recordings of classical music.",DEU,education,Developed economies,"[-2.6608615, 4.636771]","[-0.8026772, 0.08271337]","[-7.5714965, -7.062958, -4.11052]","[3.061931, -2.0077078, -0.885913]","[12.294889, 7.8332157]","[8.298336, 2.5544863]","[12.949337, 13.745309, -0.90480644]","[10.320281, 7.35786, 11.603158]"
101,Jeffrey J. Scott;Youngmoo E. Kim,Analysis of Acoustic Features for Automated Multi-Track Mixing.,2011,https://doi.org/10.5281/zenodo.1416038,Jeffrey Scott+Drexel University>USA>education|Music and Entertainment Technology Laboratory (MET-lab)>USA>facility;Youngmoo E. Kim+Drexel University>USA>education|Music and Entertainment Technology Laboratory (MET-lab)>USA>facility,"The capability of the average person to generate digital music content has rapidly expanded over the past several decades. While the mechanics of creating a multi-track recording are relatively straightforward, using the available tools to create professional quality work requires substantial training and experience. We address one of the most fundamental processes to creating a finished product, namely determining the relative gain levels of each track to produce a final, mixed song. By modeling the time-varying mixing coefficients with a linear dynamical system, we train models that predict a weight vector for a given instrument using features extracted from the audio content of all of the tracks.",USA,education,Developed economies,"[18.93056, -37.19989]","[-35.8319, -21.469862]","[13.365874, -25.91237, -12.999335]","[-20.48422, 0.81106925, -25.27505]","[10.113907, 4.7905416]","[7.431457, 6.2831254]","[10.694558, 12.460782, -1.7763491]","[9.433618, 7.551673, 9.4342785]"
102,Nicola Montecchio;Arshia Cont,Accelerating The Mixing Phase In Studio Recording Productions By Automatic Audio Alignment.,2011,https://doi.org/10.5281/zenodo.1415228,Nicola Montecchio+University of Padova>ITA>education;Arshia Cont+Institut de Recherche et Coordination Acoustique/Musique (IRCAM)>FRA>facility,"We propose a system for accelerating the mixing phase in a recording production, by making use of audio alignment techniques to automatically align multiple takes of excerpts of a music piece against a performance of the whole work. We extend the approach of our previous work, based on sequential Montecarlo inference techniques, that was targeted at real-time alignment for score/audio following. The proposed approach is capable of producing partial alignments as well as identifying relevant regions in the partial results with regards to the reference, for better integration within a studio mix workflow. The approach is evaluated using data obtained from two recording sessions of classical music pieces, and we discuss its effectiveness for reducing manual work in a production chain.",ITA,education,Developed economies,"[20.229416, -34.133255]","[-15.194167, -19.028917]","[4.560103, -18.76191, -15.37747]","[-0.74300355, -24.38793, -0.5218054]","[10.703542, 5.855585]","[6.1472435, 0.77483726]","[11.5239525, 12.4668, -1.7419964]","[8.118906, 6.1238427, 10.774315]"
103,John Ashley Burgoyne;Jonathan Wild;Ichiro Fujinaga,An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.,2011,https://doi.org/10.5281/zenodo.1417547,John Ashley Burgoyne+McGill University>CAN>education;Jonathan Wild+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Audio chord recognition has attracted much interest in recent years, but a severe lack of reliable training data—both in terms of quantity and range of sampling—has hindered progress. Working with a team of trained jazz musicians, we have collected time-aligned transcriptions of the harmony in more than a thousand songs selected randomly from the Billboard “Hot 100” chart in the United States between 1958 and 1991. These transcriptions contain complete information about upper extensions and alterations as well as information about meter, phrase, and larger musical structure. We expect that these transcriptions will enable significant advances in the quality of training for audio-chord-recognition algorithms, and furthermore, because of an innovative sampling methodology, the data are usable as they stand for computational musicology. The paper includes some summary figures and statistics to help readers understand the scope of the data as well as information for obtaining the transcriptions for their own research.",CAN,education,Developed economies,"[55.53047, -8.4869995]","[-28.203098, 21.669275]","[30.478216, -11.332626, 13.759877]","[-24.570866, -1.8796211, 4.1144366]","[6.7971063, 8.573922]","[6.417992, 3.5797377]","[11.929712, 10.394879, 2.0852518]","[9.784004, 8.685569, 12.361035]"
104,Matt McVicar;Yizhao Ni;Tijl De Bie;Raúl Santos-Rodriguez,Leveraging Noisy Online Databases for Use in Chord Recognition.,2011,https://doi.org/10.5281/zenodo.1418311,Matt McVicar+University of Bristol>GBR>education;Yizhao Ni+University of Bristol>GBR>education;Tijl De Bie+University of Bristol>GBR>education;Raul Santos-Rodriguez+University Carlos III of Madrid>ESP>education,"The most significant problem faced by Machine Learning-based chord recognition systems is arguably the lack of high-quality training examples. In this paper, we address this problem by leveraging the availability of chord annotations from guitarist websites. We show that such annotations can be used as partial supervision of a semi-supervised chord recognition method—partial since accurate timing information is lacking. A particular challenge in the exploitation of these data is their low quality, potentially even leading to a performance degradation if used directly. We demonstrate however that a curriculum learning strategy can be used to automatically rank annotations according to their potential for improving the performance. Using this strategy, our experiments show a modest improvement for a simple major/minor chord alphabet, but a highly significant improvement for a much larger chord alphabet.",GBR,education,Developed economies,"[54.976875, -7.1787267]","[-32.470722, 22.796158]","[30.126032, -10.226088, 16.174494]","[-29.066544, -4.920039, 0.7817145]","[6.734792, 8.649654]","[5.9722676, 3.7968457]","[11.927791, 10.366946, 2.0817988]","[9.730577, 9.021385, 12.313743]"
105,Kazuyoshi Yoshii;Masataka Goto,A Vocabulary-Free Infinity-Gram Model for Nonparametric Bayesian Chord Progression Analysis.,2011,https://doi.org/10.5281/zenodo.1417389,Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents probabilistic n-gram models for symbolic chord sequences. To overcome the fundamental limitations in conventional models—that the model optimality is not guaranteed, that the value of n is fixed uniquely, and that a vocabulary of chord types (e.g., major, minor, · · · ) is defined in an arbitrary way—we propose a vocabulary-free infinity-gram model based on Bayesian nonparametrics. It accepts any combinations of notes as chord types and allows each chord appearing in a sequence to have an unbounded and variable-length context. All possibilities of n are taken into account when calculating the predictive probability of a next chord given a particular context, and when an unseen chord type emerges we can avoid out-of-vocabulary error by adaptively evaluating the 0-gram probability, i.e., the combinatorial probability of note components. Our experiments using Beatles songs showed that the predictive performance of the proposed model is better than that of the state-of-the-art models and that we could find stochastically-coherent chord patterns by sorting variable-length n-grams in a line according to their generative probabilities.",JPN,facility,Developed economies,"[58.67904, -2.0629575]","[-22.972847, 25.661081]","[28.341747, -17.253365, 20.524202]","[-19.509565, -0.7952362, 1.5876678]","[6.7644553, 8.756929]","[7.0161767, 3.4807174]","[11.824023, 10.453669, 2.1699352]","[9.495769, 8.0900135, 12.150732]"
106,Taemin Cho;Juan Pablo Bello,A Feature Smoothing Method for Chord Recognition Using Recurrence Plots.,2011,https://doi.org/10.5281/zenodo.1417557,Taemin Cho+New York University>USA>education;Juan P. Bello+New York University>USA>education,"In this paper, we propose a feature smoothing technique for chord recognition tasks based on repeated patterns within a song. By only considering repeated segments of a song, our method can smooth the features without losing chord boundary information and fine details of the original feature. While a similar existing technique requires several hard decisions such as beat quantization and segmentation, our method uses a simple pragmatic approach based on recurrence plot to decide which repeated parts to include in the smoothing process. This approach uses a more formal definition of the repetition search and allows shorter (“chord-size”) repeated segments to contribute to the feature improvement process. In our experiments, our method outperforms conventional and popular smoothing techniques (a moving average filter and a median filter). In particular, it shows a synergistic effect when used with the Viterbi decoder.",USA,education,Developed economies,"[54.89652, -4.724012]","[-29.558022, 19.470022]","[29.279387, -14.72574, 17.018297]","[-26.40722, -6.738369, 5.1807303]","[6.741092, 8.632518]","[6.260467, 3.5692525]","[11.870293, 10.44994, 2.0277054]","[9.807976, 8.868869, 12.158796]"
82,Benjamin Martin 0001;Pierre Hanna;Ta Vinh Thong;Myriam Desainte-Catherine;Pascal Ferraro,Exemplar-based Assignment of Large Missing Audio Parts using String Matching on Tonal Features.,2011,https://doi.org/10.5281/zenodo.1417977,"Benjamin Martin+LaBRI, Université de Bordeaux>FRA>education;Pierre Hanna+LaBRI, Université de Bordeaux>FRA>education;Vinh-Thong Ta+LaBRI, Université de Bordeaux>FRA>education;Pascal Ferraro+LaBRI, Université de Bordeaux>FRA>education;Myriam Desainte-Catherine+LaBRI, Université de Bordeaux>FRA>education","We propose a new approach for assigning audio data in large missing audio parts (from 1 to 16 seconds). Inspired by image inpainting approaches, the proposed method uses the repetitive aspect of music pieces on musical features to recover missing segments via an exemplar-based reconstruction. Tonal features combined with a string matching technique allows locating repeated segments accurately. The evaluation consists in performing on both musician and non-musician subjects listening tests of randomly reconstructed audio excerpts, and experiments highlight good results in assigning musically relevant parts. The contribution of this paper is twofold: bringing musical features to solve a signal processing problem in the case of large missing audio parts, and successfully applying exemplar-based techniques on musical signals while keeping a musical consistency on audio pieces.",FRA,education,Developed economies,"[-12.816324, -20.12992]","[-24.96862, -18.212776]","[3.0082, -9.593663, -17.675304]","[8.005767, -20.141262, -4.72346]","[12.120421, 7.743848]","[6.704494, 4.7130966]","[12.761777, 13.478999, 0.3682214]","[9.741769, 8.005159, 10.255198]"
30,Georgina Tryfou;Aki Härmä;Athanasios Mouchtaris,Tempo Estimation Based on Linear Prediction and Perceptual Modelling.,2011,https://doi.org/10.5281/zenodo.1416386,"Georgina Tryfou+Institute of Computer Science, Foundation for Research and Technology - Hellas (FORTH-ICS)>GRC>facility|University of Crete>GRC>education;Aki Härmä+Philips Research>EWN>company;Athanasios Mouchtaris+Institute of Computer Science, Foundation for Research and Technology - Hellas (FORTH-ICS)>GRC>facility|University of Crete>GRC>education","Many applications demand the automatic induction of the tempo of a musical excerpt. The tempo estimation systems follow a general scheme that consists of two main steps: the creation of a feature list and the detection of periodicities on this list. In this study, we propose a new method for the implementation of the first step, along with the addition of a final step that will enhance the tempo estimation procedure. The proposed method for the extraction of the feature list is based on Gammatone subspace analysis and Linear Prediction Error Filters (LPEFs). As a final step on the system, the application of a model that approximates the tempo perception by human listeners is proposed. The results of the evaluation indicate the proposed method compares favourably with other, state-of-the-art tempo estimation methods, using only one frame of the musical experts when most of the literature methods demand the processing of the whole piece.",GRC,facility,Developed economies,"[41.30453, -26.625689]","[-30.946426, -6.3123274]","[-0.017448682, -30.404984, 0.06342711]","[-7.241452, 10.324124, -11.609989]","[11.4833145, 4.3897014]","[5.1074553, 1.81257]","[10.872592, 13.342661, -2.8727107]","[7.3774924, 6.9961424, 10.956322]"
35,Sihyun Joo;Sanghun Park;Seokhwan Jo;Chang D. Yoo,Melody Extraction based on Harmonic Coded Structure.,2011,https://doi.org/10.5281/zenodo.1417243,Sihyun Joo+Korea Advanced Institute of Science and Technology>KOR>education;Sanghun Park+Korea Advanced Institute of Science and Technology>KOR>education;Seokhwan Jo+Korea Advanced Institute of Science and Technology>KOR>education;Chang D. Yoo+Korea Advanced Institute of Science and Technology>KOR>education,"This paper considers a melody extraction algorithm that estimates the melody in polyphonic audio using the harmonic coded structure (HCS) to model melody in the minimum mean-square-error (MMSE) sense. The HCS is harmonically modulated sinusoids with the amplitudes defined by a set of codewords. The considered algorithm performs melody extraction in two steps: i) pitch-candidate estimation and ii) pitch-sequence identification. In the estimation step, pitch candidates are estimated such that the HCS best represents the polyphonic audio in the MMSE sense. In the identification step, a melody line is selected from many possible pitch sequences based on the properties of melody line. Posterior to the melody line selection, a smoothing process is applied to refine spurious pitches and octave errors. The performance of the algorithm is evaluated and compared using the ADC04 and the MIREX05 dataset. The results show that the performance of the proposed algorithm is better than or comparable to other algorithms submitted to MIREX2009.",KOR,education,Developing economies,"[5.623441, -12.516822]","[-2.9201412, -9.456883]","[15.472018, 4.8532095, -3.641729]","[5.9232407, -7.687285, -13.891221]","[10.089236, 9.926448]","[6.7578893, 2.6335926]","[11.028835, 14.867432, -0.387257]","[9.054742, 8.031644, 11.137264]"
28,Marc J. Velasco;Edward W. Large,Pulse Detection in Syncopated Rhythms Using Neural Oscillators.,2011,https://doi.org/10.5281/zenodo.1417519,Marc J. Velasco+Florida Atlantic University>USA>education;Edward W. Large+Florida Atlantic University>USA>education,"""Pulse and meter are remarkable in part because these perceived periodicities can arise from rhythmic stimuli that are not periodic. This phenomenon is most striking in syncopated rhythms, found in many genres of music, including music of non-Western cultures. In general, syncopated rhythms may have energy at frequencies that do not correspond to perceived pulse or meter, and perceived metrical frequencies that are weak or absent in the objective rhythmic stimulus. In this paper, we consider syncopated rhythms that contain little or no energy at the pulse frequency. We used 16 rhythms (3 simple, 13 syncopated) to test a model of pulse/meter perception based on nonlinear resonance, comparing the nonlinear resonance model with a linear analysis. Both models displayed the ability to differentiate between duple and triple meters, however, only the nonlinear model exhibited resonance at the pulse frequency for the most challenging syncopated rhythms. This result suggests that nonlinear resonance may provide a viable approach to pulse detection in syncopated rhythms.""",USA,education,Developed economies,"[44.27847, -31.952343]","[-23.901358, 2.4429808]","[6.142055, -30.610056, 1.2567363]","[-0.8939354, 16.131033, -6.732229]","[11.205138, 4.490269]","[5.9892325, 1.674387]","[10.724958, 13.505499, -2.3921094]","[8.03781, 6.811086, 11.755228]"
29,Fu-Hai Frank Wu;Tsung-Chi Lee;Jyh-Shing Roger Jang;Kaichun K. Chang;Chun-Hung Lu;Wen-Nan Wang,A Two-Fold Dynamic Programming Approach to Beat Tracking for Audio Music with Time-Varying Tempo.,2011,https://doi.org/10.5281/zenodo.1416782,Fu-Hai Frank Wu+National Tsing Hua University>TWN>education;Tsung-Chi Lee+National Tsing Hua University>TWN>education;Jyh-Shing Roger Jang+National Tsing Hua University>TWN>education;Kaichun K. Chang+King's College London>GBR>education;Chun Hung Lu+Institute For Information Industry>TWN>facility;Wen Nan Wang+Institute For Information Industry>TWN>facility,"Automatic beat tracking and tempo estimation are challenging tasks, especially for audio music with time-varying tempo. This paper proposes a two-fold dynamic programming (DP) approach to deal with beat tracking with time-varying tempo. In particular, the first DP computes the tempo curve from the tempogram. The second DP identifies the optimum beat positions from the novelty and tempo curves. Experimental results demonstrate satisfactory performance for music with significant tempo variations. The proposed approach was submitted to the task of audio beat tracking in MIREX 2010 and was ranked no. 1 for 6 performance indices out of 10, for the dataset with variable tempo.",TWN,education,Developing economies,"[33.896435, -36.166138]","[-27.458344, -3.6065297]","[10.215473, -28.026724, -7.426452]","[-5.7954507, 8.150237, -7.6315117]","[10.476349, 4.3739414]","[5.310524, 1.8197434]","[10.228139, 12.833991, -2.1973078]","[7.461692, 6.868142, 11.072184]"
0,Karin Dressler,An Auditory Streaming Approach for Melody Extraction from Polyphonic Music.,2011,https://doi.org/10.5281/zenodo.1416112,Karin Dressler+Fraunhofer Institute for Digital Media Technology IDMT>DEU>facility,"This paper proposes an efficient approach for the identification of the predominant voice from polyphonic musical audio. The algorithm implements an auditory streaming model which builds upon tone objects and salient pitches. The formation of voices is based on the regular update of the frequency and the magnitude of so called streaming agents, which aim at salient tones or pitches close to their preferred frequency range. Streaming agents which succeed to assemble a big magnitude start new voice objects, which in turn add adequate tones. The algorithm was evaluated as part of a melody extraction system during the MIREX audio melody extraction evaluation, where it gained very good results in the voicing detection and overall accuracy.",DEU,facility,Developed economies,"[5.7332735, -14.185242]","[-4.1263523, -10.05591]","[15.937866, 3.083463, -5.399378]","[4.4650435, -10.021485, -13.358617]","[9.948678, 9.94793]","[6.748325, 2.622353]","[10.996674, 14.734692, -0.28112885]","[9.058201, 8.023163, 11.085713]"
1,Yu-Ren Chien;Hsin-Min Wang;Shyh-Kang Jeng,An Acoustic-Phonetic Approach to Vocal Melody Extraction.,2011,https://doi.org/10.5281/zenodo.1416450,Yu-Ren Chien+National Taiwan University>TWN>education|Academia Sinica>Unknown>Unknown;Hsin-Min Wang+Academia Sinica>Unknown>Unknown|National Taiwan University>TWN>education;Shyh-Kang Jeng+National Taiwan University>TWN>education,"This paper addresses the problem of extracting vocal melodies from polyphonic audio. In short-term processing, a timbral distance between each pitch contour and the space of human voice is measured, so as to isolate any vocal pitch contour. Computation of the timbral distance is based on an acoustic-phonetic parametrization of human voiced sound. Long-term processing organizes short-term procedures in such a manner that relatively reliable melody segments are determined first. Tested on vocal excerpts from the ADC 2004 dataset, the proposed system achieves an overall transcription accuracy of 77%.",TWN,education,Developing economies,"[4.4392233, -15.061888]","[-4.252074, -11.090743]","[19.26829, 4.32706, -3.4267592]","[4.9479403, -10.997683, -11.870487]","[9.843129, 10.095821]","[6.7689033, 2.585816]","[10.965464, 14.776092, -0.09175156]","[9.051354, 7.9799447, 11.143553]"
2,Ryunosuke Daido;Seongjun Hahm;Masashi Ito;Shozo Makino;Akinori Ito,A System for Evaluating Singing Enthusiasm for Karaoke.,2011,https://doi.org/10.5281/zenodo.1417343,Ryunosuke Daido+Tohoku University>JPN>education;Seong-Jun Hahm+Tohoku Institute of Technology>JPN>education;Masashi Ito+Tohoku Institute of Technology>JPN>education;Shozo Makino+Tohoku Bunka Gakuen University>JPN>education;Akinori Ito+Tohoku University>JPN>education,"Evaluation of singing skill is a popular function of karaoke machines. Here, we introduce a different aspect of evaluating the singing voice of an amateur singer: “enthusiasm”. First, we investigated whether human listeners can evaluate enthusiasm consistently and whether the listener’s perception matches the singer’s enthusiasm. We then identified three acoustic features relevant to the perception of enthusiasm: A-weighted power, “fall-down”, and vibrato extent. Finally, we developed a system for evaluating singing enthusiasm using these features, and obtained a correlation coefficient of 0.65 between the system output and human evaluation.",JPN,education,Developed economies,"[-4.1167145, -32.925606]","[50.659977, -17.206734]","[14.299846, 11.722643, -9.444657]","[5.5576153, 19.620892, 9.137341]","[10.333319, 11.224823]","[12.652555, 4.2205763]","[11.52637, 15.348775, 0.502522]","[13.761387, 4.7922163, 10.371751]"
4,Erdem Unal;Elaine Chew;Panayiotis G. Georgiou;Shrikanth Narayanan,A Preplexity Based Cover Song Matching System for Short Length Queries.,2011,https://doi.org/10.5281/zenodo.1415134,"Erdem Unal+TÜBİTAK BİLGEM>TUR>facility;Elaine Chew+Queen Mary, University of London>GBR>education;Panayiotis Georgiou+University of Southern California>USA>education;Shrikanth S. Narayanan+University of Southern California>USA>education","A music retrieval system that matches a short length music query with its variations in a database is proposed. In order to avoid the negative effects of different orchestration and performance style and tempo on transcription and matching, a mid-level representation schema and a tonal modeling approach is used. The mid-level representation approach transcribes the music pieces into a sequence of music tags corresponding to major and minor triad labels. From the transcribed sequence, n-gram models are built to statistically represent the harmonic progression. For retrieval, a perplexity based similarity score is calculated between each n-gram in the database and that for the query. The retrieval performance of the system is presented for a dataset of 2000 classical music pieces modeled using n-grams of sizes 2 through 6. We observe improvements in retrieval performance with increasing query length and n-gram order. The improvement converges to a little over one for all query lengths tested when n reaches 6.",TUR,facility,Developing economies,"[6.3416624, 41.41488]","[15.376981, 15.404889]","[-0.29613084, 12.283045, -19.781406]","[9.709595, -6.980867, 11.137903]","[16.067406, 11.071959]","[9.463057, 0.73629546]","[12.890307, 17.261265, -0.41135824]","[10.977169, 6.0026636, 13.046201]"
5,Cristina de la Bandera;Ana M. Barbancho;Lorenzo J. Tardón;Simone Sammartino;Isabel Barbancho,Humming Method for Content-Based Music Information Retrieval.,2011,https://doi.org/10.5281/zenodo.1416610,Cristina de la Bandera+Universidad de Málaga>ESP>education;Ana M. Barbancho+Universidad de Málaga>ESP>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Simone Sammartino+Universidad de Málaga>ESP>education;Isabel Barbancho+Universidad de Málaga>ESP>education,"""In this paper a humming method for music information retrieval is presented. The system uses a database with real songs and does not need another type of symbolic representation of them. The system employs an original fingerprint based on chroma vectors to characterize the humming and the references songs. With this fingerprint, it is possible to get the hummed songs without needed of transcription of the notes of the humming or of the songs. The system showed a good performance on Pop/Rock and Spanish folk music.""",ESP,education,Developed economies,"[-17.097286, 22.483513]","[11.653338, 10.695381]","[-10.75922, 6.317252, -13.550228]","[2.8017104, -11.9655075, 13.070161]","[14.027081, 7.6531544]","[8.832021, 0.5614872]","[13.63929, 14.878101, -2.3405142]","[10.445745, 5.7967215, 12.873816]"
6,Brian McFee;Gert R. G. Lanckriet,Large-scale music similarity search with spatial trees.,2011,https://doi.org/10.5281/zenodo.1414930,"Brian McFee+University of California, San Diego>USA>education|University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education|University of California, San Diego>USA>education","Many music information retrieval tasks require finding the nearest neighbors of a query item in a high-dimensional space. However, the complexity of computing nearest neighbors grows linearly with size of the database, making exact retrieval impractical for large databases. We investigate modern variants of the classical KD-tree algorithm, which efficiently index high-dimensional data by recursive spatial partitioning. Experiments on the Million Song Dataset demonstrate that content-based similarity search can be significantly accelerated by the use of spatial partitioning structures.",USA,education,Developed economies,"[-8.667668, 16.344223]","[21.845112, 15.450797]","[-2.0109131, 8.079327, -8.698758]","[12.403166, -7.104676, 7.5490494]","[13.309329, 9.039271]","[9.918141, 1.1086025]","[13.533481, 15.123823, -1.0729096]","[11.4602165, 6.1428175, 12.996394]"
7,Silvia García-Díez;Marco Saerens;Mathieu Senelle;François Fouss,A simple-cycles weighted kernel based on harmony structure for similarity retrieval.,2011,https://doi.org/10.5281/zenodo.1415172,Silvia García-Díez+Université catholique de Louvain>BEL>education;Marco Saerens+Université catholique de Louvain>BEL>education;Mathieu Senelle+Université catholique de Louvain – Site de Mons>BEL>education;François Fouss+Université catholique de Louvain – Site de Mons>BEL>education,"This paper introduces a novel methodology for music similarity retrieval based on chord progressions. From each chord progression, a directed labeled graph containing the interval transitions is extracted. This graph will be used as input for a graph comparison method based on simple cycles – cycles where the only repeated nodes are the first and the last one. In music, simple cycles represent the repetitive sub-structures of, e.g., modern pop/rock music. By means of a kernel function [10] whose feature space is spanned by these simple cycles, we obtain a kernel matrix (similarity matrix) which can then be used in music similarity retrieval tasks. The resulting algorithm has a time complexity of O(n+m(c+1)), where n is the number of vertices, m is the number of edges, and c is the number of simple cycles. The performance of our method is tested on both an idiom retrieval task, and a cover song retrieval task. Empirical results show the improved accuracy of our method in comparison with other string-matching, and graph-comparison methods used as baseline.",BEL,education,Developed economies,"[-12.161583, 15.392663]","[18.811657, 12.029023]","[-4.309686, 11.540273, -9.341783]","[9.117755, -2.7080383, 7.8444037]","[13.219127, 8.723475]","[9.730347, 1.5871068]","[13.520625, 14.8449955, -1.0568124]","[11.303281, 6.501969, 12.734374]"
8,W. Bas de Haas;José Pedro Magalhães;Remco C. Veltkamp;Frans Wiering,HarmTrace: Improving Harmonic Similarity Estimation Using Functional Harmony Analysis.,2011,https://doi.org/10.5281/zenodo.1417063,W. Bas de Haas+Utrecht University>NLD>education;José Pedro Magalhães+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education,"Harmony theory has been essential in composing, analysing, and performing music for centuries. Since Western tonal harmony exhibits a considerable amount of structure and regularity, it lends itself to formalisation. In this paper we present HARMTRACE, a system that, given a sequence of symbolic chord labels, automatically derives the harmonic function of a chord in its tonal context. Among other applications, these functional annotations can be used to improve the estimation of harmonic similarity in a local alignment of two annotated chord sequences. We evaluate HARMTRACE and three other harmonic similarity measures on a corpus of 5,028 chord sequences that contains harmonically related pieces. The results show that HARMTRACE outperforms all three other similarity measures, and that information about the harmonic function of a chord improves the estimation of harmonic similarity between two chord sequences.",NLD,education,Developed economies,"[27.872114, 22.047626]","[-22.985853, 18.557775]","[-2.0519047, -13.021687, 31.847141]","[-22.10492, -3.5837808, 8.423386]","[9.663745, 9.2663765]","[7.66335, 2.345303]","[11.711144, 13.976723, -0.8777144]","[10.058564, 8.045072, 12.543551]"
9,Daniel Wolff;Tillman Weyde,Adapting Metrics for Music Similarity Using Comparative Ratings.,2011,https://doi.org/10.5281/zenodo.1416440,Daniel Wolff+City University London>GBR>education|Unknown>Unknown>Unknown;Tillman Weyde+City University London>GBR>education|Unknown>Unknown>Unknown,"Understanding how we relate and compare pieces of music has been a topic of great interest in musicology as well as for business applications, such as music recommender systems. The way music is compared seems to vary among both individuals and cultures. Adapting a generic model to user ratings is useful for personalisation and can help to better understand such differences. This paper presents an approach to use machine learning techniques for analysing user data that specifies song similarity. We explore the potential for learning generalisable similarity measures with two state-of-the-art algorithms for learning metrics. We use the audio clips and user ratings in the MagnaTagATune dataset, enriched with genre annotations from the Magnatune label.",GBR,education,Developed economies,"[-5.596809, 13.024595]","[33.893417, 9.617444]","[-6.4876404, 11.697382, 0.40668902]","[13.784536, 3.8025646, 12.418865]","[13.239738, 9.37583]","[11.473402, 2.429806]","[13.71747, 15.22253, -0.6484143]","[12.943302, 6.169529, 12.399926]"
10,Dominik Schnitzer;Arthur Flexer;Markus Schedl;Gerhard Widmer,Using Mutual Proximity to Improve Content-Based Audio Similarity.,2011,https://doi.org/10.5281/zenodo.1417979,Dominik Schnitzer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Markus Schedl+Johannes Kepler University>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education,"This work introduces Mutual Proximity, an unsupervised method which transforms arbitrary distances to similarities computed from the shared neighborhood of two data points. This reinterpretation aims to correct inconsistencies in the original distance space, like the hub phenomenon. Hubs are objects which appear unwontedly often as nearest neighbors in predominantly high-dimensional spaces. We apply Mutual Proximity to a widely used and standard content-based audio similarity algorithm. The algorithm is known to be negatively affected by the high number of hubs it produces. We show that without a modification of the audio similarity features or inclusion of additional knowledge about the datasets, applying Mutual Proximity leads to a significant increase of retrieval quality: (1) hubs decrease and (2) the k-nearest-neighbor classification rates increase significantly. The results of this paper show that taking the mutual neighborhood of objects into account is an important aspect which should be considered for this class of content-based audio similarity algorithms.",AUT,facility,Developed economies,"[-6.4349074, -20.070803]","[26.684366, 9.272329]","[-0.015322257, -4.864413, -24.034369]","[17.59591, -4.8567266, 7.094536]","[12.793457, 9.110812]","[10.580958, 2.2434225]","[12.194835, 13.655141, 0.80455786]","[12.072437, 6.378894, 12.548254]"
11,Ju-Chiang Wang;Hung-Shin Lee;Hsin-Min Wang;Shyh-Kang Jeng,Learning the Similarity of Audio Music in Bag-of-frames Representation from Tagged Music Data.,2011,https://doi.org/10.5281/zenodo.1417219,Ju-Chiang Wang+National Taiwan University>TWN>education|Academia Sinica>Unknown>facility;Hung-Shin Lee+National Taiwan University>TWN>education|Academia Sinica>Unknown>facility;Hsin-Min Wang+Academia Sinica>Unknown>facility;Shyh-Kang Jeng+National Taiwan University>TWN>education,"Due to the cold-start problem, measuring the similarity between two pieces of audio music based on their low-level acoustic features is critical to many Music Information Retrieval (MIR) systems. In this paper, we apply the bag-of-frames (BOF) approach to represent low-level acoustic features of a song and exploit music tags to help improve the performance of the audio-based music similarity computation. We first introduce a Gaussian mixture model (GMM) as the encoding reference for BOF modeling, then we propose a novel learning algorithm to minimize the similarity gap between low-level acoustic features and music tags with respect to the prior weights of the pre-trained GMM. The results of audio-based query-by-example MIR experiments on the MajorMiner and Magnatagatune datasets demonstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ BOF modeling.",TWN,education,Developing economies,"[-8.433792, 13.358232]","[29.204111, -7.001159]","[-4.3661804, 15.458568, 1.4187425]","[22.416393, 1.1690676, 0.7923529]","[13.234248, 9.200546]","[10.805632, 3.0624678]","[13.777051, 14.845676, -0.5538177]","[12.375687, 6.5491576, 11.865771]"
12,Teppo E. Ahonen;Kjell Lemström;Simo Linkola,"Compression-based Similarity Measures in Symbolic, Polyphonic Music.",2011,https://doi.org/10.5281/zenodo.1414816,Teppo E. Ahonen+University of Helsinki>FIN>education;Kjell Lemström+University of Helsinki>FIN>education;Simo Linkola+University of Helsinki>FIN>education,"We present a novel compression-based method for measuring similarity between sequences of symbolic, polyphonic music. The method is based on mapping the values of binary chromagrams extracted from MIDI files to tonal centroids, then quantizing the tonal centroid representation values to sequences, and finally measuring the similarity between the quantized sequences using Normalized Compression Distance (NCD). The method is comprehensively evaluated with a test set of classical music variations, and the highest achieved precision and recall values suggest that the proposed method can be applied for similarity measuring. Also, we analyze the performance of the method and discuss what should be taken into consideration when applying the method for measurement tasks.",FIN,education,Developed economies,"[2.938961, 15.702723]","[20.312418, 6.33947]","[-2.3658013, 1.9503063, -0.37341118]","[10.149328, -1.6675533, 2.5894263]","[12.675015, 9.331202]","[9.80389, 2.1490126]","[13.05492, 15.042336, -0.732888]","[11.273665, 7.024732, 12.4588375]"
13,Dmitry Bogdanov;Perfecto Herrera,How Much Metadata Do We Need in Music Recommendation? A Subjective Evaluation Using Preference Sets.,2011,https://doi.org/10.5281/zenodo.1415104,Dmitry Bogdanov+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"In this work we consider distance-based approaches to music recommendation, relying on an explicit set of music tracks provided by the user as evidence of his/her music preferences. Firstly, we propose a purely content-based approach, working on low-level (timbral, temporal, and tonal) and inferred high-level semantic descriptions of music. Secondly, we consider its simple refinement by adding a minimum amount of genre metadata. We compare the proposed approaches with one content-based and three metadata-based baselines. As such, we consider content-based approach working on inferred semantic descriptors, a tag-based recommender exploiting artist tags, a commercial black-box recommender partially employing collaborative filtering information, and a simple genre-based random recommender. We conduct a listening experiment with 19 participants. The obtained results reveal that although the low-level/semantic content-based approach does not achieve the performance of the baseline working exclusively on the inferred semantic descriptors, the proposed refinement provides significant improvement in the listeners’ satisfaction comparable with metadata-based approaches, and surpasses these approaches by the number of novel relevant recommendations. We conclude that the proposed content-based approach refined by simple genre metadata is suited for music discovery not only in the long-tail but also within popular music items.",ESP,education,Developed economies,"[-45.789368, 24.336851]","[38.222652, 12.492047]","[-11.243053, 24.845938, -8.134661]","[16.183958, 7.550692, 13.943174]","[15.822377, 9.147963]","[12.410681, 2.1512735]","[15.758682, 15.586409, -1.4017861]","[13.534315, 5.463789, 12.411456]"
3,Felix Weninger;Martin Wöllmer;Björn W. Schuller,"Automatic Assessment of Singer Traits in Popular Music: Gender, Age, Height and Race.",2011,https://doi.org/10.5281/zenodo.1417821,Felix Weninger+Technische Universität München>DEU>education;Martin Wöllmer+Technische Universität München>DEU>education;Björn Schuller+Technische Universität München>DEU>education,"We investigate fully automatic recognition of singer traits, i. e., gender, age, height and ‘race’ of the main performing artist(s) in recorded popular music. Monaural source separation techniques are combined to simultaneously enhance harmonic parts and extract the leading voice. For evaluation the UltraStar database of 581 pop music songs with 516 distinct singers is chosen. Extensive test runs with Long Short-Term Memory sequence classification reveal that binary classification of gender, height, race and age reaches up to 89.6, 72.1, 63.3 and 57.6 % unweighted accuracy on beat level in unseen test data.",DEU,education,Developed economies,"[-13.804465, -35.90747]","[10.330564, -19.845123]","[14.520372, 15.171862, -20.001303]","[12.84027, -5.540832, -18.155869]","[10.263009, 11.583473]","[8.429602, 3.726949]","[11.488532, 15.586305, 0.67277026]","[10.443348, 8.019551, 10.0007]"
15,Jin Ha Lee,How Similar Is Too Similar?: Exploring Users' Perceptions of Similarity in Playlist Evaluation.,2011,https://doi.org/10.5281/zenodo.1417125,Jin Ha Lee+University of Washington>USA>education,"The Audio Music Similarity and Retrieval (AMS) task in the annual Music Information Retrieval eXchange relies on human-evaluation. One limitation of the current design of AMS is that evaluators are provided with scarce contextual information as to why they are evaluating the similarity of the songs and how this information will be used. This study explores the potential use of AMS results for generating playlists based on similarity. We asked participants to listen to a subset of results from the 2010 AMS task and evaluate the set of candidates generated by the algorithms as a playlist generated from a seed song (the query). We found that while similarity does affect how people feel about the candidate set as a playlist, other factors such as variety, metadata, personal preference, familiarity, mix of familiar and new music, etc. also strongly affect users' perceptions of playlist quality as well. We discuss six user behaviors in detail and the implications for the AMS evaluation task.",USA,education,Developed economies,"[-40.293297, 36.62059]","[36.794067, 19.42417]","[-5.247718, 26.788136, -4.463651]","[12.764345, 5.582623, 18.517479]","[15.970653, 8.392438]","[12.362816, 1.5447019]","[16.31228, 14.927717, -1.6301916]","[13.108536, 4.9854426, 12.680447]"
14,Yajie Hu;Mitsunori Ogihara,NextOne Player: A Music Recommendation System Based on User Behavior.,2011,https://doi.org/10.5281/zenodo.1418301,Yajie Hu+University of Miami>USA>education|University of Miami>USA>education;Mitsunori Ogihara+University of Miami>USA>education|University of Miami>USA>education,"We present a new approach to recommend suitable tracks from a collection of songs to the user. The goal of the system is to recommend songs that are favored by the user, are fresh to the user’s ear, and fit the user’s listening pattern. We use “Forgetting Curve” to assess freshness of a song and evaluate “favoredness” using user log. We analyze user’s listening pattern to estimate the level of interest of the user in the next song. Also, we treat user behavior on the song being played as feedback to adjust the recommendation strategy for the next one. We develop an application to evaluate our approach in the real world. The user logs of trial volunteers show good performance of the proposed method.",USA,education,Developed economies,"[-44.928856, 27.190742]","[38.577946, 17.346058]","[-10.509468, 24.61092, -14.295655]","[16.357656, 5.786962, 18.24567]","[15.953767, 9.241119]","[12.343303, 1.9156792]","[15.81169, 15.754626, -1.5276657]","[13.431491, 5.2262206, 12.793315]"
26,Juhan Nam;Jiquan Ngiam;Honglak Lee;Malcolm Slaney,A Classification-Based Polyphonic Piano Transcription Approach Using Learned Feature Representations.,2011,https://doi.org/10.5281/zenodo.1418351,Juhan Nam+Stanford University>USA>education;Jiquan Ngiam+Stanford University>USA>education;Honglak Lee+University of Michigan>USA>education;Malcolm Slaney+Yahoo! Research>USA>company,"Recently unsupervised feature learning methods have shown great promise as a way of extracting features from high dimensional data, such as image or audio. In this paper, we apply deep belief networks to musical data and evaluate the learned feature representations on classification-based polyphonic piano transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three public piano datasets. The results show that the learned features outperform the baseline features, and also our method gives significantly better frame-level accuracy than other state-of-the-art music transcription methods.",USA,education,Developed economies,"[30.73955, -5.9341674]","[-28.471237, -26.816154]","[14.594135, -3.2865047, 15.5606365]","[-5.840888, -9.474027, -12.307647]","[9.705878, 7.3000627]","[8.184145, 4.9326706]","[12.018247, 11.496715, -0.20318523]","[9.520947, 6.7970896, 9.277953]"
24,Peter van Kranenburg;Dániel Péter Biró;Steven R. Ness;George Tzanetakis,A Computational Investigation of Melodic Contour Stability in Jewish Torah Trope Performance Traditions.,2011,https://doi.org/10.5281/zenodo.1415612,Peter van Kranenburg+Meertens Institute>Unknown>Unknown;Dániel Péter Bíró+University of Victoria>CAN>education;Steven Ness+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"The cantillation signs of the Jewish Torah trope are of particular interest to chant scholars interested in the gradual transformation of oral music performance into notation. Each sign, placed above or below the text, acts as a “melodic idea” which either connects or divides words in order to clarify the syntax, punctuation and, in some cases, meaning of the text. Unlike standard music notation, the interpretations of each sign are flexible and influenced by regional traditions, practices of given Jewish communities, larger musical influences beyond Jewish communities, and improvisatory elements incorporated by a given reader. In this paper we describe our collaborative work in developing and using computational tools to assess the stability of melodic formulas of cantillation signs based on two different performance traditions. We also show that a musically motivated alignment algorithm obtains better results than the more commonly used dynamic time warping method for calculating similarity between pitch contours. Using a participatory design process our team, which includes a domain expert, has developed an interactive web-based interface that enables researches to explore aurally and visually chant recordings and explore the relations between signs, gestures and musical representations.",Unknown,Unknown,Unknown,"[5.836553, -4.653376]","[4.8122206, -6.4587984]","[7.2470055, 5.544769, -5.213108]","[6.18266, -10.012501, 0.20170847]","[10.886745, 10.130401]","[8.206062, 1.5477346]","[11.674227, 15.308042, -0.8894684]","[9.87603, 7.0156407, 12.544115]"
23,Joan Serrà;Gopala K. Koduri;Marius Miron;Xavier Serra,Assessing the Tuning of Sung Indian Classical Music.,2011,https://doi.org/10.5281/zenodo.1415102,Joan Serrà+Universitat Pompeu Fabra>ESP>education;Gopala K. Koduri+Universitat Pompeu Fabra>ESP>education;Marius Miron+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"The issue of tuning in Indian classical music has been, historically, a matter of theoretical debate. In this paper, we study its contemporary practice in sung performances of Carnatic and Hindustani music following an empiric and quantitative approach. To do so, we select stable fundamental frequencies, estimated via a standard algorithm, and construct interval histograms from a pool of recordings. We then compare such histograms against the ones obtained for different music sources and against the theoretical values derived from 12-note just intonation and equal temperament. Our results evidence that the tunings in Carnatic and Hindustani music differ, the former tending to a just intonation system and the latter having much equal-tempered influences. Carnatic music also presents signs of a more continuous distribution of pitches. Further subdivisions of the octave are partially investigated, finding no strong evidence of them.",ESP,education,Developed economies,"[5.6431293, 2.9840593]","[1.6999344, -17.476324]","[8.548353, 1.9006876, -9.530563]","[13.403775, -14.617543, -9.101036]","[11.342163, 10.256044]","[7.3351836, 1.3405784]","[11.922879, 15.476233, -1.1590005]","[8.910854, 7.148384, 12.446659]"
22,Xavier Serra,A Multicultural Approach in Music Information Research.,2011,https://doi.org/10.5281/zenodo.1416592,Xavier Serra+Universitat Pompeu Fabra>ESP>education,"Our information technologies do not respond to the world's multicultural reality; in fact, we are imposing the paradigms of our market-driven western culture also on IT, thus facilitating the access of a small part of the world’s information to a small part of the world's population. The current IT research efforts may even make it worse, and future IT will accentuate this information bias. Most IT research is being carried out with a western centered approach and as a result, most of our data models, cognition models, user models, interaction models, ontologies, etc., are culturally biased. This fact is quite evident in music information research, since, despite the world's richness in terms of musical culture, most research is centered on CDs and metadata of western commercial music. This is the motivation behind a large and ambitious project funded by the European Research Council entitled ""CompMusic: Computational Models for the discovery of the world's music."" In this paper we present the ideas supporting this project, the challenges that we want to work on, and the proposed approaches to tackle these challenges.",ESP,education,Developed economies,"[-28.47115, 21.468267]","[21.857214, 36.317657]","[-21.597902, 11.2815485, -6.84214]","[-0.7903331, 0.5246544, 24.999004]","[14.603699, 8.302384]","[11.29835, 0.0833755]","[14.701803, 14.900436, -1.7342374]","[12.160621, 4.6995864, 12.003089]"
25,Joren Six;Olmo Cornelis,Tarsos - a Platform to Explore Pitch Scales in Non-Western and Western Music.,2011,https://doi.org/10.5281/zenodo.1418355,"Joren Six+Royal Academy of Fine Arts & Royal Conservatory, University College Ghent>BEL>education;Olmo Cornelis+Royal Academy of Fine Arts & Royal Conservatory, University College Ghent>BEL>education","This paper presents Tarsos, a modular software platform to extract and analyze pitch and scale organization in music, especially geared towards the analysis of non-Western music. Tarsos aims to be a user-friendly, graphical tool to explore tone scales and pitch organization in music of the world. With Tarsos pitch annotations are extracted from an audio signal that are then processed to form musicologically meaningful representations. These representations cover more than the typical Western 12 pitch classes, since a fine-grained resolution of 1200 cents is used. Both scales with and without octave equivalence can be displayed graphically. The Tarsos API creates opportunities to analyse large sets of - ethnic - music automatically. The graphical user interface can be used for detailed, manually adjusted analysis of specific songs. Several output modalities make Tarsos an interesting tool for musicological analysis, educational purposes and even for artistic productions.",BEL,education,Developed economies,"[-3.8974338, 24.080189]","[-6.761414, 23.041586]","[-0.55178094, 1.2151849, -20.54057]","[-9.91585, 2.332154, 9.438994]","[11.963649, 9.578411]","[8.657021, 1.8071148]","[12.534686, 14.702821, -1.3497452]","[9.878836, 6.382833, 11.728027]"
20,Guangyu Xia;Dawen Liang;Roger B. Dannenberg;Mark Harvilla,"Segmentation, Clustering, and Display in a Personal Audio Database for Musicians.",2011,https://doi.org/10.5281/zenodo.1418185,Guangyu Xia+Carnegie Mellon University>USA>education;Dawen Liang+Carnegie Mellon University>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education;Mark J. Harvilla+Carnegie Mellon University>USA>education,"Managing music audio databases for practicing musicians presents new and interesting challenges. We describe a systematic investigation to provide useful capabilities to musicians both in rehearsal and when practicing alone. Our goal is to allow musicians to automatically record, organize, and retrieve rehearsal (and other) audio to facilitate review and practice (for example, playing along with difficult passages). We introduce a novel music classification system based on Eigenmusic and Adaboost to separate rehearsal recordings into segments, an unsupervised clustering and alignment process to organize segments, and a digital music display interface that provides both graphical input and output in terms of conventional music notation.",USA,education,Developed economies,"[-19.02874, 10.735353]","[-11.091468, -4.7766023]","[-16.702406, -2.5809336, -11.717634]","[-2.134115, -19.135967, 3.6072216]","[12.708538, 7.9112544]","[6.749232, 0.85701185]","[14.018776, 13.876523, -1.4068043]","[8.9891205, 5.7956514, 11.076949]"
19,Qingmei Xiao;Motoyuki Suzuki;Kenji Kita,Fast Hamming Space Search for Audio Fingerprinting Systems.,2011,https://doi.org/10.5281/zenodo.1418143,Qingmei Xiao+The University of Tokushima>JPN>education;Motoyuki Suzuki+The University of Tokushima>JPN>education;Kenji Kita+The University of Tokushima>JPN>education,"In music information retrieval, a huge search space has to be explored because a query audio clip can start at any position of any music in the database, and also a query is often corrupted by significant noise and distortion. Audio fingerprints have recently attracted much attention in music information retrieval, for they provide a compact representation of the perceptually relevant parts of audio signals. In this paper, we propose an extremely fast method of exploring a huge Hamming space for audio fingerprinting systems. The effectiveness of the proposed method has been evaluated by experiments using a database of 8,740 songs.",JPN,education,Developed economies,"[-16.070864, -25.743984]","[19.874252, 13.742817]","[4.350079, -12.003795, -23.598103]","[15.167041, -10.856387, 6.9071493]","[9.079035, 4.4361787]","[8.61109, 0.011740681]","[10.579986, 11.503176, -1.9423505]","[10.319215, 5.438058, 13.004386]"
18,Hendrik Schreiber;Peter Grosche;Meinard Müller,A Re-ordering Strategy for Accelerating Index-based Audio Fingerprinting.,2011,https://doi.org/10.5281/zenodo.1417607,Hendrik Schreiber+tagtraum industries>DEU>company;Peter Grosche+Saarland University>DEU>education|MPI Informatik>DEU>facility;Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility,"The Haitsma/Kalker audio fingerprinting system has been in use for years, but its search algorithm’s scalability has not been researched very well. In this paper we show that by simple re-ordering of the query fingerprint’s sub-prints in the index-based retrieval step, the overall search performance can be increased significantly. Furthermore, we show that combining longer fingerprints with re-ordering can lead to even higher performance gains, up to a factor of 9.8. The proposed re-ordering scheme is based on the observation that sub-prints, which are elements of n-runs of identical consecutive sub-prints, have a higher survival rate in distorted copies of a signal (e.g. after mp3 compression) than other sub-prints.",DEU,company,Developed economies,"[-15.756471, -25.325682]","[25.74361, -24.23267]","[4.929273, -11.681529, -21.734594]","[16.240833, -12.068588, 7.5725627]","[9.056207, 4.4435406]","[8.562507, -0.03773622]","[10.587986, 11.505595, -1.9600021]","[10.199183, 5.29326, 13.032275]"
17,Sébastien Fenet;Gaël Richard;Yves Grenier,A Scalable Audio Fingerprint Method with Robustness to Pitch-Shifting.,2011,https://doi.org/10.5281/zenodo.1417593,Sébastien Fenet+Institut TELECOM>FRA>education|TELECOM ParisTech>FRA>education|CNRS-LTCI>FRA>facility;Gaël Richard+Institut TELECOM>FRA>education|TELECOM ParisTech>FRA>education|CNRS-LTCI>FRA>facility;Yves Grenier+Institut TELECOM>FRA>education|TELECOM ParisTech>FRA>education|CNRS-LTCI>FRA>facility,"Audio fingerprint techniques should be robust to a variety of distortions due to noisy transmission channels or specific sound processing. Although most of nowadays techniques are robust to the majority of them, the quasi-systematic use of a spectral representation makes them possibly sensitive to pitch-shifting. This distortion indeed induces a modification of the spectral content of the signal. In this paper, we propose a novel fingerprint technique, relying on a hashing technique coupled with a CQT-based fingerprint, with a strong robustness to pitch-shifting. Furthermore, we have associated this method with an efficient post-processing for the removal of false alarms. We also present the adaptation of a database pruning technique to our specific context. We have evaluated our approach on a real-life broadcast monitoring scenario. The analyzed data consisted of 120 hours of real radio broadcast (thus containing all the distortions that would be found in an industrial context). The reference database consisted of 30,000 songs. Our method, thanks to its increased robustness to pitch-shifting, shows an excellent detection score.",FRA,education,Developed economies,"[-15.069793, -26.33828]","[25.29849, -22.423792]","[7.060855, -12.466721, -21.994968]","[17.823475, -13.417617, 4.6119146]","[9.053136, 4.4777374]","[8.451541, -0.122943014]","[10.583138, 11.514197, -1.9313717]","[10.152029, 5.256118, 12.98371]"
16,Ren Gang;Gregory Bocko;Justin Lundberg;Stephen Roessner;Dave Headlam;Mark F. Bocko,A Real-Time Signal Processing Framework of Musical Expressive Feature Extraction Using Matlab.,2011,https://doi.org/10.5281/zenodo.1418081,Ren Gang+University of Rochester>USA>education;Gregory Bocko+University of Rochester>USA>education;Justin Lundberg+University of Rochester>USA>education;Stephen Roessner+University of Rochester>USA>education;Dave Headlam+University of Rochester>USA>education;Mark F. Bocko+University of Rochester>USA>education,"In this paper we propose a real-time signal processing framework for musical audio that 1) aligns the audio with an existing music score or creates a musical score by automated music transcription algorithms; and 2) obtains the expressive feature descriptors of music performance by comparing the score with the audio. Real-time audio segmentation algorithms are implemented to identify the onset points of music notes in the incoming audio stream. The score related features and musical expressive features are extracted based on these segmentation results. In a real-time setting, these audio segmentation and feature extraction operations have to be accomplished at (or shortly after) the note onset points, when an incomplete length of audio signal is captured. To satisfy real-time processing requirements while maintaining feature accuracy, our proposed framework combines the processing stages of prediction, estimation, and updating in both audio segmentation and feature extraction algorithms in an integrated refinement process. The proposed framework is implemented in a MATLAB real-time signal processing framework.",USA,education,Developed economies,"[-8.470398, -16.197437]","[-18.826466, -7.7019205]","[-6.674499, -12.913921, -2.477249]","[-4.3846793, 1.7646698, -9.577989]","[11.203407, 7.199788]","[6.0611773, 2.4748778]","[13.126069, 12.986543, -0.14284831]","[8.353329, 7.3249683, 10.867714]"
21,Sébastien Gulluni;Slim Essid;Olivier Buisson;Gaël Richard,An Interactive System for Electro-Acoustic Music Analysis.,2011,https://doi.org/10.5281/zenodo.1416134,"Sébastien Gulluni+Institut National de l’Audiovisuel>FRA>facility;Olivier Buisson+Institut National de l’Audiovisuel>FRA>facility;Slim Essid+Institut Telecom, Telecom ParisTech>FRA>education;Gaël Richard+Institut Telecom, Telecom ParisTech>FRA>education","This paper presents an interactive approach for the analysis of electro-acoustic music. An original classification scheme is devised using relevance feedback and active-learning segment selection in an interactive loop. Validation and correction information given by the user is injected in the learning process at each iteration to achieve more accurate classification. An experimental study is conducted to evaluate and compare the different classification and relevance feedback approaches that are envisaged, using a database of polyphonic pieces (with a varying degree of polyphony). The results show that the different approaches are adapted to different applications and they achieve satisfying performance in a reasonable number of iterations.",FRA,facility,Developed economies,"[-3.8617003, 27.978254]","[4.3667603, 28.04491]","[-18.67525, -15.2514, -2.8432333]","[-0.101417676, -6.481129, 13.618564]","[13.437274, 7.485837]","[10.0867405, 1.3602973]","[13.828414, 13.880807, -1.9168106]","[10.943133, 5.715602, 11.67385]"
27,Toru Nakashika;Tetsuya Takiguchi;Yasuo Ariki,Constrained Spectrum Generation Using A Probabilistic Spectrum Envelope for Mixed Music Analysis.,2011,https://doi.org/10.5281/zenodo.1415774,Toru Nakashika+Kobe University>JPN>education;Tetsuya Takiguchi+Kobe University>JPN>education;Yasuo Ariki+Kobe University>JPN>education,"NMF (Non-negative Matrix Factorization) has been one of the most widely-used techniques for musical signal analysis in recent years. In particular, the supervised type of NMF is garnering much attention in source separation with respect to the analysis accuracy and speed. In this approach, a large number of spectral samples is used for analyzing a signal. If the system has a minimal number of samples, the accuracy deteriorates. Because such methods require all the possible samples for the analysis, it is hard to build a practical analysis system. To analyze signals properly even when short of samples, we propose a novel method that combines a supervised NMF and probabilistic search algorithms. In this approach, it is assumed that each instrumental category has a model-invariant feature called a probabilistic spectrum envelope (PSE). The algorithm starts with learning the PSEs of each category using a technique based on Gaussian Process Regression. Using the PSEs for spectrum generation, an observed spectrum is analyzed under the framework of a supervised NMF. The optimum spectrum can be searched by Genetic Algorithm using sparseness and density constraints.",JPN,education,Developed economies,"[10.194197, -16.469019]","[-46.477863, -22.74103]","[11.775814, 7.44798, 21.120634]","[-3.2577085, -13.196207, -28.981937]","[11.851884, 8.176912]","[6.338271, 4.7170177]","[12.812364, 13.67501, -0.3007382]","[9.764706, 8.742356, 10.040323]"
86,Carlos Rosão;Ricardo Ribeiro 0001;David Martins de Matos,Influence of Peak Selection Methods on Onset Detection.,2012,https://doi.org/10.5281/zenodo.1417271,Carlos Rosão+ISCTE-IUL>PRT>education|L2F/INESC-ID Lisboa>PRT>facility;Ricardo Ribeiro+ISCTE-IUL>PRT>education|L2F/INESC-ID Lisboa>PRT>facility;David Martins de Matos+IST/UTL>PRT>education|L2F/INESC-ID Lisboa>PRT>facility,"Finding the starting time of musical notes in an audio signal, that is, to perform onset detection, is an important task as this information can be used as the basis for high-level musical processing tasks. Many different methods exist to perform onset detection. However their results depend on a Peak Selection step that makes the decision whether an onset is present at some point in time. In this paper we review a number of different Peak Selection methods and compare their influence in the performance of different onset detection methods and on 4 distinct onset classes. Our results show that the post-processing method used deeply influences both positively and negatively the results obtained.",PRT,education,Developed economies,"[30.545366, -26.795483]","[-23.207605, -6.3020797]","[8.683677, -20.903687, -6.946959]","[-1.510206, 5.8645973, -10.81828]","[10.308492, 4.992836]","[5.70431, 2.4784787]","[10.381165, 13.245863, -1.5598414]","[7.9730062, 7.546538, 10.91466]"
77,Tom O'Hara;Nico Schüler;Yijuan Lu;Dan Tamir,Inferring Chord Sequence Meanings via Lyrics: Process and Evaluation.,2012,https://doi.org/10.5281/zenodo.1417975,Tom O’Hara+Texas State University>USA>education;Nico Sch¨uler+Texas State University>USA>education;Yijuan Lu+Texas State University>USA>education;Dan E. Tamir+Texas State University>USA>education,"We improve upon our simple approach for learning the “associational meaning” of chord sequences from lyrics based on contingency statistics induced over a set of lyrics with chord annotations. Specifically, we refine this process by using word alignment tools developed for statistical machine translation, and we also use a much larger set of chord annotations. In addition, objective evaluation measures are included. Thus, this work validates a novel application of lexicon induction techniques over parallel corpora to a domain outside of natural language learning. To confirm the associations commonly attributed to major versus minor chords (i.e., happy and sad, respectively), we compare the inferred word associations against synonyms reflecting this dichotomy. To evaluate meanings associated with chord sequences, we check how often tagged chords occur in songs labeled with the same overall meaning.",USA,education,Developed economies,"[-31.711008, -31.9665]","[34.13764, -9.705878]","[7.814521, 21.930462, -0.40605307]","[16.520054, -13.345852, 15.51208]","[11.489159, 11.751798]","[10.825121, 3.1461933]","[12.468142, 15.9046, 1.1131101]","[12.400856, 6.6086483, 11.422926]"
85,Johanna Devaney;Michael I. Mandel;Ichiro Fujinaga,A Study of Intonation in Three-Part Singing using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT).,2012,https://doi.org/10.5281/zenodo.1416210,"Johanna Devaney+CNMAT, UC Berkeley>USA>education;Michael Mandel+The Ohio State University>USA>education|Audience Inc.>USA>company;Ichiro Fujinaga+CIRMMT, Schulich School of Music, McGill University>CAN>education","This paper introduces the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT), is a MATLAB toolkit for accurately aligning monophonic audio to MIDI scores as well as extracting and analyzing timing-, pitch-, and dynamics-related performance data from the aligned recordings. This paper also presents the results of an analysis performed with AMPACT on an experiment studying intonation in three-part singing. The experiment examines the interval size and drift in four ensembles’ performances of a short exercise by Benedetti, which was designed to highlight the conflict between Just Intonation tuning and pitch drift.",USA,education,Developed economies,"[-1.5035052, -27.823862]","[-3.9311476, -18.704422]","[14.961375, 2.964103, -15.399385]","[7.1492105, -18.825823, -13.278121]","[10.362374, 10.555791]","[6.7865834, 1.9537346]","[11.316977, 15.196735, 0.069379]","[8.72681, 7.5834966, 11.528879]"
84,Laurent Pugin;Johannes Kepper;Perry Roland;Maja Hartwig;Andrew Hankinson,Separating Presentation and Content in MEI.,2012,https://doi.org/10.5281/zenodo.1416978,Laurent Pugin+Swiss RISM>CHE>facility;Johannes Kepper+Edirom>Unknown>Unknown;Perry Roland+University of Virginia>USA>education;Maja Hartwig+McGill University>CAN>education;Andrew Hankinson+McGill University>CAN>education,"""Common Western music notation is traditionally organized on staves that can be grouped into systems. When multiple systems appear on a page, they are arranged from the top to the bottom of the page, similar to lines of words in a text document. Encoding music notation documents for printing requires this arrangement to be captured. However, in the music notation model proposed by the Music Encoding Initiative (MEI), the hierarchy of the XML sub-tree representing the music emphasizes the content rather than the layout. Since systems and pages do not coincide with the musical content, they are encoded in a secondary hierarchy that contains very limited information. In this paper, we present a complementary solution for augmenting the level of detail of the layout of musical documents; that is, the layout information can be encoded in a separate sub-tree with cross-references to other elements holding the musical content. The major advantage of the proposed solution is that it enables multiple layout descriptions, each describing a different visual instantiation of the same musical content.""",CHE,facility,Developed economies,"[-8.339548, 41.936043]","[2.4155471, 39.563957]","[-10.828127, -16.014236, 16.22531]","[-10.063233, -7.390348, 20.767666]","[12.395525, 6.753702]","[10.01688, 0.12885912]","[14.013569, 12.574873, -1.3861595]","[10.5687275, 4.843441, 11.928338]"
82,Luis Jure;Ernesto López;Martín Rocamora;Pablo Cancela;Haldo Sponton;Ignacio Irigaray,Pitch Content Visualization Tools for Music Performance Analysis.,2012,https://doi.org/10.5281/zenodo.1414860,Luis Jure+Universidad de la República>URY>education;Ernesto López+Universidad de la República>URY>education;Martín Rocamora+Universidad de la República>URY>education;Pablo Cancela+Universidad de la República>URY>education;Haldo Sponton+Universidad de la República>URY>education;Ignacio Irigaray+Universidad de la República>URY>education,"This work deals with pitch content visualization tools for the analysis of music performance from audio recordings. An existing computational method for the representation of pitch contours is briefly reviewed. Its application to music analysis is exemplified with two pieces of non-notated music: a field recording of a folkloric form of polyphonic singing and a commercial recording by a noted blues musician. Both examples have vocal parts exhibiting complex pitch evolution, difficult to analyze and notate with precision using Western common music notation. By using novel time-frequency analysis techniques that improve the location of the components of a harmonic sound, the melodic content representation implemented here allows a detailed study of aspects related to pitch intonation and tuning. This in turn permits an objective measurement of essential musical characteristics that are difficult or impossible to properly evaluate by subjective perception alone, and which are often not accounted for in traditional musicological analysis. Two software tools are released that allow the practical use of the described methods.",URY,education,Developing economies,"[-10.524711, 34.253613]","[-3.5673387, -14.671928]","[-5.8573246, -9.587251, -13.971868]","[6.753822, -14.6434555, -8.631567]","[13.34575, 6.8145986]","[7.0059123, 2.2115474]","[13.661462, 13.346396, -2.1436281]","[9.150721, 7.5275335, 11.570298]"
81,Michael Terrell;György Fazekas;Andrew Simpson;Jordan B. L. Smith;Simon Dixon,Listening Level Changes Music Similarity.,2012,https://doi.org/10.5281/zenodo.1415710,Michael J. Terrell+Queen Mary University of London>GBR>education;György Fazekas+Queen Mary University of London>GBR>education;Andrew J. R. Simpson+Queen Mary University of London>GBR>education;Jordan Smith+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"We examine the effect of listening level, i.e. the absolute sound pressure level at which sounds are reproduced, on music similarity, and in particular, on playlist generation. Current methods commonly use similarity metrics based on Mel-frequency cepstral coefficients (MFCCs), which are derived from the objective frequency spectrum of a sound. We follow this approach, but use the level-dependent auditory spectrum, evaluated using the loudness models of Glasberg and Moore, at three listening levels, to produce auditory spectrum cepstral coefficients (ASCCs). The ASCCs are used to generate sets of playlists at each listening level, using a typical method, and these playlists were found to differ greatly. From this we conclude that music recommendation systems could be made more perceptually relevant if listening level information were included. We discuss the findings in relation to other fields within MIR where inclusion of listening level might also be of benefit.",GBR,education,Developed economies,"[-4.8957872, 17.206203]","[32.148315, 11.249018]","[-10.372343, 9.675201, 0.74226207]","[0.06287399, 20.973179, 12.219218]","[13.326081, 9.319144]","[10.841705, 2.5026224]","[13.836851, 15.279457, -0.8359142]","[12.359795, 6.2708097, 12.101252]"
80,Katerina Kosta;Marco Marchini;Hendrik Purwins,Unsupervised Chord-Sequence Generation from an Audio Example.,2012,https://doi.org/10.5281/zenodo.1415534,"Katerina Kosta+Centre for Digital Music, Queen Mary, University of London>GBR>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Neurotechnology Group, Berlin Institute of Technology>DEU>education;Marco Marchini+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Neurotechnology Group, Berlin Institute of Technology>DEU>education;Hendrik Purwins+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Neurotechnology Group, Berlin Institute of Technology>DEU>education","A system is presented that generates a sound sequence from an original audio chord sequence, having the following characteristics: The generation can be arbitrarily long, preserves certain musical characteristics of the original and has a reasonable degree of interestingness. The procedure comprises the following steps: 1) chord segmentation by onset detection, 2) representation as Constant Q Profiles, 3) multi-level clustering, 4) cluster level selection, 5) metrical analysis, 6) building of a suffix tree, 7) generation heuristics. The system can be seen as a computational model of the cognition of harmony consisting of an unsupervised formation of harmonic categories (via multi-level clustering) and a sequence learning module (via suffix trees) which in turn controls the harmonic categorization in a top-down manner (via a measure of regularity). In the final synthesis, the system recombines the audio material derived from the sample itself and it is able to learn various harmonic styles. The system is applied to various musical styles and is then evaluated subjectively by musicians and non-musicians, showing that it is capable of producing sequences that maintain certain musical characteristics of the original.",GBR,education,Developed economies,"[51.086845, -2.0603812]","[-24.18285, 22.563314]","[20.940752, -15.298239, 15.151871]","[-22.410536, 1.1224128, 3.3058622]","[7.16058, 8.868323]","[6.991088, 3.3197658]","[12.142128, 10.662182, 1.6670043]","[9.659954, 8.250143, 12.206005]"
79,Andrew Robertson,Decoding Tempo and Timing Variations in Music Recordings from Beat Annotations.,2012,https://doi.org/10.5281/zenodo.1416806,Andrew Robertson+Queen Mary University of London>GBR>education,"This paper addresses the problem of determining tempo and timing data from a list of beat annotations. Whilst an approximation to the tempo can be calculated from the inter-beat interval, the annotations also include timing variations due to expressively timed events, phase shifts and errors in the annotation times. These deviations tend to propagate into the tempo graph and so tempo analysis methods tend to average over recent inter-beat intervals. However, whilst this minimises the effect such timing deviations have on the local tempo estimate, it also obscures the expressive timing devices used by the performer. Here we propose a more formal method for calculation of the optimal tempo path through use of an appropriate cost function that incorporates tempo change, phase shift and expressive timing.",GBR,education,Developed economies,"[37.806164, -27.992409]","[-29.689846, 0.87170416]","[0.82842565, -27.986746, -5.545042]","[-10.907913, 9.74938, -6.4341707]","[11.368366, 4.510774]","[5.5490375, 1.5217909]","[10.825872, 13.359045, -2.6227293]","[7.6868744, 6.5428505, 11.226242]"
78,Alexander Schindler;Rudolf Mayer;Andreas Rauber,Facilitating Comprehensive Benchmarking Experiments on the Million Song Dataset.,2012,https://doi.org/10.5281/zenodo.1417521,Alexander Schindler+Vienna University of Technology>AUT>education;Rudolf Mayer+Vienna University of Technology>AUT>education;Andreas Rauber+Vienna University of Technology>AUT>education,"The Million Song Dataset (MSD), a collection of one million music pieces, enables a new era of research of Music Information Retrieval methods for large-scale applications. It comes as a collection of meta-data such as the song names, artists and albums, together with a set of features extracted with the The Echo Nest services, such as loudness, tempo, and MFCC-like features. There is, however, no easily obtainable download for the audio files. Furthermore, labels for supervised machine learning tasks are missing. Researchers thus are currently restricted on working solely with these features provided, limiting the usefulness of MSD. We therefore present in this paper a more comprehensive set of data based on the MSD, allowing its broader use as benchmark collection. Specifically, we provide a wide and growing collection of other well-known features in the MIR domain, as well as ground truth data with a set of recommended training/test splits. We obtained these features from audio samples provided by 7digital.com, and metadata from the All Music Guide. While copyright prevents re-distribution of the audio snippets per se, the features as well as metadata are publicly available on our website for benchmarking evaluations. In this paper we describe the pre-processing and cleansing steps applied, as well as feature sets and tools made available, together with first baseline classification results.",AUT,education,Developed economies,"[-20.483223, 6.942803]","[10.544048, 24.759027]","[-21.90456, 2.0308175, 4.0041966]","[-3.0296996, 2.842477, 13.379523]","[12.688501, 7.632903]","[10.139597, 3.6829975]","[14.082716, 13.51089, -0.8652633]","[11.507855, 6.12209, 10.91898]"
70,Aggelos Pikrakis;Francisco Gómez 0001;Sergio Oramas;José Miguel Díaz-Báñez;Joaquín Mora;Francisco Escobar-Borrego;Emilia Gómez;Justin Salamon,Tracking Melodic Patterns in Flamenco Singing by Analyzing Polyphonic Music Recordings.,2012,https://doi.org/10.5281/zenodo.1415560,A. Pikrakis+University of Piraeus>GRC>education;F. Gómez+Polytechnic University of Madrid>ESP>education;S. Oramas+Polytechnic University of Madrid>ESP>education;J. M. D. Báñez+University of Sevilla>ESP>education;J. Mora+University of Sevilla>ESP>education;F. Escobar+University of Sevilla>ESP>education;E. Gómez+Universitat Pompeu Fabra>ESP>education;J. Salamon+Universitat Pompeu Fabra>ESP>education,"The purpose of this paper is to present an algorithmic pipeline for melodic pattern detection in audio files. Our method follows a two-stage approach: first, vocal pitch sequences are extracted from the audio recordings by means of a predominant fundamental frequency estimation technique; second, instances of the patterns are detected directly in the pitch sequences by means of a dynamic programming algorithm which is robust to pitch estimation errors. In order to test the proposed method, an analysis of characteristic melodic patterns in the context of the flamenco fandango style was performed. To this end, a number of such patterns were defined in symbolic format by flamenco experts and were later detected in music corpora, which were composed of un-segmented audio recordings taken from two fandango styles, namely Valverde fandangos and Huelva capital fandangos. These two styles are representative of the fandango tradition and also differ with respect to their musical characteristics. Finally, the strategy in the evaluation of the algorithm performance was discussed by flamenco experts and their conclusions are presented in this paper.",GRC,education,Developed economies,"[0.46807054, -14.98732]","[9.516569, 0.23850863]","[14.353006, 6.7447915, -21.565203]","[2.1255326, 8.5324955, -0.8090388]","[11.385685, 9.942376]","[7.8405504, 1.3509855]","[12.03891, 15.123391, -0.29605705]","[9.30374, 6.8761497, 12.453097]"
75,Ciril Bohak;Matija Marolt,Finding Repeating Stanzas in Folk Songs.,2012,https://doi.org/10.5281/zenodo.1417597,Ciril Bohak+University of Ljubljana>SVN>education;Matija Marolt+University of Ljubljana>SVN>education,"Folk songs are typically composed of repeating parts - stanzas. To find such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of specific issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several methods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method consists of several steps. In the first step breathing (vocal) pauses are detected, which represent the candidate beginnings of individual stanzas. Next, a similarity measure is calculated between the first and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate beginnings represent the actual boundaries between stanzas, a scoring function is defined based on the calculated similarities between stanzas. A peak picking method is used in combination with global thresholding for the final selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive.",SVN,education,Developed economies,"[-28.071568, -3.9992998]","[1.3322881, -4.177907]","[5.068733, 12.654794, -2.266749]","[-5.402107, -3.4319499, -3.367561]","[12.356749, 9.925381]","[8.114481, 2.78778]","[12.747747, 15.236386, -0.28097478]","[10.209171, 7.303377, 11.600671]"
74,Ruofeng Chen;Weibin Shen;Ajay Srinivasamurthy;Parag Chordia,Chord Recognition Using Duration-explicit Hidden Markov Models.,2012,https://doi.org/10.5281/zenodo.1417077,Ruofeng Chen+Georgia Tech Center for Music Technology>USA>education;Weibin Shen+Georgia Tech Center for Music Technology>USA>education;Ajay Srinivasamurthy+Georgia Tech Center for Music Technology>USA>education;Parag Chordia+Smule Inc.>USA>company,"We present an audio chord recognition system based on a generalization of the Hidden Markov Model (HMM) in which the duration of chords is explicitly considered - a type of HMM referred to as a hidden semi-Markov model, or duration-explicit HMM (DHMM). We find that such a system recognizes chords at a level consistent with the state-of-the-art systems – 84.23% on Uspop dataset at the major/minor level. The duration distribution is estimated from chord duration histograms on the training data. It is found that the state-of-the-art recognition result can be improved upon by using several duration distributions, which are found automatically by clustering song-level duration histograms. The paper further describes experiments which shed light on the extent to which context information, in the sense of transition matrices, is useful for the audio chord recognition task. We present evidence that the context provides surprisingly little improvement in performance, compared to isolated frame-wise recognition with simple smoothing. We discuss possible reasons for this, such as the inherent entropy of chord sequences in our training database.",USA,education,Developed economies,"[55.635662, -5.4438686]","[-32.067753, 18.705996]","[27.769442, -13.0940695, 18.319803]","[-23.330486, -4.3892903, 0.6493666]","[6.6448092, 8.703263]","[6.1334276, 3.654243]","[11.879392, 10.349209, 2.1148689]","[9.70437, 8.90046, 12.138965]"
73,Geoffroy Peeters;Frédéric Cornu;Christophe Charbuillet;Damien Tardieu;Juan José Burred;Marie Vian;Valérie Botherel;Jean-Bernard Rault;Jean-Philippe Cabanal,"A Multimedia Search and Navigation Prototype, Including Music and Video-clips.",2012,https://doi.org/10.5281/zenodo.1417761,G. Peeters+STMS IRCAM-CNRS-UPMC>FRA>education;F. Cornu+STMS IRCAM-CNRS-UPMC>FRA>education;Ch. Charbuillet+STMS IRCAM-CNRS-UPMC>FRA>education;D. Tardieu+STMS IRCAM-CNRS-UPMC>FRA>education;J.J. Burred+STMS IRCAM-CNRS-UPMC>FRA>education;M. Vian+Bertin Technologies>FRA>company|Orange-Labs>FRA>company;V. Botherel+Bertin Technologies>FRA>company|Orange-Labs>FRA>company;J.-B. Rault+Bertin Technologies>FRA>company|Orange-Labs>FRA>company;J.-Ph. Cabanal+Bertin Technologies>FRA>company|Orange-Labs>FRA>company,"Moving music indexing technologies developed in a research lab to their integration and use in the context of a third-party search and navigation engine that indexes music files, archives of TV music programs and video clips, involves a set of choices and works that we relate here. First one has to choose technologies that perform well, which are scalable (in terms of computation time of extraction and item comparison for search-by-similarity), and which are not sensitive to media quality (being able to process equally music files or audio tracks from video archives). These technologies must be applied to estimate tags chosen to be understandable and useful for users (the specific genre and mood tags or other content-descriptions). For training the related technologies, relevant and reliable annotated corpus must be created. For using them, relevant user-scenarios must be created and friendly Graphical User-Interface designed. In this paper, we share the experience we had in a recent project on integrating six state-of-the-art music-indexing technologies in a multimedia search and navigation prototype.",FRA,education,Developed economies,"[-10.071203, 29.565977]","[23.736382, 21.396688]","[-6.361588, 3.5131388, -21.29708]","[11.294051, -2.3626134, 17.788523]","[14.132257, 7.4398646]","[11.293614, 1.538217]","[14.076598, 14.24716, -2.3778522]","[12.269589, 5.4490128, 12.7046795]"
72,Andreas Arzt;Sebastian Böck;Gerhard Widmer,Fast Identification of Piece and Score Position via Symbolic Fingerprinting.,2012,https://doi.org/10.5281/zenodo.1417022,Andreas Arzt+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Sebastian Böck+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"In this paper we present a novel algorithm that, given a short snippet of an audio performance (piano music, for the time being), identifies the piece and the score position. Instead of using audio matching methods we propose a combination of a state-of-the-art music transcription algorithm and a new symbolic fingerprinting method. The resulting system is usable in both on-line and off-line scenarios and thus may be of use in many application areas. As the evaluation shows the system operates with only minimal lag and achieves high precision even with very short queries.",AUT,education,Developed economies,"[37.549614, 10.811551]","[2.292622, 10.079809]","[-3.5934412, -37.009106, -1.4561504]","[2.7352476, -16.369877, 2.3610356]","[10.994107, 6.538558]","[8.16151, 0.3380302]","[12.418266, 12.366869, -1.2640028]","[9.969292, 5.5478325, 12.612618]"
71,Meinard Müller;Thomas Prätzlich;Jonathan Driedger,A Cross-version Approach for Stabilizing Tempo-based Novelty Detection.,2012,https://doi.org/10.5281/zenodo.1417753,Meinard Müller+Bonn University>DEU>education|MPI Informatik>DEU>facility;Thomas Prätzlich+Saarland University>DEU>education|MPI Informatik>DEU>facility;Jonathan Driedger+Bonn University>DEU>education|Saarland University>DEU>education,"The task of novelty detection with the objective of detecting changes regarding musical properties such as harmony, dynamics, timbre, or tempo is of fundamental importance when analyzing structural properties of music recordings. But for a specific audio version of a given piece of music, the novelty detection result may also crucially depend on the individual performance style of the musician. This particularly holds true for tempo-related properties, which may vary significantly across different performances of the same piece of music. In this paper, we show that tempo-based novelty detection can be stabilized and improved by simultaneously analyzing a set of different performances. We first warp the version-dependent novelty curves onto a common musical time axis, and then combine the individual curves to produce a single fusion curve. Our hypothesis is that musically relevant points of novelty tend to be consistent across different performances. This hypothesis is supported by our experiments in the context of music structure analysis, where the cross-version fusion curves yield, on average, better results than the novelty curves obtained from individual recordings.",DEU,education,Developed economies,"[36.37398, -26.887466]","[-26.09227, -0.072826296]","[-1.0098263, -31.719261, -6.2886853]","[-5.595244, 5.882004, -4.6161404]","[11.310553, 4.500111]","[5.788546, 1.5487262]","[10.858801, 13.34306, -2.6756673]","[8.114841, 6.5478215, 11.311364]"
69,Holger Kirchhoff;Simon Dixon;Anssi Klapuri,Multi-Template Shift-Variant Non-Negative Matrix Deconvolution for Semi-Automatic Music Transcription.,2012,https://doi.org/10.5281/zenodo.1418207,Holger Kirchhoff+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education;Anssi Klapuri+Queen Mary University of London>GBR>education,"For the task of semi-automatic music transcription, we extended our framework for shift-variant non-negative matrix deconvolution (svNMD) to work with multiple templates per instrument and pitch. A k-means clustering based learning algorithm is proposed that infers the templates from the data based on the provided user information. We experimentally explored the maximum achievable transcription accuracy of the algorithm and evaluated the prospective performance in a realistic setting. The results showed a clear superiority of the Itakura-Saito divergence over the Kullback-Leibler divergence and a consistent improvement of the maximum achievable accuracy when each pitch is represented by more than one spectral template.",GBR,education,Developed economies,"[29.212023, -12.11835]","[-51.52097, -23.319286]","[11.980837, -8.163629, 6.3361273]","[-5.538918, -18.053844, -27.842735]","[9.2826395, 7.7800546]","[6.0727534, 4.7265496]","[11.645902, 12.344305, 0.26226145]","[9.32509, 8.731423, 9.947263]"
68,Kevin R. Page;Benjamin Fields;David De Roure;Tim Crawford;J. Stephen Downie,"Reuse, Remix, Repeat: the Workflows of MIR.",2012,https://doi.org/10.5281/zenodo.1417299,"Kevin R. Page+Oxford e-Research Centre, University of Oxford>GBR>education;Ben Fields+Musicmetric (Semetric Ltd.)>GBR>company|Department of Computing, Goldsmiths, University of London>GBR>education;David De Roure+Oxford e-Research Centre, University of Oxford>GBR>education;Tim Crawford+Department of Computing, Goldsmiths, University of London>GBR>education;J. Stephen Downie+Graduate School of Library and Information Sciences, University of Illinois>USA>education","Many solutions for the reuse and remixing of MIR methods and the tools implementing them have been introduced over recent years. Proposals for achieving the necessary interoperability have ranged from shared software libraries and interfaces, through common frameworks and portals, to standardised file formats and metadata. Each proposal shares the desire to reuse and combine repurposable components into assemblies (or “workflows”) that can be used in novel and possibly more ambitious ways. Reuse and remixing also have great implications for the process of MIR research. The encapsulation of any algorithm and its operation – including inputs, parameters, and outputs – is fundamental to the repeatability and reproducibility of any experiment. This is desirable both for the open and reliable evaluation of algorithms (e.g. in MIREX) and for the advancement of MIR by building more effectively upon prior research. At present there is no clear best practice widely adopted throughout the community. Should this be considered a failure? Are there limits to interoperability unique to MIR, and how might they be overcome? In this paper we assess contemporary MIR solutions to these issues, aligning them with the emerging notion of Research Objects for reproducible research in other domains, and propose their adoption as a route to reuse in MIR.",GBR,education,Developed economies,"[-11.467179, 59.113907]","[20.917173, 43.53985]","[-36.304413, 3.5326302, -4.2012753]","[-4.1541376, 6.3519344, 17.25425]","[13.552078, 4.8407407]","[11.440736, 0.43311283]","[14.911913, 11.231704, -1.4394047]","[11.680281, 4.79132, 11.636257]"
67,Eric J. Humphrey;Juan Pablo Bello;Yann LeCun,Moving Beyond Feature Design: Deep Architectures and Automatic Feature Learning in Music Informatics.,2012,https://doi.org/10.5281/zenodo.1415726,"Eric J. Humphrey+Music and Audio Research Lab, NYU>USA>education;Juan Pablo Bello+Music and Audio Research Lab, NYU>USA>education;Yann LeCun+Courant School of Computer Science, NYU>USA>education","The short history of content-based music informatics research is dominated by hand-crafted feature design, and our community has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding diminishing returns. This deceleration is largely due to the tandem of heuristic feature design and shallow processing architectures. We systematically discard hopefully irrelevant information while simultaneously calling upon creativity, intuition, or sheer luck to craft useful representations, gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of deep learning, it has only recently started to be explored in our field. By reviewing deep architectures and feature learning, we hope to raise awareness in our community about alternative approaches to solving MIR challenges, new and old alike.",USA,education,Developed economies,"[-12.022385, -9.2033205]","[-16.158556, -38.637127]","[7.7169127, 8.1138115, 12.263877]","[-13.078644, 3.0334218, -20.905222]","[11.545547, 9.315761]","[9.528126, 5.476044]","[13.416635, 13.161404, 0.68927556]","[10.263379, 5.802959, 9.124587]"
66,Jean-Julien Aucouturier;Emmanuel Bigand,Mel Cepstrum & Ann Ova: The Difficult Dialog Between MIR and Music Cognition.,2012,https://doi.org/10.5281/zenodo.1417179,Jean-Julien Aucouturier+University of Burgundy>FRA>education;Emmanuel Bigand+University of Burgundy>FRA>education,"""""",FRA,education,Developed economies,"[-13.70398, 53.02409]","[50.35755, 43.812466]","[-33.484932, -0.2602374, 3.2181394]","[5.3823586, 5.7825327, 28.739132]","[13.30472, 5.3511863]","[-9.071028, 9.961345]","[14.64881, 11.739179, -1.3481492]","[-11.710318, 1.7631001, 1.0316731]"
87,Yading Song;Simon Dixon;Marcus Pearce,Evaluation of Musical Features for Emotion Classification.,2012,https://doi.org/10.5281/zenodo.1415854,"Yading Song+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education;Marcus Pearce+Centre for Digital Music, Queen Mary University of London>GBR>education","Because music conveys and evokes feelings, a wealth of research has been performed on music emotion recognition. Previous research has shown that musical mood is linked to features based on rhythm, timbre, spectrum and lyrics. For example, sad music correlates with slow tempo, while happy music is generally faster. However, only limited success has been obtained in learning automatic classifiers of emotion in music. In this paper, we collect a ground truth data set of 2904 songs that have been tagged with one of the four words “happy”, “sad”, “angry” and “relaxed”, on the Last.FM web site. An excerpt of the audio is then retrieved from 7Digital.com, and various sets of audio features are extracted using standard algorithms. Two classifiers are trained using support vector machines with the polynomial and radial basis function kernels, and these are tested with 10-fold cross validation. Our results show that spectral features outperform those based on rhythm, dynamics, and, to a lesser extent, harmony. We also find that the polynomial kernel gives better results than the radial basis function, and that the fusion of different feature sets does not always lead to improved classification.",GBR,education,Developed economies,"[-59.086906, -0.3247224]","[51.9039, -8.030069]","[-26.073805, 23.708086, 6.1114926]","[11.613493, 25.100317, 6.6803174]","[14.031216, 12.861682]","[13.137246, 4.066518]","[16.121588, 14.451072, 1.7895887]","[14.27012, 5.114681, 10.363761]"
76,Mathieu Giraud;Richard Groult;Florence Levé,Detecting Episodes with Harmonic Sequences for Fugue Analysis.,2012,https://doi.org/10.5281/zenodo.1416596,"Mathieu Giraud+LIFL, CNRS, Université Lille 1>FRA>education|INRIA Lille>FRA>facility;Richard Groult+Université Picardie Jules Verne>AUT>education;Florence Lévé+Université Picardie Jules Verne>AUT>education","Fugues alternate between instances of the subject and of other patterns, such as the counter-subject, and modulatory sections called episodes. The episodes play an important role in the overall design of a fugue: detecting them may help the analysis of the fugue, in complement to a subject and a counter-subject detection. We propose an algorithm to retrieve episodes in the fugues of the first book of Bach’s Well-Tempered Clavier, starting from a symbolic score which is already track-separated. The algorithm does not use any information on subject or counter-subject occurrences, but tries to detect partial harmonic sequences, that is similar pitch contour in at least two voices. For this, it uses a substitution function considering “quantized partially overlapping intervals” [14] and a strict length matching for all notes, except for the first and the last one. On half of the tested fugues, the algorithm has correct or good results, enabling to sketch the design of the fugue.",FRA,education,Developed economies,"[28.239157, 20.31113]","[-11.44505, 14.923076]","[-2.1503265, -15.6022215, 30.613697]","[0.4898079, -16.13498, 7.407216]","[9.542307, 9.173804]","[8.107038, 1.9036143]","[11.779818, 13.7657585, -0.75537384]","[10.0759945, 7.2028217, 12.519583]"
88,Benjamin Martin 0001;Daniel G. Brown 0001;Pierre Hanna;Pascal Ferraro,BLAST for Audio Sequences Alignment: A Fast Scalable Cover Identification Tool.,2012,https://doi.org/10.5281/zenodo.1417687,Benjamin Martin+Université de Bordeaux>FRA>education;Daniel G. Brown+University of Waterloo>CAN>education;Pierre Hanna+Université de Bordeaux>FRA>education;Pascal Ferraro+Université de Bordeaux>FRA>education,"Searching for similarities in large musical databases is common for applications such as cover song identification. These methods typically use dynamic programming to align the shared musical motifs between subparts of two recordings. Such music local alignment methods are slow, as are the bioinformatics algorithms they are closely related to. We have adapted the ideas of the Basic Local Alignment Search Tool (BLAST) for biosequence alignment to the domain of aligning sequences of chroma features. Our tool allows local music sequence alignment in near-linear time. It identifies small regions of exact match between sequences, called seeds, and builds local alignments that include these seeds. Seed determination is a key issue for the accuracy of the method and closely depends on the database, the representation and the application. We introduce a particular seeding approach for cover detection, and evaluate it on both a 2000-piece training set and the million song dataset (MSD). We show that the heuristic alignment drastically improves time computation for cover song detection. Alignment sensitivity is still very high on the small database, but is dramatically weakened on the MSD, due to differences in chroma features. We discuss the impact of different choices of these features on alignment of musical pieces.",FRA,education,Developed economies,"[-14.655794, -22.498095]","[6.011255, 7.601578]","[3.0558808, -13.111441, -18.104946]","[-6.360004, 22.30455, 4.166702]","[10.96728, 6.09177]","[8.259698, 0.793506]","[11.764074, 12.567327, -1.6645988]","[10.234707, 6.3755336, 13.187131]"
91,Emanuele Coviello;Yonatan Vaizman;Antoni B. Chan;Gert R. G. Lanckriet,Multivariate Autoregressive Mixture Models for Music Auto-Tagging.,2012,https://doi.org/10.5281/zenodo.1416640,"Emanuele Coviello+University of California, San Diego>USA>education;Yonatan Vaizman+University of California, San Diego>USA>education;Antoni B. Chan+City University of Hong Kong>HKG>education;Gert R.G. Lanckriet+University of California, San Diego>USA>education","We propose the multivariate autoregressive model for content based music auto-tagging. At the song level our approach leverages the multivariate autoregressive mixture (ARM) model, a generative time-series model for audio, which assumes each feature vector in an audio fragment is a linear function of previous feature vectors. To tackle tag-model estimation, we propose an efficient hierarchical EM algorithm for ARMs (HEM-ARM), which summarizes the acoustic information common to the ARMs modeling the individual songs associated with a tag. We compare the ARM model with the recently proposed dynamic texture mixture (DTM) model. We hence investigate the relative merits of different modeling choices for music time-series: i) the flexibility of selecting higher memory order in ARM, ii) the capability of DTM to learn specific frequency basis for each particular tag and iii) the effect of the hidden layer of the DT versus the time efficiency of learning and inference with fully observable AR components. Finally, we experiment with a support vector machine (SVM) approach that classifies songs based on a kernel calculated on the frequency responses of the corresponding song ARMs. We show that the proposed approach outperforms SVMs trained on a different kernel function, based on a competing generative model.",USA,education,Developed economies,"[-41.818188, -2.383417]","[40.197014, -4.2520747]","[-14.716786, 15.211096, 10.730225]","[26.718847, 4.1847672, 0.8437651]","[14.504637, 10.592172]","[11.408888, 3.6598632]","[15.637069, 14.081338, 0.15853739]","[13.043163, 6.4497895, 10.961291]"
11,Pablo Sprechmann;Alexander M. Bronstein;Guillermo Sapiro,Real-time Online Singing Voice Separation from Monaural Recordings Using Robust Low-rank Modeling.,2012,https://doi.org/10.5281/zenodo.1414820,Pablo Sprechmann+University of Minnesota>USA>education;Alex Bronstein+Tel Aviv University>ISR>education;Guillermo Sapiro+University of Minnesota>USA>education,"Separating the leading vocals from the musical accompaniment is a challenging task that appears naturally in several music processing applications. Robust principal component analysis (RPCA) has been recently employed to this problem producing very successful results. The method decomposes the signal into a low-rank component corresponding to the accompaniment with its repetitive structure, and a sparse component corresponding to the voice with its quasi-harmonic structure. In this paper we first introduce a non-negative variant of RPCA, termed as robust low-rank non-negative matrix factorization (RNMF). This new framework better suits audio applications. We then propose two efficient feed-forward architectures that approximate the RPCA and RNMF with low latency and a fraction of the complexity of the original optimization method. These approximants allow incorporating elements of unsupervised, semi- and fully-supervised learning into the RPCA and RNMF frameworks. Our basic implementation shows several orders of magnitude speedup compared to the exact solvers with no performance degradation, and allows online and faster-than-real-time processing. Evaluation on the MIR-1K dataset demonstrates state-of-the-art performance.",USA,education,Developed economies,"[-0.09391483, -42.06653]","[-49.432896, -26.152554]","[25.580215, 5.572698, -8.559404]","[-7.7884817, -15.061525, -28.59027]","[9.110567, 10.695135]","[6.2617545, 4.9096317]","[10.87891, 14.532205, 1.3675208]","[9.5236635, 8.721917, 9.792274]"
100,Emilia Gómez;Francisco J. Cañadas-Quesada;Justin Salamon;Jordi Bonada;Pedro Vera-Candeas;Pablo Cabañas Molero,Predominant Fundamental Frequency Estimation vs Singing Voice Separation for the Automatic Transcription of Accompanied Flamenco Singing.,2012,https://doi.org/10.5281/zenodo.1416990,"E. Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education;F. Caňadas+University of Jaen>ESP>education;J. Salamon+Music Technology Group, Universitat Pompeu Fabra>ESP>education;J. Bonada+Music Technology Group, Universitat Pompeu Fabra>ESP>education;P. Vera+University of Jaen>ESP>education;P. Cabañas+University of Jaen>ESP>education","This work evaluates two strategies for predominant fundamental frequency (f0) estimation in the context of melodic transcription from flamenco singing with guitar accompaniment. The first strategy extracts the f0 from salient pitch contours computed from the mixed spectrum; the second separates the voice from the guitar and then performs monophonic f0 estimation. We integrate both approaches with an automatic transcription system, which first estimates the tuning frequency and then implements an iterative strategy for note segmentation and labeling. We evaluate them on a flamenco music collection, including a wide range of singers and recording conditions. Both strategies achieve satisfying results. The separation-based approach yields a good overall accuracy (76.81%), although instrumental segments have to be manually located. The predominant f0 estimator yields slightly higher accuracy (79.72%) but does not require any manual annotation. Furthermore, its accuracy increases (84.68%) if we adapt some algorithm parameters to each analyzed excerpt. Most transcription errors are due to incorrect f0 estimations (typically octave and voicing errors in strong presence of guitar) and incorrect note segmentation in highly ornamented sections. Our study confirms the difficulty of transcribing flamenco singing and the need for repertoire-specific and assisted algorithms for improving state-of-the-art methods.",ESP,education,Developed economies,"[0.3165909, -40.459953]","[-4.6861415, -12.898561]","[28.251612, 5.3130383, -10.41113]","[5.083355, -13.758274, -11.577821]","[9.358124, 10.627824]","[6.871416, 2.3577425]","[10.940052, 14.619897, 1.1764935]","[9.033874, 7.6703615, 11.392359]"
99,Mathieu Lagrange;Alexey Ozerov;Emmanuel Vincent,Robust Singer Identification in Polyphonic Music using Melody Enhancement and Uncertainty-based Learning.,2012,https://doi.org/10.5281/zenodo.1416526,Mathieu Lagrange+IRCAM>FRA>facility|CNRS>FRA>facility|UPMC>FRA>education;Alexey Ozerov+Technicolor>FRA>company;Emmanuel Vincent+INRIA>FRA>facility,"Enhancing specific parts of a polyphonic music signal is believed to be a promising way of breaking the glass ceiling that most Music Information Retrieval (MIR) systems are now facing. The use of signal enhancement as a pre-processing step has led to limited improvement though, because distortions inevitably remain in the enhanced signals that may propagate to the subsequent feature extraction and classification stages. Previous studies attempting to reduce the impact of these distortions have relied on the use of feature weighting or missing feature theory. Based on advances in the field of noise-robust speech recognition, we represent the uncertainty about the enhanced signals via a Gaussian distribution instead that is subsequently propagated to the features and to the classifier. We introduce new methods to estimate the uncertainty from the signal in a fully automatic manner and to learn the classifier directly from polyphonic data. We illustrate the results by considering the task of identifying, from a given set of singers, which one is singing at a given time in a given song. Experimental results demonstrate the relevance of our approach.",FRA,facility,Developed economies,"[-12.525603, -37.678215]","[14.162077, -12.717816]","[18.339167, 14.157273, -21.455997]","[11.579682, 3.8104289, -8.954655]","[10.118606, 11.490322]","[9.185507, 3.5546336]","[11.386596, 15.560193, 0.6962197]","[10.951155, 7.42493, 10.410732]"
98,Polina Proutskova;Christophe Rhodes;Geraint A. Wiggins;Tim Crawford,Breathy or Resonant - A Controlled and Curated Dataset for Phonation Mode Detection in Singing.,2012,https://doi.org/10.5281/zenodo.1415872,"Polina Proutskova+Goldsmiths, University of London>GBR>education;Christophe Rhodes+Goldsmiths, University of London>GBR>education;Geraint Wiggins+Queen Mary, University of London>GBR>education;Tim Crawford+Goldsmiths, University of London>GBR>education","This paper presents a new reference dataset of sustained, sung vowels with attached labels indicating the phonation mode. The dataset is intended for training computational models for automated phonation mode detection. Four phonation modes are distinguished by Johan Sundberg [15]: breathy, neutral, flow (or resonant) and pressed. The presented dataset consists of ca. 700 recordings of nine vowels from several languages, sung at various pitches in various phonation modes. The recorded sounds were produced by one female singer under controlled conditions, following recommendations by voice acoustics researchers. While datasets on phonation modes in speech exist, such resources for singing are not available. Our dataset closes this gap and offers researchers in various disciplines a reference and a training set. It will be made available online under Creative Commons license. Also, the format of the dataset is extensible. Further content additions and future support for the dataset are planned.",GBR,education,Developed economies,"[-8.936144, -41.71782]","[-32.711803, -42.70874]","[20.438478, 6.7258916, -18.669992]","[2.8635237, -11.239763, -19.273369]","[9.835604, 11.02194]","[7.6919627, 4.748457]","[11.118811, 15.138574, 0.6632408]","[10.331944, 7.512366, 9.013926]"
97,Zafar Rafii;Bryan Pardo,Music/Voice Separation Using the Similarity Matrix.,2012,https://doi.org/10.5281/zenodo.1417631,Zafar Rafii+Northwestern University>USA>education|Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education|Northwestern University>USA>education,"Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient.",USA,education,Developed economies,"[3.2422216, -44.225918]","[-44.700443, -29.562218]","[27.404242, 1.6639674, -4.0058126]","[-4.998001, -5.2070217, -30.96344]","[8.660982, 10.2446575]","[6.4859695, 5.151003]","[11.036724, 13.937628, 1.5291433]","[9.865099, 8.646791, 9.720781]"
96,Andrew Hankinson;John Ashley Burgoyne;Gabriel Vigliensoni;Alastair Porter;Jessica Thompson 0001;Wendy Liu;Remi Chiu;Ichiro Fujinaga,Digital Document Image Retrieval Using Optical Music Recognition.,2012,https://doi.org/10.5281/zenodo.1415562,Andrew Hankinson+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;John Ashley Burgoyne+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Gabriel Vigliensoni+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Alastair Porter+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Jessica Thompson+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Wendy Liu+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Remi Chiu+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Ichiro Fujinaga+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility,"Optical music recognition (OMR) and optical character recognition (OCR) have traditionally been used for document transcription—that is, extracting text or symbolic music from page images for use in an editor while discarding all spatial relationships between the transcribed notation and the original image. In this paper we discuss how OCR has shifted fundamentally from a transcription tool to an indexing tool for document image collections resulting from large digitization efforts. OMR tools and procedures, in contrast, are still focused on small-scale modes of operation. We argue that a shift in OMR development towards document image indexing would present new opportunities for searching, browsing, and analyzing large musical document collections. We present a prototype system we built to evaluate the tools and to develop practices needed to process print and manuscript sources.",CAN,facility,Developed economies,"[37.378014, 21.772808]","[-22.449184, 38.724644]","[20.86604, 12.214513, 7.6884456]","[-11.959801, -22.742676, 0.8598459]","[8.735267, 6.184219]","[6.7584, -0.76619846]","[10.759806, 11.225702, -0.17103377]","[7.842335, 4.0655556, 10.572768]"
95,Véronique Sébastien;Henri Ralambondrainy;Olivier Sébastien;Noël Conruyt,Score Analyzer: Automatically Determining Scores Difficulty Level for Instrumental e-Learning.,2012,https://doi.org/10.5281/zenodo.1416518,Véronique Sébastien+University of Reunion Island>FRA>education;Henri Ralambondrainy+University of Reunion Island>FRA>education;Olivier Sébastien+University of Reunion Island>FRA>education;Noël Conruyt+University of Reunion Island>FRA>education,"Nowadays, huge sheet music collections exist on the Web, allowing people to access public domain scores for free. However, beginners may be lost in finding a score appropriate to their instrument level, and should often rely on themselves to start out on the chosen piece. In this instrumental e-Learning context, we propose a Score Analyzer prototype in order to automatically extract the difficulty level of a MusicXML piece and suggest advice thanks to a Musical Sign Base (MSB). To do so, we first review methods related to score performance information retrieval. We then identify seven criteria to characterize technical instrumental difficulties and propose methods to extract them from a MusicXML score. The relevance of these criteria is then evaluated through a Principal Components Analysis and compared to human estimations. Lastly we discuss the integration of this work to @-MUSE, a collaborative score annotation platform based on multimedia contents indexation.",FRA,education,Developed economies,"[-11.066919, 2.8763716]","[2.4763775, 34.50667]","[-16.066502, -19.164515, -5.3912215]","[-6.2970023, -8.144339, 18.266195]","[12.370914, 6.593794]","[10.263353, 0.5371117]","[13.332483, 12.929083, -1.2921327]","[10.767731, 5.2189307, 11.768355]"
94,Juhan Nam;Jorge Herrera;Malcolm Slaney;Julius O. Smith,Learning Sparse Feature Representations for Music Annotation and Retrieval.,2012,https://doi.org/10.5281/zenodo.1415202,Juhan Nam+Stanford University>USA>education;Jorge Herrera+Stanford University>USA>education;Malcolm Slaney+Yahoo! Research>USA>company;Julius Smith+Stanford University>USA>education,"We present a data-processing pipeline based on sparse feature learning and describe its applications to music annotation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are handcrafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features automatically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning algorithms to music data, in particular, focusing on a high-dimensional sparse-feature representation. Our experiments show that, using only a linear classifier, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and retrieval systems.",USA,education,Developed economies,"[-13.245596, 14.733173]","[21.714113, -12.653233]","[-7.082199, 12.00301, -7.572514]","[19.156982, 8.317903, -8.874707]","[13.035078, 8.955744]","[9.884368, 4.143545]","[13.794655, 14.140395, 0.048959926]","[11.453792, 6.8469276, 10.08598]"
93,Juan J. Bosch;Jordi Janer;Ferdinand Fuhrmann;Perfecto Herrera,A Comparison of Sound Segregation Techniques for Predominant Instrument Recognition in Musical Audio Signals.,2012,https://doi.org/10.5281/zenodo.1416076,Juan J. Bosch+Universitat Pompeu Fabra>ESP>education;Jordi Janer+Universitat Pompeu Fabra>ESP>education;Ferdinand Fuhrmann+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"The authors address the identification of predominant music instruments in polytimbral audio by previously dividing the original signal into several streams. Several strategies are evaluated, ranging from low to high complexity with respect to the segregation algorithm and models used for classification. The dataset of interest is built from professionally produced recordings, which typically pose problems to state-of-art source separation algorithms. The recognition results are improved a 19% with a simple sound segregation pre-step using only panning information, in comparison to the original algorithm. In order to further improve the results, we evaluated the use of a complex source separation as a pre-step. The results showed that the performance was only enhanced if the recognition models are trained with the features extracted from the separated audio streams. In this way, the typical errors of state-of-art separation algorithms are acknowledged, and the performance of the original instrument recognition algorithm is improved in up to 32%.",ESP,education,Developed economies,"[8.093232, -40.612686]","[-41.774014, -27.728388]","[22.685423, -4.2438703, -3.6670156]","[-8.215404, -5.108656, -29.402782]","[8.463316, 9.407995]","[6.719267, 5.3716087]","[11.071926, 12.998234, 0.9969707]","[9.872869, 8.445051, 9.577478]"
92,Philippe Hamel;Yoshua Bengio;Douglas Eck,Building Musically-relevant Audio Features through Multiple Timescale Representations.,2012,https://doi.org/10.5281/zenodo.1416530,Philippe Hamel+Université de Montréal>CAN>education;Yoshua Bengio+Université de Montréal>CAN>education;Douglas Eck+Google Inc.>USA>company,"Low-level aspects of music audio such as timbre, loudness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and require a better representation of time dynamics. For various music information retrieval tasks, one would benefit from modelling both low and high level aspects in a unified feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale features. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for automatic tag annotation.",CAN,education,Developed economies,"[-17.152481, -7.794892]","[39.842014, -2.9018161]","[-5.2997704, 10.477693, 11.029575]","[26.261404, 6.875614, 0.7488438]","[12.286048, 8.973786]","[11.426029, 3.534155]","[13.383284, 13.778635, 0.030425686]","[12.963431, 6.5473876, 11.13102]"
65,Jin Ha Lee;Sally Jo Cunningham,The Impact (or Non-impact) of User Studies in Music Information Retrieval.,2012,https://doi.org/10.5281/zenodo.1416272,Jin Ha Lee+University of Washington>USA>education;Sally Jo Cunningham+University of Waikato>NZL>education,"Most Music Information Retrieval (MIR) researchers will agree that understanding users' needs and behaviors is critical for developing a good MIR system. The number of user studies in the MIR domain has been gradually increasing since the early 2000s reflecting the need for empirical studies of users. However, despite the growing number of user studies and the wide recognition of their importance, it is unclear how large their impact has been in the field; on how systems are developed, evaluation tasks are created, and how we understand critical concepts such as music similarity or music mood. In this paper, we present our analysis on the growth, publication and citation patterns, and design of 155 user studies. This is followed by a discussion of a number of issues/challenges in conducting MIR user studies and distributing the research results. We conclude by making recommendations to increase the visibility and impact of user studies in the field.",USA,education,Developed economies,"[-23.548426, 22.768435]","[29.351547, 32.631577]","[-16.586197, 9.18134, -9.0965185]","[5.862952, 6.4167156, 17.77995]","[14.584755, 8.183807]","[12.035023, 0.8076984]","[14.613601, 14.917234, -1.9525373]","[12.568697, 4.450105, 12.249353]"
90,Brandon Mechtley;Andreas Spanias;Perry Cook,Shortest Path Techniques for Annotation and Retrieval of Environmental Sounds.,2012,https://doi.org/10.5281/zenodo.1416926,Brandon Mechtley+Arizona State University>USA>education|Arizona State University>USA>education;Perry Cook+Princeton University>USA>education;Andreas Spanias+Arizona State University>USA>education,"Many techniques for text-based retrieval and automatic annotation of music and sound effects rely on learning with explicit generalization, training individual classifiers for each tag. Non-parametric approaches, where queries are individually compared to training instances, can provide added flexibility, both in terms of robustness to shifts in database content and support for foreign queries, such as concepts not yet included in the database. In this paper, we build upon prior work in designing an ontological framework for annotation and retrieval",USA,education,Developed economies,"[-22.29156, -5.3742466]","[38.49484, -6.936826]","[-23.030178, -5.336144, -17.692356]","[22.336887, 14.979448, 0.46202993]","[13.084213, 8.322542]","[11.340849, 3.5138762]","[13.185929, 14.340539, -1.3141196]","[12.513528, 5.6508327, 11.231884]"
0,Nick Collins,Influence in Early Electronic Dance Music: An Audio Content Analysis Investigation.,2012,https://doi.org/10.5281/zenodo.1417621,Nick Collins+University of Sussex>GBR>education,"Audio content analysis can assist investigation of musical influence, given a corpus of date-annotated works. We study a number of techniques which illuminate musicological questions on genre and creative influence. By applying machine learning tests and statistical analysis to a database of early EDM tracks, we examine how distinct putatively different musical genres really are, the retrospectively labelled Detroit techno and Chicago house being the core case study. Further, by building predictive models based on works from earlier years, both by a priori assumed genre groups and by individual tracks, we examine questions of influence, and whether Detroit techno really is a sort of electronic future funk, and Chicago house an electronic extension of disco. We discuss the implications and prospects for modeling musical influence.",GBR,education,Developed economies,"[-36.174126, 14.884268]","[47.21789, 7.489497]","[-29.966885, 0.4159462, -17.808367]","[13.073558, 13.511462, 7.4778857]","[11.826257, 5.290388]","[12.119018, 2.7579281]","[11.662966, 13.83917, -2.333871]","[13.140502, 5.4780226, 11.687294]"
1,Kerstin Neubarth;Izaro Goienetxea;Colin Johnson;Darrell Conklin,Association Mining of Folk Music Genres and Toponyms.,2012,https://doi.org/10.5281/zenodo.1417643,"Kerstin Neubarth+Canterbury Christ Church University>GBR>education|School of Computing, University of Kent>GBR>education;Izaro Goienetxea+University of the Basque Country UPV/EHU>ESP>education;Colin G. Johnson+School of Computing, University of Kent>GBR>education;Darrell Conklin+University of the Basque Country UPV/EHU>ESP>education|IKERBASQUE, Basque Foundation for Science>ESP>facility","This paper demonstrates how association rule mining can be applied to discover relations between two ontologies of folk music: a genre and a region ontology. Genre–region associations have been widely studied in folk music research but have been neglected in music information retrieval. We present a method of association rule mining with constraints consisting of rule templates and rule evaluation measures to identify different, musicologically motivated, categories of genre–region associations. The method is applied to a corpus of 1902 Basque folk tunes, and several interesting rules and rule sets are discovered.",GBR,education,Developed economies,"[-30.1785, -6.1046705]","[14.998774, 37.30157]","[-22.548832, -0.5261688, 17.742323]","[-3.8535028, -0.37081963, 29.235142]","[13.047752, 10.21912]","[10.816499, -0.090624616]","[13.531708, 14.897614, 0.24784924]","[12.36194, 5.5030046, 11.527226]"
3,Yi-Hsuan Yang;Xiao Hu 0001,Cross-cultural Music Mood Classification: A Comparison on English and Chinese Songs.,2012,https://doi.org/10.5281/zenodo.1416666,Yi-Hsuan Yang+Academia Sinica>TWN>facility;Xiao Hu+University of Denver>USA>education,"Most existing studies on music mood classification have been focusing on Western music while little research has investigated whether mood categories, audio features, and classification models developed from Western music are applicable to non-Western music. This paper attempts to answer this question through a comparative study on English and Chinese songs. Specifically, a set of Chinese pop songs were annotated using an existing mood taxonomy developed for English songs. Six sets of audio features commonly used on Western music (e.g., timbre, rhythm) were extracted from both Chinese and English songs, and mood classification performances based on these feature sets were compared. In addition, experiments were conducted to test the generalizability of classification models across English and Chinese songs. Results of this study shed light on cross-cultural applicability of research results on music mood classification.",TWN,facility,Developing economies,"[-52.477123, 4.630494]","[54.21303, -2.4262831]","[-18.743658, 28.869795, 6.753733]","[13.092905, 21.663847, 11.927015]","[13.442549, 12.533003]","[13.221091, 3.565525]","[16.08607, 14.954607, 1.5102975]","[14.289552, 4.9748178, 10.890273]"
4,Geoffroy Peeters;Karën Fort,Towards a (Better) Definition of the Description of Annotated MIR Corpora.,2012,https://doi.org/10.5281/zenodo.1417871,Geoffroy Peeters+STMS IRCAM-CNRS-UPMC>FRA>education|STMS IRCAM-CNRS-UPMC>Unknown>Unknown;Karën Fort+INIST-CNRS & Université Paris 13>FRA>education|INIST-CNRS & Université Paris 13>Unknown>Unknown,"Today, annotated MIR corpora are provided by various research labs or companies, each one using its own annotation methodology, concept definitions, and formats. This is not an issue as such. However, the lack of descriptions of the methodology used—how the corpus was actually annotated, and by whom—and of the annotated concepts, i.e. what is actually described, is a problem with respect to the sustainability, usability, and sharing of the corpora. Experience shows that it is essential to define precisely how annotations are supplied and described. We propose here a survey and consolidation report on the nature of the annotated corpora used and shared in MIR, with proposals for the axis against which corpora can be described so to enable effective comparison and the inherent influence this has on tasks performed using them.",FRA,education,Developed economies,"[-9.6315, 55.056828]","[19.706663, 44.78586]","[-35.79113, -3.1261122, -1.1128136]","[-6.0816007, 7.3029404, 18.65281]","[13.510623, 4.8276367]","[11.498071, 0.4407621]","[14.960293, 11.213896, -1.4544632]","[11.815078, 4.8294544, 11.502697]"
5,Diane Watson;Regan L. Mandryk,Modeling Musical Mood From Audio Features and Listening Context on an In-Situ Data Set.,2012,https://doi.org/10.5281/zenodo.1415234,Diane Watson+University of Saskatchewan>CAN>education;Regan L. Mandryk+University of Saskatchewan>CAN>education,"Real-life listening experiences contain a wide range of music types and genres. We create the first model of musical mood using a data set gathered in-situ during a user’s daily life. We show that while audio features, song lyrics and socially created tags can be used to successfully model musical mood with classification accuracies greater than chance, adding contextual information such as the listener’s affective state or listening context can improve classification accuracy. We successfully classify musical arousal with a classification accuracy of 67% and musical valence with an accuracy of 75% when using both musical features and listening context.",CAN,education,Developed economies,"[-56.822144, 3.6749094]","[52.50105, -6.4310346]","[-21.588713, 24.710554, 3.188709]","[10.548883, 22.743189, 8.13046]","[13.69018, 12.669938]","[13.151806, 3.823459]","[16.21973, 14.7215185, 1.5865716]","[14.153031, 5.0004325, 10.564484]"
6,Eric Battenberg;David Wessel,Analyzing Drum Patterns Using Conditional Deep Belief Networks.,2012,https://doi.org/10.5281/zenodo.1417955,"Eric Battenberg+University of California, Berkeley>USA>education;David Wessel+University of California, Berkeley>USA>education","We present a system for the high-level analysis of beat-synchronous drum patterns to be used as part of a comprehensive rhythmic understanding system. We use a multi-layer neural network, which is greedily pre-trained layer-by-layer using restricted Boltzmann machines (RBMs), in order to model the contextual time-sequence information of a drum pattern. For the input layer of the network, we use a conditional RBM, which has been shown to be an effective generative model of multi-dimensional sequences. Subsequent layers of the neural network can be pre-trained as conditional or standard RBMs in order to learn higher-level rhythmic features. We show that this model can be fine-tuned in a discriminative manner to make accurate predictions about beat-measure alignment. The model generalizes well to multiple rhythmic styles due to the distributed state-space of the multi-layer neural network. In addition, the outputs of the discriminative network can serve as posterior probabilities over beat-alignment labels. These posterior probabilities can be used for Viterbi decoding in a hidden Markov model in order to maintain temporal continuity of the predicted information.",USA,education,Developed economies,"[25.224236, -47.678722]","[-33.733852, -13.961548]","[19.957897, -21.793232, 0.23880434]","[-2.9427648, 12.380169, -18.547718]","[7.7094054, 7.0555234]","[8.195912, 4.3926888]","[10.181955, 11.660585, 0.96253824]","[8.589281, 7.050062, 9.768987]"
7,Erdem Ünal;Baris Bozkurt;Mustafa Kemal Karaosmanoglu,N-gram Based Statistical Makam Detection on Makam Music in Turkey Using Symbolic Data.,2012,https://doi.org/10.5281/zenodo.1417459,Erdem Ünal+TÜBİTAK-BİLGEM>TUR>facility;Barış Bozkurt+Bahçeşehir University>TUR>education;M. Kemal Karaosmanoğlu+Yildiz Technical University>TUR>education,"This work studies the effect of different score representations and the potential of n-grams in makam classification for traditional makam music in Turkey. While makams are defined with various characteristics including a distinct set of pitches, pitch hierarchy, melodic direction, typical phrases and typical makam transitions, such characteristics result in certain n-gram distributions which can be used for makam detection effectively. 13 popular makams, some of which are very similar to each other, are used in this study. Using the leave-one-out strategy, makam models are created statistically and tested against the left out music piece. Tests indicate that n-gram based statistical modeling and perplexity based similarity metric can be effectively used for makam detection. However the main dimension that cannot be captured is the overall progression which is the most unique feature for classification of close makams that uses the same scale notes as well as the same tonic.",TUR,facility,Developing economies,"[10.661881, 6.001932]","[4.470551, -12.589366]","[-15.352855, -23.434599, -14.139799]","[7.0592513, -11.208735, -4.620236]","[11.739309, 10.659138]","[7.68608, 1.7854301]","[12.233385, 14.8833475, -1.5739378]","[9.488436, 6.864741, 11.785428]"
8,Sebastian Böck;Florian Krebs;Markus Schedl,Evaluating the Online Capabilities of Onset Detection Methods.,2012,https://doi.org/10.5281/zenodo.1416036,Sebastian Böck+Johannes Kepler University>AUT>education;Florian Krebs+Johannes Kepler University>AUT>education;Markus Schedl+Johannes Kepler University>AUT>education,"In this paper, we evaluate various onset detection algorithms in terms of their online capabilities. Most methods use some kind of normalization over time, which renders them unusable for online tasks. We modified existing methods to enable online application and evaluated their performance on a large dataset consisting of 27,774 annotated onsets. We focus particularly on the incorporated preprocessing and peak detection methods. We show that, with the right choice of parameters, the maximum achievable performance is in the same range as that of offline algorithms, and that preprocessing can improve the results considerably. Furthermore, we propose a new onset detection method based on the common spectral flux and a new peak-picking method which outperforms traditional methods both online and offline and works with audio signals of various volume levels.",AUT,education,Developed economies,"[30.536829, -26.827648]","[-23.130245, -6.467684]","[8.192167, -22.071648, -7.414527]","[-1.7160504, 6.13883, -11.921539]","[10.269615, 4.964638]","[5.6862454, 2.4909601]","[10.343965, 13.228761, -1.5378942]","[7.9661283, 7.5464935, 10.881158]"
9,Peter Grosche;Joan Serrà;Meinard Müller;Josep Lluís Arcos,Structure-Based Audio Fingerprinting for Music Retrieval.,2012,https://doi.org/10.5281/zenodo.1416170,Peter Grosche+Saarland University>DEU>education|MPI Informatik>DEU>facility;Joan Serrà+Artificial Intelligence Research Institute (IIIA-CSIC)>ESP>facility;Meinard Müller+Bonn University>DEU>education|MPI Informatik>DEU>facility;Josep Ll. Arcos+Artificial Intelligence Research Institute (IIIA-CSIC)>ESP>facility,"Content-based approaches to music retrieval are of great relevance as they do not require any kind of manually generated annotations. In this paper, we introduce the concept of structure fingerprints, which are compact descriptors of the musical structure of an audio recording. Given a recorded music performance, structure fingerprints facilitate the retrieval of other performances sharing the same underlying structure. Avoiding any explicit determination of musical structure, our fingerprints can be thought of as a probability density function derived from a self-similarity matrix. We show that the proposed fingerprints can be compared by using simple Euclidean distances without using any kind of complex warping operations required in previous approaches. Experiments on a collection of Chopin Mazurkas reveal that structure fingerprints facilitate robust and efficient content-based music retrieval. Furthermore, we give a musically informed discussion that also deepens the understanding of this popular Mazurka dataset.",DEU,education,Developed economies,"[-12.69368, 20.038176]","[20.13082, 9.719853]","[-5.117463, 1.922579, -14.785371]","[10.813559, -4.228597, 5.7598004]","[13.066898, 8.018486]","[9.796063, 1.65685]","[13.303755, 14.342847, -1.8028243]","[11.2514305, 6.4054637, 12.518026]"
10,Özgür Izmirli;Gyanendra Sharma,Bridging Printed Music and Audio Through Alignment Using a Mid-level Score Representation.,2012,https://doi.org/10.5281/zenodo.1414770,Özgür İzmirli+Connecticut College>USA>education;Gyanendra Sharma+Connecticut College>USA>education,"We present a system that utilizes a mid-level score representation for aligning printed music to its audio rendition. The mid-level representation is designed to capture an approximation to the musical events present in the printed score. It consists of a template based note detection front-end that seeks to detect notes without regard to musical duration, accidentals or the key signature. The presented method is designed for the commonly used grand staff and the approach is extendable to other types of scores. The image processing consists of page segmentation into lines followed by multiple stages that optimally orient the lines and establish a reference grid to be used in the note identification stage. Both the audio and the printed score are converted into compatible frequency representations. Alignment is performed using dynamic time warping with a specially designed distance measure. The insufficient pitch resolution due to the reductive nature of the mid-level representation is compensated by this pitch tolerant distance measure. Evaluation is carried out at the beat level using annotated scores and audio. The results demonstrate that the approach provides an efficient and practical alternative to methods that rely on symbolic MIDI-like information through OMR methods for alignment.",USA,education,Developed economies,"[19.8028, -11.899115]","[-13.583203, -15.629175]","[1.5082098, -11.828827, -9.901446]","[-4.493451, -20.933556, -1.2154815]","[10.843393, 6.3715262]","[6.353929, 0.38816723]","[11.983491, 12.495425, -1.5229162]","[8.141267, 5.451601, 10.821185]"
89,Xiao Hu 0001;Jin Ha Lee,A Cross-cultural Study of Music Mood Perception between American and Chinese Listeners.,2012,https://doi.org/10.5281/zenodo.1417799,Xiao Hu+The University of Hong Kong>HKG>education;Jin Ha Lee+University of Washington>USA>education,"Music mood has been recognized as an important access point for music and many online music services support browsing by mood. However, how people judge music mood has not been well studied in the Music Information Retrieval (MIR) domain. In particular, people's cultural background is often assumed to be an important factor in music mood perception, but this assumption has not been verified by empirical studies. This paper reports on a study comparing mood judgments on a set of 30 songs by American and Chinese people. Results show that mood judgments do indeed differ between American and Chinese respondents. Furthermore, respondents’ mood judgments tended to agree more with other respondents from the same culture than those from the other group. Both the song characteristics (e.g., genre, lyrical or instrumental) and the non-cultural background of the respondents (e.g., age, gender, familiarity with the songs) were analyzed to further examine the difference in mood judgments. Findings of this study help further our understanding on how cultural background affects mood perception. Also discussed in this paper are implications for designing MIR systems for cross-cultural music mood classification and recommendation.",HKG,education,Developing economies,"[-52.326794, 5.467724]","[54.67448, -2.1026201]","[-19.422333, 29.767494, 5.510237]","[12.23536, 20.44715, 12.510627]","[13.454349, 12.525014]","[13.238883, 3.489602]","[16.106627, 14.89824, 1.4366319]","[14.232571, 5.008451, 10.940996]"
64,Markus Schedl;Arthur Flexer,Putting the User in the Center of Music Information Retrieval.,2012,https://doi.org/10.5281/zenodo.1417941,Markus Schedl+Johannes Kepler University>AUT>education;Arthur Flexer+Austrian Research Institute for Artificial Intelligence>AUT>facility,"Personalized and context-aware music retrieval and recommendation algorithms ideally provide music that perfectly fits the individual listener in each imaginable situation and for each of her information or entertainment need. Although first steps towards such systems have recently been presented at ISMIR and similar venues, this vision is still far away from being a reality. In this paper, we investigate and discuss literature on the topic of user-centric music retrieval and reflect on why the breakthrough in this field has not been achieved yet. Given the different expertises of the authors, we shed light on why this topic is a particularly challenging one, taking a psychological and a computer science view. Whereas the psychological point of view is mainly concerned with proper experimental design, the computer science aspect centers on modeling and machine learning problems. We further present our ideas on aspects vital to consider when elaborating user-aware music retrieval systems, and we also describe promising evaluation methodologies, since accurately evaluating personalized systems is a notably challenging task.",AUT,education,Developed economies,"[-21.512793, 23.737692]","[36.3946, 18.601404]","[-15.242247, 8.722301, -12.801482]","[11.403088, 5.881772, 17.71125]","[14.462518, 7.8407364]","[12.379835, 1.5021781]","[14.303672, 14.707856, -2.1377432]","[13.003698, 4.923494, 12.501898]"
49,W. Bas de Haas;José Pedro Magalhães;Frans Wiering,Improving Audio Chord Transcription by Exploiting Harmonic and Metric Knowledge.,2012,https://doi.org/10.5281/zenodo.1417541,W. Bas de Haas+Utrecht University>NLD>education|Utrecht University>Unknown>Unknown;José Pedro Magalhães+University of Oxford>GBR>education;Frans Wiering+Utrecht University>NLD>education,"We present a new system for chord transcription from polyphonic musical audio that uses domain-specific knowledge about tonal harmony and metrical position to improve chord transcription performance. Low-level pulse and spectral features are extracted from an audio source using the Vamp plugin architecture. Subsequently, for each beat-synchronised chromagram we compute a list of chord candidates matching that chromagram, together with the confidence in each candidate. When one particular chord candidate matches the chromagram significantly better than all others, this chord is selected to represent the segment. However, when multiple chords match the chromagram similarly well, we use a formal music theoretical model of tonal harmony to select the chord candidate that best matches the sequence based on the surrounding chords. In an experiment we show that exploiting metrical and harmonic knowledge yields statistically significant chord transcription improvements on a corpus of 217 Beatles, Queen, and Zweieck songs.",NLD,education,Developed economies,"[50.150723, -4.794224]","[-29.190136, 20.991793]","[21.833313, -10.218951, 15.518556]","[-24.334833, -4.0822477, 4.101071]","[7.1610503, 8.744146]","[6.364504, 3.5303633]","[11.979727, 10.675176, 1.7755821]","[9.775761, 8.69873, 12.235121]"
62,Mohsen Kamalzadeh;Dominikus Baur;Torsten Möller,A Survey on Music Listening and Management Behaviours.,2012,https://doi.org/10.5281/zenodo.1415742,Mohsen Kamalzadeh+Simon Fraser University>CAN>education;Dominikus Baur+University of Calgary>CAN>education;Torsten Möller+Simon Fraser University>CAN>education,"We report the results of a survey on music listening and management behaviours. The survey was conducted online with 222 participants with mostly technical backgrounds drawn from a college age population. The median size of offline music collections was found to be roughly 2540 songs (sum of physical media and digital files). The major findings of our survey show that elements such as familiarity of songs, how distracting they are, how much they match the listener’s mood, and the desire of changing the mood within one listening session, are all affected by the activity during which music is listened to. While people want to have options for manipulating the above elements to control their experience, they prefer a minimal amount of interaction in general. Current music players lack such flexibility in their controls. Finally, online recommender systems have not gained much popularity thus far.",CAN,education,Developed economies,"[-39.764957, 23.702454]","[41.120186, 29.115837]","[-16.253971, 20.023287, -9.017345]","[9.273351, 14.041758, 17.327091]","[15.262756, 8.776445]","[12.94943, 1.1743934]","[15.131969, 15.211931, -1.5027266]","[13.452621, 4.379974, 12.087566]"
35,Wolfgang Fohl;Andreas Meisel;Ivan Turkalj,A Feature Relevance Study for Guitar Tone Classification.,2012,https://doi.org/10.5281/zenodo.1414750,Wolfgang Fohl+HAW Hamburg University of Applied Sciences>DEU>education;Ivan Turkalj+HAW Hamburg University of Applied Sciences>DEU>education;Andreas Meisel+HAW Hamburg University of Applied Sciences>DEU>education,"A series of experiments on the automatic classification of classical guitar sounds with support vector machines has been carried out to investigate the relevance of the features and to minimise the feature set for successful classification. Features used for classification were the time series of the partial tone amplitudes, and of the MFCCs, and the energy distribution of the nontonal percussive sound that is produced in the attack phase of the tone. Furthermore the influence of sound parameters as timbre, player, fret position and string number on the recognition rate is investigated. Finally, several nonlinear kernels are compared in their classification performance. It turns out, that a selection of 505 features out of the full feature set of 1155 elements does only reduce the recognition rate of a linear SVM from 82% to 78%. With the use of a polynomial instead of a linear kernel the recognition rate with the reduced feature set can even be increased to 84%.",DEU,education,Developed economies,"[-23.116865, -15.470478]","[15.031155, -16.30171]","[-12.1742, -0.9271095, 20.553556]","[12.29103, 1.7510719, -11.198738]","[12.300922, 10.55119]","[8.925583, 3.669825]","[13.287814, 13.901516, 1.1960561]","[10.776742, 7.5535583, 10.446952]"
34,Nicolas Boulanger-Lewandowski;Yoshua Bengio;Pascal Vincent,Discriminative Non-negative Matrix Factorization for Multiple Pitch Estimation.,2012,https://doi.org/10.5281/zenodo.1417435,Nicolas Boulanger-Lewandowski+Université de Montréal>CAN>education;Yoshua Bengio+Université de Montréal>CAN>education;Pascal Vincent+Université de Montréal>CAN>education,"In this paper, we present a supervised method to improve the multiple pitch estimation accuracy of the non-negative matrix factorization (NMF) algorithm. The idea is to extend the sparse NMF framework by incorporating pitch information present in time-aligned musical scores in order to extract features that enforce the separability between pitch labels. We introduce two discriminative criteria that maximize inter-class scatter and quantify the predictive potential of a given decomposition using logistic regressors. Those criteria are applied to both the latent variable and the deterministic autoencoder views of NMF, and we devise efficient update rules for each. We evaluate our method on three polyphonic datasets of piano recordings and orchestral instrument mixes. Both models greatly enhance the quality of the basis spectra learned by NMF and the accuracy of multiple pitch estimation.",CAN,education,Developed economies,"[25.470936, -22.54181]","[-48.51291, -24.83437]","[13.300418, -16.383762, -12.900013]","[-6.98359, -13.440959, -26.35659]","[9.9875555, 5.6521697]","[6.2232804, 4.9316416]","[10.867899, 13.174005, -0.841886]","[9.51693, 8.686482, 9.745802]"
33,Gopala K. Koduri;Joan Serrà;Xavier Serra,Characterization of Intonation in Carnatic Music by Parametrizing Pitch Histograms.,2012,https://doi.org/10.5281/zenodo.1416902,"Gopala K. Koduri+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Joan Serrà+Artificial Intelligence Research Institute (IIIA-CSIC)>ESP>facility;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Intonation is an important concept in Carnatic music that is characteristic of a raaga, and intrinsic to the musical expression of a performer. In this paper we approach the description of intonation from a computational perspective, obtaining a compact representation of the pitch track of a recording. First, we extract pitch contours from automatically selected voice segments. Then, we obtain a pitch histogram of its full pitch-range, normalized by the tonic frequency, from which each prominent peak is automatically labelled and parametrized. We validate such parametrization by considering an explorative classification task: three raagas are disambiguated using the characterization of a single peak (a task that would seriously challenge a more naïve parametrization). Results show consistent improvements for this particular task. Furthermore, we perform a qualitative assessment on a larger collection of raagas, showing the discriminative power of the entire representation. The proposed generic parametrization of the intonation histogram should be useful for musically relevant tasks such as performer and instrument characterization.",ESP,education,Developed economies,"[-0.82377553, -27.055996]","[-3.738268, -15.494283]","[13.557841, 0.9741643, -15.172929]","[6.935002, -15.912264, -9.569677]","[10.608104, 10.332907]","[6.9692564, 2.1099515]","[11.428389, 15.193487, -0.3159121]","[8.953549, 7.5455565, 11.533278]"
32,Joe Cheri Ross;Vinutha T. P.;Preeti Rao,Detecting Melodic Motifs from Audio for Hindustani Classical Music.,2012,https://doi.org/10.5281/zenodo.1417587,Joe Cheri Ross+Indian Institute of Technology Bombay>IND>education;Vinutha T. P.+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"Melodic motifs form essential building blocks in Indian Classical music. The motifs, or key phrases, provide strong cues to the identity of the underlying raga in both Hindustani and Carnatic styles of Indian music. Thus the automatic detection of such recurring basic melodic shapes from audio is of relevance in music information retrieval. The extraction of melodic attributes from polyphonic audio and the variability inherent in the performance, which does not follow a predefined score, make the task particularly challenging. In this work, we consider the segmentation of selected melodic motifs from audio signals by computing similarity measures on time series of automatically detected pitch values. The methods are investigated in the context of detecting the signature phrase of Hindustani vocal music compositions (bandish) within and across performances.",IND,education,Developing economies,"[7.761217, 1.7872787]","[5.4049993, -17.285547]","[6.5783916, 4.7290463, -12.208279]","[14.046667, -10.962178, -4.527401]","[11.25064, 10.3410015]","[7.6697617, 1.154076]","[11.841236, 15.439305, -1.1778514]","[9.127463, 6.986418, 12.667591]"
30,Julián Urbano;J. Stephen Downie;Brian McFee;Markus Schedl,How Significant is Statistically Significant? The case of Audio Music Similarity and Retrieval.,2012,https://doi.org/10.5281/zenodo.1418055,Julián Urbano+University Carlos III of Madrid>ESP>education;Brian McFee+University of California at San Diego>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Markus Schedl+Johannes Kepler University Linz>AUT>education,"The principal goal of the annual Music Information Retrieval Evaluation eXchange (MIREX) experiments is to determine which systems perform well and which systems perform poorly on a range of MIR tasks. However, there has been no systematic analysis regarding how well these evaluation results translate into real-world user satisfaction. For most researchers, reaching statistical significance in the evaluation results is usually the most important goal, but in this paper we show that indicators of statistical significance (i.e., small p-value) are eventually of secondary importance. Researchers who want to predict the real-world implications of formal evaluations should properly report upon practical significance (i.e., large effect-size). Using data from the 18 systems submitted to the MIREX 2011 Audio Music Similarity and Retrieval task, we ran an experiment with 100 real-world users that allows us to explicitly map system performance onto user satisfaction. Based upon 2,200 judgments, the results show that absolute system performance needs to be quite large for users to be satisfied, and differences between systems have to be very large for users to actually prefer the supposedly better system. The results also suggest a practical upper bound of 80% on user satisfaction with the current definition of the task. Reflecting upon these findings, we make some recommendations for future evaluation experiments and the reporting and interpretation of results in peer-reviewing.",ESP,education,Developed economies,"[-5.1819777, 15.090654]","[27.650492, 30.199793]","[-7.383083, 8.410357, -2.897755]","[5.8677363, 5.9416313, 13.399419]","[13.155928, 9.226261]","[11.858534, 0.95680684]","[13.640159, 15.167676, -0.7902664]","[12.459367, 4.595718, 12.343093]"
29,Arthur Flexer;Dominik Schnitzer;Jan Schlueter,A MIREX Meta-analysis of Hubness in Audio Music Similarity.,2012,https://doi.org/10.5281/zenodo.1417865,Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Dominik Schnitzer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Jan Schlüter+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"We use results from the 2011 MIREX “Audio Music Similarity and Retrieval” task for a meta analysis of the hub phenomenon. Hub songs appear similar to an undesirably high number of other songs due to a problem of measuring distances in high dimensional spaces. Comparing 17 algorithms we are able to confirm that different algorithms produce very different degrees of hubness. We also show that hub songs exhibit less perceptual similarity to the songs they are close to, according to an audio similarity function, than non-hub songs. Application of the recently introduced method of “mutual proximity” is able to decisively improve this situation.",AUT,facility,Developed economies,"[-5.705642, 16.574612]","[26.65854, 9.503537]","[-9.992693, 8.052605, -1.723907]","[17.963545, -4.089646, 7.774917]","[13.279567, 9.213798]","[10.618096, 2.2042968]","[13.781619, 15.084694, -0.74213105]","[12.118609, 6.396259, 12.623001]"
28,Jason Hockman;Matthew E. P. Davies;Ichiro Fujinaga,"One in the Jungle: Downbeat Detection in Hardcore, Jungle, and Drum and Bass.",2012,https://doi.org/10.5281/zenodo.1417054,Jason A. Hockman+McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility|Distributed Digital Archives and Libraries (DDMAL)>CAN>facility;Matthew E.P. Davies+INESC TEC>PRT>education|Sound and Music Computing Group>PRT>facility;Ichiro Fujinaga+McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility|Distributed Digital Archives and Libraries (DDMAL)>CAN>facility,"Hardcore, jungle, and drum and bass (HJDB) are fast-paced electronic dance music genres that often employ resequenced breakbeats or drum samples from jazz and funk percussionist solos. We present a style-specific method for downbeat detection specifically designed for HJDB. The presented method combines three forms of metrical information in the prediction of downbeats: low-level onset event information; periodicity information from beat tracking; and high-level information from a regression model trained with classic breakbeats. In an evaluation using 206 HJDB pieces, we demonstrate superior accuracy of our style specific method over four general downbeat detection algorithms. We present this result to motivate the need for style-specific knowledge and techniques for improved downbeat detection.",CAN,education,Developed economies,"[36.277645, -37.266872]","[-27.567873, -9.77565]","[13.197017, -27.378136, -3.4815216]","[-6.8070526, 18.09202, -11.600786]","[10.403571, 4.213003]","[5.455608, 2.14742]","[10.130688, 12.777172, -2.2335982]","[7.887189, 6.8454866, 10.773575]"
27,Matthias Mauch;Simon Dixon,A Corpus-based Study of Rhythm Patterns.,2012,https://doi.org/10.5281/zenodo.1414848,"Matthias Mauch+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","We present a corpus-based study of musical rhythm, based on a collection of 4.8 million bar-length drum patterns extracted from 48,176 pieces of symbolic music. Approaches to the analysis of rhythm in music information retrieval to date have focussed on low-level features for retrieval or on the detection of tempo, beats and drums in audio recordings. Musicological approaches are usually concerned with the description or implementation of man-made music theories. In this paper, we present a quantitative bottom-up approach to the study of rhythm that relies upon well-understood statistical methods from natural language processing. We adapt these methods to our corpus of music, based on the realisation that—unlike words—bar-length drum patterns can be systematically decomposed into sub-patterns both in time and by instrument. We show that, in some respects, our rhythm corpus behaves like natural language corpora, particularly in the sparsity of vocabulary. The same methods that detect word collocations allow us to quantify and rank idiomatic combinations of drum patterns. In other respects, our corpus has properties absent from language corpora, in particular, the high amount of repetition and strong mutual information rates between drum instruments. Our findings may be of direct interest to musicians and musicologists, and can inform the design of ground truth corpora and computational models of musical rhythm.",GBR,education,Developed economies,"[43.49914, 8.890501]","[-14.862741, 9.543707]","[-6.9205747, -21.793587, 2.2561762]","[-8.870858, 11.2269, 3.4452627]","[11.942155, 5.5544395]","[8.475678, 2.004148]","[11.42903, 14.157428, -2.0508285]","[9.609483, 6.5252094, 12.022446]"
26,José R. Zapata;Andre Holzapfel;Matthew E. P. Davies;João Lobato Oliveira;Fabien Gouyon,Assigning a Confidence Threshold on Automatic Beat Annotation in Large Datasets.,2012,https://doi.org/10.5281/zenodo.1415080,"José R. Zapata+Music Technology Group, Universitat Pompeu Fabra>ESP>education;André Holzapfel+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Matthew E. P. Davies+Sound and Music Computing Group, INESC TEC>PRT>education|Faculty of Engineering of the University of Porto>PRT>education;João L. Oliveira+Sound and Music Computing Group, INESC TEC>PRT>education|Faculty of Engineering of the University of Porto>PRT>education;Fabien Gouyon+Sound and Music Computing Group, INESC TEC>PRT>education|Faculty of Engineering of the University of Porto>PRT>education","In this paper we establish a threshold for perceptually acceptable beat tracking based on the mutual agreement of a committee of beat trackers. In the first step we use an existing annotated dataset to show that mutual agreement can be used to select one committee member as the most reliable beat tracker for a song. Then we conduct a listening test using a subset of the Million Song Dataset to establish a threshold which results in acceptable quality of the chosen beat output. For both datasets, we obtain a percentage of trackable music of about 73%, and we investigate which data tags are related to acceptable and problematic beat tracking. The results indicate that current datasets are biased towards genres which tend to be easy for beat tracking. The proposed methods provide a means to automatically obtain a confidence value for beat tracking in non-annotated data and to choose between a number of beat tracker outputs.",ESP,education,Developed economies,"[35.701843, -30.401775]","[-33.31435, -1.2680751]","[3.838409, -27.57637, -7.617819]","[-6.722002, 14.023959, -5.9442506]","[10.836007, 4.4193306]","[5.2327895, 1.7481532]","[10.561221, 13.003069, -2.2200868]","[7.396457, 6.7983027, 11.073947]"
24,Yupeng Gu;Christopher Raphael,Modeling Piano Interpretation Using Switching Kalman Filter.,2012,https://doi.org/10.5281/zenodo.1415028,Yupeng Gu+Indiana University>USA>education|Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education|Indiana University>USA>education,"An approach of parsing piano music interpretation is presented. We focus mainly on quantifying expressive timing activities. A small number of different expressive timing behaviors (constant, slowing down, speeding up, accent) are defined in order to explain the tempo discretely. Given a MIDI performance of a piano music, we simultaneously estimate both discrete variables that corresponds to the behaviors and continuous variables that describe tempo. A graphical model is introduced to represent the evolution of the discrete behaviors and tempo progression. We demonstrate a computational method that acquires the approximate most likely configuration of the discrete behaviors and the hidden continuous variable tempo. This configuration represent a “smoothed” version of the performance which greatly reduces parametrization while retaining most of its musicality. Experiments are presented on several MIDI piano music performed on a digital piano. An user study is performed to evaluate our method.",USA,education,Developed economies,"[33.29285, -2.9854732]","[-30.299835, 3.3517437]","[14.830621, 0.48441425, 17.835714]","[-12.4693985, 6.6897354, -5.0067863]","[10.020547, 7.188962]","[5.876077, 1.5469556]","[12.107646, 11.767465, -0.43935567]","[8.330431, 6.3364744, 11.172292]"
23,Jan Wülfing;Martin A. Riedmiller,Unsupervised Learning of Local Features for Music Classification.,2012,https://doi.org/10.5281/zenodo.1414782,Jan Wülfing+University of Freiburg>DEU>education|Unknown>Unknown>Unknown;Martin Riedmiller+University of Freiburg>DEU>education|Unknown>Unknown>Unknown,"In this work we investigate the applicability of unsupervised feature learning methods to the task of automatic genre prediction of music pieces. More specifically we evaluate a framework that recently has been successfully used to recognize objects in images. We first extract local patches from the time-frequency transformed audio signal, which are then pre-processed and used for unsupervised learning of an overcomplete dictionary of local features. For learning we either use a bootstrapped k-means clustering approach or select features randomly. We further extract feature responses in a convolutional manner and train a linear SVM for classification. We extensively evaluate the approach on the GTZAN dataset, emphasizing the influence of important design choices such as dimensionality reduction, pooling and patch dimension on the classification accuracy. We show that convolutional extraction of local feature responses is crucial to reach high performance. Furthermore we find that using this approach, simple and fast learning techniques such as k-means or randomly selected features are competitive with previously published results which also learn features from audio signals.",DEU,education,Developed economies,"[-24.413303, -11.895162]","[21.069864, -13.563625]","[-11.427755, -0.46575126, 13.722613]","[19.361439, 6.5720563, -7.530503]","[12.429153, 10.365115]","[9.902189, 3.8894567]","[13.659933, 13.963486, 1.0520712]","[11.542423, 6.9067645, 10.360293]"
22,David Meredith 0001,A Geometric Language for Representing Structure in Polyphonic Music.,2012,https://doi.org/10.5281/zenodo.1414736,David Meredith+Aalborg University>DNK>education,"In 1981, Deutsch and Feroe proposed a formal language for representing melodic pitch structure that employed the powerful concept of hierarchically-related pitch alphabets. However, neither rhythmic structure nor pitch structure in polyphonic music can be adequately represented using this language. A new language is proposed here that incorporates certain features of Deutsch and Feroe’s model but extends and generalises it to allow for the representation of both rhythm and pitch structure in polyphonic music. The new language adopts a geometric approach in which a passage of polyphonic music is represented as a set of multi-dimensional points, generated by performing transformations on component patterns. The language introduces the concept of a periodic mask, a generalisation of Deutsch and Feroe’s notion of a pitch alphabet, that can be applied to any dimension of a geometric representation, allowing for both rhythms and pitch collections to be represented parsimoniously in a uniform way.",DNK,education,Developed economies,"[11.67032, 10.976943]","[-12.593421, 17.118877]","[1.5580077, -13.888987, 5.3521276]","[-6.6967278, -7.0846243, 4.1840544]","[11.420968, 7.501541]","[7.9323564, 1.7364677]","[12.465815, 13.177564, -0.96239525]","[9.734227, 7.0961423, 12.339612]"
20,Gregory Burlet;Alastair Porter;Andrew Hankinson;Ichiro Fujinaga,Neon.js: Neume Editor Online.,2012,https://doi.org/10.5281/zenodo.1418367,Gregory Burlet+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Alastair Porter+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Andrew Hankinson+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Ichiro Fujinaga+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility,"This paper introduces Neon.js, a browser-based music notation editor written in JavaScript. The editor can be used to manipulate digitally encoded musical scores in square-note notation. This type of notation presents certain challenges to a music notation editor, since many neumes (groups of pitches) are ligatures—continuous graphical symbols that represent multiple notes. Neon.js will serve as a component within an online optical music recognition framework. The primary purpose of the editor is to provide a readily accessible interface to easily correct errors made in the process of optical music recognition. In this context, we envision an environment that promotes crowdsourcing to further the creation of editable and searchable online symbolic music collections and for generating and editing ground-truth data to train optical music recognition algorithms.",CAN,facility,Developed economies,"[-12.6888485, 39.529938]","[-2.0284538, 38.59667]","[-15.9264965, -12.649631, -12.693583]","[-12.4053955, -9.150412, 16.810753]","[12.956099, 6.9963512]","[6.8495083, -0.5026515]","[13.853462, 13.118845, -1.6596823]","[8.1402855, 4.2666545, 10.706897]"
19,Augustin Lefèvre;Francis R. Bach;Cédric Févotte,Semi-supervised NMF with Time-frequency Annotations for Single-channel Source Separation.,2012,https://doi.org/10.5281/zenodo.1415156,Augustin Lefevre+INRIA team SIERRA>FRA>facility|LTCI/Telecom ParisTech>FRA>education;Francis Bach+INRIA team SIERRA>FRA>facility;Cédric Févotte+LTCI/Telecom ParisTech>FRA>education,"We formulate a novel extension of nonnegative matrix fac- torization (NMF) to take into account partial information on source-specific activity in the spectrogram. This infor- mation comes in the form of masking coefficients, such as those found in an ideal binary mask. We show that state-of- the-art results in source separation may be achieved with only a limited amount of correct annotation, and further- more our algorithm is robust to incorrect annotations. Since in practice ideal annotations are not observed, we propose several supervision scenarios to estimate the ideal mask- ing coefficients. First, manual annotations by a trained user on a dedicated graphical user interface are shown to provide satisfactory performance although they are prone to errors. Second, we investigate simple learning strate- gies to predict the Wiener coefficients based on local in- formation around a given time-frequency bin of the spec- trogram. Results on single-channel source separation show that time-frequency annotations allow to disambiguate the source separation problem, and learned annotations open the way for a completely unsupervised learning procedure for source separation with no human intervention.",FRA,facility,Developed economies,"[10.687639, -45.86111]","[-47.502235, -26.700705]","[30.426409, -2.9536505, -9.132974]","[-8.443639, -12.274789, -32.738396]","[8.511146, 9.951995]","[6.2984457, 5.2441883]","[11.05576, 13.664634, 1.5568409]","[9.821322, 8.865468, 9.78145]"
18,Yizhao Ni;Matt McVicar;Raúl Santos-Rodriguez;Tijl De Bie,Using Hyper-genre Training to Explore Genre Information for Automatic Chord Estimation.,2012,https://doi.org/10.5281/zenodo.1415932,Yizhao Ni+University of Bristol>GBR>education;Matt Mcvicar+University of Bristol>GBR>education;Raúl Santos-Rodríguez+University of Bristol>GBR>education;Tijl De Bie+University of Bristol>GBR>education,"Recently a large amount of new chord annotations have been made available. This raises hopes for further development in automatic chord estimation. While more data seems to imply better performance, a major challenge however, is the wide variety of genres covered by these new data. As a result, the genre-independent training scheme as is common today is bound to fail. In this paper we investigate various options for exploring genre information for chord estimation, while also maximally exploiting the full dataset. More specifically, we propose a hyper-genre training scheme in which each genre cluster has its own parameters, tied together by hyper parameters as a Bayesian prior. The results are promising, showing significant improvements over other prevailing training schemes.",GBR,education,Developed economies,"[55.96844, -3.5467741]","[-31.394007, 22.818981]","[23.38114, -10.941427, 19.991428]","[-28.91508, -5.051472, 2.7305079]","[6.824157, 8.680509]","[6.215194, 3.6225898]","[11.909073, 10.406071, 2.0722413]","[9.796085, 8.896634, 12.296512]"
17,Daniel Wolff;Sebastian Stober;Andreas Nürnberger;Tillman Weyde,A Systematic Comparison of Music Similarity Adaptation Approaches.,2012,https://doi.org/10.5281/zenodo.1416600,Daniel Wolff+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Sebastian Stober+Otto-von-Guericke-Universität Magdeburg>DEU>education;Andreas Nurnberger+Otto-von-Guericke-Universität Magdeburg>DEU>education,"In order to support individual user perspectives and different retrieval tasks, music similarity can no longer be considered as a static element of Music Information Retrieval (MIR) systems. Various approaches have been proposed recently that allow dynamic adaptation of music similarity measures. This paper provides a systematic comparison of algorithms for metric learning and higher-level facet distance weighting on the MagnaTagATune dataset. A cross-validation variant taking into account clip availability is presented. Applied on user generated similarity data, its effect on adaptation performance is analyzed. Special attention is paid to the amount of training data necessary for making similarity predictions on unknown data, the number of model parameters and the amount of information available about the music itself.",GBR,education,Developed economies,"[-5.7304897, 13.015233]","[28.147902, 7.6626506]","[-6.852437, 10.763865, 1.3933303]","[10.971313, 1.2077054, 10.94932]","[13.258412, 9.380987]","[10.722841, 2.320521]","[13.627064, 15.270624, -0.68168277]","[12.266638, 6.3958793, 12.357891]"
16,Meinard Müller;Nanzhu Jiang,A Scape Plot Representation for Visualizing Repetitive Structures of Music Recordings.,2012,https://doi.org/10.5281/zenodo.1416866,Meinard Müller+Bonn University>DEU>education|MPI Informatik>DEU>facility;Nanzhu Jiang+Saarland University>DEU>education|MPI Informatik>DEU>facility,"The development of automated methods for revealing the repetitive structure of a given music recording is of central importance in music information retrieval. In this paper, we present a novel scape plot representation that allows for visualizing repetitive structures of the entire music recording in a hierarchical, compact, and intuitive way. In a scape plot, each point corresponds to an audio segment identified by its center and length. As our main contribution, we assign to each point a color value so that two segment properties become apparent. Firstly, we use the lightness component of the color to indicate the repetitiveness of the encoded segment, where we revert to a recently introduced fitness measure. Secondly, we use the hue component of the color to reveal the relations between different segments. To this end, we introduce a novel grouping procedure that automatically maps related segments to similar hue values. By discussing a number of popular and classical music examples, we illustrate the potential and visual appeal of our representation and also indicate limitations.",DEU,education,Developed economies,"[0.4239755, 21.339333]","[5.21387, 4.171407]","[-5.803537, -6.4740033, -4.7119036]","[9.358842, -5.411568, 1.9928538]","[13.426268, 6.8735366]","[9.351199, 2.043057]","[13.635264, 13.538025, -2.2579966]","[10.955994, 6.9474535, 12.320355]"
15,Daichi Sakaue;Takuma Otsuka;Katsutoshi Itoyama;Hiroshi G. Okuno,Bayesian Nonnegative Harmonic-Temporal Factorization and Its Application to Multipitch Analysis.,2012,https://doi.org/10.5281/zenodo.1418163,Daichi Sakaue+Kyoto University>JPN>education;Takuma Otsuka+Kyoto University>JPN>education;Katsutoshi Itoyama+Kyoto University>JPN>education;Hiroshi G. Okuno+Kyoto University>JPN>education,"Since important musical features are mutually dependent, their relations should be analyzed simultaneously. Their Bayesian analysis is particularly important to reveal their statistical relation. As the first step for a unified music content analyzer, we focus on the harmonic and temporal structures of the wavelet spectrogram obtained from harmonic sounds. In this paper, we present a new Bayesian multipitch analyzer, called Bayesian non-negative harmonic-temporal factorization (BNHTF). BNHTF models the harmonic and temporal structures separately based on Gaussian mixture model. The input signal is assumed to contain a finite number of harmonic sounds. Each harmonic sound is assumed to emit a large number of sound quanta over the time-log-frequency domain. The observation probability is expressed as the product of two Gaussian mixtures. The number of quanta is calculated in the ϵ-neighborhood of each grid point on the spectrogram. BNHTF integrates latent harmonic allocation (LHA) and nonnegative matrix factorization (NMF) to estimate both the observation probability and the number of quanta. The model is optimized by newly designed deterministic procedures with several approximations for the variational Bayesian inference. Results of experiments on multipitch estimation with 40 musical pieces showed that BNHTF outperforms the conventional method by 0.018 in terms of F-measure on average.",JPN,education,Developed economies,"[36.953743, -14.120388]","[-44.360664, -21.123583]","[13.052836, -22.784197, 11.098544]","[-1.0615611, -10.990817, -28.619827]","[8.950979, 8.994724]","[6.406112, 4.6547146]","[11.364036, 13.413325, 0.21443221]","[9.7515, 8.645018, 10.214243]"
14,Timothée Gerber;Martin Dutasta;Laurent Girin;Cédric Févotte,Professionally-produced Music Separation Guided by Covers.,2012,https://doi.org/10.5281/zenodo.1417157,Timothée Gerber+Grenoble-INP>FRA>education;Martin Dutasta+Grenoble-INP>FRA>education;Laurent Girin+Grenoble-INP>FRA>education;Cédric Févotte+TELECOM ParisTech>FRA>education|CNRS LTCI>FRA>facility,"This paper addresses the problem of demixing professionally produced music, i.e., recovering the musical source signals that compose a (2-channel stereo) commercial mix signal. Inspired by previous studies using MIDI synthesized or hummed signals as external references, we propose to use the multitrack signals of a cover interpretation to guide the separation process with a relevant initialization. This process is carried out within the framework of the multichannel convolutive NMF model and associated EM/MU estimation algorithms. Although subject to the limitations of the convolutive assumption, our experiments confirm the potential of using multitrack cover signals for source separation of commercial music.",FRA,education,Developed economies,"[5.6441054, -43.42854]","[-43.26534, -27.763678]","[28.776253, -2.9056695, 0.60716146]","[-9.10985, -8.617168, -28.92935]","[8.406831, 9.987018]","[6.4567027, 5.3301287]","[11.031674, 13.615537, 1.6452953]","[9.839439, 8.683855, 9.5951395]"
13,Kazuyoshi Yoshii;Masataka Goto,Infinite Composite Autoregressive Models for Music Signal Analysis.,2012,https://doi.org/10.5281/zenodo.1414992,Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents novel probabilistic models that can be used to estimate multiple fundamental frequencies (F0s) from polyphonic audio signals. These models are nonparametric Bayesian extensions of nonnegative matrix factorization (NMF) based on the source-filter paradigm, and in them an amplitude or power spectrogram is decomposed as the product of two kinds of spectral atoms (sources and filters) and time-varying gains of source-filter pairs. In this study we model musical instruments as autoregressive systems that combine two types of sources—periodic signals (comb-shaped densities) and white noise (flat density)—with all-pole filters representing resonance characteristics. One of the main problems with such composite autoregressive models (CARMs) is that the numbers of sources and filters should be given in advance. To solve this problem, we propose nonparametric Bayesian models based on gamma processes and efficient variational and multiplicative learning algorithms. These infinite CARMs (iCARMs) can discover appropriate numbers of sources and filters in a data-driven manner. We report the experimental results of multipitch analysis on the MAPS piano database.",JPN,facility,Developed economies,"[21.157803, -1.5120075]","[-44.998337, -21.112804]","[3.4322517, -2.3072264, 12.915041]","[-0.9760103, -12.173854, -29.729118]","[10.311841, 8.36257]","[6.404616, 4.6645594]","[12.808145, 12.617754, 0.098527804]","[9.768192, 8.659585, 10.222111]"
12,Frederic Font;Joan Serrà;Xavier Serra,Folksonomy-based Tag Recommendation for Online Audio Clip Sharing.,2012,https://doi.org/10.5281/zenodo.1415700,"Frederic Font+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Joan Serrà+Artificial Intelligence Research Institute (IIIA-CSIC)>ESP>facility;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Collaborative tagging has emerged as an efficient way to semantically describe online resources shared by a community of users. However, tag descriptions present some drawbacks such as tag scarcity or concept inconsistencies. In these situations, tag recommendation strategies can help users in adding meaningful tags to the resources being described. Freesound is an online audio clip sharing site that uses collaborative tagging to describe a collection of more than 140,000 sound samples. In this paper we propose four algorithm variants for tag recommendation based on tag co-occurrence in the Freesound folksonomy. On the basis of removing a number of tags that have to be later predicted by the algorithms, we find that using ranks instead of raw tag similarities produces statistically significant improvements. Moreover, we show how specific strategies for selecting the appropriate number of tags to be recommended can significantly improve algorithms’ performance. These two aspects provide insight into some of the most basic components of tag recommendation systems, and we plan to exploit them in future real-world deployments.",ESP,education,Developed economies,"[-42.71953, 0.701075]","[40.145252, 1.931879]","[-16.887848, 16.811552, 6.250419]","[23.64151, 6.7366166, 7.0724487]","[14.56876, 10.440665]","[11.808046, 3.0684009]","[15.602423, 14.253168, -0.03369727]","[13.238182, 6.1690187, 11.582281]"
36,Ruben Hillewaere;Bernard Manderick;Darrell Conklin,String Methods for Folk Tune Genre Classification.,2012,https://doi.org/10.5281/zenodo.1416690,"Ruben Hillewaere+Vrije Universiteit Brussel>BEL>education;Bernard Manderick+Vrije Universiteit Brussel>BEL>education;Darrell Conklin+IKERBASQUE, Basque Foundation for Science>ESP>facility","In folk song research, string methods have been widely used to retrieve highly similar tunes or to perform tune family classification. In this study, we investigate how various string methods perform on a fundamentally different classification task, which is to classify folk tunes into genres, the genres being the dance types of the tunes. A new data set Dance-9 is therefore introduced. The different string method classification accuracies are compared with each other and also with n-gram models and global feature models which have been proven to be useful in previous folk song research. They are shown to yield similar results to the global feature models, but are outperformed by the n-gram models.",BEL,education,Developed economies,"[-30.061733, -8.373612]","[18.583736, -5.283593]","[-20.077175, 0.9343908, 19.586279]","[8.061251, 10.0727, -3.6703532]","[12.831377, 10.592479]","[9.52856, 3.2477524]","[13.724554, 14.457314, 1.086521]","[11.413037, 7.1061215, 10.931559]"
63,Emmanouil Benetos;Simon Dixon;Dimitrios Giannoulis;Holger Kirchhoff;Anssi Klapuri,Automatic Music Transcription: Breaking the Glass Ceiling.,2012,https://doi.org/10.5281/zenodo.1415088,Emmanouil Benetos+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education;Dimitrios Giannoulis+Queen Mary University of London>GBR>education;Holger Kirchhoff+Queen Mary University of London>GBR>education;Anssi Klapuri+Queen Mary University of London>GBR>education,"Automatic music transcription is considered by many to be the Holy Grail in the field of music signal analysis. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. In order to overcome the limited performance of transcription systems, algorithms have to be tailored to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information across different methods and musical aspects.",GBR,education,Developed economies,"[26.351248, -9.08629]","[-9.999128, -14.682911]","[16.878899, -3.5615556, 10.625843]","[-1.2053384, -13.42674, -9.669964]","[9.687994, 7.709953]","[7.1074085, 2.5489297]","[11.812812, 12.336014, -0.07553169]","[9.140078, 7.0223737, 10.684562]"
37,Mustafa Kemal Karaosmanoglu,A Turkish Makam Music Symbolic Database for Music Information Retrieval: SymbTr.,2012,https://doi.org/10.5281/zenodo.1416722,M. Kemal Karaosmanoğlu+Yıldız Technical University>TUR>education,"Turkish makam music needs a comprehensive database for public consumption, to be used in MIR. This article introduces SymbTr, a Turkish Makam Music Symbolic Representation Database, aimed at filling this void. SymbTr consists of musical information in text, PDF, and MIDI formats. Raw data, drawn from reliable sources, and consisting of 1,700 musical pieces in Turkish art and folk music was processed featuring distinct examples in 155 diverse makams, 100 usuls and 48 forms. Special care was devoted to selection of works that scatter across a broad historical time span and were among those still performed today. Total number of musical notes in these pieces was 630,000, corresponding to a nominal playback time of 72 hours. Synthesized sounds particular to Turkish makam music were used in MIDI playback, and transcription/playback errors were corrected by input from experts. Symbolic representation data, open to the public, is output from a computer program developed exclusively for Turkish makam music. SymbTr was designed as a wholesome representation of aforementioned distinct auditory and visual features that distinguish Turkish makam music from other music genres. This article explains the database format in detail, and also provides, through examples, statistical information on pitch/interval allocation and distribution.",TUR,education,Developing economies,"[15.516473, 16.270432]","[-4.205531, 23.104815]","[-5.2517076, -5.9225607, 17.242361]","[6.3104777, -12.734501, -2.8139045]","[12.446852, 7.1968517]","[7.8550415, 1.5277561]","[13.607048, 12.698444, -1.2565405]","[9.668325, 6.3147435, 11.747065]"
39,Frédéric Bimbot;Emmanuel Deruty;Gabriel Sargent;Emmanuel Vincent,"Semiotic Structure Labeling of Music Pieces: Concepts, Methods and Annotation Conventions.",2012,https://doi.org/10.5281/zenodo.1416412,Frédéric Bimbot+IRISA / METISS>FRA>facility;Emmanuel Deruty+INRIA / METISS>FRA>facility;Gabriel Sargent+IRISA / METISS>FRA>facility;Emmanuel Vincent+INRIA / METISS>FRA>facility,"Music structure description, i.e. the task of representing the high-level organization of music pieces in a concise, generic and reproducible way, is currently a scientific challenge both algorithmically and conceptually. In this paper, we focus on semiotic structure, i.e. the description of similarities and internal relationships within a music piece, as a low-rate stream of arbitrary symbols from a limited alphabet and we address methodological questions related to annotation. We formulate the labeling task as a blind demodulation problem, whose goal is to identify a minimal set of semiotic codewords, whose realizations within the music piece are subject to a number of connotative variations viewed as modulations. The determination of labels is achieved by combining morphological, paradigmatic and syntagmatic considerations relying respectively on (i) a morphological model of semiotic blocks in order to define their individual properties, (ii) the support of prototypical structural patterns to guide the comparison between blocks and (iii) a methodology for the determination of distinctive features across semiotic classes. Specific notations are introduced to account for unresolvable semiotic ambiguities, which are occasional but must be considered as inherent to the music matter itself. A set of 500 music pieces labeled in accordance with the proposed concepts and annotation conventions is being released with this article.",FRA,facility,Developed economies,"[0.48446864, 7.147362]","[-3.2213652, 18.681665]","[-7.4741874, -6.186333, 1.7862002]","[-4.081963, -2.3152988, 5.432584]","[12.325925, 8.506882]","[8.793984, 2.0518208]","[12.8971815, 14.065481, -0.9340819]","[10.311352, 6.6234865, 11.7942505]"
61,Jill Palzkill Woelfer;Jin Ha Lee,The Role Of Music in the Lives of Homeless Young People: A Preliminary Report.,2012,https://doi.org/10.5281/zenodo.1415864,Jill Palzkill Woelfer+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"This paper is a preliminary report of findings in an ongoing study of the role of music in the lives of homeless young people which is taking place in Vancouver, British Columbia and Seattle, WA. One hundred homeless young people in Vancouver took part in online surveys, 20 of these young people participated in interviews and 64 completed design activities. Surveys included demographic and music questions. Interviews consisted of questions about music listening and preferences. In the design activities, participants envisioned a music device and provided a drawing and a scenario. Since the study is ongoing, findings are limited to descriptive analysis of survey data supplemented with interview data. These findings provide initial insights into music listening behaviors, social aspects of shared music interests, and preferred music genres, bands and artists, and moods.",USA,education,Developed economies,"[-35.5504, 23.18839]","[40.341534, 33.4925]","[-20.44644, 17.387745, -5.979655]","[7.2668643, 14.461416, 21.54543]","[15.262428, 8.679574]","[12.922521, 0.9605893]","[15.203925, 15.116365, -1.4852462]","[13.383761, 4.257012, 11.9131]"
60,Robert Macrae;Simon Dixon,Ranking Lyrics for Online Search.,2012,https://doi.org/10.5281/zenodo.1416168,Robert Macrae+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"When someone wishes to find the lyrics for a song they typically go online and use a search engine. There are a large number of lyrics available on the internet as the effort required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the user with customary search engine page ranking formula deciding the ordering of these results based on links, views, clicks, etc. However the content, and specifically, the accuracy of the lyrics in question are not analysed or used in any way to determine the rank of the lyrics, despite this being of concern to the searcher. In this work, we show that online lyrics are often inaccurate and the ranking methods used by search engines do not distinguish the more accurate annotations. We present an alternative method for ranking lyrics based purely on the collection of lyrics themselves using the Lyrics Concurrence.",GBR,education,Developed economies,"[-30.598618, -32.445454]","[33.82356, -11.898704]","[5.9390783, 21.816242, -2.7288291]","[14.459853, -10.715353, 15.238585]","[11.490397, 11.705331]","[10.729116, 2.8536515]","[12.446359, 15.96042, 1.0524346]","[12.279142, 6.4370294, 11.666988]"
59,Mohamed Sordo;Joan Serrà;Gopala K. Koduri;Xavier Serra,Extracting Semantic Information from an Online Carnatic Music Forum.,2012,https://doi.org/10.5281/zenodo.1416828,"Mohamed Sordo+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Artificial Intelligence Research Institute (IIIA-CSIC)>ESP>facility;Joan Serra+Artificial Intelligence Research Institute (IIIA-CSIC)>ESP>facility;Gopala K. Koduri+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","By mining user-generated text content we can obtain music-related information that could not otherwise be extracted from audio signals or symbolic score representations. In this paper we propose a methodology for extracting music-related semantic information from an online discussion forum, rasikas.org, dedicated to the Carnatic music tradition. We first define a dictionary of relevant terms within categories such as raagas, taalas, performers, composers, and instruments, and create a complex network representation by matching such dictionary against the forum posts. This network representation is used to identify popular terms within the forum, as well as relevant co-occurrences and semantic relationships. This way, for instance, we are able to learn the instrument played by a performer with 95% accuracy, to discover the confusion between two raagas with different naming conventions, or to infer semantic relationships regarding lineage or musical influence. This contribution is a first step towards the automatic creation of ontologies for specific musical cultures.",ESP,education,Developed economies,"[-30.756607, 13.239236]","[15.817445, 38.02946]","[-19.228846, 10.054299, 1.6035532]","[-3.2634742, -1.5578238, 27.563066]","[14.033969, 9.250531]","[10.85053, -0.14985777]","[14.891359, 13.9761095, -0.8550407]","[12.241887, 5.245857, 11.563124]"
58,Joshua L. Moore;Shuo Chen;Thorsten Joachims;Douglas Turnbull,Learning to Embed Songs and Tags for Playlist Prediction.,2012,https://doi.org/10.5281/zenodo.1416966,Joshua L. Moore+Cornell University>USA>education;Shuo Chen+Cornell University>USA>education;Thorsten Joachims+Cornell University>USA>education;Douglas Turnbull+Ithaca College>USA>education,"Automatically generated playlists have become an important medium for accessing and exploring large collections of music. In this paper, we present a probabilistic model for generating coherent playlists by embedding songs and social tags in a unified metric space. We show how the embedding can be learned from example playlists, providing the metric space with a probabilistic meaning for song/song, song/tag, and tag/tag distances. This enables at least three types of inference. First, our models can generate new playlists, outperforming conventional n-gram models in terms of predictive likelihood by orders of magnitude. Second, the learned tag embeddings provide a generalizing representation for embedding new songs, allowing it to create playlists even for songs it has never observed in training. Third, we show that the embedding space provides an effective metric for matching songs to natural-language queries, even if tags for a large fraction of the songs are missing.",USA,education,Developed economies,"[-43.312897, -0.58620787]","[38.782906, 0.4685478]","[-12.947043, 16.59585, 5.750867]","[22.314466, 6.7915597, 4.488503]","[14.47668, 10.428386]","[11.705979, 3.076302]","[15.626011, 14.202781, 0.051913515]","[13.200565, 6.18779, 11.623199]"
57,Brian McFee;Gert R. G. Lanckriet,Hypergraph Models of Playlist Dialects.,2012,https://doi.org/10.5281/zenodo.1415618,"Brian McFee+University of California, San Diego>USA>education|University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education|University of California, San Diego>USA>education","Playlist generation is an important task in music information retrieval. While previous work has treated a playlist collection as an undifferentiated whole, we propose to build playlist models which are tuned to specific categories or dialects of playlists. Toward this end, we develop a general class of flexible and scalable playlist models based upon hypergraph random walks. To evaluate the proposed models, we present a large corpus of categorically annotated, user-generated playlists. Experimental results indicate that category-specific models can provide substantial improvements in accuracy over global playlist models.",USA,education,Developed economies,"[-42.539433, 37.514023]","[38.271805, 22.210484]","[-7.246322, 31.941994, -2.1828055]","[17.873161, 6.0737314, 22.349613]","[16.083418, 8.257409]","[12.267761, 1.7055008]","[16.428846, 14.838469, -1.6682875]","[13.21989, 5.0777726, 12.996908]"
56,Rudolf Mayer;Andreas Rauber,Towards Time-resilient MIR Processes.,2012,https://doi.org/10.5281/zenodo.1416310,Rudolf Mayer+Secure Business Austria>AUT>company;Andreas Rauber+Secure Business Austria>AUT>company,"In experimental sciences, under which we may likely subsume most research areas in MIR, repeatability is one of the key cornerstones of validating research and measuring progress. Yet, due to the complexity of typical MIR experiments, ensuring the capability of re-running any experiment, achieving exactly identical outputs is challenging at best. Performance differences observed may be attributed to incomplete documentation of the process, slight variations in data (preprocessing) or software libraries used, and others. Digital preservation aims at keeping digital objects authentically accessible and usable over long time spans. While traditionally focussed on individual objects, research is now moving towards the preservation of entire processes. In this paper we present the challenges of preserving a classical MIR process, i.e. music genre classifications, discuss the kinds of context information to be captured, as well as means to validate the re-execution of a preserved process.",AUT,company,Developed economies,"[-9.376571, 57.67585]","[22.379385, 43.56422]","[-37.88096, -0.8181841, -6.394656]","[-3.1732297, 6.9802527, 15.687225]","[13.651188, 4.6514606]","[11.494246, 0.5751765]","[15.046941, 11.072169, -1.4933134]","[11.84088, 4.654424, 11.630415]"
55,Xiao Hu 0001;Noriko Kando,User-centered Measures vs. System Effectiveness in Finding Similar Songs.,2012,https://doi.org/10.5281/zenodo.1416868,Xiao Hu+The University of Hong Kong>HKG>education;Noriko Kando+National Institute of Informatics>JPN>facility,"User evaluation in the domain of Music Information Retrieval (MIR) has been very scarce, while algorithms and systems in MIR have been improving rapidly. With the maturity of system-centered evaluation in MIR, time is ripe for MIR evaluation to involve users. In this study, we compare user-centered measures to a system effectiveness measure on the task of retrieving similar songs. To collect user-centered measures, we conducted a user experiment with 50 participants using a set of music retrieval systems that have been evaluated by a system-centered approach in the Music Information Retrieval Evaluation eXchange (MIREX). The results reveal weak correlation between user-centered measures and system effectiveness. It is also found that user-centered measures can disclose difference between systems when there was no difference on system-effectiveness.",HKG,education,Developing economies,"[-28.836756, 23.964853]","[27.958374, 31.069109]","[-8.523215, 14.557241, -3.85349]","[5.3811655, 5.953178, 14.922202]","[14.679741, 8.746084]","[11.8897295, 0.8264908]","[14.644965, 15.09457, -1.4855366]","[12.439694, 4.525176, 12.318892]"
54,Erik M. Schmidt;Jeffrey J. Scott;Youngmoo E. Kim,Feature Learning in Dynamic Environments: Modeling the Acoustic Structure of Musical Emotion.,2012,https://doi.org/10.5281/zenodo.1414846,Erik M. Schmidt+Drexel University>USA>education;Jeffrey Scott+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"While emotion-based music organization is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant feature representation for music emotion recognition has yet emerged. Much of the difficulty in developing emotion-based features is the ambiguity of the ground-truth. Even using the smallest time window, opinions about emotion are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled human response labels to music in the arousal-valence (A-V) emotion space with time-varying stochastic distributions. Current methods for automatic detection of emotion in music seek performance increases by combining several feature domains (e.g. loudness, timbre, harmony, rhythm). Such work has focused largely in dimensionality reduction for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this work, we seek to employ regression-based deep belief networks to learn features directly from magnitude spectra. Taking into account the dynamic nature of music, we investigate combining multiple timescales of aggregated magnitude spectra as a basis for feature learning.",USA,education,Developed economies,"[-59.524063, 2.2812967]","[49.17275, -9.9760275]","[-26.752092, 26.667433, 3.8510072]","[10.397643, 25.32847, 2.861264]","[14.004332, 12.791704]","[13.019271, 4.368649]","[16.113462, 14.40038, 1.8362877]","[14.149858, 5.0142245, 10.126776]"
53,Mert Bay;Andreas F. Ehmann;James W. Beauchamp;Paris Smaragdis;J. Stephen Downie,Second Fiddle is Important Too: Pitch Tracking Individual Voices in Polyphonic Music.,2012,https://doi.org/10.5281/zenodo.1418121,Mert Bay+University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education;James W. Beauchamp+University of Illinois at Urbana-Champaign>USA>education;Paris Smaragdis+University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education,"Recently, there has been much interest in automatic pitch estimation and note tracking of polyphonic music. To date, however, most techniques produce a representation where pitch estimates are not associated with any particular instrument or voice. Therefore, the actual tracks for each instrument are not readily accessible. Access to individual tracks is needed for more complete music transcription and additionally will provide a window to the analysis of higher constructs such as counterpoint and instrument theme imitation during a composition. In this paper, we present a method for tracking the pitches (F0s) of individual instruments in polyphonic music. The system uses a pre-learned dictionary of spectral basis vectors for each note for a variety of musical instruments. The method then formulates the tracking of pitches of individual voices in a probabilistic manner by attempting to explain the input spectrum as the most likely combination of musical instruments and notes drawn from the dictionary. The method has been evaluated on a subset of the MIREX multiple-F0 estimation test dataset, showing promising results.",USA,education,Developed economies,"[15.780087, -24.586874]","[-7.9841413, -9.485437]","[10.744294, -13.640961, -9.142743]","[-0.13890621, -10.905403, -13.231034]","[10.093906, 5.7709713]","[6.7413435, 2.8441002]","[11.041954, 13.291181, -0.89491284]","[8.983168, 7.791019, 10.770875]"
52,Oriol Nieto;Eric J. Humphrey;Juan Pablo Bello,Compressing Music Recordings into Audio Summaries.,2012,https://doi.org/10.5281/zenodo.1415802,Oriol Nieto+New York University>USA>education;Eric J. Humphrey+New York University>USA>education;Juan Pablo Bello+New York University>USA>education,"We present a criterion to generate audible summaries of music recordings that optimally explain a given track with mutually disjoint segments of itself. We represent audio as sequences of beat-synchronous harmonic features and use an exhaustive search to identify the best summary. To demonstrate the merit of this approach, we evaluate the criterion and show consistency across a collection of multiple recordings of different works. Finally, we present a fast algorithm that approximates the exhaustive search and allows us to automatically learn the hyperparameters of the algorithm for a given track.",USA,education,Developed economies,"[-2.9861891, -13.025392]","[-1.2772788, 0.08054368]","[11.051201, -7.2084475, -17.129225]","[2.9645832, -1.3727744, -2.4417183]","[12.295066, 8.177747]","[7.9670205, 2.7048159]","[13.098186, 14.02687, -0.59675336]","[10.19578, 7.526729, 11.60826]"
51,Hirokazu Kameoka;Kazuki Ochiai;Masahiro Nakano;Masato Tsuchiya;Shigeki Sagayama,Context-free 2D Tree Structure Model of Musical Notes for Bayesian Modeling of Polyphonic Spectrograms.,2012,https://doi.org/10.5281/zenodo.1415252,Hirokazu Kameoka+The University of Tokyo>JPN>education|NTT Corporation>JPN>company;Kazuki Ochiai+The University of Tokyo>JPN>education;Masahiro Nakano+NTT Corporation>JPN>company;Masato Tsuchiya+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"This paper proposes a Bayesian model for automatic music transcription. Automatic music transcription involves several subproblems that are interdependent of each other: multiple fundamental frequency estimation, onset detection, and rhythm/tempo recognition. In general, simultaneous estimation is preferable when several estimation problems have chicken-and-egg relationships. This paper proposes modeling the generative process of an entire music spectrogram by combining the sub-process by which a musically natural tempo curve is generated, the sub-process by which a set of note onset positions is generated based on a 2-dimensional tree structure representation of music, and the sub-process by which a music spectrogram is generated according to the tempo curve and the note onset positions. Most conventional approaches to music transcription perform note extraction prior to structure analysis, but accurate note extraction has been a difficult task. By contrast, thanks to the combined generative model, the present method performs note extraction and structure estimation simultaneously and thus the optimal solution is obtained within a unified framework. We show some of the transcription results obtained with the present method.",JPN,education,Developed economies,"[38.37151, -11.78478]","[-17.724127, -5.3578453]","[11.45715, -19.059416, 7.663184]","[0.81302845, -1.2885493, -12.730186]","[10.587887, 7.842812]","[6.0037093, 2.199378]","[11.783655, 13.28521, -0.30158198]","[8.161746, 6.935724, 10.964235]"
50,Aggelos Gkiokas;Vassilios Katsouros;George Carayannis,Reducing Tempo Octave Errors by Periodicity Vector Coding And SVM Learning.,2012,https://doi.org/10.5281/zenodo.1417439,Aggelos Gkiokas+Institute for Language and Speech Processing / R.C. Athena>GRC>facility|National Technical University of Athens>GRC>education;Vassilis Katsouros+Institute for Language and Speech Processing / R.C. Athena>GRC>facility;George Carayannis+National Technical University of Athens>GRC>education,"In this paper we present a method for learning tempo classes in order to reduce tempo octave errors. There are two main contributions of this paper in the rhythm analysis field. Firstly, a novel technique is proposed to code the rhythm periodicity functions of a music signal. Target tempi range is divided into overlapping “tempo bands” and the periodicity function is filtered by triangular masks aligned to those tempo bands, in order to calculate the respective saliencies, followed by the application of the DCT transform on band strengths. The second contribution is the adoption of Support Vector Machines to learn broad tempo classes from the coded periodicity vectors. Training instances are assigned a tempo class according to annotated tempo. The classes are assumed to correspond to “music speed”. At classification phase, each target excerpt is assigned a tempo class label by the SVM. Target periodicity vector is masked by the predicted tempo class range, and tempo is estimated by peak picking in the reduced periodicity vector. The proposed method was evaluated on the benchmark ISMIR 2004 Tempo Induction Evaluation Exchange Dataset for both tempo class and tempo value estimation tasks. Results indicate that the proposed approach provides an efficient framework to tackle the tempo estimation task.",GRC,facility,Developed economies,"[44.19933, -27.155527]","[-32.039913, -6.2791333]","[1.5253282, -31.749922, 4.322129]","[-9.40725, 9.8005705, -11.866634]","[11.434968, 4.3570323]","[5.0411344, 1.9292852]","[10.833342, 13.365112, -2.894265]","[7.392645, 6.9945755, 10.988584]"
48,Justin Salamon;Julián Urbano,Current Challenges in the Evaluation of Predominant Melody Extraction Algorithms.,2012,https://doi.org/10.5281/zenodo.1418041,Justin Salamon+Universitat Pompeu Fabra>ESP>education;Julián Urbano+University Carlos III of Madrid>ESP>education,"In this paper we analyze the reliability of the evaluation of Audio Melody Extraction algorithms. We focus on the procedures and collections currently used as part of the annual Music Information Retrieval Evaluation eXchange (MIREX), which has become the de-facto benchmark for evaluating and comparing melody extraction algorithms. We study several factors: the duration of the audio clips, time offsets in the ground truth annotations, and the size and musical content of the collection. The results show that the clips currently used are too short to predict performance on full songs, highlighting the paramount need to use complete musical pieces. Concerning the ground truth, we show how a minor error, specifically a time offset between the annotation and the audio, can have a dramatic effect on the results, emphasizing the importance of establishing a common protocol for ground truth annotation and system output. We also show that results based on the small ADC04, MIREX05 and INDIAN08 collections are unreliable, while the MIREX09 collections are larger than necessary. This evidences the need for new and larger collections containing realistic music material, for reliable and meaningful evaluation of Audio Melody Extraction.",ESP,education,Developed economies,"[5.8768067, -12.047865]","[-5.5186563, 9.091662]","[14.910923, 6.229454, -2.7220592]","[-1.4734282, 3.6610901, 9.740786]","[10.142582, 9.954899]","[9.18897, 2.600651]","[11.034378, 14.9439125, -0.40333307]","[10.808574, 6.2421536, 11.118305]"
47,George Sioros;Andre Holzapfel;Carlos Guedes,On Measuring Syncopation to Drive an Interactive Music System.,2012,https://doi.org/10.5281/zenodo.1416194,"George Sioros+Faculdade de Engenharia da Universidade do Porto>PRT>education;André Holzapfel+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Carlos Guedes+Faculdade de Engenharia da Universidade do Porto>PRT>education|INESC Porto>PRT>facility","In this paper we address the problem of measuring syncopation in order to mediate a musically meaningful interaction between a live music performance and an automatically generated rhythm. To this end we present a simple, yet effective interactive music system we developed. We shed some light on the complex nature of syncopation by looking into MIDI data from drum loops and whole songs. We conclude that segregation into individual rhythmic layers is necessary in order to measure the syncopation of a music ensemble. This implies that measuring syncopation on polyphonic audio signals is not yet tractable using the current state-of-the-art in audio analysis.",PRT,education,Developed economies,"[25.506779, -31.515718]","[-25.508383, 1.7881294]","[-2.571691, -21.439234, -14.04674]","[-1.3814492, 13.80746, -3.2976217]","[11.278447, 5.8196926]","[5.914827, 1.420071]","[12.098938, 12.528151, -2.217982]","[8.011294, 6.4428525, 11.48]"
46,Cyril Joder;Björn W. Schuller,Score-Informed Leading Voice Separation from Monaural Audio.,2012,https://doi.org/10.5281/zenodo.1415882,Cyril Joder+Technische Universität München>DEU>education|Institute for Human-Machine Communication>DEU>facility;Björn Schuller+Technische Universität München>DEU>education|Institute for Human-Machine Communication>DEU>facility,"Separating the leading voice from a musical recording seems to be natural to the human ear. Yet, it remains a difficult problem for automatic systems, in particular in the blind case, where no information is known about the signal. However, in the case where a musical score is available, one can take advantage of this additional information. In this paper, we present a novel application of this idea for leading voice separation exploiting a temporally-aligned MIDI Score. The model used is based on Nonnegative Matrix Factorization (NMF), whose solo part is represented by a source-filter model. We exploit the score information by constraining the source activations to conform to the aligned MIDI file. Experiments run on a database of real popular songs show that the use of these constraints can significantly improve the separation quality, in terms of both signal-based and perceptual evaluation metrics.",DEU,education,Developed economies,"[1.5049927, -44.75956]","[-45.208107, -27.465235]","[28.784626, 4.9277067, -3.4089773]","[-8.189695, -10.837277, -27.982466]","[8.788032, 10.478266]","[6.3902206, 5.216186]","[10.887624, 14.283261, 1.4770743]","[9.702015, 8.729122, 9.594266]"
45,Yannis Panagakis;Constantine Kotropoulos,Music Structure Analysis by Ridge Regression of Beat-synchronous Audio Features.,2012,https://doi.org/10.5281/zenodo.1417059,Yannis Panagakis+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown;Constantine Kotropoulos+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown,"A novel unsupervised method for automatic music structure analysis is proposed. Three types of audio features, namely the mel-frequency cepstral coefficients, the chroma features, and the auditory temporal modulations are employed in order to form beat-synchronous feature sequences modeling the audio signal. Assume that the feature vectors from each segment lie in a subspace and the song as a whole occupies the union of several subspaces. Then any feature vector can be represented as a linear combination of the feature vectors stemming from the same subspace. The coefficients of such a linear combination are found by solving an appropriate ridge regression problem, resulting to the ridge representation (RR) of the audio features. The RR yields an affinity matrix with nonzero within-subspace affinities and zero between-subspace ones, revealing the structure of the music recording. The segmentation of the feature sequence into music segments is found by applying the normalized cuts algorithm to the RR-based affinity matrix. In the same context, the combination of multiple audio features is investigated as well. The proposed method is referred to as ridge regression-based music structure analysis (RRMSA). State-of-the-art performance is reported for the RRMSA by conducting experiments on the manually annotated Beatles benchmark dataset.",GRC,education,Developed economies,"[-3.3432348, 0.51230764]","[1.7009994, -0.2593775]","[-1.0536884, -8.51084, -0.8623043]","[2.7979038, 1.5629416, -6.9146943]","[11.830093, 8.249635]","[7.661816, 3.3106756]","[12.746001, 13.783928, -0.41910848]","[10.517219, 8.1654215, 11.232819]"
44,Rafael Cabredo;Roberto S. Legaspi;Paul Salvador Inventado;Masayuki Numao,An Emotion Model for Music Using Brain Waves.,2012,https://doi.org/10.5281/zenodo.1416398,"Rafael Cabredo+Institute of Scientific and Industrial Research, Osaka University>JPN>education|Center for Empathic Human-Computer Interactions, De La Salle University>PHL>education;Roberto Legaspi+Institute of Scientific and Industrial Research, Osaka University>JPN>education|Center for Empathic Human-Computer Interactions, De La Salle University>PHL>education;Paul Salvador Inventado+Institute of Scientific and Industrial Research, Osaka University>JPN>education|Center for Empathic Human-Computer Interactions, De La Salle University>PHL>education;Masayuki Numao+Institute of Scientific and Industrial Research, Osaka University>JPN>education|Center for Empathic Human-Computer Interactions, De La Salle University>PHL>education",Every person reacts differently to music. The task then is to identify a specific set of music features that have a significant effect on emotion for an individual. Previous research have used self-reported emotions or tags to annotate short segments of music using discrete labels. Our approach uses an electroencephalograph to record the subject’s reaction to music. Emotion spectrum analysis method is used to analyse the electric potentials and provide continuous-valued annotations of four emotional states for different segments of the music. Music features are obtained by processing music information from the MIDI files which are separated into several segments using a windowing technique. The music features extracted are used in two separate supervised classification algorithms to build the emotion models. Classifiers have a minimum error rate of 5% predicting the emotion labels.,JPN,education,Developed economies,"[-60.715893, 2.9401622]","[52.780552, -7.627104]","[-26.572989, 26.191452, 0.66598606]","[9.829045, 23.970972, 7.456431]","[14.019786, 12.770037]","[13.140945, 3.9870956]","[16.208502, 14.474288, 1.8041738]","[14.23336, 5.0851164, 10.42362]"
43,Sally Jo Cunningham;David Bainbridge 0001;J. Stephen Downie,The Impact of MIREX on Scholarly Research (2005 - 2010).,2012,https://doi.org/10.5281/zenodo.1418217,Sally Jo Cunningham+University of Waikato>NZL>education;David Bainbridge+University of Waikato>NZL>education;J. Stephen Downie+University of Illinois>USA>education,"This paper explores the impact of the MIREX (Music Information Retrieval Evaluation eXchange) evaluation initiative on scholarly research. Impact is assessed through a bibliometric evaluation of both the MIREX extended abstracts and the papers citing the MIREX results, the trial framework and methodology, or MIREX datasets. Impact is examined through number of publications and citation analysis. We further explore the primary publication venues for MIREX results, the geographic distribution of both MIREX contributors and researchers citing MIREX results, and the spread of MIREX-based research beyond the MIREX contributor teams. This analysis indicates that research in this area is highly collaborative, has achieved an international dissemination, and has grown to have a significant profile in the research literature.",NZL,education,Developed economies,"[-7.5413313, 54.441917]","[27.940302, 34.910034]","[-32.760113, -3.9130065, -4.6901217]","[3.328904, 7.6335864, 19.158737]","[13.648829, 4.7890887]","[11.956534, 0.53340065]","[15.11164, 11.207582, -1.5527924]","[12.275503, 4.2659903, 12.222021]"
42,Jin Ha Lee;Nichole Maiman Waterman,Understanding User Requirements for Music Information Services.,2012,https://doi.org/10.5281/zenodo.1417625,Jin Ha Lee+University of Washington>USA>education;Nichole Maiman Waterman+University of Washington>USA>education,"User studies in the music information retrieval and music digital library fields have been gradually increasing in recent years, but large-scale studies that can help detect common user behaviors are still lacking. We have conducted a large-scale user survey in which we asked numerous questions related to users’ music needs, uses, seeking, and management behaviors. In this paper, we present our preliminary findings, specifically focusing on the responses to questions of users’ favorite music related websites/applications and the reasons why they like them. We provide a list of popular music services, as well as an analysis of how these services are used, and what qualities are valued. Our findings suggest several trends in the types of music services people like: an increase in the popularity of music streaming and mobile music consumption, the emergence of new functionality, such as music identification and cloud music services, an appreciation of music videos, serendipitous discovery of music, and customizability, as well as users’ changing expectations of particular types of music information.",USA,education,Developed economies,"[-33.899902, 26.745216]","[39.757492, 29.418512]","[-20.060915, 16.300402, -12.835554]","[8.626752, 12.284072, 16.722198]","[14.91141, 8.136782]","[12.8710575, 1.1530162]","[14.935052, 14.804782, -1.97919]","[13.332106, 4.3381414, 12.138048]"
41,Ching-Hua Chuan;Elaine Chew,Creating Ground Truth for Audio Key Finding: When the Title Key May Not Be the Key.,2012,https://doi.org/10.5281/zenodo.1414972,"Ching-Hua Chuan+University of North Florida>USA>education;Elaine Chew+Queen Mary, University of London>GBR>education","In this paper, we present an effective and efficient way to create an accurately labeled dataset to advance audio key finding research. The MIREX audio key finding contest has been held twice using classical compositions for which the key is designated in the title. The problem with this accepted practice is that the title key may not be the perceived key in the audio excerpt. To reduce manual annotation, which is costly, we use a confusion index generated by existing audio key finding algorithms to determine if an audio excerpt requires manual annotation. We collected 3224 excerpts and identified 727 excerpts requiring manual annotation. We evaluate the algorithms’ performance on these challenging cases using the title keys, and the re-labeled keys. The musicians who aurally identify the key also provide comments on the reasons for their choice. The relabeling process reveals the mismatch between title and perceived keys to be caused by tuning practices (in 471 of the 727 excerpts, 64.79%), and other factors (188 excerpts, 25.86%) including key modulation and intonation choices. The remaining 68 challenging cases provide useful information for algorithm design.",USA,education,Developed economies,"[32.79545, 14.815633]","[0.12056572, 10.075166]","[8.548133, -14.504287, 8.494255]","[-6.5002017, 4.7479634, 2.9410346]","[10.785174, 7.511225]","[8.320804, 1.9600251]","[12.141534, 13.130991, 0.30937216]","[10.284401, 6.4711585, 11.680392]"
40,Thierry Bertin-Mahieux;Daniel P. W. Ellis,Large-Scale Cover Song Recognition Using the 2D Fourier Transform Magnitude.,2012,https://doi.org/10.5281/zenodo.1414956,Thierry Bertin-Mahieux+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Large-scale cover song recognition involves calculating item-to-item similarities that can accommodate differences in timing and tempo, rendering simple Euclidean measures unsuitable. Expensive solutions such as dynamic time warping do not scale to million of instances, making them inappropriate for commercial-scale applications. In this work, we transform a beat-synchronous chroma matrix with a 2D Fourier transform and show that the resulting representation has properties that fit the cover song recognition task. We can also apply PCA to efficiently scale comparisons. We report the best results to date on the largest available dataset of around 18,000 cover songs amid one million tracks, giving a mean average precision of 3.0%.",USA,education,Developed economies,"[8.364038, 42.323715]","[26.268545, -11.87825]","[2.830668, 9.444884, -22.714273]","[15.683678, -4.1159983, -0.11617916]","[16.090816, 11.100012]","[10.22031, 2.6603432]","[12.868475, 17.301664, -0.35919666]","[11.74975, 6.796813, 11.830104]"
38,Yoko Anan;Kohei Hatano;Hideo Bannai;Masayuki Takeda;Ken Satoh,Polyphonic Music Classification on Symbolic Data Using Dissimilarity Functions.,2012,https://doi.org/10.5281/zenodo.1415672,Yoko Anan+Kyushu University>JPN>education;Kohei Hatano+Kyushu University>JPN>education;Hideo Bannai+Kyushu University>JPN>education;Masayuki Takeda+Kyushu University>JPN>education;Ken Satoh+National Institute of Informatics>JPN>facility,"This paper addresses the polyphonic music classification problem on symbolic data. A new method is proposed which converts music pieces into binary chroma vector sequences and then classifies them by applying the dissimilarity-based classification method TWIST proposed in our previous work. One advantage of using TWIST is that it works with any dissimilarity measure. Computational experiments show that the proposed method drastically outperforms SVM and k-NN, the state-of-the-art classification methods.",JPN,education,Developed economies,"[12.724685, 15.180746]","[14.50085, -6.7597346]","[-5.7254224, -5.005201, 12.387439]","[11.978626, 8.433476, -8.481292]","[11.826828, 7.3894267]","[9.361912, 3.3040612]","[13.3596115, 12.603333, -0.82028717]","[11.161362, 7.3959765, 10.858066]"
25,Rong Jin;Christopher Raphael,Interpreting Rhythm in Optical Music Recognition.,2012,https://doi.org/10.5281/zenodo.1415848,"Rong Jin+Indiana University, Bloomington>USA>education|Indiana University, Bloomington>USA>education;Christopher Raphael+Indiana University, Bloomington>USA>education|Indiana University, Bloomington>USA>education","We present a method for understanding the rhythmic content of a collection of identified symbols in optical music recognition, designed for polyphonic music. Our object of study is a measure of music symbols. Our model explains the symbols as a collection of voices, while the number of voices is variable throughout a measure. We introduce a dynamic programming framework that identifies the best-scoring interpretation subject to the constraint that each voice accounts for the musical time indicated by the known time signature. Our approach applies as well to the situation in which there are multiple possible hypotheses for each symbol, and thus combines interpretation with recognition in a top-down manner. We present experiments demonstrating a nearly 4-fold decrease in the number of false positive symbols with monophonic music, identify missing tuplets, and show preliminary results with polyphonic music.",USA,education,Developed economies,"[41.53752, 18.240717]","[-18.76502, 32.80114]","[-3.0225382, -21.184647, 2.749309]","[-11.067843, -14.66252, 4.196693]","[9.034004, 5.9619145]","[6.9122734, -0.16467638]","[11.389595, 13.794803, -2.0200477]","[8.294433, 4.5100074, 10.786209]"
41,Florian Kaiser;Geoffroy Peeters,A Simple Fusion Method of State And Sequence Segmentation for Music Structure Discovery.,2013,https://doi.org/10.5281/zenodo.1416046,Florian Kaiser+STMS IRCAM-CNRS-UPMC>FRA>facility;Geoffroy Peeters+STMS IRCAM-CNRS-UPMC>FRA>facility,"Methods for music structure segmentation are based on strong assumptions on the acoustical properties of structural segments. These assumptions relate to the novelty, homogeneity, repetition and/or regularity of the content. Each of these assumptions provide a different perspective on the music piece. These assumptions are however often considered separately in the methods. In this paper we propose a method for estimating the music structure segmentation based on the fusion of the novelty and repetition assumptions. This combination of different perspectives on the music pieces allows to generate more coherent acoustic segments and strongly improves the final music structure segmentation’s performance.",FRA,facility,Developed economies,"[-2.4775505, -2.0435097]","[-3.126586, 3.2020316]","[3.0804245, -4.91063, -0.49394488]","[-2.6018224, -0.020761369, -3.6602583]","[11.6836605, 8.221579]","[8.10444, 2.9317937]","[12.509031, 13.991027, -0.05760538]","[10.395103, 7.6327696, 11.490385]"
0,Sander Dieleman;Benjamin Schrauwen,Multiscale Approaches To Music Audio Feature Learning.,2013,https://doi.org/10.5281/zenodo.1416676,Sander Dieleman+Ghent University>BEL>education;Benjamin Schrauwen+Ghent University>BEL>education,"Content-based music information retrieval tasks are typically solved with a two-stage approach: features are extracted from music audio signals, and are then used as input to a regressor or classifier. These features can be engineered or learned from data. Although the former approach was dominant in the past, feature learning has started to receive more attention from the MIR community in recent years. Recent results in feature learning indicate that simple algorithms such as K-means can be very effective, sometimes surpassing more complicated approaches based on restricted Boltzmann machines, autoencoders or sparse coding. Furthermore, there has been increased interest in multiscale representations of music audio recently. Such representations are more versatile because music audio exhibits structure on multiple timescales, which are relevant for different MIR tasks to varying degrees. We develop and compare three approaches to multiscale audio feature learning using the spherical K-means algorithm. We evaluate them in an automatic tagging task and a similarity metric learning task on the Magnatagatune dataset.",BEL,education,Developed economies,"[-17.361256, -7.8124413]","[28.968273, -7.444247]","[-6.335062, 9.992993, 10.252839]","[22.070135, 0.8623487, -0.01834062]","[12.156303, 9.231597]","[10.029192, 3.9706123]","[13.5306225, 13.600549, 0.62398785]","[11.5471525, 6.644, 10.318383]"
66,Bin Wu;Simon Wun;Chung Lee;Andrew Horner,Spectral Correlates in Emotion Labeling of Sustained Musical Instrument Tones.,2013,https://doi.org/10.5281/zenodo.1417443,Bin Wu+Hong Kong University of Science and Technology>HKG>education|Singapore University of Technology and Design>SGP>education;Simon Wun+Hong Kong University of Science and Technology>HKG>education;Chung Lee+Singapore University of Technology and Design>SGP>education;Andrew Horner+Hong Kong University of Science and Technology>HKG>education,"Music is one of the strongest inducers of emotion in humans. Melody, rhythm, and harmony provide the primary triggers, but what about timbre? Do the musical instruments have underlying emotional characters? For example, is the well-known melancholy sound of the English horn due to its timbre or to how composers use it? Though music emotion recognition has received a lot of attention, researchers have only recently begun considering the relationship between emotion and timbre. To this end, we devised a listening test to compare representative tones from eight different wind and string instruments. The goal was to determine if some tones were consistently perceived as being happier or sadder in pairwise comparisons. A total of eight emotions were tested in the study. The results showed strong underlying emotional characters for each instrument. The emotions Happy, Joyful, Heroic, and Comic were strongly correlated with one another. The violin, trumpet, and clarinet best represented these emotions. Sad and Depressed were also strongly correlated. These two emotions were best represented by the horn and flute. Scary was the emotional outlier of the group, while the oboe had the most emotionally neutral timbre. Also, we found that emotional judgment correlates significantly with average spectral centroid for the more distinctive emotions, including Happy, Joyful, Sad, Depressed, and Shy. These results can provide insights in orchestration, and lay the groundwork for future studies on emotion and timbre.",HKG,education,Developing economies,"[-61.546085, 4.4451776]","[51.075752, -14.737629]","[-23.951765, 27.02426, -0.16212612]","[3.6380541, 21.252182, 6.263112]","[13.981384, 12.7448015]","[12.634249, 4.2842565]","[16.186945, 14.470738, 1.8185879]","[13.861308, 4.7510414, 10.2843275]"
67,Mathieu Barthet;David Marston;Chris Baume;György Fazekas;Mark B. Sandler,Design and Evaluation of Semantic Mood Models for Music Recommendation using Editorial Tags.,2013,https://doi.org/10.5281/zenodo.1418005,"Mathieu Barthet+Centre for Digital Music, Queen Mary University of London>GBR>education|BBC R&D London>GBR>company;David Marston+BBC R&D London>GBR>company;Chris Baume+BBC R&D London>GBR>company;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","In this paper we present and evaluate two semantic music mood models relying on metadata extracted from over 180,000 production music tracks sourced from I Like Music (ILM)’s collection. We performed non-metric multidimensional scaling (MDS) analyses of mood stem dissimilarity matrices (1 to 13 dimensions) and devised five different mood tag summarisation methods to map tracks in the dimensional mood spaces. We then conducted a listening test to assess the ability of the proposed models to match tracks by mood in a recommendation task. The models were compared against a classic audio content-based similarity model relying on Mel Frequency Cepstral Coefficients (MFCCs). The best performance (60% of correct match, on average) was yielded by coupling the five-dimensional MDS model with the term-frequency weighted tag centroid method to map tracks in the mood space.",GBR,education,Developed economies,"[-50.873432, 3.1331916]","[50.67404, -2.2811427]","[-19.382076, 19.638304, 4.16104]","[14.114778, 17.788054, 8.002186]","[13.657792, 12.35735]","[12.97391, 3.6501799]","[16.032747, 14.778891, 1.2567629]","[14.18024, 5.242033, 10.776286]"
68,Yi-Hsuan Yang,Low-Rank Representation of Both Singing Voice and Music Accompaniment Via Learned Dictionaries.,2013,https://doi.org/10.5281/zenodo.1418089,"Yi-Hsuan Yang+Research Center for IT Innovation, Academia Sinica>TWN>facility","Recent research work has shown that the magnitude spectrogram of a song can be considered as a superposition of a low-rank component and a sparse component, which appear to correspond to the instrumental part and the vocal part of the song, respectively. Based on this observation, one can separate singing voice from the background music. However, the quality of such separation might be limited, because the vocal part of a song can sometimes be low-rank as well. Therefore, we propose to learn the subspace structures of vocal and instrumental sounds from a collection of clean signals first, and then compute the low-rank representations of both the vocal and instrumental parts of a song based on the learned subspaces. Specifically, we use online dictionary learning to learn the subspaces, and propose a new algorithm called multiple low-rank representation (MLRR) to decompose a magnitude spectrogram into two low-rank matrices. Our approach is flexible in that the subspaces of singing voice and music accompaniment are both learned from data. Evaluation on the MIR-1K dataset shows that the approach improves the source-to-distortion ratio (SDR) and the source-to-interference ratio (SIR), but not the source-to-artifact ratio (SAR).",TWN,facility,Developing economies,"[-6.362909, -40.392296]","[-49.166893, -26.893955]","[19.783205, 15.934911, -6.7917314]","[-7.672797, -14.919606, -29.942333]","[10.164569, 11.194903]","[6.4041543, 5.0070868]","[11.430601, 15.32579, 0.74759424]","[9.707094, 8.719024, 9.90407]"
69,Sebastian Stober;Thomas Low;Tatiana Gossen;Andreas Nürnberger,Incremental Visualization of Growing Music Collections.,2013,https://doi.org/10.5281/zenodo.1415110,Sebastian Stober+University of Magdeburg>DEU>education;Thomas Low+University of Magdeburg>DEU>education;Tatiana Gossen+University of Magdeburg>DEU>education;Andreas Nurnberger+University of Magdeburg>DEU>education,"Map-based visualizations – sometimes also called projections – are a popular means for exploring music collections. But how useful are they if the collection is not static but grows over time? Ideally, a map that a user is already familiar with should be altered as little as possible and only as much as necessary to reflect the changes of the underlying collection. This paper demonstrates to what extent existing approaches are able to incrementally integrate new songs into existing maps and discusses their technical limitations. To this end, Growing Self-Organizing Maps, (Landmark) Multidimensional Scaling, Stochastic Neighbor Embedding, and the Neighbor Retrieval Visualizer are considered. The different algorithms are experimentally compared based on objective quality measurements as well as in a user study with an interactive user interface. In the experiments, the well-known Beatles corpus comprising the 180 songs from the twelve official albums is used – adding one album at a time to the collection.",DEU,education,Developed economies,"[-16.35381, 30.715927]","[27.02909, 18.444576]","[-12.397361, 7.508045, -23.115223]","[14.072167, -6.3959327, 21.994696]","[14.090438, 7.067641]","[11.139324, 1.8130671]","[14.129329, 13.890382, -2.5154316]","[12.357517, 5.747927, 13.213929]"
70,Laurent Pugin;Tim Crawford,Evaluating OMR on the Early Music Online Collection.,2013,https://doi.org/10.5281/zenodo.1415946,"Laurent Pugin+Swiss RISM Office>CHE>facility;Tim Crawford+Goldsmiths College, University of London>GBR>education","The Early Music Online (EMO) collection consists of about 300 printed music books of the sixteenth century held at the British Library. They were recently digitized from microfilms and made available online. In total, about 35,000 pages were digitized. This paper presents an optical music recognition (OMR) evaluation on the EMO collection. Firstly, the content of the collection is reviewed, looking at the type of music notation and the type of printing technique. Secondly, for the books for which it is possible (260 books), an OMR evaluation performed using the Aruspix OMR software application is presented. For each book, one randomly selected page of music was processed and the recognition rate was computed using a corrected transcription of the page. This evaluation shows very promising results for large-scale OMR on the EMO or similar collections. The paper also highlights critical points that should be taken into account in such an enterprise.",CHE,facility,Developed economies,"[-21.928911, 20.287483]","[-22.810236, 39.14685]","[-17.52316, 1.7216295, -11.100212]","[-12.809033, -23.669464, 1.5090872]","[14.202654, 7.9350643]","[6.7456074, -0.70390296]","[14.343709, 14.501554, -1.9511871]","[7.7784634, 3.99965, 10.530633]"
71,Boyang Gao;Emmanuel Dellandréa;Liming Chen 0002,Sparse Music Decomposition onto a MIDI Dictionary Driven by Statistical Music Knowledge.,2013,https://doi.org/10.5281/zenodo.1416994,Boyang Gao+Université de Lyon>FRA>education|CNRS>FRA>facility|Ecole Centrale de Lyon>FRA>education|LIRIS>FRA>facility;Emmanuel Dellandréa+Université de Lyon>FRA>education|CNRS>FRA>facility|Ecole Centrale de Lyon>FRA>education|LIRIS>FRA>facility;Liming Chen+Université de Lyon>FRA>education|CNRS>FRA>facility|Ecole Centrale de Lyon>FRA>education|LIRIS>FRA>facility,"The general goal of music signal decomposition is to represent the music structure into a note level to provide valuable semantic features for further music analysis tasks. In this paper, we propose a new method to sparsely decompose the music signal onto a MIDI dictionary made of musical notes. Statistical music knowledge is further integrated into the whole sparse decomposition process. The proposed method is divided into a frame level sparse decomposition stage and a whole music level optimal note path searching. In the first stage note co-occurrence probabilities are embedded to generate a sparse multiple candidate graph while in the second stage note transition probabilities are incorporated into the optimal path searching. Experiments on real-world polyphonic music show that embedding music knowledge within the sparse decomposition achieves notable improvement in terms of note recognition precision and recall.",FRA,education,Developed economies,"[3.850928, -26.275673]","[4.12129, -2.286456]","[12.377022, 14.213701, 17.365877]","[18.32461, 11.026223, -13.027535]","[11.274325, 7.728081]","[6.914787, 4.1962943]","[12.731852, 13.068027, -0.13852142]","[10.5480995, 8.474252, 10.69043]"
72,Véronique Sébastien;Didier Sébastien;Noël Conruyt,Annotating Works for Music Education: Propositions for a Musical Forms and Structures Ontology and a Musical Performance Ontology.,2013,https://doi.org/10.5281/zenodo.1418199,Véronique Sébastien+University of Reunion Island>FRA>education;Didier Sébastien+University of Reunion Island>FRA>education;Noël Conruyt+University of Reunion Island>FRA>education,"Web applications and mobile tablets are changing the way musicians practice their instrument. Now, they can access instantaneously thousands of musical scores online and play them while watching their tablet, put on their music stand. However musicians may have difficulties in getting appropriate tips and advice to play the chosen piece correctly. This is why we conceived a collaborative platform to annotate digital scores on tablets in previous work. However, we noticed that the current Music Ontology (MO) do not allow to tag these annotations appropriately. Thus, we present in this paper a proposition for a Musical Forms and Structures Ontology (MFSO) and a Musical Performance Ontology (MPO) based on music practice. A construction methodology and a model are first detailed. Then, a practical use case is presented. Lastly, inherent theoretical and practical difficulties encountered during the ontology framework’s conception are discussed.",FRA,education,Developed economies,"[-25.675045, 37.88533]","[15.991059, 40.019634]","[-20.82551, -0.48291406, -2.0590096]","[-1.2117347, -3.9600627, 25.81799]","[14.190516, 8.856185]","[10.83736, -0.17902012]","[14.997871, 13.841579, -1.3021091]","[12.099868, 5.0582733, 11.591347]"
73,Satoru Fukayama;Kazuyoshi Yoshii;Masataka Goto,Chord-Sequence-Factory: A Chord Arrangement System Modifying Factorized Chord Sequence Probabilities.,2013,https://doi.org/10.5281/zenodo.1417701,Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a system named ChordSequenceFactory for automatically generating chord arrangements. A key element of musical composition is the arrangement of chord sequences because good chord arrangements have the potential to enrich the listening experience and create a pleasant feeling of surprise by borrowing elements from different musical styles in unexpected ways. While chord sequences have conventionally been modeled by using N-grams, generative grammars, or music theoretic rules, our system decomposes a matrix consisting of chord transition probabilities by using nonnegative matrix factorization. This enables us to not only generate chord sequences from scratch but also transfer characteristic transition patterns from one chord sequence to another. ChordSequenceFactory can assist users to edit chord sequences by modifying factorized chord transition probabilities and then automatically re-arranging them. By leveraging knowledge from chord sequences of over 2000 songs, our system can help users generate a wide range of musically interesting and entertaining chord arrangements.",JPN,facility,Developed economies,"[51.607777, -2.369772]","[-25.04524, 24.841402]","[22.31363, -15.700982, 14.411586]","[-21.947544, 0.47760347, 1.377514]","[7.0860734, 8.789906]","[6.9909573, 3.4847476]","[12.065948, 10.516625, 1.8652427]","[9.514419, 8.242685, 12.122843]"
74,I-Ting Liu;Yin-Tzu Lin;Ja-Ling Wu,Music Cut and Paste: A Personalized Musical Medley Generating System.,2013,https://doi.org/10.5281/zenodo.1415764,I-Ting Liu+National Taiwan University>TWN>education|Graduate Institute of Networking and Multimedia>TWN>education;Yin-Tzu Lin+National Taiwan University>TWN>education|Graduate Institute of Networking and Multimedia>TWN>education;Ja-Ling Wu+National Taiwan University>TWN>education|Graduate Institute of Networking and Multimedia>TWN>education,"A musical medley is a piece of music that is composed of parts of existing pieces. Manually creating medley is time consuming because it is not easy to find out proper clips to put in succession and seamlessly connect them. In this work, we propose a framework for creating personalized music medleys from users’ music collection. Unlike existing similar works in which only low-level features are used to select candidate clips and locate possible transition points among clips, we take song structures and song phrasing into account during medley creation. Inspired by the musical dice game, we treat the medley generation process as an audio version of musical dice game. That is, once the analysis on the songs of user collection has been done, the system is able to generate various medleys with different probabilities. This flexibility brings us the ability to create medleys according to the user-specified conditions, such as the medley structure or some must-use clips. The preliminary subjective evaluations showed that the proposed system is effective in selecting connectable clips that preserved chord progression structure. Besides, connecting the clips at phrase boundaries acquired more user preference than previous works did.",TWN,education,Developing economies,"[-10.652066, 7.5569615]","[34.17394, 24.570507]","[9.800372, 14.781502, 24.051243]","[2.6118195, 12.645961, 10.341254]","[11.565887, 7.7912693]","[10.869452, 1.8265735]","[13.374509, 12.449312, -0.42047378]","[10.245275, 5.469536, 10.264861]"
65,Andryw Marques Ramos;Nazareno Andrade;Leandro Balby Marinho,Exploring the Relation Between Novelty Aspects and Preferences in Music Listening.,2013,https://doi.org/10.5281/zenodo.1416660,Andryw Marques+Universidade Federal de Campina Grande>BRA>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education;Leandro Balby+Universidade Federal de Campina Grande>BRA>education,"The discovery of new music, e.g. song tracks and artists, is a central aspect of music consumption. In order to assist users in this task, several mechanisms have been proposed to incorporate novelty awareness into music recommender systems. In this paper, we complement these efforts by investigating how the music preferences of users are affected by two different aspects of novel artists, namely familiarity and mainstreamness. We collected historical data from Last.fm users, a popular online music discovery service, to investigate how these aspects of novel artists relate to the preferences of music listeners for novel artists. The results of this analysis suggests that the users tend to cluster according to their novelty related preferences. We then conducted a comprehensive study on these groups, from where we derive implications and useful insights for developers of music retrieval services.",BRA,education,Developing economies,"[-40.730057, 22.342691]","[39.316658, 19.26846]","[-15.211865, 19.510706, -5.8705025]","[13.0718155, 9.352879, 19.116434]","[15.28959, 9.030865]","[12.752338, 1.5849737]","[15.197302, 15.315431, -1.3327203]","[13.440057, 4.718679, 12.363628]"
75,Jordan B. L. Smith;Elaine Chew,A Meta-Analysis of the MIREX Structural Segmentation Task.,2013,https://doi.org/10.5281/zenodo.1415816,"Jordan B. L. Smith+Queen Mary, University of London>GBR>education;Elaine Chew+Queen Mary, University of London>GBR>education","The Music Information Retrieval Evaluation eXchange (MIREX) serves an essential function in the MIR community, but researchers have noted that the anonymity of its datasets, while useful, has made it difficult to interpret the successes and failures of the algorithms. We use the results of the 2012 MIREX Structural Segmentation task, which was accompanied by anonymous ground truth, to conduct a meta-evaluation of the algorithms. We hope this demonstrates the benefits, to both the participants and evaluators of MIREX, of releasing more data in evaluation tasks.  Our aim is to learn more about the performance of the algorithms by studying how their success relates to properties of the annotations and recordings. We find that some evaluation metrics are redundant, and that several algorithms do not adequately model the true number of segments in typical annotations. We also use publicly available ground truth to identify many of the recordings in the MIREX test sets, allowing us to identify specific pieces on which algorithms generally performed poorly and to discover where the most improvement is needed.",GBR,education,Developed economies,"[-8.718634, 55.496246]","[-3.770805, 7.6576304]","[-36.287457, -4.459593, -3.6476314]","[-1.0239116, 4.406056, 11.648909]","[13.61535, 4.7163157]","[9.089648, 2.4979494]","[15.020485, 11.125377, -1.4817232]","[10.871019, 6.3012705, 11.240929]"
77,Anders Elowsson;Anders Friberg;Guy Madison;Johan Paulin,Modelling the Speed of Music using Features from Harmonic/Percussive Separated Audio.,2013,https://doi.org/10.5281/zenodo.1414928,Anders Elowsson+KTH Royal Institute of Technology>SWE>education;Anders Friberg+KTH Royal Institute of Technology>SWE>education;Guy Madison+Umeå University>Unknown>education;Johan Paulin+Umeå University>Unknown>education,"One of the major parameters in music is the overall speed of a musical performance. In this study, a computational model of speed in music audio has been developed using a custom set of rhythmic features. Speed is often associated with tempo, but as shown in this study, factors such as note density (onsets per second) and spectral flux are important as well. The original audio was first separated into a harmonic part and a percussive part and the features were extracted separately from the different layers. In previous studies, listeners had rated the speed of 136 songs, and the ratings were used in a regression to evaluate the validity of the model as well as to find appropriate features. The final models, consisting of 5 or 8 features, were able to explain about 90% of the variation in the training set, with little or no degradation for the test set.",SWE,education,Developed economies,"[33.573654, -18.495596]","[-38.7981, 0.5732686]","[24.104195, -8.464655, -9.014364]","[9.303198, 15.832517, -13.472337]","[9.101269, 9.387363]","[9.010528, 3.7179284]","[11.368567, 13.533476, 0.658843]","[10.674725, 7.418633, 10.48736]"
78,Padi Sarala;Hema A. Murthy,Inter and Intra Item Segmentation of Continuous Audio Recordings of Carnatic Music for Archival.,2013,https://doi.org/10.5281/zenodo.1414892,"Padi Sarala+Indian Institute of Technology, Madras>IND>education;Hema A. Murthy+Indian Institute of Technology, Madras>IND>education","The purpose of this paper is to segment carnatic music recordings into individual items for archival purposes using applauses. A concert in carnatic music is replete with applauses. These applauses may be inter-item or intra-item applauses. A property of an item in carnatic music, is that within every item, a small portion of the audio corresponds to the rendering of a composition which is rendered by the entire ensemble of lead performer and accompanying instruments. A concert is divided into segments using applauses and the location of the ensemble in every item is first obtained using Cent Filterbank Cepstral Coefficients (CFCC) combined with Gaussian Mixture Models (GMMs). Since constituent parts of an item are rendered in a single raga, raga information is used to merge adjacent segments belonging to the same item. Inter-item applauses are used to locate the end of an item in a concert. The results are evaluated for fifty live recordings with 990 applauses in total. The classification accuracy for inter and intra item applauses is 93%. Given a song list and the audio, the song list is mapped to the segmented audio of items, which are then stored in the database.",IND,education,Developing economies,"[-5.7567735, 2.9892683]","[-5.4823365, 3.1579745]","[1.2587383, -5.1580358, -7.7570786]","[0.5584285, -4.5847216, -0.87681377]","[11.919811, 8.068202]","[8.198822, 3.011213]","[12.479137, 14.219418, -0.377572]","[10.406641, 7.6703706, 11.187024]"
79,Dmitry Bogdanov;Nicolas Wack;Emilia Gómez;Sankalp Gulati;Perfecto Herrera;Oscar Mayor;Gerard Roma;Justin Salamon;José R. Zapata;Xavier Serra,Essentia: An Audio Analysis Library for Music Information Retrieval.,2013,https://doi.org/10.5281/zenodo.1415016,"Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Nicolas Wack+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sankalp Gulati+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Oscar Mayor+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Gerard Roma+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Justin Salamon+Music Technology Group, Universitat Pompeu Fabra>ESP>education;José Zapata+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.",ESP,education,Developed economies,"[-18.829672, 15.78944]","[6.823487, 30.003466]","[-14.105058, -0.9952129, -13.916217]","[-5.7961226, -6.9571123, 13.864813]","[13.524279, 7.687625]","[10.358831, 1.2975076]","[14.02159, 13.996717, -1.6998327]","[11.160821, 5.5717688, 11.585071]"
80,Vignesh Ishwar;Shrey Dutta;Ashwin Bellur;Hema A. Murthy,Motif Spotting in an Alapana in Carnatic Music.,2013,https://doi.org/10.5281/zenodo.1416332,Vignesh Ishwar+IIT Madras>IND>education;Shrey Dutta+IIT Madras>IND>education;Ashwin Bellur+IIT Madras>IND>education;Hema A Murthy+IIT Madras>IND>education,"This work addresses the problem of melodic motif spotting, given a query, in Carnatic music. Melody in Carnatic music is based on the concept of raga. Melodic motifs are signature phrases which give a raga its identity. They are also the fundamental units that enable extempore elaborations of a raga. In this paper, an attempt is made to spot typical melodic motifs of a raga queried in a musical piece using a two pass dynamic programming approach, with pitch as the basic feature. In the first pass, the rough longest common subsequence (RLCS) matching is performed between the saddle points of the pitch contours of the reference motif and the musical piece. These saddle points corresponding to quasi-stationary points of the motifs, are relevant entities of the raga. Multiple sequences are identified in this step, not all of which correspond to the motif that is queried. To reduce the false alarms, in the second pass a fine search using RLCS is performed between the continuous pitch contours of the reference motif and the subsequences obtained in the first pass. The proposed methodology is validated by testing on Alapanas of 20 different musicians.",IND,education,Developing economies,"[5.1461124, 0.3343798]","[8.728646, 2.7337377]","[7.0428195, 3.822412, -16.0821]","[11.752161, -12.490385, -1.0388469]","[11.239739, 10.335977]","[7.9668183, 1.116325]","[11.642664, 15.092075, -1.4770846]","[9.595974, 6.785529, 12.743075]"
81,Thor Kell;George Tzanetakis,Empirical Analysis of Track Selection and Ordering in Electronic Dance Music using Audio Feature Extraction.,2013,https://doi.org/10.5281/zenodo.1415654,Thor Kell+McGill University>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"Disc jockeys are in some ways the ultimate experts at selecting and playing recorded music for an audience, especially in the context of dance music. In this work, we empirically investigate factors affecting track selection and ordering using DJ-created mixes of electronic dance music. We use automatic content-based analysis and discuss the implications of our findings to playlist generation and ordering. Timbre appears to be an important factor when selecting tracks and ordering tracks, and track order itself matters, as shown by statistically significant differences in the transitions between the original order and a shuffled version. We also apply this analysis to ordering heuristics and suggest that the standard playlist generation model of returning tracks in order of decreasing similarity to the initial track may not be optimal, at least in the context of track ordering for electronic dance music.",CAN,education,Developed economies,"[-22.045397, -22.377438]","[36.250534, 22.694052]","[-4.524917, -22.432724, -6.145912]","[14.827374, 5.509436, 24.506006]","[11.577392, 5.1313868]","[12.07281, 1.5770938]","[11.485741, 13.729962, -2.335007]","[13.103312, 5.107818, 13.222207]"
82,Alessandro L. Koerich,Improving the Reliability of Music Genre Classification using Rejection and Verification.,2013,https://doi.org/10.5281/zenodo.1416570,Alessandro L. Koerich+Pontifical Catholic University of Paraná (PUCPR)>BRA>education|Federal University of Paraná (UFPR)>BRA>education,"This paper presents a novel approach for post-processing the music genre hypotheses generated by a baseline classifier. Given a music piece, the baseline classifier produces a ranked list of the N best hypotheses consisting of music genre labels and recognition scores. A rejection strategy is then applied to either reject or accept the output of the baseline classifier. Some of the rejected instances are handled by a verification stage which extracts visual features from the spectrogram of the music signal and employs binary support vector machine classifiers to disambiguate between confusing classes. The rejection and verification approach has improved the reliability in classifying music genres. Our approach is described in detail and the experimental results on a benchmark dataset are presented.",BRA,education,Developing economies,"[-31.414278, -14.322531]","[19.034182, -8.778514]","[-16.767633, 7.1805153, 18.403875]","[12.735668, 10.965286, -5.805606]","[13.004554, 10.872267]","[9.8339, 3.4550698]","[14.0364, 14.313351, 1.3595955]","[11.681881, 7.0818486, 10.803128]"
83,Gregory Burlet;Ichiro Fujinaga,Robotaba Guitar Tablature Transcription Framework.,2013,https://doi.org/10.5281/zenodo.1415042,Gregory Burlet+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper presents Robotaba, a web-based guitar tablature transcription framework. The framework facilitates the creation of web applications in which polyphonic transcription and guitar tablature arrangement algorithms can be embedded. Such a web application is implemented, and consists of an existing polyphonic transcription algorithm and a new guitar tablature arrangement algorithm. The result is a unified system that is capable of transcribing guitar tablature from a digital audio recording and displaying the resulting tablature in the web browser. Additionally, two ground-truth datasets for polyphonic transcription and guitar tablature arrangement are compiled from manual transcriptions gathered from the tablature website ultimate-guitar.com. The implemented transcription web application is evaluated on the compiled ground-truth datasets using several metrics.",CAN,education,Developed economies,"[44.80662, -8.222287]","[-44.651154, -7.0660267]","[21.931738, -10.886519, 8.513332]","[-19.822538, -9.582169, -2.794573]","[7.756214, 8.333443]","[7.1603494, 4.0691724]","[11.6507015, 11.173533, 1.312312]","[8.996804, 6.8661885, 10.142128]"
84,Richard Polfreman,Comparing Onset Detection & Perceptual Attack Time.,2013,https://doi.org/10.5281/zenodo.1415232,Dr Richard Polfreman+University of Southampton>GBR>education,"Accurate performance timing is associated with the perceptual attack time (PAT) of notes, rather than their physical or perceptual onsets (PhOT, POT). Since manual annotation of PAT for analysis is both time-consuming and impractical for real-time applications, automatic transcription is desirable. However, computational methods for onset detection in audio signals are conventionally measured against PhOT or POT data. This paper describes a comparison between PAT and onset detection data to assess whether in some circumstances they are similar enough to be equivalent, or whether additional models for PAT-PhOT difference are always necessary. Eight published onset algorithms, and one commercial system, were tested with five onset types in short monophonic sequences. Ground truth was established by multiple human transcription of the audio for PATs using rhythm adjustment with synchronous presentation, and parameters for each detection algorithm manually adjusted to produce the maximum agreement with the ground truth. Results indicate that for percussive attacks, a number of algorithms produce data close to or within the limits of human agreement and therefore may be substituted for PATs, while for non-percussive sounds corrective measures are necessary to match detector outputs to human estimates.",GBR,education,Developed economies,"[30.613571, -25.950994]","[-20.98252, -7.4698443]","[10.330276, -21.684734, -7.524066]","[-1.0420388, 3.333347, -11.132773]","[10.277029, 5.0388045]","[5.783655, 2.5112875]","[10.371806, 13.237723, -1.5499201]","[8.138262, 7.4786563, 10.836151]"
85,Jin Ha Lee;Kahyun Choi;Xiao Hu 0001;J. Stephen Downie,K-Pop Genres: A Cross-Cultural Exploration.,2013,https://doi.org/10.5281/zenodo.1416222,Jin Ha Lee+University of Washington>USA>education;Kahyun Choi+University of Illinois>USA>education;Xiao Hu+University of Hong Kong>HKG>education;J. Stephen Downie+University of Illinois>USA>education,"Current music genre research tends to focus heavily on classical and popular music from Western cultures. Few studies discuss the particular challenges and issues related to non-Western music. The objective of this study is to improve our understanding of how genres are used and perceived in different cultures. In particular, this study attempts to fill gaps in our understanding by examining K-pop music genres used in Korea and comparing them with genres used in North America. We provide background information on K-pop genres by analyzing 602 genre-related labels collected from eight major music distribution websites in Korea. In addition, we report upon a user study in which American and Korean users annotated genre information for 1894 K-pop songs in order to understand how their perceptions might differ or agree. The results show higher consistency among Korean users than American users demonstrated by the difference in Fleiss’ Kappa values and proportion of agreed genre labels. Asymmetric disagreements between Americans and Koreans on specific genres reveal some interesting differences in the perception of genres. Our findings provide some insights into challenges developers may face in creating global music services.",USA,education,Developed economies,"[-35.916363, -13.6710825]","[46.857067, 15.61247]","[-19.789658, -0.26486248, 14.751838]","[16.168125, 16.161472, 14.397885]","[13.301677, 10.679888]","[11.16141, 3.1313007]","[14.177671, 14.246376, 1.1475353]","[13.088534, 5.1896534, 11.724021]"
76,Tian Cheng 0001;Simon Dixon;Matthias Mauch,A Deterministic Annealing EM Algorithm for Automatic Music Transcription.,2013,https://doi.org/10.5281/zenodo.1417014,"Tian Cheng+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education;Matthias Mauch+Centre for Digital Music, Queen Mary University of London>GBR>education","In the past decade, non-negative matrix factorisation (NMF) and probabilistic latent component analysis (PLCA) have been used widely in automatic music transcription. Despite their successes, these methods only guarantee that the decomposition converges to a local minimum in the cost function. In order to find better local minima, we propose to extend an existing PLCA-based transcription method with the deterministic annealing EM (DAEM) algorithm. The PLCA update rules are modified by introducing a “temperature” parameter. At higher temperatures, general areas of the search space containing good solutions are found. As the temperature is gradually decreased, distinctions in the data are sharpened, resulting in a more fine-grained optimisation at each successive temperature. This process reduces the dependence on the initialisation, which is otherwise a limitation of NMF and PLCA approaches. The method was tested on two standard multi-instrument transcription data sets (MIREX and Bach10). Experimental results show that the proposed method significantly outperforms a state-of-the-art reference method, according to both frame-based and note-based metrics. An additional analysis of instrument assignment results shows that instrument spectra are typically modelled as mixtures of templates from several instruments.",GBR,education,Developed economies,"[27.501125, -11.215521]","[-50.938988, -24.27268]","[14.510221, -5.972363, 7.391502]","[-6.838875, -17.788725, -25.597214]","[9.8117075, 7.6744967]","[6.041269, 4.7238336]","[11.926532, 12.521794, -0.029859137]","[9.336173, 8.723769, 9.901692]"
86,Felipe Mendonça Scheeren;Marcelo Soares Pimenta;Damián Keller;Victor Lazzarini,Coupling Social Network Services and Support for Online Communities in Codes Environment.,2013,https://doi.org/10.5281/zenodo.1415604,"Felipe M. Scheeren+Institute of Informatics UFRGS>BRA>education;Marcelo S. Pimenta+Institute of Informatics UFRGS>BRA>education;Damián Keller+Amazon Center for Music Research UFAC>BRA>education;Victor Lazzarini+National University of Ireland, Maynooth>IRL>education","In recent years, our research group has been investigating the use of computing technology to support novice-oriented computer-based musical activities. CODES (Cooperative Music Prototyping Design) is a Web-based environment designed to allow novice users to create musical prototypes through combining basic sound patterns. This paper shows how CODES has been changed to provide support to some concepts originally from of Social Networks and also to Online Communities having Music Creation as intrinsic motivation.",BRA,education,Developing economies,"[-35.192173, 30.245344]","[38.94047, 36.954327]","[-26.442024, 14.221547, -13.065436]","[3.2775478, 15.409781, 22.441454]","[14.848689, 9.0709305]","[12.283968, 0.6415542]","[15.294283, 14.572838, -1.1395555]","[12.191364, 4.2685823, 11.643565]"
64,Joshua L. Moore;Shuo Chen;Douglas Turnbull;Thorsten Joachims,Taste Over Time: The Temporal Dynamics of User Preferences.,2013,https://doi.org/10.5281/zenodo.1416148,Joshua L. Moore+Cornell University>USA>education;Shuo Chen+Cornell University>USA>education;Thorsten Joachims+Cornell University>USA>education;Douglas Turnbull+Ithaca College>USA>education,"We develop temporal embedding models for exploring how listening preferences of a population develop over time. In particular, we propose time-dynamic probabilistic embedding models that incorporate users and songs in a joint Euclidian space in which they gradually change position over time. Using large-scale Scrobbler data from Last.fm spanning a period of 8 years, our models generate trajectories of how user tastes changed over time, how artists developed, and how songs move in the embedded space. This ability to visualize and quantify listening preferences of a large population of people over a multi-year time period provides exciting opportunities for data-driven exploration of musicological trends and patterns.",USA,education,Developed economies,"[-43.706062, 20.817297]","[43.478745, 18.738756]","[-10.990821, 19.517727, -6.1228943]","[19.593204, 10.250891, 18.036196]","[15.368847, 9.096807]","[12.240206, 2.0836449]","[15.271581, 15.3838825, -1.3088001]","[13.429091, 5.6002007, 12.51527]"
62,Michael Schoeffler;Fabian-Robert Stöter;Harald Bayerlein;Bernd Edler;Jürgen Herre,An Experiment about Estimating the Number of Instruments in Polyphonic Music: A Comparison Between Internet and Laboratory Results.,2013,https://doi.org/10.5281/zenodo.1417943,Michael Schoeffler+International Audio Laboratories Erlangen>DEU>facility;Fabian-Robert Stöter+International Audio Laboratories Erlangen>DEU>facility;Harald Bayerlein+International Audio Laboratories Erlangen>DEU>facility;Bernd Edler+International Audio Laboratories Erlangen>DEU>facility;Jürgen Herre+International Audio Laboratories Erlangen>DEU>facility,"Internet experiments in the fields of music perception and music information retrieval are becoming more and more popular. However, not many Internet experiments are compared to laboratory experiments, the consequence being that the effect of the uncontrolled Internet environment on the results is unknown. In this paper the results of an Internet experiment with 1168 participants are compared to those of the same experiment with 62 participants but previously conducted in a controlled environment. The comparison of the Internet and laboratory results enabled us to make a point on whether the Internet can be used for our experiment procedure. The experiment aimed to investigate the listeners ability to correctly estimate the number of instruments being played back in a given excerpt of music. The participants listened to twelve short classical and pop music excerpts each composed using one to six instruments. For each music excerpt the participants were asked how many instruments they could hear and how certain they were about their estimation.",DEU,facility,Developed economies,"[7.32979, -20.13703]","[34.7929, 34.861004]","[15.622965, -4.132926, -6.167455]","[2.5744069, 15.93621, 16.345226]","[8.86563, 7.226959]","[12.338904, 0.93288815]","[11.07492, 12.601333, 0.38870934]","[12.754917, 4.4748693, 11.568352]"
42,Geoffray Bonnin;Dietmar Jannach,Evaluating The Quality of Generated Playlists Based on Hand-Crafted Samples.,2013,https://doi.org/10.5281/zenodo.1418001,Geoffray Bonnin+TU Dortmund>DEU>education;Dietmar Jannach+TU Dortmund>DEU>education,"The automated generation of playlists represents a particular type of the music recommendation problem with two special characteristics. First, the tracks of the list are usually consumed immediately at recommendation time; second, tracks are listened to mostly in consecutive order so that the sequence of the recommended tracks can be relevant. A number of different approaches for playlist generation have been proposed in the literature. In this paper, we review the existing core approaches to playlist generation, discuss aspects of appropriate offline evaluation designs and report the results of a comparative evaluation based on different data sets. Based on the insights from these experiments, we propose a comparably simple and computationally tractable new baseline algorithm for future comparisons, which is based on track popularity and artist information and is competitive with more sophisticated techniques in our evaluation settings.",DEU,education,Developed economies,"[-40.238617, 38.162037]","[37.063435, 21.26697]","[-4.5133996, 29.50693, -4.389487]","[15.39603, 5.2633677, 21.4653]","[16.135983, 8.18249]","[12.221019, 1.6975737]","[16.503845, 14.836282, -1.7145733]","[13.295679, 5.169836, 12.994379]"
43,Emmanouil Benetos;Tillman Weyde,Explicit Duration Hidden Markov Models for Multiple-Instrument Polyphonic Music Transcription.,2013,https://doi.org/10.5281/zenodo.1416088,Emmanouil Benetos+City University London>GBR>education;Tillman Weyde+City University London>GBR>education,"In this paper, a method for multiple-instrument automatic music transcription is proposed that models the temporal evolution and duration of tones. The proposed model supports the use of spectral templates per pitch and instrument which correspond to sound states such as attack, sustain, and decay. Pitch-wise explicit duration hidden Markov models (EDHMMs) are integrated into a convolutive probabilistic framework for modelling the temporal evolution and duration of the sound states. A two-stage transcription procedure integrating note tracking information is performed in order to provide more robust pitch estimates. The proposed system is evaluated on multi-pitch detection and instrument assignment using various publicly available datasets. Results show that the proposed system outperforms a hidden Markov model-based transcription system using the same framework, as well as several state-of-the-art automatic music transcription systems.",GBR,education,Developed economies,"[29.012611, -8.681214]","[-9.614826, -8.15761]","[12.211394, -7.2147655, 12.04451]","[-0.115119465, -8.278917, -12.6715765]","[9.561305, 7.776527]","[6.5483427, 3.0126996]","[11.801062, 12.3223295, 0.106311224]","[8.838696, 7.690477, 10.741295]"
44,François Pachet;Jeff Suzda;Dani Martínez,A Comprehensive Online Database of Machine-Readable Lead-Sheets for Jazz Standards.,2013,https://doi.org/10.5281/zenodo.1417473,François Pachet+Sony CSL>Unknown>company;Jeff Suzda+Sony CSL>Unknown>company;Daniel Martín+Sony CSL>Unknown>company,"Jazz standards are songs representative of a body of musical knowledge shared by most professional jazz musicians. As such, the corpus of jazz standards constitutes a unique opportunity to study a musical genre with a “closed-world” approach, since most jazz composers are no longer in activity today. Although many scores for jazz standards can be found on the Internet, no effort, to our knowledge, has been dedicated so far to building a comprehensive database of machine-readable scores for jazz standards. This paper reports on the rationale, design and population of such a database, containing harmonic (chord progressions) as well as melodic and structural information. The database can be used to feed both analysis and generation systems. We report on preliminary results in this vein. We get around the tricky and often unclear copyright issues imposed by the publishing industry, by providing only statistical information about songs. The completeness of such a database should benefit many research experiments in MIR and opens up novel and exciting applications in music generation exploiting symbolic information, notably in style modeling.",Unknown,company,Unknown,"[9.871928, 13.380266]","[-21.25426, 20.629843]","[-11.2548485, -6.2513056, 28.456488]","[-19.69076, -0.47605106, 8.413502]","[10.822934, 9.389273]","[7.498244, 3.3914874]","[12.226552, 14.3331995, -0.71415186]","[9.70689, 6.221781, 11.445328]"
45,Maurice Grant;Adeesha Ekanayake;Douglas Turnbull,MeUse: Recommending Internet Radio Stations.,2013,https://doi.org/10.5281/zenodo.1418027,Maurice Grant+Ithaca College>USA>education;Adeesha Ekanayake+Ithaca College>USA>education;Douglas Turnbull+Ithaca College>USA>education,"In this paper, we describe a novel Internet radio recommendation system called MeUse. We use the Shoutcast API to collect historical data about the artists that are played on a large set of Internet radio stations. This data is used to populate an artist-station index that is similar to the term-document matrix of a traditional text-based information retrieval system. When a user wants to find stations for a given seed artist, we check the index to determine a set of stations that are either currently playing or have recently played that artist. These stations are grouped into three clusters and one representative station is selected from each cluster. This promotes diversity among the stations that are returned to the user. In addition, we provide additional information such as relevant tags (e.g., genres, emotions) and similar artists to give the user more contextual information about the recommended stations. Finally, we describe a web-based user interface that provides an interactive experience that is more like a personalized Internet radio player (e.g., Pandora) and less like a search engine for Internet radio stations (e.g., Shoutcast). A small-scale user study suggests that the majority of users enjoyed using MeUse but that providing additional contextual information may be needed to help with recommendation transparency.",USA,education,Developed economies,"[-34.370853, 33.811886]","[34.241608, 16.830385]","[-12.852733, 20.921923, -17.364508]","[15.30499, -0.9416801, 17.38161]","[14.955543, 7.597395]","[11.73577, 1.8348315]","[15.066985, 14.44967, -2.014687]","[12.826878, 5.464598, 12.748002]"
46,Stéphane Dupont;Thierry Ravet,Improved Audio Classification Using a Novel Non-Linear Dimensionality Reduction Ensemble Approach.,2013,https://doi.org/10.5281/zenodo.1417705,Stéphane Dupont+University of Mons>BEL>education|Unknown>Unknown>Unknown;Thierry Ravet+University of Mons>BEL>education|Unknown>Unknown>Unknown,"Two important categories of machine learning methodologies have recently attracted much interest in classification research and its applications. On one side, unsupervised and semi-supervised learning allow to benefit from the availability of larger sets of training data, even if not fully annotated with class labels, and of larger sets of diverse feature representations, through novel dimensionality reduction schemes. On the other side, ensemble methods allow to benefit from more diversity in base learners though larger data and feature sets. In this paper, we propose a novel ensemble learning approach making use of recent non-linear dimensionality reduction methods. More precisely, we apply t-SNE (t-distributed Stochastic Neighbor Embedding) to a large feature set to come up with embeddings of various dimensionality. A k-NN classifier is then obtained for each embedding, leading to an ensemble whose estimates can then be combined, making use of various ensemble combination rules from the literature. The rationale of this approach resides in its potential capacity to better handle manifolds of different dimensionality in different regions of the feature space. We evaluate the approach on a transductive audio classification task, where only part of the whole data set is labeled. We confirm that dimensionality reduction by itself can improve performance (by 40% relative), and that creating an ensemble through the proposed approach further reduces classification error rate by about 10% relative.",BEL,education,Developed economies,"[-20.170612, -15.823725]","[23.879517, -14.6331625]","[-12.094383, -2.470661, 17.439493]","[21.520601, 4.6011925, -4.792911]","[12.350419, 10.43559]","[10.127707, 3.526498]","[13.437618, 13.884708, 1.0799735]","[11.767221, 6.913698, 10.668803]"
47,Bogdan Vera;Elaine Chew;Patrick G. T. Healey,A Study of Ensemble Synchronisation Under Restricted Line of Sight.,2013,https://doi.org/10.5281/zenodo.1417363,Bogdan Vera+Queen Mary University of London>GBR>education;Elaine Chew+Queen Mary University of London>GBR>education;Patrick G. T. Healey+Queen Mary University of London>GBR>education,"This paper presents a quantitative study of musician synchronisation in ensemble performance under restricted line of sight, an inherent condition in scenarios like distributed music performance. The study focuses on the relevance of gestural (e.g. visual, breath) cues in achieving note onset synchrony in a violin and cello duo, in which musicians must fulfill a mutual conducting role. The musicians performed two pieces – one with long notes separated by long pauses, another with long notes but no pauses – under direct, partial (silhouettes), and no line of sight. Analysis of the musicians’ note synchrony shows that visual contact significantly impacts synchronization in the first piece, but not significantly in the second piece, leading to the hypothesis that opportunities to shape notes may provide further cues for synchronization. The results also show that breath cues are important, and that the relative positions of these cues impact note asynchrony at the ends of pauses; thus, the advance timing information provided by breath cues could form a basis for generating virtual cues in distributed performance, where network latency delays sonic and visual cues. This study demonstrates the need to account for structure (e.g. pauses, long notes) and prosodic gestures in ensemble synchronisation.",GBR,education,Developed economies,"[26.405825, -32.69901]","[-26.594875, 3.3168037]","[-1.1480086, -23.935003, -14.879282]","[-9.511463, 10.007373, -0.9040716]","[11.328138, 5.7303977]","[5.900072, 1.3494557]","[12.018897, 12.604334, -2.2556837]","[8.088746, 6.3346496, 11.411619]"
48,Andy M. Sarroff;Michael A. Casey,Groove Kernels as Rhythmic-Acoustic Motif Descriptors.,2013,https://doi.org/10.5281/zenodo.1417903,Andy M. Sarroff+Dartmouth College>USA>education;Michael Casey+Dartmouth College>USA>education,"The “groove” of a song correlates with enjoyment and bodily movement. Recent work has shown that humans often agree whether a song does or does not have groove and how much groove a song has. It is therefore useful to develop algorithms that characterize the quality of groove across songs. We evaluate three unsupervised tempo-invariant models for measuring pairwise musical groove similarity: A temporal model, a timbre-temporal model, and a pitch-timbre-temporal model. The temporal model uses a rhythm similarity metric proposed by Holzapfel and Stylianou, while the timbre-inclusive models are built on shift invariant probabilistic latent component analysis. We evaluate the models using a dataset of over 8000 real-world musical recordings spanning approximately 10 genres, several decades, multiple meters, a large range of tempos, and Western and non-Western localities. A blind perceptual study is conducted: given a random music query, humans rate the groove similarity of the top three retrievals chosen by each of the models, as well as three random retrievals.",USA,education,Developed economies,"[41.633514, 7.903195]","[12.034992, 0.40181583]","[-8.278985, -19.287224, 2.110877]","[2.5216718, 17.077997, -1.8030752]","[11.929811, 5.7165575]","[6.430783, 1.7023402]","[11.538113, 14.0935955, -1.9575226]","[8.700105, 6.543941, 12.062605]"
49,Jeffrey J. Scott;Youngmoo E. Kim,Instrument Identification Informed Multi-Track Mixing.,2013,https://doi.org/10.5281/zenodo.1416578,Jeffrey Scott+Drexel University>USA>education|Music and Entertainment Technology Laboratory (MET-lab)>USA>facility;Youngmoo E. Kim+Drexel University>USA>education|Music and Entertainment Technology Laboratory (MET-lab)>USA>facility,"Although digital music production technology has become more accessible over the years, the tools are complex and often difficult to navigate, resulting in a large learning curve for new users. This paper approaches the task of automated multi-track mixing from the perspective of applying common practices based on the instrument types present in a mixture. We apply basic principles to each track automatically, varying the parameters of gain, stereo panning, and coarse equalization. Assuming all instruments are known, a small listening evaluation is completed on the mixed tracks to validate the assumptions of the mixing model. This work represents an exploratory analysis into the efficacy of a hierarchical approach to multi-track mixing using instrument class as a guide to processing techniques.",USA,education,Developed economies,"[18.931437, -37.2209]","[-36.416695, -21.546194]","[12.961308, -25.16736, -12.899885]","[-19.691998, 0.77310085, -26.480991]","[10.011947, 4.741106]","[7.3952017, 6.2350626]","[10.63232, 12.440559, -1.7534494]","[9.425566, 7.616055, 9.440643]"
50,Daniel Gärtner,Tempo Detection of Urban Music Using Tatum Grid Non Negative Matrix Factorization.,2013,https://doi.org/10.5281/zenodo.1415552,Daniel Gärtner+Fraunhofer Institute for Media Technology IDMT>DEU>facility,"High tempo detection accuracies have been reported for the analysis of percussive, constant-tempo, Western music audio signals. As a consequence, active research in the tempo detection domain has been shifted to yet open tasks like tempo analysis of non-percussive, expressive, or non-western music. Also, tempo detection is included in a large range of music-related software. In DJ software, features like beat-synching or tempo-synchronized sound effects are widely accepted in the DJ community, and their users rely on correct tempo hypothesis as their basis. In this paper, we are evaluating both academic and commercial tempo detection systems on a typical dataset of an urban club music DJ. Based on this evaluation, we identify octave errors as a problem that has not yet been solved. Further, an approach based on non-negative matrix factorization is presented. In its current state it can compete with the state of the art. It further provides a foundation to tackle the octave error issue in future research.",DEU,facility,Developed economies,"[36.521954, -24.87738]","[-28.458458, -4.739608]","[-4.716522, -28.64141, -4.057906]","[-6.675987, 10.299802, -9.298942]","[11.590678, 4.504226]","[5.233826, 1.7815396]","[11.015011, 13.404244, -2.6207569]","[7.42402, 6.869755, 11.075196]"
63,Mark Brozier Cartwright;Bryan Pardo,Social-EQ: Crowdsourcing an Equalization Descriptor Map.,2013,https://doi.org/10.5281/zenodo.1415792,Mark Cartwright+Northwestern University>USA>education|Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education|Northwestern University>USA>education,"We seek to simplify audio production interfaces (such as those for equalization) by letting users communicate their audio production objectives with descriptive language (e.g. “Make the violin sound ‘warmer.’”). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin “warmer” with a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions need to be taken. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialEQ, a web-based project for learning a vocabulary of actionable audio equalization descriptors. Since deployment, SocialEQ has learned 324 distinct words in 731 learning sessions. Data on these terms is made available for download. We examine terms users have provided, exploring which ones map well to equalization, which ones have broadly-agreed upon meaning, which term have meanings specific small groups, and which terms are synonymous.",USA,education,Developed economies,"[-8.201023, 6.6092353]","[1.7914162, 26.666195]","[-8.477977, 17.388897, 2.6681762]","[-14.240228, 18.779978, 10.767494]","[13.029235, 9.348644]","[9.641277, 5.4153123]","[13.396955, 15.018731, -0.27834535]","[10.822613, 5.5504875, 10.241084]"
51,Katerina Kosta;Yading Song;György Fazekas;Mark B. Sandler,A Study of Cultural Dependence of Perceived Mood in Greek Music.,2013,https://doi.org/10.5281/zenodo.1415748,Katerina Kosta+Queen Mary University of London>GBR>education;Yading Song+Queen Mary University of London>GBR>education;György Fazekas+Queen Mary University of London>GBR>education;Mark B. Sandler+Queen Mary University of London>GBR>education,"Several algorithms have been developed in the music information retrieval community for predicting mood in music in order to facilitate organising and accessing large audio collections. Little attention has been paid however to how perceived emotion depends on cultural factors, such as listeners’ acculturation or familiarity with musical background or language. In this study, we examine this dependence in the context of Greek music. A large representative database of Greek songs has been created and sampled observing predefined criteria such as the balance between Eastern and Western influenced musical genres. Listeners were then asked to rate songs according to their perceived mood. We collected continuous ratings of arousal and valence for short song excerpts and also asked participants to select a mood tag from a controlled mood vocabulary that best described the music. We analysed the consistency of ratings between Greek and non-Greek listeners and the relationships between the categorical and dimensional representations of emotions. Our results show that there is a greater agreement in listener’s judgements with Greek background compared to the group with varying background. These findings suggest valuable implications on the future development of mood prediction systems.",GBR,education,Developed economies,"[-52.548622, 6.117842]","[54.59614, -3.0241814]","[-19.764347, 30.112219, 3.622739]","[11.034981, 20.630476, 10.960761]","[13.434673, 12.458375]","[13.2233095, 3.5550263]","[16.06735, 14.91854, 1.3743174]","[14.232225, 5.0185366, 10.846862]"
53,Blair Kaneshiro;Hyung-Suk Kim;Jorge Herrera;Jieun Oh;Jonathan Berger;Malcolm Slaney,QBT-Extended: An Annotated Dataset of Melodically Contoured Tapped Queries.,2013,https://doi.org/10.5281/zenodo.1415756,Blair Kaneshiro+Stanford University>USA>education;Hyung-Suk Kim+Stanford University>USA>education;Jorge Herrera+Stanford University>USA>education;Jieun Oh+Stanford University>USA>education;Jonathan Berger+Stanford University>USA>education;Malcolm Slaney+Microsoft Research>USA>company,"Query by tapping remains an intuitive yet underdeveloped form of content-based querying. Tapping databases suffer from small size and often lack useful annotations about users and query cues. More broadly, tapped representations of music are inherently lossy, as they lack pitch information. To address these issues, we publish QBT-Extended—an annotated dataset of over 3,300 tapped queries of pop song excerpts, along with a system for collecting them. The queries, collected from 60 users for 51 songs, contain both time stamps and pitch positions of tap events and are annotated with information about the user, such as musical training and familiarity with each excerpt. Queries were performed from both short-term and long-term memory, cued by lyrics alone or lyrics and audio. In the present paper, we characterize and evaluate the dataset and perform initial analyses, providing early insights into the added value of the novel information. While the current data were collected under controlled experimental conditions, the system is designed for large-scale, crowdsourced data collection, presenting an opportunity to expand upon this richer form of tapping data.",USA,education,Developed economies,"[2.8797028, 21.0304]","[32.48986, 28.071447]","[4.197455, 7.9849052, -5.580605]","[7.3367825, 3.0765228, 23.364355]","[11.641127, 9.898555]","[12.096142, 1.1221188]","[12.24828, 15.464009, -0.9484267]","[12.512536, 5.0497193, 12.395832]"
54,Nicolas Boulanger-Lewandowski;Yoshua Bengio;Pascal Vincent,Audio Chord Recognition with Recurrent Neural Networks.,2013,https://doi.org/10.5281/zenodo.1418319,Nicolas Boulanger-Lewandowski+Université de Montréal>CAN>education;Yoshua Bengio+Université de Montréal>CAN>education;Pascal Vincent+Université de Montréal>CAN>education,"In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.",CAN,education,Developed economies,"[56.99597, -7.593818]","[-35.419556, 21.055468]","[26.251324, -9.00866, 16.67357]","[-25.64801, -2.8464353, -2.2852345]","[6.647795, 8.709531]","[5.900191, 3.8783634]","[11.893402, 10.278331, 2.1723132]","[9.617047, 9.141272, 12.263757]"
55,Julian Moreira;Pierre Roy;François Pachet,Virtualband: Interacting with Stylistically Consistent Agents.,2013,https://doi.org/10.5281/zenodo.1414798,Julian Moreira+Sony CSL>JPN>company;Pierre Roy+Sony CSL>JPN>company;François Pachet+Sony CSL>JPN>company,"VirtualBand is a multi-agent system dedicated to live computer-enhanced music performances. VirtualBand enables one or several musicians to interact in real-time with stylistically plausible virtual agents. The problem addressed is the generation of virtual agents, each representing the style of a given musician, while reacting to human players. We propose a generation framework that relies on feature-based interaction. Virtual agents exploit a style database, which consists of audio signals from which a set of MIR features are extracted. Musical interactions are represented by directed connections between agents through these features. The connections are themselves specified as mappings and database filters. We claim that such a connection framework allows to implement meaningful musical interactions and to produce stylistically consistent musical output. We illustrate this concept through several examples in jazz improvisation, beatboxing and interactive mash-ups.",JPN,company,Developed economies,"[22.730974, 12.368692]","[-7.669642, 35.79485]","[3.7258406, 9.204177, 24.564112]","[-16.966738, 3.5722117, 15.441318]","[10.523953, 8.566694]","[9.426957, 5.8286066]","[13.559763, 11.9886055, 0.019891942]","[10.120343, 5.3811374, 9.999946]"
56,Li Su;Yi-Hsuan Yang,Sparse Modeling for Artist Identification: Exploiting Phase Information and Vocal Separation.,2013,https://doi.org/10.5281/zenodo.1417107,"Li Su+Research Center for Information Technology Innovation, Academia Sinica>TWN>facility;Yi-Hsuan Yang+Research Center for Information Technology Innovation, Academia Sinica>TWN>facility","As artist identification deals with the vocal part of music, techniques such as vocal sound separation and speech feature extraction has been found relevant. In this paper, we argue that the phase information, which is usually overlooked in the literature, is also informative in modeling the voice timbre of a singer, given the necessary processing techniques. Specifically, instead of directly using the raw phase spectrum as features, we show that significantly better performance can be obtained by learning sparse features from the negative derivative of phase with respect to frequency (i.e., group delay function) using unsupervised feature learning algorithms. Moreover, better performance is achieved by using singing voice separation as a pre-processing step, and then learning features from both the magnitude spectrum and the group delay function. The proposed system achieves 66% accuracy in identifying 20 artists from the artist20 dataset, which is better than a prior art by 7%.",TWN,facility,Developing economies,"[-11.863166, -38.76382]","[-45.878277, -31.054016]","[21.29654, 13.563449, -20.659683]","[-3.5697834, -7.758738, -27.476797]","[10.045457, 11.423292]","[6.6380153, 5.1132874]","[11.36031, 15.479457, 0.78905725]","[10.155158, 8.468905, 9.714271]"
57,Emmanouil Benetos;Andre Holzapfel,Automatic Transcription of Turkish Makam Music.,2013,https://doi.org/10.5281/zenodo.1416468,Emmanouil Benetos+City University London>GBR>education;Andre Holzapfel+Boğaziçi University>TUR>education,"In this paper we propose an automatic system for transcribing makam music of Turkey. We document the specific traits of this music that deviate from properties that were targeted by transcription tools so far and we compile a dataset of makam recordings along with aligned microtonal ground-truth. An existing multi-pitch detection algorithm is adapted for transcribing music in 20 cent resolution, and the final transcription is centered around the tonic frequency of the recording. Evaluation metrics for transcribing microtonal music are utilized and results show that transcription of Turkish makam music in e.g. an interactive transcription software is feasible using the current state-of-the-art.",GBR,education,Developed economies,"[10.5602045, 5.2201047]","[-8.736096, -15.484226]","[-13.935522, -23.580826, -13.149403]","[1.7042713, -11.643625, -9.08275]","[11.70989, 10.696347]","[6.8619127, 2.237567]","[12.177421, 14.946486, -1.559087]","[9.05273, 7.2947187, 11.348451]"
58,Sebastian Böck;Gerhard Widmer,Local Group Delay Based Vibrato and Tremolo Suppression for Onset Detection.,2013,https://doi.org/10.5281/zenodo.1416460,Sebastian Böck+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"In this paper we present a new vibrato and tremolo suppression technique for onset detection. It weights the differences of the magnitude spectrogram used for the calculation of the spectral flux onset detection function on the basis of the local group delay information. With this weighting technique applied, the onset detection function is able to reliably distinguish between genuine onsets and spectral energy peaks originating from vibrato or tremolo present in the signal and lowers the number of false positive detections considerably. Especially in cases of music with numerous vibratos and tremolos (e.g. opera singing or string performances) the number of false positive detections can be reduced by up to 50% without missing any additional events. Performance is evaluated and compared to current state-of-the-art algorithms using three different datasets comprising mixed audio material (25,927 onsets), violin recordings (7,677 onsets) and solo voice recordings of operas (1,448 onsets).",AUT,education,Developed economies,"[31.595331, -27.608343]","[-21.701706, -6.017618]","[8.603061, -22.571472, -5.273072]","[-0.0923819, 5.2912946, -12.87189]","[10.303083, 4.943529]","[5.8432136, 2.5238554]","[10.350563, 13.216983, -1.5743808]","[8.108428, 7.6370144, 10.912747]"
59,Kazuyoshi Yoshii;Ryota Tomioka;Daichi Mochihashi;Masataka Goto,Beyond NMF: Time-Domain Audio Source Separation without Phase Reconstruction.,2013,https://doi.org/10.5281/zenodo.1415060,Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility|The University of Tokyo>JPN>education|The Institute of Statistical Mathematics (ISM)>JPN>facility;Ryota Tomioka+The University of Tokyo>JPN>education;Daichi Mochihashi+The Institute of Statistical Mathematics (ISM)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a new fundamental technique for source separation of single-channel audio signals. Although non-negative matrix factorization (NMF) has recently become very popular for music source separation, it deals only with the amplitude or power of the spectrogram of a given mixture signal and completely discards the phase. The component spectrograms are typically estimated using a Wiener filter that reuses the phase of the mixture spectrogram, but such rough phase reconstruction makes it hard to recover high-quality source signals because the estimated spectrograms are inconsistent, i.e., they do not correspond to any real time-domain signals. To avoid the frequency-domain phase reconstruction, we use positive semidefinite tensor factorization (PSDTF) for directly estimating source signals from the mixture signal in the time domain. Since PSDTF is a natural extension of NMF, an efficient multiplicative update algorithm for PSDTF can be derived. Experimental results show that PSDTF outperforms conventional NMF variants in terms of source separation quality.",JPN,facility,Developed economies,"[10.509604, -46.056606]","[-45.754322, -25.75579]","[29.972523, -1.4576774, -8.511752]","[-6.9019217, -10.868071, -31.366396]","[8.532164, 9.901712]","[6.2725554, 5.2190876]","[11.067925, 13.697507, 1.488393]","[9.81343, 8.8739605, 9.766795]"
60,Dawen Liang;Matthew D. Hoffman;Daniel P. W. Ellis,Beta Process Sparse Nonnegative Matrix Factorization for Music.,2013,https://doi.org/10.5281/zenodo.1416206,Dawen Liang+Columbia University>USA>education|Adobe Research>USA>company;Matthew D. Hoffman+Adobe Systems Incorporated>USA>company;Daniel P. W. Ellis+Columbia University>USA>education,"Nonnegative matrix factorization (NMF) has been widely used for discovering physically meaningful latent components in audio signals to facilitate source separation. Most of the existing NMF algorithms require that the number of latent components is provided a priori, which is not always possible. In this paper, we leverage developments from the Bayesian nonparametrics and compressive sensing literature to propose a probabilistic Beta Process Sparse NMF (BP-NMF) model, which can automatically infer the proper number of latent components based on the data. Unlike previous models, BP-NMF explicitly assumes that these latent components are often completely silent. We derive a novel mean-field variational inference algorithm for this nonconjugate model and evaluate it on both synthetic data and real recordings on various tasks.",USA,education,Developed economies,"[-5.7403855, -1.424041]","[-46.986996, -24.98892]","[7.426079, -7.8561597, 5.080776]","[-6.0769553, -12.677654, -29.869562]","[11.053278, 8.200739]","[6.235065, 5.0756006]","[12.147909, 13.467611, 0.35721126]","[9.708526, 8.864078, 9.799857]"
61,Vipul Arora;Laxmidhar Behera,Semi-Supervised Polyphonic Source Identification using PLCA Based Graph Clustering.,2013,https://doi.org/10.5281/zenodo.1418293,"Vipul Arora+Indian Institute of Technology, Kanpur>IND>education;Laxmidhar Behera+Indian Institute of Technology, Kanpur>IND>education","For identifying instruments or singers in the polyphonic audio, supervised probabilistic latent component analysis (PLCA) is a popular tool. But in many cases individual source audio is not available for training. To address this problem, this paper proposes a novel scheme using semi-supervised PLCA with probabilistic graph clustering, which does not require individual sources for training. The PLCA is based on source-filter approach which models the spectral envelope as a weighted sum of elementary band-pass filters. The novel graph based approach, embedded in the PLCA framework, takes into account various perceptual cues for characterizing a source. These cues include temporal cues like the evolution of F0 contours as well as the acoustic cues like mel-frequency cepstral coefficients. The proposed scheme shows better results in identifying vocal sources than a state of the art unsupervised scheme. In addition, the proposed framework can be used to incorporate perceptual cues so as to enhance the performance of supervised schemes too.",IND,education,Developing economies,"[4.490543, -21.27163]","[-41.96655, -22.211994]","[12.453067, -3.9609652, -2.9560256]","[-2.335053, -9.614823, -25.097424]","[9.333328, 7.4976206]","[6.7423987, 4.7241826]","[11.461154, 13.013708, 0.2994372]","[10.043015, 8.414169, 10.169303]"
52,Yajie Hu;Dingding Li;Mitsunori Ogihara,Evaluation on Feature Importance for Favorite Song Detection.,2013,https://doi.org/10.5281/zenodo.1416300,Yajie Hu+University of Miami>USA>education;Dingding Li+University of Miami>USA>education;Ogihara Mitsunori+University of Miami>USA>education,"""Detecting whether a song is favorite for a user is an important but also challenging task in music recommendation. One of critical steps to do this task is to select important features for the detection. This paper presents two methods to evaluate feature importance, in which we compared nine available features based on a large user log in the real world. The set of features includes song metadata, acoustic feature, and user preference used by Collaborative Filtering techniques. The evaluation methods are designed from two views: i) the correlation between the estimated scores by song similarity in respect of a feature and the scores estimated by real play count, ii) feature selection methods over a binary classification problem, i.e., “like” or “dislike”. The experimental results show the user preference is the most important feature and artist similarity is of the second importance among these nine features.""",USA,education,Developed economies,"[-25.77324, 13.024043]","[36.20778, 14.5298195]","[0.80744594, 17.409643, -6.330654]","[15.170713, 3.2682762, 16.006886]","[13.773331, 8.735999]","[12.345709, 1.9094542]","[13.920685, 14.994493, -1.4644]","[13.316323, 5.428833, 12.722607]"
87,Zhouhong Cai;Robert J. Ellis;Zhiyan Duan;Hong Lu;Ye Wang,Basic Evaluation of Auditory Temporal Stability (Beats): A Novel Rationale and Implementation.,2013,https://doi.org/10.5281/zenodo.1415096,Zhuohong Cai+National University of Singapore>SGP>education|Fudan University>CHN>education;Robert J. Ellis+National University of Singapore>SGP>education;Zhiyan Duan+National University of Singapore>SGP>education;Hong Lu+Fudan University>CHN>education;Ye Wang+National University of Singapore>SGP>education,"The accurate detection of pulse-level temporal stability has important practical applications; for example, the creation of fixed-tempo playlists for recreational exercise (e.g., jogging), rehabilitation therapy (e.g., rhythmic gait training), or disc jockeying (e.g., dance mixes). Although there are numerous software algorithms which return simple point estimate statistics of “overall” tempo, none has operationalized the beat-to-beat stability of an inter-beat interval series. We propose such a method here, along with several novel summary statistics. We illustrate this approach using a public data set (the 10,000-item subset of the Million Song Dataset) and outline a series of future steps for this project.",SGP,education,Developing economies,"[22.995539, -26.569622]","[-25.801598, -1.3809519]","[-1.0138518, -20.586147, -3.0631497]","[-3.77658, 11.067142, -5.423855]","[10.679384, 5.171959]","[5.4908705, 1.7424368]","[10.834573, 13.276605, -1.7271191]","[7.641327, 6.9039073, 11.224486]"
88,Tom Collins;Andreas Arzt;Sebastian Flossmann;Gerhard Widmer,SIARCT-CFP: Improving Precision and the Discovery of Inexact Musical Patterns in Point-Set Representations.,2013,https://doi.org/10.5281/zenodo.1416622,Tom Collins+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Andreas Arzt+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Sebastian Flossmann+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"The geometric approach to intra-opus pattern discovery (in which notes are represented as points in pitch-time space in order to discover repeated patterns within a piece of music) shows promise particularly for polyphonic music, but has attracted some criticism because: (1) the approach extends to a limited number of inexact repetition types only; (2) typically geometric pattern discovery algorithms have poor precision, returning many false positives. This paper describes and evaluates a solution to the inexactness problem where algorithms for pattern discovery and inexact pattern matching are integrated for the first time. Two complementary solutions are proposed and assessed for the precision problem, one involving categorisation (hence reduction) of output patterns, and the second involving a new algorithm that calculates the difference between consecutive point pairs, rather than all point pairs.",AUT,education,Developed economies,"[18.694508, 21.374493]","[5.153083, 17.204086]","[-2.8631222, -11.703107, 5.012457]","[1.8388335, -11.726693, 5.037744]","[11.489743, 7.6887937]","[8.819454, 1.1292926]","[12.694377, 13.303589, -0.78176194]","[10.424255, 6.758419, 12.924018]"
89,Reinier de Valk;Tillman Weyde;Emmanouil Benetos,A Machine Learning Approach to Voice Separation in Lute Tablature.,2013,https://doi.org/10.5281/zenodo.1418279,Reinier de Valk+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Emmanouil Benetos+City University London>GBR>education,"In this paper, we propose a machine learning model for voice separation in lute tablature. Lute tablature is a practical notation that reveals only very limited information about polyphonic structure. This has complicated research into the large surviving corpus of lute music, notated exclusively in tablature. A solution may be found in automatic transcription, of which voice separation is a necessary step. During the last decade, several methods for separating voices in symbolic polyphonic music formats have been developed. However, all but two of these methods adopt a rule-based approach; moreover, none of them is designed for tablature. Our method differs on both these points. First, rather than using fixed rules, we use a model that learns from data: a neural network that predicts voice assignments for notes. Second, our method is specifically designed for tablature—tablature information is included in the features used as input for the models—but it can also be applied to other music corpora. We have experimented on a dataset containing tablature pieces of different polyphonic textures, and compare the results against those obtained from a baseline hidden Markov model (HMM) model. Additionally, we have performed a preliminary comparison of the neural network model with several existing methods for voice separation on a small dataset. We have found that the neural network model performs clearly better than the baseline model, and competitively with the existing methods.",GBR,education,Developed economies,"[0.28204492, -45.9006]","[-34.95116, -34.14774]","[31.028643, 6.475146, -4.3759394]","[-13.718211, -12.924022, -21.974857]","[8.820939, 10.546697]","[7.270912, 5.3042493]","[10.8506155, 14.351926, 1.4708984]","[9.516301, 7.785435, 9.250688]"
20,Gabriel Vigliensoni;Gregory Burlet;Ichiro Fujinaga,Optical Measure Recognition in Common Music Notation.,2013,https://doi.org/10.5281/zenodo.1417024,Gabriel Vigliensoni+McGill University>CAN>education;Gregory Burlet+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper presents work on the automatic recognition of measures in common Western music notation scores using optical music recognition techniques. It is important to extract the bounding boxes of measures within a music score to facilitate some methods of multimodal navigation of music catalogues. We present an image processing algorithm that extracts the position of barlines on an input music score in order to deduce the number and position of measures on the page. An open-source implementation of this algorithm is made publicly available. In addition, we have created a ground-truth dataset of 100 images of music scores with manually annotated measures. We conducted several experiments using different combinations of values for two critical parameters to evaluate our measure recognition algorithm. Our algorithm obtained an f-score of 91 percent with the optimal set of parameters. Although our implementation obtained results similar to previous approaches, the scope and size of the evaluation dataset is significantly larger.",CAN,education,Developed economies,"[41.459465, 19.521196]","[-21.428526, 34.779827]","[23.178946, 14.71136, 11.397994]","[-8.56451, -19.686056, -0.2987334]","[8.751082, 6.020379]","[6.673341, -0.21815342]","[10.696019, 11.140932, -0.24815343]","[8.064582, 4.4250717, 10.702557]"
19,Nicholas J. Bryan;Gautham J. Mysore;Ge Wang 0002,Source Separation of Polyphonic Music with Interactive User-Feedback on a Piano Roll Display.,2013,https://doi.org/10.5281/zenodo.1418247,Nicholas J. Bryan+Stanford University>USA>education|Adobe Research>USA>company;Gautham J. Mysore+Adobe Research>USA>company;Ge Wang+Stanford University>USA>education,"The task of separating a single recording of a polyphonic instrument (e.g. piano, guitar, etc.) into distinctive pitch tracks is challenging. One promising class of methods to accomplish this task is based on non-negative matrix factorization (NMF). Such methods, however, are still far from perfect. Distinct pitches from a single instrument have similar timbre, similar note attacks, and contain overlapping harmonics that all make separation difficult. In an attempt to overcome these issues, we use a database of synthesized piano and guitar recordings to learn the harmonic structure of distinct pitches, perform NMF-based separation, and then extend the method to allow an end-user to interactively correct for errors in the output separation estimates by drawing on a piano roll display of the separated tracks. The user-annotations are mapped to linear grouping regularization parameters within a modified NMF-based algorithm and are then used to refine the separation estimates in an iterative manner. For evaluation, a prototype user-interface was built and used to separate several polyphonic guitar and piano recordings. Initial results show that the method of interactive feedback can significantly increase the separation quality and produce high-quality separation results.",USA,education,Developed economies,"[4.815699, -44.648453]","[-45.402237, -26.828215]","[28.436354, -0.03488212, 0.9247243]","[-7.4623055, -12.2471285, -27.118935]","[8.412473, 10.007696]","[6.3296905, 5.1270885]","[10.999983, 13.473536, 1.5374385]","[9.629982, 8.736118, 9.717847]"
18,Mika Kuuskankare;Craig Sapp,Visual Humdrum-Library for PWGL.,2013,https://doi.org/10.5281/zenodo.1418057,Mika Kuuskankare+Sibelius Academy>FIN>education;Craig Stuart Sapp+Stanford University>USA>education,"We introduce a PWGL Humdrum interface that integrates command-line unix tools for music analysis into a visual programming environment. This symbiosis allows users access to the strengths of each system—algorithmic composition and visual programming components of PWGL along with computational analysis and data processing features of Humdrum tools. Our novel interface for Humdrum graphical programming allows non-programmers better access to Humdrum analysis tools, particularly with the built-in music notation display capabilities of PWGL. ENP (Expressive Notation Package) data from PWGL can be exported as Humdrum data. Humdrum files in turn can be converted back into ENP data, allowing bi-directional communication between the two software systems.",FIN,education,Developed economies,"[5.7213316, 33.645126]","[-2.2421393, 32.462605]","[-21.830147, -17.916931, -16.290472]","[-11.924251, -4.2372684, 13.888313]","[13.194943, 6.764086]","[9.809036, 0.5972141]","[13.793053, 13.049156, -2.002391]","[10.383368, 5.3753963, 11.741453]"
17,Jan Van Balen;John Ashley Burgoyne;Frans Wiering;Remco C. Veltkamp,An Analysis of Chorus Features in Popular Song.,2013,https://doi.org/10.5281/zenodo.1415624,Jan Van Balen+Utrecht University>NLD>education;John Ashley Burgoyne+Universiteit van Amsterdam>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"This paper presents a computational study of the perceptual and musicological audio features that correlate with the structural function of sections in pop songs, specifically the chorus. Choruses have been described as more prominent, more catchy and more memorable than other sections in a song, yet chorus detection applications have always been primarily based on identifying the most-repeated section in a song. Inspired by cognitive research rather than applied signal processing, this computational analysis compiles a list of robust and interpretable features and models their influence on the ‘chorusness’ of a collection of song sections from the Billboard dataset. This is done through the unsupervised learning of a probabilistic graphical model. We show that timbre and timbre variety are more strongly related to chorus qualities than harmony and absolute pitch height. A regression and a classification experiment are performed to quantify these relations.",NLD,education,Developed economies,"[-28.894331, -28.202728]","[24.264746, -5.112752]","[10.445394, 22.522942, -8.750652]","[5.8057113, 11.370014, 0.846848]","[11.560842, 11.415234]","[9.006657, 2.67891]","[12.405707, 15.7820425, 0.7723326]","[10.827569, 7.193033, 11.541761]"
16,Alastair Porter;Mohamed Sordo;Xavier Serra,Dunya: A System to Browse Audio Music Collections Exploiting Cultural Context.,2013,https://doi.org/10.5281/zenodo.1417355,"Alastair Porter+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Mohamed Sordo+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Music recommendation and discovery is an important MIR application with a strong impact in the music industry, but most music recommendation systems are still quite generic and without much musical knowledge. In this paper we present a web-based software application that lets users interact with an audio music collection through the use of musical concepts that are derived from a specific musical culture, in this case Carnatic music. The application includes a database containing information relevant to that music collection, such as audio recordings, editorial information, and metadata obtained from various sources. An analysis module extracts features from the audio recordings that are related to Carnatic music, which are then used to create musically meaningful relationships between all of the items in the database. The application displays the content of these items, allowing users to navigate through the collection by identifying and showing other information that is related to the currently viewed item, either by showing the relationships between them or by using culturally relevant similarity measures. The basic architecture and the design principles developed are reusable for other music collections with different characteristics.",ESP,education,Developed economies,"[-17.759773, 27.128183]","[25.878557, 22.853659]","[-10.6744995, 8.240943, -18.743788]","[9.299538, 2.1743903, 18.828566]","[14.203822, 7.676469]","[11.251819, 1.0964993]","[14.248051, 14.562235, -2.2337017]","[12.176354, 5.1790013, 12.397375]"
15,Diego Furtado Silva;Hélène Papadopoulos;Gustavo Enrique De Almeida Prado Alves Batista;Daniel P. W. Ellis,A Video Compression-Based Approach to Measure Music Structural Similarity.,2013,https://doi.org/10.5281/zenodo.1417087,"Diego F. Silva+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education;Hélène Papadopoulos+Laboratoire des Signaux et Systèmes (L2S), CNRS UMR 8506>FRA>education;Gustavo E.A.P.A. Batista+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education;Daniel P.W. Ellis+Columbia University>USA>education","The choice of the distance measure between time-series representations can be decisive to achieve good classification results in many content-based information retrieval applications. In the field of Music Information Retrieval, two-dimensional representations of the music signal are ubiquitous. Such representations are useful to display patterns of evidence that are not clearly revealed directly in the time domain. Among these representations, self-similarity matrices have become common representations for visualizing the time structure of an audio signal. In the context of organizing recordings, recent work has shown that, given a collection of recordings, it is possible to group performances of the same musical work based on the pairwise similarity between structural representations of the audio signal. In this work, we introduce the use of the Campana-Keogh distance, a video compression-based measure, to compare musical items based on their structure. Through extensive experiments, we show that the use of this distance measure outperforms the results of previous work using similar approaches but other distance measures. Along with quantitative results, detailed examples are provided to illustrate the benefits of using the newly proposed distance measure.",BRA,education,Developing economies,"[-5.7559137, 10.79464]","[20.546003, 9.349725]","[-2.8653502, 1.3894075, -2.19341]","[11.486451, -3.9148004, 5.0794363]","[13.014411, 9.014716]","[9.887383, 2.035345]","[13.448957, 14.847249, -0.7056019]","[11.446554, 6.7451267, 12.427253]"
14,Yading Song;Simon Dixon;Marcus Pearce;Andrea R. Halpern,Do Online Social Tags Predict Perceived or Induced Emotional Responses to Music?,2013,https://doi.org/10.5281/zenodo.1415212,Yading Song+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education;Marcus Pearce+Queen Mary University of London>GBR>education;Andrea Halpern+Bucknell University>USA>education,"Music provides a powerful means of communication and self-expression. A wealth of research has been performed on the study of music and emotion, including emotion modelling and emotion classification. The emergence of online social tags (OST) has provided highly relevant information for the study of mood, as well as an important impetus for using discrete emotion terms in the study of continuous models of affect. Yet, the extent to which human annotation reveals either perceived emotion or induced emotion remains unknown. 80 musical excerpts were randomly selected from a collection of 2904 songs labelled with the Last.fm tags “happy”, “sad”, “angry” and “relax”. Forty-seven participants provided emotion ratings on the two continuous dimensions of valence and arousal for both perceived and induced emotion. Analysis of variance did not reveal significant differences in ratings between perceived emotion and induced emotion. Moreover, the results indicated that, regardless of the discrete type of emotion experienced, listeners’ ratings of perceived and induced emotion were highly positively correlated. Finally, the emotion tags “happy”, “sad” and “angry” but not “relax” predicted the corresponding experimentally provided emotion categories.",GBR,education,Developed economies,"[-48.69673, 3.865021]","[54.64243, -4.6336775]","[-20.634506, 19.723219, -0.58535343]","[9.657615, 22.396645, 10.141362]","[13.816249, 12.237911]","[13.120932, 3.6687026]","[15.925904, 14.74748, 1.09501]","[14.078074, 4.9201946, 10.708304]"
13,Matthias Mauch;Sebastian Ewert,The Audio Degradation Toolbox and Its Application to Robustness Evaluation.,2013,https://doi.org/10.5281/zenodo.1415862,Matthias Mauch+Queen Mary University of London>GBR>education;Sebastian Ewert+Queen Mary University of London>GBR>education,"We introduce the Audio Degradation Toolbox (ADT) for the controlled degradation of audio signals, and propose its usage as a means of evaluating and comparing the robustness of audio processing algorithms. Music recordings encountered in practical applications are subject to varied, sometimes unpredictable degradation. For example, audio is degraded by low-quality microphones, noisy recording environments, MP3 compression, dynamic compression in broadcasting or vinyl decay. In spite of this, no standard software for the degradation of audio exists, and music processing methods are usually evaluated against clean data. The ADT fills this gap by providing Matlab scripts that emulate a wide range of degradation types. We describe 14 degradation units, and how they can be chained to create more complex, ‘real-world’ degradations. The ADT also provides functionality to adjust existing ground-truth, correcting for temporal distortions introduced by degradation. Using four different music informatics tasks, we show that performance strongly depends on the combination of method and degradation applied. We demonstrate that specific degradations can reduce or even reverse the performance difference between two competing methods. ADT source code, sounds, impulse responses and definitions are freely available for download.",GBR,education,Developed economies,"[-11.870666, -23.818562]","[-50.78786, -12.221384]","[7.3148646, -6.944845, -21.05303]","[-5.052831, -16.17543, -9.168288]","[9.18741, 4.8397355]","[7.7301865, 6.176444]","[10.876743, 11.705473, -1.5861033]","[9.891352, 6.4235725, 8.510432]"
12,Nicola Orio;Roberto Piva,Combining Timbric and Rhythmic Features for Semantic Music Tagging.,2013,https://doi.org/10.5281/zenodo.1415834,Nicola Orio+University of Padua>ITA>education|University of Padua>ITA>education;Roberto Piva+University of Padua>ITA>education|University of Padua>ITA>education,"In this paper we propose a novel approach to music tagging. The approach uses a statistical framework to model two acoustic features: timbre and rhythm. A collection of tagged music is thus represented as a graph where the states correspond to the songs and the models probabilities are related to the timbric and rhythmic similarity. Under the assumption that acoustically similar songs have similar tags, we infer the tags of a new song by adding it to the graph structure and observing the tags visited in acoustically meaningful random walks. The approach has been tested using the CAL500 dataset, with encouraging results in terms of precision.",ITA,education,Developed economies,"[-40.48917, -4.2125893]","[39.634792, -1.4025254]","[-12.846586, 12.238696, 11.864046]","[24.587566, 5.92389, 3.5254257]","[14.303485, 10.370672]","[11.510672, 3.4185538]","[15.503329, 14.0622425, 0.032914404]","[13.048924, 6.409715, 11.231319]"
21,Gabriel Vigliensoni;John Ashley Burgoyne;Ichiro Fujinaga,Musicbrainz for The World: The Chilean Experience.,2013,https://doi.org/10.5281/zenodo.1417951,Gabriel Vigliensoni+McGill University>CAN>education|Unknown>Unknown>Unknown;John Ashley Burgoyne+University of Amsterdam>NLD>education|Unknown>Unknown>Unknown;Ichiro Fujinaga+McGill University>CAN>education|Unknown>Unknown>Unknown,"In this paper we present our research in gathering data from several semi-structured collections of cultural heritage—Chilean music-related websites—and uploading the data into an open-source music database, where the data can be easily searched, discovered, and interlinked. This paper also reviews the characteristics of four user-contributed, music metadatabases (MusicBrainz, Discogs, MusicMoz, and FreeDB), and explains why we chose MusicBrainz as the repository for our data. We also explain how we collected data from the five most important sources of Chilean music-related data, and we give details about the context, design, and results of an experiment for artist name comparison to verify which of the artists that we have in our database exist in the MusicBrainz database already. Although it represents a single case study, we believe this information will be of great help to other MIR researchers who are trying to design their own studies of world music.",CAN,education,Developed economies,"[-29.733416, 32.755077]","[21.095406, 36.241657]","[-26.139498, 8.160716, -9.309987]","[-2.0359943, 1.4057109, 23.205036]","[14.470554, 8.316349]","[11.217142, 0.16160966]","[14.67307, 14.327734, -1.7843482]","[12.06513, 4.820777, 12.064367]"
11,Cory McKay,JProductionCritic: An Educational Tool for Detecting Technical Errors in Audio Mixes.,2013,https://doi.org/10.5281/zenodo.1416180,Cory McKay+Marianopolis College>CAN>education,"jProductionCritic is an open-source educational framework for automatically detecting technical recording, editing and mixing problems in audio files. It is intended to be used as a learning and proofreading tool by students and amateur producers, and can also assist teachers as a timesaving tool when grading recordings. A number of novel error detection algorithms are implemented by jProductionCritic. Problems detected include edit errors, clipping, noise infiltration, poor use of dynamics, poor track balancing, and many others. The error detection algorithms are highly configurable, in order to meet the varying aesthetics of different musical genres (e.g. Baroque vs. noise music). Effective general-purpose default settings were developed based on experiments with a variety of student pieces, and these settings were then validated using a reserved set of student pieces. jProductionCritic is also designed to serve as an extensible framework to which new detection modules can be easily plugged in. It is hoped that this will help to galvanize MIR research relating to audio production, an area that is currently underrepresented in the MIR literature, and that this work will also help to address the current general lack of educational production software.",CAN,education,Developed economies,"[-8.562491, -23.124416]","[-8.607231, 31.001068]","[13.398279, -8.571135, -22.586628]","[-5.962886, -16.274576, 6.379243]","[10.053409, 6.4913726]","[7.4609666, 1.8098735]","[11.466264, 11.957741, -1.1332327]","[9.919856, 5.8246946, 10.722975]"
9,Monojit Choudhury;Ranjita Bhagwan;Kalika Bali,The Use Of Melodic Scales In Bollywood Music: An Empirical Study.,2013,https://doi.org/10.5281/zenodo.1418235,Monojit Choudhury+Microsoft Research Lab India>IND>company;Ranjita Bhagwan+Microsoft Research Lab India>IND>company;Kalika Bali+Microsoft Research Lab India>IND>company,"Hindi film music, which is commonly referred to as “Bollywood” music, is one of the most popular forms of music in the world today. One of the reasons for its popularity has been the willingness of Bollywood composers to adopt and be influenced by various musical forms including Western pop, jazz, rock, and classical music. However, till date, we are unaware of any systematic quantitative analysis of how this genre has changed and evolved over the years since its inception in the early 20th century. In this paper, we study the evolution of Bollywood music with respect to the use of melodic scales. We analyse songs composed over seven decades using a database of top-lists, which reveals many interesting patterns. We also analyze the scale usage patterns in the music of some of the most popular composers, which clearly brings out certain idiosyncrasies and preferences of each of them.",IND,company,Developing economies,"[6.5079794, 2.775126]","[2.9469712, -17.489388]","[5.8376117, 2.8703728, -9.53358]","[14.310791, -12.600037, -8.437787]","[11.3733635, 10.272992]","[7.588076, 1.2416173]","[11.980462, 15.554923, -1.1193944]","[8.975955, 7.0623074, 12.58974]"
8,Bernhard Lehner;Reinhard Sonnleitner;Gerhard Widmer,"Towards Light-Weight, Real-Time-Capable Singing Voice Detection.",2013,https://doi.org/10.5281/zenodo.1415512,Bernhard Lehner+Johannes Kepler University of Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Reinhard Sonnleitner+Johannes Kepler University of Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Gerhard Widmer+Johannes Kepler University of Linz>AUT>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"We present a study that indicates that singing voice detection – the problem of identifying those parts of a polyphonic audio recording where one or several persons sing(s) – can be realised with substantially fewer (and less expensive) features than used in current state-of-the-art methods. Essentially, we show that MFCCs alone, if appropriately optimised and used with a suitable classifier, are sufficient to achieve detection results that seem on par with the state of the art – at least as far as this can be ascertained by direct, fair comparisons to existing systems. To make this comparison, we select three relevant publications from the literature where publicly accessible training/test data were used, and where the experimental setup is described in enough detail for us to perform fair comparison experiments. The result of the experiments is that with our simple, optimised MFCC-based classifier we achieve at least comparable identification results, but with (in some cases much) less computational effort, and without any need for extensive lookahead, thus paving the way to on-line, real-time voice detection applications.",AUT,education,Developed economies,"[-6.906189, -35.98883]","[9.012177, -25.098713]","[21.30332, 9.0506, -13.613043]","[7.822429, -4.5209856, -18.580648]","[9.785623, 11.057345]","[8.68792, 3.66968]","[11.141482, 15.099934, 0.7749099]","[10.794871, 7.5534062, 9.783989]"
7,Antti Laaksonen;Kjell Lemström,On Finding Symbolic Themes Directly From Audio Files Using Dynamic Programming.,2013,https://doi.org/10.5281/zenodo.1418349,Antti Laaksonen+University of Helsinki>FIN>education;Kjell Lemström+Laurea University of Applied Sciences>FIN>education,"""In this paper our goal is to find occurrences of a theme within a musical work. The theme is given in a symbolic form that is searched for directly in an audio file. We present a dynamic programming algorithm that is related to an existing time-warp invariant algorithm. However, the new algorithm is computationally more efficient than its predecessor, and it can also be used for approximate time-scale invariant search. In the latter case the note durations in the query are taken into account, but some time jittering is allowed for. When dealing with audio, these are important properties because the number of possible note events is large and the note positions are not exact. We evaluate the algorithm using a collection of themes from Tchaikovsky’s symphonies. The new approximate time-scaled algorithm seems to be a good choice for this setting.""",FIN,education,Developed economies,"[12.919237, 19.589039]","[5.5377326, 12.26533]","[-6.5223694, -13.663036, 14.578287]","[5.5125957, -12.642286, 6.792438]","[11.900829, 7.0123696]","[8.385701, 1.014617]","[13.603364, 12.333859, -1.0105116]","[10.026469, 6.475698, 12.906808]"
6,Zafar Rafii;François G. Germain;Dennis L. Sun;Gautham J. Mysore,Combining Modeling Of Singing Voice And Background Music For Automatic Separation Of Musical Mixtures.,2013,https://doi.org/10.5281/zenodo.1415066,Zafar Raﬁi+Northwestern University>USA>education;François G. Germain+Stanford University>USA>education;Dennis L. Sun+Stanford University>USA>education;Gautham J. Mysore+Adobe Research>USA>company,"Musical mixtures can be modeled as being composed of two characteristic sources: singing voice and background music. Many music/voice separation techniques tend to focus on modeling one source; the residual is then used to explain the other source. In such cases, separation performance is often unsatisfactory for the source that has not been explicitly modeled. In this work, we propose to combine a method that explicitly models singing voice with a method that explicitly models background music, to address separation performance from the point of view of both sources. One method learns a singer-independent model of voice from singing examples using a Non-negative Matrix Factorization (NMF) based technique, while the other method derives a model of music by identifying and extracting repeating patterns using a similarity matrix and a median filter. Since the model of voice is singer-independent and the model of music does not require training data, the proposed method does not require training data from a user, once deployed. Evaluation on a data set of 1,000 song clips showed that combining modeling of both sources can improve separation performance, when compared with modeling only one of the sources, and also compared with two other state-of-the-art methods.",USA,education,Developed economies,"[1.9910109, -42.44301]","[-45.59385, -28.140478]","[25.443226, 2.1358602, -5.9958982]","[-6.456672, -10.637716, -27.79609]","[8.933653, 10.556337]","[6.347408, 5.161244]","[10.922367, 14.2242365, 1.3987193]","[9.76117, 8.729153, 9.668686]"
5,Pranay Dighe;Harish Karnick;Bhiksha Raj,Swara Histogram Based Structural Analysis And Identification Of Indian Classical Ragas.,2013,https://doi.org/10.5281/zenodo.1417841,"Pranay Dighe+Indian Institute of Technology, Kanpur>IND>education|Carnegie Mellon University>USA>education;Harish Karnick+Indian Institute of Technology, Kanpur>IND>education;Bhiksha Raj+Carnegie Mellon University>USA>education","This work is an attempt towards robust automated analysis of Indian classical ragas through machine learning and signal processing tools and techniques. Indian classical music has a definite hierarchical structure where macro level concepts like thaats and raga are defined in terms of micro entities like swaras and shrutis. Swaras or notes in Indian music are defined only in terms of their relation to one another (akin to the movable do-re-mi-fa system), and an inference must be made from patterns of sounds, rather than their absolute frequency structure. We have developed methods to perform scale-independent raga identification using a random forest classifier on swara histograms and achieved state-of-the-art results for the same. The approach is robust as it directly works on partly noisy raga recordings from Youtube videos without knowledge of the scale used, whereas previous work in this direction often use audios generated in a controlled environment with the desired scale. The current work demonstrates the approach for 8 ragas namely Darbari, Khamaj, Malhar, Sohini, Bahar, Basant, Bhairavi and Yaman and we have achieved an average identification accuracy of 94.28% through the framework.",IND,education,Developing economies,"[3.0273612, -1.3921351]","[7.7007227, -16.84744]","[5.7122526, 1.343359, -21.087496]","[14.851246, -8.073998, -7.3215456]","[11.260155, 10.594022]","[7.5269604, 1.348864]","[11.557462, 15.121301, -1.6757891]","[9.16399, 7.149558, 12.513707]"
4,Parul Agarwal;Harish Karnick;Bhiksha Raj,A Comparative Study Of Indian And Western Music Forms.,2013,https://doi.org/10.5281/zenodo.1416882,"Parul Agarwal+Indian Institute of Technology, Kanpur>IND>education|Unknown>Unknown>Unknown;Harish Karnick+Indian Institute of Technology, Kanpur>IND>education|Unknown>Unknown>Unknown;Bhiksha Raj+Carnegie Mellon University>USA>education","Music in India has very ancient roots. Indian classical music is considered to be one of the oldest musical traditions in the world but compared to Western music very little work has been done in the areas of genre recognition, classification, automatic tagging, comparative studies etc. In this work, we investigate the structural differences between Indian and Western music forms and compare the two forms of music in terms of harmony, rhythm, microtones, timbre and other spectral features. To capture the temporal and static structure of the spectrogram, we form a set of global and local frame-wise features for 5 genres of each music form. We then apply Adaboost classification and GMM based Hidden Markov Models for four types of feature sets and observe that Indian Music performs better as compared to Western Music. We have achieved a best accuracy of 98.0% and 77.5% for Indian and Western musical genres respectively. Our comparative analysis indicates that features that work well with one form of music may not necessarily perform well with the other form. The results obtained on Indian Music Genres are better than the previous state-of-the-art.",IND,education,Developing economies,"[5.989689, 3.1943638]","[7.354643, -15.627621]","[7.5149107, 2.0350432, -8.057289]","[13.742762, -9.72085, -8.130283]","[11.511115, 10.261994]","[7.6229224, 1.3393955]","[12.025095, 15.430433, -1.1997782]","[9.247263, 7.1334963, 12.509748]"
3,Erik M. Schmidt;Youngmoo Kim,Learning Rhythm And Melody Features With Deep Belief Networks.,2013,https://doi.org/10.5281/zenodo.1417185,Erik M. Schmidt+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"Deep learning techniques provide powerful methods for the development of deep structured projections connecting multiple domains of data. But the fine-tuning of such networks for supervised problems is challenging, and many current approaches are therefore heavily reliant on pre-training, which consists of unsupervised processing on the input observation data. In previous work, we have investigated using magnitude spectra as the network observations, finding reasonable improvements over standard acoustic representations. However, in necessarily supervised problems such as music emotion recognition, there is no guarantee that the starting points for optimization are anywhere near optimal, as emotion is unlikely to be the most dominant aspect of the data. In this new work, we develop input representations using harmonic/percussive source separation designed to inform rhythm and melodic contour. These representations are beat synchronous, providing an event-driven representation, and potentially the ability to learn emotion informative representations from pre-training alone. In order to provide a large dataset for our pre-training experiments, we select a subset of 50,000 songs from the Million Song Dataset, and employ their 30-60 second preview clips from 7digital to compute our custom feature representations.",USA,education,Developed economies,"[-11.197624, -10.27523]","[-33.258007, -33.98629]","[9.341117, 8.28804, 9.299633]","[-9.134479, -4.428132, -21.438225]","[11.453042, 9.437333]","[7.704905, 5.611853]","[13.352393, 13.269838, 0.7179904]","[10.227913, 7.032505, 8.696176]"
2,Srikanth Cherla;Tillman Weyde;Artur S. d'Avila Garcez;Marcus Pearce,A Distributed Model For Multiple-Viewpoint Melodic Prediction.,2013,https://doi.org/10.5281/zenodo.1415682,Srikanth Cherla+City University London>GBR>education|Queen Mary University of London>GBR>education;Tillman Weyde+City University London>GBR>education|Queen Mary University of London>GBR>education;Artur d’Avila Garcez+City University London>GBR>education|Queen Mary University of London>GBR>education;Marcus Pearce+Queen Mary University of London>GBR>education,"The analysis of sequences is important for extracting information from music owing to its fundamentally temporal nature. In this paper, we present a distributed model based on the Restricted Boltzmann Machine (RBM) for melodic sequences. The model is similar to a previous successful neural network model for natural language. It is first trained to predict the next pitch in a given pitch sequence, and then extended to also make use of information in sequences of note-durations in monophonic melodies on the same task. In doing so, we also propose an efficient way of representing this additional information that takes advantage of the RBM’s structure. In our evaluation, this RBM-based prediction model performs slightly better than previously evaluated n-gram models in most cases. Results on a corpus of chorale and folk melodies showed that it is able to make use of information present in longer contexts more effectively than n-gram models, while scaling linearly in the number of free parameters required.",GBR,education,Developed economies,"[14.91758, -4.927311]","[-6.844775, -31.799097]","[7.5447974, 4.1245637, 5.0260477]","[-19.556343, -0.60298365, -4.5276747]","[10.777144, 9.302288]","[8.748281, 5.6201625]","[11.927624, 14.881576, -0.5954934]","[9.445072, 5.9604673, 9.41161]"
1,Philippe Hamel;Matthew E. P. Davies;Kazuyoshi Yoshii;Masataka Goto,Transfer Learning In Mir: Sharing Learned Latent Representations For Music Audio Classification And Similarity.,2013,https://doi.org/10.5281/zenodo.1416108,Philippe Hamel+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Matthew E. P. Davies+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper discusses the concept of transfer learning and its potential applications to MIR tasks such as music audio classification and similarity. In a traditional supervised machine learning setting, a system can only use labeled data from a single dataset to solve a given task. The labels associated with the dataset define the nature of the task to solve. A key advantage of transfer learning is in leveraging knowledge from related tasks to improve performance on a given target task. One way to transfer knowledge is to learn a shared latent representation across related tasks. This method has shown to be beneficial in many domains of machine learning, but has yet to be explored in MIR. Many MIR datasets for audio classification present a semantic overlap in their labels. Furthermore, these datasets often contain relatively few songs. Thus, there is a strong case for exploring methods to share knowledge between these datasets towards a more general and robust understanding of high level musical concepts such as genre and similarity. Our results show that shared representations can improve classification accuracy. We also show how transfer learning can improve performance for music similarity.",JPN,facility,Developed economies,"[-16.705606, -10.954062]","[-19.663376, -35.472927]","[3.4124076, 4.397596, 11.56925]","[-8.961703, 1.1484495, -16.204062]","[11.336503, 9.358593]","[9.610982, 4.6551647]","[13.229641, 13.022903, 0.90095156]","[10.76016, 6.43303, 9.192805]"
10,Tal Ben Yakar;Roee Litman;Pablo Sprechmann;Alexander M. Bronstein;Guillermo Sapiro,Bilevel Sparse Models for Polyphonic Music Transcription.,2013,https://doi.org/10.5281/zenodo.1415914,Tal Ben Yakar+Tel Aviv University>ISR>education;Roee Litman+Tel Aviv University>ISR>education;Pablo Sprechmann+Duke University>USA>education;Alex Bronstein+Tel Aviv University>ISR>education;Guillermo Sapiro+Duke University>USA>education,"In this work, we propose a trainable sparse model for automatic polyphonic music transcription, which incorporates several successful approaches into a unified optimization framework. Our model combines unsupervised synthesis models similar to latent component analysis and nonnegative factorization with metric learning techniques that allow supervised discriminative learning. We develop efficient stochastic gradient training schemes allowing unsupervised, semi-, and fully supervised training of the model as well its adaptation to test data. We show efficient fixed complexity and latency approximation that can replace iterative minimization algorithms in time-critical applications. Experimental evaluation on synthetic and real data shows promising initial results.",ISR,education,Developing economies,"[29.846664, -9.836324]","[-49.912876, -25.492783]","[9.660591, -6.2355213, 9.961056]","[-8.218972, -16.214731, -27.686743]","[9.471302, 7.8946943]","[6.1581697, 4.8246374]","[11.851541, 12.276984, 0.20531331]","[9.475309, 8.721309, 9.815887]"
22,Sally Jo Cunningham;Jin Ha Lee,Influences of ISMIR and MIREX Research on Technology Patents.,2013,https://doi.org/10.5281/zenodo.1417303,Sally Jo Cunningham+University of Waikato>NZL>education;Jin Ha Lee+University of Washington>USA>education,"Much of the current Music Information Retrieval (MIR) research aims to contribute to the field by creating practical music applications or algorithms that can be used as part of such applications. Understanding how academic research results influence and translate to commercial products can be useful for MIR researchers, especially when we try to measure the impact of our research. This study aims to improve our understanding of the commercial influence of academic MIR research by analyzing the patents citing publications from ISMIR (International Society for Music Information Retrieval) Conference proceedings and its associated MIREX (Music Information Retrieval Evaluation eXchange) MIR algorithm trials. In this paper, we provide our preliminary analyses of the relevant patents as well as the ISMIR publications that are referenced in those patents.",NZL,education,Developed economies,"[-7.399515, 53.8589]","[27.71594, 34.56865]","[-31.677752, -4.9552917, -4.123086]","[2.5606213, 6.601121, 18.826664]","[13.749605, 5.124252]","[11.887852, 0.5058178]","[15.145116, 11.442656, -1.6121614]","[12.251907, 4.308004, 12.21324]"
23,Matthew Prockup;Erik M. Schmidt;Jeffrey J. Scott;Youngmoo E. Kim,Toward Understanding Expressive Percussion Through Content Based Analysis.,2013,https://doi.org/10.5281/zenodo.1414726,Matthew Prockup+Drexel University>USA>education;Erik M. Schmidt+Drexel University>USA>education;Jeffrey Scott+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"Musical expression is the creative nuance through which a musician conveys emotion and connects with a listener. In un-pitched percussion instruments, these nuances are a very important component of performance. In this work, we present a system that seeks to classify different expressive articulation techniques independent of percussion instrument. One use of this system is to enhance the organization of large percussion sample libraries, which can be cumbersome and daunting to navigate. This work is also a necessary first step towards understanding musical expression as it relates to percussion performance. The ability to classify expressive techniques can lead to the development of models that learn the functionality of articulations in patterns, as well as how certain performers use them to communicate their ideas and define their musical style. Additionally, in working towards understanding expressive percussion, we introduce a publicly available dataset of articulations recorded from a standard four piece drum kit that captures the instrument’s expressive range.",USA,education,Developed economies,"[21.62907, -45.711643]","[-17.253036, 4.496773]","[16.616411, -17.586504, -3.2856386]","[5.01625, 12.827941, -13.843157]","[7.999997, 7.028355]","[8.15948, 3.9047372]","[10.413624, 11.919467, 0.8012234]","[9.347658, 6.8637695, 10.40474]"
24,Eric J. Humphrey;Oriol Nieto;Juan Pablo Bello,Data Driven and Discriminative Projections for Large-Scale Cover Song Identification.,2013,https://doi.org/10.5281/zenodo.1416548,Eric J. Humphrey+New York University>USA>education;Oriol Nieto+New York University>USA>education;Juan P. Bello+New York University>USA>education,"The predominant approach to computing document similarity in web scale applications proceeds by encoding task-specific invariance in a vectorized representation, such that the relationship between items can be computed efficiently by a simple scoring function, e.g. Euclidean distance. Here, we improve upon previous work in large-scale cover song identification by using data-driven projections at different time-scales to capture local features and embed summary vectors into a semantically organized space. We achieve this by projecting 2D-Fourier Magnitude Coefficients (2D-FMCs) of beat-chroma patches into a sparse, high dimensional representation which, due to the shift invariance properties of the Fourier Transform, is similar in principle to convolutional sparse coding. After aggregating these local beat-chroma projections, we apply supervised dimensionality reduction to recover an embedding where distance is useful for cover song retrieval. Evaluating on the Million Song Dataset, we find our method outperforms the current state of the art overall, but significantly so for top-k metrics, which indicate improved usability.",USA,education,Developed economies,"[7.917511, 43.433247]","[27.939342, -10.299965]","[3.574211, 11.241029, -23.124857]","[19.32853, -1.3853042, 1.9842]","[16.095053, 11.112506]","[10.394558, 2.75082]","[12.895171, 17.304333, -0.3758547]","[11.845958, 6.768618, 11.751934]"
90,Nicolas Gonzalez Thomas;Philippe Pasquier;Arne Eigenfeldt;James B. Maxwell,A Methodology for the Comparison of Melodic Generation Models Using Meta-Melo.,2013,https://doi.org/10.5281/zenodo.1416092,Nicolas Gonzalez Thomas+Simon Fraser University>CAN>education;Philippe Pasquier+Simon Fraser University>CAN>education;Arne Eigenfeldt+Simon Fraser University>CAN>education;James B. Maxwell+Simon Fraser University>CAN>education,"We investigate Musical Metacreation algorithms by applying Music Information Retrieval techniques for comparing the output of three off-line, corpus-based style imitation models. The first is Variable Order Markov Chains, a statistical model; second is the Factor Oracle, a pattern matcher; and third, MusiCOG, a novel graphical model based on perceptual and cognitive processes. Our focus is on discovering which musical biases are introduced by the models, that is, the characteristics of the output which are shaped directly by the formalism of the models and not by the corpus itself. We describe META-MELO, a system that implements the three models, along with a methodology for the quantitative analysis of model output, when trained on a corpus of melodies in symbolic form. Results show that the models’ output are indeed different and suggest that the cognitive approach is more successful at the tasks, although none of them encompass the full creative space of the corpus. We conclude that this methodology is promising for aiding in the informed application and development of generative models for music composition problems.",CAN,education,Developed economies,"[2.2501006, 18.079021]","[-13.100459, 23.23219]","[2.4669826, 6.731761, 3.8457398]","[-8.35319, -0.0042642686, 5.8945365]","[12.177742, 9.813536]","[8.783285, 2.1562934]","[12.688144, 15.5189905, -0.7946955]","[10.075887, 6.247207, 11.565305]"
92,Matthew E. P. Davies;Philippe Hamel;Kazuyoshi Yoshii;Masataka Goto,AutoMashUpper: An Automatic Multi-Song Mashup System.,2013,https://doi.org/10.5281/zenodo.1416050,Matthew E. P. Davies+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Philippe Hamel+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper describes AutoMashUpper, an interactive system for creating music mashups by automatically selecting and mixing multiple songs together. Given a user-specified input song, the system first identifies the phrase-level structure and then estimates the “mashability” between each phrase section of the input and songs in the user’s music collection. Mashability is calculated based on the harmonic similarity between beat synchronous chromagrams over a user-definable range of allowable key shifts and tempi. Once a match in the collection for a given section of the input song has been found, a pitch-shifting and time-stretching algorithm is used to harmonically and temporally align the sections, after which the loudness of the transformed section is modified to ensure a balanced mix. AutoMashUpper has a user interface to allow visualisation and manipulation of mashups. When creating a mashup, users can specify a list of songs to choose from, modify the mashability parameters and change the granularity of the phrase segmentation. Once created, users can also switch, add, or remove sections from the mashup to suit their taste. In this way, AutoMashUpper can assist users to actively create new music content by enabling and encouraging them to explore the mashup space.",JPN,facility,Developed economies,"[-43.9319, -21.003773]","[-7.9417095, 25.18556]","[3.2714484, 32.83965, -6.9567213]","[-15.266535, 5.3687983, 10.695706]","[11.320256, 5.9637246]","[10.671054, 1.8337734]","[12.39471, 12.526044, -2.1723707]","[10.127515, 5.5694284, 10.361751]"
93,Jan Schlüter,Learning Binary Codes For Efficient Large-Scale Music Similarity Search.,2013,https://doi.org/10.5281/zenodo.1416508,Jan Schlüter+Austrian Research Institute for Artificial Intelligence>AUT>facility,"Content-based music similarity estimation provides a way to find songs in the unpopular “long tail” of commercial catalogs. However, state-of-the-art music similarity measures are too slow to apply to large databases, as they are based on finding nearest neighbors among very high-dimensional or non-vector song representations that are difficult to index. In this work, we adopt recent machine learning methods to map such song representations to binary codes. A linear scan over the codes quickly finds a small set of likely neighbors for a query to be refined with the original expensive similarity measure. Although search costs grow linearly with the collection size, we show that for commercial-scale databases and two state-of-the-art similarity measures, this outperforms five previous attempts at approximate nearest neighbor search. When required to return 90% of true nearest neighbors, our method is expected to answer 4.2 1-NN queries or 1.3 50-NN queries per second on a collection of 30 million songs using a single CPU core; an up to 260 fold speedup over a full scan of 90% of the database.",AUT,facility,Developed economies,"[-8.684254, 16.310377]","[22.658594, 13.670764]","[-0.58805794, 8.260788, -8.973173]","[14.1616335, -5.8237534, 6.6985765]","[13.238496, 9.033701]","[10.370103, 2.0650594]","[13.559051, 15.081474, -1.0293567]","[11.862945, 6.343097, 12.641891]"
94,Thomas Prätzlich;Meinard Müller,Freischütz Digital: A Case Study for Reference-Based Audio Segmentation for Operas.,2013,https://doi.org/10.5281/zenodo.1416814,Thomas Prätzlich+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Music information retrieval has started to become more and more important in the humanities by providing tools for computer-assisted processing and analysis of music data. However, when applied to real-world scenarios, even established techniques, which are often developed and tested under lab conditions, reach their limits. In this paper, we illustrate some of these challenges by presenting a study on automated audio segmentation in the context of the interdisciplinary project “Freischütz Digital”. One basic task arising in this project is to automatically segment different recordings of the opera “Der Freischütz” according to a reference segmentation specified by a domain expert (musicologist). As it turns out, the task is more complex as one may think at first glance due to significant acoustic and structural variations across the various recordings. As our main contribution, we reveal and discuss these variations by systematically adapting segmentation procedures based on synchronization and matching techniques.",DEU,facility,Developed economies,"[-1.7229892, -8.08768]","[-0.7734808, -3.4421453]","[8.3744755, -3.4459038, -8.857544]","[-2.5362523, -4.876059, -3.3832252]","[11.57948, 8.340659]","[8.15858, 2.8424435]","[12.490472, 14.225597, 0.24549823]","[10.240738, 7.3017306, 11.28354]"
95,Nanzhu Jiang;Meinard Müller,Automated Methods for Analyzing Music Recordings in Sonata Form.,2013,https://doi.org/10.5281/zenodo.1418283,Nanzhu Jiang+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"The sonata form has been one of the most important large-scale musical structures used since the early Classical period. Typically, the first movements of symphonies and sonatas follow the sonata form, which (in its most basic form) starts with an exposition and a repetition thereof, continues with a development, and closes with a recapitulation. The recapitulation can be regarded as an altered repeat of the exposition, where certain substructures (first and second subject groups) appear in musically modified forms. In this paper, we introduce automated methods for analyzing music recordings in sonata form, where we proceed in two steps. In the first step, we derive the coarse structure by exploiting that the recapitulation is a kind of repetition of the exposition. This requires audio structure analysis tools that are invariant under local modulations. In the second step, we identify finer substructures by capturing relative modulations between the subject groups in exposition and recapitulation. We evaluate and discuss our results by means of the Beethoven piano sonatas. In particular, we introduce a novel visualization that not only indicates the benefits and limitations of our methods, but also yields some interesting musical insights into the data.",DEU,facility,Developed economies,"[14.018239, 5.91957]","[-15.741417, 15.278921]","[4.351043, -5.4039927, 8.259498]","[-12.26716, -1.9573588, 3.200382]","[10.830334, 7.6851025]","[8.153724, 2.2153542]","[12.430741, 12.69102, -0.47204223]","[9.947112, 6.85212, 11.856709]"
96,Johan Pauwels;Florian Kaiser;Geoffroy Peeters,Combining Harmony-Based and Novelty-Based Approaches for Structural Segmentation.,2013,https://doi.org/10.5281/zenodo.1416104,Johan Pauwels+STMS IRCAM-CNRS-UPMC>FRA>Unknown;Florian Kaiser+STMS IRCAM-CNRS-UPMC>FRA>Unknown;Geoffroy Peeters+STMS IRCAM-CNRS-UPMC>FRA>Unknown,"This paper describes a novel way to combine a well-proven method of structural segmentation through novelty detection with a recently introduced method based on harmonic analysis. The former system works by looking for peaks in novelty curves derived from self-similarity matrices. The latter relies on the detection of key changes and on the differences in prior probability of chord transitions according to their position in a structural segment. Both approaches are integrated into a probabilistic system that jointly estimates keys, chords and structural boundaries. The novelty curves are herein used as observations. In addition, chroma profiles are used as features for the harmony analysis. These observations are then subjected to a constrained transition model that is musically motivated. An information theoretic justification of this model is also given. Finally, an evaluation of the resulting system is performed. It is shown that the combined system improves the results of both constituting components in isolation.",FRA,Unknown,Developed economies,"[-1.4935473, -4.1370096]","[-3.5724, 2.203168]","[5.400368, -7.504438, -5.08656]","[-2.5615535, -0.8982599, -5.521317]","[11.50811, 8.307806]","[7.761036, 2.9816542]","[12.388082, 14.112057, 0.17399837]","[10.150164, 7.925119, 11.63661]"
97,Maarten Grachten;Martin Gasser;Andreas Arzt;Gerhard Widmer,Automatic Alignment of Music Performances with Structural Differences.,2013,https://doi.org/10.5281/zenodo.1416628,Maarten Grachten+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Andreas Arzt+Johannes Kepler Universität>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"Both in interactive music listening, and in music performance research, there is a need for automatic alignment of different recordings of the same musical piece. This task is challenging, because musical pieces often contain parts that may or may not be repeated by the performer, possibly leading to structural differences between performances (or between performance and score). The most common alignment method, dynamic time warping (DTW), cannot handle structural differences adequately, and existing approaches to deal with structural differences explicitly rely on the annotation of “break points” in one of the sequences. We propose a simple extension of the Needleman-Wunsch algorithm to deal effectively with structural differences, without relying on annotations. We evaluate several audio features for alignment, and show how an optimal value can be found for the cost-parameter of the alignment algorithm. A single cost value is demonstrated to be valid across different types of music. We demonstrate that our approach yields roughly equal alignment accuracies compared to DTW in the absence of structural differences, and superior accuracies when structural differences occur.",AUT,facility,Developed economies,"[16.12241, -12.267477]","[-17.284147, -15.005597]","[-1.680314, -10.713455, -6.1604123]","[-0.021487417, -22.233702, -4.203013]","[11.023388, 6.5434775]","[6.1580896, 0.76209384]","[12.042962, 12.729694, -1.5625736]","[8.153472, 5.930045, 10.861316]"
39,John Ashley Burgoyne;Dimitrios Bountouridis;Jan Van Balen;Henkjan Honing,Hooked: A Game For Discovering What Makes Music Catchy.,2013,https://doi.org/10.5281/zenodo.1417599,John Ashley Burgoyne+University of Amsterdam>NLD>education;Dimitrios Bountouridis+Utrecht University>NLD>education;Jan Van Balen+Utrecht University>NLD>education;Henkjan Honing+University of Amsterdam>NLD>education,"Although there has been some empirical research on earworms, songs that become caught and replayed in one’s memory over and over again, there has been surprisingly little empirical research on the more general concept of the musical hook, the most salient moment in a piece of music, or the even more general concept of what may make music ‘catchy’. Almost by definition, people like catchy music, and thus this question is a natural candidate for approaching with ‘gamification’. We present the design of Hooked, a game we are using to study musical catchiness, as well as the theories underlying its design and the results of a pilot study we undertook to check its scientific validity. We found significant differences in time to recall pieces of music across different segments, identified parameters for making recall tasks more or less challenging, and found that players are not as reliable as one might expect at predicting their own recall performance.",NLD,education,Developed economies,"[-32.882893, 12.793033]","[42.20117, 25.440138]","[-15.675375, 10.202312, -0.34616226]","[3.758769, 16.446873, 13.368184]","[14.372133, 9.0081215]","[12.572521, 1.2375621]","[15.081849, 13.957606, -1.0308696]","[13.036948, 4.509216, 11.427196]"
38,Sai Sumanth Miryala;Kalika Bali;Ranjita Bhagwan;Monojit Choudhury,Automatically Identifying Vocal Expressions for Music Transcription.,2013,https://doi.org/10.5281/zenodo.1416624,Sai Sumanth Miryala+Microsoft Research India>IND>company;Kalika Bali+Microsoft Research India>IND>company;Ranjita Bhagwan+Microsoft Research India>IND>company;Monojit Choudhury+Microsoft Research India>IND>company,"Music transcription has many uses ranging from music information retrieval to better education tools. An important component of automated transcription is the identification and labeling of different kinds of vocal expressions such as vibrato, glides, and riffs. In Indian Classical Music such expressions are particularly important since a raga is often established and identified by the correct use of these expressions. It is not only important to classify what the expression is, but also when it starts and ends in a vocal rendition. Some examples of such expressions that are key to Indian music are Meend (vocal glides) and Andolan (very slow vibrato). In this paper, we present an algorithm for the automatic transcription and expression identification of vocal renditions with specific application to North Indian Classical Music. Using expert human annotation as the ground truth, we evaluate this algorithm and compare it with two machine-learning approaches. Our results show that we correctly identify the expressions and transcribe vocal music with 85% accuracy. As a part of this effort, we have created a corpus of 35 voice recordings, of which 12 recordings are annotated by experts. The corpus is available for download.",IND,company,Developing economies,"[-8.223395, -33.019894]","[7.200274, -17.447489]","[16.186872, 12.048902, -12.539791]","[15.031217, -8.555564, -6.0473666]","[10.282623, 10.943351]","[7.5104613, 1.3308551]","[11.499992, 15.031155, 0.5467119]","[9.110459, 7.0910015, 12.508404]"
37,Maksim Khadkevich;Maurizio Omologo,Large-Scale Cover Song Identification Using Chord Profiles.,2013,https://doi.org/10.5281/zenodo.1415844,Maksim Khadkevich+Fondazione Bruno Kessler-irst>ITA>facility;Maurizio Omologo+Fondazione Bruno Kessler-irst>ITA>facility,"This paper focuses on cover song identification among datasets potentially containing millions of songs. A compact representation of music contents plays an important role in large-scale analysis and retrieval. The proposed approach is based on high-level summarization of musical songs using chord profiles. Search is performed in two steps. In the first step, the Locality Sensitive Hashing (LHS) method is used to retrieve songs with similar chord profiles. On the resulting list of songs a second processing step is applied to progressively refine the ranking. Experiments conducted on both the Million Song Dataset (MSD) and a subset of the Second Hand Songs (SHS) dataset showed the effectiveness of the proposed solution, which provides state-of-the-art results.",ITA,facility,Developed economies,"[6.8292103, 43.097225]","[22.760653, 12.000923]","[2.33167, 13.572067, -21.094524]","[16.458544, -2.9350727, 4.037514]","[16.037924, 11.095584]","[10.314939, 2.4044359]","[12.88671, 17.288734, -0.37277013]","[11.768048, 6.4457088, 12.115918]"
36,Florian Krebs;Sebastian Böck;Gerhard Widmer,Rhythmic Pattern Modeling for Beat and Downbeat Tracking in Musical Audio.,2013,https://doi.org/10.5281/zenodo.1416392,Florian Krebs+Johannes Kepler University>AUT>education;Sebastian Böck+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"Rhythmic patterns are an important structural element in music. This paper investigates the use of rhythmic pattern modeling to infer metrical structure in musical audio recordings. We present a Hidden Markov Model (HMM) based system that simultaneously extracts beats, downbeats, tempo, meter, and rhythmic patterns. Our model builds upon the basic structure proposed by Whiteley et. al [20], which we further modified by introducing a new observation model: rhythmic patterns are learned directly from data, which makes the model adaptable to the rhythmical structure of any kind of music. For learning rhythmic patterns and evaluating beat and downbeat tracking, 697 ballroom dance pieces were annotated with beat and measure information. The results showed that explicitly modeling rhythmic patterns of dance styles drastically reduces octave errors (detection of half or double tempo) and substantially improves downbeat tracking.",AUT,education,Developed economies,"[33.974598, -36.738026]","[-27.092155, -9.488887]","[11.500999, -26.972536, -7.3412557]","[-5.1913843, 17.573633, -11.311397]","[10.445632, 4.4073095]","[5.6703124, 2.1319196]","[10.331241, 12.941534, -2.174471]","[7.947855, 6.911046, 11.0853615]"
35,Yi Lin;Xiaoou Chen;Deshun Yang,Exploration of Music Emotion Recognition Based on MIDI.,2013,https://doi.org/10.5281/zenodo.1416604,Yi Lin+Peking University>CHN>education;Xiaoou Chen+Peking University>CHN>education;Deshun Yang+Peking University>CHN>education,"Audio and lyric features are commonly considered in the research of music emotion recognition, whereas MIDI features are rarely used. Some research revealed that among the features employed in music emotion recognition, lyric has the best performance on valence, MIDI takes the second place, and audio is the worst. However, lyric cannot be found in some music types, such as instrumental music. In this case, MIDI features can be considered as a choice for music emotion recognition on valence dimension. In this presented work, we systematically explored the effect and value of using MIDI features for music emotion recognition. Emotion recognition was treated as a regression problem in this paper. We also discussed the emotion regression performance of three aspects of music in terms of edited MIDI: chorus, melody, and accompaniment. We found that the MIDI features performed better than audio features on valence. And under the realistic conditions, converted MIDI performed better than edited MIDI on valence. We found that melody was more important to valence regression than accompaniment, which was in contrary to arousal. We also found that the chorus part of an edited MIDI might contain as sufficient information as the entire edited MIDI for valence regression.",CHN,education,Developing economies,"[-60.15725, -0.5830519]","[51.322975, -10.533014]","[-29.277113, 20.378956, 5.3597374]","[8.9002905, 21.22169, 4.848028]","[14.067793, 12.957671]","[12.967219, 4.2684827]","[16.057117, 14.371468, 1.8316905]","[14.138254, 4.9731174, 10.2307825]"
33,Harald Grohganz;Michael Clausen;Nanzhu Jiang;Meinard Müller,Converting Path Structures Into Block Structures Using Eigenvalue Decompositions of Self-Similarity Matrices.,2013,https://doi.org/10.5281/zenodo.1417813,Harald Grohganz+Bonn University>DEU>education;Michael Clausen+Bonn University>DEU>education;Nanzhu Jiang+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"In music structure analysis the two principles of repetition and homogeneity are fundamental for partitioning a given audio recording into musically meaningful structural elements. When converting the audio recording into a suitable self-similarity matrix (SSM), repetitions typically lead to path structures, whereas homogeneous regions yield block structures. In previous research, handling both structural elements at the same time has turned out to be a challenging task. In this paper, we introduce a novel procedure for converting path structures into block structures by applying an eigenvalue decomposition of the SSM in combination with suitable clustering techniques. We demonstrate the effectiveness of our conversion approach by showing that algorithms previously designed for homogeneity-based structure analysis can now be applied for repetition-based structure analysis. Thus, our conversion may open up novel ways for handling both principles within a unified structure analysis framework.",DEU,education,Developed economies,"[13.033582, 31.058107]","[3.034585, 1.6139932]","[-20.883812, -20.543417, 0.9766267]","[0.25126562, 3.7952683, -5.8419003]","[12.409861, 8.8274355]","[7.746141, 3.154565]","[13.063782, 14.380298, -0.4643326]","[10.629054, 8.025258, 11.495688]"
32,Pasi Saari;Tuomas Eerola;György Fazekas;Mathieu Barthet;Olivier Lartillot;Mark B. Sandler,The Role of Audio and Tags in Music Mood Prediction: A Study Using Semantic Layer Projection.,2013,https://doi.org/10.5281/zenodo.1418049,"Pasi Saari+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Tuomas Eerola+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education;Mathieu Barthet+Centre for Digital Music, Queen Mary University of London>GBR>education;Olivier Lartillot+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Mark Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","Semantic Layer Projection (SLP) is a method for automatically annotating music tracks according to expressed mood based on audio. We evaluate this method by comparing it to a system that infers the mood of a given track using associated tags only. SLP differs from conventional auto-tagging algorithms in that it maps audio features to a low-dimensional semantic layer congruent with the circumplex model of emotion, rather than training a model for each tag separately. We build the semantic layer using two large-scale data sets – crowd-sourced tags from Last.fm, and editorial annotations from the I Like Music (ILM) production music corpus – and use subsets of these corpora to train SLP for mapping audio features to the semantic layer. The performance of the system is assessed in predicting mood ratings on continuous scales in the two data sets mentioned above. The results show that audio is in general more efficient in predicting perceived mood than tags. Furthermore, we analytically demonstrate the benefit of using a combination of semantic tags and audio features in automatic mood annotation.",FIN,education,Developed economies,"[-51.021744, 3.134331]","[48.726234, -4.7283063]","[-20.441631, 20.751558, 3.4786298]","[15.218215, 21.103134, 4.8976617]","[13.636651, 12.5626545]","[12.817306, 3.8440428]","[16.088646, 14.8559885, 1.3838205]","[13.951442, 5.3508115, 10.6369295]"
31,Tom Arjannikov;Chris Sanden;John Z. Zhang,Verifying Music Tag Annotation Via Association Analysis.,2013,https://doi.org/10.5281/zenodo.1417123,Tom Arjannikov+University of Lethbridge>CAN>education;Chris Sanden+University of Lethbridge>CAN>education;John Z. Zhang+University of Lethbridge>CAN>education,"Music tags provide descriptive and rich information about a music piece, including its genre, artist, emotion, instrument, etc. While many work on automating it, at present, tag annotation is largely a manual process. It often involves judgements and opinions from people of different background and level of musical expertise. Therefore, the resulting tags are usually subjective, ambiguous, and error-prone. To deal with this situation, we seek automatic methods to verify and monitor this process. Furthermore, because multiple tags can annotate each music piece, our task lends itself to multi-label methods which capture the inherent associations among annotations in a given music repository. In this paper, we propose a novel approach to verify the quality of music tag annotations via association analysis. We demonstrate the effectiveness of our approach through a series of simulations using four publicly available music datasets. To our knowledge, our work is among the initial efforts in verifying music tag annotations.",CAN,education,Developed economies,"[-38.968536, -0.5174003]","[37.702156, 1.2388499]","[-16.084438, 11.955107, 9.414789]","[23.17839, 10.2669935, 4.7067637]","[14.296407, 10.281536]","[11.494294, 3.2796426]","[15.382759, 14.140691, -0.0053452468]","[12.939539, 6.144429, 11.313258]"
30,David Hauger;Markus Schedl;Andrej Kosir;Marko Tkalcic,The Million Musical Tweet Dataset - What We Can Learn From Microblogs.,2013,https://doi.org/10.5281/zenodo.1417649,David Hauger+University Linz>AUT>education;Markus Schedl+University Linz>AUT>education;Andrej Košir+University of Ljubljana>SVN>education;Marko Tkalčič+University Linz>AUT>education,"Microblogs and Social Media applications are continuously growing in spread and importance. Users of Twitter, the currently most popular platform for microblogging, create more than a billion posts (called tweets) every week. Among all the different types of information being shared, some people post their music listening behavior, which is why Twitter became interesting for the Music Information Retrieval (MIR) community. Depending on the device and personal settings, some users provide geographic coordinates for their microposts. Having continuously crawled and analyzed tweets for more than 500 days (17 months) we can now present the “Million Musical Tweet Dataset” (MMTD) – the biggest publicly available source of microblog-based music listening histories that includes geographic, temporal, and other contextual information. These extended information makes the MMTD outstanding from other datasets providing music listening histories. We introduce the dataset, give basic statistics about its composition, and show how this dataset allows to detect new contextual music listening patterns by performing a comprehensive statistical investigation with respect to correlation between music taste and day of the week, hour of day, and country.",AUT,education,Developed economies,"[-45.59402, 10.802538]","[49.994263, 13.413177]","[-28.579178, 10.134381, -4.925286]","[20.43023, 13.653045, 13.2052355]","[14.280542, 8.78776]","[12.443158, 2.346266]","[14.843909, 14.094974, -1.0685576]","[13.40313, 5.3850017, 12.180521]"
29,Dimitrios Bountouridis;Remco C. Veltkamp;Jan Van Balen,Placing Music Artists and Songs in Time Using Editorial Metadata and Web Mining Techniques.,2013,https://doi.org/10.5281/zenodo.1414948,Dimitrios Bountouridis+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education;Jan Van Balen+Utrecht University>NLD>education,"This paper investigates the novel task of situating music artists and songs in time, thereby adding contextual information that typically correlates with an artist’s similarities, collaborations and influences. The proposed method makes use of editorial metadata in conjunction with web mining techniques, aiming to infer an artist’s productivity over time and estimate the original year of release of a song. Experimental evaluation over a set of Dutch and American music confirms the practicality and reliability of the proposed methods. As a consequence, large-scale correlational analyses between artist productivity and other musical characteristics (e.g. versatility, eminence) become possible.",NLD,education,Developed economies,"[-27.657291, 15.203749]","[46.162003, 7.243246]","[-23.392689, 5.125912, -5.8996034]","[13.04964, 11.451428, 7.417269]","[14.273085, 9.074464]","[12.069159, 2.7074995]","[15.046839, 13.847503, -1.1043991]","[13.103386, 5.5359955, 11.725009]"
28,Sertan Sentürk;Sankalp Gulati;Xavier Serra,Score Informed Tonic Identification for Makam Music of Turkey.,2013,https://doi.org/10.5281/zenodo.1416950,"Sertan Şentürk+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sankalp Gulati+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Tonic is a fundamental concept in many music traditions and its automatic identification should be relevant for establishing the reference pitch when we analyse the melodic content of the music. In this paper, we present two methodologies for the identification of the tonic in audio recordings of makam music of Turkey, both taking advantage of some score information. First, we compute a prominent pitch and a audio kernel-density pitch class distribution (KPCD) from the audio recording. The peaks in the KPCD are selected as tonic candidates. The first method computes a score KPCD from the monophonic melody extracted from the score. Then, the audio KPCD is circular-shifted with respect to each tonic candidate and compared with the score KPCD. The best matching shift indicates the estimated tonic. The second method extracts the monophonic melody of the most repetitive section of the score. Normalising the audio prominent pitch with respect to each tonic candidate, the method attempts to link the repetitive structural element given in the score with the respective time-intervals in the audio recording. The result producing the most confident links marks the estimated tonic. We have tested the methods on a dataset of makam music of Turkey, achieving a very high accuracy (94.9%) with the first method, and almost perfect identification (99.6%) with the second method. We conclude that score informed tonic identification can be a useful first step in the computational analysis (e.g. expressive analysis, intonation analysis, audio-score alignment) of music collections involving melody-dominant content.",ESP,education,Developed economies,"[10.285084, 5.726211]","[4.363554, -12.616408]","[-13.483787, -23.688843, -14.895935]","[7.3655357, -10.907245, -5.4290867]","[11.73759, 10.664107]","[7.3265157, 2.0273988]","[12.199443, 14.95745, -1.5416435]","[9.385359, 7.1768637, 11.669613]"
27,Maria Panteli;Hendrik Purwins,A Computational Comparison of Theory And Practice of Scale Intonation in Byzantine Chant.,2013,https://doi.org/10.5281/zenodo.1417040,Maria Panteli+University of Cyprus>CYP>education;Hendrik Purwins+Berlin Institute of Technology>DEU>education|Aalborg University Copenhagen>DNK>education,"Byzantine Chant performance practice is quantitatively compared to the Chrysanthine theory. The intonation of scale degrees is quantified, based on pitch class profiles. An analysis procedure is introduced that consists of the following steps: 1) Pitch class histograms are calculated via non-parametric kernel smoothing. 2) Histogram peaks are detected. 3) Phrase ending analysis aids the finding of the tonic to align histogram peaks. 4) The theoretical scale degrees are mapped to the practical ones. 5) A schema of statistical tests detects significant deviations of theoretical scale tuning from the estimated ones in performance practice. The analysis of 94 echoi shows a tendency of the singer to level theoretic particularities of the echos that stand out of the general norm in the octoechos: theoretically extremely large scale steps are diminished in performance.",CYP,education,Developed economies,"[11.267889, 0.36838633]","[-1.6301682, -19.535717]","[14.795845, -0.24216127, -20.800795]","[10.200216, -15.939798, -13.278534]","[11.047485, 10.176762]","[7.0868587, 1.9286829]","[11.845759, 15.305308, -0.9088672]","[8.952323, 7.5510054, 11.677938]"
26,Anja Volk;W. Bas de Haas,A Corpus-Based Study on Ragtime Syncopation.,2013,https://doi.org/10.5281/zenodo.1415846,Anja Volk+Utrecht University>NLD>education|Utrecht University>Unknown>Unknown;W. Bas de Haas+Utrecht University>NLD>education|Utrecht University>Unknown>Unknown,"This paper presents a corpus-based study on syncopation patterns in ragtime. We discuss open questions on the ragtime genre and the potential of computational tools in addressing these questions, contributing to the fields of Musicology and Music Information Retrieval (MIR), and giving back to the ragtime enthusiasts community. We introduce the RAG-collection of around 11000 ragtime MIDI files collected, organised, and distributed by many ragtime lovers around the world. The collection is accompanied by a compendium, providing useful metadata on ragtime compositions. Using this collection and the compendium, we investigate syncopation patterns in ragtime melodies, for which we tailored a melody extraction algorithm. We test and confirm musicological hypotheses about the occurrence of syncopation patterns that are considered typical for ragtime on the extracted melodies. Thus, the paper presents a first step towards modelling typical characteristics of the ragtime genre, which is an important means for enabling automatic genre classification.",NLD,education,Developed economies,"[46.86616, 11.324185]","[-17.31022, 8.834991]","[-12.8647175, -25.991344, -0.7010748]","[-7.1143117, 13.559248, 2.6083465]","[11.946361, 5.2755857]","[6.7919436, 1.3586478]","[11.250235, 14.3082285, -2.1252937]","[8.848929, 6.578813, 12.194959]"
25,Dekai Wu,"Simultaneous Unsupervised Learning of Flamenco Metrical Structure, Hypermetrical Structure, and Multipart Structural Relations.",2013,https://doi.org/10.5281/zenodo.1414772,Dekai Wu+HKUST>HKG>education,"We show how a new unsupervised approach to learning musical relationships can exploit Bayesian MAP induction of stochastic transduction grammars to overcome the challenges of learning complex relationships between multiple rhythmic parts that previously lay outside the scope of general computational approaches to music structure learning. A good illustrative genre is flamenco, which employs not only regular but also irregular hypermetrical structures that rapidly switch between 3/4 and 6/8 mediocompas blocks. Moreover, typical flamenco idioms employ heavy syncopation and sudden, misleading off-beat accents and patterns, while often elliding the downbeat accents that humans as well as existing meter-finding algorithms rely on, thus creating a high degree of listener “surprise” that makes not only the structural relations, but even the metrical structure itself, elusive to learn. Flamenco musicians rely on both complex regular hypermetrical knowledge as well as irregular real-time clues to recognize when to switch meters and patterns. Our new approach envisions this as an integrated problem of learning a bilingual transduction, i.e., a structural relation between two languages—where there are different musical languages of, say, flamenco percussion versus zapateado footwork or palmas hand clapping. We apply minimum description length criteria to induce transduction grammars that simultaneously learn (1) the multiple metrical structures, (2) the hypermetrical structure that stochastically governs meter switching, and (3) the probabilistic transduction relationship between patterns of different rhythmic languages that enables musicians to predict when to switch meters and how to select patterns depending on what fellow musicians are generating.",HKG,education,Developing economies,"[-0.62026703, -15.07769]","[-27.494282, -7.6086345]","[13.944288, 6.6788507, -24.460096]","[-5.756995, 19.03037, -8.124355]","[11.590979, 9.906407]","[5.704936, 2.1001985]","[12.334777, 15.166134, -0.3436549]","[8.147034, 6.839854, 11.212393]"
40,H. G. Ranjani;T. V. Sreenivas,Hierarchical Classification of Carnatic Music Forms.,2013,https://doi.org/10.5281/zenodo.1415182,Ranjani H. G.+Indian Institute of Science>IND>education|Indian Institute of Science>IND>education;T. V. Sreenivas+Indian Institute of Science>IND>education|Indian Institute of Science>IND>education,"We address the problem of classifying a given piece of Carnatic art music into one of its several forms recognized pedagogically. We propose a hierarchical approach for classification of these forms as different combinations of rhythm, percussion and repetitive syllabic structures. The proposed 3-level hierarchy is based on various signal processing measures and classifiers. Features derived from short term energy contours, along with formant information are used to obtain discriminative features. The statistics of the features are used to design simple classifiers at each level of the hierarchy. The method is validated on a subset of IIT-M Carnatic concert music database, comprising of more than 20 hours of music. Using 10 s audio clips, we get an average f-ratio performance of 0.62 for the classification of the following six types of Carnatic art music: /AlApana/, /viruttam/, /thillAna/, /krithi/, /thani-Avarthanam/ and /thAnam/.",IND,education,Developing economies,"[6.427551, -1.1656603]","[15.862465, -6.0489526]","[8.896904, 0.34095642, -14.65937]","[9.50272, 9.126278, -6.462275]","[11.33789, 10.192861]","[9.171248, 3.184338]","[11.735346, 15.121145, -1.3075534]","[10.843216, 7.298291, 10.925462]"
34,Thomas Wilmering;György Fazekas;Mark B. Sandler,The Audio Effects Ontology.,2013,https://doi.org/10.5281/zenodo.1415004,Thomas Wilmering+Queen Mary University of London>GBR>education;György Fazekas+Queen Mary University of London>GBR>education;Mark B. Sandler+Queen Mary University of London>GBR>education,"In this paper we present the Audio Effects Ontology for the ontological representation of audio effects in music production workflows. Designed as an extension to the Studio Ontology, its aim is to provide a framework for the detailed description and sharing of information about audio effects, their implementations, and how they are applied in real-world production scenarios. The ontology enables capturing and structuring data about the use of audio effects and thus facilitates reproducibility of audio effect application, as well as the detailed analysis of music production practices. Furthermore, the ontology may inform the creation of metadata standards for adaptive audio effects that map high-level semantic descriptors to control parameter values. The ontology is using Semantic Web technologies that enable knowledge representation and sharing, and is based on modular ontology design methodologies. It is evaluated by examining how it fulfills requirements in a number of production and retrieval use cases.",GBR,education,Developed economies,"[-26.854494, 39.501503]","[17.703434, 39.86936]","[-22.363626, -2.8744442, -4.588804]","[-4.025957, -4.4509125, 25.963764]","[14.218029, 8.938258]","[10.840591, -0.17665704]","[15.053561, 13.68894, -1.2807095]","[12.165839, 5.0777407, 11.614008]"
0,Abhishek Singhi;Daniel G. Brown 0001,"On Cultural, Textual and Experiential Aspects of Music Mood.",2014,https://doi.org/10.5281/zenodo.1417391,Abhishek Singhi+University of Waterloo>CAN>education;Daniel G. Brown+University of Waterloo>CAN>education,"We study the impact of the presence of lyrics on music mood perception for both Canadian and Chinese listeners by conducting a user study of Canadians not of Chinese origin, Chinese-Canadians, and Chinese people who have lived in Canada for fewer than three years. While our original hypotheses were largely connected to cultural components of mood perception, we also analyzed how stable mood assignments were when listeners could read the lyrics of recent popular English songs they were hearing versus when they only heard the songs. We also showed the lyrics of some songs to participants without playing the recorded music. We conclude that people assign different moods to the same song in these three scenarios. People tend to assign a song to the mood cluster that includes “melancholy” more often when they read the lyrics without listening to it, and having access to the lyrics does not help reduce the difference in music mood perception between Canadian and Chinese listeners significantly. Our results cause us to question the idea that songs have “inherent mood”. Rather, we suggest that the mood depends on both cultural and experiential context.",CAN,education,Developed economies,"[-52.83301, 5.927663]","[54.82653, -1.7270583]","[-19.336721, 28.899612, 3.4409325]","[12.1170635, 20.727663, 13.528579]","[13.460224, 12.454943]","[13.223079, 3.4399824]","[16.074785, 14.874825, 1.3213654]","[14.18993, 4.9795084, 10.90934]"
46,Chun-Ta Chen;Jyh-Shing Roger Jang;Chun-Hung Lu,Improving Query by Tapping via Tempo Alignment.,2014,https://doi.org/10.5281/zenodo.1416910,Chun-Ta Chen+National Tsing Hua University>TWN>education;Jyh-Shing Roger Jang+National Taiwan University>TWN>education;Chun-Hung Lu+Institute for Information Industry>TWN>company,"Query by tapping (QBT) is a content retrieval method that can retrieve a song by taking the user’s tapping or clapping at the note onsets of the intended song in the database for comparison. This paper proposes a new query-by-tapping algorithm that aligns the IOI (inter-onset interval) vector of the query sequence with songs in the dataset by building an IOI ratio matrix, and then applies a dynamic programming (DP) method to compute the optimum path with minimum cost. Experiments on different datasets indicate that our algorithm outperforms other previous approaches in accuracy (top-10 and MRR), with a speedup factor of 3 in computation. With the advent of personal handheld devices, QBT provides an interesting and innovative way for music retrieval by shaking or tapping the devices, which is also discussed in the paper.",TWN,education,Developing economies,"[43.20211, -21.935944]","[8.158987, 12.320162]","[-1.1433126, -28.58476, -9.445552]","[9.275518, -15.529423, 10.204784]","[11.507873, 4.797965]","[8.874929, 0.48127583]","[11.117227, 13.45544, -2.6582615]","[10.385809, 5.9953814, 13.283908]"
77,Katayoun Farrahi;Markus Schedl;Andreu Vall;David Hauger;Marko Tkalcic,Impact of Listening Behavior on Music Recommendation.,2014,https://doi.org/10.5281/zenodo.1417505,"Katayoun Farrahi+Goldsmiths, University of London>GBR>education;Markus Schedl+Johannes Kepler University>AUT>education;Andreu Vall+Johannes Kepler University>AUT>education;David Hauger+Johannes Kepler University>AUT>education;Marko Tkalčič+Johannes Kepler University>AUT>education","The next generation of music recommendation systems will be increasingly intelligent and likely take into account user behavior for more personalized recommendations. In this work we consider user behavior when making recommendations with features extracted from a user’s history of listening events. We investigate the impact of listener’s behavior by considering features such as play counts, “mainstreaminess”, and diversity in music taste on the performance of various music recommendation approaches. The underlying dataset has been collected by crawling social media (specifically Twitter) for listening events. Each user’s listening behavior is characterized into a three dimensional feature space consisting of play count, “mainstreaminess” (i.e. the degree to which the observed user listens to currently popular artists), and diversity (i.e. the diversity of genres the observed user listens to). Drawing subsets of the 28,000 users in our dataset, according to these three dimensions, we evaluate whether these dimensions influence figures of merit of various music recommendation approaches, in particular, collaborative filtering (CF) and CF enhanced by cultural information such as users located in the same city or country.",GBR,education,Developed economies,"[-41.68354, 23.491776]","[40.7565, 17.4211]","[-14.835755, 21.856169, -7.6044965]","[15.630135, 10.359598, 16.300339]","[15.49265, 9.03661]","[12.740714, 1.8651751]","[15.381235, 15.398276, -1.3670846]","[13.547384, 5.06451, 12.466554]"
76,Po-Sen Huang;Minje Kim;Mark Hasegawa-Johnson;Paris Smaragdis,Singing-Voice Separation from Monaural Recordings using Deep Recurrent Neural Networks.,2014,https://doi.org/10.5281/zenodo.1415678,Po-Sen Huang+University of Illinois at Urbana-Champaign>USA>education;Minje Kim+University of Illinois at Urbana-Champaign>USA>education;Mark Hasegawa-Johnson+University of Illinois at Urbana-Champaign>USA>education;Paris Smaragdis+University of Illinois at Urbana-Champaign>USA>education|Adobe Research>USA>company,"Monaural source separation is important for many real world applications. It is challenging since only single channel information is available. In this paper, we explore using deep recurrent neural networks for singing voice separation from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal connections are explored. We propose jointly optimizing the networks for multiple source signals by including the separation step as a nonlinear operation in the last layer. Different discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30∼2.48 dB GNSDR gain and 4.32∼5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset.",USA,education,Developed economies,"[-0.87569255, -41.978203]","[-38.53689, -34.607338]","[26.105131, 7.374927, -8.275516]","[-13.722181, -8.780017, -24.416973]","[9.060182, 10.658917]","[6.8911343, 5.830861]","[10.904024, 14.505725, 1.3774067]","[9.708591, 8.273066, 8.887524]"
75,Abhishek Singhi;Daniel G. Brown 0001,Are Poetry and Lyrics All That Different?,2014,https://doi.org/10.5281/zenodo.1417004,Mohamed Morchid+LIA - University of Avignon>FRA>education;Richard Dufour+LIA - University of Avignon>FRA>education;Georges Linares+LIA - University of Avignon>FRA>education,"Most of modern advertisements contain a song to illustrate the commercial message. The success of a product, and its economic impact, can be directly linked to this choice. Finding the most appropriate song is usually made manually. Nonetheless, a single person is not able to listen and choose the best music among millions. The need for an automatic system for this particular task becomes increasingly critical. This paper describes the LIA music recommendation system for advertisements using both textual and acoustic features. This system aims at providing a song to a given commercial video and was evaluated in the context of the MediaEval 2013 Soundtrack task [14]. The goal of this task is to predict the most suitable soundtrack from a list of candidate songs, given a TV commercial. The organizers provide a development dataset including multimedia features. The initial assumption of the proposed system is that commercials which sell the same type of product, should also share the same music rhythm. A two-fold system is proposed: find commercials with close subjects in order to determine the mean rhythm of this subset, and then extract, from the candidate songs, the music which better corresponds to this mean rhythm.",FRA,education,Developed economies,"[-32.920567, -24.958235]","[34.643272, 19.130087]","[8.751068, 23.934116, 5.6753683]","[12.746879, 5.066791, 21.435822]","[12.250056, 11.675628]","[12.23333, 1.9267925]","[12.882902, 15.767852, 0.98907775]","[13.204562, 5.3164606, 12.843632]"
74,Mohamed Morchid;Richard Dufour;Georges Linarès,A Combined Thematic and Acoustic Approach for a Music Recommendation Service in TV Commercials.,2014,https://doi.org/10.5281/zenodo.1415098,Abhishek Singhi+University of Waterloo>CAN>education;Daniel G. Brown+University of Waterloo>CAN>education,"We hypothesize that different genres of writing use different adjectives for the same concept. We test our hypothesis on lyrics, articles and poetry. We use the English Wikipedia and over 13,000 news articles from four leading newspapers for the article data set. Our lyrics data set consists of lyrics of more than 10,000 songs by 56 popular English singers, and our poetry dataset is made up of more than 20,000 poems from 60 famous poets. We find the probability distribution of synonymous adjectives in all the three different categories and use it to predict if a document is an article, lyrics or poetry given its set of adjectives. We achieve an accuracy level of 67% for lyrics, 80% for articles and 57% for poetry. Using these probability distribution we show that adjectives more likely to be used in lyrics are more rhymable than those more likely to be used in poetry, but they do not differ significantly in their semantic orientations. Furthermore we show that our algorithm is successfully able to detect poetic lyricists like Bob Dylan from non-poetic ones like Bryan Adams, as their lyrics are more often misclassified as poetry.",CAN,education,Developed economies,"[-43.292297, 26.788736]","[34.0215, -9.886359]","[-13.5567465, 27.214674, -12.236231]","[15.713861, -12.757783, 15.3575]","[15.822519, 9.281109]","[10.721108, 2.99285]","[15.723982, 15.688345, -1.4543576]","[12.301913, 6.561806, 11.473108]"
73,Sally Jo Cunningham;David M. Nichols;David Bainbridge 0001;Hassan Ali,Social Music in Cars.,2014,https://doi.org/10.5281/zenodo.1415676,Sally Jo Cunningham+University of Waikato>NZL>education;David M. Nichols+University of Waikato>NZL>education;David Bainbridge+University of Waikato>NZL>education;Hasan Ali+University of Waikato>NZL>education,"This paper builds an understanding of how music is currently experienced by a social group travelling together in a car—how songs are chosen for playing, how music both reflects and influences the group’s mood and social interaction, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of ethnographic data (participant observations and interviews) focusing primarily on the experience of in-car music on moderate length and long trips. We suggest features and functionality for music software to enhance the social experience when travelling in cars, and prototype and test a user interface based on design suggestions drawn from the data.",NZL,education,Developed economies,"[-38.021805, 21.92207]","[41.567158, 34.100452]","[-21.213062, 19.380043, -3.6591125]","[5.8786592, 16.280169, 20.228916]","[15.204146, 8.894794]","[12.938925, 0.95318997]","[15.216838, 15.190107, -1.3562831]","[13.370039, 4.240096, 11.837868]"
72,Audrey Laplante,Improving Music Recommender Systems: What Can We Learn from Research on Music Tastes?,2014,https://doi.org/10.5281/zenodo.1417797,Audrey Laplante+Université de Montréal>CAN>education,"The success of a music recommender system depends on its ability to predict how much a particular user will like or dislike each item in its catalogue. However, such predictions are difficult to make accurately due to the complex nature of music tastes. In this paper, we review the literature on music tastes from social psychology and sociology of music to identify the correlates of music tastes and to understand how music tastes are formed and evolve through time. Research shows associations between music preferences and a wide variety of sociodemographic and individual characteristics, including personality traits, values, ethnicity, gender, social class, and political orientation. It also reveals the importance of social influences on music tastes, more specifically from family and peers, as well as the central role of music tastes in the construction of personal and social identities. Suggestions for the design of music recommender systems are made based on this literature review.",CAN,education,Developed economies,"[-45.174496, 24.979143]","[43.10454, 29.751518]","[-11.856965, 24.460121, -10.077477]","[11.623968, 13.761883, 20.004627]","[15.787989, 9.113294]","[13.012034, 1.2568334]","[15.729228, 15.60378, -1.3847611]","[13.518249, 4.3837132, 12.115726]"
71,Zhe Xing;Xinxi Wang;Ye Wang,Enhancing Collaborative Filtering Music Recommendation by Balancing Exploration and Exploitation.,2014,https://doi.org/10.5281/zenodo.1416776,Zhe Xing+National University of Singapore>SGP>education;Xinxi Wang+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"Collaborative filtering (CF) techniques have shown great success in music recommendation applications. However, traditional collaborative-filtering music recommendation algorithms work in a greedy way, invariably recommending songs with the highest predicted user ratings. Such a purely exploitative strategy may result in suboptimal performance over the long term. Using a novel reinforcement learning approach, we introduce exploration into CF and try to balance between exploration and exploitation. In order to learn users’ musical tastes, we use a Bayesian graphical model that takes account of both CF latent factors and recommendation novelty. Moreover, we designed a Bayesian inference algorithm to efficiently estimate the posterior rating distributions. In music recommendation, this is the first attempt to remedy the greedy nature of CF approaches. Results from both simulation experiments and user study show that our proposed approach significantly improves recommendation performance.",SGP,education,Developing economies,"[-46.55221, 26.252266]","[39.491467, 15.629981]","[-8.194559, 24.543638, -11.270407]","[18.394794, 5.7167563, 17.650154]","[15.941325, 9.2751465]","[12.562157, 2.0317726]","[15.788804, 15.715047, -1.4898233]","[13.568298, 5.301369, 12.756773]"
70,Joshua L. Moore;Thorsten Joachims;Douglas Turnbull,Taste Space Versus the World: an Embedding Analysis of Listening Habits and Geography.,2014,https://doi.org/10.5281/zenodo.1415178,Joshua L. Moore+Cornell University>USA>education;Thorsten Joachims+Cornell University>USA>education;Douglas Turnbull+Ithaca College>USA>education,"Probabilistic embedding methods provide a principled way of deriving new spatial representations of discrete objects from human interaction data. The resulting assignment of objects to positions in a continuous, low-dimensional space not only provides a compact and accurate predictive model, but also a compact and flexible representation for understanding the data. In this paper, we demonstrate how probabilistic embedding methods reveal the “taste space” in the recently released Million Musical Tweets Dataset (MMTD), and how it transcends geographic space. In particular, by embedding cities around the world along with preferred artists, we are able to distill information about cultural and geographical differences in listening patterns into spatial representations. These representations yield a similarity metric among city pairs, artist pairs, and city-artist pairs, which can then be used to draw conclusions about the similarities and contrasts between taste space and geographic location.",USA,education,Developed economies,"[-43.4445, 20.440182]","[35.89817, 8.466679]","[-12.072415, 18.04442, -6.0510583]","[17.138685, 7.7857485, 11.057842]","[15.200516, 9.019169]","[11.878795, 2.5433555]","[15.173282, 15.300546, -1.3171661]","[13.277819, 5.8770795, 12.236528]"
69,Ajay Srinivasamurthy;Rafael Caro Repetto;Sundar Harshavardhan;Xavier Serra,Transcription and Recognition of Syllable based Percussion Patterns: The Case of Beijing Opera.,2014,https://doi.org/10.5281/zenodo.1417267,"Ajay Srinivasamurthy+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rafael Caro Repetto+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Harshavardhan Sundar+Indian Institute of Science>IND>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","In many cultures of the world, traditional percussion music uses mnemonic syllables that are representative of the timbres of instruments. These syllables are orally transmitted and often provide a language for percussion in those music cultures. Percussion patterns in these cultures thus have a well defined representation in the form of these syllables, which can be utilized in several computational percussion pattern analysis tasks. We explore a connected word speech recognition based framework that can effectively utilize the syllabic representation for automatic transcription and recognition of audio percussion patterns. In particular, we consider the case of Beijing opera and present a syllable level hidden markov model (HMM) based system for transcription and classification of percussion patterns. The encouraging classification results on a representative dataset of Beijing opera percussion patterns supports our approach and provides further insights on the utility of these syllables for computational description of percussion patterns.",ESP,education,Developed economies,"[-5.1145844, -26.226402]","[-14.675163, -0.91680735]","[10.072431, 6.909492, -14.984024]","[6.2001305, 4.7583847, -15.294572]","[10.659356, 10.58661]","[8.042669, 3.5196579]","[11.527613, 15.235509, -0.102613024]","[9.486593, 7.4355283, 10.558655]"
68,Andre Holzapfel;Florian Krebs;Ajay Srinivasamurthy,"Tracking the ""Odd"": Meter Inference in a Culturally Diverse Music Corpus.",2014,https://doi.org/10.5281/zenodo.1415000,Andre Holzapfel+New York University Abu Dhabi>ARE>education;Florian Krebs+Johannes Kepler University>AUT>education;Ajay Srinivasamurthy+Universitat Pompeu Fabra>ESP>education,"In this paper, we approach the tasks of beat tracking, downbeat recognition and rhythmic style classification in non-Western music. Our approach is based on a Bayesian model, which infers tempo, downbeats and rhythmic style, from an audio signal. The model can be automatically adapted to rhythmic styles and time signatures. For evaluation, we compiled and annotated a music corpus consisting of eight rhythmic styles from three cultures, containing a variety of meter types. We demonstrate that by adapting the model to specific styles, we can track beats and downbeats in odd meter types like 9/8 or 7/8 with an accuracy significantly improved over the state of the art. Even if the rhythmic style is not known in advance, a unified model is able to recognize the meter and track the beat with comparable results, providing a novel method for inferring the metrical structure in culturally diverse datasets.",ARE,education,Developing economies,"[8.6814, 16.605549]","[-27.251755, -8.877071]","[-13.499587, -8.8737755, 1.5336845]","[-5.403223, 16.837025, -10.143806]","[12.511785, 7.336384]","[5.637403, 2.136743]","[12.891753, 13.695911, -1.4831921]","[7.9438815, 6.8906245, 11.086424]"
78,Bogdan Vera;Elaine Chew,Towards Seamless Network Music Performance: Predicting an Ensemble's Expressive Decisions for Distributed Performance.,2014,https://doi.org/10.5281/zenodo.1416186,Bogdan Vera+Queen Mary University of London>GBR>education;Elaine Chew+Queen Mary University of London>GBR>education,"Internet performance faces the challenge of network latency. One proposed solution is music prediction, wherein musical events are predicted in advance and transmitted to distributed musicians ahead of the network delay. We present a context-aware music prediction system focusing on expressive timing: a Bayesian network that incorporates stylistic model selection and linear conditional gaussian distributions on variables representing proportional tempo change. The system can be trained using rehearsals of distributed or co-located ensembles. We evaluate the model by comparing its prediction accuracy to two others: one employing only linear conditional dependencies between expressive timing nodes but no stylistic clustering, and one using only independent distributions for timing changes. The three models are tested on performances of a custom-composed piece that is played ten times, each in one of two styles. The results are promising, with the proposed system outperforming the other two. In predictable parts of the performance, the system with conditional dependencies and stylistic clustering achieves errors of 15ms; in more difficult sections, the errors rise to 100ms; and, in unpredictable sections, the error is too great for seamless timing emulation. Finally, we discuss avenues for further research and propose the use of predictive timing cues using our system.",GBR,education,Developed economies,"[-16.026571, 3.8861518]","[-28.546547, 2.9967635]","[-6.0910993, 10.9623995, 19.191093]","[-10.906048, 9.555965, -3.6796255]","[13.055381, 7.7893095]","[5.6852455, 1.4989914]","[13.907674, 13.458943, -1.2474614]","[7.8682747, 6.4775815, 11.151105]"
67,Karen Ullrich;Jan Schlüter;Thomas Grill,Boundary Detection in Music Structure Analysis using Convolutional Neural Networks.,2014,https://doi.org/10.5281/zenodo.1415886,Karen Ullrich+Austrian Research Institute for Artificial Intelligence>AUT>facility;Jan Schlüter+Austrian Research Institute for Artificial Intelligence>AUT>facility;Thomas Grill+Austrian Research Institute for Artificial Intelligence>AUT>facility,"The recognition of boundaries, e.g., between chorus and verse, is an important task in music structure analysis. The goal is to automatically detect such boundaries in audio signals so that the results are close to human annotation. In this work, we apply Convolutional Neural Networks to the task, trained directly on mel-scaled magnitude spectrograms. On a representative subset of the SALAMI structural annotation dataset, our method outperforms current techniques in terms of boundary retrieval F-measure at different temporal tolerances: We advance the state-of-the-art from 0.33 to 0.46 for tolerances of ±0.5 seconds, and from 0.52 to 0.62 for tolerances of ±3 seconds. As the algorithm is trained on annotated audio data without the need of expert knowledge, we expect it to be easily adaptable to changed annotation guidelines and also to related tasks such as the detection of song transitions.",AUT,facility,Developed economies,"[-6.072165, -9.178998]","[-18.482594, -29.057945]","[9.876724, -1.5918205, -0.1920905]","[-3.867494, -3.0012538, -13.402744]","[11.718075, 8.783106]","[8.652953, 4.4515624]","[12.588637, 14.0772915, 0.3612781]","[10.298658, 6.9579163, 9.599192]"
65,Brian McFee;Dan Ellis,Analyzing Song Structure with Spectral Clustering.,2014,https://doi.org/10.5281/zenodo.1415778,Brian McFee+Columbia University>USA>education;Daniel P.W. Ellis+Columbia University>USA>education,"Many approaches to analyzing the structure of a musical recording involve detecting sequential patterns within a self-similarity matrix derived from time-series features. Such patterns ideally capture repeated sequences, which then form the building blocks of large-scale structure. In this work, techniques from spectral graph theory are applied to analyze repeated patterns in musical recordings. The proposed method produces a low-dimensional encoding of repetition structure, and exposes the hierarchical relationships among structural components at differing levels of granularity. Finally, we demonstrate how to apply the proposed method to the task of music segmentation.",USA,education,Developed economies,"[-5.912706, 0.9595806]","[2.4183009, 1.3342261]","[1.8065693, -3.4497795, -4.6627645]","[1.3901185, 2.8627508, -5.0431495]","[11.922737, 8.329906]","[7.7118216, 3.1892219]","[12.773558, 14.172787, -0.05869765]","[10.533304, 8.047219, 11.419609]"
64,Shrey Dutta;Hema A. Murthy,Discovering Typical Motifs of a Raga from One-Liners of Songs in Carnatic Music.,2014,https://doi.org/10.5281/zenodo.1418157,Shrey Dutta+Indian Institute of Technology Madras>IND>education;Hema A. Murthy+Indian Institute of Technology Madras>IND>education,"Typical motifs of a raga can be found in the various songs that are composed in the same raga by different composers. The compositions in Carnatic music have a definite structure, the one commonly seen being pallavi, anupallavi and charanam. The tala is also fixed for every song. Taking lines corresponding to one or more cycles of the pallavi, anupallavi and charanam as one-liners, one-liners across different songs are compared using a dynamic programming based algorithm. The density of match between the one-liners and normalized cost along-with a new measure, which uses the stationary points in the pitch contour to reduce the false alarms, are used to determine and locate the matched pattern. The typical motifs of a raga are then filtered using compositions of various ragas. Motifs are considered typical if they are present in the compositions of the given raga and are not found in compositions of other ragas.",IND,education,Developing economies,"[4.075388, -0.44320887]","[8.7145605, 2.725784]","[6.2902427, 2.551093, -17.893665]","[11.532568, -12.378759, -1.2210965]","[11.285694, 10.427933]","[8.039806, 1.1351088]","[11.615137, 15.193862, -1.5343908]","[9.7098875, 6.791453, 12.746906]"
63,Peter van Kranenburg;Folgert Karsdorp,Cadence Detection in Western Traditional Stanzaic Songs using Melodic and Textual Features.,2014,https://doi.org/10.5281/zenodo.1416172,Peter van Kranenburg+Meertens Institute>NLD>education;Folgert Karsdorp+Meertens Institute>NLD>education,"Many Western songs are hierarchically structured in stanzas and phrases. The melody of the song is repeated for each stanza, while the lyrics vary. Each stanza is subdivided into phrases. It is to be expected that melodic and textual formulas at the end of the phrases offer intrinsic clues of closure to a listener or singer. In the current paper we aim at a method to detect such cadences in symbolically encoded folk songs. We take a trigram approach in which we classify trigrams of notes and pitches as cadential or as non-cadential. We use pitch, contour, rhythmic, textual, and contextual features, and a group of features based on the conditions of closure as stated by Narmour. We employ a random forest classification algorithm. The precision of the classifier is considerably improved by taking the class labels of adjacent trigrams into account. An ablation study shows that none of the kinds of features is sufficient to account for good classification, while some of the groups perform moderately well on their own.",NLD,education,Developed economies,"[13.109206, 2.8957233]","[10.492848, -2.4477327]","[-5.1054153, -21.812298, 14.871504]","[-0.858724, -6.520122, 2.2122526]","[12.018381, 10.034313]","[8.66149, 2.5711486]","[12.519086, 15.173682, -0.268549]","[10.335576, 7.1944695, 11.870239]"
61,Jan Van Balen;Dimitrios Bountouridis;Frans Wiering;Remco C. Veltkamp,Cognition-inspired Descriptors for Scalable Cover Song Retrieval.,2014,https://doi.org/10.5281/zenodo.1417795,Jan Van Balen+Utrecht University>NLD>education;Dimitrios Bountouridis+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco Veltkamp+Utrecht University>NLD>education,"Inspired by representations used in music cognition studies and computational musicology, we propose three simple and interpretable descriptors for use in mid- to high-level computational analysis of musical audio and applications in content-based retrieval. We also argue that the task of scalable cover song retrieval is very suitable for the development of descriptors that effectively capture musical structures at the song level. The performance of the proposed descriptions in a cover song problem is presented. We further demonstrate that, due to the musically-informed nature of the descriptors, an independently established model of stability and variation in covers songs can be integrated to improve performance.",NLD,education,Developed economies,"[7.504968, 40.96538]","[18.757324, 10.592533]","[0.76469177, 9.81911, -21.739891]","[6.600769, -1.3087901, 11.690248]","[16.067228, 11.017001]","[9.813866, 1.9208922]","[12.929882, 17.30101, -0.44768476]","[11.255109, 6.3941374, 12.225512]"
60,Anna Aljanaki;Frans Wiering;Remco C. Veltkamp,Computational Modeling of Induced Emotion Using GEMS.,2014,https://doi.org/10.5281/zenodo.1415192,Anna Aljanaki+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"Most researchers in the automatic music emotion recognition field focus on the two-dimensional valence and arousal model. This model though does not account for the whole diversity of emotions expressible through music. Moreover, in many cases it might be important to model induced (felt) emotion, rather than perceived emotion. In this paper we explore a multidimensional emotional space, the Geneva Emotional Music Scales (GEMS), which addresses these two issues. We collected the data for our study using a game with a purpose. We exploit a comprehensive set of features from several state-of-the-art toolboxes and propose a new set of harmonically motivated features. The performance of these feature sets is compared. Additionally, we use expert human annotations to explore the dependency between musicologically meaningful characteristics of music and emotional categories of GEMS, demonstrating the need for algorithms that can better approximate human perception.",NLD,education,Developed economies,"[-61.78319, 2.3243825]","[49.91092, -8.829561]","[-28.49605, 24.479534, -0.67738616]","[10.076853, 22.306286, 3.50015]","[13.983814, 12.818155]","[12.96657, 4.2466345]","[16.171988, 14.445747, 1.8240882]","[14.068217, 5.041256, 10.247064]"
59,Colin Raffel;Brian McFee;Eric J. Humphrey;Justin Salamon;Oriol Nieto;Dawen Liang;Daniel P. W. Ellis,MIR_EVAL: A Transparent Implementation of Common MIR Metrics.,2014,https://doi.org/10.5281/zenodo.1416528,"Colin Raffel+LabROSA, Columbia University>USA>education;Brian McFee+LabROSA, Columbia University>USA>education|Music and Audio Research Lab, New York University>USA>education;Eric J. Humphrey+Music and Audio Research Lab, New York University>USA>education;Justin Salamon+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Oriol Nieto+Music and Audio Research Lab, New York University>USA>education;Dawen Liang+LabROSA, Columbia University>USA>education;Daniel P. W. Ellis+LabROSA, Columbia University>USA>education","Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir_eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir_eval and quantitatively compare each to existing implementations. When the scores reported by mir_eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir_eval’s architecture, design, and intended use.",USA,education,Developed economies,"[-8.846406, 56.93328]","[14.559951, 32.141777]","[-37.02492, -1.6976018, -4.677975]","[-0.73165214, 7.469386, 14.317983]","[13.598055, 4.7258773]","[11.314842, 0.619194]","[15.033888, 11.125035, -1.4895265]","[11.582864, 4.867408, 11.693734]"
58,Olivier Lartillot,In-depth Motivic Analysis based on Multiparametric Closed Pattern and Cyclic Sequence Mining.,2014,https://doi.org/10.5281/zenodo.1418345,Olivier Lartillot+Aalborg University>DNK>education,"The paper describes a computational system for exhaustive but compact description of repeated motivic patterns in symbolic representations of music. The approach follows a method based on closed heterogeneous pattern mining in multiparametrical space with control of pattern cyclicity. This paper presents a much simpler description and justification of this general strategy, as well as significant simplifications of the model, in particular concerning the management of pattern cyclicity. A new method for automated bundling of patterns belonging to same motivic or thematic classes is also presented. The good performance of the method is shown through the analysis of a piece from the JKUPDD database. Ground-truth motives are detected, while additional relevant information completes the ground-truth musicological analysis. The system, implemented in Matlab, is made publicly available as part of MiningSuite, a new open-source framework for audio and music analysis.",DNK,education,Developed economies,"[14.017695, 23.890608]","[1.3356276, 16.683456]","[-4.1961646, -11.272551, 12.279795]","[-3.264821, -9.394219, 5.470373]","[11.699444, 7.3392186]","[8.946862, 1.4643521]","[12.937482, 12.823946, -0.88878757]","[10.452243, 6.742022, 12.581032]"
57,Matevz Pesek;Primoz Godec;Mojca Poredos;Gregor Strle;Joze Guna;Emilija Stojmenova;Matevz Pogacnik;Matija Marolt,Introducing a Dataset of Emotional and Color Responses to Music.,2014,https://doi.org/10.5281/zenodo.1415926,Matevž Pesek+University of Ljubljana>SVN>education;Primož Godec+University of Ljubljana>SVN>education;Mojca Poredoš+University of Ljubljana>SVN>education;Gregor Strle+Scientific Research Centre of the Slovenian Academy of Sciences and Arts>SVN>facility;Jože Guna+University of Ljubljana>SVN>education;Emilija Stojmenova+University of Ljubljana>SVN>education;Matevž Pogačnik+University of Ljubljana>SVN>education;Matija Marolt+University of Ljubljana>SVN>education,"The paper presents a new dataset of mood-dependent and color responses to music. The methodology of gathering user responses is described along with two new interfaces for capturing emotional states: the MoodGraph and MoodStripe. An evaluation study showed both interfaces have significant advantage over more traditional methods in terms of intuitiveness, usability and time complexity. The preliminary analysis of current data (over 6.000 responses) gives an interesting insight into participants’ emotional states and color associations, as well as relationships between musically perceived and induced emotions. We believe the size of the dataset, interfaces and multi-modal approach (connecting emotional, visual and auditory aspects of human perception) give a valuable contribution to current research.",SVN,education,Developed economies,"[-60.939747, 1.2496217]","[53.870064, -6.284247]","[-26.01346, 22.15069, 0.2863258]","[8.959885, 21.048944, 7.850643]","[13.966853, 12.803067]","[13.085894, 3.9819589]","[16.200373, 14.466891, 1.8130347]","[14.106466, 4.927505, 10.548442]"
56,Christian Frisson;Stéphane Dupont;Willy Yvart;Nicolas Riche;Xavier Siebert;Thierry Dutoit,A Proximity Grid Optimization Method to Improve Audio Search for Sound Design.,2014,https://doi.org/10.5281/zenodo.1417245,"Christian Frisson+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Stéphane Dupont+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Willy Yvart+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Nicolas Riche+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Xavier Siebert+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Thierry Dutoit+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education","Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques.",BEL,education,Developed economies,"[-5.836513, -20.404112]","[25.897953, 14.740382]","[-1.8703842, -4.4924526, -24.354214]","[14.383596, -2.80124, 9.045167]","[12.747115, 9.001461]","[10.636342, 1.9796939]","[12.070504, 13.6082735, 0.8521443]","[12.043322, 6.164277, 12.658144]"
55,Shuo Zhang;Rafael Caro Repetto;Xavier Serra,Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing.,2014,https://doi.org/10.5281/zenodo.1416250,Shuo Zhang+Universitat Pompeu Fabra>ESP>education;Rafael Caro Repetto+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"Features of linguistic tone contours are important factors that shape the distinct melodic characteristics of different genres of Chinese opera. In Beijing opera, the presence of a two-dialectal tone system makes the tone-melody relationship more complex. In this paper, we propose a novel data-driven approach to analyze syllable-sized tone-pitch contour similarity in a corpus of Beijing Opera (381 arias) with statistical modeling and machine learning methods. A total number of 1,993 pitch contour units and attributes were extracted from a selection of 20 arias. We then build Smoothing Spline ANOVA models to compute matrixes of average melodic contour curves by tone category and other attributes. A set of machine learning and statistical analysis methods are applied to 30-point pitch contour vectors as well as dimensionality-reduced representations using Symbolic Aggregate approXimation(SAX). The results indicate an even mixture of shapes within all tone categories, with the absence of evidence for a predominant dialectal tone system in Beijing opera. We discuss the key methodological issues in melody-tone analysis and future work on pair-wise contour unit analysis.",ESP,education,Developed economies,"[-4.7218566, -27.127722]","[5.8557973, -9.79212]","[11.933968, 6.569494, -16.162428]","[10.733605, -7.0456185, -2.4978752]","[10.674577, 10.5798025]","[8.152612, 1.9610797]","[11.499491, 15.298325, -0.13749188]","[9.851595, 7.194075, 12.283519]"
66,Oriol Nieto;Morwaread M. Farbood,Identifying Polyphonic Musical Patterns From Audio Recordings Using Music Segmentation Techniques.,2014,https://doi.org/10.5281/zenodo.1417259,Oriol Nieto+New York University>USA>education|Unknown>Unknown>Unknown;Morwaread M. Farbood+New York University>USA>education|Unknown>Unknown>Unknown,"This paper presents a method for discovering patterns of note collections that repeatedly occur in a piece of music. We assume occurrences of these patterns must appear at least twice across a musical work and that they may contain slight differences in harmony, timbre, or rhythm. We describe an algorithm that makes use of techniques from the music information retrieval task of music segmentation, which exploits repetitive features in order to automatically identify polyphonic musical patterns from audio recordings. The novel algorithm is assessed using the recently published JKU Patterns Development Dataset, and we show how it obtains state-of-the-art results employing the standard evaluation metrics.",USA,education,Developed economies,"[-1.8609761, -6.0953474]","[3.7680635, 15.320648]","[9.290545, -4.871716, -3.6843257]","[0.4907719, -9.239567, 7.283532]","[11.528829, 8.027833]","[8.902564, 1.2965379]","[12.26044, 13.997173, 0.005376766]","[10.719759, 6.826802, 12.64783]"
54,Geoffroy Peeters;Victor Bisot,Improving Music Structure Segmentation using lag-priors.,2014,https://doi.org/10.5281/zenodo.1414764,Geoffroy Peeters+STMS IRCAM-CNRS-UPMC>FRA>Unknown;Victor Bisot+STMS IRCAM-CNRS-UPMC>FRA>Unknown,"Methods for music structure discovery usually process a music track by first detecting segments and then labeling them. Depending on the assumptions made on the signal content (repetition, homogeneity or novelty), different methods are used for these two steps. In this paper, we deal with the segmentation in the case of repetitive content. In this field, segments are usually identified by looking for sub-diagonals in a Self-Similarity-Matrix (SSM). In order to make this identification more robust, Goto proposed in 2003 to cumulate the values of the SSM over constant-lag and search only for segments in the SSM when this sum is large. Since the various repetitions of a segment start simultaneously in a self-similarity-matrix, Serra et al. proposed in 2012 to cumulate these simultaneous values (using a so-called structure feature) to enhance the novelty of the starting and ending time of a segment. In this work, we propose to combine both approaches by using Goto method locally as a prior to the lag-dimensions of Serra et al. structure features used to compute the novelty curve. Through a large experiment on RWC and Isophonics test-sets and using MIREX segmentation evaluation measure, we show that this simple combination allows a large improvement of the segmentation results.",FRA,Unknown,Developed economies,"[-3.022175, -2.5055141]","[2.1607149, 2.1775343]","[4.2264442, -6.269086, 0.26002443]","[0.10076791, 3.4764447, -3.874223]","[11.683569, 8.320268]","[7.8471823, 3.1549032]","[12.494684, 14.084963, 0.059420276]","[10.616651, 7.9638286, 11.477089]"
79,Ling-Chi Hsu;Yu-Lin Wang;Yi-Ju Lin;Cheryl D. Metcalf;Alvin W. Y. Su,Detection of Motor Changes in Violin Playing by EMG Signals.,2014,https://doi.org/10.5281/zenodo.1416452,Ling-Chi Hsu+National Cheng-Kung University>TWN>education;Yu-Lin Wang+National Cheng-Kung University>TWN>education;Yi-Ju Lin+National Cheng-Kung University>TWN>education;Alvin W.Y. Su+National Cheng-Kung University>TWN>education;Cheryl D. Metcalf+University of Southampton>GBR>education,"Playing a music instrument relies on the harmonious body movements. Motor sequences are trained to achieve the perfect performances in musicians. Thus, the information from audio signal is not enough to understand the sensorimotor programming in players. Recently, the investigation of muscular activities of players during performance has attracted our interests. In this work, we propose a multi-channel system that records the audio sounds and electromyography (EMG) signal simultaneously and also develop algorithms to analyze the music performance and discover its relation to player’s motor sequences. The movement segment was first identified by the information of audio sounds, and the direction of violin bowing was detected by the EMG signal. Six features were introduced to reveal the variations of muscular activities during violin playing. With the additional information of the audio signal, the proposed work could efficiently extract the period and detect the direction of motor changes in violin bowing. Therefore, the proposed work could provide a better understanding of how players activate the muscles to organize the multi-joint movement during violin performance.",TWN,education,Developing economies,"[15.778614, -21.396936]","[-38.198208, 11.157782]","[8.7813225, -16.352434, -2.3596866]","[-16.107279, 16.857433, -4.6999187]","[9.475744, 6.7073636]","[6.3192677, 2.8213894]","[11.258941, 12.357128, -0.49655846]","[8.81953, 7.203664, 10.71489]"
82,Filip Korzeniowski;Sebastian Böck;Gerhard Widmer,Probabilistic Extraction of Beat Positions from a Beat Activation Function.,2014,https://doi.org/10.5281/zenodo.1415118,Filip Korzeniowski+Johannes Kepler University>AUT>education;Sebastian Böck+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"We present a probabilistic way to extract beat positions from the output (activations) of the neural network that is at the heart of an existing beat tracker. The method can serve as a replacement for the greedy search the beat tracker currently uses for this purpose. Our experiments show improvement upon the current method for a variety of data sets and quality measures, as well as better results compared to other state-of-the-art algorithms.",AUT,education,Developed economies,"[35.333244, -32.46842]","[-29.524426, -12.6789055]","[6.5339465, -27.742306, -5.6736946]","[-8.994279, 16.20966, -15.601714]","[10.614855, 4.3139396]","[5.057707, 2.3022408]","[10.238362, 12.972287, -2.2090867]","[7.592732, 6.96603, 10.501073]"
104,Sebastian Stober;Daniel J. Cameron;Jessica A. Grahn,Classifying EEG Recordings of Rhythm Perception.,2014,https://doi.org/10.5281/zenodo.1415734,Sebastian Stober+Western University>CAN>education;Daniel J. Cameron+Western University>CAN>education;Jessica A. Grahn+Western University>CAN>education,"Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. In this paper, we present first classification results using deep learning techniques on EEG data recorded within a rhythm perception study in Kigali, Rwanda. We tested 13 adults, mean age 21, who performed three behavioral tasks using rhythmic tone sequences derived from either East African or Western music. For the EEG testing, 24 rhythms – half East African and half Western with identical tempo and based on a 2-bar 12/8 scheme – were each repeated for 32 seconds. During presentation, the participants’ brain waves were recorded via 14 EEG channels. We applied stacked denoising autoencoders and convolutional neural networks on the collected data to distinguish African and Western rhythms on a group and individual participant level. Furthermore, we investigated how far these techniques can be used to recognize the individual rhythms.",CAN,education,Developed economies,"[42.44039, 9.468373]","[-10.928297, -50.665497]","[-5.1685853, -23.625057, 1.0768613]","[-1.9992288, 16.12349, -0.03996427]","[11.883645, 5.307568]","[5.8701363, 2.0219142]","[11.234701, 13.985283, -2.2069035]","[8.2764, 6.7742796, 11.442401]"
103,Maura Church;Michael Scott Cuthbert,Improving Rhythmic Transcriptions via Probability Models Applied Post-OMR.,2014,https://doi.org/10.5281/zenodo.1416752,Maura Church+Harvard University>USA>education|Google Inc.>USA>company;Michael Scott Cuthbert+M.I.T.>USA>education,"Despite many improvements in the recognition of graphical elements, even the best implementations of Optical Music Recognition (OMR) introduce inaccuracies in the resultant score. These errors, particularly rhythmic errors, are time consuming to fix. Most musical compositions repeat rhythms between parts and at various places throughout the score. Information about rhythmic self-similarity, however, has not previously been used in OMR systems. This paper describes and implements methods for using the prior probabilities for rhythmic similarities in scores produced by a commercial OMR system to correct rhythmic errors which cause a contradiction between the notes of a measure and the underlying time signature. Comparing the OMR output and post-correction results to hand-encoded scores of 37 polyphonic pieces and movements (mostly drawn from the classical repertory), the system reduces incorrect rhythms by an average of 19% (min: 2%, max: 36%). The paper includes a public release of an implementation of the model in music21 and also suggests future refinements and applications to pitch correction that could further improve the accuracy of OMR systems.",USA,education,Developed economies,"[43.142685, -0.62945896]","[-18.594627, 34.910034]","[8.298203, -17.139439, 21.669334]","[-9.463128, -17.939772, 4.0557885]","[9.551407, 6.571743]","[6.648303, -0.45864883]","[11.588504, 11.888372, -0.1417876]","[8.05345, 4.2655425, 10.669513]"
102,Matthew E. P. Davies;Sebastian Böck,Evaluating the Evaluation Measures for Beat Tracking.,2014,https://doi.org/10.5281/zenodo.1415128,Mathew E. P. Davies+INESC TEC>PRT>facility;Sebastian Böck+Johannes Kepler University>AUT>education,"The evaluation of audio beat tracking systems is normally addressed in one of two ways. One approach is for human listeners to judge performance by listening to beat times mixed as clicks with music signals. The more common alternative is to compare beat times against ground truth annotations via one or more of the many objective evaluation measures. However, despite a large body of work in audio beat tracking, there is currently no consensus over which evaluation measure(s) to use, meaning multiple accuracy scores are typically reported. In this paper, we seek to evaluate the evaluation measures by examining the relationship between objective accuracy scores and human judgements of beat tracking performance. First, we present the raw correlation between objective scores and subjective ratings, and show that evaluation measures which allow alternative metrical levels appear more correlated than those which do not. Second, we explore the effect of parameterisation of objective evaluation measures, and demonstrate that correlation is maximised for smaller tolerance windows than those currently used. Our analysis suggests that true beat tracking performance is currently being overestimated via objective evaluation.",PRT,facility,Developed economies,"[34.853462, -32.966175]","[-33.293465, -1.2212156]","[6.668158, -29.448551, -7.2332273]","[-6.5534863, 14.0178, -5.3731694]","[10.516986, 4.279056]","[5.3260446, 1.6031219]","[10.216145, 12.8893795, -2.2472157]","[7.3867345, 6.7608438, 11.192148]"
101,Mi Tian;György Fazekas;Dawn A. A. Black;Mark B. Sandler,Design And Evaluation of Onset Detectors using Different Fusion Policies.,2014,https://doi.org/10.5281/zenodo.1415002,"Mi Tian+Centre for Digital Music, Queen Mary University of London>GBR>education;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education;Dawn A. A. Black+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","Note onset detection is one of the most investigated tasks in Music Information Retrieval (MIR) and various detection methods have been proposed in previous research. The primary aim of this paper is to investigate different fusion policies to combine existing onset detectors, thus achieving better results. Existing algorithms are fused using three strategies, first by combining different algorithms, second, by using the linear combination of detection functions, and third, by using a late decision fusion approach. Large scale evaluation was carried out on two published datasets and a new percussion database composed of Chinese traditional instrument samples. An exhaustive search through the parameter space was used enabling a systematic analysis of the impact of each parameter, as well as reporting the most generally applicable parameter settings for the onset detectors and the fusion. We demonstrate improved results attributed to both fusion and the optimised parameter settings.",GBR,education,Developed economies,"[30.600616, -27.520678]","[-23.209766, -6.9529915]","[8.351069, -20.769512, -9.172426]","[-3.3413892, 5.5745564, -11.055277]","[10.285855, 4.976111]","[5.652323, 2.4097557]","[10.326346, 13.246287, -1.54651]","[7.9827547, 7.4895267, 10.883977]"
100,Tomohiko Nakamura;Kotaro Shikata;Norihiro Takamune;Hirokazu Kameoka,Harmonic-Temporal Factor Decomposition Incorporating Music Prior Information for Informed Monaural Source Separation.,2014,https://doi.org/10.5281/zenodo.1417463,Tomohiko Nakamura+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility;Kotaro Shikata+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility;Norihiro Takamune+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility;Hirokazu Kameoka+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility,"For monaural source separation two main approaches have thus far been adopted. One approach involves applying non-negative matrix factorization (NMF) to an observed magnitude spectrogram, interpreted as a non-negative matrix. The other approach is based on the concept of computational auditory scene analysis (CASA). A CASA-based approach called the “harmonic-temporal clustering (HTC)” aims to cluster the time-frequency components of an observed signal based on a constraint designed according to the local time-frequency structure common in many sound sources (such as harmonicity and the continuity of frequency and amplitude modulations). This paper proposes a new approach for monaural source separation called the “Harmonic-Temporal Factor Decomposition (HTFD)” by introducing a spectrogram model that combines the features of the models employed in the NMF and HTC approaches. We further describe some ideas how to design the prior distributions for the present model to incorporate musically relevant information into the separation scheme.",JPN,education,Developed economies,"[6.4742584, -45.46774]","[-45.15247, -25.787506]","[29.297106, 0.013481139, -4.121333]","[-6.583708, -9.670378, -30.98844]","[8.47978, 9.944237]","[6.3593025, 5.198228]","[11.114719, 13.611674, 1.5621694]","[9.82478, 8.805614, 9.773188]"
99,Frederick Z. Yen;Yin-Jyun Luo;Tai-Shih Chi,Singing Voice Separation Using Spectro-Temporal Modulation Features.,2014,https://doi.org/10.5281/zenodo.1417695,Frederick Yen+National Chiao-Tung University>TWN>education;Yin-Jyun Luo+National Chiao-Tung University>TWN>education;Tai-Shih Chi+National Chiao-Tung University>TWN>education,"An auditory-perception inspired singing voice separation algorithm for monaural music recordings is proposed in this paper. Under the framework of computational auditory scene analysis (CASA), the music recordings are first transformed into auditory spectrograms. After extracting the spectral-temporal modulation contents of the time-frequency (T-F) units through a two-stage auditory model, we define modulation features pertaining to three categories in music audio signals: vocal, harmonic, and percussive. The T-F units are then clustered into three categories and the singing voice is synthesized from T-F units in the vocal category via time-frequency masking. The algorithm was tested using the MIR-1K dataset and demonstrated comparable results to other unsupervised masking approaches. Meanwhile, the set of novel features gives a possible explanation on how the auditory cortex analyzes and identifies singing voice in music audio mixtures.",TWN,education,Developing economies,"[-1.0347718, -42.71806]","[-44.55467, -30.594807]","[27.750395, 6.4435, -7.3342013]","[-4.35176, -6.452565, -29.237999]","[9.101058, 10.719979]","[6.584803, 5.2581472]","[10.850127, 14.56085, 1.3562969]","[9.901529, 8.585016, 9.599158]"
98,Jonathan Driedger;Meinard Müller;Sascha Disch,Extending Harmonic-Percussive Separation of Audio Signals.,2014,https://doi.org/10.5281/zenodo.1415226,Jonathan Driedger+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Sascha Disch+Fraunhofer Institute for Integrated Circuits IIS>DEU>facility,"In recent years, methods to decompose an audio signal into a harmonic and a percussive component have received a lot of interest and are frequently applied as a processing step in a variety of scenarios. One problem is that the computed components are often not of purely harmonic or percussive nature but also contain noise-like sounds that are neither clearly harmonic nor percussive. Furthermore, depending on the parameter settings, one often can observe a leakage of harmonic sounds into the percussive component and vice versa. In this paper we present two extensions to a state-of-the-art harmonic-percussive separation procedure to target these problems. First, we introduce a separation factor parameter into the decomposition process that allows for tightening separation results and for enforcing the components to be clearly harmonic or percussive. As second contribution, inspired by the classical sines+transients+noise (STN) audio model, this novel concept is exploited to add a third residual component to the decomposition which captures the sounds that lie in between the clearly harmonic and percussive sounds of the audio signal.",DEU,facility,Developed economies,"[10.821028, -43.650963]","[-43.923965, -26.194311]","[27.3047, -5.616181, -8.032627]","[-5.628261, -7.9767137, -32.44796]","[8.732219, 9.644415]","[6.449523, 5.442476]","[11.118561, 13.536968, 1.1259758]","[9.785287, 8.691461, 9.586158]"
97,Sebastian Böck;Florian Krebs;Gerhard Widmer,A Multi-model Approach to Beat Tracking Considering Heterogeneous Music Styles.,2014,https://doi.org/10.5281/zenodo.1415240,Sebastian Böck+Johannes Kepler University>AUT>education;Florian Krebs+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possible beat positions. It chooses the model with the most appropriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27% over existing state-of-the-art methods. Under certain conditions the system is able to match even human tapping performance.",AUT,education,Developed economies,"[33.729908, -36.355343]","[-28.379324, -11.57407]","[11.194166, -28.368038, -8.269522]","[-6.9356527, 16.756931, -13.988215]","[10.418286, 4.322514]","[5.1481776, 2.3441317]","[10.277412, 12.812804, -2.1608722]","[7.6661425, 6.937125, 10.465914]"
96,Pierre Saurel;Francis Rousseaux;Marc Danger,On The Changing Regulations of Privacy and Personal Information in MIR.,2014,https://doi.org/10.5281/zenodo.1416638,Pierre Saurel+Université Paris-Sorbonne>FRA>education;Francis Rousseaux+IRCAM>FRA>facility;Marc Danger+ADAMI>FRA>company,"In recent years, MIR research has continued to focus more and more on user feedback, human subjects data, and other forms of personal information. Concurrently, the European Union has adopted new, stringent regulations to take effect in the coming years regarding how such information can be collected, stored and manipulated, with equally strict penalties for being found in violation of the law. Here, we provide a summary of these changes, consider how they relate to our data sources and research practices, and identify promising methodologies that may serve researchers well, both in order to be in compliance with the law and conduct more subject-friendly research. We additionally provide a case study of how such changes might affect a recent human subjects project on the topic of style, and conclude with a few recommendations for the near future. This paper is not intended to be legal advice: our personal legal interpretations are strictly mentioned for illustration purpose, and reader should seek proper legal counsel.",FRA,education,Developed economies,"[-6.399483, 55.95279]","[20.136217, 45.71998]","[-32.109035, -2.3366985, -8.004547]","[-5.0742145, 8.700313, 19.593704]","[13.645609, 4.772582]","[11.677611, 0.3214309]","[15.144551, 11.201323, -1.5370195]","[11.896901, 4.5944204, 11.572745]"
95,Eric J. Humphrey;Justin Salamon;Oriol Nieto;Jon Forsyth;Rachel M. Bittner;Juan Pablo Bello,JAMS: A JSON Annotated Music Specification for Reproducible MIR Research.,2014,https://doi.org/10.5281/zenodo.1415924,"Eric J. Humphrey+Music and Audio Research Lab, New York University>USA>education;Justin Salamon+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Oriol Nieto+Music and Audio Research Lab, New York University>USA>education;Jon Forsyth+Music and Audio Research Lab, New York University>USA>education;Rachel M. Bittner+Music and Audio Research Lab, New York University>USA>education;Juan P. Bello+Music and Audio Research Lab, New York University>USA>education","The continued growth of MIR is motivating more complex annotation data, consisting of richer information, multiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of addressing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, comprehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we provide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and discuss how now is a crucial time to make a concerted effort toward sustainable annotation standards.",USA,education,Developed economies,"[-12.390142, 54.78615]","[18.72902, 43.95903]","[-36.427708, 0.77967316, 1.1013091]","[-7.051219, 5.742292, 18.024689]","[13.268621, 5.286193]","[11.316375, 0.528597]","[14.641124, 11.581492, -1.3337054]","[11.783066, 5.063308, 11.502022]"
80,Wang-Kong Lam;Tan Lee,Automatic Key Partition Based on Tonal Organization Information of Classical Music.,2014,https://doi.org/10.5281/zenodo.1414748,Lam Wang Kong+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown;Tan Lee+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown,"Key information is a useful information for tonal music analysis. It is related to chord progressions, which follows some specific structures and rules. In this paper, we describe a generative account of chord progression consisting of phrase-structure grammar rules proposed by Martin Rohrmeier. With some modifications, these rules can be used to partition a chord symbol sequence into different key areas, if modulation occurs. Exploiting tonal grammar rules, the most musically sensible key partition of chord sequence is derived. Some examples of classical music excerpts are evaluated. This rule-based system is compared against another system which is based on dynamic programming of harmonic-hierarchy information. Using Kostka-Payne corpus as testing data, the experimental result shows that our system is better in terms of key detection accuracy.",HKG,education,Developing economies,"[29.987, 13.317874]","[-19.17051, 16.15461]","[7.5150113, -16.163967, 12.088638]","[-18.067179, -3.6292677, 5.330361]","[10.844777, 7.477861]","[7.2181544, 2.9438589]","[12.21612, 13.213793, 0.103677124]","[9.904429, 8.2043915, 12.137653]"
94,Shoto Sasaki;Kazuyoshi Yoshii;Tomoyasu Nakano;Masataka Goto;Shigeo Morishima,LyricsRadar: A Lyrics Retrieval System Based on Latent Topics of Lyrics.,2014,https://doi.org/10.5281/zenodo.1418075,Shoto Sasaki+Waseda University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Shigeo Morishima+Waseda University>JPN>education,"This paper presents a lyrics retrieval system called LyricsRadar that enables users to interactively browse song lyrics by visualizing their topics. Since conventional lyrics retrieval systems are based on simple word search, those systems often fail to reflect user’s intention behind a query when a word given as a query can be used in different contexts. For example, the word “tears” can appear not only in sad songs (e.g., feel heartrending), but also in happy songs (e.g., weep for joy). To overcome this limitation, we propose to automatically analyze and visualize topics of lyrics by using a well-known text analysis method called latent Dirichlet allocation (LDA). This enables LyricsRadar to offer two types of topic visualization. One is the topic radar chart that visualizes the relative weights of five latent topics of each song on a pentagon-shaped chart. The other is radar-like arrangement of all songs in a two-dimensional space in which song lyrics having similar topics are arranged close to each other. The subjective experiments using 6,902 Japanese popular songs showed that our system can appropriately navigate users to lyrics of interests.",JPN,education,Developed economies,"[-29.436806, -31.51504]","[30.451286, 13.440398]","[6.81041, 20.038195, -5.2622876]","[16.662657, -3.3846197, 13.777462]","[11.390174, 11.648187]","[11.457434, 2.3487568]","[12.342989, 15.943495, 0.9551541]","[12.575745, 5.995118, 12.392561]"
92,Julián Urbano;Dmitry Bogdanov;Perfecto Herrera;Emilia Gómez;Xavier Serra,What is the Effect of Audio Quality on the Robustness of MFCCs and Chroma Features?,2014,https://doi.org/10.5281/zenodo.1416276,"Julián Urbano+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education","Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical applications they are to be computed on music corpora containing audio files encoded in a variety of lossy formats. Such encodings distort the original signal and therefore may affect the computation of descriptors. This raises the question of the robustness of these descriptors across various audio encodings. We examine this assumption for the case of MFCCs and chroma features. In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre. Using two different audio analysis tools over a diverse collection of music tracks, we compute several statistics to quantify the robustness of the resulting descriptors, and then estimate the practical effects for a sample task like genre classification.",ESP,education,Developed economies,"[-11.504956, -21.195585]","[20.114628, -18.005716]","[4.1847334, -6.251972, -20.13226]","[18.639555, -0.2892139, -7.311423]","[12.110436, 7.7347493]","[9.744359, 2.9522116]","[12.876058, 13.613786, 0.5468697]","[11.363133, 6.981873, 11.095835]"
91,Emilio Molina;Ana M. Barbancho;Lorenzo J. Tardón;Isabel Barbancho,Evaluation Framework for Automatic Singing Transcription.,2014,https://doi.org/10.5281/zenodo.1417729,Emilio Molina+Universidad de Málaga>ESP>education;Ana M. Barbancho+Universidad de Málaga>ESP>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Isabel Barbancho+Universidad de Málaga>ESP>education,"In this paper, we analyse the evaluation strategies used in previous works on automatic singing transcription, and we present a novel, comprehensive and freely available evaluation framework for automatic singing transcription. This framework consists of a cross-annotated dataset and a set of extended evaluation measures, which are integrated in a Matlab toolbox. The presented evaluation measures are based on standard MIREX note-tracking measures, but they provide extra information about the type of errors made by the singing transcriber. Finally, a practical case of use is presented, in which the evaluation framework has been used to perform a comparison in detail of several state-of-the-art singing transcribers.",ESP,education,Developed economies,"[-5.338576, -33.976147]","[-6.387044, -14.445689]","[17.612518, 10.925583, -11.112297]","[5.730793, -10.232563, -17.313728]","[9.945345, 10.977448]","[6.9377775, 2.4182339]","[11.306442, 15.05744, 0.69886816]","[9.051034, 7.3807273, 11.083959]"
90,Daniel Boland;Roderick Murray-Smith,Information-Theoretic Measures of Music Listening Behaviour.,2014,https://doi.org/10.5281/zenodo.1416192,Daniel Boland+University of Glasgow>GBR>education;Roderick Murray-Smith+University of Glasgow>GBR>education,"We present an information-theoretic approach to the measurement of users’ music listening behaviour and selection of music features. Existing ethnographic studies of music use have guided the design of music retrieval systems however are typically qualitative and exploratory in nature. We introduce the SPUD dataset, comprising 10,000 handmade playlists, with user and audio stream metadata. With this, we illustrate the use of entropy for analysing music listening behaviour, e.g. identifying when a user changed music retrieval system. We then develop an approach to identifying music features that reflect users’ criteria for playlist curation, rejecting features that are independent of user behaviour. The dataset and the code used to produce it are made available. The techniques described support a quantitative yet user-centred approach to the evaluation of music features and retrieval systems, without assuming objective ground truth labels.",GBR,education,Developed economies,"[-39.757484, 23.618422]","[36.581318, 18.883097]","[-15.721589, 19.170767, -8.196811]","[11.673553, 7.0115385, 17.900251]","[15.21068, 8.845088]","[12.405266, 1.5168806]","[15.076295, 15.222741, -1.4221289]","[13.125926, 4.9237742, 12.529697]"
89,Ming-Ju Wu;Jyh-Shing Roger Jang;Chun-Hung Lu,Gender Identification and Age Estimation of Users Based on Music Metadata.,2014,https://doi.org/10.5281/zenodo.1417605,Ming-Ju Wu+National Tsing Hua University>TWN>education;Jyh-Shing Roger Jang+National Taiwan University>TWN>education;Chun-Hung Lu+Institute for Information Industry>TWN>facility,"Music recommendation is a crucial task in the field of music information retrieval. However, users frequently withhold their real-world identity, which creates a negative impact on music recommendation. Thus, the proposed method recognizes users’ real-world identities based on music metadata. The approach is based on using the tracks most frequently listened to by a user to predict their gender and age. Experimental results showed that the approach achieved an accuracy of 78.87% for gender identification and a mean absolute error of 3.69 years for the age estimation of 48403 users, demonstrating its effectiveness and feasibility, and paving the way for improving music recommendation based on such personal information.",TWN,education,Developing economies,"[-46.195576, 16.060728]","[40.842587, 19.523365]","[-10.061299, 29.449467, 3.4051828]","[13.022737, 9.029145, 16.330462]","[15.26327, 8.585192]","[12.606632, 1.7096996]","[15.329713, 14.844535, -1.3723481]","[13.350332, 4.949929, 12.515082]"
88,Andreas Arzt;Gerhard Widmer;Reinhard Sonnleitner,Tempo- and Transposition-invariant Identification of Piece and Score Position.,2014,https://doi.org/10.5281/zenodo.1415850,Andreas Arzt+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Reinhard Sonnleitner+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"We present an algorithm that, given a very small snippet of an audio performance and a database of musical scores, quickly identifies the piece and the position in the score. The algorithm is both tempo- and transposition-invariant. We approach the problem by extending an existing tempo-invariant symbolic fingerprinting method, replacing the absolute pitch information in the fingerprints with a relative representation. Not surprisingly, this leads to a big decrease in the discriminative power of the fingerprints. To overcome this problem, we propose an additional verification step to filter out the introduced noise. Finally, we present a simple tracking algorithm that increases the retrieval precision for longer queries. Experiments show that both modifications improve the results, and make the new algorithm usable for a wide range of applications.",AUT,education,Developed economies,"[37.617764, 10.606658]","[2.4027824, 10.072997]","[-3.200444, -36.011967, -1.2996833]","[3.3099627, -16.458796, 2.7588692]","[11.530066, 4.6009393]","[8.26922, 0.2655259]","[11.117361, 13.112265, -2.7229545]","[10.034154, 5.47035, 12.731399]"
87,Martin Przyjaciel-Zablocki;Thomas Hornung 0001;Alexander Schätzle;Sven Gauß;Io Taxidou;Georg Lausen,MuSe: A Music Recommendation Management System.,2014,https://doi.org/10.5281/zenodo.1418195,Martin Przyjaciel-Zablocki+University of Freiburg>DEU>education;Thomas Hornung+University of Freiburg>DEU>education;Alexander Schätzle+University of Freiburg>DEU>education;Sven Gauß+University of Freiburg>DEU>education;Io Taxidou+University of Freiburg>DEU>education;Georg Lausen+University of Freiburg>DEU>education,"Evaluating music recommender systems is a highly repetitive, yet non-trivial, task. But it has the advantage over other domains that recommended songs can be evaluated immediately by just listening to them. In this paper, we present MUSE – a music recommendation management system – for solving the typical tasks of an in vivo evaluation. MUSE provides the typical off-the-shelf evaluation algorithms, offers an online evaluation system with automatic reporting, and by integrating online streaming services also a legal possibility to evaluate the quality of recommended songs in real time. Finally, it has a built-in user management system that conforms with state-of-the-art privacy standards. New recommender algorithms can be plugged in comfortably and evaluations can be configured and managed online.",DEU,education,Developed economies,"[-44.49964, 27.18672]","[37.446587, 17.048471]","[-11.414487, 26.29527, -14.437259]","[13.952805, 6.74687, 16.311953]","[15.978332, 9.244866]","[12.476604, 1.8551081]","[15.859703, 15.746541, -1.5499924]","[13.413938, 5.1488934, 12.66463]"
86,Maria Panteli;Niels Bogaards;Aline K. Honingh,Modeling Rhythm Similarity for Electronic Dance Music.,2014,https://doi.org/10.5281/zenodo.1416664,Maria Panteli+University of Amsterdam>NLD>education|Elephantcandy>NLD>company;Niels Bogaards+Elephantcandy>NLD>company;Aline Honingh+University of Amsterdam>NLD>education,"A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a ‘loop’, a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between segments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most notably: attack phase of onsets, periodicity of rhythmic elements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, after which the similarity between segments can be calculated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the overall performance of the model with perceptual ratings of rhythm similarity.",NLD,education,Developed economies,"[1.5075299, 12.923818]","[-22.865374, 1.7435293]","[-9.268176, -21.234922, -1.8190997]","[0.09037295, 14.156001, -6.1883082]","[11.837691, 5.368003]","[6.037663, 1.7242997]","[11.62049, 13.98463, -2.1830423]","[8.117469, 6.7462287, 11.712874]"
85,Eita Nakamura;Nobutaka Ono;Shigeki Sagayama,Merged-Output HMM for Piano Fingering of Both Hands.,2014,https://doi.org/10.5281/zenodo.1415152,Eita Nakamura+National Institute of Informatics>JPN>facility;Nobutaka Ono+National Institute of Informatics>JPN>facility;Shigeki Sagayama+Meiji University>JPN>education,"This paper discusses a piano fingering model for both hands and its applications. One of our motivations behind the study is automating piano reduction from ensemble scores. For this, quantifying the difficulty of piano performance is important where a fingering model of both hands should be relevant. Such a fingering model is proposed that is based on merged-output hidden Markov model and can be applied to scores in which the voice part for each hand is not indicated. The model is applied for decision of fingering for both hands and voice-part separation, automation of which is itself of great use and were previously difficult. A measure of difficulty of performance based on the fingering model is also proposed and yields reasonable results.",JPN,facility,Developed economies,"[29.14348, -2.899012]","[-46.507816, -4.2577934]","[20.78076, -2.4407918, 17.517178]","[-8.859789, -19.900534, -9.965801]","[9.738105, 7.1707478]","[6.925609, 4.32045]","[11.999645, 11.559781, -0.29800478]","[8.942901, 7.075421, 10.388078]"
84,John Ashley Burgoyne;W. Bas de Haas;Johan Pauwels,On Comparative Statistics for Labelling Tasks: What can We Learn from MIREX ACE 2013?,2014,https://doi.org/10.5281/zenodo.1417091,John Ashley Burgoyne+Universiteit van Amsterdam>NLD>education;W. Bas de Haas+Universiteit Utrecht>NLD>education;Johan Pauwels+Unknown>Unknown>Unknown,"For the evaluation of audio chord estimation, the evaluation of audio chord estimation followed a new scheme. Using chord vocabularies of differing complexity as well as segmentation measures, the new scheme provides more information than the evaluations from previous years. With this new information, however, comes new interpretive challenges. What are the correlations among different songs and, more importantly, different submissions across the new measures? Performance falls off for all submissions as the vocabularies increase in complexity, but does it do so directly in proportion to the number of more complex chords, or are certain algorithms indeed more robust? What are the outliers, song-algorithm pairs where the performance was substantially higher or lower than would be predicted, and how can they be explained? Answering these questions requires moving beyond the Friedman tests that have most often been used to compare algorithms to a richer underlying model. We propose a logistic-regression approach for generating comparative statistics for audio chord estimation, supported with generalised estimating equations to correct for repeated measures. We use the MIREX ACE 2013 results as a case study to illustrate our proposed method, including some of interesting aspects of the evaluation that might not apparent from the headline results alone.",NLD,education,Developed economies,"[-9.3520355, 53.823997]","[-29.742231, 22.859358]","[-33.40223, -1.1511334, -1.9071777]","[-27.842415, -3.0584588, 4.399774]","[13.335901, 4.9123755]","[6.2617707, 3.596402]","[14.805215, 11.347153, -1.4175506]","[9.838238, 8.746961, 12.3154545]"
83,Sanghoon Jun;Seungmin Rho;Eenjun Hwang,Geographical Region Mapping Scheme Based on Musical Preferences.,2014,https://doi.org/10.5281/zenodo.1418011,Sanghoon Jun+Korea University>KOR>education|Korea University>KOR>education;Seungmin Rho+Sungkyul University>KOR>education;Eenjun Hwang+Korea University>KOR>education,"Many countries and cities in the world tend to have different types of preferred or popular music, such as pop, K-pop, and reggae. Music-related applications utilize geographical proximity for evaluating the similarity of music preferences between two regions. Sometimes, this can lead to incorrect results due to other factors such as culture and religion. To solve this problem, in this paper, we propose a scheme for constructing a music map in which regions are positioned close to one another depending on the similarity of the musical preferences of their populations. That is, countries or cities in a traditional map are rearranged in the music map such that regions with similar musical preferences are close to one another. To do this, we collect users’ music play history and extract popular artists and tag information from the collected data. Similarities among regions are calculated using the tags and their frequencies. And then, an iterative algorithm for rearranging the regions into a music map is applied. We present a method for constructing the music map along with some experimental results.",KOR,education,Developing economies,"[-29.602306, 17.788536]","[43.285316, 13.337641]","[-19.181984, -7.026609, 3.05898]","[19.361832, -4.3141136, 20.529345]","[14.289229, 8.581541]","[11.739452, 2.407743]","[14.49776, 14.860463, -1.249362]","[13.042645, 5.8876, 12.242523]"
93,Xiao Hu 0001;Jin Ha Lee;Leanne Ka Yan Wong,Music Information Behaviors and System Preferences of University Students in Hong Kong.,2014,https://doi.org/10.5281/zenodo.1414778,Xiao Hu+University of Hong Kong>HKG>education|University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education;Leanne Ka Yan Wong+University of Hong Kong>HKG>education,"This paper presents a user study on music information needs and behaviors of university students in Hong Kong. A mix of quantitative and qualitative methods was used. A survey was completed by 101 participants and supplemental interviews were conducted in order to investigate users’ music information related activities. We found that university students in Hong Kong listened to music frequently and mainly for the purposes of entertainment, singing and playing instruments, and stress reduction. This user group often searches for music with multiple methods, but common access points like genre and time period were rarely used. Sharing music with people in their online social networks such as Facebook and Weibo was a common activity. Furthermore, the popularity of smartphones prompted the need for streaming music and mobile music applications. We also examined users’ preferences on music services available in Hong Kong such as YouTube and KKBox, as well as the characteristics liked and disliked by the users. The results not only offer insights into non-Western users’ music behaviors but also for designing online music services for young music listeners in Hong Kong.",HKG,education,Developing economies,"[-38.80768, 23.76749]","[39.57088, 30.588184]","[-17.66466, 18.107464, -8.680236]","[8.26892, 12.215849, 18.42701]","[15.269144, 8.768866]","[12.918212, 1.0379201]","[15.169694, 15.230819, -1.4921591]","[13.291728, 4.2802587, 12.099326]"
53,Marcelo F. Caetano;Frans Wiering,Theoretical Framework of A Computational Model of Auditory Memory for Music Emotion Recognition.,2014,https://doi.org/10.5281/zenodo.1417211,Marcelo Caetano+INESC TEC>PRT>facility;Frans Wiering+Utrecht University>NLD>education,"The bag of frames (BOF) approach commonly used in music emotion recognition (MER) has several limitations. The semantic gap is believed to be responsible for the glass ceiling on the performance of BOF MER systems. However, there are hardly any alternative proposals to address it. In this article, we introduce the theoretical framework of a computational model of auditory memory that incorporates temporal information into MER systems. We advocate that the organization of auditory memory places time at the core of the link between musical meaning and musical emotions. The main goal is to motivate MER researchers to develop an improved class of systems capable of overcoming the limitations of the BOF approach and coping with the inherent complexity of musical emotions.",PRT,facility,Developed economies,"[-60.764217, 2.8117578]","[52.033363, -9.629799]","[-27.13196, 27.553398, 1.0859377]","[7.6373544, 24.099962, 6.194919]","[14.003308, 12.819759]","[13.109322, 4.1453485]","[16.174341, 14.469264, 1.8053453]","[14.1669, 4.9965076, 10.319234]"
81,Po-Kai Yang;Chung-Chien Hsu;Jen-Tzung Chien,Bayesian Singing-Voice Separation.,2014,https://doi.org/10.5281/zenodo.1417373,Po-Kai Yang+National Chiao Tung University>TWN>education;Chung-Chien Hsu+National Chiao Tung University>TWN>education;Jen-Tzung Chien+National Chiao Tung University>TWN>education,"This paper presents a Bayesian nonnegative matrix factorization (NMF) approach to extract singing voice from background music accompaniment. Using this approach, the likelihood function based on NMF is represented by a Poisson distribution and the NMF parameters, consisting of basis and weight matrices, are characterized by the exponential priors. A variational Bayesian expectation-maximization algorithm is developed to learn variational parameters and model parameters for monaural source separation. A clustering algorithm is performed to establish two groups of bases: one is for singing voice and the other is for background music. Model complexity is controlled by adaptively selecting the number of bases for different mixed signals according to the variational lower bound. Model regularization is tackled through the uncertainty modeling via variational inference based on marginal likelihood. The experimental results on MIR-1K database show that the proposed method performs better than various unsupervised separation algorithms in terms of the global normalized source to distortion ratio.",TWN,education,Developing economies,"[-0.55071944, -42.87914]","[-46.52514, -24.514992]","[24.825089, 7.109467, -6.1480827]","[-5.2392855, -11.610683, -28.564014]","[9.191114, 10.756553]","[6.3089943, 4.9347596]","[10.906454, 14.643985, 1.2671975]","[9.717821, 8.775337, 9.894474]"
51,Jens Madsen;Bjørn Sand Jensen;Jan Larsen,Modeling Temporal Structure in Music for Emotion Prediction using Pairwise Comparisons.,2014,https://doi.org/10.5281/zenodo.1416384,Jens Madsen+Technical University of Denmark>DNK>education;Bjørn Sand Jensen+Technical University of Denmark>DNK>education;Jan Larsen+Technical University of Denmark>DNK>education,"The temporal structure of music is essential for the cognitive processes related to the emotions expressed in music. However, such temporal information is often disregarded in typical Music Information Retrieval modeling tasks of predicting higher-level cognitive or semantic aspects of music such as emotions, genre, and similarity. This paper addresses the specific hypothesis whether temporal information is essential for predicting expressed emotions in music, as a prototypical example of a cognitive aspect of music. We propose to test this hypothesis using a novel processing pipeline: 1) Extracting audio features for each track resulting in a multivariate ""feature time series"". 2) Using generative models to represent these time series (acquiring a complete track representation). Specifically, we explore the Gaussian Mixture model, Vector Quantization, Autoregressive model, Markov and Hidden Markov models. 3) Utilizing the generative models in a discriminative setting by selecting the Probability Product Kernel as the natural kernel for all considered track representations. We evaluate the representations using a kernel based model specifically extended to support the robust two-alternative forced choice self-report paradigm, used for eliciting expressed emotions in music. The methods are evaluated using two data sets and show increased predictive performance using temporal information, thus supporting the overall hypothesis.",DNK,education,Developed economies,"[-58.360935, 2.621926]","[46.56222, -10.627727]","[-24.864532, 24.083052, 2.5192692]","[8.207678, 24.568966, -0.5603193]","[13.899837, 12.826184]","[11.894201, 4.0905776]","[16.18307, 14.553189, 1.7743499]","[13.539628, 5.6474714, 10.251049]"
22,Brecht De Man;Brett Leonard;Richard L. King;Joshua D. Reiss,An Analysis and Evaluation of Audio Features for Multitrack Music Mixtures.,2014,https://doi.org/10.5281/zenodo.1416832,"Brecht De Man+Centre for Digital Music, Queen Mary University of London>GBR>education;Brett Leonard+The Graduate Program in Sound Recording, Schulich School of Music, McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology>CAN>facility;Richard King+The Graduate Program in Sound Recording, Schulich School of Music, McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology>CAN>facility;Joshua D. Reiss+Centre for Digital Music, Queen Mary University of London>GBR>education","Mixing multitrack music is an expert task where characteristics of the individual elements and their sum are manipulated in terms of balance, timbre and positioning, to resolve technical issues and to meet the creative vision of the artist or engineer. In this paper we conduct a mixing experiment where eight songs are each mixed by eight different engineers. We consider a range of features describing the dynamic, spatial and spectral characteristics of each track, and perform a multidimensional analysis of variance to assess whether the instrument, song and/or engineer is the determining factor that explains the resulting variance, trend, or consistency in mixing methodology. A number of assumed mixing rules from literature are discussed in the light of this data, and implications regarding the automation of various mixing processes are explored. Part of the data used in this work is published in a new online multitrack dataset through which public domain recordings, mixes, and mix settings (DAW projects) can be shared.",GBR,education,Developed economies,"[4.470991, -23.274391]","[-35.95737, -21.550058]","[19.99325, -3.2437062, -6.9123664]","[-19.535244, 1.7819176, -25.897203]","[12.558881, 8.732218]","[7.384489, 6.2248197]","[12.082014, 13.360751, 0.683627]","[9.425407, 7.545334, 9.517487]"
21,Matevz Pesek;Ales Leonardis;Matija Marolt,A Compositional Hierarchical Model for Music Information Retrieval.,2014,https://doi.org/10.5281/zenodo.1416084,Matevž Pesek+University of Ljubljana>SVN>education;Aleš Leonardis+University of Birmingham>GBR>education;Matija Marolt+University of Ljubljana>SVN>education,"This paper presents a biologically-inspired compositional hierarchical model for MIR. The model can be treated as a deep learning model, and poses an alternative to deep architectures based on neural networks. Its main features are generativeness and transparency that allow clear insight into concepts learned from the input music signals. The model consists of multiple layers, each is composed of a number of parts. The hierarchical nature of the model corresponds well with the hierarchical structures in music. Parts in lower layers correspond to low-level concepts (e.g. tone partials), while parts in higher layers combine lower-level representations into more complex concepts (tones, chords). The layers are unsupervisedly learned one-by-one from music signals. Parts in each layer are compositions of parts from previous layers based on statistical co-occurrences as the driving force of the learning process. We present the model’s structure and compare it to other deep architectures. A preliminary evaluation of the model’s usefulness for automated chord estimation and multiple fundamental frequency estimation tasks is provided. Additionally, we show how the model can be extended to event-based music processing, which is our final goal.",SVN,education,Developed economies,"[-15.304761, 17.56165]","[-9.342931, -37.801693]","[-9.56666, 7.617625, -10.97344]","[-23.850668, 4.16563, -10.411271]","[13.595629, 8.352524]","[9.141123, 5.93356]","[13.845542, 14.663195, -1.617776]","[9.788945, 5.66762, 9.06606]"
20,Marius Miron;Julio José Carabias-Orti;Jordi Janer,Audio-to-score Alignment at the Note Level for Orchestral Recordings.,2014,https://doi.org/10.5281/zenodo.1416150,Marius Miron+Universitat Pompeu Fabra>ESP>education;Julio José Carabias-Orti+Universitat Pompeu Fabra>ESP>education;Jordi Janer+Universitat Pompeu Fabra>ESP>education,"In this paper we propose an offline method for refining audio-to-score alignment at the note level in the context of orchestral recordings. State-of-the-art score alignment systems estimate note onsets with a low time resolution, and without detecting note offsets. For applications such as score-informed source separation we need a precise alignment at note level. Thus, we propose a novel method that refines alignment by determining the note onsets and offsets in complex orchestral mixtures by combining audio and image processing techniques. First, we introduce a note-wise pitch salience function that weighs the harmonic contribution according to the notes present in the score. Second, we perform image binarization and blob detection based on connectivity rules. Then, we pick the best combination of blobs, using dynamic programming. We finally obtain onset and offset times from the boundaries of the most salient blob. We evaluate our method on a dataset of Bach chorales, showing that the proposed approach can accurately estimate note onsets and offsets.",ESP,education,Developed economies,"[17.659182, -14.321518]","[-14.148942, -15.009366]","[2.569114, -13.925738, -8.529363]","[-3.1115873, -20.420254, -1.7543937]","[10.824025, 6.3414755]","[6.262876, 0.5473714]","[11.9291725, 12.544677, -1.4799744]","[8.193578, 5.703399, 10.82408]"
19,Antti Laaksonen,Automatic Melody Transcription based on Chord Transcription.,2014,https://doi.org/10.5281/zenodo.1415218,Antti Laaksonen+University of Helsinki>FIN>education,"This paper focuses on automatic melody transcription in a situation where a chord transcription is already available. Given an excerpt of music in audio form and a chord transcription in symbolic form, the task is to create a symbolic melody transcription that consists of note onset times and pitches. We present an algorithm that divides the audio into segments based on the chord transcription, and then matches potential melody patterns to each segment. The algorithm uses chord information to favor melody patterns that are probable in the given harmony context. To evaluate the algorithm, we present a new ground truth dataset that consists of 1.5 hours of audio excerpts together with hand-made melody and chord transcriptions.",FIN,education,Developed economies,"[49.441254, -3.9186714]","[-25.509682, 22.021559]","[19.889326, -11.546407, 14.412288]","[-24.457102, 1.4757944, 4.864432]","[7.4198866, 8.835772]","[6.7355747, 3.377384]","[11.956807, 10.835324, 1.6383888]","[9.711371, 8.422248, 12.259892]"
18,Diego Furtado Silva;Rafael Geraldeli Rossi;Solange Oliveira Rezende;Gustavo Enrique De Almeida Prado Alves Batista,Music Classification by Transductive Learning Using Bipartite Heterogeneous Networks.,2014,https://doi.org/10.5281/zenodo.1418265,"Diego F. Silva+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Rafael G. Rossi+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Solange O. Rezende+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Gustavo E. A. P. A. Batista+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education","The popularization of music distribution in electronic format has increased the amount of music with incomplete metadata. The incompleteness of data can hamper some important tasks, such as music and artist recommendation. In this scenario, transductive classification can be used to classify the whole dataset considering just few labeled instances. Usually transductive classification is performed through label propagation, in which data are represented as networks and the examples propagate their labels through their connections. Similarity-based networks are usually applied to model data as network. However, this kind of representation requires the definition of parameters, which significantly affect the classification accuracy, and presents a high cost due to the computation of similarities among all dataset instances. In contrast, bipartite heterogeneous networks have appeared as an alternative to similarity-based networks in text mining applications. In these networks, the words are connected to the documents which they occur. Thus, there is no parameter or additional costs to generate such networks. In this paper, we propose the use of the bipartite network representation to perform transductive classification of music, using a bag-of-frames approach to describe music signals. We demonstrate that the proposed approach outperforms other music classification approaches when few labeled instances are available.",BRA,education,Developing economies,"[-18.44298, -10.397524]","[35.559227, 5.174101]","[1.2201371, 5.78929, 9.072681]","[16.170076, 4.6209526, 5.1521907]","[11.705133, 9.509146]","[11.59372, 2.673002]","[13.427379, 13.286842, 0.89582276]","[12.934219, 6.174689, 12.065756]"
17,Laurent Pugin;Rodolfo Zitellini;Perry Roland,Verovio: A library for Engraving MEI Music Notation into SVG.,2014,https://doi.org/10.5281/zenodo.1417589,Laurent Pugin+Swiss RISM Office>CHE>facility;Rodolfo Zitellini+Swiss RISM Office>CHE>facility;Perry Roland+University of Virginia>USA>education,"Rendering symbolic music notation is a common component of many MIR applications, and many tools are available for this task. There is, however, a need for a tool that can natively render the Music Encoding Initiative (MEI) notation encodings that are increasingly used in music research projects. In this paper, we present Verovio, a library and toolkit for rendering MEI. A significant advantage of Verovio is that it implements MEI’s structure internally, making it the best suited solution for rendering features that make MEI unique. Verovio is designed as a fast, portable, lightweight tool written in pure standard C++ with no dependencies on third-party frameworks or libraries. It can be used as a command-line rendering tool, as a library, or it can be compiled to JavaScript using the Emscripten LLVM-to-JavaScript compiler. This last option is particularly interesting because it provides a complete in-browser music MEI typesetter. The SVG output from Verovio is organized in such a way that the MEI structure is preserved as much as possible. Since every graphic in SVG is an XML element that is easily addressable, Verovio is particularly well-suited for interactive applications, especially in web browsers. Verovio is available under the GPL open-source license.",CHE,facility,Developed economies,"[10.465447, 24.06088]","[-0.93229353, 38.658356]","[-10.681948, -11.8563, 17.571627]","[-12.140331, -8.118725, 18.289267]","[12.086815, 6.8099246]","[9.801947, 0.23527628]","[13.59593, 12.305623, -1.4204205]","[10.315549, 4.995698, 11.729858]"
16,Srikanth Cherla;Tillman Weyde;Artur S. d'Avila Garcez,Multiple Viewpiont Melodic Prediction with Fixed-Context Neural Networks.,2014,https://doi.org/10.5281/zenodo.1416944,Srikanth Cherla+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Artur d’Avila Garcez+City University London>GBR>education,"The multiple viewpoints representation is an event-based representation of symbolic music data which offers a means for the analysis and generation of notated music. Previous work using this representation has predominantly relied on n-gram and variable order Markov models for music sequence modelling. Recently the efficacy of a class of distributed models, namely restricted Boltzmann machines, was demonstrated for this purpose. In this paper, we demonstrate the use of two neural network models which use fixed-length sequences of various viewpoint types as input to predict the pitch of the next note in the sequence. The predictive performance of each of these models is comparable to that of models previously evaluated on the same task. We then combine the predictions of individual models using an entropy-weighted combination scheme to improve the overall prediction performance, and compare this with the predictions of a single equivalent model which takes as input all the viewpoint types of each of the individual models in the combination.",GBR,education,Developed economies,"[14.716113, -4.833615]","[-6.7912946, -31.183685]","[7.2734327, 4.945209, 5.043914]","[-18.4577, -0.7026583, -3.8958068]","[10.891213, 9.633783]","[8.811907, 5.6802006]","[11.871942, 15.060745, -0.58727074]","[9.464156, 5.9253974, 9.385363]"
15,Tom Arjannikov;John Z. Zhang,An Association-based Approach to Genre Classification in Music.,2014,https://doi.org/10.5281/zenodo.1415786,Tom Arjannikov+University of Lethbridge>CAN>education;John Z. Zhang+University of Lethbridge>CAN>education,"Music Information Retrieval (MIR) is a multi-disciplinary research area that aims to automate the access to large-volume music data, including browsing, retrieval, storage, etc. The work that we present in this paper tackles a non-trivial problem in the field, namely music genre classification, which is one of the core tasks in MIR. In our proposed approach, we make use of association analysis to study and predict music genres based on the acoustic features extracted directly from music. In essence, we build an associative classifier, which finds inherent associations between content-based features and individual genres and then uses them to predict the genre(s) of a new music piece. We demonstrate the feasibility of our approach through a series of experiments using two publicly available music datasets. One of them is the largest available in MIR and contains real world data, while the other has been widely used and provides a good benchmarking basis. We show the effectiveness of our approach and discuss various related issues. In addition, due to its associative nature, our classifier can assign multiple genres to a single music piece; hopefully this would offer insights into the prevalent multi-label situation in genre classification.",CAN,education,Developed economies,"[-30.144215, -12.46789]","[30.314865, -0.27492464]","[-17.890049, 4.776678, 15.01525]","[15.395755, 8.19118, 1.6187499]","[13.095971, 10.821997]","[10.646731, 3.3513103]","[14.038881, 14.341198, 1.3691801]","[12.215121, 6.3739176, 11.126146]"
14,Bob L. Sturm;Rolf Bardeli;Thibault Langlois;Valentin Emiya,Formalizing the Problem of Music Description.,2014,https://doi.org/10.5281/zenodo.1414914,Bob L. Sturm+Aalborg University>DNK>education;Rolf Bardeli+Fraunhofer IAIS>DEU>facility;Thibault Langlois+Lisbon University>PRT>education;Valentin Emiya+Aix-Marseille Université>FRA>education,"The lack of a formalism for “the problem of music description” results in, among other things: ambiguity in what problem a music description system must address, how it should be evaluated, what criteria define its success, and the paradox that a music description system can reproduce the “ground truth” of a music dataset without attending to the music it contains. To address these issues, we formalize the problem of music description such that all elements of an instance of it are made explicit. This can thus inform the building of a system, and how it should be evaluated in a meaningful way. We provide illustrations of this formalism applied to three examples drawn from the literature.",DNK,education,Developed economies,"[4.3120217, 8.746154]","[-3.7637098, 18.058556]","[-11.427168, -3.6884847, 3.257665]","[-4.593858, -1.5494908, 6.951092]","[12.469738, 8.462532]","[9.088703, 1.9475595]","[13.270474, 13.811707, -1.0071988]","[10.469791, 6.192002, 11.808877]"
12,Yoonchang Han;Kyogu Lee,Hierarchical Approach to Detect Common Mistakes of Beginner Flute Players.,2014,https://doi.org/10.5281/zenodo.1415878,Yoonchang Han+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"Music lessons are a repetitive process of giving feedback on a student’s performance techniques. The manner in which performance skills are improved depends on the particular instrument, and therefore, it is important to consider the unique characteristics of the target instrument. In this paper, we investigate the common mistakes of beginner flute players and propose a hierarchical approach to detect such mistakes. We first examine the structure and mechanism of the flute, and define several types of common mistakes that can be caused by incorrect assembly, poor blowing skills, or mis-fingering. We propose tailored algorithms for detecting each case by combining deterministic signal processing and deep learning, to quantify the quality of a flute sound. The system is structured hierarchically, as mis-fingering detection requires the input sound to be correctly assembled and blown to discriminate minor sound difference. Experimental results show that it is possible to identify different mistakes in flute performance using our proposed algorithms.",KOR,education,Developing economies,"[16.930948, -21.920923]","[-41.851818, 1.488053]","[10.445197, -15.336236, -0.066319786]","[-11.468109, 21.808178, -5.3091745]","[9.390451, 6.869135]","[7.9687448, 4.1618953]","[11.324294, 12.266978, -0.37399977]","[9.542078, 6.872391, 10.1507435]"
23,Karthik Yadati;Martha Larson;Cynthia C. S. Liem;Alan Hanjalic,Detecting Drops in Electronic Dance Music: Content based approaches to a socially significant music event.,2014,https://doi.org/10.5281/zenodo.1417081,Karthik Yadati+Delft University of Technology>NLD>education;Martha Larson+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education;Alan Hanjalic+Delft University of Technology>NLD>education,"Electronic dance music (EDM) is a popular genre of music. In this paper, we propose a method to automatically detect the characteristic event in an EDM recording that is referred to as a drop. Its importance is reflected in the number of users who leave comments in the general neighborhood of drop events in music on online audio distribution platforms like SoundCloud. The variability that characterizes realizations of drop events in EDM makes automatic drop detection challenging. We propose a two-stage approach to drop detection that first models the sound characteristics during drop events and then incorporates temporal structure by zeroing in on a watershed moment. We also explore the possibility of using the drop-related social comments on the SoundCloud platform as weak reference labels to improve drop detection. The method is evaluated using data from SoundCloud. Performance is measured as the overlap between tolerance windows centered around the hypothesized and the actual drop. Initial experimental results are promising, revealing the potential of the proposed method for combining content analysis and social activity to detect events in music recordings.",NLD,education,Developed economies,"[-36.190723, 14.915895]","[35.387115, 1.6089466]","[-30.149073, 1.6838635, -17.803944]","[25.091064, 12.052142, 5.331486]","[11.830709, 5.2623825]","[11.314102, 3.227334]","[11.681826, 13.834095, -2.3937411]","[12.790205, 6.1162066, 11.273736]"
11,Christopher Antila;Julie Cumming,The VIS Framework: Analyzing Counterpoint in Large Datasets.,2014,https://doi.org/10.5281/zenodo.1417767,Christopher Antila+McGill University>CAN>education;Julie Cumming+McGill University>CAN>education,"The VIS Framework for Music Analysis is a modular Python library designed for “big data” queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strategy in VIS, and the flexibility of this approach for future researchers.",CAN,education,Developed economies,"[19.408634, 34.442154]","[-3.9974017, 26.805935]","[-20.10801, -14.972826, 10.632452]","[-10.384669, 1.9946681, 12.506921]","[12.502477, 6.34199]","[9.062244, 1.8905567]","[14.231901, 12.659555, -0.8267501]","[10.3563595, 6.070028, 11.651435]"
9,Mathieu Giraud;Florence Levé;Florent Mercier;Marc Rigaudière;Donatien Thorez,Towards Modeling Texture in Symbolic Data.,2014,https://doi.org/10.5281/zenodo.1415030,"Mathieu Giraud+LIFL, CNRS>FRA>facility|University of Lille 1>FRA>education;Florence Léve+MIS, UPJV>FRA>education|LIFL, University of Lille 1>FRA>facility;Florent Mercier+University of Lille 1>FRA>education;Marc Rigaudière+University of Lorraine>FRA>education;Donatien Thorez+University of Lille 1>FRA>education","Studying texture is a part of many musicological analyses. The change of texture plays an important role in the cognition of musical structures. Texture is a feature commonly used to analyze musical audio data, but it is rarely taken into account in symbolic studies. We propose to formalize the texture in classical Western instrumental music as melody and accompaniment layers, and provide an algorithm able to detect homorhythmic layers in polyphonic data where voices are not separated. We present an evaluation of these methods for parallel motions against a ground truth analysis of ten instrumental pieces, including the first movements of the six quatuors op. 33 by Haydn.",FRA,facility,Developed economies,"[17.106148, 18.51604]","[-14.670194, 18.148914]","[-7.734685, -7.491153, 20.418068]","[-11.9710245, -6.7056594, 4.679574]","[11.817203, 7.153465]","[8.296694, 2.0728106]","[13.490187, 12.333842, -1.0288563]","[9.937731, 6.9013643, 11.948139]"
8,Siddharth Sigtia;Emmanouil Benetos;Srikanth Cherla;Tillman Weyde;Artur S. d'Avila Garcez;Simon Dixon,An RNN-based Music Language Model for Improving Automatic Music Transcription.,2014,https://doi.org/10.5281/zenodo.1416792,"Siddharth Sigtia+Centre for Digital Music, Queen Mary University of London>GBR>education;Emmanouil Benetos+City University London>GBR>education;Srikanth Cherla+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Artur S. d’Avila Garcez+City University London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a significant 3% improvement in terms of F-measure, when compared to using an acoustic-only model.",GBR,education,Developed economies,"[26.553837, -6.260376]","[-9.739962, -31.806587]","[11.436114, -1.614959, 13.299913]","[-16.504911, 1.8123974, -6.2916965]","[9.557688, 7.9643455]","[8.840705, 5.5149355]","[12.106624, 12.067612, 0.21270981]","[9.580272, 6.012765, 9.404263]"
7,Sam van Herwaarden;Maarten Grachten;W. Bas de Haas,Predicting Expressive Dynamics in Piano Performances using Neural Networks.,2014,https://doi.org/10.5281/zenodo.1416678,Sam van Herwaarden+Austrian Research Institute for AI>AUT>facility;Maarten Grachten+Austrian Research Institute for AI>AUT>facility;W. Bas de Haas+Utrecht University>NLD>education,"This paper presents a model for predicting expressive accentuation in piano performances with neural networks. Using Restricted Boltzmann Machines (RBMs), features are learned from performance data, after which these features are used to predict performed loudness. During feature learning, data describing more than 6000 musical pieces is used; when training for prediction, two datasets are used, both recorded on a Bösendorfer piano (accurately measuring note on- and offset times and velocity values), but describing different compositions performed by different pianists. The resulting model is tested by predicting note velocity for unseen performances. Our approach differs from earlier work in a number of ways: (1) an additional input representation based on a local history of velocity values is used, (2) the RBMs are trained to result in a network with sparse activations, (3) network connectivity is increased by adding skip-connections, and (4) more data is used for training. These modifications result in a network performing better than the state-of-the-art on the same data and more descriptive features, which can be used for rendering performances, or for gaining insight into which aspects of a musical piece influence its performance.",AUT,facility,Developed economies,"[32.327393, -1.9815427]","[-28.695578, -22.98721]","[14.591836, -0.8399984, 20.681883]","[-8.996725, -12.349029, -10.58067]","[10.020756, 7.1796746]","[7.9836287, 5.6792006]","[12.303079, 11.678985, -0.52086055]","[9.013047, 6.3636384, 9.026684]"
6,Isabel Barbancho;George Tzanetakis;Lorenzo J. Tardón;Peter F. Driessen;Ana M. Barbancho,Estimation of the Direction of Strokes and Arpeggios.,2014,https://doi.org/10.5281/zenodo.1418053,Isabel Barbancho+Universidad de Málaga>ESP>education|University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Peter F. Driessen+University of Victoria>CAN>education;Ana M. Barbancho+Universidad de Málaga>ESP>education,"Whenever a chord is played in a musical instrument, the notes are not commonly played at the same time. Actually, in some instruments, it is impossible to trigger multiple notes simultaneously. In others, the player can consciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke). In this paper, we describe a system to automatically estimate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analysis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direction, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ.",ESP,education,Developed economies,"[34.14055, -48.741005]","[-35.41508, 16.399607]","[27.808353, -19.59413, 6.459522]","[-26.650335, -10.614306, -1.327192]","[7.7766376, 7.8517723]","[6.668807, 3.6927106]","[11.083874, 11.453938, 1.0954623]","[9.51345, 8.580957, 11.7471695]"
5,Harald Grohganz;Michael Clausen;Meinard Müller,Estimating Musical Time Information from Performed MIDI Files.,2014,https://doi.org/10.5281/zenodo.1417925,Harald Grohganz+Bonn University>DEU>education;Michael Clausen+Bonn University>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Even though originally developed for exchanging control commands between electronic instruments, MIDI has been used as quasi standard for encoding and storing score-related parameters. MIDI allows for representing musical time information as specified by sheet music as well as physical time information that reflects performance aspects. However, in many of the available MIDI files the musical beat and tempo information is set to a preset value with no relation to the actual music content. In this paper, we introduce a procedure to determine the musical beat grid from a given performed MIDI file. As one main contribution, we show how the global estimate of the time signature can be used to correct local errors in the pulse grid estimation. Different to MIDI quantization, where one tries to map MIDI note onsets onto a given musical pulse grid, our goal is to actually estimate such a grid. In this sense, our procedure can be used in combination with existing MIDI quantization procedures to convert performed MIDI files into semantically enriched score-like MIDI files.",DEU,education,Developed economies,"[38.51033, -22.981339]","[-28.057676, 1.3872453]","[0.60560447, -24.254122, -0.1342827]","[-11.247236, 5.4385276, -7.195]","[11.1676855, 5.2822857]","[5.995693, 1.3797653]","[11.291406, 12.9591055, -2.3869221]","[8.244769, 6.3757424, 11.114101]"
4,Aäron van den Oord;Sander Dieleman;Benjamin Schrauwen,Transfer Learning by Supervised Pre-training for Audio-based Music Classification.,2014,https://doi.org/10.5281/zenodo.1415890,A¨aron van den Oord+Ghent University>BEL>education;Sander Dieleman+Ghent University>BEL>education;Benjamin Schrauwen+Ghent University>BEL>education,"Very few large-scale music research datasets are publicly available. There is an increasing need for such datasets, because the shift from physical to digital distribution in the music industry has given the listener access to a large body of music, which needs to be cataloged efficiently and be easily browsable. Additionally, deep learning and feature learning techniques are becoming increasingly popular for music information retrieval applications, and they typically require large amounts of training data to work well. In this paper, we propose to exploit an available large-scale music dataset, the Million Song Dataset (MSD), for classification tasks on other datasets, by reusing models trained on the MSD for feature extraction. This transfer learning approach, which we refer to as supervised pre-training, was previously shown to be very effective for computer vision problems. We show that features learned from MSD audio fragments in a supervised manner, using tag labels and user listening data, consistently outperform features learned in an unsupervised manner in this setting, provided that the learned feature extractor is of limited complexity. We evaluate our approach on the GTZAN, 1517-Artists, Unique and Magnatagatune datasets.",BEL,education,Developed economies,"[-17.573154, -11.508713]","[-22.676544, -36.809734]","[2.7495089, 3.41221, 9.660564]","[-6.1310115, -0.079751946, -19.051033]","[11.438701, 9.4753065]","[9.79129, 4.5830584]","[13.21664, 13.12129, 0.9342591]","[10.930699, 6.5348015, 9.1220045]"
3,Bob L. Sturm;Nick Collins,The Kiki-Bouba Challenge: Algorithmic Composition for Content-based MIR Research and Development.,2014,https://doi.org/10.5281/zenodo.1416364,Bob L. Sturm+Aalborg University>DNK>education;Nick Collins+Durham University>GBR>education,"We propose the “Kiki-Bouba Challenge” (KBC) for the research and development of content-based music information retrieval (MIR) systems. This challenge is unencumbered by several problems typically encountered in MIR research: insufficient data, restrictive copyrights, imperfect ground truth, a lack of specific criteria for classes (e.g., genre), a lack of explicit problem definition, and irreproducibility. KBC provides a limitless amount of free data, a perfect ground truth, and well-specifiable and meaningful characteristics defining each class. These ideal conditions are made possible by open source algorithmic composition — a hitherto under-exploited resource for MIR.",DNK,education,Developed economies,"[-9.542037, 56.26595]","[17.212372, 30.18678]","[-37.63, -2.3400571, -2.7583022]","[-0.020309137, 1.1664671, 16.618923]","[13.641969, 4.7121134]","[11.149525, 0.66505384]","[15.01681, 11.11377, -1.4794668]","[11.680532, 5.0504007, 12.020959]"
2,Münevver Köküer;Peter Jancovic;Islah Ali-MacLachlan;Cham Athwal,Automated Detection of Single- and Multi-Note Ornaments in Irish Traditional Flute Playing.,2014,https://doi.org/10.5281/zenodo.1415266,Münevver Köküer+Birmingham City University>GBR>education|University of Birmingham>GBR>education;Peter Jančovič+Birmingham City University>GBR>education|University of Birmingham>GBR>education;Islah Ali-MacLachlan+Birmingham City University>GBR>education|University of Birmingham>GBR>education;Cham Athwal+Birmingham City University>GBR>education|University of Birmingham>GBR>education,"This paper presents an automatic system for the detection of single- and multi-note ornaments in Irish traditional flute playing. This is a challenging problem because ornaments are notes of a very short duration. The presented ornament detection system is based on first detecting onsets and then exploiting the knowledge of musical ornamentation. We employed onset detection methods based on signal envelope and fundamental frequency and customised their parameters to the detection of soft onsets of possibly short duration. Single-note ornaments are detected based on the duration and pitch of segments, determined by adjacent onsets. Multi-note ornaments are detected based on analysing the sequence of segments. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD, which was manually annotated by an experienced flute player. The onset and single- and multi-note ornament detection performance is presented in terms of the precision, recall and F-measure.",GBR,education,Developed economies,"[18.601831, -20.626348]","[-14.5353985, -3.042901]","[12.150059, -12.836172, 1.4227585]","[2.792054, 2.946508, -13.477967]","[9.155617, 7.0472536]","[6.1318197, 2.6964252]","[11.256659, 12.303123, -0.13167733]","[8.48611, 7.479261, 10.789186]"
1,Li Su;Li-Fan Yu;Yi-Hsuan Yang,"Sparse Cepstral, Phase Codes for Guitar Playing Technique Classification.",2014,https://doi.org/10.5281/zenodo.1417215,Li Su+Academia Sinica>TWN>education;Li-Fan Yu+Academia Sinica>TWN>education;Yi-Hsuan Yang+Academia Sinica>TWN>education,"Automatic recognition of guitar playing techniques is challenging as it is concerned with subtle nuances of guitar timbres. In this paper, we investigate this research problem by a comparative study on the performance of features extracted from the magnitude spectrum, cepstrum and phase derivatives such as group-delay function (GDF) and instantaneous frequency deviation (IFD) for classifying the playing techniques of electric guitar recordings. We consider up to 7 distinct playing techniques of electric guitar and create a new individual-note dataset comprising of 7 types of guitar tones for each playing technique. The dataset contains 6,580 clips and 11,928 notes. Our evaluation shows that sparse coding is an effective means of mining useful patterns from the primitive time-frequency representations and that combining the sparse representations of logarithm cepstrum, GDF and IFD leads to the highest average F-score of 71.7%. Moreover, from analyzing the confusion matrices we find that cepstral and phase features are particularly important in discriminating highly similar techniques such as pull-off, hammer-on and bending. We also report a preliminary study that demonstrates the potential of the proposed methods in automatic transcription of real-world electric guitar solos.",TWN,education,Developing economies,"[49.313812, -12.30717]","[-42.191788, -4.7554235]","[28.06666, -7.7460213, 6.436249]","[-15.778872, -8.178085, -1.4520143]","[7.714334, 8.145172]","[7.492161, 4.014907]","[11.813165, 11.464273, 1.503096]","[10.325863, 7.473333, 10.423123]"
52,Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo,Musical Structural Analysis Database Based on GTTM.,2014,https://doi.org/10.5281/zenodo.1415658,Masatoshi Hamanaka+Kyoto University>JPN>education;Keiji Hirata+Future University Hakodate>JPN>education;Satoshi Tojo+JAIST>JPN>education,"This paper, we present the publication of our analysis data and analyzing tool based on the generative theory of tonal music (GTTM). Musical databases such as score databases, instrument sound databases, and musical pieces with standard MIDI files and annotated data are key to advancements in the field of music information technology. We started implementing the GTTM on a computer in 2004 and ever since have collected and publicized test data by musicologists in a step-by-step manner. In our efforts to further advance the research on musical structure analysis, we are now publicizing 300 pieces of analysis data as well as the analyzer. Experiments showed that for 267 of 300 pieces the analysis results obtained by a new musicologist were almost the same as the original results in the GTTM database and that the other 33 pieces had different interpretations.",JPN,education,Developed economies,"[-2.1969652, 2.701388]","[-9.942681, 22.81199]","[-3.0889487, -5.8299522, -1.7625914]","[-13.757662, 1.156819, 9.987315]","[12.004765, 8.354684]","[8.533379, 1.8204669]","[12.734383, 13.906845, -0.46748984]","[10.020946, 6.2678747, 11.860605]"
10,Nadine Kroher;Emilia Gómez;Catherine Guastavino;Francisco Gómez 0001;Jordi Bonada,Computational Models for Perceived Melodic Similarity in A Cappella Flamenco Singing.,2014,https://doi.org/10.5281/zenodo.1416306,N. Kroher+Universitat Pompeu Fabra>ESP>education|McGill University>CAN>education|Technical University of Madrid>ESP>education;E. Gómez+Universitat Pompeu Fabra>ESP>education|McGill University>CAN>education|Technical University of Madrid>ESP>education;C. Guastavino+McGill University>CAN>education;F. Gómez+Technical University of Madrid>ESP>education;J. Bonada+Universitat Pompeu Fabra>ESP>education,"The present study investigates the mechanisms involved in the perception of melodic similarity in the context of a cappella flamenco singing performances. Flamenco songs belonging to the same style are characterized by a common melodic skeleton, which is subject to spontaneous improvisation containing strong prolongations and ornamentations. For our research we collected human similarity judgements from naïve and expert listeners who listened to audio recordings of a cappella flamenco performances as well as synthesized versions of the same songs. We furthermore calculated distances from manually extracted high-level descriptors defined by flamenco experts. The suitability of a set of computational melodic similarity measures was evaluated by analyzing the correlation between computed similarity and human ratings. We observed significant differences between listener groups and stimuli types. Furthermore, we observed a high correlation between human ratings and similarities computed from features from flamenco experts. We also observed that computational models based on temporal deviation, dynamics and ornamentation are better suited to model perceived similarity for this material than models based on chroma distance.",ESP,education,Developed economies,"[0.30082732, -14.938607]","[16.981451, 1.5650066]","[14.595613, 5.8482876, -22.23334]","[5.349998, 5.3797107, 4.754273]","[11.613895, 9.990905]","[9.498328, 2.0209348]","[12.286816, 15.33014, -0.4814115]","[11.366692, 7.0688357, 12.963115]"
24,Nikolay Glazyrin,Towards Automatic Content-Based Separation of DJ Mixes into Single Tracks.,2014,https://doi.org/10.5281/zenodo.1415566,Nikolay Glazyrin+Ural Federal University>RUS>education,"DJ mixes and radio show recordings constitute an important and underexploited music and data source. In this paper we try to approach the problem of separation of a continuous DJ mix into single tracks or timestamping a mix. Sharing some aspects with the task of structural segmentation, this problem has a number of distinctive features that make difficulties for structural segmentation algorithms designed to work with a single track. We use the information derived from spectrum data to separate tracks from each other. We show that the metadata that usually comes with DJ mixes can be exploited to improve the separation. An iterative algorithm that can consider both content-based data and user provided metadata is proposed and evaluated on a collection of freely available timestamped DJ mix recordings of various styles.",RUS,education,Economies in transition,"[17.55354, -39.919724]","[-13.537543, -19.866673]","[9.990129, -26.473806, -16.113342]","[0.47822195, 15.3355465, -14.722444]","[9.857033, 4.6691523]","[8.114505, 2.614582]","[10.699335, 12.174432, -1.8311428]","[10.346567, 6.9030566, 11.123739]"
13,Siying Wang;Sebastian Ewert;Simon Dixon,Robust Joint Alignment of Multiple Versions of a Piece of Music.,2014,https://doi.org/10.5281/zenodo.1416382,Siying Wang+Queen Mary University of London>GBR>education;Sebastian Ewert+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Large music content libraries often comprise multiple versions of a piece of music. To establish a link between different versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Due to the leeway in interpreting a piece, any two versions can differ significantly, for example, in terms of local tempo, articulation, or playing style. For a given pair of versions, these differences can be significant such that even state-of-the-art methods fail to identify a correct alignment. In this paper, we present a novel method that increases the robustness for difficult to align cases. Instead of aligning only pairs of versions as done in previous methods, our method aligns multiple versions in a joint manner. This way, the alignment can be computed by comparing each version not only with one but with several versions, which stabilizes the comparison and leads to an increase in alignment robustness. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state-of-the-art method, with significantly less outliers (standard deviation 53% lower).",GBR,education,Developed economies,"[16.04664, -12.007432]","[-15.703936, -14.667764]","[-0.6689246, -9.801129, -6.3771973]","[-0.6186354, -21.779444, -0.6072556]","[11.024681, 6.528494]","[6.217828, 0.7226549]","[12.081541, 12.728201, -1.5336628]","[8.116093, 5.8531957, 10.909758]"
26,Zheng Tang;Dawn A. A. Black,Melody Extraction from Polyphonic Audio of Western Opera: A Method based on Detection of the Singer's Formant.,2014,https://doi.org/10.5281/zenodo.1416714,Zheng Tang+University of Washington>USA>education;Dawn A. A. Black+Queen Mary University of London>GBR>education,"Current melody extraction approaches perform poorly on the genre of opera [1, 2]. The singer’s formant is defined as a prominent spectral-envelope peak around 3 kHz found in the singing of professional Western opera singers [3]. In this paper we introduce a novel melody extraction algorithm based on this feature for opera signals. At the front end, it automatically detects the singer’s formant according to the Long-Term Average Spectrum (LTAS). This detection function is also applied to the short-term spectrum in each frame to determine the melody. The Fan Chirp Transform (FChT) [4] is used to compute pitch salience as its high time-frequency resolution overcomes the difficulties introduced by vibrato. Subharmonic attenuation is adopted to handle octave errors which are common in opera vocals. We improve the FChT algorithm so that it is capable of correcting outliers in pitch detection. The performance of our method is compared to 5 state-of-the-art melody extraction algorithms on a newly created dataset and parts of the ADC2004 dataset. Our algorithm achieves an accuracy of 87.5% in singer’s formant detection. In the evaluation of melody extraction, it has the best performance in voicing detection (91.6%), voicing false alarm (5.3%) and overall accuracy (82.3%).",USA,education,Developed economies,"[5.1762843, -15.494906]","[-2.550066, -11.78495]","[17.851805, 2.5728087, -4.2152295]","[7.402288, -10.192943, -12.212015]","[9.934403, 9.999148]","[6.6341634, 2.5304475]","[10.94837, 14.682861, -0.14243439]","[8.984143, 7.995156, 11.225546]"
50,Rafael Caro Repetto;Xavier Serra,Creating a Corpus of Jingju (Beijing Opera) Music and Possibilities for Melodic Analysis.,2014,https://doi.org/10.5281/zenodo.1416030,Rafael Caro Repetto+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"Jingju (Beijing opera) is a Chinese traditional performing art form in which theatrical and musical elements are intimately combined. As an oral tradition, its musical dimension is the result of the application of a series of predefined conventions and it offers unique concepts for musicological research. Computational analyses of jingju music are still scarce, and only a few studies have dealt with it from an MIR perspective. In this paper we present the creation of a corpus of jingju music in the framework of the CompMusic project that is formed by audio, editorial metadata, lyrics and scores. We discuss the criteria followed for the acquisition of the data, describe the content of the corpus, and evaluate its suitability for computational and musicological research. We also identify several research problems that can take advantage of this corpus in the context of computational musicology, especially for melodic analysis, and suggest approaches for future work.",ESP,education,Developed economies,"[-6.5485926, -26.58286]","[-2.2137659, 13.9986315]","[9.642658, 8.818528, -16.32026]","[-2.064654, 9.543069, 6.8717613]","[10.773279, 10.717993]","[8.270005, 1.6095917]","[11.702711, 15.341219, -0.12668523]","[9.740352, 6.4443936, 11.964011]"
25,Rachel M. Bittner;Justin Salamon;Mike Tierney;Matthias Mauch;Chris Cannam;Juan Pablo Bello,MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research.,2014,https://doi.org/10.5281/zenodo.1417889,"Rachel Bittner+Music and Audio Research Lab, New York University>USA>education;Justin Salamon+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Mike Tierney+Music and Audio Research Lab, New York University>USA>education;Matthias Mauch+Centre for Digital Music, Queen Mary University of London>GBR>education;Chris Cannam+Centre for Digital Music, Queen Mary University of London>GBR>education;Juan Bello+Music and Audio Research Lab, New York University>USA>education","We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.",USA,education,Developed economies,"[-12.44938, 54.725254]","[-5.7716346, 8.635354]","[-35.911114, -0.20732747, 0.4499704]","[-1.3478032, 3.7114289, 8.277819]","[13.285658, 5.20221]","[8.9559555, 2.628936]","[14.659781, 11.553255, -1.3330044]","[10.542604, 6.4092712, 10.947582]"
49,Thomas Prätzlich;Meinard Müller,Frame-Level Audio Segmentation for Abridged Musical Works.,2014,https://doi.org/10.5281/zenodo.1418039,Thomas Prätzlich+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Large-scale musical works such as operas may last several hours and typically involve a huge number of musicians. For such compositions, one often finds different arrangements and abridged versions (often lasting less than an hour), which can also be performed by smaller ensembles. Abridged versions still convey the flavor of the musical work containing the most important excerpts and melodies. In this paper, we consider the task of automatically segmenting an audio recording of a given version into semantically meaningful parts. Following previous work, the general strategy is to transfer a reference segmentation of the original complete work to the given version. Our main contribution is to show how this can be accomplished when dealing with strongly abridged versions. To this end, opposed to previously suggested segment-level matching procedures, we adapt a frame-level matching approach for transferring the reference segment information to the unknown version. Considering the opera “Der Freischütz” as an example scenario, we discuss how to balance out flexibility and robustness properties of our proposed frame-level segmentation procedure.",DEU,facility,Developed economies,"[-1.7964721, -7.8662195]","[-0.8189936, -3.5411375]","[7.030554, -3.4980166, -8.871228]","[-2.5770807, -5.4642453, -3.4748993]","[11.642053, 8.345482]","[6.9726963, 1.5363078]","[12.568758, 14.179029, 0.25974384]","[9.9545, 7.1250052, 11.330771]"
48,Kirill A. Sidorov;Andrew Jones;A. David Marshall,Music Analysis as a Smallest Grammar Problem.,2014,https://doi.org/10.5281/zenodo.1417653,Kirill Sidorov+Cardiff University>GBR>education;Andrew Jones+Cardiff University>GBR>education;David Marshall+Cardiff University>GBR>education,"In this paper we present a novel approach to music analysis, in which a grammar is automatically generated explaining a musical work’s structure. The proposed method is predicated on the hypothesis that the shortest possible grammar provides a model of the musical structure which is a good representation of the composer’s intent. The effectiveness of our approach is demonstrated by comparison of the results with previously-published expert analysis; our automated approach produces results comparable to human annotation. We also illustrate the power of our approach by showing that it is able to locate errors in scores, such as introduced by OMR or human transcription. Further, our approach provides a novel mechanism for intuitive high-level editing and creative transformation of music. A wide range of other possible applications exists, including automatic summarization and simplification; estimation of musical complexity and similarity, and plagiarism detection.",GBR,education,Developed economies,"[4.303197, 8.587776]","[-12.931275, 23.25996]","[-11.264286, -4.612051, 3.8011205]","[-8.109111, 0.9236936, 5.29305]","[12.314748, 8.324009]","[8.841623, 1.7911576]","[13.032223, 13.828345, -0.90642995]","[10.241583, 6.1371646, 11.841495]"
47,Dominique Fourer;Jean-Luc Rouas;Pierre Hanna;Matthias Robine,Automatic Instrument Classification of Ethnomusicological Audio Recordings.,2014,https://doi.org/10.5281/zenodo.1417655,Dominique Fourer+University of Bordeaux>FRA>education;Jean-Luc Rouas+University of Bordeaux>FRA>education;Pierre Hanna+University of Bordeaux>FRA>education;Matthias Robine+University of Bordeaux>FRA>education,"Automatic timbre characterization of audio signals can help to measure similarities between sounds and is of interest for automatic or semi-automatic databases indexing. The most effective methods use machine learning approaches which require qualitative and diversified training databases to obtain accurate results. In this paper, we introduce a diversified database composed of worldwide non-western instruments audio recordings on which is evaluated an effective timbre classification method. A comparative evaluation based on the well studied Iowa musical instruments database shows results comparable with those of state-of-the-art methods. Thus, the proposed method offers a practical solution for automatic ethnomusicological indexing of a database composed of diversified sounds with various quality. The relevance of audio features for the timbre characterization is also discussed in the context of non-western instruments analysis.",FRA,education,Developed economies,"[-28.170763, -0.9019929]","[18.389368, -2.9438572]","[-23.62578, -10.795928, -3.7096527]","[12.042533, 5.6515074, -1.4777132]","[14.014987, 8.676791]","[9.387267, 3.3232286]","[14.23526, 14.538392, -1.15273]","[11.110222, 7.1462426, 10.653723]"
45,Shrikant Venkataramani;Nagesh Nayak;Preeti Rao;Rajbabu Velmurugan,Vocal Separation using Singer-Vowel Priors Obtained from Polyphonic Audio.,2014,https://doi.org/10.5281/zenodo.1414974,Shrikant Venkataramani+IIT Bombay>IND>education|Sensibol Audio Technologies Pvt. Ltd.>IND>company;Nagesh Nayak+Sensibol Audio Technologies Pvt. Ltd.>IND>company;Preeti Rao+IIT Bombay>IND>education;Rajbabu Velmurugan+IIT Bombay>IND>education,"Single-channel methods for the separation of the lead vocal from mixed audio have traditionally included harmonic-sinusoidal modeling and matrix decomposition methods, each with its own strengths and shortcomings. In this work we use a hybrid framework to incorporate prior knowledge about singer and phone identity to achieve the superior separation of the lead vocal from the instrumental background. Singer specific dictionaries learned from available polyphonic recordings provide the soft mask that effectively attenuates the bleeding-through of accompanying melodic instruments typical of purely harmonic-sinusoidal model based separation. The dictionary learning uses NMF optimization across a training set of mixed signal utterances while keeping the vocal signal bases constant across the utterances. A soft mask is determined for each test mixed utterance frame by imposing sparseness constraints in the NMF partial co-factorization. We demonstrate significant improvements in reconstructed signal quality arising from the more accurate estimation of singer-vowel spectral envelope.",IND,education,Developing economies,"[-0.44887412, -43.48431]","[-46.30128, -30.054703]","[24.093523, 5.414231, -5.3874497]","[-4.609058, -8.969234, -27.931507]","[9.131115, 10.666771]","[6.4033914, 5.3066587]","[10.832698, 14.597763, 1.2438158]","[9.823203, 8.754887, 9.648248]"
44,Emilio Molina;Lorenzo J. Tardón;Isabel Barbancho;Ana M. Barbancho,The Importance of F0 Tracking in Query-by-singing-humming.,2014,https://doi.org/10.5281/zenodo.1416818,Emilio Molina+Universidad de Málaga>ESP>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Isabel Barbancho+Universidad de Málaga>ESP>education;Ana M. Barbancho+Universidad de Málaga>ESP>education,"In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of query-by-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baseline method. For the evaluation, we measured the QBSH performance for 189 different combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In addition, we also found clear differences in robustness to F0 transcription errors between different matchers.",ESP,education,Developed economies,"[-4.225199, 35.77356]","[1.2127433, -13.213746]","[-14.887075, -5.881152, -21.748743]","[3.8132195, -17.657145, 11.02039]","[14.863218, 6.145482]","[8.251438, 0.5331477]","[13.175881, 15.341637, -2.9212842]","[10.039297, 5.9200006, 12.906741]"
43,Anna M. Kruspe,Keyword Spotting in A-capella Singing.,2014,https://doi.org/10.5281/zenodo.1416870,Anna M. Kruspe+Fraunhofer IDMT>DEU>facility|Johns Hopkins University>USA>education,"Keyword spotting (or spoken term detection) is an interesting task in Music Information Retrieval that can be applied to a number of problems. Its purposes include topical search and improvements for genre classification. Keyword spotting is a well-researched task on pure speech, but state-of-the-art approaches cannot be easily transferred to singing because phoneme durations have much higher variations in singing. To our knowledge, no keyword spotting system for singing has been presented yet. We present a keyword spotting approach based on keyword-filler Hidden Markov Models (HMMs) and test it on a-capella singing and spoken lyrics. We test Mel-Frequency Cepstral Coefficients (MFCCs), Perceptual Linear Predictive Features (PLPs), and Temporal Patterns (TRAPs) as front ends. These features are then used to generate phoneme posteriors using Multilayer Perceptrons (MLPs) trained on speech data. The phoneme posteriors are then used as the system input. Our approach produces useful results on a-capella singing, but depend heavily on the chosen keyword. We show that results can be further improved by training the MLP on a-capella data. We also test two post-processing methods on our phoneme posteriors before the keyword spotting step. First, we average the posteriors of all three feature sets. Second, we run the three concatenated posteriors through a fusion classifier.",DEU,facility,Developed economies,"[-4.9721274, -35.779045]","[-7.060401, -20.808098]","[20.4776, 14.6551695, -12.28927]","[4.192985, -6.114637, -18.07646]","[9.879845, 11.109931]","[7.7763224, 4.3121605]","[11.1493025, 15.147343, 0.76540166]","[10.299844, 7.523955, 9.422282]"
41,Joren Six;Marc Leman,Panako - A Scalable Acoustic Fingerprinting System Handling Time-Scale and Pitch Modification.,2014,https://doi.org/10.5281/zenodo.1416190,Six Joren+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>facility;Marc Leman+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>facility,"This paper presents a scalable granular acoustic fingerprinting system. An acoustic fingerprinting system uses condensed representation of audio signals, acoustic fingerprints, to identify short audio fragments in large audio databases. A robust fingerprinting system generates similar fingerprints for perceptually similar audio signals. The system presented here is designed to handle time-scale and pitch modifications. The open source implementation of the system is called Panako and is evaluated on commodity hardware using a freely available reference database with fingerprints of over 30,000 songs. The results show that the system responds quickly and reliably on queries, while handling time-scale and pitch modifications of up to ten percent. The system is also shown to handle GSM-compression, several audio effects and band-pass filtering. After a query, the system returns the start time in the reference audio and how much the query has been pitch-shifted or time-stretched with respect to the reference audio. The design of the system that offers this combination of features is the main contribution of this paper.",BEL,education,Developed economies,"[-14.787126, -26.819351]","[25.057386, -23.004206]","[7.7466774, -13.767684, -21.579424]","[16.571129, -12.489443, 5.1360083]","[9.061435, 4.4674277]","[8.470414, -0.11665894]","[10.577901, 11.520133, -1.9284526]","[10.145905, 5.253876, 13.005482]"
40,Bin Wu;Andrew Horner;Chung Lee,Emotional Predisposition of Musical Instrument Timbres with Static Spectra.,2014,https://doi.org/10.5281/zenodo.1417153,Bin Wu+Hong Kong University of Science and Technology>HKG>education;Andrew Horner+Hong Kong University of Science and Technology>HKG>education;Chung Lee+Singapore University of Technology and Design>SGP>education,"Music is one of the strongest triggers of emotions. Recent studies have shown strong emotional predispositions for musical instrument timbres. They have also shown significant correlations between spectral centroid and many emotions. Our recent study on spectral centroid-equalized tones further suggested that the even/odd harmonic ratio is a salient timbral feature after attack time and brightness. The emergence of the even/odd harmonic ratio motivated us to go a step further: to see whether the spectral shape of musical instruments alone can have a strong emotional predisposition. To address this issue, we conducted follow-up listening tests of static tones. The results showed that the even/odd harmonic ratio again significantly correlated with most emotions, consistent with the theory that static spectral shapes have a strong emotional predisposition.",HKG,education,Developing economies,"[-61.732346, 4.8143744]","[51.226616, -14.768475]","[-23.829422, 27.880272, -1.1643455]","[3.458241, 21.939596, 6.608694]","[14.047276, 12.703029]","[12.651101, 4.3036895]","[16.175709, 14.417701, 1.8443997]","[13.843248, 4.8115654, 10.2796755]"
39,Arthur Flexer,On Inter-rater Agreement in Audio Music Similarity.,2014,https://doi.org/10.5281/zenodo.1416970,Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,One of the central tasks in the annual MIREX evaluation campaign is the ”Audio Music Similarity and Retrieval (AMS)” task. Songs which are ranked as being highly similar by algorithms are evaluated by human graders as to how similar they are according to their subjective judgment. By analyzing results from the AMS tasks of the years 2006 to 2013 we demonstrate that: (i) due to low inter-rater agreement there exists an upper bound of performance in terms of subjective gradings; (ii) this upper bound has already been achieved by participating algorithms in 2009 and not been surpassed since then. Based on this sobering result we discuss ways to improve future evaluations of audio music similarity.,AUT,facility,Developed economies,"[-3.5945017, 10.09807]","[30.527271, 9.043109]","[-8.926223, 9.949874, 4.7202005]","[8.830602, 4.880155, 10.680259]","[13.040466, 9.477738]","[11.251152, 2.2295156]","[13.584713, 15.28668, -0.6507104]","[12.586482, 6.386948, 12.580293]"
38,Ju-Chiang Wang;Ming-Chi Yen;Yi-Hsuan Yang;Hsin-Min Wang,Automatic Set List Identification and Song Segmentation for Full-Length Concert Videos.,2014,https://doi.org/10.5281/zenodo.1417361,"Ju-Chiang Wang+Academia Sinica>TWN>education|University of California, San Diego>USA>education;Ming-Chi Yen+Academia Sinica>TWN>education;Yi-Hsuan Yang+Academia Sinica>TWN>education;Hsin-Min Wang+Academia Sinica>TWN>education","Recently, plenty of full-length concert videos have become available on video-sharing websites such as YouTube. As each video generally contains multiple songs, natural questions that arise include “what is the set list?” and “when does each song begin and end?” Indeed, many full concert videos on YouTube contain song lists and timecodes contributed by uploaders and viewers. However, newly uploaded content and videos of lesser-known artists typically lack this metadata. Manually labeling such metadata would be labor-intensive, and thus an automated solution is desirable. In this paper, we define a novel research problem, automatic set list segmentation of full concert videos, which calls for techniques in music information retrieval (MIR) such as audio fingerprinting, cover song identification, musical event detection, music alignment, and structural segmentation. Moreover, we propose a greedy approach that sequentially identifies a song from a database of studio versions and simultaneously estimates its probable boundaries in the concert. We conduct preliminary evaluations on a collection of 20 full concerts and 1,152 studio tracks. Our result demonstrates the effectiveness of the proposed greedy algorithm.",TWN,education,Developing economies,"[-4.1363254, -5.734577]","[23.862339, 9.867429]","[5.281662, -4.197619, -6.3760552]","[15.26413, 0.2835405, 2.9933097]","[11.788466, 8.421251]","[10.052968, 2.3950849]","[12.439642, 14.3297, 0.07648052]","[11.31726, 6.527668, 11.736144]"
42,Oriol Nieto;Morwaread M. Farbood;Tristan Jehan;Juan Pablo Bello,Perceptual Analysis of the F-Measure to Evaluate Section Boundaries in Music.,2014,https://doi.org/10.5281/zenodo.1414958,"Oriol Nieto+Music and Audio Research Lab, New York University>USA>education;Morwaread M. Farbood+Music and Audio Research Lab, New York University>USA>education;Tristan Jehan+The Echo Nest>USA>company;Juan Pablo Bello+Music and Audio Research Lab, New York University>USA>education","In this paper, we aim to raise awareness of the limitations of the F-measure when evaluating the quality of the boundaries found in the automatic segmentation of music. We present and discuss the results of various experiments where subjects listened to different musical excerpts containing boundary indications and had to rate the quality of the boundaries. These boundaries were carefully generated from state-of-the-art segmentation algorithms as well as human-annotated data. The results show that humans tend to give more relevance to the precision component of the F-measure rather than the recall component, therefore making the classical F-measure not as perceptually informative as currently assumed. Based on the results of the experiments, we discuss the potential of an alternative evaluation based on the F-measure that emphasizes precision over recall, making the section boundary evaluation more expressive and reliable.",USA,education,Developed economies,"[-1.591608, 7.3396997]","[-0.5847363, 5.240381]","[-5.0965757, -9.010026, 0.97542864]","[-2.935035, -2.0355854, -0.56244004]","[12.049952, 8.593734]","[8.425928, 2.7582753]","[12.758084, 14.311559, -0.28750217]","[10.399525, 7.2256217, 11.4808035]"
27,Dawen Liang;John William Paisley;Dan Ellis,Codebook-based Scalable Music Tagging with Poisson Matrix Factorization.,2014,https://doi.org/10.5281/zenodo.1416120,Dawen Liang+Columbia University>USA>education;John Paisley+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Automatic music tagging is an important but challenging problem within MIR. In this paper, we treat music tagging as a matrix completion problem. We apply the Poisson matrix factorization model jointly on the vector-quantized audio features and a “bag-of-tags” representation. This approach exploits the shared latent structure between semantic tags and acoustic codewords. Leveraging the recently-developed technique of stochastic variational inference, the model can tractably analyze massive music collections. We present experimental results on the CAL500 dataset and the Million Song Dataset for both annotation and retrieval tasks, illustrating the steady improvement in performance as more data is used.",USA,education,Developed economies,"[-41.65839, -0.8302589]","[29.793863, -6.54996]","[-15.686539, 15.131889, 8.290729]","[23.048042, 2.4319518, 1.33869]","[14.482321, 10.554118]","[11.244378, 3.378789]","[15.588396, 14.14487, 0.08393327]","[12.853588, 6.6111207, 11.395967]"
36,Taro Masuda;Kazuyoshi Yoshii;Masataka Goto;Shigeo Morishima,Spotting a Query Phrase from Polyphonic Music Audio Signals Based on Semi-supervised Nonnegative Matrix Factorization.,2014,https://doi.org/10.5281/zenodo.1415780,Taro Masuda+Waseda University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Shigeo Morishima+Waseda University>JPN>education,"This paper proposes a query-by-audio system that aims to detect temporal locations where a musical phrase given as a query is played in musical pieces. The “phrase” in this paper means a short audio excerpt that is not limited to a main melody (singing part) and is usually played by a single musical instrument. A main problem of this task is that the query is often buried in mixture signals consisting of various instruments. To solve this problem, we propose a method that can appropriately calculate the distance between a query and partial components of a musical piece. More specifically, gamma process nonnegative matrix factorization (GaP-NMF) is used for decomposing the spectrogram of the query into an appropriate number of basis spectra and their activation patterns. Semi-supervised GaP-NMF is then used for estimating activation patterns of the learned basis spectra in the musical piece by presuming the piece to partially consist of those spectra. This enables distance calculation based on activation patterns. The experimental results showed that our method outperformed conventional matching methods.",JPN,education,Developed economies,"[-6.5847077, -2.5299733]","[-46.699734, -22.61179]","[8.104251, -10.149306, 6.452678]","[-3.303614, -13.778571, -28.326143]","[10.972526, 7.926431]","[6.501766, 4.5016003]","[12.049982, 13.35292, 0.18755496]","[9.833667, 8.588544, 10.211594]"
35,Agustín Martorell;Emilia Gómez,Systematic Multi-scale Set-class Analysis.,2014,https://doi.org/10.5281/zenodo.1417595,Agustín Martorell+Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Universitat Pompeu Fabra>ESP>education,"This work reviews and elaborates a methodology for hierarchical multi-scale set-class analysis of music pieces. The method extends the systematic segmentation and representation of Sapp’s ‘keyscapes’ to the description stage, by introducing a set-class level of description. This provides a systematic, mid-level, and standard analytical lexicon, which allows the description of any notated music based on fixed temperaments. The method benefits from the representation completeness, the compromise between generalisation and discrimination of the set-class spaces, and the access to hierarchical inclusion relations over time. The proposed class-matrices are multidimensional time series encoding the pitch content of every possible music segment over time, regardless the involved time-scales, in terms of a given set-class space. They provide the simplest information mining methods with the ability of capturing sophisticated tonal relations. The proposed class-vectors, quantifying the presence of every possible set-class in a piece, are discussed for advanced explorations of corpora. The compromise between dimensionality and informativeness provided by the class-matrices and class-vectors, is discussed in relation with standard content-based tonal descriptors, and music information retrieval applications.",ESP,education,Developed economies,"[19.569916, 34.133522]","[13.01378, -1.6923356]","[-18.514896, -15.432272, 10.04976]","[4.769483, 5.593767, -0.060948133]","[12.292525, 6.4010797]","[9.082611, 2.5019617]","[14.090968, 12.772058, -0.856945]","[10.72321, 6.993035, 11.782381]"
34,Phillip B. Kirlin,A Data Set for Computational Studies of Schenkerian Analysis.,2014,https://doi.org/10.5281/zenodo.1417833,Phillip B. Kirlin+Rhodes College>USA>education,"Schenkerian analysis, a kind of hierarchical music analysis, is widely used by music theorists. Though it is part of the standard repertoire of analytical techniques, computational studies of Schenkerian analysis have been hindered by the lack of available data sets containing both musical compositions and ground-truth analyses of those compositions. Without such data sets, it is difficult to empirically study the patterns that arise in analyses or rigorously evaluate the performance of intelligent systems for this kind of analysis. To combat this, we introduce the first publicly available large-scale data set of computer-processable Schenkerian analyses. We discuss the choice of musical selections in the data set, the encoding of the music and the corresponding ground-truth analyses, and the possible uses of these data. As an example of the utility of the data set, we present an algorithm that transforms the Schenkerian analyses into hierarchically-organized data structures that are easily manipulated in software.",USA,education,Developed economies,"[21.970034, 30.488073]","[-10.576515, 20.720213]","[-14.188089, -15.686265, 10.827413]","[-11.887865, -1.1103616, 8.58465]","[11.943896, 6.7885127]","[8.650913, 1.9227105]","[13.121029, 12.536715, -1.456606]","[10.104184, 6.257223, 11.826457]"
33,Marcelo Enrique Rodríguez-López;Anja Volk;Dimitrios Bountouridis,Multi-Strategy Segmentation of Melodies.,2014,https://doi.org/10.5281/zenodo.1418013,Marcelo Rodríguez-López+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education;Dimitrios Bountouridis+Utrecht University>NLD>education,"Melodic segmentation is a fundamental yet unsolved problem in automatic music processing. At present most melody segmentation models rely on a ‘single strategy’ (i.e. they model a single perceptual segmentation cue). However, cognitive studies suggest that multiple cues need to be considered. In this paper we thus propose and evaluate a ‘multi-strategy’ system to automatically segment symbolically encoded melodies. Our system combines the contribution of different single strategy boundary detection models. First, it assesses the perceptual relevance of a given boundary detection model for a given input melody; then it uses the boundaries predicted by relevant detection models to search for the most plausible segmentation of the melody. We use our system to automatically segment a corpus of instrumental and vocal folk melodies. We compare the predictions to human annotated segments, and to state of the art segmentation methods. Our results show that our system outperforms the state-of-the-art in the instrumental set.",NLD,education,Developed economies,"[7.1747136, -7.7718196]","[0.9665412, 5.330305]","[9.387604, 6.8978972, -0.06566858]","[-3.854262, -4.047724, 0.5144223]","[10.569074, 9.844253]","[8.35129, 2.530673]","[11.440092, 15.133447, -0.8352839]","[10.049497, 7.235511, 11.717459]"
32,Alessio Bazzica;Cynthia C. S. Liem;Alan Hanjalic,Exploiting Instrument-wise Playing/Non-Playing Labels for Score Synchronization of Symphonic Music.,2014,https://doi.org/10.5281/zenodo.1415772,Alessio Bazzica+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education;Alan Hanjalic+Delft University of Technology>NLD>education,"Synchronization of a score to an audio-visual music performance recording is usually done by solving an audio-to-MIDI alignment problem. In this paper, we focus on the possibility to represent both the score and the performance using information about which instrument is active at a given time stamp. More specifically, we investigate to what extent instrument-wise “playing” (P) and “non-playing” (NP) labels are informative in the synchronization process and what role the visual channel can have for the extraction of P/NP labels. After introducing the P/NP-based representation of the music piece, both at the score and performance level, we define an efficient way of computing the distance between the two representations, which serves as input for the synchronization step based on dynamic time warping. In parallel with assessing the effectiveness of the proposed representation, we also study its robustness when missing and/or erroneous labels occur. Our experimental results show that P/NP-based music piece representation is informative for performance-to-score synchronization and may benefit the existing audio-only approaches.",NLD,education,Developed economies,"[25.485323, -29.82489]","[-17.361322, -14.049402]","[-3.3490913, -19.073286, -12.111366]","[0.9698199, -22.086542, -2.3914611]","[11.097462, 6.0394673]","[5.848698, 0.9675183]","[12.109369, 12.590594, -1.9228876]","[8.007665, 6.046556, 11.137978]"
31,Carlos Eduardo Cancino Chacón;Stefan Lattner;Maarten Grachten,Developing Tonal Perception through Unsupervised Learning.,2014,https://doi.org/10.5281/zenodo.1416058,Carlos Eduardo Cancino Chacón+Austrian Research Institute for Artificial Intelligence>AUT>facility;Stefan Lattner+Austrian Research Institute for Artificial Intelligence>AUT>facility;Maarten Grachten+Austrian Research Institute for Artificial Intelligence>AUT>facility,"The perception of tonal structure in music seems to be rooted both in low-level perceptual mechanisms and in enculturation, the latter accounting for cross-cultural differences in perceived tonal structure. Unsupervised machine learning methods are a powerful tool for studying how musical concepts may emerge from exposure to music. In this paper, we investigate to what degree tonal structure can be learned from musical data by unsupervised training of a Restricted Boltzmann Machine, a generative stochastic neural network. We show that even based on a limited set of musical data, the model learns several aspects of tonal structure. Firstly, the model learns an organization of musical material from different keys that conveys the topology of the circle of fifths (CoF). Although such a topology can be learned using principal component analysis (PCA) when using pitch-only representations, we found that using a pitch-duration representation impedes the extraction of the CoF topology much more for PCA than for the RBM. Furthermore, we replicate probe-tone experiments by Krumhansl and Shepard, measuring the organization of tones within a key in human perception. We find that the responses of the RBM share qualitative characteristics with those of both trained and untrained listeners.",AUT,facility,Developed economies,"[-2.0450802, -18.62307]","[-4.721622, -33.515305]","[1.3427699, -9.766397, 27.582657]","[-16.865849, 8.713591, -17.374783]","[11.062382, 9.185035]","[8.520516, 5.630646]","[12.315156, 13.608075, -0.22411665]","[9.521271, 5.945411, 9.435439]"
30,Lucas Thompson;Simon Dixon;Matthias Mauch,Drum Transcription via Classification of Bar-Level Rhythmic Patterns.,2014,https://doi.org/10.5281/zenodo.1417341,"Lucas Thompson+Centre for Digital Music, Queen Mary University of London>GBR>education;Matthias Mauch+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","We propose a novel method for automatic drum transcription from audio that achieves the recognition of individual drums by classifying bar-level drum patterns. Automatic drum transcription has to date been tackled by recognising individual drums or drum combinations. In high-level tasks such as audio similarity, statistics of longer rhythmic patterns have been used, reflecting that musical rhythm emerges over time. We combine these two approaches by classifying bar-level drum patterns on sub-beat quantised timbre features using support vector machines. We train the classifier using synthesised audio and carry out a series of experiments to evaluate our approach. Using six different drum kits, we show that the classifier generalises to previously unseen drum kits when trained on the other five (80% accuracy). Measures of precision and recall show that even for incorrectly classified patterns many individual drum events are correctly transcribed. Tests on 14 acoustic performances from the ENST-Drums dataset indicate that the system generalises to real-world recordings. Limited by the set of learned patterns, performance is slightly below that of a comparable method. However, we show that for rock music, the proposed method performs as well as the other method and is substantially more robust to added polyphonic accompaniment.",GBR,education,Developed economies,"[29.071442, -45.830147]","[-15.068391, 2.6110978]","[22.188213, -20.515621, 3.7339787]","[7.149457, 9.643042, -13.461045]","[7.617334, 7.175787]","[8.467017, 4.122012]","[10.36642, 11.491122, 1.0354035]","[9.558868, 7.29133, 10.063617]"
29,Zhiyao Duan;David Temperley,Note-level Music Transcription by Maximum Likelihood Sampling.,2014,https://doi.org/10.5281/zenodo.1416534,Zhiyao Duan+University of Rochester>USA>education;David Temperley+University of Rochester>USA>education,"Note-level music transcription, which aims to transcribe note events (often represented by pitch, onset and offset times) from music audio, is an important intermediate step towards complete music transcription. In this paper, we present a note-level music transcription system, which is built on a state-of-the-art frame-level multi-pitch estimation (MPE) system. Preliminary note-level transcription achieved by connecting pitch estimates into notes often lead to many spurious notes due to MPE errors. In this paper, we propose to address this problem by randomly sampling notes in the preliminary note-level transcription. Each sample is a subset of all notes and is viewed as a note-level transcription candidate. We evaluate the likelihood of each candidate using the MPE model, and select the one with the highest likelihood as the final transcription. The likelihood treats notes in a transcription as a whole and favors transcriptions with less spurious notes. Experiments conducted on 110 pieces of J.S. Bach chorales with polyphony from 2 to 4 show that the proposed sampling scheme significantly improves the transcription performance from the preliminary approach. The proposed system also significantly outperforms two other state-of-the-art systems in both frame-level and note-level transcriptions.",USA,education,Developed economies,"[24.765335, -10.893692]","[-7.5884094, -10.766426]","[17.761377, -0.039016355, 13.276995]","[1.0351695, -12.559207, -11.879989]","[9.8159, 7.654828]","[6.7220707, 2.8056276]","[11.814901, 12.529621, -0.11385691]","[8.963933, 7.7588916, 10.731352]"
28,Emmanouil Benetos;Roland Badeau;Tillman Weyde;Gaël Richard,Template Adaptation for Improving Automatic Music Transcription.,2014,https://doi.org/10.5281/zenodo.1418059,Emmanouil Benetos+City University London>GBR>education|Institut Mines-Télécom>FRA>education;Roland Badeau+Institut Mines-Télécom>FRA>education;Tillman Weyde+City University London>GBR>education;Gaël Richard+Institut Mines-Télécom>FRA>education,"In this work, we propose a system for automatic music transcription which adapts dictionary templates so that they closely match the spectral shape of the instrument sources present in each recording. Current dictionary-based automatic transcription systems keep the input dictionary fixed, thus the spectral shape of the dictionary components might not match the shape of the test instrument sources. By performing a conservative transcription pre-processing step, the spectral shape of detected notes can be extracted and utilized in order to adapt the template dictionary. We propose two variants for adaptive transcription, namely for single-instrument transcription and for multiple-instrument transcription. Experiments are carried out using the MAPS and Bach10 databases. Results in terms of multi-pitch detection and instrument assignment show that there is a clear and consistent improvement when adapting the dictionary in contrast with keeping the dictionary fixed.",GBR,education,Developed economies,"[26.612577, -7.900262]","[-8.892831, -9.167053]","[13.823399, -3.2793868, 11.727739]","[-1.2108059, -10.077987, -12.208823]","[9.481543, 7.800811]","[6.616511, 3.0396001]","[11.833548, 12.222217, 0.10006053]","[8.943375, 7.673723, 10.539541]"
37,Akira Maezawa;Katsutoshi Itoyama;Kazuyoshi Yoshii;Hiroshi G. Okuno,Bayesian Audio Alignment based on a Unified Model of Music Composition and Performance.,2014,https://doi.org/10.5281/zenodo.1415594,Akira Maezawa+Yamaha Corporation>JPN>company;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Hiroshi G. Okuno+Waseda University>JPN>education,"This paper presents a new probabilistic model that can align multiple performances of a particular piece of music. Conventionally, dynamic time warping (DTW) and left-to-right hidden Markov models (HMMs) have often been used for audio-to-audio alignment based on a shallow acoustic similarity between performances. Those methods, however, cannot distinguish latent musical structures common to all performances and temporal dynamics unique to each performance. To solve this problem, our model explicitly represents two state sequences: a top-level sequence that determines the common structure inherent in the music itself and a bottom-level sequence that determines the actual temporal fluctuation of each performance. These two sequences are fused into a hierarchical Bayesian HMM and can be learned at the same time from the given performances. Since the top-level sequence assigns the same state for note combinations that repeatedly appear within a piece of music, we can unveil the latent structure of the piece. Moreover, we can easily compare different performances of the same piece by analyzing the bottom-level sequences. Experimental evaluation showed that our method outperformed the conventional methods.",JPN,company,Developed economies,"[16.303244, -12.393506]","[-17.526236, -12.113713]","[-1.0082253, -11.981345, -5.5752454]","[2.5220182, -19.320604, -5.6951547]","[10.970786, 6.437991]","[6.1868706, 1.0589023]","[11.939576, 12.674634, -1.5306438]","[8.268729, 6.186147, 10.88166]"
73,Blair Kaneshiro;Jacek P. Dmochowski,Neuroimaging Methods for Music Information Retrieval: Current Findings and Future Prospects.,2015,https://doi.org/10.5281/zenodo.1416082,Blair Kaneshiro+Stanford University>USA>education;Jacek P. Dmochowski+Stanford University>USA>education,"Over the past decade and a half, music information retrieval (MIR) has grown into a robust, cross-disciplinary field spanning a variety of research domains. Collaborations between MIR and neuroscience researchers, however, are still rare, and to date only a few studies using approaches from one domain have successfully reached an audience in the other. In this paper, we take an initial step toward bridging these two fields by reviewing studies from the music neuroscience literature, with an emphasis on imaging modalities and analysis techniques that might be of practical interest to the MIR community. We show that certain approaches currently used in a neuroscientific setting align with those used in MIR research, and discuss implications for potential areas of future research. We additionally consider the impact of disparate research objectives between the two fields, and how such a discrepancy may have hindered cross-discipline output thus far. It is hoped that a heightened awareness of this literature will foster interaction and collaboration between MIR and neuroscience researchers, leading to advances in both fields that would not have been achieved independently.",USA,education,Developed economies,"[-20.277128, 17.98909]","[31.06829, 34.33593]","[-11.46314, 11.381579, -7.185527]","[0.23155259, 13.477156, 13.864041]","[13.955421, 8.23209]","[11.878147, 0.69181746]","[14.097783, 14.68126, -1.6979451]","[12.14103, 4.461128, 11.591025]"
74,Arthur Flexer,Improving Visualization of High-Dimensional Music Similarity Spaces.,2015,https://doi.org/10.5281/zenodo.1416472,Arthur Flexer+Austrian Research Institute for Artificial Intelligence>AUT>facility,Visualizations of music databases are a popular form of interface allowing intuitive exploration of music catalogs. They are often based on lower dimensional projections of high dimensional music similarity spaces. Such similarity spaces have already been shown to be negatively impacted by so-called hubs and anti-hubs. These are points that appear very close or very far to many other data points due to a problem of measuring distances in high-dimensional spaces. We present an empirical study on how this phenomenon impacts three popular approaches to compute two-dimensional visualizations of music databases. We also show how the negative impact of hubs and anti-hubs can be reduced by re-scaling the high dimensional spaces before low dimensional projection.,AUT,facility,Developed economies,"[-15.420513, 31.895247]","[26.956284, 10.498182]","[-14.223634, 7.8456573, -25.848038]","[17.839634, -3.1959574, 9.64021]","[13.847166, 7.0396347]","[10.799189, 2.0156033]","[13.986627, 13.652889, -2.3898568]","[12.137308, 6.298341, 12.80912]"
75,Hamid Eghbal-zadeh;Bernhard Lehner;Markus Schedl;Gerhard Widmer,I-Vectors for Timbre-Based Music Similarity and Music Artist Classification.,2015,https://doi.org/10.5281/zenodo.1416762,Hamid Eghbal-zadeh+Johannes Kepler University of Linz>AUT>education;Bernhard Lehner+Johannes Kepler University of Linz>AUT>education;Markus Schedl+Johannes Kepler University of Linz>AUT>education;Gerhard Widmer+Johannes Kepler University of Linz>AUT>education,"In this paper, we present a novel approach to extract song-level descriptors built from frame-level timbral features such as Mel-frequency cepstral coefficient (MFCC). These descriptors are called identity vectors or i-vectors and are the results of a factor analysis procedure applied on frame-level features. The i-vectors provide a low-dimensional and fixed-length representation for each song and can be used in a supervised and unsupervised manner. First, we use the i-vectors for an unsupervised music similarity estimation, where we calculate the distance between i-vectors in order to predict the genre of songs. Second, for a supervised artist classification task we report the performance measures using multiple classifiers trained on the i-vectors. Standard datasets for each task are used to evaluate our method and the results are compared with the state of the art. By only using timbral information, we already achieved the state of the art performance in music similarity (which uses extra information such as rhythm). In artist classification using timbre descriptors, our method outperformed the state of the art.",AUT,education,Developed economies,"[-8.94643, 11.774216]","[16.598436, -13.370364]","[-12.045006, 5.0335717, 7.9767847]","[14.87535, 3.5152478, -6.797158]","[13.175095, 9.8875265]","[9.5632105, 3.3249042]","[13.891632, 14.779651, -0.086824305]","[11.327813, 7.2559786, 10.817484]"
76,Dylan Freedman;Eddie Kohler;Hans Tutschku,Correlating Extracted and Ground-Truth Harmonic Data in Music Retrieval Tasks.,2015,https://doi.org/10.5281/zenodo.1418111,Dylan Freedman+Harvard University>USA>education;Eddie Kohler+Harvard University>USA>education;Hans Tutschku+Harvard University>USA>education,"We show that traditional music information retrieval tasks with well-chosen parameters perform similarly using computationally extracted chord annotations and ground-truth annotations. Using a collection of Billboard songs with provided ground-truth chord labels, we use established chord identification algorithms to produce a corresponding extracted chord label dataset. We implement methods to compare chord progressions between two songs on the basis of their optimal local alignment scores. We create a set of chord progression comparison parameters defined by chord distance metrics, gap costs, and normalization measures and run a black-box global optimization algorithm to stochastically search for the best parameter set to maximize the rank correlation for two harmonic retrieval tasks across the ground-truth and extracted chord Billboard datasets. The first task evaluates chord progression similarity between all pairwise combinations of songs, separately ranks results for ground-truth and extracted chord labels, and returns a rank correlation coefficient. The second task queries the set of songs with fabricated chord progressions, ranks each query’s results across ground-truth and extracted chord labels, and returns rank correlations. The end results suggest that practical retrieval systems can be constructed to work effectively without the guide of human ground-truthing.",USA,education,Developed economies,"[-8.402866, -3.7495065]","[17.972305, 14.017786]","[-6.162359, -2.1018703, -19.146046]","[-27.081198, -1.6237102, 7.2684603]","[9.58667, 9.243631]","[10.000476, 2.3231645]","[12.421823, 14.147884, -1.2483282]","[11.236038, 6.863611, 12.34151]"
77,Martin Gasser;Andreas Arzt;Thassilo Gadermaier;Maarten Grachten;Gerhard Widmer,Classical Music on the Web - User Interfaces and Data Representations.,2015,https://doi.org/10.5281/zenodo.1417717,Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler Universität>AUT>education;Andreas Arzt+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler Universität>AUT>education;Thassilo Gadermaier+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Maarten Grachten+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler Universität>AUT>education,"We present a set of web-based user interfaces for explorative analysis and visualization of classical orchestral music and a web API that serves as a backend to those applications; we describe use cases that motivated our developments within the PHENICX project, which promotes a vital interaction between Music Information Retrieval research groups and a world-renowned symphony orchestra. Furthermore, we describe two real-world applications that involve the work presented here. Firstly, our web applications are used in the editorial stage of a periodically released subscription-based mobile app by the Royal Concertgebouw Orchestra (RCO), which serves as a content-distribution channel for multi-modally enhanced recordings of classical concerts. Secondly, our web API and user interfaces have been successfully used to provide real-time information (such as the score, and explanatory comments from musicologists) to the audience during a live concert of the RCO.",AUT,facility,Developed economies,"[-22.912897, 28.140995]","[26.699675, 26.686098]","[-17.334143, 3.761453, -16.986258]","[2.2131462, -6.215629, 25.608335]","[14.306865, 7.433376]","[11.139315, 1.0433717]","[14.317216, 14.052398, -2.205922]","[11.959863, 5.0476203, 12.563057]"
81,Ning Chen;J. Stephen Downie;Haidong Xiao;Yu Zhu;Jie Zhu,Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC) Model for Pop Cover Song Recognition.,2015,https://doi.org/10.5281/zenodo.1416096,"Ning Chen+East China University of Science and Technology>CHN>education|University of Illinois at Urbana-Champaign>USA>education|Shanghai Advanced Research Institute, Chinese Academy of Sciences>CHN>facility|Shanghai Jiao Tong University>CHN>education;J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Haidong Xiao+Shanghai Advanced Research Institute, Chinese Academy of Sciences>CHN>facility;Yu Zhu+East China University of Science and Technology>CHN>education|Shanghai Jiao Tong University>CHN>education;Jie Zhu+Shanghai Jiao Tong University>CHN>education","Most of the features of Cover Song Identification (CSI), for example, Pitch Class Profile (PCP) related features, are based on the musical facets shared among cover versions: melody evolution and harmonic progression. In this work, the perceptual feature was studied for CSI. Our idea was to modify the Perceptual Linear Prediction (PLP) model in the field of Automatic Speech Recognition (ASR) by (a) introducing new research achievements in psychophysics, and (b) considering the difference between speech and music signals to make it consistent with human hearing and more suitable for music signal analysis. Furthermore, the obtained Linear Prediction Coefficients (LPCs) were mapped to LPC cepstrum coefficients, on which liftering was applied, to boost the timbre invariance of the resultant feature: Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC). Experimental results showed that both LPC cepstrum coefficients mapping and cepstrum liftering were crucial in ensuring the identification power of the MPLPLC feature. The MPLPLC feature outperformed state-of-the-art features in the context of CSI and in resisting instrumental accompaniment variation. This study verifies that the mature techniques in the ASR or Computational Auditory Scene Analysis (CASA) fields may be modified and included to enhance the performance of the Music Information Retrieval (MIR) scheme.",CHN,education,Developing economies,"[8.880494, 46.02426]","[13.990418, -15.940768]","[6.056063, 9.880071, -23.78204]","[13.73813, 0.41786397, -9.580566]","[16.128601, 11.142099]","[9.082767, 3.6318612]","[12.856532, 17.331482, -0.3403098]","[10.869249, 7.651611, 10.474219]"
79,Srikanth Cherla;Son N. Tran;Tillman Weyde;Artur S. d'Avila Garcez,Hybrid Long- and Short-Term Models of Folk Melodies.,2015,https://doi.org/10.5281/zenodo.1417315,Srikanth Cherla+City University London>GBR>education;Son N. Tran+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Artur d’Avila Garcez+City University London>GBR>education,"In this paper, we present the results of a study on dynamic models for predicting sequences of musical pitch in melodies. Such models predict a probability distribution over the possible values of the next pitch in a sequence, which is obtained by combining the prediction of two components (1) a long-term model (LTM) learned offline on a corpus of melodies, as well as (2) a short-term model (STM) which incorporates context-specific information available during prediction. Both the LTM and the STM learn regularities in pitch sequences solely from data. The models are combined in an ensemble, wherein they are weighted by the relative entropies of their respective predictions. Going by previous work that demonstrates the success of Connectionist LTMs, we employ the recently proposed Recurrent Temporal Discriminative Restricted Boltzmann Machine (RTDRBM) as the LTM here. While it is indeed possible for the same model to also serve as an STM, our experiments showed that n-gram models tended to learn faster than the RTDRBM in an online setting and that the hybrid of an RTDRBM LTM and an n-gram STM gives us the best predictive performance yet on a corpus of monophonic chorale and folk melodies.",GBR,education,Developed economies,"[9.903462, -2.6237476]","[-6.7538333, -32.040546]","[6.289359, 10.727393, 1.9565643]","[-19.799877, -0.4769235, -5.1357636]","[11.124601, 9.866144]","[8.715976, 5.6067586]","[11.846962, 15.213959, -0.89514095]","[9.44903, 6.020329, 9.329374]"
82,Shrey Dutta;Krishnaraj Sekhar PV;Hema A. Murthy,Raga Verification in Carnatic Music Using Longest Common Segment Set.,2015,https://doi.org/10.5281/zenodo.1417751,Shrey Dutta+Indian Institute of Technology Madras>IND>education;Krishnaraj Sekhar PV+Indian Institute of Technology Madras>IND>education;Hema A. Murthy+Indian Institute of Technology Madras>IND>education,"There are at least 100 r¯agas that are regularly performed in Carnatic music concerts. The audience determines the identity of r¯agas within a few seconds of listening to an item. Most of the audience consists of people who are only avid listeners and not performers. In this paper, an attempt is made to mimic the listener. A r¯aga verification framework is therefore suggested. The r¯aga verification system assumes that a specific r¯aga is claimed based on similarity of movements and motivic patterns. The system then checks whether this claimed r¯aga is correct. For every r¯aga, a set of cohorts are chosen. A r¯aga and its cohorts are represented using pallavi lines of compositions. A novel approach for matching, called Longest Common Segment Set (LCSS), is introduced. The LCSS scores for a r¯aga are then normalized with respect to its cohorts in two different ways. The resulting systems and a baseline system are compared for two partitionings of a dataset. A dataset of 30 r¯agas from Charsur Foundation is used for analysis. An equal error rate (EER) of 12% is obtained.",IND,education,Developing economies,"[3.6376846, -0.9405075]","[-6.353557, 3.693196]","[6.0541787, 0.84225047, -19.157572]","[2.0849981, -4.7728567, 0.42804402]","[11.268186, 10.56203]","[8.119721, 2.7397282]","[11.5790615, 15.136908, -1.6239042]","[10.227597, 7.5087924, 11.321656]"
80,Kaustuv Kanti Ganguli;Abhinav Rastogi;Vedhas Pandit;Prithvi Kantan;Preeti Rao,Efficient Melodic Query Based Audio Search for Hindustani Vocal Compositions.,2015,https://doi.org/10.5281/zenodo.1417203,Kaustuv Kanti Ganguli+Indian Institute of Technology Bombay>IND>education;Abhinav Rastogi+Stanford University>USA>education;Vedhas Pandit+Indian Institute of Technology Bombay>IND>education;Prithvi Kantan+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"Time-series pattern matching methods that incorporate time warping have recently been used with varying degrees of success on tasks of search and discovery of melodic phrases from audio for Indian classical vocal music. While these methods perform effectively due to the minimal assumptions they place on the nature of the sampled pitch temporal trajectories, their practical applicability to retrieval tasks on real-world databases is seriously limited by their prohibitively large computational complexity. While dimensionality reduction of the time-series to discrete symbol strings is a standard approach that can exploit computational gains from the data compression as well as the availability of efficient string matching algorithms, the compressed representation of the pitch time series itself is not well understood given the pervasiveness of pitch inflections in the melodic shape of the raga phrases. We propose methods that are informed by domain knowledge to design the representation and to optimize parameter settings for the subsequent string matching algorithm. The methods are evaluated in the context of an audio query based search for Hindustani vocal compositions in audio recordings via the mukhda (refrain of the song). We present results that demonstrate performance close to that achieved by time-series matching but at orders of magnitude reduction in complexity.",IND,education,Developing economies,"[8.268387, 2.0719092]","[5.734551, -16.735552]","[5.925111, 6.296772, -12.336422]","[13.744294, -10.0844755, -3.4538343]","[11.199338, 10.399873]","[7.742048, 1.0285223]","[11.970404, 15.589107, -1.0319877]","[9.16365, 6.8726153, 12.778627]"
72,Thomas Grill;Jan Schlüter,Music Boundary Detection Using Neural Networks on Combined Features and Two-Level Annotations.,2015,https://doi.org/10.5281/zenodo.1417461,Thomas Grill+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Jan Schlüter+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"The determination of structural boundaries is a key task for understanding the structure of a musical piece, but it is also highly ambiguous. Recently, Convolutional Neural Networks (CNN) trained on spectrogram features and human annotations have been successfully used to tackle the problem, but still fall clearly behind human performance. We expand on the CNN approach by combining spectrograms with self-similarity lag matrices as audio features, thereby capturing more facets of the underlying structural information. Furthermore, in order to consider the hierarchical nature of structural organization, we explore different strategies to learn from the two-level annotations of main and secondary boundaries available in the SALAMI structural annotation dataset. We show that both measures improve boundary recognition performance, resulting in a significant improvement over the previous state of the art. As a side-effect, our algorithm can predict boundaries on two different structural levels, equivalent to the training data.",AUT,facility,Developed economies,"[-5.904322, -9.1073265]","[-18.481646, -28.964115]","[9.582342, -0.36026043, -0.8561656]","[-3.6311007, -3.3361595, -13.052042]","[11.76476, 8.811669]","[8.751633, 4.4436617]","[12.639698, 14.160086, 0.43906754]","[10.29414, 6.910087, 9.634491]"
78,Mutian Fu;Guangyu Xia;Roger B. Dannenberg;Larry A. Wasserman,A Statistical View on the Expressive Timing of Piano Rolled Chords.,2015,https://doi.org/10.5281/zenodo.1417034,Mutian Fu+Carnegie Mellon University>USA>education;Guangyu Xia+Carnegie Mellon University>USA>education;Roger Dannenberg+Carnegie Mellon University>USA>education;Larry Wasserman+Carnegie Mellon University>USA>education,"Rolled or arpeggiated chords are notated chords performed by playing the notes sequentially, usually from lowest to highest in pitch. Arpeggiation is a characteristic of musical expression, or expressive timing, in piano performance. However, very few studies have investigated rolled chord performance. In this paper, we investigate two expressive timing properties of piano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can functionally replace the onsets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We ask two research questions. First, what is the equivalent onset of a rolled chord? Second, are the onset spans of different chords interpreted in the same way? The first question is answered by local tempo estimation while the second question is answered by Analysis of Variance. Also, we contribute a piano duet dataset for rolled chords analysis and other studies on expressive music performance. The dataset contains three pieces of music, each performed multiple times by different pairs of musicians.",USA,education,Developed economies,"[35.267387, 3.3821356]","[-30.449179, 3.5627022]","[22.043816, -0.24617006, 22.684628]","[-12.96477, 6.101937, -4.396464]","[10.153774, 6.919362]","[5.819229, 1.5996965]","[12.16771, 11.669999, -0.73634493]","[8.38312, 6.3837414, 11.133268]"
71,Richard F. E. Sutcliffe;Tim Crawford;Chris Fox;Deane L. Root;Eduard H. Hovy;Richard Lewis 0001,Relating Natural Language Text to Musical Passages.,2015,https://doi.org/10.5281/zenodo.1415270,"Richard Sutcliffe+University of Essex>GBR>education;Tim Crawford+Goldsmiths, University of London>GBR>education;Chris Fox+University of Essex>GBR>education;Deane L. Root+University of Pittsburgh>USA>education;Eduard Hovy+Carnegie-Mellon University>USA>education;Richard Lewis+Goldsmiths, University of London>GBR>education","There is a vast body of musicological literature containing detailed analyses of musical works. These texts make frequent references to musical passages in scores by means of natural language phrases. Our long-term aim is to investigate whether these phrases can be linked automatically to the musical passages to which they refer. As a first step, we have organised for two years running a shared evaluation in which participants must develop software to identify passages in a MusicXML score based on a short noun phrase in English. In this paper, we present the rationale for this work, discuss the kind of references to musical passages which can occur in actual scholarly texts, describe the first two years of the evaluation and finally appraise the results to establish what progress we have made.",GBR,education,Developed economies,"[3.5247371, 9.937949]","[-5.2441792, 14.16358]","[-13.678067, -2.3814952, 2.475913]","[-7.6326385, 4.0982895, 6.8495126]","[13.443408, 8.727246]","[9.064792, 1.5443848]","[13.817603, 14.103429, -1.2063855]","[10.428574, 6.036232, 11.988565]"
59,Athanasios Lykartsis;Chih-Wei Wu;Alexander Lerch,Beat Histogram Features from NMF-Based Novelty Functions for Music Classification.,2015,https://doi.org/10.5281/zenodo.1418145,Athanasios Lykartsis+Technische Universität Berlin>DEU>education;Chih-Wei Wu+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"In this paper we present novel rhythm features derived from drum tracks extracted from polyphonic music and evaluate them in a genre classification task. Musical excerpts are analyzed using an optimized, partially fixed Non-Negative Matrix Factorization (NMF) method and beat histogram features are calculated on basis of the resulting activation functions for each one out of three drum tracks extracted (Hi-Hat, Snare Drum and Bass Drum). The features are evaluated on two widely used genre datasets (GTZAN and Ballroom) using standard classification methods, concerning the achieved overall classification accuracy. Furthermore, their suitability in distinguishing between rhythmically similar genres and the performance of the features resulting from individual activation functions is discussed. Results show that the presented NMF-based beat histogram features can provide comparable performance to other classification systems, while considering strictly drum patterns.",DEU,education,Developed economies,"[-23.34023, -12.215817]","[-16.942533, 2.2621875]","[-8.920568, 0.039359063, 11.689675]","[6.1631365, 11.02369, -10.512553]","[12.465994, 10.345734]","[8.796786, 3.8778698]","[13.488101, 14.0325985, 0.88032556]","[10.183009, 7.400688, 10.362704]"
69,Rafael Caro Repetto;Rong Gong;Nadine Kroher;Xavier Serra,Comparison of the Singing Style of Two Jingju Schools.,2015,https://doi.org/10.5281/zenodo.1416692,"Rafael Caro Repetto+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rong Gong+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Nadine Kroher+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Performing schools (liupai) in jingju (also known as Peking or Beijing opera) are one of the most important elements for the appreciation of this genre among connoisseurs. In the current paper, we study the potential of MIR techniques for supporting and enhancing musicological descriptions of the singing style of two of the most renowned jingju schools for the dan role-type, namely Mei and Cheng schools. To this aim, from the characteristics commonly used for describing singing style in musicological literature, we have selected those that can be studied using standard audio features. We have selected eight recordings from our jingju music research corpus and have applied current algorithms for the measurement of the selected features. Obtained results support the descriptions from musicological sources in all cases but one, and also add precision to them by providing specific measurements. Besides, our methodology suggests some characteristics not accounted for in our musicological sources. Finally, we discuss the need for engaging jingju experts in our future research and applying this approach for musicological and educational purposes as a way of better validating our methodology.",ESP,education,Developed economies,"[-6.9542227, -27.09584]","[-2.271401, 14.1012745]","[10.599453, 9.530624, -18.433414]","[-1.9891713, 9.265114, 7.7698364]","[10.590422, 10.767647]","[8.336076, 1.8396876]","[11.589412, 15.38617, 0.11184061]","[9.815955, 6.521433, 11.733006]"
68,Rachel M. Bittner;Justin Salamon;Slim Essid;Juan Pablo Bello,Melody Extraction by Contour Classification.,2015,https://doi.org/10.5281/zenodo.1416322,"Rachel M. Bittner+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Justin Salamon+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Slim Essid+Télécop Paris-Tech>FRA>education;Juan P. Bello+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education","Due to the scarcity of labeled data, most melody extraction algorithms do not rely on fully data-driven processing blocks but rather on careful engineering. For example, the Melodia melody extraction algorithm employs a pitch contour selection stage that relies on a number of heuristics for selecting the melodic output. In this paper we explore the use of a discriminative model to perform purely data-driven melodic contour selection. Specifically, a discriminative binary classifier is trained to distinguish melodic from non-melodic contours. This classifier is then used to predict likelihoods for a track’s extracted contours, and these scores are decoded to generate a single melody output. The results are compared with the Melodia algorithm and with a generative model used in a previous study. We show that the discriminative model outperforms the generative model in terms of contour classification accuracy, and the melody output from our proposed system performs comparatively to Melodia. The results are complemented with error analysis and avenues for future improvements.",USA,education,Developed economies,"[6.1446733, -11.176319]","[7.069275, -5.764272]","[13.226881, 5.6255875, -2.4802303]","[2.6267428, -8.60162, 0.18054211]","[10.225186, 9.991726]","[8.267465, 2.3074207]","[11.149608, 15.05928, -0.5220133]","[9.865733, 7.2906, 12.103805]"
67,Nicolas Guiomard-Kagan;Mathieu Giraud;Richard Groult;Florence Levé,Comparing Voice and Stream Segmentation Algorithms.,2015,https://doi.org/10.5281/zenodo.1414916,Nicolas Guiomard-Kagan+University of Picardie Jules Verne>FRA>education|National Institute of Research in Computer Science and Automation (INRIA)>FRA>facility;Mathieu Giraud+University of Lille>FRA>education|National Institute of Research in Computer Science and Automation (INRIA)>FRA>facility;Richard Groult+University of Picardie Jules Verne>FRA>education|National Institute of Research in Computer Science and Automation (INRIA)>FRA>facility;Florence Levé+University of Picardie Jules Verne>FRA>education|National Institute of Research in Computer Science and Automation (INRIA)>FRA>facility,"Voice and stream segmentation algorithms group notes from polyphonic data into relevant units, providing a better understanding of a musical score. Voice segmentation algorithms usually extract voices from the beginning to the end of the piece, whereas stream segmentation algorithms identify smaller segments. In both cases, the goal can be to obtain mostly monophonic units, but streams with polyphonic data are also relevant. These algorithms usually cluster contiguous notes with close pitches. We propose an independent evaluation of four of these algorithms (Temperley, Chew and Wu, Ishigaki et al., and Rafailidis et al.) using several evaluation metrics. We benchmark the algorithms on a corpus containing the 48 fugues of Well-Tempered Clavier by J. S. Bach as well as 97 files of popular music containing actual polyphonic information. We discuss how to compare together voice and stream segmentation algorithms, and discuss their strengths and weaknesses.",FRA,education,Developed economies,"[-15.39239, -18.88455]","[-7.7632356, 5.9443393]","[9.229074, -7.1800666, -11.293043]","[-1.2390698, -17.617, 8.319019]","[11.250173, 8.809423]","[8.161191, 2.690722]","[12.39911, 14.209418, 0.79906464]","[10.1507845, 7.2184873, 11.452006]"
66,Hendrik Vincent Koops;Anja Volk;W. Bas de Haas,Corpus-Based Rhythmic Pattern Analysis of Ragtime Syncopation.,2015,https://doi.org/10.5281/zenodo.1417213,Hendrik Vincent Koops+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education;W. Bas de Haas+Utrecht University>NLD>education,"This paper presents a corpus-based study on rhythmic patterns in the RAG-collection of approximately 11,000 symbolically encoded ragtime pieces. While characteristic musical features that define ragtime as a genre have been debated since its inception, musicologists argue that specific syncopation patterns are most typical for this genre. Therefore, we investigate the use of syncopation patterns in the RAG-collection from its beginnings until the present time in this paper. Using computational methods, this paper provides an overview on the use of rhythmical patterns of the ragtime genre, thereby offering valuable new insights that complement musicological hypotheses about this genre. Specifically, we measure the amount of syncopation for each bar using Longuet-Higgins and Lee’s model of syncopation, determine the most frequent rhythmic patterns, and discuss the role of a specific short-long-short syncopation pattern that musicologists argue is characteristic for ragtime. A comparison between the ragtime (pre-1920) and modern (post-1920) era shows that the two eras differ in syncopation pattern use. Onset density and amount of syncopation increase after 1920. Moreover, our study confirms the musicological hypothesis on the important role of the short-long-short syncopation pattern in ragtime. These findings are pivotal in developing ragtime genre-specific features.",NLD,education,Developed economies,"[46.756042, 11.293424]","[-17.365662, 8.625088]","[-11.58301, -25.777887, -0.1371268]","[-8.245988, 14.651187, 2.4616184]","[12.003115, 5.3552966]","[6.7018714, 1.346378]","[11.280841, 14.406005, -2.0673711]","[8.580917, 6.7102237, 12.130863]"
65,Jin Ha Lee;Rachel Price,Understanding Users of Commercial Music Services through Personas: Design Implications.,2015,https://doi.org/10.5281/zenodo.1418297,Jin Ha Lee+University of Washington>USA>education;Rachel Price+University of Washington>USA>education,"Most of the previous literature on music users’ needs, habits, and interactions with music information retrieval (MIR) systems focuses on investigating user groups of particular demographics or testing the usability of specific interfaces/systems. In order to improve our understanding of how users’ personalities and characteristics affect their needs and interactions with MIR systems, we conducted a qualitative user study across multiple commercial music services, utilizing interviews and think-aloud sessions. Based on the empirical user data, we have developed seven personas. These personas offer a deeper understanding of the different types of MIR system users and the relative importance of various design implications for each user type. Implications for system design include a renegotiation of our understanding of desired user engagement, especially with the habit of context-switching, designing systems for specialized uses, and addressing user concerns around privacy, transparency, and control.",USA,education,Developed economies,"[-35.170013, 26.462257]","[37.24384, 31.267832]","[-20.480404, 18.26411, -12.070667]","[5.1409974, 11.97936, 18.777729]","[15.108473, 8.282937]","[12.558705, 0.88131523]","[15.150291, 14.866385, -1.8494363]","[12.908952, 4.305036, 12.10233]"
64,Bochen Li;Zhiyao Duan,Score Following for Piano Performances with Sustain-Pedal Effects.,2015,https://doi.org/10.5281/zenodo.1418127,Bochen Li+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"One challenge in score following (i.e., mapping audio frames to score positions in real time) for piano performances is the mismatch between audio and score caused by the usage of the sustain pedal. When the pedal is pressed, notes played will continue to sound until the string vibration naturally ceases. This makes the notes longer than their notated lengths and overlap with later notes. In this paper, we propose an approach to address this problem. Given that the most competitive wrong score positions for each audio frame are the ones before the correct position due to the sustained sounds, we remove partials of sustained notes and only retain partials of “new notes” in the audio representation. This operation reduces sustain-pedal effects by weakening the match between the audio frame and previous wrong score positions, hence encourages the system to align to the correct score position. We implement this idea based on a state-of-the-art score following framework. Experiments on synthetic and real piano performances from the MAPS dataset show significant improvements on both alignment accuracy and robustness.",USA,education,Developed economies,"[31.75553, -0.1706396]","[-21.653893, -12.895721]","[18.08632, -2.1355064, 23.831093]","[-4.9787097, -18.89991, -4.7007494]","[9.890701, 7.049785]","[6.21393, 0.81282246]","[12.194844, 11.462601, -0.47319213]","[8.361695, 5.9223056, 10.714984]"
63,Julien Osmalskyj;Peter Foster;Simon Dixon;Jean-Jacques Embrechts,Combining Features for Cover Song Identification.,2015,https://doi.org/10.5281/zenodo.1417295,Julien Osmalskyj+University of Liège>BEL>education;Peter Foster+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education;Jean-Jacques Embrechts+University of Liège>BEL>education,"In this paper, we evaluate a set of methods for combining features for cover song identification. We first create multiple classifiers based on global tempo, duration, loudness, beats and chroma average features, training a random forest for each feature. Subsequently, we evaluate standard combination rules for merging these single classifiers into a composite classifier based on global features. We further obtain two higher level classifiers based on chroma features: one based on comparing histograms of quantized chroma features, and a second one based on computing cross-correlations between sequences of chroma features, to account for temporal information. For combining the latter chroma-based classifiers with the composite classifier based on global features, we use standard rank aggregation methods adapted from the information retrieval literature. We evaluate performance with the Second Hand Song dataset, where we quantify performance using multiple statistics. We observe that each combination rule outperforms single methods in terms of the total number of identified queries. Experiments with rank aggregation methods show an increase of up to 23.5 % of the number of identified queries, compared to single classifiers.",BEL,education,Developed economies,"[7.7868767, 43.786762]","[24.932344, -10.611689]","[2.4748266, 13.692965, -23.593718]","[16.453331, -0.6685183, -1.0082988]","[16.05181, 11.107068]","[10.030128, 3.1155531]","[12.850326, 17.341053, -0.32910392]","[11.760169, 7.000649, 11.223521]"
62,Charles Inskip;Frans Wiering,In Their Own Words: Using Text Analysis to Identify Musicologists' Attitudes towards Technology.,2015,https://doi.org/10.5281/zenodo.1416422,Charles Inskip+University College London>GBR>education;Frans Wiering+Utrecht University>GBR>education,"A widely distributed online survey gathered quantitative and qualitative data relating to the use of technology in the research practices of musicologists. This survey builds on existing work in the digital humanities and provides insights into the specific nature of musicology in relation to use and perceptions of technology. Analysis of the data (n=621) notes the preferences in resource format and the digital skills of the survey participants. The themes of comments on rewards, benefits, frustrations, risks, and limitations are explored using an h-point approach derived from applied linguistics. It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities, and that efforts should be made into supporting development of their digital skills and providing usable, useful and reliable software created with a ‘musicology-centred’ design approach. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level.",GBR,education,Developed economies,"[-35.560646, 21.162329]","[28.43977, 37.953712]","[-23.836897, 15.53565, -7.9089446]","[-0.43852827, 9.150379, 22.126766]","[15.019983, 8.522661]","[11.766183, 0.20939237]","[15.005294, 15.004462, -1.6666127]","[12.062295, 4.2893643, 11.893931]"
61,Marius Miron;Julio José Carabias-Orti;Jordi Janer,Improving Score-Informed Source Separation for Classical Music through Note Refinement.,2015,https://doi.org/10.5281/zenodo.1417689,Marius Miron+Universitat Pompeu Fabra>ESP>education;Julio José Carabias-Orti+Universitat Pompeu Fabra>ESP>education;Jordi Janer+Universitat Pompeu Fabra>ESP>education,"Signal decomposition methods such as Non-negative Matrix Factorization (NMF) demonstrated to be a suitable approach for music signal processing applications, including sound source separation. To better control this decomposition, NMF has been extended using prior knowledge and parametric models. In fact, using score information considerably improved separation results. Nevertheless, one of the main problems of using score information is the misalignment between the score and the actual performance. A potential solution to this problem is the use of audio to score alignment systems. However, most of them rely on a tolerance window that clearly affects the separation results. To overcome this problem, we propose a novel method to refine the aligned score at note level by detecting both, onset and offset for each note present in the score. Note refinement is achieved by detecting shapes and contours in the estimated instrument-wise time activation (gains) matrix. Decomposition is performed in a supervised way, using training instrument models and coarsely-aligned score information. The detected contours define time-frequency note boundaries, and they increase the sparsity. Finally, we have evaluated our method for informed source separation using a dataset of Bach chorales obtaining satisfactory results, especially in terms of SIR.",ESP,education,Developed economies,"[5.1759987, -46.446003]","[-45.4383, -27.2615]","[31.501451, 0.48068023, -0.8983101]","[-8.837801, -11.566594, -27.355417]","[8.382095, 10.167579]","[6.316879, 5.253986]","[10.965417, 13.629369, 1.7263634]","[9.646011, 8.728202, 9.639894]"
60,Diego Furtado Silva;Vinícius M. A. de Souza;Gustavo E. A. P. A. Batista,Music Shapelets for Fast Cover Song Recognition.,2015,https://doi.org/10.5281/zenodo.1416236,Diego F. Silva+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education;Vinícius M. A. Souza+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education;Gustavo E. A. P. A. Batista+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education,"A cover song is a new performance or recording of a previously recorded music by an artist other than the original one. The automatic identification of cover songs is useful for a wide range of tasks, from fans looking for new versions of their favorite songs to organizations involved in licensing copyrighted songs. This is a difficult task given that a cover may differ from the original song in key, timbre, tempo, structure, arrangement and even language of the vocals. Cover song identification has attracted some attention recently. However, most of the state-of-the-art approaches are based on similarity search, which involves a large number of similarity computations to retrieve potential cover versions for a query recording. In this paper, we adapt the idea of time series shapelets for content-based music retrieval. Our proposal adds a training phase that finds small excerpts of feature vectors that best describe each song. We demonstrate that we can use such small segments to identify cover songs with higher identification rates and more than one order of magnitude faster than methods that use features to describe the whole music.",BRA,education,Developing economies,"[8.376276, 42.408775]","[23.081875, 10.628371]","[4.014059, 9.685536, -21.234497]","[14.660743, -1.7402127, 3.5350347]","[16.095982, 11.11322]","[10.230344, 2.4203136]","[12.917407, 17.28541, -0.4058889]","[11.665682, 6.583128, 11.972593]"
83,Yucong Jiang;Christopher Raphael,Instrument Identification in Optical Music Recognition.,2015,https://doi.org/10.5281/zenodo.1416116,Yucong Jiang+Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education,"We present a method for recognizing and interpreting the text labels for the instruments in an orchestra score, thereby associating staves with instruments. This task is one of many necessary in optical music recognition. Our approach treats the score system as the basic unit of processing. A graph structure describes the possible orderings of instruments in the system. Each instrument may apply to several staves, may be represented with several possible text strings, and may appear at several possible positions relative to the staves. We find the optimal labeling of staves using a globally optimal dynamic programming approach that embeds simple template-based optical character recognition within the overall recognition scheme. When given an entire score, we simultaneously optimize on the text labeling for each system, as well as the character template models, thus adapting to the font at hand. Our implementation alternately optimizes over the text label identification and re-estimates the character templates. Experiments are presented on 10 different scores showing a significant improvement due to adaptation.",USA,education,Developed economies,"[8.715031, -23.14037]","[-22.472733, 35.945442]","[17.865196, -6.774916, -1.3667995]","[-10.286577, -21.34593, -1.8496829]","[8.798189, 6.7683163]","[6.7885356, -0.4863108]","[10.963969, 12.18653, 0.24117441]","[7.9808855, 4.2270703, 10.636183]"
58,Maximos A. Kaliakatsos-Papakostas;Asterios I. Zacharakis;Costas Tsougras;Emilios Cambouropoulos,Evaluating the General Chord Type Representation in Tonal Music and Organising GCT Chord Labels in Functional Chord Categories.,2015,https://doi.org/10.5281/zenodo.1417379,Maximos Kaliakatsos-Papakostas+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Asterios Zacharakis+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Costas Tsougras+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Emilios Cambouropoulos+Aristotle University of Thessaloniki>GRC>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"The General Chord Type (GCT) representation is appropriate for encoding tone simultaneities in any harmonic context (such as tonal, modal, jazz, octatonic, atonal). The GCT allows the re-arrangement of the notes of a harmonic sonority such that abstract idiom-specific types of chords may be derived. This encoding is inspired by the standard roman numeral chord type labelling and is, therefore, ideal for hierarchic harmonic systems such as the tonal system and its many variations; at the same time, it adjusts to any other harmonic system such as post-tonal, atonal music, or traditional polyphonic systems. In this paper the descriptive potential of the GCT is assessed in the tonal idiom by comparing GCT harmonic labels with human expert annotations (Kostka & Payne harmonic dataset). Additionally, novel methods for grouping and clustering chords, according to their GCT encoding and their functional role in chord sequences, are introduced. The results of both harmonic labelling and functional clustering indicate that the GCT representation constitutes a suitable scheme for representing effectively harmony in computational systems.",GRC,education,Developed economies,"[54.165264, -2.04747]","[-19.824686, 19.108826]","[24.104109, -15.858452, 18.449594]","[-19.158522, -6.0759397, 9.031441]","[6.9716244, 8.631423]","[7.745299, 2.1902072]","[12.049684, 10.537509, 1.8620278]","[9.983548, 7.9259205, 12.500381]"
70,Victor Padilla;Alex McLean;Alan Marsden;Kia Ng,Improving Optical Music Recognition by Combining Outputs from Multiple Sources.,2015,https://doi.org/10.5281/zenodo.1416258,Victor Padilla+Lancaster University>GBR>education;Alex McLean+University of Leeds>GBR>education;Alan Marsden+Lancaster University>GBR>education;Kia Ng+University of Leeds>GBR>education,"Current software for Optical Music Recognition (OMR) produces outputs with too many errors that render it an unrealistic option for the production of a large corpus of symbolic music files. In this paper, we propose a system which applies image pre-processing techniques to scans of scores and combines the outputs of different commercial OMR programs when applied to images of different scores of the same piece of music. As a result of this procedure, the combined output has around 50% fewer errors when compared to the output of any one OMR program. Image pre-processing splits scores into separate movements and sections and removes ossia staves which confuse OMR software. Post-processing aligns the outputs from different OMR programs and from different sources, rejecting outputs with the most errors and using majority voting to determine the likely correct details. Our software produces output in MusicXML, concentrating on accurate pitch and rhythm and ignoring grace notes. Results of tests on the six string quartets by Mozart dedicated to Joseph Haydn and the first six piano sonatas by Mozart are presented, showing an average recognition rate of around 95%.",GBR,education,Developed economies,"[38.706524, 22.31108]","[-19.570244, 36.93007]","[17.741, 15.794623, 10.666006]","[-9.86792, -19.756092, 3.604074]","[8.548688, 6.2376146]","[6.6429577, -0.4758547]","[10.610342, 11.069236, -0.08481355]","[8.046211, 4.269048, 10.723867]"
84,Christian Dittmar;Bernhard Lehner;Thomas Prätzlich;Meinard Müller;Gerhard Widmer,Cross-Version Singing Voice Detection in Classical Opera Recordings.,2015,https://doi.org/10.5281/zenodo.1416958,"Christian Dittmar+International Audio Laboratories, Erlangen>DEU>facility;Bernhard Lehner+Johannes Kepler University, Linz>AUT>education;Thomas Prätzlich+International Audio Laboratories, Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories, Erlangen>DEU>facility;Gerhard Widmer+Johannes Kepler University, Linz>AUT>education","In the field of Music Information Retrieval (MIR), the automated detection of the singing voice within a given music recording constitutes a challenging and important research problem. In this study, our goal is to find those segments within a classical opera recording, where one or several singers are active. As our main contributions, we first propose a novel audio feature that extends a state-of-the-art feature set that has previously been applied to singing voice detection in popular music recordings. Second, we describe a simple bootstrapping procedure that helps to improve the results in the case that the test data is not reflected well by the training data. Third, we show that a cross-version approach can help to stabilize the results even further.",DEU,facility,Developed economies,"[-7.6720896, -35.414333]","[8.7216625, -24.03266]","[19.731613, 10.443921, -14.789389]","[8.816754, -2.5587492, -18.35872]","[9.91325, 11.142554]","[8.622144, 3.5464985]","[11.138121, 15.159282, 0.76332617]","[10.76777, 7.5280395, 9.934826]"
111,Pei-Ching Li;Li Su;Yi-Hsuan Yang;Alvin W. Y. Su,Analysis of Expressive Musical Terms in Violin Using Score-Informed and Expression-Based Audio Features.,2015,https://doi.org/10.5281/zenodo.1416784,Pei-Ching Li+National Cheng-Kung University>TWN>education|Academia Sinica>Unknown>Unknown;Li Su+Academia Sinica>Unknown>Unknown|National Cheng-Kung University>Unknown>Unknown;Yi-Hsuan Yang+Academia Sinica>Unknown>Unknown|National Cheng-Kung University>Unknown>Unknown;Alvin W. Y. Su+National Cheng-Kung University>TWN>education|Academia Sinica>Unknown>Unknown,"The manipulation of different interpretational factors, including dynamics, duration, and vibrato, constitutes the realization of different expressions in music. Therefore, a deeper understanding of the workings of these factors is critical for advanced expressive synthesis and computer-aided music education. In this paper, we propose the novel task of automatic expressive musical term classification as a direct means to study the interpretational factors. Specifically, we consider up to 10 expressive musical terms, such as Scherzando and Tranquillo, and compile a new dataset of solo violin excerpts featuring the realization of different expressive terms by different musicians for the same set of classical music pieces. Under a score-informed scheme, we design and evaluate a number of note-level features characterizing the interpretational aspects of music for the classification task. Our evaluation shows that the proposed features lead to significantly higher classification accuracy than a baseline feature set commonly used in music information retrieval tasks. Moreover, taking the contrast of feature values between an expressive and its corresponding non-expressive version (if given) of a music piece greatly improves the accuracy in classifying the presented expressive one. We also draw insights from analyzing the feature relevance and the class-wise accuracy of the prediction.",TWN,education,Developing economies,"[16.363022, -17.77034]","[-10.401697, 11.0395775]","[4.369828, -16.06149, -5.353541]","[-9.433837, 1.2923155, -2.3858683]","[9.826419, 6.692087]","[8.793091, 2.8853035]","[11.5558405, 12.390457, -0.5958666]","[10.259843, 6.539595, 10.999881]"
86,Erik Duval;Marnix van Berchum;Anja Jentzsch;Gonzalo Alberto Parra Chico;Andreas Drakos,Musicology of Early Music with Europeana Tools and Services.,2015,https://doi.org/10.5281/zenodo.1417509,Erik Duval+KU Leuven>BEL>education;Marnix van Berchum+Utrecht University>NLD>education;Anja Jentzsch+Open Knowledge Foundation>DEU>company;Gonzalo Alberto Parra Chico+KU Leuven>BEL>education;Andreas Drakos+AgroKnow>GRC>company,"The Europeana repository hosts large collections of digitized music manuscripts and prints. This paper investigates how tools and services for this repository can enable Early Music musicologists to carry out their research in a more effective or efficient way, or to carry out research that is impossible to do without such tools or services. We report on the methodology, user-centered development of a suite of tools that we have integrated loosely, in order to experiment with this specific target audience and an evaluation of the impact that such tools may have on how these musicologists carry out their research. Positive feedback relates to the automation of data sharing between the loosely coupled tools and support for an integrated workflow. Participants in this study wanted to have the ability to work not only with individual items, but also with collections of such items. The use of search facets to filter, and visualization around time and place were positively evaluated, as was the use of Optical Music Recognition and computer-supported analysis of music scores. The musicologists were not convinced of the value of activity streams. They also wanted a less strictly linear organization of their workflow and the ability to not only consume items from the repository, but to also push their research results back into the Europeana repository.",BEL,education,Developed economies,"[-27.11227, 32.36096]","[25.781542, 37.08327]","[-18.307373, 0.9231893, -8.013556]","[-2.741935, 6.6437645, 22.957506]","[14.252853, 8.329507]","[11.45419, 0.20677143]","[14.591082, 14.383899, -1.8377715]","[11.965127, 4.4692173, 11.991157]"
57,Jiajie Dai;Matthias Mauch;Simon Dixon,Analysis of Intonation Trajectories in Solo Singing.,2015,https://doi.org/10.5281/zenodo.1417169,Jiajie Dai+Queen Mary University of London>GBR>education;Matthias Mauch+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"We present a new dataset for singing analysis and modeling, and an exploratory analysis of pitch accuracy and pitch trajectories. Shortened versions of three pieces from The Sound of Music were selected: “Edelweiss”, “Do-Re-Mi” and “My Favourite Things”. 39 participants sang three repetitions of each excerpt without accompaniment, resulting in a dataset of 21762 notes in 117 recordings. To obtain pitch estimates we used the Tony software’s automatic transcription and manual correction tools. Pitch accuracy was measured in terms of pitch error and interval error. We show that singers’ pitch accuracy correlates significantly with self-reported singing skill and musical training. Larger intervals led to larger errors, and the tritone interval in particular led to average errors of one third of a semitone. Note duration (or inter-onset interval) had a significant effect on pitch accuracy, with greater accuracy on longer notes. To model drift in the tonal centre over time, we present a sliding window model which reveals patterns in the pitch errors of some singers. Based on the trajectory, we propose a measure for the magnitude of drift: tonal reference deviation (TRD). The data and software are freely available.",GBR,education,Developed economies,"[-1.6083438, -27.847975]","[-3.4025536, -18.42787]","[15.963132, 2.693073, -15.702315]","[6.8932624, -17.453827, -13.92378]","[10.313217, 10.547808]","[6.9037833, 2.1106389]","[11.289257, 15.177452, 0.07258391]","[8.806893, 7.630949, 11.492913]"
109,Elad Liebman;Peter Stone;Corey N. White,How Music Alters Decision Making - Impact of Music Stimuli on Emotional Classification.,2015,https://doi.org/10.5281/zenodo.1414908,Elad Liebman+The University of Texas at Austin>USA>education;Peter Stone+The University of Texas at Austin>USA>education;Corey N. White+Syracuse University>USA>education,"Numerous studies have demonstrated that mood can affect emotional processing. The goal of this study was to explore which components of the decision process are affected when exposed to music; we do so within the context of a stochastic sequential model of simple decisions, the drift-diffusion model (DDM). In our experiment, participants decided whether words were emotionally positive or negative while listening to music that was chosen to induce positive or negative mood. The behavioral results show that the music manipulation was effective, as participants were biased to label words positive in the positive music condition. The DDM shows that this bias was driven by a change in the starting point of evidence accumulation, which indicates an a priori response bias. In contrast, there was no evidence that music affected how participants evaluated the emotional content of the stimuli. To better understand the correspondence between auditory features and decision-making, we proceeded to study how individual aspects of music affect response patterns. Our results have implications for future studies of the connection between music and mood.",USA,education,Developed economies,"[-60.26342, 4.28894]","[58.61094, 0.94193244]","[-24.103945, 24.26552, -0.95572263]","[5.706728, 20.446909, 14.069532]","[13.973441, 12.599515]","[13.224354, 3.0596013]","[16.193415, 14.510689, 1.6656678]","[13.616192, 4.4920235, 11.096359]"
110,Mark S. Melenhorst;Cynthia C. S. Liem,Put the Concert Attendee in the Spotlight. A User-Centered Design and Development Approach for Classical Concert Applications.,2015,https://doi.org/10.5281/zenodo.1416820,Mark S. Melenhorst+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education,"As the importance of real-life use cases in the music information retrieval (MIR) field is increasing, so does the importance of understanding user needs. The development of innovative real-life applications that draw on MIR technology requires a user-centered design and development approach that assesses user needs and aligns them with technological and academic ambitions in the MIR domain. In this paper we present such an approach, and apply it to the development of technological applications to enrich classical symphonic concerts. A user-driven approach is particularly important in this area, as orchestras need to innovate the concert experience to meet the needs and expectations of younger generations without alienating the current audience. We illustrate this approach with the results of five focus groups for three audience segments, which allow us to formulate informed user requirements for classical concert applications.",NLD,education,Developed economies,"[-26.97112, 30.248604]","[27.040377, 27.144361]","[-21.924536, 11.387224, -22.26486]","[2.2313645, -7.1466, 25.054493]","[15.065037, 7.9718857]","[11.856793, 0.8857387]","[15.093042, 14.56799, -1.9837875]","[12.383082, 4.5660863, 12.283319]"
112,Guangyu Xia;Yun Wang;Roger B. Dannenberg;Geoffrey Gordon,Spectral Learning for Expressive Interactive Ensemble Music Performance.,2015,https://doi.org/10.5281/zenodo.1415806,Guangyu Xia+Carnegie Mellon University>USA>education;Yun Wang+Carnegie Mellon University>USA>education;Roger Dannenberg+Carnegie Mellon University>USA>education;Geoffrey Gordon+Carnegie Mellon University>USA>education,"We apply machine learning to a database of recorded ensemble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers’ musical expression as co-evolving time series and learn their interactive relationship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspondence not only between different performers but also between the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to generate a more human-like interaction.",USA,education,Developed economies,"[-15.68608, 3.531827]","[-33.833508, 7.218877]","[-5.4859376, 9.842403, 19.738298]","[-14.13911, 1.2091682, -2.1953502]","[11.246078, 8.053065]","[7.6085453, 3.742554]","[13.539449, 13.055062, -0.83946496]","[9.041772, 6.1992965, 10.607122]"
113,Jakob Abeßer;Estefanía Cano;Klaus Frieler;Martin Pfleiderer;Wolf-Georg Zaddach,Score-Informed Analysis of Intonation and Pitch Modulation in Jazz Solos.,2015,https://doi.org/10.5281/zenodo.1416836,"Jakob Abeßer+Jazzomat Research Project, University of Music Franz Liszt>DEU>education|Semantic Music Technologies Group, Fraunhofer IDMT>DEU>company;Estefanía Cano+Semantic Music Technologies Group, Fraunhofer IDMT>DEU>company;Klaus Frieler+Jazzomat Research Project, University of Music Franz Liszt>DEU>education;Martin Pﬂeiderer+Jazzomat Research Project, University of Music Franz Liszt>DEU>education;Wolf-Georg Zaddach+Jazzomat Research Project, University of Music Franz Liszt>DEU>education","The paper presents new approaches for analyzing the characteristics of intonation and pitch modulation of woodwind and brass solos in jazz recordings. To this end, we use score-informed analysis techniques for source separation and fundamental frequency tracking. After splitting the audio into a solo and a backing track, a reference tuning frequency is estimated from the backing track. Next, we compute the fundamental frequency contour for each tone in the solo and a set of features describing its temporal shape. Based on this data, we first investigate, whether the tuning frequencies of jazz recordings changed over the decades of the last century. Second, we analyze whether the intonation is artist-specific. Finally, we examine how the modulation frequency of vibrato tones depends on contextual parameters such as pitch, duration, and tempo as well as the performing artist.",DEU,education,Developed economies,"[-0.5380851, -26.651217]","[-34.14716, 1.8033828]","[15.739719, 0.51789886, -14.64705]","[-11.836897, -4.916701, -1.268574]","[10.6101465, 10.076385]","[6.5633655, 2.3285985]","[11.5843115, 14.9916935, -0.2525598]","[8.843648, 7.41959, 11.39408]"
106,Anna Aljanaki;Frans Wiering;Remco C. Veltkamp,Emotion Based Segmentation of Musical Audio.,2015,https://doi.org/10.5281/zenodo.1418201,Anna Aljanaki+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"The dominant approach to musical emotion variation detection tracks emotion over time continuously and usually deals with time resolutions of one second. In this paper we discuss the problems associated with this approach and propose to move to bigger time resolutions when tracking emotion over time. We argue that it is more natural from the listener’s point of view to regard emotional variation in music as a progression of emotionally stable segments. In order to enable such tracking of emotion over time it is necessary to segment music at the emotional boundaries. To address this problem we conduct a formal evaluation of different segmentation methods as applied to a task of emotional boundary detection. We collect emotional boundary annotations from three annotators for 52 musical pieces from the RWC music collection that already have structural annotations from the SALAMI dataset. We investigate how well structural segmentation explains emotional segmentation and find that there is a large overlap, though about a quarter of emotional boundaries do not coincide with structural ones. We also study inter-annotator agreement on emotional segmentation. Lastly, we evaluate different unsupervised segmentation methods when applied to emotional boundary detection and find that, in terms of F-measure, the Structural Features method performs best.",NLD,education,Developed economies,"[-58.96863, 0.6650756]","[-1.0159909, 6.3417478]","[-28.968388, 22.884737, 3.8476505]","[-2.8681011, 3.5124154, -0.2855386]","[13.998545, 12.891578]","[8.624239, 2.697126]","[16.064789, 14.416531, 1.8177459]","[10.553695, 6.9350953, 11.39477]"
105,Sebastian Stober;Avital Sternin;Adrian M. Owen;Jessica A. Grahn,Towards Music Imagery Information Retrieval: Introducing the OpenMIIR Dataset of EEG Recordings from Music Perception and Imagination.,2015,https://doi.org/10.5281/zenodo.1416270,Sebastian Stober+Brain and Mind Institute>CAN>facility;Avital Sternin+Brain and Mind Institute>CAN>facility;Adrian M. Owen+Brain and Mind Institute>CAN>facility;Jessica A. Grahn+Brain and Mind Institute>CAN>facility,"Music imagery information retrieval (MIIR) systems may one day be able to recognize a song from only our thoughts. As a step towards such technology, we are presenting a public domain dataset of electroencephalography (EEG) recordings taken during music perception and imagination. We acquired this data during an ongoing study that so far comprises 10 subjects listening to and imagining 12 short music fragments – each 7–16s long – taken from well-known pieces. These stimuli were selected from different genres and systematically vary along musical dimensions such as meter, tempo and the presence of lyrics. This way, various retrieval scenarios can be addressed and the success of classifying based on specific dimensions can be tested. The dataset is aimed to enable music information retrieval researchers interested in these new MIIR challenges to easily test and adapt their existing approaches for music analysis like fingerprinting, beat tracking, or tempo estimation on EEG data.",CAN,facility,Developed economies,"[-20.50162, 17.619328]","[-0.2907908, 24.089376]","[-10.451026, 11.305422, -5.84988]","[-1.2551837, 15.619328, 10.387483]","[14.001343, 8.292306]","[10.941663, 4.38222]","[14.153561, 14.745944, -1.622301]","[11.777206, 4.7288094, 10.643937]"
104,Peter Jancovic;Münevver Köküer;Wrena Baptiste,Automatic Transcription of Ornamented Irish Traditional Flute Music Using Hidden Markov Models.,2015,https://doi.org/10.5281/zenodo.1415116,Peter Jančovič+University of Birmingham>GBR>education;Münevver Kökuer+Birmingham City University>GBR>education;Wrena Baptiste+University of Birmingham>GBR>education|Birmingham City University>GBR>education,"This paper presents an automatic system for note transcription of Irish traditional flute music containing ornamentation. This is a challenging problem due to the soft nature of onsets and short durations of ornaments. The proposed automatic transcription system is based on hidden Markov models, with separate models being built for notes and for single-note ornaments. Mel-frequency cepstral coefficients are employed to represent the acoustic signal. Different setups of parameters in feature extraction and acoustic modelling are explored. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD. The performance of the system is evaluated in terms of the transcription of notes as well as detection of onsets. It is demonstrated that the proposed system can achieve a very good note transcription and onset detection performance. Over 28% relative improvement in terms of the F-measure is achieved for onset detection in comparison to conventional onset detection methods based on signal energy and fundamental frequency.",GBR,education,Developed economies,"[18.689314, -20.573133]","[-14.565404, -2.944675]","[12.304913, -12.313312, 2.2442875]","[3.1886146, 3.0251257, -13.806065]","[9.564239, 7.5481944]","[6.3069487, 2.7234962]","[11.505023, 12.333594, -0.1862045]","[8.676459, 7.4387875, 10.718681]"
103,Daniel Matz;Estefanía Cano;Jakob Abeßer,New Sonorities for Early Jazz Recordings Using Sound Source Separation and Automatic Mixing Tools.,2015,https://doi.org/10.5281/zenodo.1416568,Daniel Matz+University of Applied Sciences Düsseldorf>DEU>education;Estefanía Cano+Fraunhofer IDMT>DEU>facility;Jakob Abeßer+Fraunhofer IDMT>DEU>facility,"In this paper, a framework for automatic mixing of early jazz recordings is presented. In particular, we propose the use of sound source separation techniques as a preprocessing step of the mixing process. In addition to an initial solo and accompaniment separation step, the proposed mixing framework is composed of six processing blocks: harmonic-percussive separation (HPS), cross-adaptive multi-track scaling (CAMTS), cross-adaptive equalizer (CAEQ), cross-adaptive dynamic spectral panning (CADSP), automatic excitation (AE), and time-frequency selective panning (TFSP). The effects of the different processing steps in the final quality of the mix are evaluated through a listening test procedure. The results show that the desired quality improvements in terms of sound balance, transparency, stereo impression, timbre, and overall impression can be achieved with the proposed framework.",DEU,education,Developed economies,"[8.507748, -43.599365]","[-37.065727, -21.217033]","[28.198519, -6.027827, -2.5998209]","[-18.58905, 0.15570426, -27.198534]","[8.387676, 9.896002]","[7.3429737, 6.150713]","[11.061596, 13.523122, 1.5683904]","[9.422721, 7.653057, 9.476294]"
102,Julio José Carabias-Orti;Francisco J. Rodríguez-Serrano;Pedro Vera-Candeas;Nicolás Ruiz-Reyes;Francisco J. Cañadas-Quesada,An Audio to Score Alignment Framework Using Spectral Factorization and Dynamic Time Warping.,2015,https://doi.org/10.5281/zenodo.1418371,"J.J. Carabias-Orti+Music Technology Group (MTG), Universitat Pompeu Fabra>ESP>education;F.J. Rodriguez-Serrano+Polytechnical School of Linares, Universidad de Jaen>ESP>education;P. Vera-Candeas+Polytechnical School of Linares, Universidad de Jaen>ESP>education;N. Ruiz-Reyes+Polytechnical School of Linares, Universidad de Jaen>ESP>education;F.J. Ca˜nadas-Quesada+Polytechnical School of Linares, Universidad de Jaen>ESP>education","In this paper, we present an audio to score alignment framework based on spectral factorization and online Dynamic Time Warping (DTW). The proposed framework has two separated stages: preprocessing and alignment. In the first stage, we use Non-negative Matrix Factorization (NMF) to learn spectral patterns (i.e. basis functions) associated to each combination of concurrent notes in the score. In the second stage, a low latency signal decomposition method with fixed spectral patterns per combination of notes is used over the magnitude spectrogram of the input signal resulting in a divergence matrix that can be interpreted as the cost of the matching for each combination of notes at each frame. Finally, a Dynamic Time Warping (DTW) approach has been used to find the path with the minimum cost and then determine the relation between the performance and the musical score times. Our framework have been evaluated using a dataset of baroque-era pieces and compared to other systems, yielding solid results and performance.",ESP,education,Developed economies,"[20.53486, -15.75146]","[-18.319517, -15.367807]","[2.138199, -14.173844, -12.933523]","[1.1053296, -21.612593, -6.039701]","[10.830672, 6.0041246]","[6.070649, 0.6880328]","[11.669365, 12.50876, -1.7869099]","[8.120357, 5.843693, 10.8865]"
101,Isabel Barbancho;Lorenzo J. Tardón;Ana M. Barbancho;Mateu Sbert,Benford's Law for Music Analysis.,2015,https://doi.org/10.5281/zenodo.1417012,Isabel Barbancho+Universidad de Málaga>ESP>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Ana M. Barbancho+Universidad de Málaga>ESP>education;Mateu Sbert+University of Girona>ESP>education,"Benford’s law defines a peculiar distribution of the leading digits of a set of numbers. The behavior is logarithmic, with the leading digit 1 reflecting largest probability of occurrence and the remaining ones showing decreasing probabilities of appearance following a logarithmic trend. Many discussions have been carried out about the application of Benford’s law to many different fields. In this paper, a novel exploitation of Benford’s law for the analysis of audio signals is proposed. Three new audio features based on the evaluation of the degree of agreement of a certain audio dataset to Benford’s law are presented. These new proposed features are successfully tested in two concrete audio tasks: the detection of artificially assembled chords and the estimation of the quality of the MIDI conversions.",ESP,education,Developed economies,"[-0.8377353, 5.5075455]","[-12.475483, -7.564526]","[-4.495174, -2.3918424, 2.556439]","[2.2973452, -11.430248, -5.418762]","[12.55442, 8.505398]","[6.761171, 2.9153204]","[13.2295265, 14.072293, -0.79357296]","[9.061199, 7.416631, 11.084267]"
100,Johanna Devaney;Claire Arthur;Nathaniel Condit-Schultz;Kirsten Nisula,Theme And Variation Encodings with Roman Numerals (TAVERN): A New Data Set for Symbolic Music Analysis.,2015,https://doi.org/10.5281/zenodo.1417497,Johanna Devaney+Ohio State University>USA>education;Claire Arthur+Ohio State University>USA>education;Nathaniel Condit-Schultz+Ohio State University>USA>education;Kirsten Nisula+Ohio State University>USA>education,"The Theme And Variation Encodings with Roman Nu- merals (TAVERN) dataset consists of 27 complete sets of theme and variations for piano composed between 1765 and 1810 by Mozart and Beethoven. In these theme and variation sets, comparable harmonic structures are realized in different ways. This facilitates an evaluation of the effectiveness of automatic analysis algorithms in generalizing across different musical textures. The pieces are encoded in standard **kern format, with analyses jointly encoded using an extension to **kern. The harmonic content of the music was analyzed with both Roman numerals and function labels in duplicate by two different expert analyzers. The pieces are divided into musical phrases, allowing for multiple-levels of automatic analysis, including chord labeling and phrase parsing. This paper describes the content of the dataset in detail, including the types of chords represented, and discusses the ways in which the analyzers sometimes disagreed on the lower-level harmonic content (the Roman numerals) while converging at similar high-level structures (the function of the chords within the phrase).",USA,education,Developed economies,"[18.706846, 16.350967]","[-18.929623, 17.942705]","[-5.0378857, -12.338612, 18.287348]","[-16.812038, -5.0262203, 7.358122]","[11.671439, 6.8435802]","[7.9637666, 2.0864923]","[13.42191, 12.162972, -1.1747974]","[10.003622, 7.699618, 12.467848]"
99,Emmanuel Deruty;François Pachet,The MIR Perspective on the Evolution of Dynamics in Mainstream Music.,2015,https://doi.org/10.5281/zenodo.1417995,Emmanuel Deruty+Sony Computer Science Laboratory>FRA>facility;François Pachet+Sony Computer Science Laboratory>FRA>facility,"Understanding the evolution of mainstream music is of high interest for the music production industry. In this context, we argue that a MIR perspective may be used to highlight, in particular, relations between dynamics and various properties of mainstream music. We illustrate this claim with two results obtained from a diachronic analysis performed on 7200 tracks released between 1967 and 2014. This analysis suggests that 1) the so-called “loudness war” has peaked in 2007, and 2) its influence has been important enough to override the impact of genre on dynamics. In other words, dynamics in mainstream music are primarily related to a track’s year of release, rather than to its genre.",FRA,facility,Developed economies,"[-14.218474, 52.84364]","[49.128754, 7.788286]","[-32.6996, 0.7911785, 4.2667303]","[9.433571, 10.748166, 6.582167]","[13.278677, 5.501099]","[11.216647, 2.8180926]","[14.610509, 11.821053, -1.3360199]","[12.831138, 5.586056, 11.612652]"
98,Phillip B. Kirlin;David L. Thomas,Extending a Model of Monophonic Hierarchical Music Analysis to Homophony.,2015,https://doi.org/10.5281/zenodo.1416296,Phillip B. Kirlin+Rhodes College>USA>education|Unknown>Unknown>Unknown;David L. Thomas+Rhodes College>USA>education|Unknown>Unknown>Unknown,"Computers are now powerful enough and data sets large enough to enable completely data-driven studies of Schenkerian analysis, the most well-established variety of hierarchical music analysis. In particular, we now have probabilistic models that can be trained via machine learning algorithms to analyze music in a hierarchical fashion as a music theorist would. Most of these models, however, only analyze the monophonic melodic content of the music, as opposed to taking all of the musical voices into account. In this paper, we explore the feasibility of extending a probabilistic model developed for analyzing monophonic music to function with homophonic music. We present details of the new model, an algorithm for determining the most probable analysis of the music, and a number of experiments evaluating the quality of the analyses predicted by the model. We also describe how varying the way the model interprets rests in the input music affects the resulting analyses produced.",USA,education,Developed economies,"[0.5662102, 3.544838]","[-10.540158, 20.05891]","[-2.080182, -4.7563295, 6.566722]","[-11.634607, 0.17705628, 7.537077]","[12.155645, 8.262048]","[8.727486, 2.0417473]","[13.108193, 13.703488, -0.69936556]","[10.118164, 6.4308434, 11.807362]"
97,Yuan-Ping Chen;Li Su;Yi-Hsuan Yang,Electric Guitar Playing Technique Detection in Real-World Recording Based on F0 Sequence Pattern Recognition.,2015,https://doi.org/10.5281/zenodo.1414806,Yuan-Ping Chen+Academia Sinica>TWN>education|Research Center for Information Technology Innovation>Unknown>Unknown;Li Su+Academia Sinica>TWN>education|Research Center for Information Technology Innovation>Unknown>Unknown;Yi-Hsuan Yang+Academia Sinica>TWN>education|Research Center for Information Technology Innovation>Unknown>Unknown,"For a complete transcription of a guitar performance, the detection of playing techniques such as bend and vibrato is important, because playing techniques suggest how the melody is interpreted through the manipulation of the guitar strings. While existing work mostly focused on playing technique detection for individual single notes, this paper attempts to expand this endeavor to recordings of guitar solo tracks. Specifically, we treat the task as a time sequence pattern recognition problem, and develop a two-stage framework for detecting five fundamental playing techniques used by the electric guitar. Given an audio track, the first stage identifies prominent candidates by analyzing the extracted melody contour, and the second stage applies a pre-trained classifier to the candidates for playing technique detection using a set of timbre and pitch features. The effectiveness of the proposed framework is validated on a new dataset comprising of 42 electric guitar solo tracks without accompaniment, each of which covers 10 to 25 notes. The best average F-score achieves 74% in two-fold cross validation. Furthermore, we also evaluate the performance of the proposed framework for bend detection in five studio mixtures, to discuss how it can be applied in transcribing real-world electric guitar solos with accompaniment.",TWN,education,Developing economies,"[49.291626, -12.379022]","[-42.134884, -4.9649587]","[26.568258, -7.495741, 6.236116]","[-15.57359, -8.423897, -2.4124887]","[7.6642904, 8.142088]","[7.0934744, 3.9797723]","[11.748407, 11.374772, 1.498201]","[9.073557, 6.9193163, 10.321848]"
96,Emmanouil Benetos;Tillman Weyde,An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription.,2015,https://doi.org/10.5281/zenodo.1418017,Emmanouil Benetos+Queen Mary University of London>GBR>education;Tillman Weyde+City University London>GBR>education,"In this paper, an efficient, general-purpose model for multiple instrument polyphonic music transcription is proposed. The model is based on probabilistic latent component analysis and supports the use of sound state spectral templates, which represent the temporal evolution of each note (e.g. attack, sustain, decay). As input, a variable-Q transform (VQT) time-frequency representation is used. Computational efficiency is achieved by supporting the use of pre-extracted and pre-shifted sound state templates. Two variants are presented: without temporal constraints and with hidden Markov model-based constraints controlling the appearance of sound states. Experiments are performed on benchmark transcription datasets: MAPS, TRIOS, MIREX multiF0, and Bach10; results on multi-pitch detection and instrument assignment show that the proposed models outperform the state-of-the-art for multiple-instrument transcription and is more than 20 times faster compared to a previous sound state-based model. We finally show that a VQT representation can lead to improved multi-pitch detection performance compared with constant-Q representations.",GBR,education,Developed economies,"[28.923561, -8.693489]","[-9.54198, -8.220002]","[12.558386, -7.1825314, 11.135929]","[-0.41104114, -8.53054, -13.054527]","[9.648644, 7.795502]","[6.6404767, 3.3655534]","[11.844425, 12.342948, 0.008872433]","[9.048163, 7.823709, 10.4816065]"
95,Robert J. Ellis;Zhe Xing;Jiakun Fang;Ye Wang,Quantifying Lexical Novelty in Song Lyrics.,2015,https://doi.org/10.5281/zenodo.1417577,Robert J Ellis+National University of Singapore>SGP>education;Zhe Xing+National University of Singapore>SGP>education;Jiakun Fang+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"Novelty is an important psychological construct that affects both perceptual and behavioral processes. Here, we propose a lexical novelty score (LNS) for a song’s lyric, based on the statistical properties of a corpus of 275,905 lyrics (available at www.smcnus.org/lyrics/). A lyric-level LNS was derived as a function of the inverse document frequencies of its unique words. An artist-level LNS was then computed using the LNSs of lyrics uniquely associated with each artist. Statistical tests were performed to determine whether lyrics and artists on Billboard Magazine’s lists of “All-Time Top 100” songs and artists had significantly lower LNSs than “non-top” songs and artists. An affirmative and highly consistent answer was found in both cases. These results highlight the potential utility of the LNS as a feature for MIR.",SGP,education,Developing economies,"[-31.90519, -30.217892]","[34.81388, -16.546137]","[5.5424256, 23.866068, 0.14109659]","[5.6432548, 13.795281, 5.8799515]","[11.682682, 11.658679]","[10.302345, 3.0470712]","[12.682282, 15.850356, 0.8946517]","[11.809308, 6.1576185, 11.195915]"
94,Georgi Dzhambazov;Sertan Sentürk;Xavier Serra,Searching Lyrical Phrases in A-Capella Turkish Makam Recordings.,2015,https://doi.org/10.5281/zenodo.1417921,Georgi Dzhambazov+Universitat Pompeu Fabra>ESP>education;Sertan Şentürk+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"Search by lyrics, the problem of locating the exact occurrences of a phrase from lyrics in musical audio, is a recently emerging research topic. Unlike key-phrases in speech, lyrical key-phrases have durations that bear important relation to other musical aspects like the structure of a composition. In this work we propose an approach that address the differences of syllable durations, specific for singing. First a phrase is expanded to MFCC-based phoneme models, trained on speech. Then, we apply dynamic time warping between the phrase and audio to estimate candidate audio segments in the given audio recording. Next, the retrieved audio segments are ranked by means of a novel score-informed hidden Markov model, in which durations of the syllables within a phrase are explicitly modeled. The proposed approach is evaluated on 12 a-capella audio recordings of Turkish Makam music. Relying on standard speech phonetic models, we arrive at promising results that outperform a baseline approach unaware of lyrics durations. To the best of our knowledge, this is the first work tackling the problem of search by lyrical key-phrases. We expect that it can serve as a baseline for further research on singing material with similar musical characteristics.",ESP,education,Developed economies,"[10.543777, 4.7221107]","[-7.9271226, -20.674377]","[-12.83829, -24.699532, -12.352179]","[2.9746032, -5.1960487, -17.326525]","[11.694865, 10.747213]","[7.6504455, 4.108777]","[12.182865, 14.966354, -1.5409293]","[10.219416, 7.41243, 9.555207]"
93,Sankalp Gulati;Joan Serrà;Xavier Serra,Improving Melodic Similarity in Indian Art Music Using Culture-Specific Melodic Characteristics.,2015,https://doi.org/10.5281/zenodo.1418261,"Sankalp Gulati+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Joan Serrà+Telefonica Research>ESP>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Detecting the occurrences of r¯ags’ characteristic melodic phrases from polyphonic audio recordings is a fundamental task for the analysis and retrieval of Indian art music. We propose an abstraction process and a complexity weighting scheme which improve melodic similarity by exploiting specific melodic characteristics in this music. In addition, we propose a tetrachord normalization to handle transposed phrase occurrences. The melodic abstraction is based on the partial transcription of the steady regions in the melody, followed by a duration truncation step. The proposed complexity weighting accounts for the differences in the melodic complexities of the phrases, a crucial aspect known to distinguish phrases in Carnatic music. For evaluation we use over 5 hours of audio data comprising 625 annotated melodic phrases belonging to 10 different phrase categories. Results show that the proposed melodic abstraction and complexity weighting schemes significantly improve the phrase detection accuracy, and that tetrachord normalization is a successful strategy for dealing with transposed phrase occurrences in Carnatic music. In the future, it would be worthwhile to explore the applicability of the proposed approach to other melody dominant music traditions such as Flamenco, Beijing opera and Turkish Makam music.",ESP,education,Developed economies,"[6.4563303, 2.848289]","[4.8863573, -16.857973]","[6.254711, 2.9636643, -8.367981]","[12.473838, -11.496882, -4.5480795]","[11.424538, 10.244626]","[7.7389326, 1.2129157]","[12.021828, 15.543574, -1.0961518]","[9.113509, 7.0201654, 12.667613]"
92,Eric J. Humphrey;Juan Pablo Bello,Four Timely Insights on Automatic Chord Estimation.,2015,https://doi.org/10.5281/zenodo.1417549,"Eric J. Humphrey+New York University>USA>education|MuseAmi, Inc.>USA>company;Juan P. Bello+New York University>USA>education|MuseAmi, Inc.>USA>company","Automatic chord estimation (ACE) is a hallmark research topic in content-based music informatics, but like many other tasks, system performance appears to be converging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite arguably achieving some of the highest results to date, both approaches plateau well short of having solved the problem. Therefore, this work explores the behavior of these two high performing systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that invalidate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, standard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conventional approaches conflate the competing goals of recognition and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjective, making the very notion of “ground truth” annotations tenuous. Synthesizing these observations, this paper offers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large.",USA,education,Developed economies,"[53.89594, -5.7446165]","[-29.790316, 22.83089]","[28.139162, -13.278783, 14.44318]","[-27.527306, -2.3771248, 4.0568795]","[6.838798, 8.593242]","[6.2025867, 3.622627]","[11.903223, 10.454242, 2.0338411]","[9.818093, 8.775617, 12.371549]"
91,Daniel Gómez-Marín;Sergi Jordà;Perfecto Herrera,PAD and SAD: Two Awareness-Weighted Rhythmic Similarity Distances.,2015,https://doi.org/10.5281/zenodo.1416286,Daniel Gómez-Marín+Universitat Pompeu Fabra>ESP>education;Sergi Jordà+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education,"Measuring rhythm similarity is relevant for the analysis and generation of music. Existing similarity metrics tend to consider our perception of rhythms as being in time without discriminating the importance of some regions over others. In a previously reported experiment we observed that measures of similarity may differ given the presence or absence of a pulse inducing sound and the importance of those measures is not constant along the pattern. These results are now reinterpreted by refining the previously proposed metrics. We consider that the perceptual contribution of each beat to the measured similarity is non-homogeneous but might indeed depend on the temporal positions of the beat along the bar. We show that with these improvements, the correlation between the previously evaluated experimental similarity and predictions based on our metrics increases substantially. We conclude by discussing a possible new methodology for evaluating rhythmic similarity between audio loops.",ESP,education,Developed economies,"[45.272182, 7.649535]","[-22.977358, 2.8671696]","[-9.558102, -25.118048, 3.3047678]","[-0.24039932, 15.334399, -4.8440723]","[12.029472, 5.4112515]","[6.06737, 1.5656643]","[11.3751135, 14.232494, -2.1415715]","[8.127077, 6.6883354, 11.841462]"
90,Berit Janssen;Peter van Kranenburg;Anja Volk,A Comparison of Symbolic Similarity Measures for Finding Occurrences of Melodic Segments.,2015,https://doi.org/10.5281/zenodo.1418357,Berit Janssen+Meertens Institute>Unknown>facility;Peter van Kranenburg+Meertens Institute>Unknown>facility;Anja Volk+Utrecht University>NLD>education,"To find occurrences of melodic segments, such as themes, phrases and motifs, in musical works, a well-performing similarity measure is needed to support human analysis of large music corpora. We evaluate the performance of a range of melodic similarity measures to find occurrences of phrases in folk song melodies. We compare the similarity measures correlation distance, city-block distance, Euclidean distance and alignment, proposed for melody comparison in computational ethnomusicology; furthermore Implication-Realization structure alignment and B-spline alignment, forming successful approaches in symbolic melodic similarity; moreover, wavelet transform and the geometric approach Structure Induction, having performed well in musical pattern discovery. We evaluate the success of the different similarity measures through observing retrieval success in relation to human annotations. Our results show that local alignment and SIAM perform on an almost equal level to human annotators.",Unknown,facility,Unknown,"[0.9138933, 16.57786]","[15.494309, 3.6590402]","[1.0913045, 5.3975363, -1.3170195]","[6.301217, 0.863355, 3.5392628]","[12.311182, 9.782122]","[9.434313, 1.7788734]","[12.664333, 15.454839, -0.85702795]","[11.215648, 7.065795, 13.085381]"
89,Jotthi Bansal;Matthew Woolhouse,Predictive Power of Personality on Music-Genre Exclusivity.,2015,https://doi.org/10.5281/zenodo.1417467,Jotthi Bansal+McMaster University>CAN>education;Matthew Woolhouse+McMaster University>CAN>education,"Studies reveal a strong relationship between personality and preferred musical genre. Our study explored this relationship using a new methodology: genre dispersion among people’s mobile-phone music collections. By analyzing the download behaviours of genre-defined user subgroups, we investigated the following questions: (1) do genre-preferring subgroups show distinct patterns of genre consumption and genre exclusivity; (2) does genre exclusivity relate to Big Five personality factors? We hypothesized that genre-preferring subgroups would vary in genre exclusivity, and that their degree of exclusivity would be linearly associated with the openness personality factor (if people have open personalities, they should be “open” to different musical styles). Consistent with our hypothesis, results showed that greater genre inclusivity, i.e. many genres in people’s music collections, positively associated with openness and (unexpectedly) agreeableness, suggesting that individuals with high openness and agreeableness have wider musical tastes than those with low openness and agreeableness. Our study corroborated previous research linking genre preference and personality, and revealed, in a novel way, the predictive power of personality on music consumption.",CAN,education,Developed economies,"[-51.380234, 11.2972145]","[43.13149, 29.126415]","[-19.668617, 23.001219, -3.7329419]","[12.193384, 14.971494, 20.080873]","[13.710927, 11.933834]","[12.964919, 1.2704225]","[15.414818, 15.141534, -0.18100938]","[13.491839, 4.420959, 12.116315]"
88,Andie Sigler;Jon Wild;Eliot Handelman,Schematizing the Treatment of Dissonance in 16th-Century Counterpoint.,2015,https://doi.org/10.5281/zenodo.1417369,Andie Sigler+McGill University>CAN>education;Jon Wild+McGill University>CAN>education;Eliot Handelman+Computing Music>Unknown>Unknown,"We describe a computational project concerning labeling of dissonance treatments – schematic descriptions of the uses of dissonances. We use automatic score annotation and database methods to develop schemata for a large corpus of 16th-century polyphonic music. We then apply structural techniques to investigate coincidence of schemata, and to extrapolate from found structures to unused possibilities.",CAN,education,Developed economies,"[6.1704216, 22.066242]","[-3.061098, 10.388631]","[2.8402307, 0.35243812, 4.8975124]","[-5.3808484, 2.9608755, 0.59012675]","[11.984808, 9.828419]","[8.707934, 2.0989683]","[12.510151, 15.056317, -1.0183426]","[10.402827, 6.580406, 11.562014]"
87,Hye-Seung Cho;Jun-Yong Lee;Hyoung-Gook Kim,Singing Voice Separation from Monaural Music Based on Kernel Back-Fitting Using Beta-Order Spectral Amplitude Estimation.,2015,https://doi.org/10.5281/zenodo.1417044,Hye-Seung Cho+Kwangwoon University>KOR>education;Jun-Yong Lee+Kwangwoon University>KOR>education;Hyoung-Gook Kim+Kwangwoon University>KOR>education,"Separating the leading singing voice from the musical background from a monaural recording is a challenging task that appears naturally in several music processing applications. Recently, kernel additive modeling with generalized spatial Wiener filtering (GW) was presented for music/voice separation. In this paper, an adaptive auditory filtering based on β-order minimum mean-square error spectral amplitude estimation (bSA) is applied to the kernel additive modeling for improving the singing voice separation performance from monaural music signal. The proposed algorithm is composed of five modules: short time Fourier transform, music/voice separation based on bSA, determination of back-fitting, back-fitting, and inverse short time Fourier transform. In the proposed method, the Singular Value Decomposition (SVD)-based factorized spectral amplitude exponent β for each kernel component is adaptively calculated for effective bSA-based auditory filtering performance during kernel back-fitting. Using a back-fitting threshold, the kernel back-fitting process can automatically be iteratively performed until convergence. Experimental results show that the proposed method achieves better separation performance than GW based on kernel additive modeling.",KOR,education,Developing economies,"[0.36236647, -42.10562]","[-47.54283, -29.642155]","[26.544626, 4.3246427, -8.109429]","[-3.0293713, -8.420265, -30.685776]","[9.055023, 10.679482]","[6.397935, 5.181519]","[10.867401, 14.5129795, 1.3556283]","[9.82891, 8.736959, 9.700257]"
85,Sebastian Böck;Florian Krebs;Gerhard Widmer,Accurate Tempo Estimation Based on Recurrent Neural Networks and Resonating Comb Filters.,2015,https://doi.org/10.5281/zenodo.1416026,Sebastian Böck+Johannes Kepler University>AUT>education;Florian Krebs+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"In this paper we present a new tempo estimation algorithm which uses a bank of resonating comb filters to determine the dominant periodicity of a musical excerpt. Unlike existing (comb filter based) approaches, we do not use handcrafted features derived from the audio signal, but rather let a recurrent neural network learn an intermediate beat-level representation of the signal and use this information as input to the comb filter bank. While most approaches apply complex post-processing to the output of the comb filter bank like tracking multiple time scales, processing different accent bands, modelling metrical relations, categorising the excerpts into slow / fast or any other advanced processing, we achieve state-of-the-art performance on nine of ten datasets by simply reporting the highest resonator’s histogram peak.",AUT,education,Developed economies,"[41.827923, -27.793983]","[-31.186306, -7.972489]","[2.2439263, -31.011744, 0.751705]","[-6.877021, 9.295039, -13.455016]","[11.396078, 4.326858]","[5.0993505, 2.0281343]","[10.788071, 13.313943, -2.8556378]","[7.438079, 7.030625, 10.822117]"
56,Masaki Otsuka;Tetsuro Kitahara,Improving MIDI Guitar's Accuracy with NMF and Neural Net.,2015,https://doi.org/10.5281/zenodo.1417531,Masaki Otsuka+Nihon University>JPN>education|Unknown>Unknown>Unknown;Tetsuro Kitahara+Nihon University>JPN>education|Unknown>Unknown>Unknown,"In this paper, we propose a method for improving the accuracy of MIDI guitars. MIDI guitars are useful tools for various purposes from inputting MIDI data to enjoying a jam session system, but existing MIDI guitars do not have sufficient accuracy in converting the performance to an MIDI form. In this paper, we make an attempt on improving the accuracy of a MIDI guitar by integrating it with an audio transcription method based on non-negative matrix factorization (NMF). First, we investigate an NMF-based algorithm for transcribing guitar performances. Although the NMF is a promising method, an effective post-process (i.e., converting the NMF’s output to an MIDI form) is a non-trivial problem. We propose use of a neural network for this conversion. Next, we investigate a method for integrating the outputs of the MIDI guitar and NMF. Because they have different tendencies in wrong outputs, we take an policy of outputting only common parts in the two outputs. Experimental results showed that the F-score of our method was 0.626 whereas those of the MIDI-guitar-only and NMF-and-neural-network-only methods were 0.347 and 0.526, respectively.",JPN,education,Developed economies,"[39.618958, -5.9935465]","[-49.630203, -20.684431]","[8.085219, -6.085766, 17.27029]","[-7.786704, -14.785235, -23.800406]","[8.154715, 8.233669]","[6.261204, 5.007649]","[11.969731, 11.386082, 0.90910333]","[9.403117, 8.674262, 9.749301]"
5,Xinquan Zhou;Alexander Lerch,Chord Detection Using Deep Learning.,2015,https://doi.org/10.5281/zenodo.1416968,Xinquan Zhou+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education,"In this paper, we utilize deep learning to learn high-level features for audio chord detection. The learned features, obtained by a deep network in bottleneck architecture, give promising results and outperform state-of-the-art systems. We present and evaluate the results for various methods and configurations, including input pre-processing, a bottleneck architecture, and SVMs vs. HMMs for chord classification.",USA,education,Developed economies,"[57.11539, -6.477921]","[-34.645027, 20.738749]","[28.275793, -8.736381, 18.0829]","[-25.94097, -3.6198785, -0.9556561]","[6.6073256, 8.6562195]","[5.9134316, 3.835072]","[11.928646, 10.241308, 2.1725101]","[9.620698, 9.166091, 12.247817]"
54,Chuan-Lung Lee;Yin-Tzu Lin;Zun-Ren Yao;Feng-Yi Lee;Ja-Ling Wu,Automatic Mashup Creation by Considering both Vertical and Horizontal Mashabilities.,2015,https://doi.org/10.5281/zenodo.1416712,Chuan-Lung Lee+National Taiwan University>TWN>education|Communications and Multimedia Laboratory>TWN>facility;Yin-Tzu Lin+National Taiwan University>TWN>education|Communications and Multimedia Laboratory>TWN>facility;Zun-Ren Yao+National Taiwan University>TWN>education|Communications and Multimedia Laboratory>TWN>facility;Feng-Yi Lee+National Taiwan University>TWN>education|Communications and Multimedia Laboratory>TWN>facility;Ja-Ling Wu+National Taiwan University>TWN>education|Communications and Multimedia Laboratory>TWN>facility,"In this paper, we proposed a system to effectively create music mashups – a kind of re-created music that is made by mixing parts of multiple existing music pieces. Unlike previous studies which merely generate mashups by overlaying music segments on one single base track, the proposed system creates mashups with multiple background (e.g. instrumental) and lead (e.g. vocal) track segments. So, besides the suitability between the vertically overlaid tracks (i.e. vertical mashability) used in previous studies, we proposed to further consider the suitability between the horizontally connected consecutive music segments (i.e. horizontal mashability) when searching for proper music segments to be combined. On the vertical side, two new factors: “harmonic change balance” and “volume weight” have been considered. On the horizontal side, the methods used in the studies of medley creation are incorporated. Combining vertical and horizontal mashabilities together, we defined four levels of mashability that may be encountered and found the proper solution to each of them. Subjective evaluations showed that the proposed four levels of mashability can appropriately reflect the degrees of listening enjoyment. Besides, by taking the newly proposed vertical mashability measurement into account, the improvement in user satisfaction is statistically significant.",TWN,education,Developing economies,"[-43.932907, -21.010862]","[3.954167, -39.368847]","[3.993077, 33.154636, -7.438716]","[-19.479086, 3.3441422, -26.219477]","[11.283152, 5.9304924]","[7.356668, 6.146648]","[12.388883, 12.489557, -2.151077]","[9.442783, 7.476664, 9.553888]"
24,Rodrigo Schramm;Helena de Souza Nunes;Cláudio Rosito Jung,Automatic Solfège Assessment.,2015,https://doi.org/10.5281/zenodo.1414720,Rodrigo Schramm+Federal University of Rio Grande do Sul>BRA>education;Helena de Souza Nunes+Federal University of Rio Grande do Sul>BRA>education;Cláudio Rosito Jung+Federal University of Rio Grande do Sul>BRA>education,"This paper presents a note-by-note approach for automatic solfège assessment. The proposed system uses melodic transcription techniques to extract the sung notes from the audio signal, and the sequence of melodic segments is subsequently processed by a two stage algorithm. On the first stage, an aggregation process is introduced to perform the temporal alignment between the transcribed melody and the music score (ground truth). This stage implicitly aggregates and links the best combination of the extracted melodic segments with the expected note in the ground truth. On the second stage, a statistical method is used to evaluate the accuracy of each detected sung note. The technique is implemented using a Bayesian classifier, which is trained using an audio dataset containing individual scores provided by a committee of expert listeners. These individual scores were measured at each musical note, regarding the pitch, onset, and offset accuracy. Experimental results indicate that the classification scheme is suitable to be used as an assessment tool, providing useful feedback to the student.",BRA,education,Developing economies,"[18.237135, -5.6252775]","[0.5263965, -8.353088]","[-18.50328, -20.860796, -5.024346]","[1.2730817, -10.428981, -0.98726714]","[12.45816, 6.295118]","[7.318576, 2.641347]","[13.325109, 12.642932, -1.5755678]","[9.431221, 7.327696, 11.440634]"
23,Cheng-i Wang;Jennifer Hsu;Shlomo Dubnov,Music Pattern Discovery with Variable Markov Oracle: A Unified Approach to Symbolic and Audio Representations.,2015,https://doi.org/10.5281/zenodo.1416480,"Cheng-i Wang+University of California, San Diego>USA>education;Jennifer Hsu+University of California, San Diego>USA>education;Shlomo Dubnov+University of California, San Diego>USA>education","This paper presents a framework for automatically discovering patterns in a polyphonic music piece. The proposed framework is capable of handling both symbolic and audio representations. Chroma features are post-processed with heuristics stemming from musical knowledge and fed into the pattern discovery framework. The pattern-finding algorithm is based on Variable Markov Oracle. The Variable Markov Oracle data structure is capable of locating repeated suffixes within a time series, thus making it an appropriate tool for the pattern discovery task. Evaluation of the proposed framework is performed on the JKU Patterns Development Dataset with state of the art performance.",USA,education,Developed economies,"[14.8164425, 20.95256]","[3.5729184, 15.50364]","[-2.1827824, -10.653015, 9.590719]","[-0.1860879, -9.565511, 7.0484443]","[11.586039, 7.533394]","[8.901696, 1.277998]","[12.773255, 13.137867, -0.6286995]","[10.561315, 6.751018, 12.677444]"
22,Igor Vatolkin;Günter Rudolph;Claus Weihs,Evaluation of Album Effect for Feature Selection in Music Genre Recognition.,2015,https://doi.org/10.5281/zenodo.1416328,Igor Vatolkin+TU Dortmund>DEU>education;Günter Rudolph+TU Dortmund>DEU>education;Claus Weihs+TU Dortmund>DEU>education,"With an increasing number of available music characteristics, feature selection becomes more important for various categorisation tasks, helping to identify relevant features and remove irrelevant and redundant ones. Another advantage is the decrease of runtime and storage demands. However, sometimes feature selection may lead to “over-optimisation” when data in the optimisation set is too different from data in the independent validation set. In this paper, we extend our previous work on feature selection for music genre recognition and focus on so-called “album effect” meaning that optimised classification models may overemphasize relevant characteristics of particular artists and albums rather than learning relevant properties of genres. For that case we examine the performance of classification models on two validation sets after the optimisation with feature selection: the first set with tracks not used for training and feature selection but randomly selected from the same albums, and the second set with tracks selected from other albums. As it can be expected, the classification performance on the second set decreases. Nevertheless, in almost all cases the feature selection remains beneficial compared to complete feature sets and a baseline using MFCCs, if applied for an ensemble of classifiers, proving robust generalisation performance.",DEU,education,Developed economies,"[-26.27505, -12.184413]","[20.843699, -8.610875]","[-15.294249, -1.6722964, 14.897416]","[14.476189, 11.837281, -3.6330473]","[12.926791, 10.615373]","[9.842463, 3.320393]","[13.872641, 14.130718, 1.0566242]","[11.576023, 6.9928675, 10.8190565]"
21,Patrick E. Savage;Quentin D. Atkinson,Automatic Tune Family Identification by Musical Sequence Alignment.,2015,https://doi.org/10.5281/zenodo.1417135,Patrick E. Savage+Tokyo University of the Arts>JPN>education;Quentin D. Atkinson+Auckland University>NZL>education,"Musics, like languages and genes, evolve through a process of transmission, variation, and selection. Evolution of musical tune families has been studied qualitatively for over a century, but quantitative analysis has been hampered by an inability to objectively distinguish between musical similarities that are due to chance and those that are due to descent from a common ancestor. Here we propose an automated method to identify tune families by adapting genetic sequence alignment algorithms designed for automatic identification and alignment of protein families. We tested the effectiveness of our method against a high-quality ground-truth dataset of 26 folk tunes from four diverse tune families (two English, two Japanese) that had previously been identified and aligned manually by expert musicologists. We tested different combinations of parameters related to sequence alignment and to modeling of pitch, rhythm, and text to find the combination that best matched the ground-truth classifications. The best-performing automated model correctly grouped 100% (26/26) of the tunes in terms of overall similarity to other tunes, identifying 85% (22/26) of these tunes as forming distinct tune families. The success of our approach on a diverse, cross-cultural ground-truth dataset suggests promise for future automated reconstruction of musical evolution on a wide scale.",JPN,education,Developed economies,"[15.26341, -9.61499]","[5.3131104, 6.9436746]","[-2.0694177, -13.303034, -2.4364243]","[-5.3872123, 22.051212, 2.375779]","[11.230992, 6.6836042]","[8.2192745, 0.90693325]","[12.143584, 13.141415, -1.4160047]","[10.223213, 6.4865756, 13.164712]"
20,Wai Man Szeto;Kin Hong Wong,A Hierarchical Bayesian Framework for Score-Informed Source Separation of Piano Music Signals.,2015,https://doi.org/10.5281/zenodo.1417061,Wai Man Szeto+The Chinese University of Hong Kong>HKG>education|The Chinese University of Hong Kong>HKG>education;Kin Hong Wong+The Chinese University of Hong Kong>HKG>education|The Chinese University of Hong Kong>HKG>education,"Here we propose a score-informed monaural source separation system to extract every tone from a mixture of piano tone signals. Two sinusoidal models in our earlier work are employed in the above-mentioned system to represent piano tones: the General Model and the Piano Model. The General Model, a variant of sinusoidal modeling, can represent a single tone with high modeling quality, yet it fails to separate mixtures of tones due to the overlapping partials. The Piano Model, on the other hand, is an instrument-specific model tailored for piano. Its modeling quality is lower but it can learn from training data (consisting entirely of isolated tones), resolve the overlapping partials and thus separate the mixtures. We formulate a new hierarchical Bayesian framework to run both Models in the source separation process so that the mixtures with overlapping partials can be separated with high quality. The results show that our proposed system gives robust and accurate separation of piano tone signal mixtures (including octaves) while achieving significantly better quality than those reported in related work done previously.",HKG,education,Developing economies,"[5.077657, -45.56905]","[-40.82028, -26.544209]","[29.318459, 1.206232, -1.5479282]","[-9.926071, -5.5172143, -32.093742]","[8.3330765, 10.124406]","[6.5486574, 5.362477]","[10.974657, 13.569863, 1.6761365]","[9.758566, 8.51517, 9.505114]"
19,Jeongsoo Park 0001;Kyogu Lee,Harmonic-Percussive Source Separation Using Harmonicity and Sparsity Constraints.,2015,https://doi.org/10.5281/zenodo.1417323,Jeongsoo Park+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"In this paper, we propose a novel approach to harmonic-percussive sound separation (HPSS) using Non-negative Matrix Factorization (NMF) with sparsity and harmonicity constraints. Conventional HPSS methods have focused on temporal continuity of harmonic components and spectral continuity of percussive components. However, it may not be appropriate to use them to separate time-varying harmonic signals such as vocals, vibratos, and glissandos, as they lack in temporal continuity. Based on the observation that the spectral distributions of harmonic and percussive signals differ – i.e., harmonic components have harmonic and sparse structure while percussive components are broadband – we propose an algorithm that successfully separates the rapidly time-varying harmonic signals from the percussive ones by imposing different constraints on the two groups of spectral bases. Experiments with real recordings as well as synthesized sounds show that the proposed method outperforms the conventional methods.",KOR,education,Developing economies,"[10.805113, -44.089375]","[-44.601128, -26.103317]","[28.443007, -5.0454097, -8.192503]","[-5.4706535, -8.84298, -31.619629]","[8.526204, 9.838195]","[6.315357, 5.27843]","[11.046626, 13.605984, 1.3735082]","[9.790084, 8.798077, 9.697889]"
18,Asterios I. Zacharakis;Maximos A. Kaliakatsos-Papakostas;Emilios Cambouropoulos,Conceptual Blending in Music Cadences: A Formal Model and Subjective Evaluation.,2015,https://doi.org/10.5281/zenodo.1416056,Asterios Zacharakis+Aristotle University of Thessaloniki>GRC>education;Maximos Kaliakatsos-Papakostas+Aristotle University of Thessaloniki>GRC>education;Emilios Cambouropoulos+Aristotle University of Thessaloniki>GRC>education,"Conceptual blending is a cognitive theory whereby elements from diverse, but structurally-related, mental spaces are ‘blended’ giving rise to new conceptual spaces. This study focuses on structural blending utilising an algorithmic formalisation for conceptual blending applied to harmonic concepts. More specifically, it investigates the ability of the system to produce meaningful blends between harmonic cadences, which arguably constitute the most fundamental harmonic concept. The system creates a variety of blends combining elements of the penultimate chords of two input cadences and it further estimates the expected relationships between the produced blends. Then, a preliminary subjective evaluation of the proposed blending system is presented. A pairwise dissimilarity listening test was conducted using original and blended cadences as stimuli. Subsequent multidimensional scaling analysis produced spatial configurations for both behavioural data and dissimilarity estimations by the algorithm. Comparison of the two configurations showed that the system is capable of making fair predictions of the perceived dissimilarities between the blended cadences. This implies that this conceptual blending approach is able to create perceptually meaningful blends based on self-evaluation of its outcome.",GRC,education,Developed economies,"[6.0441966, 10.108759]","[20.889421, 0.74158853]","[-6.29186, -14.228542, 2.8598948]","[2.03386, 16.644619, 3.5996382]","[12.357849, 8.423825]","[9.632834, 2.3881578]","[12.96769, 13.951222, -0.8855277]","[11.426183, 7.1576743, 12.843677]"
17,Beatrix Vad;Daniel Boland;John Williamson;Roderick Murray-Smith;Peter Berg Steffensen,Design and Evaluation of a Probabilistic Music Projection Interface.,2015,https://doi.org/10.5281/zenodo.1416670,Beatrix Vad+University of Glasgow>GBR>education;Daniel Boland+University of Glasgow>GBR>education;John Williamson+University of Glasgow>GBR>education;Roderick Murray-Smith+University of Glasgow>GBR>education;Peter Berg Steffensen+Syntonetic A/S>DNK>company,"We describe the design and evaluation of a probabilistic interface for music exploration and casual playlist generation. Predicted subjective features, such as mood and genre, inferred from low-level audio features create a 34-dimensional feature space. We use a nonlinear dimensionality reduction algorithm to create 2D music maps of tracks, and augment these with visualisations of probabilistic mappings of selected features and their uncertainty. We evaluated the system in a longitudinal trial in users’ homes over several weeks. Users said they had fun with the interface and liked the casual nature of the playlist generation. Users preferred to generate playlists from a local neighbourhood of the map, rather than from a trajectory, using neighbourhood selection more than three times more often than path selection. Probabilistic highlighting of subjective features led to more focused exploration in mouse activity logs, and 6 of 8 users said they preferred the probabilistic highlighting mode.",GBR,education,Developed economies,"[16.945505, -27.993553]","[30.746706, 20.701252]","[15.522632, 18.402529, 13.006203]","[15.72792, -1.7351676, 23.62396]","[12.484416, 8.020005]","[11.656173, 1.5391024]","[13.225791, 13.713171, -0.5736654]","[12.573229, 5.343615, 13.187147]"
16,Siddharth Sigtia;Nicolas Boulanger-Lewandowski;Simon Dixon,Audio Chord Recognition with a Hybrid Recurrent Neural Network.,2015,https://doi.org/10.5281/zenodo.1416594,"Siddharth Sigtia+Centre for Digital Music, Queen Mary University of London>GBR>education;Nicolas Boulanger-Lewandowski+Université de Montréal>CAN>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","In this paper, we present a novel architecture for audio chord estimation using a hybrid recurrent neural network. The architecture replaces hidden Markov models (HMMs) with recurrent neural network (RNN) based language models for modelling temporal dependencies between chords. We demonstrate the ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic signal, eliminating the need for a complex feature extraction stage. For the hybrid RNN architecture, inference over the output variables of interest is performed using beam search. In addition to the hybrid model, we propose a modification to beam search using a hash table which yields improved results while reducing memory requirements by an order of magnitude, thus making the proposed model suitable for real-time applications. We evaluate our model's performance on a dataset with publicly available annotations and demonstrate that the performance is comparable to existing state of the art approaches for chord recognition.",GBR,education,Developed economies,"[57.066742, -7.695201]","[-35.521404, 21.2202]","[25.825432, -8.081463, 16.616892]","[-25.632742, -2.0933423, -1.8037075]","[6.613751, 8.681872]","[5.909545, 3.889282]","[11.886484, 10.250091, 2.200385]","[9.618721, 9.155544, 12.225659]"
15,Jan Schlüter;Thomas Grill,Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks.,2015,https://doi.org/10.5281/zenodo.1417745,Jan Schlüter+Austrian Research Institute for Artificial Intelligence>AUT>facility;Thomas Grill+Austrian Research Institute for Artificial Intelligence>AUT>facility,"In computer vision, state-of-the-art object recognition systems rely on label-preserving image transformations such as scaling and rotation to augment the training datasets. The additional training examples help the system to learn invariances that are difficult to build into the model, and improve generalization to unseen data. To the best of our knowledge, this approach has not been systematically explored for music signals. Using the problem of singing voice detection with neural networks as an example, we apply a range of label-preserving audio transformations to assess their utility for music data augmentation. In line with recent research in speech recognition, we find pitch shifting to be the most helpful augmentation method. Combined with time stretching and random frequency filtering, we achieve a reduction in classification error between 10 and 30%, reaching the state of the art on two public datasets. We expect that audio data augmentation would yield significant gains for several other sequence labelling and event detection tasks in music information retrieval.",AUT,facility,Developed economies,"[-6.3783545, -36.747288]","[-26.20475, -35.351807]","[22.042263, 10.959808, -12.21094]","[-2.8977735, -5.989684, -19.509426]","[9.88954, 11.080915]","[8.017847, 5.0647287]","[11.276924, 15.132525, 0.91134214]","[9.935936, 7.0808215, 8.998278]"
14,Graham Percival;Satoru Fukayama;Masataka Goto,Song2Quartet: A System for Generating String Quartet Cover Songs from Polyphonic Audio of Popular Music.,2015,https://doi.org/10.5281/zenodo.1415014,Graham Percival+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We present Song2Quartet, a system for generating string quartet versions of popular songs by combining probabilistic models estimated from a corpus of symbolic classical music with the target audio file of any song. Song2Quartet allows users to add novelty to listening experience of their favorite songs and gain familiarity with string quartets. Previous work in automatic arrangement of music only used symbolic scores to achieve a particular musical style; our challenge is to also consider audio features of the target popular song. In addition to typical audio music content analysis such as beat and chord estimation, we also use time-frequency spectral analysis in order to better reflect partial phrases of the song in its cover version. Song2Quartet produces a probabilistic network of possible musical notes at every sixteenth note for each accompanying instrument of the quartet by combining beats, chords, and spectrogram from the target song with Markov chains estimated from our corpora of quartet music. As a result, the musical score of the cover version can be generated by finding the optimal paths through these networks. We show that the generated results follow the conventions of classical string quartet music while retaining some partial phrases and chord voicings from the target audio.",JPN,facility,Developed economies,"[16.39458, 7.7115374]","[-11.997827, 12.896052]","[5.449121, -10.287395, 17.360617]","[1.095659, -0.75171345, 4.7274046]","[11.102352, 7.79434]","[8.549539, 1.9037813]","[12.944736, 12.627386, -0.34823853]","[10.330812, 7.0474854, 12.192468]"
13,Shuo Zhang;Rafael Caro Repetto;Xavier Serra,Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing.,2015,https://doi.org/10.5281/zenodo.1416066,"Shuo Zhang+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rafael Caro Repetto+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","The similarity between linguistic tones and melodic pitch contours in Beijing Opera can be captured either by the contour shape of single syllable units, or by the pairwise pitch height relations in adjacent syllable units. In this paper, we investigate the latter problem with a novel machine learning approach, using techniques from time series data mining. Approximately 1300 pairwise contour segments are extracted from a selection of 20 arias. We then formulate the problem as a supervised machine-learning task of predicting types of pairwise melodic relations based on linguistic tone information. The results give a comparative view of fixed and mixed-effects models that achieved around 70% of maximum accuracy. We discuss the superiority of the current method to that of the unsupervised learning in single-syllable-unit contour analysis of similarity in Beijing Opera.",ESP,education,Developed economies,"[-4.6705117, -27.234068]","[5.8789396, -9.826466]","[12.81641, 6.617101, -16.072035]","[10.917622, -7.2439213, -2.5310411]","[10.456394, 10.628227]","[8.263852, 1.9656467]","[11.433793, 15.240138, -0.0064234226]","[9.870441, 7.2199078, 12.389585]"
12,Sergio Oramas;Mohamed Sordo;Luis Espinosa Anke;Xavier Serra,A Semantic-Based Approach for Artist Similarity.,2015,https://doi.org/10.5281/zenodo.1415976,"Sergio Oramas+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Center for Computational Science, University of Miami>USA>education|TALN Group, Universitat Pompeu Fabra>ESP>education;Mohamed Sordo+Center for Computational Science, University of Miami>USA>education;Luis Espinosa-Anke+Music Technology Group, Universitat Pompeu Fabra>ESP>education|TALN Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","This paper describes and evaluates a method for computing artist similarity from a set of artist biographies. The proposed method aims at leveraging semantic information present in these biographies, and can be divided in three main steps, namely: (1) entity linking, i.e. detecting mentions to named entities in the text and linking them to an external knowledge base; (2) deriving a knowledge representation from these mentions in the form of a semantic graph or a mapping to a vector-space model; and (3) computing semantic similarity between documents. We test this approach on a corpus of 188 artist biographies and a slightly larger dataset of 2,336 artists, both gathered from Last.fm. The former is mapped to the MIREX Audio and Music Similarity evaluation dataset, so that its similarity judgments can be used as ground truth. For the latter dataset we use the similarity between artists as provided by the Last.fm API. Our evaluation results show that an approach that computes similarity over a graph of entities and semantic categories clearly outperforms a baseline that exploits word co-occurrences and latent factors.",ESP,education,Developed economies,"[-41.42569, 6.9693236]","[34.32435, 6.810945]","[-25.042236, 11.488736, 9.858627]","[14.830991, 5.6789904, 8.804955]","[14.168367, 10.00759]","[11.6402025, 2.5203393]","[14.9748, 14.854415, -0.017286543]","[13.059074, 6.1234803, 12.211945]"
11,Laura Risk;Lillio Mok;Andrew Hankinson;Julie Cumming,Melodic Similarity in Traditional French-Canadian Instrumental Dance Tunes.,2015,https://doi.org/10.5281/zenodo.1414858,Laura Risk+McGill University>CAN>education;Lillio Mok+McGill University>CAN>education;Andrew Hankinson+McGill University>CAN>education;Julie Cumming+McGill University>CAN>education,"Commercial recordings of French-Canadian instrumental dance tunes represent a varied and complex corpus of study. This was a primarily aural tradition, transmitted from performer to performer with few notated sources until the late 20th century. Practitioners routinely combined tune segments to create new tunes and personalized settings of existing tunes. This has resulted in a corpus that exhibits an extreme amount of variation, even among tunes with the same name. In addition, the same tune or tune segment may appear under several different names. Previous attempts at building systems for automated retrieval and ranking of instrumental dance tunes perform well for near-exact matching of tunes, but do not work as well in retrieving and ranking, in order of most to least similar, variants of a tune; especially those with variations as extreme as this particular corpus. In this paper we will describe a new approach capable of ranked retrieval of variant tunes, and demonstrate its effectiveness on a transcribed corpus of incipits.",CAN,education,Developed economies,"[1.9764704, 13.872955]","[-4.783497, 12.231116]","[-10.648481, -20.132694, -7.864967]","[-5.090076, 6.3628874, 6.1666055]","[12.137059, 9.782614]","[8.638604, 1.6021912]","[12.378888, 15.182923, -1.1953126]","[9.808731, 6.2812047, 12.227077]"
10,Kazuyoshi Yoshii;Katsutoshi Itoyama;Masataka Goto,Infinite Superimposed Discrete All-Pole Modeling for Multipitch Analysis of Wavelet Spectrograms.,2015,https://doi.org/10.5281/zenodo.1417575,Kazuyoshi Yoshii+Kyoto University>JPN>education;Katsutoshi Itoyama+Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a statistical multipitch analyzer based on a source-filter model that decomposes a target music audio signal in terms of three major kinds of sound quantities: pitch (fundamental frequency: F0), timbre (spectral envelope), and intensity (amplitude). If the spectral envelope of an isolated sound is represented by an all-pole filter, linear predictive coding (LPC) can be used for filter estimation in the linear-frequency domain. The main problem of LPC is that although only the amplitudes of harmonic partials are reliable samples drawn from the spectral envelope, the whole spectrum is used for filter estimation. To solve this problem, we propose an infinite superimposed discrete all-pole (iSDAP) model that, given a music signal, can estimate an appropriate number of superimposed harmonic structures whose harmonic partials are drawn from a limited number of spectral envelopes. Our nonparametric Bayesian source-filter model is formulated in the log-frequency domain that better suits the frequency characteristics of human audition. Experimental results showed that the proposed model outperformed the counterpart model formulated in the linear frequency domain.",JPN,education,Developed economies,"[38.01067, -13.473904]","[-45.120037, -19.80975]","[11.010605, -22.399647, 9.11301]","[-0.39719102, -10.382393, -30.786076]","[8.930652, 8.890809]","[6.3975787, 4.6509094]","[11.322314, 13.405169, 0.13989308]","[9.71716, 8.565, 10.27907]"
9,Yu-Hui Huang;Xuanli Chen;Serafina Beck;David Burn;Luc J. Van Gool,Automatic Handwritten Mensural Notation Interpreter: From Manuscript to MIDI Performance.,2015,https://doi.org/10.5281/zenodo.1418267,Yu-Hui Huang+KU Leuven>BEL>education|ETH Zürich>CHE>education;Xuanli Chen+KU Leuven>BEL>education|ETH Zürich>CHE>education;Seraﬁna Beck+KU Leuven>BEL>education;David Burn+KU Leuven>BEL>education;Luc Van Gool+KU Leuven>BEL>education|ETH Zürich>CHE>education,"This paper presents a novel automatic recognition framework for hand-written mensural music. It takes a scanned manuscript as input and yields as output modern music scores. Compared to the previous mensural Optical Music Recognition (OMR) systems, ours shows not only promising performance in music recognition, but also works as a complete pipeline which integrates both recognition and transcription. There are three main parts in this pipeline: i) region-of-interest detection, ii) music symbol detection and classification, and iii) transcription to modern music. In addition to the output in modern notation, our system can generate a MIDI file as well. It provides an easy platform for the musicologists to analyze old manuscripts. Moreover, it renders these valuable cultural heritage resources available to non-specialists as well, as they can now access such ancient music in a better understandable form.",BEL,education,Developed economies,"[33.49591, 1.3142812]","[-23.348589, 38.583206]","[15.8558035, -5.70306, 22.418346]","[-14.126462, -21.863256, 1.0818756]","[10.441248, 7.167482]","[6.7759957, -0.75935435]","[12.618262, 11.727767, -0.6614289]","[7.829427, 4.0373664, 10.5073805]"
8,Florian Krebs;Sebastian Böck;Gerhard Widmer,An Efficient State-Space Model for Joint Tempo and Meter Tracking.,2015,https://doi.org/10.5281/zenodo.1414966,Florian Krebs+Johannes Kepler University>AUT>education;Sebastian Böck+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"Dynamic Bayesian networks (e.g., Hidden Markov Models) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhythmic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic parameters from a piece of music. While this allows the mutual dependencies between these parameters to be exploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We incorporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets.",AUT,education,Developed economies,"[42.059315, -29.349474]","[-27.484715, -10.835607]","[2.249315, -33.8937, -1.0475506]","[-5.354697, 16.57457, -12.975734]","[11.289603, 4.330218]","[5.3622274, 2.2682564]","[10.688654, 13.1890545, -2.728486]","[7.8298054, 6.93529, 10.743873]"
7,Andreu Vall;Marcin Skowron;Peter Knees;Markus Schedl,Improving Music Recommendations with a Weighted Factorization of the Tagging Activity.,2015,https://doi.org/10.5281/zenodo.1416802,Andreu Vall+Johannes Kepler University>AUT>education;Marcin Skowron+Johannes Kepler University>AUT>education;Peter Knees+Johannes Kepler University>AUT>education;Markus Schedl+Johannes Kepler University>AUT>education,"Collaborative filtering systems for music recommendations are often based on implicit feedback derived from listening activity. Hybrid approaches further incorporate additional sources of information in order to improve the quality of the recommendations. In the context of a music streaming service, we present a hybrid model based on matrix factorization techniques that fuses the implicit feedback derived from the users’ listening activity with the tags that users have given to musical items. In contrast to existing work, we introduce a novel approach to exploit tags by performing a weighted factorization of the tagging activity. We evaluate the model for the task of artist recommendation, using the expected percentile rank as metric, extended with confidence intervals to enable the comparison between models. Thus, our contribution is twofold: (1) we introduce a novel model that uses tags to improve music recommendations and (2) we extend the evaluation methodology to compare the performance of different recommender systems.",AUT,education,Developed economies,"[-42.410378, 0.0029234556]","[39.208843, 13.73386]","[-15.226777, 15.741159, 6.28211]","[18.262564, 6.9624906, 15.201596]","[14.625813, 10.3773365]","[12.531149, 2.0485814]","[15.638275, 14.311633, -0.042003028]","[13.637035, 5.358147, 12.642777]"
6,Cameron Summers;Phillip Popp,Temporal Music Context Identification with User Listening Data.,2015,https://doi.org/10.5281/zenodo.1415938,Cameron Summers+Gracenote>USA>company;Phillip Popp+Gracenote>USA>company,"The times when music is played can indicate context for listeners. From the peaceful song for waking up each morning to the traditional song for celebrating a holiday to an up-beat song for enjoying the summer, the relationship between the music and the temporal context is clearly important. For music search and recommendation systems, an understanding of these relationships provides a richer environment to discover and listen. But with the large number of tracks available in music catalogues today, manually labeling track-temporal context associations is difficult, time consuming, and costly. This paper examines track-day contexts with the purpose of identifying relationships with specific music tracks. Improvements are made to an existing method for classifying Christmas tracks and a generalization to the approach is shown that allows automated discovery of music for any day of the year. Analyzing the top 50 tracks obtained from this method for three well-known holidays, Halloween, Saint Patrick’s Day, and July 4th, precision@50 was 95%, 99%, and 73%, respectively.",USA,company,Developed economies,"[-23.175388, 11.631068]","[31.659416, 1.8269097]","[-13.0464, 16.329924, -12.2476225]","[16.935505, 5.0646043, 1.2697849]","[14.644599, 8.255268]","[10.590395, 2.8645344]","[14.746723, 14.353027, -1.470815]","[12.194326, 6.4294233, 11.36368]"
108,Alastair Porter;Dmitry Bogdanov;Robert Kaye;Roman Tsukanov;Xavier Serra,AcousticBrainz: A Community Platform for Gathering Music Information Obtained from Audio.,2015,https://doi.org/10.5281/zenodo.1414938,"Alastair Porter+Music Technology Group, Universitat Pompeu Fabra>ESP>education|MetaBrainz Foundation>USA>company;Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education|MetaBrainz Foundation>USA>company;Robert Kaye+Music Technology Group, Universitat Pompeu Fabra>ESP>education|MetaBrainz Foundation>USA>company;Roman Tsukanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education|MetaBrainz Foundation>USA>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education|MetaBrainz Foundation>USA>company","We introduce the AcousticBrainz project, an open platform for gathering music information. At its core, AcousticBrainz is a database of music descriptors computed from audio recordings using a number of state-of-the-art Music Information Retrieval algorithms. Users run a supplied feature extractor on audio files and upload the analysis results to the AcousticBrainz server. All submissions include a MusicBrainz identifier allowing them to be linked to various sources of editorial information. The feature extractor is based on the open source Essentia audio analysis library. From the data submitted by the community, we run classifiers aimed at adding musically relevant semantic information. These classifiers can be developed by the community using tools available on the AcousticBrainz website. All data in AcousticBrainz is freely available and can be accessed through the website or API. For AcousticBrainz to be successful we need to have an active community that contributes to and uses this platform, and it is this community that will define the actual uses and applications of its data.",ESP,education,Developed economies,"[-21.147955, 10.104426]","[8.965379, 26.855476]","[-17.811209, -4.4599667, -14.91051]","[-3.5983756, -4.076295, 16.071213]","[13.784682, 7.898363]","[10.442142, 1.3993378]","[14.457127, 14.047465, -1.5714593]","[11.381702, 5.6480336, 11.479877]"
4,Christof Weiß;Maximilian Schaab,On the Impact of Key Detection Performance for Identifying Classical Music Styles.,2015,https://doi.org/10.5281/zenodo.1416246,Christof Weiß+Fraunhofer Institute for Digital Media Technology>DEU>facility|Fraunhofer Institute for Digital Media Technology>DEU>facility;Maximilian Schaab+Fraunhofer Institute for Digital Media Technology>DEU>facility|Fraunhofer Institute for Digital Media Technology>DEU>facility,"We study the automatic identification of Western classical music styles by directly using chroma histograms as classification features. Thereby, we evaluate the benefits of knowing a piece’s global key for estimating key-related pitch classes. First, we present four automatic key detection systems. We compare their performance on suitable datasets of classical music and optimize the algorithms’ free parameters. Using a second dataset, we evaluate automatic classification into the four style periods Baroque, Classical, Romantic, and Modern. To that end, we calculate global chroma statistics of each audio track. We then split up the tracks according to major and minor keys and circularly shift the chroma histograms with respect to the tonic note. Based on these features, we train two individual classifier models for major and minor keys. We test the efficiency of four chroma extraction algorithms for classification. Furthermore, we evaluate the impact of key detection performance on the classification results. Additionally, we compare the key-related chroma features to other chroma-based features. We obtain improved performance when using an efficient key detection method for shifting the chroma histograms.",DEU,facility,Developed economies,"[29.460056, 12.752896]","[-6.69013, -2.7417474]","[7.370381, -15.437103, 13.905733]","[6.9477572, -4.512489, -4.231606]","[10.737867, 7.4671144]","[7.525209, 2.7130759]","[12.21374, 13.0694685, 0.13606752]","[10.282246, 8.112722, 11.837718]"
3,Christopher J. Tralie;Paul Bendich,Cover Song Identification with Timbral Shape Sequences.,2015,https://doi.org/10.5281/zenodo.1416824,Christopher J. Tralie+Duke University>USA>education|Duke University>USA>education;Paul Bendich+Duke University>USA>education|Duke University>USA>education,"We introduce a novel low level feature for identifying cover songs which quantifies the relative changes in the smoothed frequency spectrum of a song. Our key insight is that a sliding window representation of a chunk of audio can be viewed as a time-ordered point cloud in high dimensions. For corresponding chunks of audio between different versions of the same song, these point clouds are approximately rotated, translated, and scaled copies of each other. If we treat MFCC embeddings as point clouds and cast the problem as a relative shape sequence, we are able to correctly identify 42/80 cover songs in the “Covers 80” dataset. By contrast, all other work to date on cover songs exclusively relies on matching note sequences from Chroma derived features.",USA,education,Developed economies,"[7.4491816, 42.59757]","[26.102139, -12.079683]","[3.8409, 11.752207, -19.950844]","[14.997028, -3.3123088, 0.079693235]","[16.084614, 11.113922]","[10.140402, 2.6094196]","[12.867721, 17.33406, -0.3505708]","[11.7118435, 6.8039017, 11.823539]"
2,Matthew Prockup;Andreas F. Ehmann;Fabien Gouyon;Erik M. Schmidt;Òscar Celma;Youngmoo E. Kim,Modeling Genre with the Music Genome Project: Comparing Human-Labeled Attributes and Audio Features.,2015,https://doi.org/10.5281/zenodo.1417523,Matthew Prockup+Drexel University>USA>education|Pandora Media Inc.>USA>company;Andreas F. Ehmann+Drexel University>USA>education|Pandora Media Inc.>USA>company;Fabien Gouyon+Drexel University>USA>education|Pandora Media Inc.>USA>company;Erik M. Schmidt+Drexel University>USA>education|Pandora Media Inc.>USA>company;Oscar Celma+Drexel University>USA>education|Pandora Media Inc.>USA>company;Youngmoo E. Kim+Drexel University>USA>education|Pandora Media Inc.>USA>company,"Genre provides one of the most convenient categorizations of music, but it is often regarded as a poorly defined or largely subjective musical construct. In this work, we provide evidence that musical genres can to a large extent be objectively modeled via a combination of musical attributes. We employ a data-driven approach utilizing a subset of 48 hand-labeled musical attributes comprising instrumentation, timbre, and rhythm across more than one million examples from Pandora Internet Radio’s Music Genome Project. A set of audio features motivated by timbre and rhythm are then implemented to model genre both directly and through audio-driven models derived from the hand-labeled musical attributes. In most cases, machine learning models built directly from hand-labeled attributes outperform models based on audio features. Among the audio-based models, those that combine audio features and learned musical attributes perform better than those derived from audio features alone.",USA,education,Developed economies,"[-31.810114, 5.998092]","[17.969261, -6.827841]","[-18.572231, 3.8555431, 6.8445053]","[11.89452, 8.944526, -3.0920331]","[13.599974, 9.688803]","[9.451204, 4.5052233]","[14.466501, 13.950529, -0.30114916]","[10.5135565, 6.253947, 9.841442]"
1,Daniel Wolff;Andrew MacFarlane 0001;Tillman Weyde,Comparative Music Similarity Modelling Using Transfer Learning Across User Groups.,2015,https://doi.org/10.5281/zenodo.1417835,Daniel Wolff+City University London>GBR>education;Andrew MacFarlane+City University London>GBR>education;Tillman Weyde+City University London>GBR>education,"We introduce a new application of transfer learning for training and comparing music similarity models based on relative user data: The proposed Relative Information-Theoretic Metric Learning (RITML) algorithm adapts a Mahalanobis distance using an iterative application of the ITML algorithm, thereby extending it to relative similarity data. RITML supports transfer learning by training models with respect to a given template model that can provide prior information for regularisation. With this feature we use information from larger datasets to build better models for more specific datasets, such as user groups from different cultures or of different age. We then evaluate what model parameters, in this case acoustic features, are relevant for the specific models when compared to the general user data. We to this end introduce the new CASimIR dataset, the first openly available relative similarity dataset with user attributes. With two age-related subsets, we show that transfer learning with RITML leads to better age-specific models. RITML here improves learning on small datasets. Using the larger MagnaTagATune dataset, we show that RITML performs as well as state-of-the-art algorithms in terms of general similarity estimation.",GBR,education,Developed economies,"[-6.3216767, 12.583714]","[28.461275, 7.560899]","[-6.237291, 12.131186, 2.5916991]","[11.886626, 1.7804182, 11.513309]","[13.162322, 9.418829]","[10.97346, 2.4771266]","[13.682299, 15.2178, -0.63921905]","[12.544275, 6.389237, 12.283862]"
0,Dan Ringwalt;Roger B. Dannenberg,Image Quality Estimation for Multi-Score OMR.,2015,https://doi.org/10.5281/zenodo.1414890,Dan Ringwalt+Carnegie Mellon University>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education,"Optical music recognition (OMR) is the recognition of images of musical scores. Recent research has suggested aligning the results of OMR from multiple scores of the same work (multi-score OMR, MS-OMR) to improve accuracy. As a simpler alternative, we have developed features which predict the quality of a given score, allowing us to select the highest-quality score to use for OMR. Furthermore, quality may be used to weight each score in an alignment, which should improve existing systems’ robustness. Using commercial OMR software on a test set of MIDI recordings and multiple corresponding scores, our predicted OMR accuracy is weakly but significantly correlated with the true accuracy. Improved features should be able to produce highly consistent results.",USA,education,Developed economies,"[34.298172, 31.44861]","[-20.026823, 37.19198]","[6.14599, -23.54787, 21.77792]","[-10.398738, -20.544634, 3.0047765]","[9.814106, 6.2426467]","[6.5779, -0.4167342]","[12.633756, 11.353913, -1.5390446]","[8.023117, 4.2929296, 10.704534]"
25,Felipe Vieira;Nazareno Andrade,Evaluating Conflict Management Mechanisms for Online Social Jukeboxes.,2015,https://doi.org/10.5281/zenodo.1416232,Felipe Vieira+Universidade Federal de Campina Grande>BRA>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education,"Social music listening is a prevalent and often fruitful experience. Social jukeboxes are systems that enable social music listening with listeners collaboratively choosing the music to be played. Naturally, because music tastes are diverse, using social jukeboxes often involves conflicting interests. Because of that, virtually all social jukeboxes incorporate conflict management mechanisms. In contrast with their widespread use, however, little attention has been given to evaluating how different conflict management mechanisms function to preserve the positive experience of music listeners. This paper presents an experiment with three conflict management mechanisms and three groups of listeners. The mechanisms were chosen to represent those most commonly used in the state of the practice. Our study employs a mixed-methods approach to quantitatively analyze listeners’ satisfaction and to examine their impressions and views on conflict, conflict management mechanisms, and social jukeboxing.",BRA,education,Developing economies,"[-35.39848, 30.298481]","[42.864307, 33.300358]","[-27.541334, 14.776962, -12.674712]","[7.236206, 18.275238, 18.835733]","[14.866256, 9.031308]","[13.0372505, 1.0050076]","[15.369151, 14.560722, -1.127018]","[13.434992, 4.221395, 11.903507]"
55,Brian McFee;Oriol Nieto;Juan Pablo Bello,Hierarchical Evaluation of Segment Boundary Detection.,2015,https://doi.org/10.5281/zenodo.1414866,Brian McFee+New York University>USA>education|Music and Audio Research Laboratory>USA>facility;Oriol Nieto+New York University>USA>education|Music and Audio Research Laboratory>USA>facility;Juan P. Bello+New York University>USA>education|Music and Audio Research Laboratory>USA>facility,"Structure in music is traditionally analyzed hierarchically: large-scale sections can be sub-divided and refined down to the short melodic ideas at the motivic level. However, typical algorithmic approaches to structural annotation produce flat temporal partitions of a track, which are commonly evaluated against a similarly flat, human-produced annotation. Evaluating structure analysis as represented by flat annotations effectively discards all notions of structural depth in the evaluation. Although collections of hierarchical structure annotations have been recently published, no techniques yet exist to measure an algorithm’s accuracy against these rich structural annotations. In this work, we propose a method to evaluate structural boundary detection with hierarchical annotations. The proposed method transforms boundary detection into a ranking problem, and facilitates the comparison of both flat and hierarchical annotations. We demonstrate the behavior of the proposed method with various synthetic and real examples drawn from the SALAMI dataset.",USA,education,Developed economies,"[-5.5410185, -11.483211]","[-1.2732987, 7.2426133]","[6.7698054, -8.172968, -7.6826644]","[-2.6679845, 2.5466712, 1.7457216]","[11.439252, 8.655684]","[8.558971, 2.6217356]","[12.438296, 14.049144, 0.45228913]","[10.476411, 6.8816686, 11.399643]"
26,Ajay Srinivasamurthy;Andre Holzapfel;Ali Taylan Cemgil;Xavier Serra,Particle Filters for Efficient Meter Tracking with Dynamic Bayesian Networks.,2015,https://doi.org/10.5281/zenodo.1416736,"Ajay Srinivasamurthy+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Dept. of Computer Engineering, Boğaziçi University>TUR>education;Andre Holzapfel+Dept. of Computer Engineering, Boğaziçi University>TUR>education;Ali Taylan Cemgil+Dept. of Computer Engineering, Boğaziçi University>TUR>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Recent approaches in meter tracking have successfully applied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the application of exact inference is computationally demanding. More efficient approximate inference algorithms using particle filters (PF) can be developed to overcome this limitation. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an existing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Filter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter tracking accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We document that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music.",ESP,education,Developed economies,"[41.758373, -34.19599]","[-26.640678, -11.007646]","[5.1106853, -36.50713, -0.62645656]","[-3.867794, 16.352009, -13.279786]","[11.057416, 4.2826986]","[5.406423, 2.2416706]","[10.511261, 13.113569, -2.5672767]","[7.850696, 6.911974, 10.765868]"
28,Kevin R. Page;Terhi Nurmikko-Fuller;Carolin Rindfleisch;David M. Weigl;Richard Lewis 0001;Laurence Dreyfus;David De Roure,A Toolkit for Live Annotation of Opera Performance: Experiences Capturing Wagner's Ring Cycle.,2015,https://doi.org/10.5281/zenodo.1415582,"Kevin R. Page+Oxford e-Research Centre, University of Oxford>GBR>education;Terhi Nurmikko-Fuller+University of Oxford>GBR>education;Carolin Rindfleisch+University of Oxford>GBR>education;David M. Weigl+Oxford e-Research Centre, University of Oxford>GBR>education;Richard Lewis+Goldsmiths, University of London>GBR>education;Laurence Dreyfus+University of Oxford>GBR>education;David De Roure+Oxford e-Research Centre, University of Oxford>GBR>education","""Performance of a musical work potentially provides a rich source of multimedia material for future investigation, both for musicologists’ study of reception and perception, and in improvement of computational methods applied to its analysis. This is particularly true of music theatre, where a traditional recording cannot sufficiently capture the ephemeral phenomena unique to each staging. In this paper we introduce a toolkit developed with, and used by, a musicologist throughout a complete multi-day production of Richard Wagner’s Der Ring des Nibelungen. The toolkit is centred on a tablet-based score interface through which the scholar makes notes on the scenic setting of the performance as it unfolds, supplemented by a variety of digital data gathered to structure and index the annotations. We report on our experience developing a system suitable for real-time use by the musicologist, structuring the data for reuse and further investigation using semantic web technologies, and of the practical challenges and compromises of fieldwork within a working theatre. Finally we consider the utility of our tooling from both a user perspective and through an initial quantitative investigation of the data gathered.""",GBR,education,Developed economies,"[5.254012, 24.72784]","[14.532121, 40.183193]","[-12.667997, -12.7379265, -4.5620155]","[0.8647023, -4.6696563, 26.41942]","[12.6404295, 7.1273894]","[10.822679, 0.16742054]","[13.265119, 13.426428, -1.7950854]","[11.824842, 4.965408, 11.900961]"
53,Eita Nakamura;Philippe Cuvillier;Arshia Cont;Nobutaka Ono;Shigeki Sagayama,Autoregressive Hidden Semi-Markov Model of Symbolic Music Performance for Score Following.,2015,https://doi.org/10.5281/zenodo.1417030,Eita Nakamura+National Institute of Informatics>JPN>facility;Philippe Cuvillier+Inria MuTant Project-Team>FRA>facility;Arshia Cont+Inria MuTant Project-Team>FRA>facility;Nobutaka Ono+National Institute of Informatics>JPN>facility;Shigeki Sagayama+Meiji University>JPN>education,"A stochastic model of symbolic (MIDI) performance of polyphonic scores is presented and applied to score following. Stochastic modelling has been one of the most successful strategies in this field. We describe the performance as a hierarchical process of performer’s progression in the score and the production of performed notes, and represent the process as an extension of the hidden semi-Markov model. The model is compared with a previously studied model based on hidden Markov model (HMM), and reasons are given that the present model is advantageous for score following especially for scores with trills, tremolos, and arpeggios. This is also confirmed empirically by comparing the accuracy of score following and analysing the errors. We also provide a hybrid of this model and the HMM-based model which is computationally more efficient and retains the advantages of the former model. The present model yields one of the state-of-the-art score following algorithms for symbolic performance and can possibly be applicable for other music recognition problems.",JPN,facility,Developed economies,"[0.57178974, 0.901322]","[-15.345358, -7.838631]","[0.7379495, -7.0380416, 4.379071]","[2.6578662, -13.72292, -2.63931]","[11.341814, 8.000313]","[6.960428, 2.6885595]","[12.829512, 13.233175, -0.5470754]","[8.857812, 7.0587, 10.854551]"
52,Swapnil Gupta;Ajay Srinivasamurthy;Manoj Kumar P. A.;Hema A. Murthy;Xavier Serra,Discovery of Syllabic Percussion Patterns in Tabla Solo Recordings.,2015,https://doi.org/10.5281/zenodo.1416558,"Swapnil Gupta+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Ajay Srinivasamurthy+DONlab, Indian Institute of Technology Madras>IND>education;Manoj Kumar+DONlab, Indian Institute of Technology Madras>IND>education;Hema A. Murthy+DONlab, Indian Institute of Technology Madras>IND>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","We address the unexplored problem of percussion pattern discovery in Indian art music. Percussion in Indian art music uses onomatopoeic oral mnemonic syllables for the transmission of repertoire and technique. This is utilized for the task of percussion pattern discovery from audio recordings. From a parallel corpus of audio and expert curated scores for 38 tabla solo recordings, we use the scores to build a set of most frequent syllabic patterns of different lengths. From this set, we manually select a subset of musically representative query patterns. To discover these query patterns in an audio recording, we use syllable-level hidden Markov models (HMM) to automatically transcribe the recording into a syllable sequence, in which we search for the query pattern instances using a Rough Longest Common Subsequence (RLCS) approach. We show that the use of RLCS makes the approach robust to errors in automatic transcription, significantly improving the pattern recall rate and F-measure. We further propose possible enhancements to improve the results.",ESP,education,Developed economies,"[31.51451, -49.368824]","[-14.661012, -1.1646011]","[27.816814, -18.836107, 0.10374337]","[5.6396794, 4.2396855, -15.495014]","[7.870036, 7.5249724]","[8.010746, 3.0728457]","[10.694225, 11.624037, 1.0333298]","[9.267093, 7.351086, 10.765715]"
51,Sergio Oramas;Francisco Gómez 0001;Emilia Gómez;Joaquín Mora,FlaBase: Towards the Creation of a Flamenco Music Knowledge Base.,2015,https://doi.org/10.5281/zenodo.1417183,"Sergio Oramas+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Francisco Gómez+Technical University of Madrid>ESP>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Joaquín Mora+University of Sevilla>ESP>education","Online information about flamenco music is scattered over different sites and knowledge bases. Unfortunately, there is no common repository that indexes all these data. In this work, information related to flamenco music is gathered from general knowledge bases (e.g., Wikipedia, DBpedia), music encyclopedias (e.g., MusicBrainz), and specialized flamenco websites, and is then integrated into a new knowledge base called FlaBase. As resources from different data sources do not share common identifiers, a process of pair-wise entity resolution has been performed. FlaBase contains information about 1,174 artists, 76 palos (flamenco genres), 2,913 albums, 14,078 tracks, and 771 Andalusian locations. It is freely available in RDF and JSON formats. In addition, a method for entity recognition and disambiguation for FlaBase has been created. The system can recognize and disambiguate FlaBase entity references in Spanish texts with an f-measure value of 0.77. We applied it to biographical texts present in Flabase. By using the extracted information, the knowledge base is populated with relevant information and a semantic graph is created connecting the entities of FlaBase. Artists relevance is then computed over the graph and evaluated according to a flamenco expert criteria. Accuracy of results shows a high degree of quality and completeness of the knowledge base.",ESP,education,Developed economies,"[-27.069542, 34.319317]","[20.352018, 36.873123]","[-18.470573, -0.6228511, -4.842685]","[-3.2731411, 1.4904414, 24.248928]","[14.130282, 8.667199]","[11.048952, 0.06323113]","[14.790217, 14.084184, -1.6512094]","[12.159678, 4.9826922, 11.942506]"
50,Taku Kuribayashi;Yasuhito Asano;Masatoshi Yoshikawa,Towards Support for Understanding Classical Music: Alignment of Content Descriptions on the Web.,2015,https://doi.org/10.5281/zenodo.1416138,Taku Kuribayashi+Kyoto University>JPN>education;Yasuhito Asano+Kyoto University>JPN>education;Masatoshi Yoshikawa+Kyoto University>JPN>education,"Supporting the understanding of classical music is an important topic that involves various research fields such as text analysis and acoustics analysis. Content descriptions are explanations of classical music compositions that help a person to understand technical aspects of the music. Recently, Kuribayashi et al. proposed a method for obtaining content descriptions from the web. However, the content descriptions on a single page frequently explain a specific part of a composition only. Therefore, a person who wants to fully understand the composition suffers from a time-consuming task, which seems almost impossible for a novice of classical music. To integrate the content descriptions obtained from multiple pages, we propose a method for aligning each pair of paragraphs of such descriptions. Using dynamic time warping-based method along with our new ideas, (a) a distribution-based distance measure named w2DD, and (b) the concept of passage expressions, it is possible to align content descriptions of classical music better than when using cutting-edge text analysis methods. Our method can be extended in future studies to create applications systems to integrate descriptions with musical scores and performances.",JPN,education,Developed economies,"[-22.938944, 27.773134]","[22.842812, 8.60681]","[-17.457256, 2.5240211, -16.467974]","[-0.08137314, -1.192303, 9.724342]","[13.870268, 8.453426]","[10.145421, 1.8663571]","[14.119462, 14.341797, -1.5517904]","[11.0952835, 6.2644997, 12.011048]"
49,Peter Knees;Ángel Faraldo;Perfecto Herrera;Richard Vogl;Sebastian Böck;Florian Hörschläger;Mickael Le Goff,Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections.,2015,https://doi.org/10.5281/zenodo.1414996,Peter Knees+Johannes Kepler University>AUT>education;Ángel Faraldo+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education;Richard Vogl+Johannes Kepler University>AUT>education;Sebastian Böck+Johannes Kepler University>AUT>education;Florian Hörschläger+Johannes Kepler University>AUT>education;Mickael Le Goff+Native Instruments GmbH>DEU>company,"We present two new data sets for automatic evaluation of tempo estimation and key detection algorithms. In contrast to existing collections, both released data sets focus on electronic dance music (EDM). The data sets have been automatically created from user feedback and annotations extracted from web sources. More precisely, we utilize user corrections submitted to an online forum to report wrong tempo and key annotations on the Beatport website. Beatport is a digital record store targeted at DJs and focusing on EDM genres. For all annotated tracks in the data sets, samples of at least one-minute length can be freely downloaded. For key detection, further ground truth is extracted from expert annotations manually assigned to Beatport tracks for benchmarking purposes. The set for tempo estimation comprises 664 tracks and the set for key detection 604 tracks. We detail the creation process of both data sets and perform extensive benchmarks using state-of-the-art algorithms from both academic research and commercial products.",AUT,education,Developed economies,"[37.982876, -24.621899]","[-32.80412, -3.561321]","[-2.9872022, -25.100569, -3.008447]","[-12.050469, 12.06053, -8.615449]","[11.474036, 4.79065]","[5.164126, 1.7082814]","[11.102549, 13.480864, -2.7010765]","[7.500079, 6.681723, 11.037834]"
48,Andreas Arzt;Gerhard Widmer,Real-Time Music Tracking Using Multiple Performances as a Reference.,2015,https://doi.org/10.5281/zenodo.1417205,Andreas Arzt+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"In general, algorithms for real-time music tracking directly use a symbolic representation of the score, or a synthesised version thereof, as a reference for the on-line alignment process. In this paper we present an alternative approach. First, different performances of the piece in question are collected and aligned (off-line) to the symbolic score. Then, multiple instances of the on-line tracking algorithm (each using a different performance as a reference) are used to follow the live performance, and their output is combined to come up with the current position in the score. As the evaluation shows, this strategy improves both the robustness and the precision, especially on pieces that are generally hard to track (e.g. pieces with extreme, abrupt tempo changes, or orchestral pieces with a high degree of polyphony). Finally, we describe a real-world application, where this music tracking algorithm was used to follow a world-famous orchestra in a concert hall in order to show synchronised visual content (the sheet music, explanatory text and videos) to members of the audience.",AUT,education,Developed economies,"[-13.72044, 8.528131]","[-16.625505, -14.095507]","[-6.0837975, -5.2208333, -9.179313]","[1.8961905, -21.480343, -1.0395151]","[12.768383, 7.150504]","[5.938348, 1.0471282]","[13.234998, 13.459755, -1.6056654]","[8.044635, 6.0270963, 11.019768]"
47,Jonathan Driedger;Thomas Prätzlich;Meinard Müller,Let it Bee - Towards NMF-Inspired Audio Mosaicing.,2015,https://doi.org/10.5281/zenodo.1415698,Jonathan Driedger+International Audio Laboratories Erlangen>DEU>facility;Thomas Prätzlich+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"A swarm of bees buzzing “Let it be” by the Beatles or the wind gently howling the romantic “Gute Nacht” by Schubert – these are examples of audio mosaics as we want to create them. Given a target and a source recording, the goal of audio mosaicing is to generate a mosaic recording that conveys musical aspects (like melody and rhythm) of the target, using sound components taken from the source. In this work, we propose a novel approach for automatically generating audio mosaics with the objective to preserve the source’s timbre in the mosaic. Inspired by algorithms for non-negative matrix factorization (NMF), our idea is to use update rules to learn an activation matrix that, when multiplied with the spectrogram of the source recording, resembles the spectrogram of the target recording. However, when applying the original NMF procedure, the resulting mosaic does not adequately reflect the source’s timbre. As our main technical contribution, we propose an extended set of update rules for the iterative learning procedure that supports the development of sparse diagonal structures in the activation matrix. We show how these structures better retain the source’s timbral characteristics in the resulting mosaic.",DEU,facility,Developed economies,"[8.434844, -35.329346]","[-52.09568, -27.481802]","[1.2207817, -3.7982032, -25.388834]","[-10.323078, -17.076519, -25.056538]","[9.336909, 8.931339]","[6.2714505, 4.8060646]","[11.966955, 13.366107, 0.9644638]","[9.500894, 8.568633, 9.821754]"
46,Rong Jin;Christopher Raphael,Graph-Based Rhythm Interpretation.,2015,https://doi.org/10.5281/zenodo.1418287,Rong Jin+Indiana University>USA>education;Christopher Raphael+Indiana University>USA>education,"We present a system that interprets the notated rhythm obtained from optical music recognition (OMR). Our approach represents the notes and rests in a system measure as the vertices of a graph. We connect the graph by adding voice edges and coincidence edges between pairs of vertices, while the rhythmic interpretation follows simply from the connected graph. The graph identification problem is cast as an optimization where each potential edge is scored according to its plausibility. We seek the optimally scoring graph where the score is represented as a sum of edge scores. Experiments were performed on about 60 score pages showing that our system can handle difficult rhythmic situations including multiple voices, voices that merge and split, voices spanning two staves, and missing tuplets.",USA,education,Developed economies,"[43.936214, 9.684024]","[-18.770372, 32.834827]","[-5.649576, -20.705442, 3.562602]","[-10.715632, -14.614204, 4.5120006]","[11.927751, 5.541167]","[6.8390927, -0.2005156]","[11.403151, 14.122783, -2.0569444]","[8.303107, 4.5040536, 10.744541]"
45,Anna M. Kruspe,"Training Phoneme Models for Singing with ""Songified"" Speech Data.",2015,https://doi.org/10.5281/zenodo.1416858,Anna M. Kruspe+Fraunhofer IDMT>DEU>facility,"Speech recognition in singing is a task that has not been widely researched so far. Singing possesses several characteristics that differentiate it from speech. Therefore, algorithms and models that were developed for speech usually perform worse on singing. One of the bottlenecks in many algorithms is the recognition of phonemes in singing. We noticed that this recognition step can be improved when using singing data in model training, but to our knowledge, there are no large datasets of singing data annotated with phonemes. However, such data does exist for speech. We therefore propose to make phoneme recognition models more robust for singing by training them on speech data that has artificially been made more “song-like”. We test two main modifications on speech data: Time stretching and pitch shifting. Artificial vibrato is also tested. We then evaluate models trained on different combinations of these modified speech recordings. The utilized modeling algorithms are Neural Networks and Deep Belief Networks.",DEU,facility,Developed economies,"[-4.6074595, -37.354675]","[-30.223091, -41.30049]","[21.234037, 12.687594, -9.984749]","[3.6371968, -7.7512064, -20.04892]","[9.783782, 10.996831]","[7.770425, 4.7009044]","[11.210059, 15.019201, 0.9153899]","[10.316011, 7.3896112, 9.010056]"
44,Anis Khlif;Vidhyasaharan Sethu,An Iterative Multi Range Non-Negative Matrix Factorization Algorithm for Polyphonic Music Transcription.,2015,https://doi.org/10.5281/zenodo.1416686,Anis Khlif+École des Mines ParisTech>FRA>education;Vidhyasaharan Sethu+University of New South Wales> AUS>education,"This article presents a novel iterative algorithm based on Non-negative Matrix Factorisation (NMF) that is particularly well suited to the task of automatic music transcription (AMT). Compared with previous NMF based techniques, this one does not aim at factorizing the time-frequency representation of the entire musical signal into a combination of the possible set of notes. Instead, the proposed algorithm proceeds iteratively by initially decomposing a part of the time-frequency representation into a combination of a small subset of all possible notes then reinvesting this information in the following step involving a large subset of notes. Specifically, starting with the lowest octave of notes that is of interest, each iteration increases the set of notes under consideration by an octave. The resolution of a lower dimensionality problem used to properly initialize matrices for a more complex problem, results in a gain of some percent in the transcription accuracy.",FRA,education,Developed economies,"[29.680716, -11.720837]","[-50.229782, -23.755198]","[9.829353, -7.6826296, 6.337324]","[-6.284403, -16.35668, -25.484549]","[9.264441, 7.8418016]","[6.089603, 4.7704625]","[11.597323, 12.580456, 0.3523709]","[9.332519, 8.716789, 9.886298]"
43,Dimos Makris;Maximos A. Kaliakatsos-Papakostas;Emilios Cambouropoulos,Probabilistic Modular Bass Voice Leading in Melodic Harmonisation.,2015,https://doi.org/10.5281/zenodo.1416374,Dimos Makris+Ionian University>GRC>education;Maximos Kaliakatsos-Papakostas+Aristotle University of Thessaloniki>GRC>education;Emilios Cambouropoulos+Aristotle University of Thessaloniki>GRC>education,"Probabilistic methodologies provide successful tools for automated music composition, such as melodic harmonisation, since they capture statistical rules of the music idioms they are trained with. Proposed methodologies focus either on specific aspects of harmony (e.g., generating abstract chord symbols) or incorporate the determination of many harmonic characteristics in a single probabilistic generative scheme. This paper addresses the problem of assigning voice leading focussing on the bass voice, i.e., the realisation of the actual bass pitches of an abstract chord sequence, under the scope of a modular melodic harmonisation system where different aspects of the generative process are arranged by different modules. The proposed technique defines the motion of the bass voice according to several statistical aspects: melody voice contour, previous bass line motion, bass-to-melody distances and statistics regarding inversions and note doublings in chords. The aforementioned aspects of voicing are modular, i.e., each criterion is defined by independent statistical learning tools. Experimental results on diverse music idioms indicate that the proposed methodology captures efficiently the voice layout characteristics of each idiom, whilst additional analyses on separate statistically trained modules reveal distinctive aspects of each idiom. The proposed system is designed to be flexible and adaptable (for instance, for the generation of novel blended melodic harmonisations).",GRC,education,Developed economies,"[12.392308, -6.8182454]","[-23.623274, 23.835213]","[14.579736, 13.747491, 0.027379308]","[-21.632399, 3.3892193, 2.6738064]","[10.396769, 9.601997]","[7.0613184, 3.3129976]","[11.606575, 14.961647, -0.5739631]","[9.615962, 8.016952, 12.205072]"
42,Yin-Jyun Luo;Li Su;Yi-Hsuan Yang;Tai-Shih Chi,Detection of Common Mistakes in Novice Violin Playing.,2015,https://doi.org/10.5281/zenodo.1415968,Yin-Jyun Luo+National Chiao Tung University>TWN>education|Academia Sinica>Unknown>facility;Li Su+National Chiao Tung University>TWN>education|Academia Sinica>Unknown>facility;Yi-Hsuan Yang+National Chiao Tung University>TWN>education|Academia Sinica>Unknown>facility;Tai-Shih Chi+National Chiao Tung University>TWN>education|Academia Sinica>Unknown>facility,"Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musical instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset comprising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are generated from the same feature set with different scales, including two note-level representations and three segment-level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vector machine for classification. Results show that the F-measures using different feature representations can vary up to 20% for two types of playing mistakes. It demonstrates the different sensitivities of each feature representation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed.",TWN,education,Developing economies,"[16.749918, -21.834877]","[-41.637, 1.4712195]","[9.675313, -15.806383, -0.59566355]","[-11.156522, 21.261436, -4.5296125]","[9.519222, 6.7718024]","[8.13141, 3.904343]","[11.346624, 12.261605, -0.4956756]","[9.663318, 6.6961203, 10.344345]"
41,Bernhard Lehner;Gerhard Widmer,Monaural Blind Source Separation in the Context of Vocal Detection.,2015,https://doi.org/10.5281/zenodo.1415940,Bernhard Lehner+Johannes Kepler University of Linz>AUT>education;Gerhard Widmer+Johannes Kepler University of Linz>AUT>education,"In this paper, we evaluate the usefulness of several monaural blind source separation (BSS) algorithms in the context of vocal detection (VD). BSS is the problem of recovering several sources, given only a mixture. VD is the problem of automatically identifying the parts in a mixed audio signal, where at least one person is singing. We compare the results of three different strategies for utilising the estimated singing voice signals from four state-of-the-art source separation algorithms. In order to assess the performance of those strategies on an internal data set, we use two different feature sets, each fed to two different classifiers. After selecting the most promising approach, the results on two publicly available data sets are presented. In an additional experiment, we use the improved VD for a simple post-processing technique: For the final estimation of the source signals, we decide to use either silence, or the mixed, or the separated signals, according to the VD. The results of traditionally used BSS evaluation methods suggest that this is useful for both the estimated background signals, as well as for the estimated vocals.",AUT,education,Developed economies,"[-0.06131155, -44.32418]","[-42.953495, -30.032213]","[26.416494, 6.276276, -3.8709743]","[-7.0554414, -6.4772725, -27.631613]","[9.022389, 10.678766]","[6.6053715, 5.432508]","[10.867805, 14.51081, 1.383165]","[9.8807535, 8.52949, 9.451531]"
40,Cynthia C. S. Liem;Alan Hanjalic,Comparative Analysis of Orchestral Performance Recordings: An Image-Based Approach.,2015,https://doi.org/10.5281/zenodo.1416248,Cynthia C. S. Liem+Delft University of Technology>NLD>education|Delft University of Technology>Unknown>Unknown;Alan Hanjalic+Delft University of Technology>NLD>education|Delft University of Technology>Unknown>Unknown,"Traditionally, the computer-assisted comparison of multiple performances of the same piece focused on performances on single instruments. Due to data availability, there also has been a strong bias towards analyzing piano performances, in which local timing, dynamics and articulation are important expressive performance features. In this paper, we consider the problem of analyzing multiple performances of the same symphonic piece, performed by different orchestras and different conductors. While differences between interpretations in this genre may include commonly studied features on timing, dynamics and articulation, the timbre of the orchestra and choices of balance within the ensemble are other important aspects distinguishing different orchestral interpretations from one another. While it is hard to model these higher-level aspects as explicit audio features, they can usually be noted visually in spectrogram plots. We therefore propose a method to compare orchestra performances by examining visual spectrogram characteristics. Inspired by eigenfaces in human face recognition, we apply Principal Components Analysis on synchronized performance fragments to localize areas of cross-performance variation in time and frequency. We discuss how this information can be used to examine performer differences, and how beyond pairwise comparison, relative differences can be studied between multiple performances in a corpus at once.",NLD,education,Developed economies,"[-14.200007, 6.2499075]","[-29.105425, 6.609186]","[-6.884364, -10.875828, -7.943749]","[-9.479879, 7.5397825, -1.2940006]","[12.412871, 7.239887]","[5.9498124, 1.2648697]","[13.174929, 13.21541, -1.5901906]","[8.449921, 6.2460656, 11.237256]"
39,Dawen Liang;Minshu Zhan;Daniel P. W. Ellis,Content-Aware Collaborative Music Recommendation Using Pre-trained Neural Networks.,2015,https://doi.org/10.5281/zenodo.1416308,Dawen Liang+Columbia University>USA>education;Minshu Zhan+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Although content is fundamental to our music listening preferences, the leading performance in music recommendation is achieved by collaborative-filtering-based methods which exploit the similarity patterns in user’s listening history rather than the audio content of songs. Meanwhile, collaborative filtering has the well-known “cold-start” problem, i.e., it is unable to work with new songs that no one has listened to. Efforts on incorporating content information into collaborative filtering methods have shown success in many non-musical applications, such as scientific article recommendation. Inspired by the related work, we train a neural network on semantic tagging information as a content model and use it as a prior in a collaborative filtering model. Such a system still allows the user listening data to “speak for itself”. The proposed system is evaluated on the Million Song Dataset and shows comparably better result than the collaborative filtering approaches, in addition to the favorable performance in the cold-start case.",USA,education,Developed economies,"[-46.356453, 28.389856]","[38.722042, 13.623929]","[-7.789741, 23.856045, -14.541595]","[17.986174, 6.178459, 14.169688]","[15.948002, 9.227965]","[12.569696, 2.1468937]","[15.772189, 15.701733, -1.4572005]","[13.574239, 5.4065948, 12.67628]"
38,Bill Z. Manaris;Seth Stoudenmier,Specter: Combining Music Information Retrieval with Sound Spatialization.,2015,https://doi.org/10.5281/zenodo.1415596,Bill Manaris+College of Charleston>USA>education;Seth Stoudenmier+College of Charleston>USA>education,"Specter combines music information retrieval (MIR) with sound spatialization to provide a simple, yet versatile environment to experiment with sound spatialization for music composition and live performance. Through various interfaces and sensors, users may position sounds at arbitrary locations and trajectories in a three-dimensional plane. The system utilizes the JythonMusic environment for symbolic music processing, music information retrieval, and live audio manipulation. It also incorporates Iannix, a 3D graphical, open-source sequencer, for real-time generation, manipulation, and storing of sound trajectory scores. Finally, through Glaser, a sound manipulation instrument, Specter renders the various sounds in space. The system architecture supports different sound spatialization techniques including Ambisonics and Vector Based Amplitude Panning. Various interfaces are discussed, including a Kinect-based sensor system, a Leap-Motion-based hand-tracking interface, and a smartphone-based OSC controller. Finally, we present Migrant, a music composition, which utilizes and demonstrates Specter’s ability to combine MIR techniques with sound spatialization through inexpensive, minimal hardware.",USA,education,Developed economies,"[-17.152882, 18.819864]","[7.0214415, 30.862776]","[-8.134933, 2.308996, -15.015604]","[-5.296247, -8.470564, 14.03306]","[13.620493, 7.8964906]","[10.585933, 1.2072953]","[13.697896, 14.423901, -1.9754691]","[11.044764, 5.4004917, 11.8428]"
37,Che-Yuan Liang;Li Su;Yi-Hsuan Yang;Hsin-Ming Lin,Musical Offset Detection of Pitched Instruments: The Case of Violin.,2015,https://doi.org/10.5281/zenodo.1416834,"Che-Yuan Liang+Academia Sinica>TWN>education|University of California, San Diego>USA>education;Li Su+Academia Sinica>TWN>education;Yi-Hsuan Yang+Academia Sinica>TWN>education;Hsin-Ming Lin+University of California, San Diego>USA>education","Musical offset detection is an integral part of a music signal processing system that requires complete characterization of note events. However, unlike onset detection, offset detection has seldom been the subject of an in-depth study in the music information retrieval community, possibly because of the ambiguity involved in the determination of offset times in music. This paper presents a preliminary study aiming at discussing ways to annotate and to evaluate offset times for pitched non-percussive instruments. Moreover, we conduct a case study of offset detection in violin recordings by evaluating a number of energy, spectral flux, and pitch based methods using a new dataset covering 6 different violin playing techniques. The new dataset, which is going to be shared with the research community, consists of 63 violin recordings that are thoroughly annotated based on perceptual loudness and note transition. The offset detection methods, which are adapted from well-known methods for onset detection, are evaluated using an onset-aware method we propose for this task. Result shows that the accuracy of offset detection is highly dependent on the playing techniques involved. Moreover, pitch-based methods can better get rid of the soft-decaying behavior of offsets and achieve the best result among others.",TWN,education,Developing economies,"[14.850808, -21.99486]","[-21.729464, -6.9079275]","[9.794704, -16.684155, -4.8081264]","[-1.6572522, 3.756229, -13.658374]","[9.315689, 6.7507396]","[5.838763, 2.5594785]","[11.094786, 12.524388, -0.3041183]","[8.193577, 7.469767, 10.824852]"
36,Christian Dittmar;Martin Pfleiderer;Meinard Müller,Automated Estimation of Ride Cymbal Swing Ratios in Jazz Recordings.,2015,https://doi.org/10.5281/zenodo.1418149,Christian Dittmar+International Audio Laboratories Erlangen>DEU>facility|International Audio Laboratories Erlangen>DEU>facility;Martin Pfléiderer+The Liszt University of Music Weimar>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility|International Audio Laboratories Erlangen>DEU>facility,"In this paper, we propose a new method suitable for the automatic analysis of microtiming played by drummers in jazz recordings. Specifically, we aim to estimate the drummers’ swing ratio in excerpts of jazz recordings taken from the Weimar Jazz Database. A first approach is based on automatic detection of ride cymbal (RC) onsets and evaluation of relative time intervals between them. However, small errors in the onset detection propagate considerably into the swing ratio estimates. As our main technical contribution, we propose to use the log-lag autocorrelation function (LLACF) as a mid-level representation for estimating swing ratios, circumventing the error-prone detection of RC onsets. In our experiments, the LLACF-based swing ratio estimates prove to be more reliable than the ones based on RC onset detection. Therefore, the LLACF seems to be the method of choice to process large amounts of jazz recordings. Finally, we indicate some implications of our method for microtiming studies in jazz research.",DEU,facility,Developed economies,"[13.476954, -17.820671]","[-33.86642, 1.5002956]","[-6.1028395, -29.735945, 11.484287]","[-12.86976, -5.2112217, -0.16501048]","[10.842802, 9.406004]","[6.281164, 2.1672082]","[12.085365, 14.51705, -0.6834112]","[8.334096, 7.060809, 11.435277]"
35,Leonardo O. Nunes;Martín Rocamora;Luis Jure;Luiz W. P. Biscainho,Beat and Downbeat Tracking Based on Rhythmic Patterns Applied to the Uruguayan Candombe Drumming.,2015,https://doi.org/10.5281/zenodo.1415728,Leonardo Nunes+ATL-Brazil>Unknown>company|Microsoft>USA>company;Martín Rocamora+Universidad de la República>URY>education;Luis Jure+Universidad de la República>URY>education;Luiz W. P. Biscainho+Federal University of Rio de Janeiro>BRA>education,"Computational analysis of the rhythmic/metrical structure of music from recorded audio is a hot research topic in music information retrieval. Recent research has explored the explicit modeling of characteristic rhythmic patterns as a way to improve upon existing beat-tracking algorithms, which typically fail on dealing with syncopated or polyrhythmic music. This work takes the Uruguayan Candombe drumming (an afro-rooted rhythm from Latin America) as a case study. After analyzing the aspects that make this music genre troublesome for usual algorithmic approaches and describing its basic rhythmic patterns, the paper proposes a supervised scheme for rhythmic pattern tracking that aims at finding the metric structure from a Candombe recording, including beat and downbeat phases. Then it evaluates and compares the performance of the method with those of general-purpose beat-tracking algorithms through a set of experiments involving a database of annotated recordings totaling over two hours of audio. The results of this work reinforce the advantages of tracking rhythmic patterns (possibly learned from annotated music) when it comes to automatically following complex rhythms. A software implementation of the proposal as well as the annotated database utilized are available to the research community with the publication of this paper.",Unknown,company,Unknown,"[34.24874, -37.929813]","[-23.443878, 0.92578673]","[12.676752, -25.59006, -6.5322924]","[-1.9448204, 13.53896, -6.053189]","[10.609079, 4.447975]","[5.9796586, 1.785723]","[10.359537, 13.038029, -2.2096932]","[8.116333, 6.869763, 11.657713]"
34,Chih-Wei Wu;Alexander Lerch,Drum Transcription Using Partially Fixed Non-Negative Matrix Factorization with Template Adaptation.,2015,https://doi.org/10.5281/zenodo.1417839,Chih-Wei Wu+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education,"In this paper, a template adaptive drum transcription algorithm using partially fixed Non-negative Matrix Factorization (NMF) is presented. The proposed method detects percussive events in complex mixtures of music with a minimal training set. The algorithm decomposes the music signal into two dictionaries: a percussive dictionary initialized with pre-defined drum templates and a harmonic dictionary initialized with undeﬁned entries. The harmonic dictionary is adapted to the non-percussive music content in a standard NMF procedure. The percussive dictionary is adapted to each individual signal in an iterative scheme: it is fixed during the decomposition process, and is updated based on the result of the previous convergence. Two template adaptation methods are proposed to provide more flexibility and robustness in the case of unknown data. The performance of the proposed system has been evaluated and compared to state of the art systems. The results show that template adaptation improves the transcription performance, and the detection accuracy is in the same range as more complex systems.",USA,education,Developed economies,"[27.65221, -44.243233]","[-50.97806, -23.027285]","[18.455517, -19.471584, 5.770455]","[-4.8629217, -17.44114, -26.805141]","[7.618871, 7.2014623]","[6.0524616, 4.7056956]","[10.385995, 11.445959, 1.0466821]","[9.335437, 8.723564, 9.925426]"
33,Brian McFee;Eric J. Humphrey;Juan Pablo Bello,A Software Framework for Musical Data Augmentation.,2015,https://doi.org/10.5281/zenodo.1418365,"Brian McFee+New York University>USA>education|MuseAmi, Inc.>USA>company;Eric J. Humphrey+New York University>USA>education|MuseAmi, Inc.>USA>company;Juan P. Bello+New York University>USA>education|MuseAmi, Inc.>USA>company","Predictive models for music annotation tasks are practically limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of “data augmentation” — supplementing a training set with carefully perturbed samples — has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated perturbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals.",USA,education,Developed economies,"[17.106028, 10.066824]","[-23.08085, -27.46236]","[-6.457229, 3.7874897, 18.862709]","[-13.853826, -9.073491, -13.715279]","[11.2898655, 7.535728]","[8.342291, 4.812995]","[13.328516, 12.777835, -0.56731564]","[9.58814, 6.8864202, 9.376919]"
32,Hendrik Schreiber,Improving Genre Annotations for the Million Song Dataset.,2015,https://doi.org/10.5281/zenodo.1414760,Hendrik Schreiber+tagtraum industries incorporated>DEU>company,"Any automatic music genre recognition (MGR) system must show its value in tests against a ground truth dataset. Recently, the public dataset most often used for this purpose has been proven problematic, because of mislabeling, duplications, and its relatively small size. Another dataset, the Million Song Dataset (MSD), a collection of features and metadata for one million tracks, unfortunately does not contain readily accessible genre labels. Therefore, multiple attempts have been made to add song-level genre annotations, which are required for supervised machine learning tasks. Thus far, the quality of these annotations has not been evaluated. In this paper we present a method for creating additional genre annotations for the MSD from databases, which contain multiple, crowd-sourced genre labels per song (Last.fm, beaTunes). Based on label co-occurrence rates, we derive taxonomies, which allow inference of top-level genres. These are most often used in MGR systems. We then combine multiple datasets using majority voting. This both promises a more reliable ground truth and allows the evaluation of the newly generated and pre-existing datasets. To facilitate further research, all derived genre annotations are publicly available on our website.",DEU,company,Developed economies,"[-32.080593, 6.9008245]","[32.485657, -2.3985877]","[-18.092487, 5.353243, 5.606537]","[16.774853, 12.450113, 1.1700965]","[13.861586, 9.679092]","[10.722645, 3.534367]","[14.705742, 13.857126, -0.4881986]","[12.28559, 6.3221064, 10.924275]"
31,Colin Raffel;Daniel P. W. Ellis,Large-Scale Content-Based Matching of MIDI and Audio Files.,2015,https://doi.org/10.5281/zenodo.1417371,Colin Raffel+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"MIDI files, when paired with corresponding audio recordings, can be used as ground truth for many music information retrieval tasks. We present a system which can efficiently match and align MIDI files to entries in a large corpus of audio content based solely on content, i.e., without using any metadata. The core of our approach is a convolutional network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space. Once represented in this way, we can efficiently perform large-scale dynamic time warping searches to match MIDI data to audio recordings. We evaluate our approach on the task of matching a huge corpus of MIDI files to the Million Song Dataset.",USA,education,Developed economies,"[38.382904, 0.12517908]","[21.02472, 13.327755]","[7.851648, -8.449585, 22.96478]","[15.902651, -8.107632, 5.593518]","[10.426045, 7.1868987]","[10.20803, 1.6824383]","[12.644328, 11.5722, -0.63734424]","[11.499981, 6.159945, 12.454901]"
30,Jan Van Balen;John Ashley Burgoyne;Dimitrios Bountouridis;Daniel Müllensiefen;Remco C. Veltkamp,Corpus Analysis Tools for Computational Hook Discovery.,2015,https://doi.org/10.5281/zenodo.1415038,"Jan Van Balen+Utrecht University>NLD>education;John Ashley Burgoyne+University of Amsterdam>NLD>education;Dimitrios Bountouridis+Utrecht University>NLD>education;Daniel M¨ullensiefen+Goldsmiths, University of London>GBR>education;Remco C. Veltkamp+Utrecht University>NLD>education","Compared to studies with symbolic music data, advances in music description from audio have overwhelmingly focused on ground truth reconstruction and maximizing prediction accuracy, with only a small fraction of studies using audio description to gain insight into musical data. We present a strategy for the corpus analysis of audio data that is optimized for interpretable results. The approach brings two previously unexplored concepts to the audio domain: audio bigram distributions, and the use of corpus-relative or “second-order” descriptors. To test the real-world applicability of our method, we present an experiment in which we model song recognition data collected in a widely-played music game. By using the proposed corpus analysis pipeline we are able to present a cognitively adequate analysis that allows a model interpretation in terms of the listening history and experience of our participants. We find that our corpus-based audio features are able to explain a comparable amount of variance to symbolic features for this task when used alone and that they can supplement symbolic features profitably when the two types of features are used in tandem. Finally, we highlight new insights into what makes music recognizable.",NLD,education,Developed economies,"[17.545633, 35.30536]","[-12.961713, 10.638653]","[-1.9919404, 21.917847, 25.20643]","[-11.395361, 9.159722, 3.6082666]","[12.339583, 6.5167727]","[9.525164, 4.409152]","[14.016642, 12.669691, -0.77288574]","[10.505864, 6.136921, 10.384904]"
29,Marcelo E. Rodríguez-López;Anja Volk,Selective Acquisition Techniques for Enculturation-Based Melodic Phrase Segmentation.,2015,https://doi.org/10.5281/zenodo.1415064,Marcelo E. Rodríguez-López+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education,"Automatic melody segmentation is an important yet unsolved problem in Music Information Retrieval. Research in the field of Music Cognition suggests that previous listening experience plays a considerable role in the perception of melodic segment structure. At present automatic melody segmenters that model listening experience commonly do so using unsupervised statistical learning with ‘non-selective’ information acquisition techniques, i.e. the learners gather and store information indiscriminately into memory. In this paper we investigate techniques for ‘selective’ information acquisition, i.e. our learning model uses a goal-oriented approach to select what to store in memory. We test the usefulness of the segmentations produced using selective acquisition learning in a melody classification experiment involving melodies of different cultures. Our results show that the segments produced by our selective learner segmenters substantially improve classification accuracy when compared to segments produced by a non-selective learner segmenter, two local segmentation methods, and two naïve baselines.",NLD,education,Developed economies,"[4.232886, -8.026045]","[1.4128606, 5.4567866]","[11.349455, 9.480003, -6.404658]","[-3.1221569, -3.5506067, 1.1479529]","[10.730068, 9.900801]","[8.412513, 2.4902194]","[11.754217, 15.168433, -0.61167747]","[10.11846, 7.2105675, 11.734678]"
27,Mohamed Sordo;Mitsunori Ogihara;Stefan Wuchty,Analysis of the Evolution of Research Groups and Topics in the ISMIR Conference.,2015,https://doi.org/10.5281/zenodo.1416354,Mohamed Sordo+University of Miami>USA>education;Mitsunori Ogihara+University of Miami>USA>education;Stefan Wuchty+University of Miami>USA>education,"We present an analysis of the topics and research groups that participated in the ISMIR conference over the last 15 years, based on its proceedings. While we first investigate the topological changes of the co-authorship network as well as topics over time, we also identify groups of researchers, allowing us to investigate their evolution and topic dependence. Notably, we find that large groups last longer if they actively alter their membership. Furthermore, such groups tend to cover a wider selection of topics, suggesting that a change of members as well as of research topics increases their adaptability. In turn, smaller groups show the opposite behavior, persisting longer if their membership is altered minimally and focus on a smaller set of topics. Finally, by analyzing the effect of group size and lifespan on research impact, we observed that papers penned by medium sized and long lasting groups tend to have a citation advantage.",USA,education,Developed economies,"[-13.560301, 46.452038]","[29.168705, 41.025215]","[-31.377535, -7.646529, -1.8628029]","[3.9628694, 8.8208475, 23.330683]","[13.799861, 5.6868773]","[12.014484, 0.42533523]","[15.156579, 11.974576, -1.7188525]","[12.346165, 4.187725, 12.190029]"
107,Jin Ha Lee;Xiao Hu 0001;Kahyun Choi;J. Stephen Downie,MIREX Grand Challenge 2014 User Experience: Qualitative Analysis of User Feedback.,2015,https://doi.org/10.5281/zenodo.1417103,Jin Ha Lee+University of Washington>USA>education;Xiao Hu+University of Hong Kong>HKG>education;Kahyun Choi+University of Illinois>USA>education;J. Stephen Downie+University of Illinois>USA>education,"Evaluation has always been fundamental to the Music Information Retrieval (MIR) community, as evidenced by the popularity of the Music Information Retrieval Evaluation eXchange (MIREX). However, prior MIREX tasks have primarily focused on testing specialized MIR algorithms that sit on the back end of systems. Not until the Grand Challenge 2014 User Experience (GC14UX) task had the users’ overall interaction and experience with complete systems been formally evaluated. Three systems were evaluated based on five criteria. This paper reports the results of GC14UX, with a special focus on the qualitative analysis of 99 free text responses collected from evaluators. The analysis revealed additional user opinions, not fully captured by score ratings on the given criteria, and demonstrated the challenge of evaluating a variety of systems with different user goals. We conclude with a discussion on the implications of findings and recommendations for future UX evaluation tasks, including adding new criteria: Aesthetics, Performance, and Utility.",USA,education,Developed economies,"[-6.5803795, 58.490097]","[27.53762, 31.51445]","[-32.273514, 0.8630382, -5.3147535]","[4.185133, 5.3097067, 14.633369]","[13.549096, 4.7638493]","[11.929979, 0.82710606]","[15.075745, 11.151964, -1.5322486]","[12.351776, 4.492675, 12.288975]"
112,Sangeun Kum;Changheun Oh;Juhan Nam,Melody Extraction on Vocal Segments Using Multi-Column Deep Neural Networks.,2016,https://doi.org/10.5281/zenodo.1414788,Sangeun Kum+Korea Advanced Institute of Science and Technology>KOR>education;Changheun Oh+Korea Advanced Institute of Science and Technology>KOR>education;Juhan Nam+Korea Advanced Institute of Science and Technology>KOR>education,"Singing melody extraction is a task that tracks pitch contour of singing voice in polyphonic music. While the majority of melody extraction algorithms are based on computing a saliency function of pitch candidates or separating the melody source from the mixture, data-driven approaches based on classification have been rarely explored. In this paper, we present a classification-based approach for melody extraction on vocal segments using multi-column deep neural networks. In the proposed model, each of neural networks is trained to predict a pitch label of singing voice from spectrogram, but their outputs have different pitch resolutions. The final melody contour is inferred by combining the outputs of the networks and post-processing it with a hidden Markov model. In order to take advantage of the data-driven approach, we also augment training data by pitch-shifting the audio content and modifying the pitch label accordingly. We use the RWC dataset and vocal tracks of the MedleyDB dataset for training the model and evaluate it on the ADC 2004, MIREX 2005 and MIR-1k datasets. Through several settings of experiments, we show incremental improvements of the melody prediction. Lastly, we compare our best result to those of previous state-of-the-arts.",KOR,education,Developing economies,"[4.0072374, -13.403467]","[-29.353178, -33.61608]","[17.758749, 6.785082, -2.412853]","[-4.019591, -9.847886, -17.8608]","[9.914859, 10.118906]","[7.7012434, 5.2409267]","[10.975499, 14.937292, -0.313703]","[9.774962, 7.177836, 8.963039]"
108,Matthias Dorfer;Andreas Arzt;Gerhard Widmer,Towards Score Following In Sheet Music Images.,2016,https://doi.org/10.5281/zenodo.1415548,Matthias Dorfer+Johannes Kepler University Linz>AUT>education;Andreas Arzt+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,"This paper addresses the matching of short music audio snippets to the corresponding pixel location in images of sheet music. A system is presented that simultaneously learns to read notes, listens to music and matches the currently played music to its corresponding notes in the sheet. It consists of an end-to-end multi-modal convolutional neural network that takes as input images of sheet music and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (covering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks – which have proven to be powerful image processing models – working with sheet music becomes feasible and a promising future research direction.",AUT,education,Developed economies,"[19.943012, -7.8460245]","[-22.379671, -30.23233]","[22.227665, 22.678328, -2.4312928]","[-11.9421425, -16.05408, -13.538465]","[10.576329, 6.623772]","[8.55238, 5.070348]","[12.294355, 12.242147, -1.4121034]","[9.821182, 6.4106216, 8.956226]"
26,Romain Hennequin;François Rigaud,Long-Term Reverberation Modeling for Under-Determined Audio Source Separation with Application to Vocal Melody Extraction.,2016,https://doi.org/10.5281/zenodo.1417489,Romain Hennequin+Deezer R&D>FRA>company;François Rigaud+Audionamix R&D>FRA>company,"In this paper, we present a way to model long-term reverberation effects in under-determined source separation algorithms based on a non-negative decomposition framework. A general model for the sources affected by reverberation is introduced and update rules for the estimation of the parameters are presented. Combined with a well-known source-filter model for singing voice, an application to the extraction of reverberated vocal tracks from polyphonic music signals is proposed. Finally, an objective evaluation of this application is described. Performance improvements are obtained compared to the same model without reverberation modeling, in particular by significantly reducing the amount of interference between sources.",FRA,company,Developed economies,"[1.6948199, -43.357403]","[-43.051285, -29.392395]","[27.902662, 2.9233086, -6.5644894]","[-6.853198, -7.4595933, -29.071447]","[8.825116, 10.436861]","[6.4777513, 5.3238907]","[10.866538, 14.188937, 1.4771358]","[9.88768, 8.668225, 9.540311]"
27,Elliot Creager;Noah D. Stein;Roland Badeau;Philippe Depalle,Nonnegative Tensor Factorization with Frequency Modulation Cues for Blind Audio Source Separation.,2016,https://doi.org/10.5281/zenodo.1415736,"Elliot Creager+Analog Devices Lyric Labs>USA>company|CIRMMT, McGill University>CAN>education;Noah D. Stein+Analog Devices Lyric Labs>USA>company|CIRMMT, McGill University>CAN>education;Roland Badeau+LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay>FRA>education|CIRMMT, McGill University>CAN>education;Philippe Depalle+CIRMMT, McGill University>CAN>education","We present Vibrato Nonnegative Tensor Factorization, an algorithm for single-channel unsupervised audio source separation with an application to separating instrumental or vocal sources with nonstationary pitch from music recordings. Our approach extends Nonnegative Matrix Factorization for audio modeling by including local estimates of frequency modulation as cues in the separation. This permits the modeling and unsupervised separation of vibrato or glissando musical sources, which is not possible with the basic matrix factorization formulation. The algorithm factorizes a sparse nonnegative tensor comprising the audio spectrogram and local frequency-slope-to-frequency ratios, which are estimated at each time-frequency bin using the Distributed Derivative Method. The use of local frequency modulations as separation cues is motivated by the principle of common fate partial grouping from Auditory Scene Analysis, which hypothesizes that each latent source in a mixture is characterized perceptually by coherent frequency and amplitude modulations shared by its component partials. We derive multiplicative factor updates by Minorization-Maximization, which guarantees convergence to a local optimum by iteration. We then compare our method to the baseline on two separation tasks: one considers synthetic vibrato notes, while the other considers vibrato string instrument recordings.",USA,company,Developed economies,"[11.868536, -46.588066]","[-45.916225, -25.776083]","[29.174135, -0.4453735, -10.467832]","[-6.943628, -11.068731, -30.137596]","[8.6399765, 9.864968]","[6.3083014, 5.2028894]","[11.113589, 13.690521, 1.4508653]","[9.724105, 8.927096, 9.75407]"
28,Chih-Wei Wu;Alexander Lerch,On Drum Playing Technique Detection in Polyphonic Mixtures.,2016,https://doi.org/10.5281/zenodo.1416680,Chih-Wei Wu+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"In this paper, the problem of drum playing technique detection in polyphonic mixtures of music is addressed. We focus on the identification of 4 rudimentary techniques: strike, buzz roll, flam, and drag. The specifics and the challenges of this task are being discussed, and different sets of features are compared, including various features extracted from NMF-based activation functions, as well as baseline spectral features. We investigate the capabilities and limitations of the presented system in the case of real-world recordings and polyphonic mixtures. To design and evaluate the system, two datasets are introduced: a training dataset generated from individual drum hits, and additional annotations of the well-known ENST drum dataset minus one subset as test dataset. The results demonstrate issues with the traditionally used spectral features, and indicate the potential of using NMF activation functions for playing technique detection, however, the performance of polyphonic music still leaves room for future improvement.",USA,education,Developed economies,"[23.36266, -45.91308]","[-16.50183, 2.3047078]","[16.155218, -19.687181, -0.4224712]","[6.1967335, 11.194621, -11.915769]","[7.920631, 7.04441]","[8.4884815, 3.979386]","[10.31247, 11.788098, 0.91214895]","[9.744666, 7.4112754, 10.231957]"
29,I-Ting Liu;Richard Randall,Predicting Missing Music Components with Bidirectional Long Short-Term Memory Neural Networks.,2016,https://doi.org/10.5281/zenodo.1417239,I-Ting Liu+Carnegie Mellon University>USA>education;Richard Randall+Carnegie Mellon University>USA>education,"Successfully predicting missing components (entire parts or voices) from complex multipart musical textures has attracted researchers of music information retrieval and music theory. However, these applications were limited to either two-part melody and accompaniment (MA) textures or four-part Soprano-Alto-Tenor-Bass (SATB) textures. This paper proposes a robust framework applicable to both textures using a Bidirectional Long-Short Term Memory (BLSTM) recurrent neural network. The BLSTM system was evaluated using frame-wise accuracies on the Nottingham Folk Song dataset and J. S. Bach Chorales. Experimental results demonstrated that adding bidirectional links to the neural network improves prediction accuracy by 3% on average. Specifically, BLSTM outperforms other neural-network based methods by 4.6% on average for four-part SATB and two-part MA textures (employing a transition matrix). The high accuracies obtained with BLSTM on both two-part and four-part textures indicated that BLSTM is the most robust and applicable structure for predicting missing components from multi-part musical textures.",USA,education,Developed economies,"[-15.370426, -2.3622386]","[-4.177886, -30.80689]","[-0.0006642862, 6.51622, 13.519536]","[-2.0055323, 5.4305816, -20.714369]","[11.65841, 8.813169]","[8.774268, 5.3146477]","[13.362117, 13.016642, 0.023099268]","[9.686332, 6.255646, 9.305662]"
30,Vinutha T. P.;Suryanarayana Sankagiri;Kaustuv Kanti Ganguli;Preeti Rao,Structural Segmentation and Visualization of Sitar and Sarod Concert Audio.,2016,https://doi.org/10.5281/zenodo.1414924,Vinutha T. P.+IIT Bombay>IND>education;Suryanarayana Sankagiri+IIT Bombay>IND>education;Kaustuv Kanti Ganguli+IIT Bombay>IND>education;Preeti Rao+IIT Bombay>IND>education,"Hindustani classical instrumental concerts follow an episodic development that, musicologically, is described via changes in the rhythmic structure. Uncovering this structure in a musically relevant form can provide for powerful visual representations of the concert audio that is of potential value in music appreciation and pedagogy. We investigate the structural analysis of the metered section (gat) of concerts of two plucked string instruments, the sitar and sarod. A prominent aspect of the gat is the interplay between the melody soloist and the accompanying drummer (tabla). The tempo as provided by the tabla together with the rhythmic density of the sitar/sarod plucks serve as the main dimensions that predict the transition between concert sections. We present methods to access the stream of tabla onsets separately from the sitar/sarod onsets, addressing challenges that arise in the instrument separation. Further, the robust detection of tempo and the estimation of rhythmic density of sitar/sarod plucks are discussed. A case study of a fully annotated concert is presented, and is followed by results of achieved segmentation accuracy on a database of sitar and sarod gats across artists.",IND,education,Developing economies,"[-0.32592985, -5.3603306]","[-23.968224, 8.15917]","[7.269769, -7.8576283, -2.460104]","[0.29498932, 10.189813, -14.807279]","[11.561839, 8.190662]","[6.5949736, 1.6060311]","[12.316094, 14.216786, 0.03817282]","[8.545623, 7.0089674, 11.783034]"
31,Jonathan Driedger;Stefan Balke;Sebastian Ewert;Meinard Müller,Template-Based Vibrato Analysis in Complex Music Signals.,2016,https://doi.org/10.5281/zenodo.1417006,Jonathan Driedger+International Audio Laboratories Erlangen>DEU>facility|Queen Mary University of London>GBR>education;Stefan Balke+International Audio Laboratories Erlangen>DEU>facility|Queen Mary University of London>GBR>education;Sebastian Ewert+Queen Mary University of London>GBR>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"The automated analysis of vibrato in complex music signals is a highly challenging task. A common strategy is to proceed in a two-step fashion. First, a fundamental frequency (F0) trajectory for the musical voice that is likely to exhibit vibrato is estimated. In a second step, the trajectory is then analyzed with respect to periodic frequency modulations. As a major drawback, however, such a method cannot recover from errors made in the inherently difficult first step, which severely limits the performance during the second step. In this work, we present a novel vibrato analysis approach that avoids the first error-prone F0-estimation step. Our core idea is to perform the analysis directly on a signal’s spectrogram representation where vibrato is evident in the form of characteristic spectro-temporal patterns. We detect and parameterize these patterns by locally comparing the spectrogram with a predefined set of vibrato templates. Our systematic experiments indicate that this approach is more robust than F0-based strategies.",DEU,facility,Developed economies,"[51.308033, 18.004362]","[-2.6871445, -13.043554]","[9.891968, -28.902584, 13.766629]","[9.628796, -10.024823, -12.066757]","[10.978639, 7.1954684]","[6.5386796, 2.3960001]","[11.704499, 13.190194, -0.26847008]","[8.855326, 7.709363, 11.304472]"
32,Stefan Balke;Jonathan Driedger;Jakob Abeßer;Christian Dittmar;Meinard Müller,Towards Evaluating Multiple Predominant Melody Annotations in Jazz Recordings.,2016,https://doi.org/10.5281/zenodo.1415076,"Stefan Balke+International Audio Laboratories Erlangen>DEU>facility;Jonathan Driedger+International Audio Laboratories Erlangen>DEU>facility;Jakob Abeßer+Semantic Music Technologies Group, Fraunhofer IDMT>DEU>facility;Christian Dittmar+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility","Melody estimation algorithms are typically evaluated by separately assessing the task of voice activity detection and fundamental frequency estimation. For both subtasks, computed results are typically compared to a single human reference annotation. This is problematic since different human experts may differ in how they specify a predominant melody, thus leading to a pool of equally valid reference annotations. In this paper, we address the problem of evaluating melody extraction algorithms within a jazz music scenario. Using four human and two automatically computed annotations, we discuss the limitations of standard evaluation measures and introduce an adaptation of Fleiss’ kappa that can better account for multiple reference annotations. Our experiments not only highlight the behavior of the different evaluation measures, but also give deeper insights into the melody extraction task.",DEU,facility,Developed economies,"[7.8091974, 14.863691]","[-6.2411375, 8.760461]","[-7.8223095, -4.458665, 32.221966]","[-2.0978522, 4.806309, 8.274296]","[10.74659, 9.531859]","[8.851009, 2.5918653]","[12.114447, 14.426294, -0.70382017]","[10.305202, 6.5709457, 11.044374]"
33,Sebastian Böck;Florian Krebs;Gerhard Widmer,Joint Beat and Downbeat Tracking with Recurrent Neural Networks.,2016,https://doi.org/10.5281/zenodo.1415836,Sebastian Böck+Johannes Kepler University Linz>AUT>education;Florian Krebs+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,In this paper we present a novel method for jointly extracting beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectrograms is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and downbeat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles.,AUT,education,Developed economies,"[36.49817, -35.283672]","[-28.21628, -11.955043]","[9.796554, -29.177773, -3.9008656]","[-7.3040805, 18.170023, -14.369466]","[10.4887295, 4.1351385]","[5.1584325, 2.3551216]","[10.101048, 12.817123, -2.309394]","[7.7281775, 6.94916, 10.442969]"
34,Andre Holzapfel;Thomas Grill,Bayesian Meter Tracking on Learned Signal Representations.,2016,https://doi.org/10.5281/zenodo.1417263,Andre Holzapfel+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Thomas Grill+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"Most music exhibits a pulsating temporal structure, known as meter. Consequently, the task of meter tracking is of great importance for the domain of Music Information Retrieval. In our contribution, we specifically focus on Indian art musics, where meter is conceptualized at several hierarchical levels, and a diverse variety of metrical hierarchies exist, which poses a challenge for state of the art analysis methods. To this end, for the first time, we combine Convolutional Neural Networks (CNN), allowing to transcend manually tailored signal representations, with subsequent Dynamic Bayesian Tracking (BT), modeling the recurrent metrical structure in music. Our approach estimates meter structures simultaneously at two metrical levels. The results constitute a clear advance in meter tracking performance for Indian art music, and we also demonstrate that these results generalize to a set of Ballroom dances. Furthermore, the incorporation of neural network output allows a computationally efficient inference. We expect the combination of learned signal representations through CNNs and higher-level temporal modeling to be applicable to all styles of metered music, provided the availability of sufficient training data.",AUT,facility,Developed economies,"[41.77925, -34.172497]","[-27.607712, -13.689901]","[4.5111175, -36.145298, -0.030354775]","[-10.002053, 12.979203, -18.514729]","[11.127375, 4.3195634]","[5.084605, 2.2477686]","[10.582086, 13.155955, -2.6280823]","[7.6489897, 6.8189416, 10.435919]"
35,Frederic Font;Xavier Serra,Tempo Estimation for Music Loops and a Simple Confidence Measure.,2016,https://doi.org/10.5281/zenodo.1417659,"Frederic Font+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Tempo estimation is a common task within the music information retrieval community, but existing works are rarely evaluated with datasets of music loops and the algorithms are not tailored to this particular type of content. In addition to this, existing works on tempo estimation do not put an emphasis on providing a confidence value that indicates how reliable their tempo estimations are. In current music creation contexts, it is common for users to search for and use loops shared in online repositories. These loops are typically not produced by professionals and lack annotations. Hence, the existence of reliable tempo estimation algorithms becomes necessary to enhance the reusability of loops shared in such repositories. In this paper, we test six existing tempo estimation algorithms against four music loop datasets containing more than 35k loops. We also propose a simple and computationally cheap confidence measure that can be applied to any existing algorithm to estimate the reliability of their tempo predictions when applied to music loops. We analyse the accuracy of the algorithms in combination with our proposed confidence measure, and see that we can significantly improve the algorithms’ performance when only considering music loops with high estimated confidence.",ESP,education,Developed economies,"[39.195946, -25.352102]","[-33.85644, -4.5348816]","[-0.91687334, -26.465998, -0.46065223]","[-12.166445, 13.9605255, -9.526232]","[11.486748, 4.4697585]","[5.0956306, 1.7446194]","[10.982949, 13.34285, -2.8480809]","[7.4058, 6.757454, 11.044132]"
36,Sebastian Stober;Thomas Prätzlich;Meinard Müller,Brain Beats: Tempo Extraction from EEG Data.,2016,https://doi.org/10.5281/zenodo.1416128,Sebastian Stober+University of Potsdam>DEU>education;Thomas Prätzlich+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"This paper addresses the question how music information retrieval techniques originally developed to process audio recordings can be adapted for the analysis of corresponding brain activity data. In particular, we conducted a case study applying beat tracking techniques to extract the tempo from electroencephalography (EEG) recordings obtained from people listening to music stimuli. We point out similarities and differences in processing audio and EEG data and show to which extent the tempo can be successfully extracted from EEG signals. Furthermore, we demonstrate how the tempo extraction from EEG signals can be stabilized by applying different fusion approaches on the mid-level tempogram features.",DEU,education,Developed economies,"[39.803402, -28.70865]","[-25.780754, -2.6776774]","[3.2887692, -29.859425, -1.5192386]","[-3.649243, 8.351737, -6.023262]","[11.324572, 4.422959]","[5.5200524, 1.8909246]","[10.802169, 13.384161, -2.667936]","[7.712546, 7.041123, 11.244996]"
25,Ben Fields;Christophe Rhodes,Listen To Me - Don't Listen To Me: What Communities of Critics Tell Us About Music.,2016,https://doi.org/10.5281/zenodo.1417801,Ben Fields+Goldsmiths University of London>GBR>education;Christophe Rhodes+Goldsmiths University of London>GBR>education,"Social knowledge and data sharing on the Web takes many forms. So too do the ways people share ideas and opinions. In this paper we examine one such emerging form: the amateur critic. In particular, we examine genius.com, a website which allows its users to annotate and explain the meaning of segments of lyrics in music and other written works. We describe a novel dataset of approximately 700,000 users’ activity on genius.com, their social connections, and song annotation activity. The dataset encompasses over 120,000 songs, with more than 3 million unique annotations. Using this dataset, we model overlap in interest or expertise through the proxy of co-annotation. This is the basis for a complex network model of the activity on genius.com, which is then used for community detection. We introduce a new measure of network community activity: community skew. Through this analysis we draw a comparison of between co-annotation and notions of genre and categorisation in music. We show a new view on the social constructs of genre in music.",GBR,education,Developed economies,"[-39.49455, 19.836254]","[42.22423, 7.1057744]","[-15.934557, 24.481041, -5.54967]","[18.1371, 12.186296, 8.687157]","[15.308293, 9.18939]","[12.1240225, 2.737453]","[15.321201, 15.286837, -1.1720271]","[13.276343, 5.7238097, 11.720192]"
37,Brian McFee;Eric J. Humphrey;Julián Urbano,A Plan for Sustainable MIR Evaluation.,2016,https://doi.org/10.5281/zenodo.1417775,"Brian McFee+New York University>USA>education;Eric J. Humphrey+Spotify, Ltd.>USA>company;Julián Urbano+Universitat Pompeu Fabra>ESP>education","The Music Information Retrieval Evaluation eXchange (MIREX) is a valuable community service, having established standard datasets, metrics, baselines, methodologies, and infrastructure for comparing MIR methods. While MIREX has managed to successfully maintain operations for over a decade, its long-term sustainability is at risk. The imposed constraint that input data cannot be made freely available to participants necessitates that all algorithms run on centralized computational resources, which are administered by a limited number of people. This incurs an approximately linear cost with the number of submissions, exacting significant tolls on both human and financial resources, such that the current paradigm becomes less tenable as participation increases. To alleviate the recurring costs of future evaluation campaigns, we propose a distributed, community-centric paradigm for system evaluation, built upon the principles of openness, transparency, reproducibility, and incremental evaluation. We argue that this proposal has the potential to reduce operating costs to sustainable levels. Moreover, the proposed paradigm would improve scalability, and eventually result in the release of large, open datasets for improving both MIR techniques and evaluation methods.",USA,education,Developed economies,"[-8.073636, 57.67117]","[25.951023, 32.242496]","[-34.933037, -0.250328, -4.838659]","[2.0679245, 5.3324165, 14.21446]","[13.60469, 4.695125]","[11.78464, 0.62142915]","[15.084746, 11.129126, -1.5554779]","[12.14919, 4.4294667, 12.204323]"
39,Jin Ha Lee;Yea-Seul Kim;Chris Hubbles,A Look at the Cloud from Both Sides Now: An Analysis of Cloud Music Service Usage.,2016,https://doi.org/10.5281/zenodo.1417627,Jin Ha Lee+University of Washington>USA>education;Yea-Seul Kim+University of Washington>USA>education;Chris Hubbles+University of Washington>USA>education,"Despite the increasing popularity of cloud-based music services, few studies have examined how users select and utilize these services, how they manage and access their music collections in the cloud, and the issues or challenges they are facing within these services. In this paper, we present findings from an online survey with 198 responses collected from users of commercial cloud music services, exploring their selection criteria, use patterns, perceived limitations, and future predictions. We also investigate differences in these aspects by age and gender. Our results elucidate previously under-studied changes in music consumption, music listening behaviors, and music technology adoption. The findings also provide insights into how to improve the future design of cloud-based music services, and have broader implications for any cloud-based services designed for managing and accessing personal media collections.",USA,education,Developed economies,"[-32.256493, 27.343052]","[39.30583, 29.61799]","[-18.285933, 14.606555, -14.707907]","[9.128655, 11.242634, 16.939507]","[14.773796, 7.877641]","[12.8865185, 1.071117]","[14.802667, 14.76871, -2.1181023]","[13.270387, 4.352306, 12.19746]"
40,Yuta Ojima;Eita Nakamura;Katsutoshi Itoyama;Kazuyoshi Yoshii,"A Hierarchical Bayesian Model of Chords, Pitches, and Spectrograms for Multipitch Analysis.",2016,https://doi.org/10.5281/zenodo.1414968,Yuta Ojima+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"This paper presents a statistical multipitch analyzer that can simultaneously estimate pitches and chords (typical pitch combinations) from music audio signals in an unsupervised manner. A popular approach to multipitch analysis is to perform nonnegative matrix factorization (NMF) for estimating the temporal activations of semitone-level pitches and then execute thresholding for making a piano-roll representation. The major problems of this cascading approach are that an optimal threshold is hard to determine for each musical piece and that musically inappropriate pitch combinations are allowed to appear. To solve these problems, we propose a probabilistic generative model that fuses an acoustic model (NMF) for a music spectrogram with a language model (hidden Markov model; HMM) for pitch locations in a hierarchical Bayesian manner. More specifically, binary variables indicating the existences of pitches are introduced into the framework of NMF. The latent grammatical structures of those variables are regulated by an HMM that encodes chord progressions and pitch co-occurrences (chord components). Given a music spectrogram, all the latent variables (pitches and chords) are estimated jointly by using Gibbs sampling. The experimental results showed the great potential of the proposed method for unified music transcription and grammar induction.",JPN,education,Developed economies,"[38.392162, -12.017703]","[-43.302547, -21.70294]","[11.453332, -20.276808, 8.184889]","[-2.2584772, -11.523306, -26.984783]","[8.946274, 8.803609]","[6.6116753, 3.8127613]","[11.451783, 13.304258, 0.08688831]","[9.51193, 8.437767, 10.492603]"
41,Michele Buccoli;Massimiliano Zanoni;György Fazekas;Augusto Sarti;Mark B. Sandler,A Higher-Dimensional Expansion of Affective Norms for English Terms for Music Tagging.,2016,https://doi.org/10.5281/zenodo.1415194,"Michele Buccoli+Politecnico di Milano>ITA>education;Massimiliano Zanoni+Politecnico di Milano>ITA>education;György Fazekas+Queen Mary, University of London>GBR>education;Augusto Sarti+Politecnico di Milano>ITA>education;Mark Sandler+Queen Mary, University of London>GBR>education","The Valence, Arousal and Dominance (VAD) model for emotion representation is widely used in music analysis. The ANEW dataset is composed of more than 2000 emotion related descriptors annotated in the VAD space. However, due to the low number of dimensions of the VAD model, the distribution of terms of the ANEW dataset tends to be compact and cluttered. In this work, we aim at finding a possibly higher-dimensional transformation of the VAD space, where the terms of the ANEW dataset are better organised conceptually and bear more relevance to music tagging. Our approach involves the use of a kernel expansion of the ANEW dataset to exploit a higher number of dimensions, and the application of distance learning techniques to find a distance metric that is consistent with the semantic similarity among terms. In order to train the distance learning algorithms, we collect information on the semantic similarity from human annotation and editorial tags. We evaluate the quality of the method by clustering the terms in the found high-dimensional domain. Our approach exhibits promising results with objective and subjective performance metrics, showing that a higher dimensional space could be useful to model semantic similarity among terms of the ANEW dataset.",ITA,education,Developed economies,"[-49.696007, 2.1929972]","[43.705677, -7.7615247]","[-21.68739, 18.287844, 3.472387]","[18.051184, 20.576557, 2.2594042]","[13.8172455, 12.369449]","[12.725959, 3.886887]","[15.923555, 14.626566, 1.3121504]","[13.908579, 5.3826895, 10.490119]"
42,Chia-Hao Chung;Jing-Kai Lou;Homer H. Chen,"A Latent Representation of Users, Sessions, and Songs for Listening Behavior Analysis.",2016,https://doi.org/10.5281/zenodo.1416878,Chia-Hao Chung+National Taiwan University>TWN>education|KKBOX Inc.>TWN>company;Jing-Kai Lou+KKBOX Inc.>TWN>company|National Taiwan University>TWN>education;Homer Chen+National Taiwan University>TWN>education,"Understanding user listening behaviors is important to the personalization of music recommendation. In this paper, we present an approach that discovers user behavior from a large-scale, real-world listening record. The proposed approach generates a latent representation of users, listening sessions, and songs, where each of these objects is represented as a point in the multi-dimensional latent space. Since the distance between two points is an indication of the similarity of the two corresponding objects, it becomes extremely simple to evaluate the similarity between songs or the matching of songs with the user preference. By exploiting this feature, we provide a two-dimensional user behavior analysis framework for music recommendation. Exploring the relationships between user preference and the contextual or temporal information in the session data through this framework significantly facilitates personalized music recommendation. We provide experimental results to illustrate the strengths of the proposed approach for user behavior analysis.",TWN,education,Developing economies,"[-39.81053, 25.16834]","[39.484737, 17.043869]","[-13.963851, 18.18732, -9.839995]","[17.114855, 8.048265, 17.27976]","[15.073806, 8.783051]","[12.430329, 1.8524336]","[14.919501, 15.099172, -1.4366006]","[13.408413, 5.160028, 12.619522]"
43,Vincent Besson;Marco Gurrieri;Philippe Rigaux;Alice Tacaille;Virginie Thion,A Methodology for Quality Assessment in Collaborative Score Libraries.,2016,https://doi.org/10.5281/zenodo.1418105,"Vincent Besson+University of Tours>FRA>education;Marco Gurrieri+University of Tours>FRA>education;Philippe Rigaux+CEDRIC/CNAM>FRA>education;Alice Tacaille+IReMus, Sorbonne Universités>FRA>education;Virginie Thion+IRISA, University of Rennes 1>FRA>education","We examine quality issues raised by the development of XML-based Digital Score Libraries. Based on the authors’ practical experience, the paper exposes the quality shortcomings inherent to the complexity of music encoding, and the lack of support from state-of-the-art formats. We also identify the various facets of the “quality” concept with respect to usages and motivations. We finally propose a general methodology to introduce quality management as a first-level concern in the management of score collections.",FRA,education,Developed economies,"[-39.85571, 31.720354]","[6.2124996, 40.44907]","[-2.5917566, 21.523542, -5.0754747]","[-6.32006, -9.510697, 22.902405]","[15.59244, 8.116968]","[10.318449, 0.22656024]","[13.698081, 13.327953, -1.7166175]","[10.899659, 4.891737, 12.1477995]"
44,Katherine M. Kinnaird,Aligned Hierarchies: A Multi-Scale Structure-Based Representation for Music-Based Data Streams.,2016,https://doi.org/10.5281/zenodo.1417405,Katherine M. Kinnaird+Macalester College>USA>education,"We introduce aligned hierarchies, a low-dimensional representation for music-based data streams, such as recordings of songs or digitized representations of scores. The aligned hierarchies encode all hierarchical decompositions of repeated elements from a high-dimensional and noisy music-based data stream into one object. These aligned hierarchies can be embedded into a classification space with a natural notion of distance. We construct the aligned hierarchies by finding, encoding, and synthesizing all repeated structure present in a music-based data stream. For a data set of digitized scores, we conducted experiments addressing the fingerprint task that achieved perfect precision-recall values. These experiments provide an initial proof of concept for the aligned hierarchies addressing MIR tasks.",USA,education,Developed economies,"[9.18834, 36.522858]","[2.6592915, 21.899492]","[-3.6439128, 7.204124, -25.519312]","[-10.0857315, 13.408229, 15.032006]","[11.977645, 7.9395394]","[9.3279705, 1.3886019]","[13.003245, 13.679526, -0.57782614]","[10.542646, 5.7206483, 10.986949]"
45,Francisco Rodríguez-Algarra;Bob L. Sturm;Hugo Maruri-Aguilar,Analysing Scattering-Based Music Content Analysis Systems: Where's the Music?.,2016,https://doi.org/10.5281/zenodo.1414724,Francisco Rodríguez-Algarra+Centre for Digital Music>GBR>education|Queen Mary University of London>GBR>education;Bob L. Sturm+Centre for Digital Music>GBR>education|Queen Mary University of London>GBR>education;Hugo Maruri-Aguilar+Queen Mary University of London>GBR>education,"Music content analysis (MCA) systems built using scattering transform features are reported quite successful in the GTZAN benchmark music dataset. In this paper, we seek to answer why. We first analyse the feature extraction and classification components of scattering-based MCA systems. This guides us to perform intervention experiments on three factors: train/test partition, classifier and recording spectrum. The partition intervention shows a decrease in the amount of reproduced ground truth by the resulting systems. We then replace the learning algorithm with a binary decision tree, and identify the impact of specific feature dimensions. We finally alter the spectral content related to such dimensions, which reveals that these scattering-based systems exploit acoustic information below 20 Hz to reproduce GTZAN ground truth. The source code to reproduce our experiments is available online.",GBR,education,Developed economies,"[-17.285982, 15.110331]","[19.920689, -12.608672]","[-10.303541, -1.7094786, -14.15258]","[17.97612, 7.1132956, -6.0112147]","[13.78554, 7.7158422]","[9.953239, 3.7112107]","[14.015851, 14.258254, -1.8597755]","[11.611121, 6.8462048, 10.582704]"
46,Anders Elowsson,Beat Tracking with a Cepstroid Invariant Neural Network.,2016,https://doi.org/10.5281/zenodo.1416054,Anders Elowsson+KTH Royal Institute of Technology>SWE>education,"We present a novel rhythm tracking architecture that learns how to track tempo and beats through layered learning. A basic assumption of the system is that humans understand rhythm by letting salient periodicities in the music act as a framework, upon which the rhythmical structure is interpreted. Therefore, the system estimates the cepstroid (the most salient periodicity of the music), and uses a neural network that is invariant with regards to the cepstroid length. The input of the network consists mainly of features that capture onset characteristics along time, such as spectral differences. The invariant properties of the network are achieved by subsampling the input vectors with a hop size derived from a musically relevant subdivision of the computed cepstroid of each song. The output is filtered to detect relevant periodicities and then used in conjunction with two additional networks, which estimates the speed and tempo of the music, to predict the final beat positions. We show that the architecture has a high performance on music with public annotations.",SWE,education,Developed economies,"[35.22422, -34.264988]","[-30.832502, -10.908974]","[8.939066, -31.474234, -5.618824]","[-8.227178, 12.787826, -14.234118]","[10.471186, 4.216788]","[4.9851274, 2.213874]","[10.127513, 12.869765, -2.2617192]","[7.533628, 6.946376, 10.462989]"
47,Anna M. Kruspe,Bootstrapping a System for Phoneme Recognition and Keyword Spotting in Unaccompanied Singing.,2016,https://doi.org/10.5281/zenodo.1417553,Anna M. Kruspe+Fraunhofer IDMT>DEU>facility,"Speech recognition in singing is still a largely unsolved problem. Acoustic models trained on speech usually produce unsatisfactory results when used for phoneme recognition in singing. On the flipside, there is no phonetically annotated singing data set that could be used to train more accurate acoustic models for this task. In this paper, we attempt to solve this problem using the DAMP data set which contains a large number of recordings of amateur singing in good quality. We first align them to the matching textual lyrics using an acoustic model trained on speech. We then use the resulting phoneme alignment to train new acoustic models using only subsets of the DAMP singing data. These models are then tested for phoneme recognition and, on top of that, keyword spotting. Evaluation is performed for different subsets of DAMP and for an unrelated set of the vocal tracks of commercial pop songs. Results are compared to those obtained with acoustic models trained on the TIMIT speech data set and on a version of TIMIT augmented for singing. Our new approach shows significant improvements over both.",DEU,facility,Developed economies,"[-5.110571, -35.857777]","[-30.171371, -41.16274]","[20.285921, 13.495442, -11.639034]","[4.042481, -7.003283, -19.840288]","[9.859924, 11.141783]","[7.851302, 4.548955]","[11.210263, 15.168066, 0.8190074]","[10.408571, 7.40824, 9.108693]"
48,Eva Zangerle;Martin Pichl;Benedikt Hupfauf;Günther Specht,Can Microblogs Predict Music Charts? An Analysis of the Relationship Between #Nowplaying Tweets and Music Charts.,2016,https://doi.org/10.5281/zenodo.1417881,Eva Zangerle+University of Innsbruck>AUT>education;Martin Pichl+University of Innsbruck>AUT>education;Benedikt Hupfauf+University of Innsbruck>AUT>education;Günther Specht+University of Innsbruck>AUT>education,"Twitter is one of the leading social media platforms, where hundreds of millions of tweets cover a wide range of topics, including the music a user is listening to. Such #now-playing tweets may serve as an indicator for future charts, however, this has not been thoroughly studied yet. Therefore, we investigate to which extent such tweets correlate with the Billboard Hot 100 charts and whether they allow for music charts prediction. The analysis is based on #now-playing tweets and the Billboard charts of the years 2014 and 2015. We analyze three different aspects in regards to the time series representing #now-playing tweets and the Billboard charts: (i) the correlation of Twitter and the Billboard charts, (ii) the temporal relation between those two and (iii) the prediction performance in regards to charts positions of tracks. We find that while there is a mild correlation between tweets and the charts, there is a temporal lag between these two time series for 90% of all tracks. As for the predictive power of Twitter, we find that incorporating Twitter information in a multivariate model results in a significant decrease of both the mean RMSE as well as the variance of rank predictions.",AUT,education,Developed economies,"[-45.606403, 10.816327]","[50.253506, 13.7747555]","[-28.60558, 10.865669, -4.227975]","[20.24019, 14.413861, 14.195368]","[14.577153, 8.969473]","[12.40902, 2.2795665]","[15.072669, 14.418029, -1.0647974]","[13.422556, 5.4309344, 12.202449]"
49,Ricardo Scholz;Geber Ramalho;Giordano Cabral,Cross Task Study on MIREX Recent Results: An Index for Evolution Measurement and Some Stagnation Hypotheses.,2016,https://doi.org/10.5281/zenodo.1416898,Ricardo Scholz+Universidade Federal de Pernambuco>BRA>education;Geber Ramalho+Universidade Federal de Pernambuco>BRA>education;Giordano Cabral+Universidade Federal de Pernambuco>BRA>education,"In the last 20 years, Music Information Retrieval (MIR) has been an expanding research field, and the MIREX competition has become the main evaluation venue in MIR field. Analyzing recent results for various tasks of MIREX (MIR Evaluation eXchange), we observed that the evolution of task solutions follows two different patterns: for some tasks, the results apparently hit stagnation, whereas for others, they seem getting better over time. In this paper, (a) we compile the MIREX results of the last 6 years, (b) we propose a configurable quantitative index for evolution trend measurement of MIREX tasks, and (c) we discuss possible explanations or hypotheses for the stagnation phenomena hitting some of them. This paper hopes to incite a debate in the MIR research community about the progress in the field and how to adequately measure evolution trends.",BRA,education,Developing economies,"[-8.189954, 55.30582]","[25.99277, 30.68563]","[-34.502377, -3.4495468, -3.7136269]","[2.2153192, 8.603746, 15.0381365]","[13.650669, 4.7255855]","[11.721809, 0.8016927]","[15.047184, 11.149428, -1.5176462]","[12.280559, 4.6140428, 12.147018]"
38,Andrew Demetriou;Martha Larson;Cynthia C. S. Liem,Go with the Flow: When Listeners Use Music as Technology.,2016,https://doi.org/10.5281/zenodo.1415528,Andrew Demetriou+Delft University of Technology>NLD>education;Martha Larson+Radboud University>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education,"Music has been shown to have a profound effect on listeners’ internal states as evidenced by neuroscience research. Listeners report selecting and listening to music with specific intent, thereby using music as a tool to achieve desired psychological effects within a given context. In light of these observations, we argue that music information retrieval research must revisit the dominant assumption that listening to music is only an end unto itself. Instead, researchers should embrace the idea that music is also a technology used by listeners to achieve a specific desired internal state, given a particular set of circumstances and a desired goal. This paper focuses on listening to music in isolation (i.e., when the user listens to music by themselves with headphones) and surveys research from the fields of social psychology and neuroscience to build a case for a new line of research in music information retrieval on the ability of music to produce flow states in listeners. We argue that interdisciplinary collaboration is necessary in order to develop the understanding and techniques necessary to allow listeners to exploit the full potential of music as psychological technology.",NLD,education,Developed economies,"[-35.875217, 21.190676]","[33.0326, 34.56681]","[-23.907162, 16.617487, -7.366115]","[1.4383767, 14.354454, 14.899461]","[15.199272, 8.681206]","[12.382084, 1.0941818]","[15.139809, 15.077839, -1.5290371]","[12.741428, 4.471405, 11.501537]"
50,Dmitry Bogdanov;Alastair Porter;Perfecto Herrera;Xavier Serra,Cross-Collection Evaluation for Music Classification Tasks.,2016,https://doi.org/10.5281/zenodo.1418131,"Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown;Alastair Porter+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown|Music Technology Group, Universitat Pompeu Fabra>Unknown>Unknown","Many studies in music classification are concerned with obtaining the highest possible cross-validation result. However, some studies have noted that cross-validation may be prone to biases and that additional evaluations based on independent out-of-sample data are desirable. In this paper we present a methodology and software tools for cross-collection evaluation for music classification tasks. The tools allow users to conduct large-scale evaluations of classifier models trained within the AcousticBrainz platform, given an independent source of ground-truth annotations, and its mapping with the classes used for model training. To demonstrate the application of this methodology we evaluate five models trained on genre datasets commonly used by researchers for genre classification, and use collaborative tags from Last.fm as an independent source of ground truth. We study a number of evaluation strategies using our tools on validation sets from 240,000 to 1,740,000 music recordings and discuss the results.",ESP,education,Developed economies,"[-20.658957, -8.547357]","[33.02829, -2.6944287]","[-17.304323, -2.5414484, 9.532921]","[17.040741, 13.097631, -0.075010106]","[12.596825, 10.166238]","[10.489027, 3.756931]","[13.550874, 13.615287, 0.9095042]","[12.122683, 6.238518, 10.731613]"
24,Rafael Valle;Daniel J. Fremont;Ilge Akkaya;Alexandre Donzé;Adrian Freed;Sanjit A. Seshia,Learning and Visualizing Music Specifications Using Pattern Graphs.,2016,https://doi.org/10.5281/zenodo.1414954,Rafael Valle+UC Berkeley>USA>education;Daniel J. Fremont+UC Berkeley>USA>education;Ilge Akkaya+UC Berkeley>USA>education;Alexandre Donze+UC Berkeley>USA>education;Adrian Freed+UC Berkeley>USA>education;Sanjit S. Seshia+UC Berkeley>USA>education,"We describe a system to learn and visualize specifications from song(s) in symbolic and audio formats. The core of our approach is based on a software engineering procedure called specification mining. Our procedure extracts patterns from feature vectors and uses them to build pattern graphs. The feature vectors are created by segmenting song(s) and extracting time and frequency domain features from them, such as chromagrams, chord degree and interval classification. The pattern graphs built on these feature vectors provide the likelihood of a pattern between nodes, as well as start and ending nodes. The pattern graphs learned from a song(s) describe formal specifications that can be used for human interpretable quantitatively and qualitatively song comparison or to perform supervisory control in machine improvisation. We offer results in song summarization, song and style validation and machine improvisation with formal specifications.",USA,education,Developed economies,"[10.840399, 27.041424]","[-4.027213, 32.09885]","[-11.071888, -10.195793, 12.010239]","[-13.44879, -4.1290755, 11.527471]","[13.163452, 6.773855]","[9.661024, 1.0930766]","[13.677908, 12.83395, -1.7484106]","[10.333885, 5.6697474, 11.665025]"
22,Hendrik Vincent Koops;W. Bas de Haas;Dimitrios Bountouridis;Anja Volk,Integration and Quality Assessment of Heterogeneous Chord Sequences Using Data Fusion.,2016,https://doi.org/10.5281/zenodo.1415954,Hendrik Vincent Koops+Utrecht University>NLD>education;W. Bas de Haas+Chordify>NLD>company;Dimitrios Bountouridis+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education,"Two heads are better than one, and the many are smarter than the few. Integrating knowledge from multiple sources has shown to increase retrieval and classification accuracy in many domains. The recent explosion of crowdsourced information, such as on websites hosting chords and tabs for popular songs, calls for sophisticated algorithms for data-driven quality assessment and data integration to create better, and more reliable data. In this paper, we propose to integrate the heterogeneous output of multiple automatic chord extraction algorithms using data fusion. First we show that data fusion creates significantly better chord label sequences from multiple sources, outperforming its source material, majority voting and random source integration. Second, we show that data fusion is capable of assessing the quality of sources with high precision from source agreement, without any ground-truth knowledge. Our study contributes to a growing body of work showing the benefits of integrating knowledge from multiple sources in an advanced way.",NLD,education,Developed economies,"[52.195873, -3.8094165]","[32.018124, -4.3622656]","[23.492521, -14.253323, 12.3734455]","[16.595459, 14.618228, 2.0933685]","[6.9827867, 8.710549]","[10.623528, 3.4015298]","[11.957058, 10.491188, 1.9733588]","[12.145711, 6.4512753, 11.075324]"
111,Jun-qi Deng;Yu-Kwong Kwok,A Hybrid Gaussian-HMM-Deep Learning Approach for Automatic Chord Estimation with Very Large Vocabulary.,2016,https://doi.org/10.5281/zenodo.1415718,Junqi Deng+The University of Hong Kong>HKG>education;Yu-Kwong Kwok+The University of Hong Kong>HKG>education,"We propose a hybrid Gaussian-HMM-Deep-Learning approach for automatic chord estimation with very large chord vocabulary. The Gaussian-HMM part is similar to Chordino, which is used as a segmentation engine to divide input audio into note spectrogram segments. Two types of deep learning models are proposed to classify these segments into chord labels, which are then connected as chord sequences. Two sets of evaluations are conducted with two large chord vocabularies. The first evaluation is conducted in a recent MIREX standard way. Results show that our approach has obvious advantage over the state-of-the-art large-vocabulary-with-inversions supportable ACE system in terms of large vocabularies, although is outperformed by in small vocabularies. Through analyzing and deducing system behaviors behind the results, we see interesting chord confusion patterns made by different systems, which conceivably point to a demand of more balanced and consistent annotated datasets for training and testing. The second evaluation preliminarily demonstrates our approach’s superiority on a jazz chord vocabulary with 36 chord types, compared with a Chordino-like Gaussian-HMM baseline system with augmented vocabulary capacity.",HKG,education,Developing economies,"[57.45316, -5.650917]","[-33.801964, 22.13936]","[26.429605, -8.751522, 19.285133]","[-26.961927, -1.8885134, 1.1495332]","[6.5859957, 8.665936]","[5.8981905, 3.839947]","[11.903092, 10.320245, 2.1404796]","[9.694821, 9.085932, 12.32178]"
110,Keunwoo Choi;György Fazekas;Mark B. Sandler,Automatic Tagging Using Deep Convolutional Neural Networks.,2016,https://doi.org/10.5281/zenodo.1416254,Keunwoo Choi+Queen Mary University of London>GBR>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;György Fazekas+Queen Mary University of London>GBR>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Mark Sandler+Queen Mary University of London>GBR>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.",GBR,education,Developed economies,"[-44.28079, -3.8921673]","[-22.479504, -38.915924]","[-12.838244, 19.344843, 12.731421]","[-6.8993716, 0.4293385, -21.872309]","[14.513804, 10.622793]","[9.670124, 4.798677]","[15.700741, 14.020722, 0.19119036]","[10.837303, 6.5040417, 8.846362]"
11,Yen-Cheng Lu;Chih-Wei Wu;Alexander Lerch;Chang-Tien Lu,Automatic Outlier Detection in Music Genre Datasets.,2016,https://doi.org/10.5281/zenodo.1418193,Yen-Cheng Lu+Virginia Tech>USA>education;Chih-Wei Wu+Georgia Institute of Technology>USA>education;Chang-Tien Lu+Virginia Tech>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"Outlier detection, also known as anomaly detection, is an important topic that has been studied for decades. An outlier detection system is able to identify anomalies in a dataset and thus improve data integrity by removing the detected outliers. It has been successfully applied to different types of data in various fields such as cyber-security, finance, and transportation. In the field of Music Information Retrieval (MIR), however, the number of related studies is small. In this paper, we introduce different state-of-the-art outlier detection techniques and evaluate their viability in the context of music datasets. More specifically, we present a comparative study of 6 outlier detection algorithms applied to a Music Genre Recognition (MGR) dataset. It is determined how well algorithms can identify mislabeled or corrupted files, and how much the quality of the dataset can be improved. Results indicate that state-of-the-art anomaly detection systems have problems identifying anomalies in MGR datasets reliably.",USA,education,Developed economies,"[-33.041534, -11.939888]","[25.211102, -0.07859793]","[-20.382717, 8.156465, 16.56227]","[12.840134, 4.9582195, 2.6079612]","[13.106572, 10.758765]","[10.595316, 3.066272]","[14.037857, 14.28542, 1.3295861]","[12.048886, 6.573982, 11.485476]"
12,Luwei Yang;Khalid Z. Rajab;Elaine Chew,AVA: An Interactive System for Visual and Quantitative Analyses of Vibrato and Portamento Performance Styles.,2016,https://doi.org/10.5281/zenodo.1415592,"Luwei Yang+Centre for Digital Music, Queen Mary University of London>GBR>education;Khalid Z. Rajab+Antennas & Electromagnetics Group, Queen Mary University of London>GBR>education;Elaine Chew+Centre for Digital Music, Queen Mary University of London>GBR>education","Vibratos and portamenti are important expressive features for characterizing performance style on instruments capable of continuous pitch variation such as strings and voice. Accurate study of these features is impeded by time consuming manual annotations. We present AVA, an interactive tool for automated detection, analysis, and visualization of vibratos and portamenti. The system implements a Filter Diagonalization Method (FDM)-based and a Hidden Markov Model-based method for vibrato and portamento detection. Vibrato parameters are reported directly from the FDM, and portamento parameters are given by the best fit Logistic Model. The graphical user interface (GUI) allows the user to edit the detection results, to view each vibrato or portamento, and to read the output parameters. The entire set of results can also be written to a text file for further statistical analysis. Applications of AVA include music summarization, similarity assessment, music learning, and musicological analysis. We demonstrate AVA’s utility by using it to analyze vibratos and portamenti in solo performances of two Beijing opera roles and two string instruments, erhu and violin.",GBR,education,Developed economies,"[-11.279098, 35.835518]","[3.361176, 29.922192]","[-7.479228, -12.240831, -15.070855]","[-8.896877, -6.275282, 11.277286]","[13.05589, 6.8442636]","[9.034853, 1.4994514]","[13.532611, 13.223061, -1.964924]","[9.447677, 6.819701, 11.285207]"
10,Gabriel Vigliensoni;Ichiro Fujinaga,"Automatic Music Recommendation Systems: Do Demographic, Profiling, and Contextual Features Improve Their Performance?.",2016,https://doi.org/10.5281/zenodo.1417073,Gabriel Vigliensoni+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Traditional automatic music recommendation systems’ performance typically rely on the accuracy of statistical models learned from past preferences of users on music items. However, additional sources of data such as demographic attributes of listeners, their listening behaviour, and their listening contexts encode information about listeners, and their listening habits, that may be used to improve the accuracy of music recommendation models. In this paper we introduce a large dataset of music listening histories with listeners’ demographic information, and a set of features to characterize aspects of people’s listening behaviour. The longevity of the collected listening histories, covering over two years, allows the retrieval of basic forms of listening context. We use this dataset in the evaluation of accuracy of a music artist recommendation model learned from past preferences of listeners on music items and their interaction with several combinations of people’s demographic, profiling, and contextual features. Our results indicate that using listeners’ self-declared age, country, and gender improve the recommendation accuracy by 8 percent. When a new profiling feature termed exploratoryness was added, the accuracy of the model increased by 12 percent.",CAN,education,Developed economies,"[-45.32476, 25.432741]","[40.576847, 17.970728]","[-10.48621, 24.571844, -10.282866]","[15.148114, 9.172736, 16.633862]","[15.840709, 9.168639]","[12.568697, 1.7998763]","[15.780393, 15.608213, -1.4193472]","[13.4507475, 5.104432, 12.565617]"
9,Jiajie Dai;Simon Dixon,Analysis of Vocal Imitations of Pitch Trajectories.,2016,https://doi.org/10.5281/zenodo.1416318,Jiajie Dai+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"In this paper, we analyse the pitch trajectories of vocal imi- tations by non-poor singers. A group of 43 selected singers was asked to vocally imitate a set of stimuli. Five stimu- lus types were used: a constant pitch (stable), a constant pitch preceded by a pitch glide (head), a constant pitch fol- lowed by a pitch glide (tail), a pitch ramp and a pitch with vibrato; with parameters for main pitch, transient length and pitch difference. Two conditions were tested: singing simultaneously with the stimulus, and singing alternately, between repetitions of the stimulus. After automatic pitch- tracking and manual checking of the data, we calculated intonation accuracy and precision, and modelled the note trajectories according to the stimulus types. We modelled pitch error with a linear mixed-effects model, and tested factors for signiﬁcant effects using one-way analysis of variance. The results indicate: (1) Signiﬁcant factors in- clude stimulus type, main pitch, repetition, condition and musical training background, while order of stimuli, gen- der and age do not have any signiﬁcant effect. (2) The ramp, vibrato and tail stimuli have signiﬁcantly greater ab- solute pitch errors than the stable and head stimuli. (3) Pitch error shows a small but signiﬁcant linear trend with pitch difference. (4) Notes with shorter transient duration are more accurate.",GBR,education,Developed economies,"[-2.6831532, -29.08503]","[-3.0345252, -18.51073]","[17.369495, 4.4223323, -15.1242695]","[7.9213886, -17.012157, -13.97986]","[10.113239, 10.702836]","[7.0148005, 2.0077763]","[11.168508, 15.160025, 0.32375142]","[8.85915, 7.663551, 11.589706]"
8,Daniel Stoller;Simon Dixon,Analysis and Classification of Phonation Modes In Singing.,2016,https://doi.org/10.5281/zenodo.1416772,Daniel Stoller+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Phonation mode is an expressive aspect of the singing voice and can be described using the four categories neutral, breathy, pressed and flow. Previous attempts at automatically classifying the phonation mode on a dataset containing vowels sung by a female professional have been lacking in accuracy or have not sufficiently investigated the characteristic features of the different phonation modes which enable successful classification. In this paper, we extract a large range of features from this dataset, including specialised descriptors of pressedness and breathiness, to analyse their explanatory power and robustness against changes of pitch and vowel. We train and optimise a feed-forward neural network (NN) with one hidden layer on all features using cross validation to achieve a mean F-measure above 0.85 and an improved performance compared to previous work. Applying feature selection based on mutual information and retaining the nine highest ranked features as input to a NN results in a mean F-measure of 0.78, demonstrating the suitability of these features to discriminate between phonation modes. Training and pruning a decision tree yields a simple rule set based only on cepstral peak prominence (CPP), temporal flatness and average energy that correctly categorises 78% of the recordings.",GBR,education,Developed economies,"[-8.937265, -41.72893]","[-32.832676, -42.680313]","[19.986334, 6.025298, -18.409214]","[2.2738564, -11.541941, -19.264664]","[9.850628, 10.865804]","[7.705154, 4.8159223]","[11.101181, 15.151129, 0.5544529]","[10.215418, 7.4828744, 8.97116]"
7,Alo Allik;György Fazekas;Mark B. Sandler,An Ontology for Audio Features.,2016,https://doi.org/10.5281/zenodo.1416226,Alo Allik+Queen Mary University of London>GBR>education;György Fazekas+Queen Mary University of London>GBR>education;Mark Sandler+Queen Mary University of London>GBR>education,"A plurality of audio feature extraction toolsets and feature datasets are used by the MIR community. Their different conceptual organisation of features and output formats however present difficulties in exchanging or comparing data, while very limited means are provided to link features with content and provenance. These issues are hindering research reproducibility and the use of multiple tools in combination. We propose novel Semantic Web ontologies (1) to provide a common structure for feature data formats and (2) to represent computational workflows of audio features facilitating their comparison. The Audio Feature Ontology provides a descriptive framework for expressing different conceptualisations of and designing linked data formats for content-based audio features. To accommodate different views in organising features, the ontology does not impose a strict hierarchical structure, leaving this open to task and tool specific ontologies that derive from a common vocabulary. The ontologies are based on the analysis of existing feature extraction tools and the MIR literature, which was instrumental in guiding the design process. They are harmonised into a library of modular interlinked ontologies that describe the different entities and activities involved in music creation, production and consumption.",GBR,education,Developed economies,"[-26.828114, 39.51885]","[18.096636, 39.658195]","[-22.872585, -3.5739274, -4.04709]","[-4.9713316, -3.9131727, 25.476007]","[14.213761, 8.96877]","[10.818915, -0.13227522]","[15.018219, 13.666934, -1.2372024]","[12.086452, 5.088361, 11.671056]"
6,Cárthach Ó Nuanáin;Perfecto Herrera;Sergi Jordà,An Evaluation Framework and Case Study for Rhythmic Concatenative Synthesis.,2016,https://doi.org/10.5281/zenodo.1415648,C´arthach ´O Nuan´ain+Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Universitat Pompeu Fabra>ESP>education;Sergi Jord`a+Universitat Pompeu Fabra>ESP>education,In this paper we present and report on a methodology for evaluating a creative MIR-based application of concatenative synthesis. After reviewing many existing applications of concatenative synthesis we have developed an application that specifically addresses loop-based rhythmic pattern generation. We describe how such a system could be evaluated with respect to its objective retrieval performance and subjective responses of humans in a listener survey. Applying this evaluation strategy produced positive findings to help verify and validate the objectives of our system. We discuss the results of the evaluation and draw conclusions by contrasting the objective analysis with the subjective impressions of the users.,ESP,education,Developed economies,"[43.901215, 5.104404]","[-4.367778, 36.39725]","[-8.455043, -24.304502, 7.4691277]","[-15.169476, 2.3918738, 19.612625]","[11.822145, 5.7803807]","[9.818325, 1.1835274]","[11.506699, 13.942442, -1.8240579]","[10.199178, 5.5468507, 10.888554]"
5,Andrew John Lambert;Tillman Weyde;Newton Armstrong,Adaptive Frequency Neural Networks for Dynamic Pulse and Metre Perception.,2016,https://doi.org/10.5281/zenodo.1418305,Andrew J. Lambert+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Newton Armstrong+City University London>GBR>education,"Beat induction, the means by which humans listen to music and perceive a steady pulse, is achieved via a perceptual and cognitive process. Computationally modelling this phenomenon is an open problem, especially when processing expressive shaping of the music such as tempo change. To meet this challenge we propose Adaptive Frequency Neural Networks (AFNNs), an extension of Gradient Frequency Neural Networks (GFNNs). GFNNs are based on neurodynamic models and have been applied successfully to a range of difficult music perception problems including those with syncopated and polyrhythmic stimuli. AFNNs extend GFNNs by applying a Hebbian learning rule to the oscillator frequencies. Thus the frequencies in an AFNN adapt to the stimulus through an attraction to local areas of resonance, and allow for a great dimensionality reduction in the network. Where previous work with GFNNs has focused on frequency and amplitude responses, we also consider phase information as critical for pulse perception. Evaluating the time-based output, we find significantly improved responses of AFNNs compared to GFNNs to stimuli with both steady and varying pulse frequencies. This leads us to believe that AFNNs could replace the linear filtering methods commonly used in beat tracking and tempo estimation systems, and lead to more accurate methods.",GBR,education,Developed economies,"[43.911053, -31.812235]","[-30.927725, -12.887014]","[5.521483, -32.038883, 1.1403004]","[-6.9025564, 16.032356, -18.06362]","[11.103796, 4.387037]","[4.9468174, 2.4647849]","[10.550084, 13.2892885, -2.5323815]","[7.6262994, 6.9791985, 10.303827]"
4,Pierre Beauguitte;Bryan Duggan;John D. Kelleher,A Corpus of Annotated Irish Traditional Dance Music Recordings: Design and Benchmark Evaluations.,2016,https://doi.org/10.5281/zenodo.1417333,Pierre Beauguitte+Dublin Institute of Technology>IRL>education;Bryan Duggan+Dublin Institute of Technology>IRL>education;John Kelleher+Dublin Institute of Technology>IRL>education,"An emerging trend in music information retrieval (MIR) is the use of supervised machine learning to train automatic music transcription models. A prerequisite of adopting a machine learning methodology is the availability of annotated corpora. However, different genres of music have different characteristics and modelling these characteristics is an important part of creating state of the art MIR systems. Consequently, although some music corpora are available the use of these corpora is tied to the specific music genre, instrument type and recording context the corpus covers. This paper introduces the first corpus of annotations of audio recordings of Irish traditional dance music that covers multiple instrument types and both solo studio and live session recordings. We first discuss the considerations that motivated our design choices in developing the corpus. We then benchmark a number of automatic music transcription algorithms against the corpus.",IRL,education,Developed economies,"[-24.58822, -23.831894]","[-4.702195, 12.295483]","[-10.016306, -22.620852, -6.2415433]","[-4.872869, 5.677229, 6.251747]","[11.682869, 5.2080283]","[9.062528, 2.446187]","[11.729414, 13.780173, -2.2675834]","[10.398906, 6.344088, 11.283062]"
23,Reinhard Sonnleitner;Andreas Arzt;Gerhard Widmer,Landmark-Based Audio Fingerprinting for DJ Mix Monitoring.,2016,https://doi.org/10.5281/zenodo.1417008,Reinhard Sonnleitner+Johannes Kepler University>AUT>education;Andreas Arzt+Johannes Kepler University>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"Recently, the media monitoring industry shows increased interest in applying automated audio identification systems for revenue distribution of DJ performances played in discotheques. DJ mixes incorporate a wide variety of signal modifications, e.g. pitch shifting, tempo modifications, cross-fading and beat-matching. These signal modifications are expected to be more severe than what is usually encountered in the monitoring of radio and TV broadcasts. The monitoring of DJ mixes presents a hard challenge for automated music identification systems, which need to be robust to various signal modifications while maintaining a high level of specificity to avoid false revenue assignment. In this work we assess the fitness of three landmark-based audio fingerprinting systems with different properties on real-world data – DJ mixes that were performed in discotheques. To enable the research community to evaluate systems on DJ mixes, we also create and publish a freely available, creative-commons licensed dataset of DJ mixes along with their reference tracks and song-border annotations. Experiments on these datasets reveal that a recent quad-based method achieves considerably higher performance on this task than the other methods.",AUT,education,Developed economies,"[-16.73067, -27.500181]","[24.187769, -20.477098]","[7.7069697, -14.245005, -24.983301]","[17.71858, -11.785985, 2.4236782]","[9.202832, 4.494248]","[8.368986, 0.07181446]","[10.610742, 11.675694, -1.9472445]","[10.064063, 5.389948, 12.778879]"
3,Jan Schlüter,Learning to Pinpoint Singing Voice from Weakly Labeled Examples.,2016,https://doi.org/10.5281/zenodo.1417651,Jan Schlüter+Austrian Research Institute for Artificial Intelligence>AUT>facility,"Building an instrument detector usually requires temporally accurate ground truth that is expensive to create. However, song-wise information on the presence of instruments is often easily available. In this work, we investigate how well we can train a singing voice detection system merely from song-wise annotations of vocal presence. Using convolutional neural networks, multiple-instance learning and saliency maps, we can not only detect singing voice in a test signal with a temporal accuracy close to the state-of-the-art, but also localize the spectral bins with precision and recall close to a recent source separation method. Our recipe may provide a basis for other sequence labeling tasks, for improving source separation or for inspecting neural networks trained on auditory spectrograms.",AUT,facility,Developed economies,"[-5.200485, -39.61922]","[-36.380955, -37.24117]","[21.970602, 15.291643, -7.7126093]","[-2.5392442, -9.237937, -21.580765]","[9.958857, 11.048049]","[6.9868875, 5.3189898]","[11.385717, 15.122909, 0.96204776]","[9.902094, 8.090577, 9.249643]"
1,Sebastian Ewert;Siying Wang;Meinard Müller;Mark B. Sandler,Score-Informed Identification of Missing and Extra Notes in Piano Recordings.,2016,https://doi.org/10.5281/zenodo.1418317,"Sebastian Ewert+Centre for Digital Music (C4DM), Queen Mary University of London>GBR>education;Siying Wang+Centre for Digital Music (C4DM), Queen Mary University of London>GBR>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Mark Sandler+Centre for Digital Music (C4DM), Queen Mary University of London>GBR>education","A main goal in music tuition is to enable a student to play a score without mistakes, where common mistakes include missing notes or playing additional extra ones. To automatically detect these mistakes, a first idea is to use a music transcription method to detect notes played in an audio recording and to compare the results with a corresponding score. However, as the number of transcription errors produced by standard methods is often considerably higher than the number of actual mistakes, the results are often of limited use. In contrast, our method exploits that the score already provides rough information about what we seek to detect in the audio, which allows us to construct a tailored transcription method. In particular, we employ score-informed source separation techniques to learn for each score pitch a set of templates capturing the spectral properties of that pitch. After extrapolating the resulting template dictionary to pitches not in the score, we estimate the activity of each MIDI pitch over time. Finally, making again use of the score, we choose for each pitch an individualized threshold to differentiate note onsets from spurious activity in an optimized way. We indicate the accuracy of our approach on a dataset of piano pieces commonly used in education.",GBR,education,Developed economies,"[29.74756, 1.5685225]","[-11.597788, -12.488574]","[7.9742966, -17.583506, 3.3072026]","[-5.2911267, -14.547407, -6.445664]","[9.992107, 6.925109]","[7.018466, 2.8157783]","[11.895617, 12.050487, -0.6526002]","[9.120564, 6.8193974, 10.581703]"
0,Diego Furtado Silva;Chin-Chia Michael Yeh;Gustavo E. A. P. A. Batista;Eamonn J. Keogh,SiMPle: Assessing Music Similarity Using Subsequences Joins.,2016,https://doi.org/10.5281/zenodo.1415012,"Diego F. Silva+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Chin-Chia M. Yeh+University of California, Riverside>USA>education;Gustavo E. A. P. A. Batista+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Eamonn Keogh+University of California, Riverside>USA>education","Most algorithms for music information retrieval are based on the analysis of the similarity between feature sets extracted from the raw audio. A common approach to assessing similarities within or between recordings is by creating similarity matrices. However, this approach requires quadratic space for each comparison and typically requires a costly post-processing of the matrix. In this work, we propose a simple and efficient representation based on a subsequence similarity join, which may be used in several music information retrieval tasks. We apply our method to the cover song recognition problem and demonstrate that it is superior to state-of-the-art algorithms. In addition, we demonstrate how the proposed representation can be exploited for multiple applications in music processing.",BRA,education,Developing economies,"[-7.429609, 15.002302]","[21.286953, 11.566577]","[-5.8228364, 10.955581, -2.1680167]","[15.125847, -5.0147443, 4.400644]","[13.303198, 9.248756]","[10.193032, 2.118601]","[13.784918, 15.15024, -0.8366787]","[11.72147, 6.4617662, 12.35004]"
13,Gissel Velarde;Tillman Weyde;Carlos Eduardo Cancino Chacón;David Meredith 0001;Maarten Grachten,Composer Recognition Based on 2D-Filtered Piano-Rolls.,2016,https://doi.org/10.5281/zenodo.1417641,Gissel Velarde+Aalborg University>DNK>education;Tillman Weyde+City University London>GBR>education;Carlos Cancino Chacón+Austrian Research Institute for Artificial Intelligence>AUT>facility;David Meredith+Aalborg University>DNK>education;Maarten Grachten+Austrian Research Institute for Artificial Intelligence>AUT>facility,"We propose a method for music classification based on the use of convolutional models on symbolic pitch–time representations (i.e. piano-rolls) which we apply to composer recognition. An excerpt of a piece to be classified is first sampled to a 2D pitch–time representation which is then subjected to various transformations, including convolution with predefined filters (Morlet or Gaussian) and classified by means of support vector machines. We combine classifiers based on different pitch representations (MIDI and morphetic pitch) and different filter types and configurations. The method does not require parsing of the music into separate voices, or extraction of any other predefined features prior to processing; instead it is based on the analysis of texture in a 2D pitch–time representation. We show that filtering significantly improves recognition and that the method proves robust to encoding, transposition and amount of information. On discriminating between Haydn and Mozart string quartet movements, our best classifier reaches state-of-the-art performance in leave-one-out cross validation.",DNK,education,Developed economies,"[34.750134, 7.2102423]","[-10.042958, -2.0850513]","[20.203934, 5.4302382, 19.635061]","[6.95839, 2.0314147, -9.529323]","[10.26443, 6.9223366]","[8.791769, 3.5108793]","[12.1373625, 11.840778, -0.92212564]","[10.408586, 7.2882857, 10.483472]"
14,Kristina Andersen;Peter Knees,Conversations with Expert Users in Music Retrieval and Research Challenges for Creative MIR.,2016,https://doi.org/10.5281/zenodo.1418323,Kristina Andersen+Studio for Electro Instrumental Music (STEIM)>NLD>facility;Peter Knees+Johannes Kepler University Linz>AUT>education,"Sample retrieval remains a central problem in the creative process of making electronic dance music. This paper describes the findings from a series of interview sessions involving users working creatively with electronic music. We conducted in-depth interviews with expert users on location at the Red Bull Music Academies in 2014 and 2015. When asked about their wishes and expectations for future technological developments in interfaces, most participants mentioned very practical requirements of storing and retrieving files. A central aspect of the desired systems is the need to provide increased flow and unbroken periods of concentration and creativity. From the interviews, it becomes clear that for Creative MIR, and in particular, for music interfaces for creative expression, traditional requirements and paradigms for music and audio retrieval differ to those from consumer-centered MIR tasks such as playlist generation and recommendation and that new paradigms need to be considered. Despite all technical aspects being controllable by the experts themselves, searching for sounds to use in composition remains a largely semantic process. From the outcomes of the interviews, we outline a series of possible conclusions and areas and pose two research challenges for future developments of sample retrieval interfaces in the creative domain.",NLD,facility,Developed economies,"[-12.641463, 52.031765]","[30.992933, 36.148556]","[-31.480164, -0.46247476, 1.066418]","[0.3286949, 11.005092, 18.639843]","[13.366827, 5.4352984]","[11.747883, 0.5332866]","[14.6601, 11.797125, -1.3842041]","[12.105665, 4.4173665, 11.751752]"
15,Florian Krebs;Sebastian Böck;Matthias Dorfer;Gerhard Widmer,Downbeat Tracking Using Beat Synchronous Features with Recurrent Neural Networks.,2016,https://doi.org/10.5281/zenodo.1417819,Florian Krebs+Johannes Kepler University Linz>AUT>education;Sebastian Böck+Johannes Kepler University Linz>AUT>education;Matthias Dorfer+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,"In this paper, we propose a system that extracts the downbeat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results.",AUT,education,Developed economies,"[36.484695, -35.572712]","[-28.285984, -11.896582]","[10.82841, -29.427252, -4.2705026]","[-6.8156247, 17.737106, -14.717258]","[10.436331, 4.147415]","[5.102007, 2.3691375]","[10.083033, 12.811018, -2.2985585]","[7.6936417, 6.969108, 10.442228]"
16,Julien Osmalskyj;Marc Van Droogenbroeck;Jean-Jacques Embrechts,Enhancing Cover Song Identification with Hierarchical Rank Aggregation.,2016,https://doi.org/10.5281/zenodo.1418109,Julien Osmalskyj+INTELSIG Laboratory - University of Liège>BEL>education;Marc Van Droogenbroeck+INTELSIG Laboratory - University of Liège>BEL>education;Jean-Jaques Embrechts+INTELSIG Laboratory - University of Liège>BEL>education,"Cover song identification involves calculating pairwise similarities between a query audio track and a database of reference tracks. While most authors make exclusively use of chroma features, recent work tends to demonstrate that combining similarity estimators based on multiple audio features increases the performance. We improve this approach by using a hierarchical rank aggregation method for combining estimators based on different features. More precisely, we first aggregate estimators based on global features such as the tempo, the duration, the overall loudness, the number of beats, and the average chroma vector. Then, we aggregate the resulting composite estimator with four popular state-of-the-art methods based on chromas as well as timbre sequences. We further introduce a refinement step for the rank aggregation called “local Kemenization” and quantify its benefit for cover song identification. The performance of our method is evaluated on the Second Hand Song dataset. Our experiments show a significant improvement of the performance, up to an increase of more than 200 % of the number of queries identified in the Top-1, compared to previous results.",BEL,education,Developed economies,"[7.5860605, 43.851334]","[25.200903, -10.88662]","[2.086096, 12.126967, -22.776407]","[16.440866, -1.3993907, -0.5230455]","[16.085123, 11.106455]","[10.158765, 2.8683634]","[12.851513, 17.333912, -0.38427204]","[11.735584, 6.8559155, 11.554671]"
17,Tim Tse;Justin Salamon;Alex C. Williams;Helga Jiang;Edith Law,Ensemble: A Hybrid Human-Machine System for Generating Melody Scores from Audio.,2016,https://doi.org/10.5281/zenodo.1416708,Tim Tse+University of Waterloo>CAN>education;Justin Salamon+New York University>USA>education;Alex Williams+University of Waterloo>CAN>education;Helga Jiang+University of Waterloo>CAN>education;Edith Law+University of Waterloo>CAN>education,"Music transcription is a highly complex task that is difficult for automated algorithms, and equally challenging to people, even those with many years of musical training. Furthermore, there is a shortage of high-quality datasets for training automated transcription algorithms. In this research, we explore a semi-automated, crowdsourced approach to generate music transcriptions, by first running an automatic melody transcription algorithm on a (polyphonic) song to produce a series of discrete notes representing the melody, and then soliciting the crowd to correct this melody. We present a novel web-based interface that enables the crowd to correct transcriptions, report results from an experiment to understand the capabilities of non-experts to perform this challenging task, and characterize the characteristics and actions of workers and how they correlate with transcription performance.",CAN,education,Developed economies,"[10.446394, -9.771223]","[-11.923465, 41.507835]","[14.841219, 8.0710945, 4.3546925]","[4.6603665, 24.033653, -8.342941]","[10.222848, 9.633044]","[9.003498, 4.3901696]","[11.2508135, 14.649717, -0.73099726]","[10.228499, 6.0621037, 10.187666]"
18,Sergio Oramas;Luis Espinosa Anke;Aonghus Lawlor;Xavier Serra;Horacio Saggion,Exploring Customer Reviews for Music Genre Classification and Evolutionary Studies.,2016,https://doi.org/10.5281/zenodo.1415544,"Sergio Oramas+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Luis Espinosa-Anke+Universitat Pompeu Fabra>ESP>education;Aonghus Lawlor+Insight Centre for Data Analytics, University College of Dublin>IRL>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Horacio Saggion+Universitat Pompeu Fabra>ESP>education","In this paper, we explore a large multimodal dataset of about 65k albums constructed from a combination of Amazon customer reviews, MusicBrainz metadata and AcousticBrainz audio descriptors. Review texts are further enriched with named entity disambiguation along with polarity information derived from an aspect-based sentiment analysis framework. This dataset constitutes the cornerstone of two main contributions: First, we perform experiments on music genre classification, exploring a variety of feature types, including semantic, sentimental and acoustic features. These experiments show that modeling semantic information contributes to outperforming strong bag-of-words baselines. Second, we provide a diachronic study of the criticism of music genres via a quantitative analysis of the polarity associated to musical aspects over time. Our analysis hints at a potential correlation between key cultural and geopolitical events and the language and evolving sentiments found in music reviews.",ESP,education,Developed economies,"[-35.561443, -4.666902]","[48.894062, -1.3112242]","[-25.265919, 1.937014, 13.691543]","[15.678533, 17.704836, 5.759003]","[13.582589, 10.484696]","[12.9200325, 3.666632]","[14.242957, 14.3752, 1.0708289]","[14.037958, 5.3427043, 10.911847]"
19,Jan Hajic Jr.;Jiri Novotný;Pavel Pecina;Jaroslav Pokorný,Further Steps Towards a Standard Testbed for Optical Music Recognition.,2016,https://doi.org/10.5281/zenodo.1418161,Jan Hajič jr.+Charles University>CZE>education;Jiří Novotný+Charles University>CZE>education;Pavel Pecina+Charles University>CZE>education;Jaroslav Pokorný+Charles University>CZE>education,"Evaluating Optical Music Recognition (OMR) is notoriously difficult and automated end-to-end OMR evaluation metrics are not available to guide development. In “Towards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images”, Byrd and Simonsen recently stress that a benchmarking standard is needed in the OMR community, both with regards to data and evaluation metrics. We build on their analysis and definitions and present a prototype of an OMR benchmark. We do not, however, presume to present a complete solution to the complex problem of OMR benchmarking. Our contributions are: (a) an attempt to define a multi-level OMR benchmark dataset and a practical prototype implementation for both printed and handwritten scores, (b) a corpus-based methodology for assessing automated evaluation metrics, and an underlying corpus of over 1000 qualified relative cost-to-correct judgments. We then assess several straightforward automated MusicXML evaluation metrics against this corpus to establish a baseline over which further metrics can improve.",CZE,education,Developed economies,"[38.323063, 20.853516]","[-20.311567, 36.77096]","[19.877573, 11.142024, 11.389577]","[-9.357321, -19.905094, 1.8489511]","[8.654422, 6.1930223]","[6.61366, -0.5596509]","[10.670797, 11.119359, -0.10171076]","[8.005638, 4.2830935, 10.613207]"
20,Nicolas Guiomard-Kagan;Mathieu Giraud;Richard Groult;Florence Levé,Improving Voice Separation by Better Connecting Contigs.,2016,https://doi.org/10.5281/zenodo.1417825,Nicolas Guiomard-Kagan+University of Picardie Jules Verne>FRA>education|University of Lille>FRA>education;Mathieu Giraud+University of Picardie Jules Verne>FRA>education|University of Lille>FRA>education;Richard Groult+University of Picardie Jules Verne>FRA>education|University of Lille>FRA>education;Florence Levé+University of Picardie Jules Verne>FRA>education|University of Lille>FRA>education,"Separating a polyphonic symbolic score into monophonic voices or streams helps to understand the music and may simplify further pattern matching. One of the best ways to compute this separation, as proposed by Chew and Wu in 2005, is to first identify contigs that are portions of the music score with a constant number of voices, then to progressively connect these contigs. This raises two questions: Which contigs should be connected first? And, how should these two contigs be connected? Here we propose to answer simultaneously these two questions by considering a set of musical features that measures the quality of any connection. The coefficients weighting the features are optimized through a genetic algorithm. We benchmark the resulting connection policy on corpora containing fugues of the Well-Tempered Clavier by J. S. Bach as well as on string quartets, and we compare it against previously proposed policies. The contig connection is improved, particularly when one takes into account the whole content of voice fragments to assess the quality of their possible connection.",FRA,education,Developed economies,"[-0.6395238, -45.73016]","[-7.8280535, 5.983476]","[28.759241, 7.8503513, -2.783761]","[-1.6922747, -17.9281, 8.688606]","[8.868777, 10.596595]","[8.247946, 2.5122645]","[10.881477, 14.386437, 1.4800016]","[10.228677, 7.0976944, 11.488814]"
21,Tsubasa Tanaka;Brian Bemman;David Meredith 0001,Integer Programming Formulation of the Problem of Generating Milton Babbitt's All-Partition Arrays.,2016,https://doi.org/10.5281/zenodo.1416164,"Tsubasa Tanaka+IRCAM, CNRS, UPMC>FRA>education;Brian Bemman+Aalborg University>DNK>education;David Meredith+Aalborg University>DNK>education","Milton Babbitt (1916–2011) was a composer of twelve-tone serial music noted for creating the all-partition array. The problem of generating an all-partition array involves finding a rectangular array of pitch-class integers that can be partitioned into regions, each of which represents a distinct integer partition of 12. Integer programming (IP) has proven to be effective for solving such combinatorial problems, however, it has never before been applied to the problem addressed in this paper. We introduce a new way of viewing this problem as one in which restricted overlaps between integer partition regions are allowed. This permits us to describe the problem using a set of linear constraints necessary for IP. In particular, we show that this problem can be defined as a special case of the well-known problem of set-covering (SCP), modified with additional constraints. Due to the difficulty of the problem, we have yet to discover a solution. However, we assess the potential practicality of our method by running it on smaller similar problems.",FRA,education,Developed economies,"[25.93366, 37.567104]","[21.718348, -33.76262]","[-4.8974843, -10.898642, -28.637262]","[21.02644, -18.190765, -1.5785154]","[11.116301, 8.204462]","[8.819512, 1.1786585]","[12.8013315, 12.766189, -0.045112837]","[10.419487, 6.6190395, 12.931367]"
2,Filip Korzeniowski;Gerhard Widmer,Feature Learning for Chord Recognition: The Deep Chroma Extractor.,2016,https://doi.org/10.5281/zenodo.1416314,Filip Korzeniowski+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,"We explore frame-level audio feature learning for chord recognition using artificial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artificial neural networks. It is trained to compute chroma features that encode harmonic information important for chord recognition, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and resolve harmonic ambiguities. We compare the resulting features to hand-crafted ones by using a simple linear frame-wise classifier for chord recognition on various data sets. The results show that the learned feature extractor produces superior chroma vectors for chord recognition.",AUT,education,Developed economies,"[56.652687, -6.5202494]","[-34.366524, 20.412708]","[28.937658, -9.676528, 19.465443]","[-26.330751, -4.7866325, -1.6668432]","[6.6268806, 8.662707]","[5.912301, 3.8148308]","[11.885429, 10.272985, 2.1770298]","[9.62543, 9.152352, 12.2564335]"
51,Simon Durand;Slim Essid,Downbeat Detection with Conditional Random Fields and Deep Learned Features.,2016,https://doi.org/10.5281/zenodo.1417739,Simon Durand+Telecom ParisTech>FRA>education|Université Paris-Saclay>FRA>education;Slim Essid+Telecom ParisTech>FRA>education|Université Paris-Saclay>FRA>education,"In this paper, we introduce a novel Conditional Random Field (CRF) system that detects the downbeat sequence of musical audio signals. Feature functions are computed from four deep learned representations based on harmony, rhythm, melody and bass content to take advantage of the high-level and multi-faceted aspect of this task. Downbeats being dynamic, the powerful CRF classification system allows us to combine our features with an adapted temporal model in a fully data-driven fashion. Some meters being under-represented in our training set, we show that data augmentation enables a statistically significant improvement of the results by taking into account class imbalance. An evaluation of different configurations of our system on nine datasets shows its efficiency and potential over a heuristic based approach and four downbeat tracking algorithms.",FRA,education,Developed economies,"[36.6584, -36.92645]","[-28.802748, -19.055761]","[12.807586, -28.8655, -3.8362734]","[-9.52103, 16.391003, -12.404999]","[10.380454, 4.1156635]","[8.728866, 5.0880795]","[10.097045, 12.799792, -2.2934513]","[9.269337, 6.8893843, 9.402323]"
52,Li Su;Tsung-Ying Chuang;Yi-Hsuan Yang,"Exploiting Frequency, Periodicity and Harmonicity Using Advanced Time-Frequency Concentration Techniques for Multipitch Estimation of Choir and Symphony.",2016,https://doi.org/10.5281/zenodo.1414838,Li Su+Academia Sinica>TWN>education;Tsung-Ying Chuang+Academia Sinica>TWN>education;Yi-Hsuan Yang+Academia Sinica>TWN>education,"To advance research on automatic music transcription (AMT), it is important to have labeled datasets with sufficient diversity and complexity that support the creation and evaluation of robust algorithms to deal with issues seen in real-world polyphonic music signals. In this paper, we propose new datasets and investigate signal processing algorithms for multipitch estimation (MPE) in choral and symphony music, which have been seldom considered in AMT research. We observe that MPE in these two types of music is challenging because of not only the high polyphony number, but also the possible imprecision in pitch for notes sung or played by multiple singers or musicians in unison. To improve the robustness of pitch estimation, experiments show that it is beneficial to measure pitch saliency by jointly considering frequency, periodicity and harmonicity information. Moreover, we can improve the localization and stability of pitch by the multi-taper methods and nonlinear time-frequency reassignment techniques such as the Concentration of Time and Frequency (ConceFT) transform. We show that the proposed unsupervised methods to MPE compare favorably with, if not superior to, state-of-the-art supervised methods in various types of music signals from both existing and the newly created datasets.",TWN,education,Developing economies,"[36.017563, -20.183533]","[-6.1404333, -10.540782]","[6.9788575, -24.005703, 4.311128]","[2.5192487, -11.957902, -13.241189]","[9.038801, 8.947546]","[6.7619405, 2.747864]","[10.979201, 13.556492, -0.20399094]","[9.055038, 7.945829, 10.85244]"
80,Carl Southall;Ryan Stables;Jason Hockman,Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks.,2016,https://doi.org/10.5281/zenodo.1416244,Carl Southall+Birmingham City University>GBR>education|Digital Media Technology Laboratory (DMT Lab)>GBR>facility;Ryan Stables+Birmingham City University>GBR>education|Digital Media Technology Laboratory (DMT Lab)>GBR>facility;Jason Hockman+Birmingham City University>GBR>education|Digital Media Technology Laboratory (DMT Lab)>GBR>facility,"Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive instruments in audio recordings. Neural networks have already been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We propose the use of neural networks for ADT in order to exploit their ability to capture a complex configuration of features associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neural network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suitable for online operation. In both systems, a separate network is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilising the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respectively. The results demonstrate the effectiveness of the presented methods for solo percussion and a capacity for identifying snare drums, which are historically the most difficult drum class to detect.",GBR,education,Developed economies,"[28.06053, -45.930004]","[-34.50498, -14.505437]","[21.413681, -22.706139, 4.136287]","[-1.2128252, 11.375528, -18.463793]","[7.5305734, 7.1837835]","[8.328203, 4.3655148]","[10.303181, 11.395685, 1.1079274]","[9.042874, 7.164557, 9.758407]"
82,Kaustuv Kanti Ganguli;Sankalp Gulati;Xavier Serra;Preeti Rao,Data-Driven Exploration of Melodic Structure in Hindustani Music.,2016,https://doi.org/10.5281/zenodo.1416520,"Kaustuv Kanti Ganguli+Indian Institute of Technology Bombay>IND>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sankalp Gulati+Indian Institute of Technology Bombay>IND>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education","Indian art music is quintessentially an improvisatory music form in which the line between ‘fixed’ and ‘free’ is extremely subtle. In a raga performance, the melody is loosely constrained by the chosen composition but otherwise improvised in accordance with the raga grammar. One of the melodic aspects that is governed by this grammar is the manner in which a melody evolves in time in the course of a performance. In this work, we aim to discover such implicit patterns or regularities present in the temporal evolution of vocal melodies of Hindustani music. We start by applying existing tools and techniques used in music information retrieval to a collection of concerts recordings of alap performances by renowned khayal vocal artists. We use svara-based and svara duration-based melodic features to study and quantify the manifestation of concepts such as vadi, samvadi, nyas and graha svara in the vocal performances. We show that the discovered patterns corroborate the musicological findings that describe the “unfolding” of a raga in vocal performances of Hindustani music. The patterns discovered from the vocal melodies might help music students to learn improvisation and can complement the oral music pedagogy followed in this music tradition.",IND,education,Developing economies,"[7.7949204, 1.9343827]","[4.2597837, -18.174002]","[5.8628764, 4.7438707, -11.122221]","[14.172597, -12.401521, -5.4671545]","[11.264727, 10.326449]","[7.5121164, 1.1455415]","[11.9021225, 15.56255, -1.1130654]","[8.952348, 7.0772753, 12.6119175]"
83,Vincent Lostanlen;Carmine-Emanuele Cella,Deep Convolutional Networks on the Pitch Spiral For Music Instrument Recognition.,2016,https://doi.org/10.5281/zenodo.1416928,"Vincent Lostanlen+École normale supérieure, PSL Research University>FRA>education;Carmine-Emanuele Cella+École normale supérieure, PSL Research University>FRA>education","""Musical performance combines a wide range of pitches, nuances, and expressive techniques. Audio-based classification of musical instruments thus requires to build signal representations that are invariant to such transformations. This article investigates the construction of learned convolutional architectures for instrument recognition, given a limited amount of annotated training data. In this context, we benchmark three different weight sharing strategies for deep convolutional networks in the time-frequency domain: temporal kernels; time-frequency kernels; and a linear combination of time-frequency kernels which are one octave apart, akin to a Shepard pitch spiral. We provide an acoustical interpretation of these strategies within the source-filter framework of quasi-harmonic sounds with a fixed spectral envelope, which are archetypal of musical notes. The best classification accuracy is obtained by hybridizing all three convolutional layers into a single deep learning architecture.""",FRA,education,Developed economies,"[12.681211, -23.550879]","[-29.796005, -30.243288]","[15.888183, -3.5801218, 3.3153908]","[-8.555651, -9.830036, -16.825085]","[8.589737, 7.36529]","[7.871515, 5.184874]","[11.113876, 12.457341, 0.46680534]","[9.427837, 7.019301, 8.951875]"
84,Bartlomiej Stasiak,DTV-Based Melody Cutting for DTW-Based Melody Search and Indexing in QbH Systems.,2016,https://doi.org/10.5281/zenodo.1415704,Bartłomiej Stasiak+Lodz University of Technology>POL>education,"Melody analysis is an important processing step in several areas of Music Information Retrieval (MIR). Processing the pitch values extracted from raw input audio signal may be computationally complex as it requires substantial effort to reduce the uncertainty resulting i.a. from tempo variability and transpositions. A typical example is the melody matching problem in Query-by-Humming (QbH) systems, where Dynamic Time Warping (DTW) and note-based approaches are typically applied. In this work we present a new, simple and efficient method of investigating the melody content which may be used for approximate, preliminary matching of melodies irrespective of their tempo and length. The proposed solution is based on Discrete Total Variation (DTV) of the melody pitch vector, which may be computed in linear time. We demonstrate its practical application for finding the appropriate melody cutting points in the R∗-tree-based DTW indexing framework. The experimental validation is based on a dataset of 4431 queries and over 4000 template melodies, constructed specially for testing Query-by-Humming systems.",POL,education,Developed economies,"[5.3021455, -10.239289]","[8.1128235, 10.543229]","[11.188696, 2.7984173, -4.3036594]","[5.9005485, -13.853577, 9.398858]","[10.342155, 9.983025]","[8.6279335, 0.7034913]","[11.190192, 15.077705, -0.6027598]","[10.20277, 6.0809226, 13.162714]"
85,John Fuller;Lauren Hubener;Yea-Seul Kim;Jin Ha Lee,Elucidating User Behavior in Music Services Through Persona and Gender.,2016,https://doi.org/10.5281/zenodo.1415928,John Fuller+University of Washington>USA>education;Lauren Hubener+University of Washington>USA>education;Yea-Seul Kim+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"Prior user studies in the music information retrieval field have identified different personas representing the needs, goals, and characteristics of specific user groups for a user-centered design of music services. However, these personas were derived from a qualitative study involving a small number of participants and their generalizability has not been tested. The objectives of this study are to explore the applicability of seven user personas, developed in prior research, with a larger group of users and to identify the correlation between personas and the use of different types of music services. In total, 962 individuals were surveyed in order to understand their behaviors and preferences when interacting with music streaming services. Using a stratified sampling framework, key characteristics of each persona were extracted to classify users into specific persona groups. Responses were also analyzed in relation to gender, which yielded significant differences. Our findings support the development of more targeted approaches in music services rather than a universal service model.",USA,education,Developed economies,"[-35.28685, 26.453133]","[37.613995, 31.14829]","[-19.843681, 18.991547, -11.978219]","[5.85912, 12.464641, 18.68246]","[15.218501, 8.586665]","[12.87042, 1.0143236]","[15.279276, 14.9082575, -1.5299337]","[13.320826, 4.2666736, 12.075501]"
86,Nazareno Andrade;Flavio Figueiredo,Exploring the Latent Structure of Collaborations in Music Recordings: A Case Study in Jazz.,2016,https://doi.org/10.5281/zenodo.1416176,Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education;Flavio Figueiredo+IBM Research - Brazil>BRA>company,"Music records are largely a byproduct of collaborative efforts. Understanding how musicians collaborate to create records provides a step to understand the social production of music. This work leverages recent methods from trajectory mining to investigate how musicians have collaborated over time to record albums. Our case study analyzes data from the Discogs.com database from the Jazz domain. Our analysis examines how to explore the latent structure of collaboration between leading artists or bands and instrumentists over time. Moreover, we leverage the latent structure of our dataset to perform large-scale quantitative analyses of typical collaboration dynamics in different artist communities.",BRA,education,Developing economies,"[-36.183804, 11.330443]","[46.47469, 8.188562]","[-8.472978, 24.229584, -3.142203]","[14.556282, 12.560638, 8.674304]","[14.571711, 9.136745]","[12.226076, 2.7750764]","[14.840535, 14.377428, -0.7991242]","[13.22437, 5.5383787, 11.73841]"
87,Phillip B. Kirlin,Global Properties of Expert and Algorithmic Hierarchical Music Analyses.,2016,https://doi.org/10.5281/zenodo.1416118,Phillip B. Kirlin+Rhodes College>USA>education,"In recent years, advances in machine learning and increases in data set sizes have produced a number of viable algorithms for analyzing music in a hierarchical fashion according to the guidelines of music theory. Many of these algorithms, however, are based on techniques that rely on a series of local decisions to construct a complete music analysis, resulting in analyses that are not guaranteed to resemble ground-truth analyses in their large-scale organizational shapes or structures. In this paper, we examine a number of hierarchical music analysis data sets — drawing from Schenkerian analysis and other analytical systems based on A Generative Theory of Tonal Music — to study three global properties calculated from the shapes of the analyses. The major finding presented in this work is that it is possible for an algorithm that only makes local decisions to produce analyses that resemble expert analyses with regards to the three global properties in question. We also illustrate specific similarities and differences in these properties across both ground-truth and algorithmically-produced analyses.",USA,education,Developed economies,"[0.33081645, 4.0668507]","[-10.429867, 20.992779]","[-3.692696, -4.0273743, 5.5484977]","[-12.547759, -0.62802076, 9.157961]","[12.304094, 8.296565]","[8.68715, 1.9354986]","[13.2632885, 13.773225, -0.7843395]","[10.091773, 6.307204, 11.839097]"
88,Liang Chen;Erik Stolterman;Christopher Raphael,Human-Interactive Optical Music Recognition.,2016,https://doi.org/10.5281/zenodo.1416184,Liang Chen+Indiana University Bloomington>USA>education;Erik Stolterman+Indiana University Bloomington>USA>education;Christopher Raphael+Indiana University Bloomington>USA>education,"We propose a human-driven Optical Music Recognition (OMR) system that creates symbolic music data from common Western notation scores. Despite decades of development, OMR still remains largely unsolved as state-of-the-art automatic systems are unable to give reliable and useful results on a wide range of documents. For this reason our system, Ceres, combines human input and machine recognition to efficiently generate high-quality symbolic data. We propose a scheme for human-in-the-loop recognition allowing the user to constrain the recognition in two ways. The human actions allow the user to impose either a pixel labeling or model constraint, while the system re-recognizes subject to these constraints. We present evaluation based on different users’ log data using both Ceres and Sibelius software to produce the same music documents. We conclude that our system shows promise for transcribing complicated music scores with high accuracy.",USA,education,Developed economies,"[39.201134, 20.883644]","[-20.093235, 35.92547]","[20.825705, 12.451387, 10.005272]","[-11.128944, -18.724459, 1.6721725]","[8.664163, 6.1734066]","[6.7105885, -0.57254577]","[10.653058, 11.100011, -0.16821456]","[8.036381, 4.291489, 10.662604]"
89,Jack Atherton;Blair Kaneshiro,I Said it First: Topological Analysis of Lyrical Influence Networks.,2016,https://doi.org/10.5281/zenodo.1418047,"Jack Atherton+Center for Computer Research in Music and Acoustics, Stanford University>USA>education;Blair Kaneshiro+Center for Computer Research in Music and Acoustics, Stanford University>USA>education","We present an analysis of musical influence using intact lyrics of over 550,000 songs, extending existing research on lyrics through a novel approach using directed networks. We form networks of lyrical influence over time at the level of three-word phrases, weighted by tf-idf. An edge reduction analysis of strongly connected components suggests highly central artist, songwriter, and genre network topologies. Visualizations of the genre network based on multidimensional scaling confirm network centrality and provide insight into the most influential genres at the heart of the network. Next, we present metrics for influence and self-referential behavior, examining their interactions with network centrality and with the genre diversity of songwriters. Here, we uncover a negative correlation between songwriters’ genre diversity and the robustness of their connections. By examining trends among the data for top genres, songwriters, and artists, we address questions related to clustering, influence, and isolation of nodes in the networks. We conclude by discussing promising future applications of lyrical influence networks in music information retrieval research. The networks constructed in this study are made publicly available for research purposes.",USA,education,Developed economies,"[-50.20239, -0.8761527]","[46.539787, 9.356169]","[-12.681894, 25.012064, 9.594296]","[12.686945, 12.958458, 10.46101]","[12.847916, 11.9544325]","[12.275203, 2.6736674]","[15.506008, 15.017734, 1.316855]","[13.329493, 5.587613, 11.780328]"
90,Elad Liebman;Peter Stone;Corey N. White,Impact of Music on Decision Making in Quantitative Tasks.,2016,https://doi.org/10.5281/zenodo.1417743,Elad Liebman+The University of Texas at Austin>USA>education;Peter Stone+The University of Texas at Austin>USA>education;Corey N. White+Syracuse University>USA>education,"The goal of this study is to explore which aspects of people’s analytical decision making are affected when exposed to music. To this end, we apply a stochastic sequential model of simple decisions, the drift-diffusion model (DDM), to understand risky decision behavior. Numerous studies have demonstrated that mood can affect emotional and cognitive processing, but the exact nature of the impact music has on decision making in quantitative tasks has not been sufficiently studied. In our experiment, participants decided whether to accept or reject multiple bets with different risk vs. reward ratios while listening to music that was chosen to induce positive or negative mood. Our results indicate that music indeed alters people’s behavior in a surprising way - happy music made people make better choices. In other words, it made people more likely to accept good bets and reject bad bets. The DDM decomposition indicated the effect focused primarily on both the caution and the information processing aspects of decision making. To further understand the correspondence between auditory features and decision making, we studied how individual aspects of music affect response patterns. Our results are particularly interesting when compared with recent results regarding the impact of music on emotional processing, as they illustrate that music affects analytical decision making in a fundamentally different way, hinting at a different psychological mechanism that music impacts.",USA,education,Developed economies,"[-37.104862, 17.555323]","[58.620316, 0.9510185]","[-13.368808, 19.199562, -0.7472848]","[5.4236026, 20.597395, 14.172136]","[15.033322, 8.967444]","[13.219483, 3.0045564]","[15.0305195, 15.139365, -1.0637867]","[13.589866, 4.47124, 11.137192]"
91,Simon Waloschek;Axel Berndt;Benjamin W. Bohl;Aristotelis Hadjakos,Interactive Scores in Classical Music Production.,2016,https://doi.org/10.5281/zenodo.1416710,Simon Waloschek+University of Music Detmold>DEU>education;Axel Berndt+University of Music Detmold>DEU>education;Benjamin W. Bohl+University of Music Detmold>DEU>education;Aristotelis Hadjakos+University of Music Detmold>DEU>education,"The recording of classical music is mostly centered around the score of a composition. During editing of these recordings, however, further technical visualizations are used. Introducing digital interactive scores to the recording and editing process can enhance the workflow significantly and speed up the production process. This paper gives a short introduction to the recording process and outlines possibilities that arise with interactive scores. Current related music information retrieval research is discussed, showing a potential path to score-based editing.",DEU,education,Developed economies,"[-12.896722, 4.5471463]","[10.314003, 37.76947]","[-8.958863, -13.67726, -9.558156]","[3.1037877, -2.816906, 26.632572]","[12.509706, 7.130541]","[10.4545355, 0.6703658]","[13.2606, 13.048431, -1.4940717]","[11.131906, 5.1929283, 12.229705]"
92,Helena Bantulà;Sergio I. Giraldo;Rafael Ramírez 0001,Jazz Ensemble Expressive Performance Modeling.,2016,https://doi.org/10.5281/zenodo.1415876,"Helena Bantula+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sergio Giraldo+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rafael Ramirez+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Computational expressive music performance studies the analysis and characterisation of the deviations that a musician introduces when performing a musical piece. It has been studied in a classical context where timing and dynamic deviations are modeled using machine learning techniques. In jazz music, work has been done previously on the study of ornament prediction in guitar performance, as well as in saxophone expressive modeling. However, little work has been done on expressive ensemble performance. In this work, we analysed the musical expressivity of jazz guitar and piano from two different perspectives: solo and ensemble performance. The aim of this paper is to study the influence of piano accompaniment into the performance of a guitar melody and vice versa. Based on a set of recordings made by professional musicians, we extracted descriptors from the score, we transcribed the guitar and the piano performances and calculated performance actions for both instruments. We applied machine learning techniques to train models for each performance action, taking into account both solo and ensemble descriptors. Finally, we compared the accuracy of the induced models. The accuracy of most models increased when ensemble information was considered, which can be explained by the interaction between musicians.",ESP,education,Developed economies,"[-15.233632, 3.2961247]","[-33.347977, 7.2463193]","[-5.481701, 8.403157, 20.792837]","[-13.181739, 1.8176185, -1.9609406]","[11.249322, 8.146081]","[7.660994, 3.6012187]","[13.490847, 13.049297, -0.8618812]","[9.102248, 6.231886, 10.772734]"
93,Daniel Shanahan;Kerstin Neubarth;Darrell Conklin,Mining Musical Traits of Social Functions in Native American Music.,2016,https://doi.org/10.5281/zenodo.1416408,"Daniel Shanahan+Louisiana State University>USA>education;Kerstin Neubarth+Canterbury Christ Church University>GBR>education;Darrell Conklin+University of the Basque Country UPV/EHU>ESP>education|IKERBASQUE, Basque Foundation for Science>ESP>facility","Native American music is perhaps one of the most documented repertoires of indigenous folk music, being the subject of empirical ethnomusicological analyses for significant portions of the early 20th century. However, it has been largely neglected in more recent computational research, partly due to a lack of encoded data. In this paper we use the symbolic encoding of Frances Densmore’s collection of over 2000 songs, digitized between 1998 and 2014, to examine the relationship between internal musical features and social function. More specifically, this paper applies contrast data mining to discover global feature patterns that describe generalized social functions. Extracted patterns are discussed with reference to early ethnomusicological work and recent approaches to music, emotion, and ethology. A more general aim of this paper is to provide a methodology in which contrast data mining can be used to further examine the interactions between musical features and external factors such as social function, geography, language, and emotion.",USA,education,Developed economies,"[-29.290005, 20.435574]","[22.353155, -1.2395362]","[-23.066475, 11.377296, -3.8438728]","[8.003592, 5.9222836, 1.0950713]","[13.9403105, 8.962745]","[9.368573, 2.3668287]","[14.32506, 15.036032, -1.3059604]","[11.034182, 6.765652, 11.814316]"
94,Flavio Figueiredo;Bruno Ribeiro 0001;Christos Faloutsos;Nazareno Andrade;Jussara M. Almeida,Mining Online Music Listening Trajectories.,2016,https://doi.org/10.5281/zenodo.1417275,Flavio Figueiredo+IBM Research - Brazil>BRA>company;Bruno Ribeiro+Purdue University>USA>education;Christos Faloutsos+Carnegie Mellon University>USA>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education;Jussara M. Almeida+Universidade Federal de Minas Gerais>BRA>education,"Understanding the listening habits of users is a valuable undertaking for musicology researchers, artists, consumers and online businesses alike. With the rise of Online Music Streaming Services (OMSSs), large amounts of user behavioral data can be exploited for this task. In this paper, we present SWIFT-FLOWS, an approach that models user listening habits in regards to how user attention transitions between artists. SWIFT-FLOWS combines recent advances in trajectory mining, coupled with modulated Markov models as a means to capture both how users switch attention from one artist to another, as well as how users fixate their attention in a single artist over short or large periods of time. We employ SWIFT-FLOWS on OMSSs datasets showing that it provides: (1) semantically meaningful representation of habits; (2) accurately models the attention span of users.",BRA,company,Developing economies,"[-23.1932, 11.78235]","[42.93143, 20.268356]","[-12.276772, 17.974556, -13.599467]","[17.948437, 9.113766, 18.947643]","[14.795491, 7.890197]","[12.402583, 1.7303493]","[14.808963, 14.319141, -1.7336636]","[13.314295, 5.12709, 12.544323]"
95,Tomoyasu Nakano;Daichi Mochihashi;Kazuyoshi Yoshii;Masataka Goto,Musical Typicality: How Many Similar Songs Exist?.,2016,https://doi.org/10.5281/zenodo.1417915,Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Daichi Mochihashi+The Institute of Statistical Mathematics>JPN>facility;Kazuyoshi Yoshii+Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We propose a method for estimating the musical “typicality” of a song from an information theoretic perspective. While musical similarity compares just two songs, musical typicality quantifies how many of the songs in a set are similar. It can be used not only to express the uniqueness of a song but also to recommend one that is representative of a set. Building on the type theory in information theory (Cover & Thomas 2006), we use a Bayesian generative model of musical features and compute the typicality of a song as the sum of the probabilities of the songs that share the type of the given song. To evaluate estimated results, we focused on vocal timbre which can be evaluated quantitatively by using the singer’s gender. Estimated typicality is evaluated against the Pearson correlation coefficient between the computed typicality and the ratio of the number of male singers to female singers of a song-set. Our result shows that the proposed measure works more effectively to estimate musical typicality than the previous model based simply on generative probabilities.",JPN,facility,Developed economies,"[-0.6927637, 13.035769]","[17.509169, -0.1332192]","[-4.1225963, 5.271442, 4.3498087]","[2.9005802, 6.6522655, 6.105112]","[12.830859, 9.546538]","[9.313511, 2.0306911]","[13.271908, 15.4682, -0.76858544]","[11.143962, 6.997137, 12.802219]"
96,Jeremy Hyrkas;Bill Howe,MusicDB: A Platform for Longitudinal Music Analytics.,2016,https://doi.org/10.5281/zenodo.1417381,Jeremy Hyrkas+University of Washington>USA>education;Bill Howe+University of Washington>USA>education,"With public data sources such as Million Song dataset, researchers can now study longitudinal questions about the patterns of popular music, but the scale and complexity of the data complicate analysis. We propose MusicDB, a new approach for longitudinal music analytics that adapts techniques from relational databases to the music setting. By representing song timeseries data relationally, we aim to dramatically decrease the programming effort required for complex analytics while significantly improving scalability. We show how our platform can improve performance by reducing the amount of data accessed for many common analytics tasks, and how such tasks can be implemented quickly in relational languages — variants of SQL. We further show that expressing music analytics tasks over relational representations allows the system to automatically parallelize and optimize the resulting programs to improve performance. We evaluate our system by expressing complex analytics tasks including calculating song density and beat-aligning features and showing significant performance improvements over previous results. Finally, we evaluate expressiveness by reproducing the results from a recent analysis of longitudinal music trends using the Million Song dataset.",USA,education,Developed economies,"[-20.216604, 13.916224]","[10.895578, 22.718222]","[-12.982943, -1.1603669, -9.717813]","[-5.7194934, 5.4907136, 12.268497]","[13.527322, 7.6411695]","[12.096136, 1.2329335]","[14.284212, 13.983906, -1.5641315]","[12.627513, 5.0836234, 12.071284]"
97,Hamid Eghbal-zadeh;Gerhard Widmer,Noise Robust Music Artist Recognition Using I-Vector Features.,2016,https://doi.org/10.5281/zenodo.1417199,Hamid Eghbal-zadeh+Johannes Kepler University of Linz>AUT>education;Gerhard Widmer+Johannes Kepler University of Linz>AUT>education,"In music information retrieval (MIR), dealing with different types of noise is important and the MIR models are frequently used in noisy environments such as live performances. Recently, i-vector features have shown great promise for some major tasks in MIR, such as music similarity and artist recognition. In this paper, we introduce a novel noise-robust music artist recognition system using i-vector features. Our method uses a short sample of noise to learn the parameters of noise, then using a Maximum A Posteriori (MAP) estimation it estimates clean i-vectors given noisy i-vectors. We examine the performance of multiple systems confronted with different kinds of additive noise in a clean training - noisy testing scenario. Using open-source tools, we have synthesized 12 different noisy versions from a standard 20-class music artist recognition dataset encountered with 4 different kinds of additive noise with 3 different Signal-to-Noise-Ratio (SNR). Using these datasets, we carried out music artist recognition experiments comparing the proposed method with the state-of-the-art. The results suggest that the proposed method outperforms the state-of-the-art.",AUT,education,Developed economies,"[-24.238543, -10.627742]","[14.656651, -12.776072]","[-12.37468, 2.746218, 9.909246]","[12.577138, 4.7850094, -8.922015]","[12.612391, 10.413493]","[9.671797, 3.4867043]","[13.534669, 14.16261, 0.96632105]","[11.288405, 7.157929, 10.58045]"
98,Georgi Dzhambazov;Ajay Srinivasamurthy;Sertan Sentürk;Xavier Serra,On the Use of Note Onsets for Improved Lyrics-To-Audio Alignment in Turkish Makam Music.,2016,https://doi.org/10.5281/zenodo.1415988,"Georgi Dzhambazov+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Ajay Srinivasamurthy+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sertan Şentürk+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Lyrics-to-audio alignment aims to automatically match given lyrics and musical audio. In this work we extend a state of the art approach for lyrics-to-audio alignment with information about note onsets. In particular, we consider the fact that transition to next lyrics syllable usually implies transition to a new musical note. To this end we formulate rules that guide the transition between consecutive phonemes when a note onset is present. These rules are incorporated into the transition matrix of a variable-time hidden Markov model (VTHMM) phonetic recognizer based on MFCCs. An estimated melodic contour is input to an automatic note transcription algorithm, from which the note onsets are derived. The proposed approach is evaluated on 12 a cappella audio recordings of Turkish Makam music using a phrase-level accuracy measure. Evaluation of the alignment is also presented on a polyphonic version of the dataset in order to assess how degradation in the extracted onsets affects performance. Results show that the proposed model outperforms a baseline approach unaware of onset transition rules. To the best of our knowledge, this is the one of the first approaches tackling lyrics tracking, which combines timbral features with a melodic feature in the alignment process itself.",ESP,education,Developed economies,"[10.568292, 4.7905254]","[-8.291239, -20.497576]","[-13.218307, -23.252178, -11.96436]","[2.8494976, -4.3929543, -17.306719]","[11.671203, 10.767576]","[6.380556, 2.6010187]","[12.180715, 14.981882, -1.5241393]","[8.687014, 6.9833407, 10.521496]"
99,Raphaël Fournier-S'niehotta;Philippe Rigaux;Nicolas Travers,Querying XML Score Databases: XQuery is not Enough!.,2016,https://doi.org/10.5281/zenodo.1416976,Raphaël Fournier-S’niehotta+CNAM>FRA>education|CNAM>FRA>education|CNAM>FRA>education;Philippe Rigaux+CNAM>FRA>education|CNAM>FRA>education|CNAM>FRA>education;Nicolas Travers+CNAM>FRA>education|CNAM>FRA>education|CNAM>FRA>education,"The paper addresses issues related to the design of query languages for searching and restructuring collections of XML-encoded music scores. We advocate against a direct approach based on XQuery, and propose a more powerful strategy that first extracts a structured representation of music notation from score encodings, and then manipulates this representation in closed form with dedicated operators. The paper exposes the content model, the resulting language, and describes our implementation on top of a large Digital Score Library (DSL).",FRA,education,Developed economies,"[-1.8794024, 31.396818]","[6.317641, 39.920982]","[-11.273576, -8.021221, -18.780169]","[-5.5806026, -8.930062, 21.52851]","[13.901995, 6.894259]","[10.217789, 0.30575135]","[13.893996, 14.427787, -2.564231]","[10.908103, 4.930412, 12.209707]"
100,Richard Vogl;Matthias Dorfer;Peter Knees,Recurrent Neural Networks for Drum Transcription.,2016,https://doi.org/10.5281/zenodo.1417613,Richard Vogl+Johannes Kepler University Linz>AUT>education;Matthias Dorfer+Johannes Kepler University Linz>AUT>education;Peter Knees+Johannes Kepler University Linz>AUT>education,"Music transcription is a core task in the field of music information retrieval. Transcribing the drum tracks of music pieces is a well-defined sub-task. The symbolic representation of a drum track contains much useful information about the piece, like meter, tempo, as well as various style and genre cues. This work introduces a novel approach for drum transcription using recurrent neural networks. We claim that recurrent neural networks can be trained to identify the onsets of percussive instruments based on general properties of their sound. Different architectures of recurrent neural networks are compared and evaluated using a well-known dataset. The outcomes are compared to results of a state-of-the-art approach on the same dataset. Furthermore, the ability of the networks to generalize is demonstrated using a second, independent dataset. The experiments yield promising results: while F-measures higher than state-of-the-art results are achieved, the networks are capable of generalizing reasonably well.",AUT,education,Developed economies,"[28.012272, -45.940315]","[-34.19466, -14.925944]","[20.343887, -23.079706, 4.6792693]","[-2.098555, 9.944528, -18.932875]","[7.5452204, 7.165802]","[8.480619, 4.9061036]","[10.351462, 11.44395, 1.0665512]","[9.259836, 6.6917505, 9.412372]"
101,François Rigaud;Mathieu Radenen,Singing Voice Melody Transcription Using Deep Neural Networks.,2016,https://doi.org/10.5281/zenodo.1418051,François Rigaud+Audionamix>FRA>company|Unknown>Unknown>Unknown;Mathieu Radenen+Audionamix>FRA>company|Unknown>Unknown>Unknown,"This paper presents a system for the transcription of singing voice melodies in polyphonic music signals based on Deep Neural Network (DNN) models. In particular, a new DNN system is introduced for performing the f0 estimation of the melody, and another DNN, inspired from recent studies, is learned for segmenting vocal sequences. Preparation of the data and learning configurations related to the specificity of both tasks are described. The performance of the melody f0 estimation system is compared with a state-of-the-art method and exhibits highest accuracy through a better generalization on two different music databases. Insights into the global functioning of this DNN are proposed. Finally, an evaluation of the global system combining the two DNNs for singing voice melody transcription is presented.",FRA,company,Developed economies,"[-2.6055725, -38.500107]","[-29.705137, -33.52097]","[26.007076, 10.605794, -11.4155035]","[-3.259977, -9.971441, -16.964958]","[9.624628, 10.61506]","[7.661433, 5.2196836]","[11.132148, 14.742086, 0.8524517]","[9.658825, 7.113806, 8.985385]"
102,Kai-Chun Hsu;Chih-Shan Lin;Tai-Shih Chi,Sparse Coding Based Music Genre Classification Using Spectro-Temporal Modulations.,2016,https://doi.org/10.5281/zenodo.1418099,Kai-Chun Hsu+National Chiao Tung University>TWN>education;Chih-Shan Lin+National Chiao Tung University>TWN>education;Tai-Shih Chi+National Chiao Tung University>TWN>education,"Spectro-temporal modulations (STMs) of the sound convey timbre and rhythm information so that they are intuitively useful for automatic music genre classification. The STMs are usually extracted from a time-frequency representation of the acoustic signal. In this paper, we investigate the efficacy of two kinds of STM features, the Gabor features and the rate-scale (RS) features, selectively extracted from various time-frequency representations, including the short-time Fourier transform (STFT) spectrogram, the constant-Q transform (CQT) spectrogram and the auditory (AUD) spectrogram, in recognizing the music genre. In our system, the dictionary learning and sparse coding techniques are adopted for training the support vector machine (SVM) classifier. Both spectral-type features and modulation-type features are used to test the system. Experiment results show that the RS features extracted from the log. magnituded CQT spectrogram produce the highest recognition rate in classifying the music genre.",TWN,education,Developing economies,"[-33.67856, -14.2709]","[18.730127, -15.29856]","[-17.990253, 10.151442, 20.454773]","[17.696346, 4.0523686, -9.644887]","[12.860692, 10.825686]","[9.38381, 3.6947963]","[13.806776, 14.05982, 1.3173063]","[11.151397, 7.48946, 10.578715]"
103,Sankalp Gulati;Joan Serrà;Kaustuv Kanti Ganguli;Sertan Sentürk;Xavier Serra,Time-Delayed Melody Surfaces for Rāga Recognition.,2016,https://doi.org/10.5281/zenodo.1417905,"Sankalp Gulati+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Telefonica Research>ESP>company|Indian Institute of Technology Bombay>IND>education;Joan Serrà+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Telefonica Research>ESP>company;Kaustuv K Ganguli+Indian Institute of Technology Bombay>IND>education;Sertan Şentürk+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","R¯aga is the melodic framework of Indian art music. It is a core concept used in composition, performance, organization, and pedagogy. Automatic r¯aga recognition is thus a fundamental information retrieval task in Indian art music. In this paper, we propose the time-delayed melody surface (TDMS), a novel feature based on delay coordinates that captures the melodic outline of a r¯aga. A TDMS describes both the tonal and the temporal characteristics of a melody, using only an estimation of the predominant pitch. Considering a simple k-nearest neighbor classifier, TDMSs outperform the state-of-the-art for r¯aga recognition by a large margin. We obtain 98% accuracy on a Hindustani music dataset of 300 recordings and 30 r¯agas, and 87% accuracy on a Carnatic music dataset of 480 recordings and 40 r¯agas. TDMSs are simple to implement, fast to compute, and have a musically meaningful interpretation. Since the concepts and formulation behind the TDMS are generic and widely applicable, we envision its usage in other music traditions beyond Indian art music.",ESP,education,Developed economies,"[3.8217251, -2.3808892]","[4.752596, -17.541481]","[8.223396, 2.6250968, -19.869268]","[13.336528, -11.144544, -6.3316264]","[11.152479, 10.589251]","[7.6352563, 1.1749762]","[11.500146, 15.151394, -1.623147]","[9.087279, 7.030424, 12.646938]"
104,Andrea Cogliati;David Temperley;Zhiyao Duan,Transcribing Human Piano Performances into Music Notation.,2016,https://doi.org/10.5281/zenodo.1416466,Andrea Cogliati+University of Rochester>USA>education|University of Rochester>USA>education;David Temperley+University of Rochester>USA>education|Eastman School of Music>USA>education;Zhiyao Duan+University of Rochester>USA>education,"Automatic music transcription aims to transcribe musical performances into music notation. However, existing transcription systems that have been described in research papers typically focus on multi-F0 estimation from audio and only output notes in absolute terms, showing frequency and absolute time (a piano-roll representation), but not in musical terms, with spelling distinctions (e.g., A♭ versus G♯) and quantized meter. To complete the transcription process, one would need to convert the piano-roll representation into a properly formatted and musically meaningful musical score. This process is non-trivial and largely unresearched. In this paper we present a system that generates music notation output from human-recorded MIDI performances of piano music. We show that the correct estimation of the meter, harmony and streams in a piano performance provides a solid foundation to produce a properly formatted score. In a blind evaluation by professional music theorists, the proposed method outperforms two commercial programs and an open source program in terms of pitch notation and rhythmic notation, and ties for the top in terms of overall voicing and staff placement.",USA,education,Developed economies,"[32.935963, 0.5851712]","[-7.756256, -13.574583]","[15.662997, -4.2119837, 21.269197]","[-3.5573068, -11.630377, -7.513241]","[10.0045, 7.163448]","[6.8625855, 2.24516]","[12.364536, 11.602044, -0.46828264]","[8.978307, 6.769872, 10.897629]"
105,Xiao Hu 0001;Kahyun Choi;Jin Ha Lee;Audrey Laplante;Yun Hao;Sally Jo Cunningham;J. Stephen Downie,WiMIR: An Informetric Study On Women Authors In ISMIR.,2016,https://doi.org/10.5281/zenodo.1414832,Xiao Hu+University of Hong Kong>HKG>education;Kahyun Choi+University of Illinois>USA>education;Jin Ha Lee+University of Washington>USA>education;Audrey Laplante+Université de Montréal>CAN>education;Yun Hao+University of Illinois>USA>education;Sally Jo Cunningham+University of Waikato>NZL>education;J. Stephen Downie+University of Illinois>USA>education,"The Music Information Retrieval (MIR) community is becoming increasingly aware of a gender imbalance evident in ISMIR participation and publication. This paper reports upon a comprehensive informetric study of the publication, authorship and citation characteristics of female researchers in the context of the ISMIR conferences. All 1,610 papers in the ISMIR proceedings written by 1,910 unique authors from 2000 to 2015 were collected and analyzed. Only 14.1% of all papers were led by female researchers. Temporal analysis shows that the percentage of lead female authors has not improved over the years, but more papers have appeared with female co-authors in very recent years. Topics and citation numbers are also analyzed and compared between female and male authors to identify research emphasis and to measure impact. The results show that the most prolific authors of both genders published similar numbers of ISMIR papers and the citation counts of lead authors in both genders had no significant difference. We also analyzed the collaboration patterns to discover whether gender is related to the number of collaborators. Implications of these findings are discussed and suggestions are proposed on how to continue encouraging and supporting female participation in the MIR field.",HKG,education,Developing economies,"[-14.359246, 46.507378]","[28.688, 40.095062]","[-28.85264, -6.704223, -1.964868]","[3.2469063, 7.3229756, 21.544348]","[13.81087, 5.6920395]","[12.013948, 0.40234265]","[15.178663, 11.9644, -1.706557]","[12.291533, 4.124778, 12.232019]"
106,Ryan Groves,Automatic Melodic Reduction Using a Supervised Probabilistic Context-Free Grammar.,2016,https://doi.org/10.5281/zenodo.1416924,Ryan Groves+Unknown>Unknown>Unknown,"This research explores a Natural Language Processing technique utilized for the automatic reduction of melodies: the Probabilistic Context-Free Grammar (PCFG). Automatic melodic reduction was previously explored by means of a probabilistic grammar [11] [1]. However, each of these methods used unsupervised learning to estimate the probabilities for the grammar rules, and thus a corpus-based evaluation was not performed. A dataset of analyses using the Generative Theory of Tonal Music (GTTM) exists [13], which contains 300 Western tonal melodies and their corresponding melodic reductions in tree format. In this work, supervised learning is used to train a PCFG for the task of melodic reduction, using the tree analyses provided by the GTTM dataset. The resulting model is evaluated on its ability to create accurate reduction trees, based on a node-by-node comparison with ground-truth trees. Multiple data representations are explored, and example output reductions are shown. Motivations for performing melodic reduction include melodic identification and similarity, efficient storage of melodies, automatic composition, variation matching, and automatic harmonic analysis.",Unknown,Unknown,Unknown,"[3.3821602, -7.8563323]","[-21.235064, 25.46398]","[8.889839, 9.54575, -5.055895]","[-19.71537, 4.0556993, 2.9116387]","[10.801328, 9.770433]","[8.016049, 2.5604265]","[11.820987, 15.07468, -0.6054896]","[9.826263, 7.4506288, 12.144172]"
81,R. Michael Winters;Siddharth Gururani;Alexander Lerch,"Automatic Practice Logging: Introduction, Dataset & Preliminary Study.",2016,https://doi.org/10.5281/zenodo.1416224,R. Michael Winters+Georgia Tech Center for Music Technology (GTCMT)>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Siddharth Gururani+Georgia Tech Center for Music Technology (GTCMT)>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Alexander Lerch+Georgia Tech Center for Music Technology (GTCMT)>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Musicians spend countless hours practicing their instruments. To document and organize this time, musicians commonly use practice charts to log their practice. However, manual techniques require time, dedication, and experience to master, are prone to fallacy and omission, and ultimately can not describe the subtle variations in each repetition. This paper presents an alternative: by analyzing and classifying the audio recorded while practicing, logging could occur automatically, with levels of detail, accuracy, and ease that would not be possible otherwise. Towards this goal, we introduce the problem of Automatic Practice Logging (APL), including a discussion of the benefits and unique challenges it raises. We then describe a new dataset of over 600 annotated recordings of solo piano practice, which can be used to design and evaluate APL systems. After framing our approach to the problem, we present an algorithm designed to align short segments of practice audio with reference recordings using pitch chroma and dynamic time warping.",USA,education,Developed economies,"[-34.41707, -3.1522663]","[-11.1715355, -4.873147]","[-28.412659, -1.5537541, 14.374882]","[-2.0779784, -19.3362, 2.9033155]","[12.326608, 6.3483453]","[6.3827486, 0.9101216]","[14.182066, 13.079532, -0.9432934]","[8.487882, 5.8956094, 10.962542]"
53,Hendrik Schreiber,Genre Ontology Learning: Comparing Curated with Crowd-Sourced Ontologies.,2016,https://doi.org/10.5281/zenodo.1417479,Hendrik Schreiber+tagtraum industries incorporated>USA>company,"The Semantic Web has made it possible to automatically find meaningful connections between musical pieces which can be used to infer their degree of similarity. Similarity in turn, can be used by recommender systems driving music discovery or playlist generation. One useful facet of knowledge for this purpose are fine-grained genres and their inter-relationships. In this paper we present a method for learning genre ontologies from crowd-sourced genre labels, exploiting genre co-occurrence rates. Using both lexical and conceptual similarity measures, we show that the quality of such learned ontologies is comparable with manually created ones. In the process, we document properties of current reference genre ontologies, in particular a high degree of disconnectivity. Further, motivated by shortcomings of the established taxonomic precision measure, we define a novel measure for highly disconnected ontologies.",USA,company,Developed economies,"[-27.853573, 37.863014]","[15.740049, 37.55343]","[-24.506802, 1.2027211, -2.7307496]","[-4.2198696, -1.0113306, 27.736864]","[14.218045, 9.111244]","[10.825396, -0.13192159]","[15.14976, 13.817475, -1.0888289]","[12.410904, 5.4662437, 11.564338]"
107,Patrick Gray;Razvan C. Bunescu,A Neural Greedy Model for Voice Separation in Symbolic Music.,2016,https://doi.org/10.5281/zenodo.1417251,Patrick Gray+Ohio University>USA>education|Ohio University>USA>education;Razvan Bunescu+Ohio University>USA>education|Ohio University>USA>education,"""Music is often experienced as a simultaneous progression of multiple streams of notes, or voices. The automatic separation of music into voices is complicated by the fact that music spans a voice-leading continuum ranging from monophonic, to homophonic, to polyphonic, often within the same work. We address this diversity by defining voice separation as the task of partitioning music into streams that exhibit both a high degree of external perceptual separation from the other streams and a high degree of internal perceptual consistency, to the maximum degree that is possible in the given musical input. Equipped with this task definition, we manually annotated a corpus of popular music and used it to train a neural network with one hidden layer that is connected to a diverse set of perceptually informed input features. The trained neural model greedily assigns notes to voices in a left to right traversal of the input chord sequence. When evaluated on the extraction of consecutive within voice note pairs, the model obtains over 91% F-measure, surpassing a strong baseline based on an iterative application of an envelope extraction function.""",USA,education,Developed economies,"[1.8269122, -47.36568]","[-35.777206, -33.524345]","[25.201155, 4.266162, -0.32887244]","[-12.117074, -11.186694, -23.149113]","[8.666679, 10.346447]","[7.09184, 5.598385]","[10.914946, 14.017844, 1.5555793]","[9.619898, 7.973063, 9.006135]"
78,Markus Schedl;Hamid Eghbal-Zadeh;Emilia Gómez;Marko Tkalcic,An Analysis of Agreement in Classical Music Perception and its Relationship to Listener Characteristics.,2016,https://doi.org/10.5281/zenodo.1417559,Markus Schedl+Johannes Kepler University>AUT>education;Hamid Eghbal-Zadeh+Johannes Kepler University>AUT>education;Emilia Gómez+Universitat Pompeu Fabra>ESP>education;Marko Tkalčič+Free University of Bozen–Bolzano>ITA>education,"We present a study, carried out on 241 participants, which investigates on classical music material the agreement of listeners on perceptual music aspects (related to emotion, tempo, complexity, and instrumentation) and the relationship between listener characteristics and these aspects. For the currently popular task of music emotion recognition, the former question is particularly important when defining a ground truth of emotions perceived in a given music collection. We characterize listeners via a range of factors, including demographics, musical inclination, experience, and education, and personality traits. Participants rate the music material under investigation, i.e., 15 expert-defined segments of Beethoven’s 3rd symphony, “Eroica”, in terms of 10 emotions, perceived tempo, complexity, and number of instrument groups. Our study indicates only slight agreement on most perceptual aspects, but significant correlations between several listener characteristics and perceptual qualities.",AUT,education,Developed economies,"[-2.7026935, 10.922934]","[51.255726, -13.190006]","[-6.9635763, 8.410164, 4.1589093]","[6.0616016, 20.91477, 6.3272743]","[12.995039, 9.500587]","[12.794982, 4.2531905]","[13.52447, 15.290371, -0.69118404]","[13.90581, 4.7290764, 10.378443]"
54,Clement Laroche;Hélène Papadopoulos;Matthieu Kowalski;Gaël Richard,Genre Specific Dictionaries for Harmonic/Percussive Source Separation.,2016,https://doi.org/10.5281/zenodo.1417147,"Clément Laroche+LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay>FRA>education|Univ Paris-Sud-CNRS-CentraleSupelec>FRA>education|Parietal project-team, INRIA, CEA-Saclay>FRA>facility;Hélène Papadopoulos+Univ Paris-Sud-CNRS-CentraleSupelec>FRA>education|Parietal project-team, INRIA, CEA-Saclay>FRA>facility;Matthieu Kowalski+Univ Paris-Sud-CNRS-CentraleSupelec>FRA>education|Parietal project-team, INRIA, CEA-Saclay>FRA>facility;Gaël Richard+LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay>FRA>education","Blind source separation usually obtains limited performance on real and polyphonic music signals. To overcome these limitations, it is common to rely on prior knowledge under the form of side information as in Informed Source Separation or on machine learning paradigms applied on a training database. In the context of source separation based on factorization models such as the Non-negative Matrix Factorization, this supervision can be introduced by learning specific dictionaries. However, due to the large diversity of musical signals it is not easy to build sufficiently compact and precise dictionaries that will well characterize the large array of audio sources. In this paper, we argue that it is relevant to construct genre-specific dictionaries. Indeed, we show on a task of harmonic/percussive source separation that the dictionaries built on genre-specific training subsets yield better performances than cross-genre dictionaries.",FRA,education,Developed economies,"[8.642898, -48.276005]","[-45.88606, -28.586578]","[31.279308, -5.5202727, -6.7365403]","[-7.1668687, -9.4857435, -26.876862]","[8.444208, 9.972395]","[6.4158454, 5.2479014]","[11.106176, 13.668291, 1.6116604]","[9.766316, 8.74821, 9.592451]"
55,Giuseppe Bandiera;Oriol Romani Picas;Hiroshi Tokuda;Wataru Hariya;Koji Oishi;Xavier Serra,Good-sounds.org: A Framework to Explore Goodness in Instrumental Sounds.,2016,https://doi.org/10.5281/zenodo.1416864,"Giuseppe Bandiera+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Oriol Romani Picas+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Hiroshi Tokuda+KORG Inc.>JPN>company;Wataru Hariya+KORG Inc.>JPN>company;Koji Oishi+KORG Inc.>JPN>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","We introduce good-sounds.org, a community driven framework based on freesound.org to explore the concept of goodness in instrumental sounds. Goodness is considered here as the common agreed basic sound quality of an instrument without taking into consideration musical expressiveness. Musicians upload their sounds and vote on existing sounds, and from the collected data the system is able to develop sound goodness measures of relevance for music education applications. The core of the system is a database of sounds, together with audio features extracted from them using MTG’s Essentia library and user annotations related to the goodness of the sounds. The web front-end provides useful data visualizations of the sound attributes and tools to facilitate user interaction. To evaluate the framework, we carried out an experiment to rate sound goodness of single notes of nine orchestral instruments. In it, users rated the sounds using an AB vote over a set of sound attributes defined to be of relevance in the characterization of single notes of instrumental sounds. With the obtained votes we built a ranking of the sounds for each attribute and developed a model that rates the goodness for each of the selected sound attributes. Using this approach, we have succeeded in obtaining results comparable to a model that was built from expert generated evaluations.",ESP,education,Developed economies,"[-16.87486, 9.235005]","[2.6028154, 34.201645]","[-13.651189, -9.82355, -9.801231]","[-6.314255, -7.0000925, 17.642082]","[13.189693, 7.2322564]","[10.411319, 0.76467717]","[13.936056, 13.41765, -1.5945779]","[11.075003, 5.2278748, 11.666886]"
56,Thomas Hedges;Geraint A. Wiggins,Improving Predictions of Derived Viewpoints in Multiple Viewpoints Systems.,2016,https://doi.org/10.5281/zenodo.1416630,Thomas Hedges+Queen Mary University of London>GBR>education|Queen Mary University of London>GBR>education;Geraint Wiggins+Queen Mary University of London>GBR>education|Queen Mary University of London>GBR>education,"This paper presents and tests a method for improving the predictive power of derived viewpoints in multiple viewpoints systems. Multiple viewpoint systems are a well established method for the statistical modelling of sequential symbolic musical data. A useful class of viewpoints known as derived viewpoints map symbols from a basic event space to a viewpoint-specific domain. Probability estimates are calculated in the derived viewpoint domain before an inverse function maps back to the basic event space to complete the model. Since an element in the derived viewpoint domain can potentially map onto multiple basic elements, probability mass is distributed between the basic elements with a uniform distribution. As an alternative, this paper proposes a distribution weighted by zero-order frequencies of the basic elements to inform this probability mapping. Results show this improves the predictive performance for certain derived viewpoints, allowing them to be selected in viewpoint selection.",GBR,education,Developed economies,"[16.214502, -5.4765515]","[-6.2527733, -30.42974]","[-1.9782209, -27.39129, 18.072824]","[-17.434107, -0.43285108, -2.9815092]","[10.629355, 8.930286]","[8.127961, 2.3384025]","[11.907317, 13.713213, -0.1838854]","[9.093695, 6.4417768, 10.812468]"
57,T. J. Tsai;Thomas Prätzlich;Meinard Müller,Known Artist Live Song ID: A Hashprint Approach.,2016,https://doi.org/10.5281/zenodo.1418223,TJ Tsai+University of California Berkeley>USA>education;Thomas Prätzlich+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"The goal of live song identification is to recognize a song based on a short, noisy cell phone recording of a live performance. We propose a system for known-artist live song identification and provide empirical evidence of its feasibility. The proposed system represents audio as a sequence of hashprints, which are binary fingerprints that are derived from applying a set of spectro-temporal filters to a spectrogram representation. The spectro-temporal filters can be learned in an unsupervised manner on a small amount of data, and can thus tailor its representation to each artist. Matching is performed using a cross-correlation approach with downsampling and rescoring. We evaluate our approach on the Gracenote live song identification benchmark data set, and compare our results to five other baseline systems. Compared to the previous state-of-the-art, the proposed system improves the mean reciprocal rank from .68 to .79, while simultaneously reducing the average runtime per query from 10 seconds down to 0.9 seconds.",USA,education,Developed economies,"[5.423582, 43.365192]","[24.32625, -20.00229]","[3.1745, 15.165802, -19.686724]","[18.261019, -10.828497, 2.2768316]","[15.880913, 11.266193]","[8.423196, 0.054922476]","[12.832813, 17.305838, -0.30951056]","[10.104653, 5.371463, 12.721283]"
58,Il-Young Jeong;Kyogu Lee,Learning Temporal Features Using a Deep Neural Network and its Application to Music Genre Classification.,2016,https://doi.org/10.5281/zenodo.1416416,Il-Young Jeong+Seoul National University>KOR>education|Music and Audio Research Group>Unknown>Unknown;Kyogu Lee+Seoul National University>KOR>education|Music and Audio Research Group>Unknown>Unknown,"In this paper, we describe a framework for temporal feature learning from audio with a deep neural network, and apply it to music genre classification. To this end, we revisit the conventional spectral feature learning framework, and reformulate it in the cepstral modulation spectrum domain, which has been successfully used in many speech and music-related applications for temporal feature extraction. Experimental results using the GTZAN dataset show that the temporal features learned from the proposed method are able to obtain classification accuracy comparable to that of the learned spectral features.",KOR,education,Developing economies,"[-27.53884, -10.8501835]","[19.244198, -15.286295]","[-12.56679, 6.275796, 12.752352]","[18.960701, 3.971304, -9.510043]","[12.355916, 10.346848]","[9.606278, 3.7539868]","[13.569638, 13.72818, 1.0177444]","[11.272341, 7.336841, 10.538039]"
59,W. Bas de Haas;Anja Volk,Meter Detection in Symbolic Music Using Inner Metric Analysis.,2016,https://doi.org/10.5281/zenodo.1417345,W. Bas de Haas+Utrecht University>NLD>education|Utrecht University>Unknown>Unknown;Anja Volk+Utrecht University>NLD>education|Utrecht University>Unknown>Unknown,"In this paper we present PRIMA: a new model tailored to symbolic music that detects the meter and the first downbeat position of a piece. Given onset data, the metrical structure of a piece is interpreted using the Inner Metric Analysis (IMA) model. IMA identifies the strong and weak metrical positions in a piece by performing a periodicity analysis, resulting in a weight profile for the entire piece. Next, we reduce IMA to a feature vector and model the detection of the meter and its first downbeat position probabilistically. In order to solve the meter detection problem effectively, we explore various feature selection and parameter optimisation strategies, including Genetic, Maximum Likelihood, and Expectation-Maximisation algorithms. PRIMA is evaluated on two datasets of MIDI files: a corpus of ragtime pieces, and a newly assembled pop dataset. We show that PRIMA outperforms autocorrelation-based meter detection as implemented in the MIDItoolbox on these datasets.",NLD,education,Developed economies,"[10.538687, 16.90498]","[-21.620115, -2.9414325]","[-11.922234, -11.398848, 3.3399336]","[-2.0860312, 13.920177, -10.178465]","[11.932937, 6.6378517]","[5.848104, 2.0095563]","[12.769579, 13.106451, -1.5083253]","[8.372251, 6.9366107, 11.140293]"
60,Gen Hori;Shigeki Sagayama,Minimax Viterbi Algorithm for HMM-Based Guitar Fingering Decision.,2016,https://doi.org/10.5281/zenodo.1417639,Gen Hori+Asia University>JPN>education|RIKEN>JPN>facility;Shigeki Sagayama+Meiji University>JPN>education,"Previous works on automatic fingering decision for string instruments have been mainly based on path optimization by minimizing the difficulty of a whole phrase that is typically defined as the sum of the difficulties of moves required for playing the phrase. However, from a practical viewpoint of beginner players, it is more important to minimize the maximum difficulty of a move required for playing the phrase, that is, to make the most difficult move easier. To this end, we introduce a variant of the Viterbi algorithm (termed the “minimax Viterbi algorithm”) that finds the path of the hidden states that maximizes the minimum transition probability (not the product of the transition probabilities) and apply it to HMM-based guitar fingering decision. We compare the resulting fingerings by the conventional Viterbi algorithm and our proposed minimax Viterbi algorithm to show the appropriateness of our new method.",JPN,education,Developed economies,"[50.019005, -11.073127]","[-44.21584, -4.263977]","[28.215021, -9.220355, 8.715035]","[-16.650995, -10.842988, -2.7539718]","[7.5449653, 8.218133]","[7.149975, 4.1186237]","[11.8091135, 11.263351, 1.5812029]","[9.04848, 6.800813, 10.262159]"
61,João Paulo V. Cardoso;Luciana Fujii Pontello;Pedro H. F. Holanda;Bruno Guilherme;Olga Goussevskaia;Ana Paula Couto da Silva,Mixtape: Direction-Based Navigation in Large Media Collections.,2016,https://doi.org/10.5281/zenodo.1416072,João Paulo V. Cardoso+Universidade Federal de Minas Gerais (UFMG)>BRA>education;Luciana Fujii Pontello+Universidade Federal de Minas Gerais (UFMG)>BRA>education;Pedro H. F. Holanda+Universidade Federal de Minas Gerais (UFMG)>BRA>education;Bruno Guilherme+Universidade Federal de Minas Gerais (UFMG)>BRA>education;Olga Goussevskaia+Universidade Federal de Minas Gerais (UFMG)>BRA>education;Ana Paula C. da Silva+Universidade Federal de Minas Gerais (UFMG)>BRA>education,"In this work we explore the increasing demand for novel user interfaces to navigate large media collections. We implement a scalable data structure to store and retrieve similarity information and propose a novel navigation framework that uses geometric vector operations and real-time user feedback to direct the outcome. In particular, we implement this framework in the domain of music. To evaluate the effectiveness of the navigation process, we propose an automatic evaluation framework, based on synthetic user profiles, which allows to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape, a simple web application that allows users to create playlists by providing real-time feedback through liking and skipping patterns.",BRA,education,Developing economies,"[-15.32631, 28.582895]","[31.275997, 20.771515]","[-8.228646, 5.811873, -22.052578]","[14.522773, -0.9185343, 22.433296]","[14.163925, 7.410685]","[11.654235, 1.5570402]","[14.196375, 14.241769, -2.5154953]","[12.688012, 5.243234, 13.127795]"
62,Ryo Nishikimi;Eita Nakamura;Katsutoshi Itoyama;Kazuyoshi Yoshii,Musical Note Estimation for F0 Trajectories of Singing Voices Based on a Bayesian Semi-Beat-Synchronous HMM.,2016,https://doi.org/10.5281/zenodo.1418023,Ryo Nishikimi+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"This paper presents a statistical method that estimates a sequence of discrete musical notes from a temporal trajectory of vocal F0s. Since considerable effort has been devoted to estimate the frame-level F0s of singing voices from music audio signals, we tackle musical note estimation for those F0s to obtain a symbolic musical score. A naïve approach to musical note estimation is to quantize the vocal F0s at a semitone level in every time unit (e.g., half beat). This approach, however, fails when the vocal F0s are significantly deviated from those specified by a musical score. The onsets of musical notes are often delayed or advanced from beat times and the vocal F0s fluctuate according to singing expressions. To deal with these deviations, we propose a Bayesian hidden Markov model that allows musical notes to change in semi-synchronization with beat times. Both the semitone-level F0s and onset deviations of musical notes are regarded as latent variables and the frequency deviations are modeled by an emission distribution. The musical notes and their onset and frequency deviations are jointly estimated by using Gibbs sampling. Experimental results showed that the proposed method improved the accuracy of musical note estimation against baseline methods.",JPN,education,Developed economies,"[-5.0654163, -30.89067]","[-16.347452, -6.3492002]","[16.0996, 6.8981075, -11.297904]","[0.79334503, -3.6483293, -12.459215]","[10.009504, 10.702257]","[6.0881896, 2.596933]","[11.304642, 14.711769, 0.46613753]","[8.30748, 7.154004, 10.909858]"
63,Maria Panteli;Simon Dixon,On the Evaluation of Rhythmic and Melodic Descriptors for Music Similarity.,2016,https://doi.org/10.5281/zenodo.1417555,Maria Panteli+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"In exploratory studies of large music collections where often no ground truth is available, it is essential to evaluate the suitability of the underlying methods prior to drawing any conclusions. In this study we focus on the evaluation of audio features that can be used for rhythmic and melodic content description and similarity estimation. We select a set of state-of-the-art rhythmic and melodic descriptors and assess their invariance with respect to transformations of timbre, recording quality, tempo and pitch. We create a dataset of synthesised audio and investigate which features are invariant to the aforementioned transformations and whether invariance is affected by characteristics of the music style and the monophonic or polyphonic character of the audio recording. From the descriptors tested, the scale transform performed best for rhythm classification and retrieval and pitch bihistogram performed best for melody. The proposed evaluation strategy can inform decisions in the feature design process leading to significant improvement in the reliability of the features.",GBR,education,Developed economies,"[0.52972066, 15.126833]","[12.241709, -5.133963]","[-1.8562946, 6.116811, 0.13292257]","[6.98176, 5.581741, -2.6686497]","[12.567253, 9.596852]","[8.960012, 2.7486408]","[12.887347, 15.338813, -0.806539]","[10.7277355, 7.1159296, 11.2753725]"
64,Rainer Kelz;Matthias Dorfer;Filip Korzeniowski;Sebastian Böck;Andreas Arzt;Gerhard Widmer,On the Potential of Simple Framewise Approaches to Piano Transcription.,2016,https://doi.org/10.5281/zenodo.1416488,Rainer Kelz+Johannes Kepler University Linz>AUT>education;Matthias Dorfer+Johannes Kepler University Linz>AUT>education;Filip Korzeniowski+Johannes Kepler University Linz>AUT>education;Sebastian Böck+Johannes Kepler University Linz>AUT>education;Andreas Arzt+Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,"""In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset – without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve.""",AUT,education,Developed economies,"[31.74412, -5.451346]","[-29.66429, -25.289082]","[15.192976, -5.427502, 16.590769]","[-7.643768, -9.095143, -9.240971]","[9.692536, 7.2834587]","[7.7650123, 5.395733]","[12.034163, 11.423018, -0.25068256]","[9.022691, 6.5871544, 9.240822]"
65,Jeff Gregorio;Youngmoo Kim,Phrase-Level Audio Segmentation of Jazz Improvisations Informed by Symbolic Data.,2016,https://doi.org/10.5281/zenodo.1414790,Jeff Gregorio+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"""Computational music structure analysis encompasses any model attempting to organize music into qualitatively salient structural units, which can include anything in the hierarchy of large scale form, down to individual phrases and notes. While much existing audio-based segmentation work attempts to capture repetition and homogeneity cues useful at the form and thematic level, the time scales involved in phrase-level segmentation and the avoidance of repetition in improvised music necessitate alternate approaches in approaching jazz structure analysis. Recently, the Weimar Jazz Database has provided transcriptions of solos by a variety of eminent jazz performers. Utilizing a subset of these transcriptions aligned to their associated audio sources, we propose a model based on supervised training of a Hidden Markov Model with ground-truth state sequences designed to encode melodic contours appearing frequently in jazz improvisations. Results indicate that representing likely melodic contours in this way allows a low-level audio feature set containing primarily timbral and harmonic information to more accurately predict phrase boundaries.""",USA,education,Developed economies,"[7.648057, 13.082346]","[-12.78782, 8.05]","[-6.499862, -3.3336341, 30.011156]","[-0.38696185, -5.659246, -8.758566]","[10.58327, 9.545949]","[7.2087607, 3.3389049]","[12.066606, 14.452307, -0.5655477]","[9.575232, 7.8531256, 11.665976]"
66,Bob L. Sturm,Revisiting Priorities: Improving MIR Evaluation Practices.,2016,https://doi.org/10.5281/zenodo.1416726,Bob L. Sturm+Queen Mary University of London>GBR>education,"While there is a consensus that evaluation practices in music informatics (MIR) must be improved, there is no consensus about what should be prioritised in order to do so. Priorities include: 1) improving data; 2) improving figures of merit; 3) employing formal statistical testing; 4) employing cross-validation; and/or 5) implementing transparent, central and immediate evaluation. In this position paper, I argue how these priorities treat only the symptoms of the problem and not its cause: MIR lacks a formal evaluation framework relevant to its aims. I argue that the principal priority is to adapt and integrate the formal design of experiments (DOE) into the MIR research pipeline. Since the aim of DOE is to help one produce the most reliable evidence at the least cost, it stands to reason that DOE will make a significant contribution to MIR. Accomplishing this, however, will not be easy, and will require far more effort than is currently being devoted to it.",GBR,education,Developed economies,"[-7.5940194, 57.857254]","[26.116653, 44.46075]","[-33.54393, -0.44470775, -4.417847]","[1.6474785, 6.666526, 12.366922]","[13.598596, 4.6970663]","[11.837383, 0.6264543]","[15.0633745, 11.138478, -1.531977]","[12.260123, 4.4376206, 12.209722]"
109,Colin Raffel;Daniel P. W. Ellis,Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.,2016,https://doi.org/10.5281/zenodo.1418233,Colin Raffel+Columbia University>USA>education|LabROSA>USA>facility;Daniel P. W. Ellis+Columbia University>USA>education|LabROSA>USA>facility,MIDI files abound and provide a bounty of information for music informatics. We enumerate the types of information available in MIDI files and describe the steps necessary for utilizing them. We also quantify the reliability of this data by comparing it to human-annotated ground truth. The results suggest that developing better methods to leverage information present in MIDI files will facilitate the creation of MIDI-derived ground truth for audio content-based MIR.,USA,education,Developed economies,"[39.89883, -0.55890787]","[-7.661152, 10.818492]","[6.983743, -10.545645, 23.112055]","[-3.9604623, -13.918854, -2.0773885]","[10.324764, 7.0808234]","[7.423021, 1.6812754]","[12.586435, 11.535718, -0.69134]","[9.808282, 5.821707, 10.981905]"
67,Prem Seetharaman;Bryan Pardo,Simultaneous Separation and Segmentation in Layered Music.,2016,https://doi.org/10.5281/zenodo.1417987,Prem Seetharaman+Northwestern University>USA>education|Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"In many pieces of music, the composer signals how individual sonic elements (samples, loops, the trumpet section) should be grouped by introducing sources or groups in a layered manner. We propose to discover and leverage the layering structure and use it for both structural segmentation and source separation. We use reconstruction error from non-negative matrix factorization (NMF) to guide structure discovery. Reconstruction error spikes at moments of significant sonic change. This guides segmentation and also lets us group basis sets for NMF. The number of sources, the types of sources, and when the sources are active are not known in advance. The only information is a specific type of layering structure. There is no separate training phase to learn a good basis set. No prior seeding of the NMF matrices is required. Unlike standard approaches to NMF there is no need for a post-processor to partition the learned basis functions by group. Source groups are learned automatically from the data. We evaluate our method on mixtures consisting of looping source groups. This separation approach outperforms a standard clustering NMF source separation approach on such mixtures. We find our segmentation approach is competitive with state-of-the-art segmentation methods on this dataset.",USA,education,Developed economies,"[6.455415, -43.846962]","[-46.84398, -26.740175]","[26.0183, -0.8103888, -3.3206217]","[-9.309846, -11.096102, -31.413887]","[8.549829, 9.998012]","[6.3072567, 5.1766434]","[11.130014, 13.691011, 1.5273088]","[9.892157, 8.800235, 9.851305]"
68,Patricio López-Serrano;Christian Dittmar;Jonathan Driedger;Meinard Müller,Towards Modeling and Decomposing Loop-Based Electronic Music.,2016,https://doi.org/10.5281/zenodo.1417999,Patricio López-Serrano+International Audio Laboratories Erlangen>DEU>facility;Christian Dittmar+International Audio Laboratories Erlangen>DEU>facility;Jonathan Driedger+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Electronic Music (EM) is a popular family of genres which has increasingly received attention as a research subject in the field of MIR. A fundamental structural unit in EM are loops—audio fragments whose length can span several seconds. The devices commonly used to produce EM, such as sequencers and digital audio workstations, impose a musical structure in which loops are repeatedly triggered and overlayed. This particular structure allows new perspectives on well-known MIR tasks. In this paper we first review a prototypical production technique for EM from which we derive a simplified model. We then use our model to illustrate approaches for the following task: given a set of loops that were used to produce a track, decompose the track by finding the points in time at which each loop was activated. To this end, we repurpose established MIR techniques such as fingerprinting and non-negative matrix factor deconvolution.",DEU,facility,Developed economies,"[26.739801, 7.6599164]","[-52.184273, -20.204626]","[5.9498396, -3.697461, 23.691578]","[-5.254823, -17.468172, -23.12991]","[10.262659, 7.9684625]","[6.012877, 4.588664]","[13.114539, 11.726169, -0.10222534]","[10.1759, 5.890071, 10.754449]"
69,Jorge Calvo-Zaragoza;David Rizo;José Manuel Iñesta Quereda,Two (Note) Heads Are Better Than One: Pen-Based Multimodal Interaction with Music Scores.,2016,https://doi.org/10.5281/zenodo.1417469,Jorge Calvo-Zaragoza+University of Alicante>ESP>education;David Rizo+University of Alicante>ESP>education;Jose M. Iñesta+University of Alicante>ESP>education,"Digitizing early music sources requires new ways of dealing with musical documents. Assuming that current technologies cannot guarantee a perfect automatic transcription, our intention is to develop an interactive system in which user and software collaborate to complete the task. Since conventional score post-editing might be tedious, the user is allowed to interact using an electronic pen. Although this provides a more ergonomic interface, this interaction must be decoded as well. In our framework, the user traces the symbols using the electronic pen over a digital surface, which provides both the underlying image (offline data) and the drawing made by the e-pen (online data) to improve classification. Applying this methodology over 70 scores of the target musical archive, a dataset of 10,230 bimodal samples of 30 different symbols was obtained and made available for research purposes. This paper presents experimental results on classification over this dataset, in which symbols are recognized by combining the two modalities. This combination of modes has demonstrated its good performance, decreasing the error rate of using each modality separately and achieving an almost error-free performance.",ESP,education,Developed economies,"[-23.23667, 14.805598]","[-25.318521, 34.531532]","[-17.162899, 1.9063932, -23.366915]","[-11.994328, -17.75082, -0.7128397]","[12.56993, 6.6161423]","[6.8612976, -0.5645533]","[13.36699, 13.236632, -2.1481683]","[8.131228, 4.4801683, 10.684156]"
70,Christof Weiß;Vlora Arifi-Müller;Thomas Prätzlich;Rainer Kleinertz;Meinard Müller,Analyzing Measure Annotations for Western Classical Music Recordings.,2016,https://doi.org/10.5281/zenodo.1417449,"Christof Weiß+International Audio Laboratories Erlangen>DEU>education;Vlora Ariﬁ-M¨uller+International Audio Laboratories Erlangen>DEU>education;Thomas Pr¨atzlich+International Audio Laboratories Erlangen>DEU>education;Rainer Kleinertz+Institut für Musikwissenschaft, Saarland University>DEU>education;Meinard M¨uller+International Audio Laboratories Erlangen>DEU>education","This paper approaches the problem of annotating measure positions in Western classical music recordings. Such annotations can be useful for navigation, segmentation, and cross-version analysis of music in different types of representations. In a case study based on Wagner’s opera “Die Walküre”, we analyze two types of annotations. First, we report on an experiment where several human listeners generated annotations in a manual fashion. Second, we examine computer-generated annotations which were obtained by using score-to-audio alignment techniques. As one main contribution of this paper, we discuss the inconsistencies of the different annotations and study possible musical reasons for deviations. As another contribution, we propose a kernel-based method for automatically estimating confidences of the computed annotations which may serve as a first step towards improving the quality of this automatic method.",DEU,education,Developed economies,"[8.330951, 16.61122]","[-1.0620328, 9.426065]","[-12.003792, -9.436573, 0.71714294]","[-5.884868, 3.1209004, 3.0633967]","[12.362629, 7.4455905]","[8.463989, 2.3111274]","[12.97491, 13.626602, -1.4234116]","[10.161174, 6.606932, 11.489827]"
71,David Lewis 0001;Tim Crawford;Daniel Müllensiefen,Instrumental Idiom in the 16th Century: Embellishment Patterns in Arrangements of Vocal Music.,2016,https://doi.org/10.5281/zenodo.1415964,"David Lewis+Goldsmiths, University of London>GBR>education;Tim Crawford+Goldsmiths, University of London>GBR>education;Daniel Müllensiefen+Goldsmiths, University of London>GBR>education","Much surviving 16th-century instrumental music consists of arrangements (‘intabulations’) of vocal music, in tablature for solo lute. Intabulating involved deciding what to omit from a score to fit the instrument, and making it fit under the hand. Notes were usually added as embellishments to the original plain score, using idiomatic patterns, typically at cadences, but often filling simple intervals in the vocal parts with faster notes. Here we test whether such patterns are both characteristic of lute intabulations as a class (vs original lute music) and of different genres within that class. We use patterns identified in the musicological literature to search two annotated corpora of encoded lute music using the SIA(M)ESE algorithm. Diatonic patterns occur in many chromatic forms, accidentals being added depending how the arranger applied the conventions of musica ficta. Rhythms must be applied at three different scales as notation is inconsistent across the repertory. This produced over 88,000 short melodic queries to search in two corpora totalling just over 6,000 encodings of lute pieces. We show that our method clearly discriminates between intabulations and original music for the lute (p < .001); it also can distinguish sacred and secular genres within the vocal models (p < .001).",GBR,education,Developed economies,"[5.928474, 21.146896]","[-7.2671676, 15.691664]","[3.261081, 1.7333603, 3.5272815]","[-6.360656, 9.271386, 4.7732425]","[11.950293, 9.914892]","[8.364542, 1.4272741]","[12.424744, 15.172128, -1.0462738]","[9.700623, 6.3549633, 12.319997]"
72,Andre Holzapfel;Emmanouil Benetos,The Sousta Corpus: Beat-Informed Automatic Transcription of Traditional Dance Tunes.,2016,https://doi.org/10.5281/zenodo.1416938,Andre Holzapfel+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Emmanouil Benetos+Queen Mary University of London>GBR>education,"In this paper, we present a new corpus for research in computational ethnomusicology and automatic music transcription, consisting of traditional dance tunes from Crete. This rich dataset includes audio recordings, scores transcribed by ethnomusicologists and aligned to the audio performances, and meter annotations. A second contribution of this paper is the creation of an automatic music transcription system able to support the detection of multiple pitches produced by lyra (a bowed string instrument). Furthermore, the transcription system is able to cope with deviations from standard tuning, and provides temporally quantized notes by combining the output of the multi-pitch detection stage with a state-of-the-art meter tracking algorithm. Experiments carried out for note tracking using 25ms onset tolerance reach 41.1% using information from the multi-pitch detection stage only, 54.6% when integrating beat information, and 57.9% when also supporting tuning estimation. The produced meter aligned transcriptions can be used to generate staff notation, a fact that increases the value of the system for studies in ethnomusicology.",AUT,facility,Developed economies,"[-24.577143, -23.831646]","[-9.046635, -12.443376]","[-9.025561, -22.440443, -5.994705]","[-0.764532, -10.000994, -9.209816]","[11.627261, 5.1953664]","[6.3495173, 2.4198992]","[11.603834, 13.775288, -2.2636037]","[8.648595, 7.221143, 10.949585]"
73,Maria Panteli;Emmanouil Benetos;Simon Dixon,Learning a Feature Space for Similarity in World Music.,2016,https://doi.org/10.5281/zenodo.1415216,Maria Panteli+Queen Mary University of London>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"In this study we investigate computational methods for assessing music similarity in world music. We use state-of-the-art audio features to describe musical content in world music recordings. Our music collection is a subset of the Smithsonian Folkways Recordings with audio examples from 31 countries from around the world. Using supervised and unsupervised dimensionality reduction techniques we learn feature representations for music similarity. We evaluate how well music styles separate in this learned space with a classification experiment. We obtained moderate performance classifying the recordings by country. Analysis of misclassifications revealed cases of geographical or cultural proximity. We further evaluate the learned space by detecting outliers, i.e. identifying recordings that stand out in the collection. We use a data mining technique based on Mahalanobis distances to detect outliers and perform a listening experiment in the ‘odd one out’ style to evaluate our findings. We are able to detect, amongst others, recordings of non-musical content as outliers as well as music with distinct timbral and harmonic content. The listening experiment reveals moderate agreement between subjects’ ratings and our outlier estimation.",GBR,education,Developed economies,"[-7.62296, 12.96982]","[24.562853, -0.26963577]","[-3.2826347, 12.850473, 0.8594582]","[11.481166, 5.359334, 2.5137115]","[12.895912, 9.127565]","[10.584938, 2.732111]","[13.572907, 14.636478, -0.3510007]","[12.141505, 6.6812296, 11.897586]"
74,Oriol Nieto;Juan Pablo Bello,Systematic Exploration of Computational Music Structure Research.,2016,https://doi.org/10.5281/zenodo.1417661,"Oriol Nieto+Pandora Media, Inc.>USA>company;Juan Pablo Bello+New York University>USA>education","In this work we present a framework containing open source implementations of multiple music structural segmentation algorithms and employ it to explore the hyper parameters of features, algorithms, evaluation metrics, datasets, and annotations of this MIR task. Besides testing and discussing the relative importance of the moving parts of the computational music structure eco-system, we also shed light on its current major challenges. Additionally, a new dataset containing multiple structural annotations for tracks that are particularly ambiguous to analyze is introduced, and used to quantify the impact of specific annotators when assessing automatic approaches to this task. Results suggest that more than one annotation per track is necessary to fully address the problem of ambiguity in music structure research.",USA,company,Developed economies,"[2.0988073, 4.98257]","[-2.7075536, 6.5163727]","[-6.293882, -6.262301, 5.372018]","[-2.8595908, 0.24812627, 1.5468407]","[12.124833, 8.227101]","[8.577546, 2.6846876]","[13.026111, 13.688778, -0.7582219]","[10.486411, 6.917406, 11.357096]"
75,Jordan B. L. Smith;Masataka Goto,Using Priors to Improve Estimates of Music Structure.,2016,https://doi.org/10.5281/zenodo.1416916,Jordan B. L. Smith+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Existing collections of annotations of musical structure possess many strong regularities: for example, the lengths of segments are approximately log-normally distributed, as is the number of segments per annotation; and the lengths of two adjacent segments are highly likely to have an integer ratio. Since many aspects of structural annotations are highly regular, but few of these regularities are taken into account by current algorithms, we propose several methods of improving predictions of musical structure by using their likelihood according to prior distributions. We test the use of priors to improve a committee of basic segmentation algorithms, and to improve a committee of cutting-edge approaches submitted to MIREX. In both cases, we are unable to improve on the best committee member, meaning that our proposed approach is outperformed by simple parameter tuning. The same negative result was found despite incorporating the priors in multiple ways. To explain the result, we show that although there is a correlation overall between output accuracy and prior likelihood, the weakness of the correlation in the high-likelihood region makes the proposed method infeasible. We suggest that to improve on the state of the art using prior likelihoods, these ought to be incorporated at a deeper level of the algorithm.",JPN,facility,Developed economies,"[-0.5789992, 4.9955096]","[-2.5413177, 7.557234]","[-3.70349, -4.0036616, 2.6672962]","[-4.882683, 0.46337277, 1.653005]","[12.224972, 8.306594]","[8.5941105, 2.550983]","[13.117922, 13.903101, -0.69653535]","[10.410106, 6.836742, 11.420976]"
76,Mi Tian;Mark B. Sandler,Music Structural Segmentation Across Genres with Gammatone Features.,2016,https://doi.org/10.5281/zenodo.1417713,"Mi Tian+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark B. Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","Music structural segmentation (MSS) studies to date mainly employ audio features describing the timbral, harmonic or rhythmic aspects of the music and are evaluated using datasets consisting primarily of Western music. A new dataset of Chinese traditional Jingju music with structural annotations is introduced in this paper to complement the existing evaluation framework. We discuss some statistics of the annotations analysing the inter-annotator agreements. We present two auditory features derived from the Gammatone filters based respectively on the cepstral analysis and the spectral contrast description. The Gammatone features and two commonly used features, Mel-frequency cepstral coefficients (MFCCs) and chromagram, are evaluated on the Jingju dataset as well as two existing used ones using several state-of-the-art algorithms. The investigated Gammatone features outperform MFCCs and chromagram when evaluated on the Jingju dataset and show similar performance with the Western datasets. We identify the presented Gammatone features as effective structure descriptors, especially for music lacking notable timbral or harmonic sectional variations. Results also indicate that the design of audio features and segmentation algorithms should be adapted to specific music genres to interpret individual structural patterns.",GBR,education,Developed economies,"[-2.2396662, -3.3158405]","[-5.094013, 1.3852717]","[5.052023, -4.8912954, -2.3312118]","[9.652879, -4.2370596, -2.5142395]","[11.690055, 8.268503]","[8.015784, 2.5100899]","[12.4836645, 14.156205, 0.090401076]","[10.231404, 7.69647, 11.783955]"
77,Juan J. Bosch;Rachel M. Bittner;Justin Salamon;Emilia Gómez,A Comparison of Melody Extraction Methods Based on Source-Filter Modelling.,2016,https://doi.org/10.5281/zenodo.1418167,"Juan J. Bosch+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rachel M. Bittner+Music and Audio Research Laboratory, New York University>USA>education;Justin Salamon+Music and Audio Research Laboratory, New York University>USA>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education","This work explores the use of source-filter models for pitch salience estimation and their combination with different pitch tracking and voicing estimation methods for automatic melody extraction. Source-filter models are used to create a mid-level representation of pitch that implicitly incorporates timbre information. The spectrogram of a musical audio signal is modelled as the sum of the leading voice (produced by human voice or pitched musical instruments) and accompaniment. The leading voice is then modelled with a Smoothed Instantaneous Mixture Model (SIMM) based on a source-filter model. The main advantage of such a pitch salience function is that it enhances the leading voice even without explicitly separating it from the rest of the signal. We show that this is beneficial for melody extraction, increasing pitch estimation accuracy and reducing octave errors in comparison with simpler pitch salience functions. The adequate combination with voicing detection techniques based on pitch contour characterisation leads to significant improvements over state-of-the-art methods, for both vocal and instrumental music.",ESP,education,Developed economies,"[5.8736873, -12.3636265]","[-5.1362653, -10.895352]","[15.312713, 6.520452, -3.9779327]","[3.7655802, -11.557692, -13.024159]","[10.098312, 9.956972]","[6.7809834, 2.595303]","[11.026728, 14.940721, -0.44555828]","[9.027159, 7.957765, 10.970188]"
79,Tian Cheng 0001;Matthias Mauch;Emmanouil Benetos;Simon Dixon,An Attack/Decay Model for Piano Transcription.,2016,https://doi.org/10.5281/zenodo.1418153,Tian Cheng+Queen Mary University of London>GBR>education;Matthias Mauch+Queen Mary University of London>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activation that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcription is performed in a supervised way, with the training and test datasets produced by the same piano. First we train parameters for the attack and decay components on isolated notes, then update only the note activations for transcription. Experiments show that the proposed model achieves 82% on note-wise and 79% on frame-wise F-measures on the ‘ENSTDkCl’ subset of the MAPS database, outperforming the current published state of the art.",GBR,education,Developed economies,"[31.813452, -6.1417413]","[-30.198961, -25.406097]","[13.42569, -6.414468, 16.709597]","[-6.95015, -9.193326, -10.14867]","[9.726785, 7.26124]","[6.5571046, 4.862282]","[12.0501, 11.365406, -0.20559391]","[9.204064, 8.211501, 9.648393]"
0,Sergio Oramas;Oriol Nieto;Francesco Barbieri;Xavier Serra,"Multi-Label Music Genre Classification from Audio, Text and Images Using Deep Features.",2017,https://doi.org/10.5281/zenodo.1417427,"Sergio Oramas+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Oriol Nieto+Pandora Media Inc.>USA>company;Francesco Barbieri+TALN Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Music genres allow to categorize musical items that share common characteristics. Although these categories are not mutually exclusive, most related research is traditionally focused on classifying tracks into a single class. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for certain applications. In this work we aim to expand this task by categorizing musical items into multiple and fine-grained labels, using three different data modalities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes. For every album we have collected the cover image, text reviews, and audio tracks. Additionally, we propose an approach for multi-label genre classification based on the combination of feature embeddings learned with state-of-the-art deep learning methodologies. Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields improved results.",ESP,education,Developed economies,"[-28.545532, -10.969872]","[31.272251, -1.7403826]","[-14.353877, 6.6733694, 12.548132]","[17.734066, 9.617996, 0.8771402]","[13.04354, 10.942348]","[10.77615, 3.6149278]","[14.103996, 14.149069, 1.2970977]","[12.388903, 6.3225183, 10.834891]"
1,Jeff Ens;Bernhard E. Riecke;Philippe Pasquier,The Significance of the Low Complexity Dimension in Music Similarity Judgements.,2017,https://doi.org/10.5281/zenodo.1416400,Jeff Ens+Simon Fraser University>CAN>education|Simon Fraser University>CAN>education|Simon Fraser University>CAN>education;Bernhard E. Riecke+Simon Fraser University>CAN>education|Simon Fraser University>CAN>education|Simon Fraser University>CAN>education;Philippe Pasquier+Simon Fraser University>CAN>education|Simon Fraser University>CAN>education|Simon Fraser University>CAN>education,"Previous research has demonstrated that similarity judgements are context specific, as they are shaped by cultural exposure, familiarity, and the musical aesthetic of the content being compared. Although such research suggests that the criterion for similarity judgement varies with respect to the musical style of the content being compared, the specific musical factors which shape this criterion are unknown. Since dimensional complexity differentiates musical genres, and has been shown to affect similarity judgements following lifelong exposure, this experiment investigates the short-term influence of dimensional complexity on similarity judgements. Rhythmic and pitch sequences with two levels of complexity were factorially combined to create four distinct types of prototype melodies. 51 participants rated the similarity of each type of prototype melody (M) to two variations, one in which the pitch content was modified ( ¯ Mp), and another in which the rhythmic content was modified ( ¯ Mr). The results indicate that rhythm and pitch complexity both play a significant role, influencing the perceived similarity of ¯ Mp, and ¯ Mr. The dimension bearing low complexity information was found to be the predominant factor in similarity judgements, as participants found modifications to this dimension to significantly decrease perceived similarity.",CAN,education,Developed economies,"[-1.6407619, 12.250771]","[19.437708, 1.3953766]","[-5.7905283, 2.3841891, 3.1251917]","[7.064906, 7.549932, 5.8204923]","[12.9667635, 9.391734]","[9.80083, 2.2627532]","[13.397552, 15.256894, -0.7458731]","[11.550278, 7.0831804, 12.92742]"
75,Jiajie Dai;Simon Dixon,Analysis of Interactive Intonation in Unaccompanied SATB Ensembles.,2017,https://doi.org/10.5281/zenodo.1418327,Jiajie Dai+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Unaccompanied ensemble singing is common in many musical cultures, yet it requires great skill for singers to listen to each other and adjust their pitch to stay in tune. The aim of this research is to investigate interaction in four-part (SATB) singing from the point of view of pitch accuracy (intonation). In particular we compare intonation accuracy of individual singers and collaborative ensembles. 20 participants (five groups of four) sang two pieces of music in three different listening conditions: solo, with one vocal part missing and with all vocal parts. After semi-automatic pitch extraction and manual correction, we annotated the recordings and calculated the pitch error, melodic interval error, harmonic interval error and note stability. We observed significant differences between individual and interactional intonation, more specifically: 1) Singing without the bass part has less mean absolute pitch error than singing with all vocal parts; 2) Mean absolute melodic interval error increases when participants can hear the other parts; 3) Mean absolute harmonic interval error is higher in the one-way interaction condition than the two-way interaction condition; and 4) Singers produce more stable notes when singing solo than with their partners.",GBR,education,Developed economies,"[-1.5727012, -26.241484]","[-3.1086638, -18.85815]","[15.98525, 0.022840148, -17.070253]","[8.578414, -17.800303, -13.407242]","[10.457536, 10.357537]","[6.9213133, 1.9902027]","[11.382074, 15.170808, -0.08145325]","[8.804842, 7.6107163, 11.56522]"
71,Shunya Ariga;Satoru Fukayama;Masataka Goto,Song2Guitar: A Difficulty-Aware Arrangement System for Generating Guitar Solo Covers from Polyphonic Audio of Popular Music.,2017,https://doi.org/10.5281/zenodo.1417501,Shunya Ariga+The University of Tokyo>JPN>education;Satoru Fukayama+AIST>JPN>facility;Masataka Goto+AIST>JPN>facility,"This paper describes Song2Guitar which automatically generates difficulty-aware guitar solo cover of popular music from its acoustic signals. Previous research has utilized hidden Markov models (HMMs) to generate playable guitar piece from music scores. Our Song2Guitar extends the framework by leveraging MIR technologies so that it can handle beats, chords and melodies extracted from polyphonic audio. Furthermore, since it is important to generate a guitar piece to meet the skill of a player, Song2Guitar generates guitar solo covers in consideration of playing difficulty. We conducted a data-driven investigation to find what factor makes a guitar piece difficult to play, and restricted Song2Guitar to use certain hand forms adaptively so that the player can play the piece without experiencing too much difficulty. The user interface of Song2Guitar is also implemented and is used to conduct user tests. The results indicated that Song2Guitar succeeded in generating guitar solo covers from polyphonic audio with various playing difficulties.",JPN,education,Developed economies,"[16.407663, 7.707306]","[-43.347076, -4.1091866]","[5.137249, -10.794238, 18.002302]","[-14.915779, -10.4243555, -2.4761071]","[11.029086, 7.7827516]","[7.167891, 4.0320067]","[12.947391, 12.599966, -0.3068372]","[9.066671, 6.7518225, 10.297385]"
70,Jilt Sebastian;Hema A. Murthy,Onset Detection in Composition Items of Carnatic Music.,2017,https://doi.org/10.5281/zenodo.1414830,"Jilt Sebastian+Indian Institute of Technology, Madras>IND>education;Hema A. Murthy+Indian Institute of Technology, Madras>IND>education","Complex rhythmic patterns associated with Carnatic music are revealed from the stroke locations of percussion instruments. However, a comprehensive approach for the detection of these locations from composition items is lacking. This is a challenging problem since the melodic sounds (typically vocal and violin) generate soft-onset locations which result in a number of false alarms. In this work, a separation-driven onset detection approach is proposed. Percussive separation is performed using a Deep Recurrent Neural Network (DRNN) in the first stage. A single model is used to separate the percussive vs the non-percussive sounds using discriminative training and time-frequency masking. This is then followed by an onset detection stage based on group delay (GD) processing on the separated percussive track. The proposed approach is evaluated on a large dataset of live Carnatic music concert recordings and compared against percussive separation and onset detection baselines. The separation performance is significantly better than that of Harmonic-Percussive Separation (HPS) algorithm and onset detection performance is better than the state-of-the-art Convolutional Neural Network (CNN) based algorithm. The proposed approach has an absolute improvement of 18.4% compared with the detection algorithm applied directly on the composition items.",IND,education,Developing economies,"[29.188139, -27.095955]","[-24.58017, -9.078945]","[6.260925, -20.046743, -5.938712]","[-2.1001842, 9.3651085, -14.026447]","[10.3165, 5.1363993]","[5.5518174, 2.5234277]","[10.477181, 13.470815, -1.4745964]","[7.967024, 7.3541064, 10.536662]"
69,Rodrigo Schramm;Andrew Mcleod;Mark Steedman;Emmanouil Benetos,Multi-Pitch Detection and Voice Assignment for A Cappella Recordings of Multiple Singers.,2017,https://doi.org/10.5281/zenodo.1417671,Rodrigo Schramm+Universidade Federal do Rio Grande do Sul>BRA>education;Andrew McLeod+University of Edinburgh>GBR>education;Mark Steedman+University of Edinburgh>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education,"This paper presents a multi-pitch detection and voice assignment method applied to audio recordings containing a cappella performances with multiple singers. A novel approach combining an acoustic model for multi-pitch detection and a music language model for voice separation and assignment is proposed. The acoustic model is a spectrogram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov models that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part compositions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70% for frame-based multi-pitch detection and over 45% for four-voice assignment.",BRA,education,Developing economies,"[-8.628872, -35.6416]","[-42.647877, -22.266829]","[19.59414, 12.038956, -16.062288]","[-2.8406034, -10.551532, -26.173141]","[9.938433, 11.124588]","[6.557541, 4.729606]","[11.179751, 15.203181, 0.7326267]","[9.736237, 8.52362, 10.100775]"
68,Kosetsu Tsukuda;Keisuke Ishida;Masataka Goto,Lyric Jumper: A Lyrics-Based Music Exploratory Web Service by Modeling Lyrics Generative Process.,2017,https://doi.org/10.5281/zenodo.1417749,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Keisuke Ishida+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Each artist has their own taste for topics of lyrics such as “love” and “friendship.” Considering such artist’s taste brings new applications in music information retrieval: choosing an artist based on topics of lyrics and finding unfamiliar artists who have similar taste to a favorite artist. Although previous studies applied latent Dirichlet allocation (LDA) to lyrics to analyze topics, LDA was not able to capture the artist’s taste. In this paper, we propose a topic model that can deal with the artist’s taste for topics of lyrics. Our model assumes each artist has a topic distribution and a topic is assigned to each song according to the distribution. Our experimental results using a real-world dataset show that our model outperforms LDA in terms of the perplexity. By applying our model to estimate topics of 147,990 lyrics by 3,722 artists, we implement a web service called Lyric Jumper that enables users to explore lyrics based on the estimated topics. Lyric Jumper provides functions such as artist’s topic taste visualization and topic-similarity-based artist recommendation. We also analyze operation logs obtained from 12,353 users on Lyric Jumper and show the usefulness of Lyric Jumper especially in recommending topic-related phrases in lyrics.",JPN,facility,Developed economies,"[-32.769257, -33.274124]","[30.676022, 13.532857]","[7.5514393, 24.931755, -4.0291166]","[16.767384, -2.5580459, 13.771342]","[11.507249, 11.799411]","[11.512802, 2.3660321]","[12.470316, 16.022667, 1.0837917]","[12.615601, 6.025591, 12.413398]"
67,Saumitra Mishra;Bob L. Sturm;Simon Dixon,Local Interpretable Model-Agnostic Explanations for Music Content Analysis.,2017,https://doi.org/10.5281/zenodo.1417387,"Saumitra Mishra+Centre for Digital Music, Queen Mary University of London>GBR>education;Bob L. Sturm+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","The interpretability of a machine learning model is essential for gaining insight into model behaviour. While some machine learning models (e.g., decision trees) are transparent, the majority of models used today are still black-boxes. Recent work in machine learning aims to analyse these models by explaining the basis of their decisions. In this work, we extend one such technique, called local interpretable model-agnostic explanations, to music content analysis. We propose three versions of explanations: one version is based on temporal segmentation, and the other two are based on frequency and time-frequency segmentation. These explanations provide meaningful ways to understand the factors that influence the classification of specific input data. We apply our proposed methods to three singing voice detection systems: the first two are designed using decision tree and random forest classifiers, respectively; the third system is based on convolutional neural network. The explanations we generate provide insights into the model behaviour. We use these insights to demonstrate that despite achieving 71.4% classification accuracy, the decision tree model fails to generalise. We also demonstrate that the model-agnostic explanations for the neural network model agree in many cases with the model-dependent saliency maps. The experimental code and results are available online.",GBR,education,Developed economies,"[-31.423222, 0.9669705]","[-34.91424, -40.4855]","[-2.8851192, 17.995361, 18.03858]","[-1.8459374, -12.252334, -21.354942]","[12.672986, 8.716765]","[7.7843943, 5.064683]","[13.840534, 13.679524, -0.2703729]","[10.01977, 7.0428085, 9.120868]"
66,Jun-qi Deng;Yu-Kwong Kwok,Large Vocabulary Automatic Chord Estimation with an Even Chance Training Scheme.,2017,https://doi.org/10.5281/zenodo.1417421,Junqi Deng+The University of Hong Kong>HKG>education|Unknown>Unknown>Unknown;Yu-Kwong Kwok+The University of Hong Kong>HKG>education|Unknown>Unknown>Unknown,"This paper presents a large vocabulary automatic chord estimation system implemented using a bidirectional long short-term memory recurrent neural network trained with a skewed-class-aware scheme. This scheme gives the uncommon chord types much more exposure during the training process. The evaluation results indicate that: compared with a normal training scheme, the proposed scheme can boost the weighted chord symbol recalls of some uncommon chords and significantly improve the average chord quality accuracy, at the expense of the overall weighted chord symbol recall.",HKG,education,Developing economies,"[57.382584, -5.0087767]","[-34.714436, 22.096766]","[25.585669, -10.096329, 19.842594]","[-26.807077, -0.8488886, -0.38024926]","[6.675539, 8.670583]","[5.894205, 3.8735175]","[11.909973, 10.36113, 2.1074057]","[9.667996, 9.142124, 12.3368435]"
65,Dorian Cazau;Yuancheng Wang;Olivier Adam;Qiao Wang;Grégory Nuel,Improving Note Segmentation in Automatic Piano Music Transcription Systems with a Two-State Pitch-Wise HMM Method.,2017,https://doi.org/10.5281/zenodo.1417929,"Dorian Cazau+Lab-STICC, ENSTA-Bretagne>FRA>education;Yuancheng Wang+Southeast China>CHN>Unknown;Olivier Adam+Institut d’Alembert, UPMC>FRA>education;Qiao Wang+Southeast China>CHN>Unknown;Grégory Nuel+Institut d’Alembert, UPMC>FRA>education","Many methods for automatic piano music transcription involve a multi-pitch estimation method that estimates an activity score for each pitch. A second processing step, called note segmentation, has to be performed for each pitch in order to identify the time intervals when the notes are played. In this study, a pitch-wise two-state on/off first-order Hidden Markov Model (HMM) is developed for note segmentation. A complete parametrization of the HMM sigmoid function is proposed, based on its original regression formulation, including a parameter α of slope smoothing and β of thresholding contrast. A comparative evaluation of different note segmentation strategies was performed, differentiated according to whether they use a fixed threshold, called “Hard Thresholding” (HT), or a HMM-based thresholding method, called “Soft Thresholding” (ST). This evaluation was done following MIREX standards and using the MAPS dataset. Also, different transcription and recording scenarios were tested using three units of the Audio Degradation toolbox. Results show that note segmentation through a HMM soft thresholding with a data-based optimization of the {α, β} parameter couple significantly enhances transcription performance.",FRA,education,Developed economies,"[29.424274, -5.0536857]","[-10.370597, -6.8967133]","[18.586226, -3.448625, 14.4147625]","[0.3694568, -6.276172, -12.280545]","[9.648849, 7.357627]","[6.558883, 2.9995148]","[11.906874, 11.614849, -0.20839442]","[8.776808, 7.6906, 10.771516]"
64,Joe Cheri Ross;Abhijit Mishra;Kaustuv Kanti Ganguli;Pushpak Bhattacharyya;Preeti Rao,Identifying Raga Similarity Through Embeddings Learned from Compositions' Notation.,2017,https://doi.org/10.5281/zenodo.1417032,Joe Cheri Ross+Indian Institute of Technology Bombay>IND>education|IBM Research India>IND>company;Abhijit Mishra+IBM Research India>IND>company;Kaustuv Kanti Ganguli+Indian Institute of Technology Bombay>IND>education;Pushpak Bhattacharyya+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"Identifying similarities between ragas in Hindustani music impacts tasks like music recommendation, music information retrieval and automatic analysis of large-scale musical content. Quantifying raga similarity becomes extremely challenging as it demands assimilation of both intrinsic (viz., notes, tempo) and extrinsic (viz. raga singing-time, emotions conveyed) properties of ragas. This paper introduces novel frameworks for quantifying similarities between ragas based on their melodic attributes alone, available in the form of bandish (composition) notation. Based on the hypothesis that notes in a particular raga are characterized by the company they keep, we design and train several deep recursive neural network variants with Long Short-term Memory (LSTM) units to learn distributed representations of notes in ragas from bandish notations. We refer to these distributed representations as note-embeddings. Note-embeddings, as we observe, capture a raga’s identity, and thus the similarity between note-embeddings signifies the similarity between the ragas. Evaluations with perplexity measure and clustering based method show the performance improvement in identifying similarities using note-embeddings over n-gram and uni-directional LSTM baselines. While our metric may not capture similarity between ragas in their entirety, it could be quite useful in various computational music settings that heavily rely on melodic information.",IND,education,Developing economies,"[3.367426, 0.26428586]","[-2.1432352, -30.02796]","[4.8617806, 3.5523639, -18.684916]","[-1.0468559, 6.799365, -23.408377]","[11.364982, 10.441762]","[8.891955, 5.155309]","[11.65952, 15.104228, -1.6540146]","[9.88353, 6.347885, 9.255288]"
63,Ning Chen;Shijun Wang,High-Level Music Descriptor Extraction Algorithm Based on Combination of Multi-Channel CNNs and LSTM.,2017,https://doi.org/10.5281/zenodo.1417901,Ning Chen+East China University of Science and Technology>CHN>education|East China University of Science and Technology>CHN>education;Shijun Wang+East China University of Science and Technology>CHN>education|East China University of Science and Technology>CHN>education,"Although Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTM) have yielded impressive performances in a variety of Music Information Retrieval (MIR) tasks, the complementarity among the CNNs of different architectures and that between CNNs and LSTM are seldom considered. In this paper, multi-channel CNNs with different architectures and LSTM are combined into one unified architecture (Multi-Channel Convolutional LSTM, MCCLSTM) to extract high-level music descriptors. First, three channels of CNNs with different shapes of filter are applied on each spectrogram image chunk to extract the pitch-, tempo-, and bass-relevant descriptors, respectively. Then, the outputs of each CNNs channel are concatenated and then passed through a fully connected layer to obtain the fused descriptor. Finally, LSTM is applied on the fused descriptor sequence of the whole track to extract its long-term structure property to obtain the high-level descriptor. To prove the efficiency of the MCCLSTM model, the obtained high-level music descriptor is applied to the music genre classification and emotion prediction task. Experimental results demonstrate that, when compared with the hand-crafted schemes or conventional deep learning (Multi Layer Perceptrons (MLP), CNNs, and LSTM) based ones, MCCLSTM achieves higher prediction accuracy on three music collections with different kinds of semantic tags.",CHN,education,Developing economies,"[-8.806305, -13.66213]","[-25.591705, -32.631477]","[2.4716797, -1.0579208, -14.659907]","[-6.228724, -11.949971, -19.238913]","[12.572806, 8.404858]","[8.091152, 5.2920685]","[13.127309, 13.886887, 0.57597274]","[9.684414, 6.94649, 8.831395]"
72,Emilia Parada-Cabaleiro;Anton Batliner;Alice Baird;Björn W. Schuller,The SEILS Dataset: Symbolically Encoded Scores in Modern-Early Notation for Computational Musicology.,2017,https://doi.org/10.5281/zenodo.1415164,Emilia Parada-Cabaleiro+University of Passau>DEU>education|Augsburg University>DEU>education;Anton Batliner+University of Passau>DEU>education|Augsburg University>DEU>education;Alice Baird+University of Passau>DEU>education|Augsburg University>DEU>education;Björn W. Schuller+University of Passau>DEU>education|Augsburg University>DEU>education|Imperial College London>GBR>education,"The automatic analysis of notated Renaissance music is restricted by a shortfall in codified repertoire. Thousands of scores have been digitised by music libraries across the world, but the absence of symbolically codified information makes these inaccessible for computational evaluation. Optical Music Recognition (OMR) made great progress in addressing this issue, however, early notation is still an on-going challenge for OMR. To this end, we present the Symbolically Encoded Il Lauro Secco (SEILS) dataset, a new dataset of codified scores for use within computational musicology. We focus on a collection of Italian madrigals from the 16th century, a polyphonic secular a cappella composition characterised by strong musical-linguistic synergies. Thirty madrigals for five unaccompanied voices are presented in modern and early notation, considering a variety of digital formats: Lilypond, Music XML, MIDI, and Finale (a total of 150 symbolically codified scores). Given the musical and poetic value of the chosen repertoire, we aim to promote synergies between computational musicology and linguistics.",DEU,education,Developed economies,"[19.244238, 16.514696]","[-16.932192, 29.414862]","[-5.2023635, -11.605594, 20.01414]","[-16.84211, -17.761114, 4.2205243]","[11.7831, 6.9126053]","[7.4291744, 0.22416373]","[13.459091, 12.214774, -1.2014416]","[9.412246, 5.881052, 11.977639]"
62,Hiroaki Tsushima;Eita Nakamura;Katsutoshi Itoyama;Kazuyoshi Yoshii,Function- and Rhythm-Aware Melody Harmonization Based on Tree-Structured Parsing and Split-Merge Sampling of Chord Sequences.,2017,https://doi.org/10.5281/zenodo.1416848,Hiroaki Tsushima+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education|RIKEN AIP>JPN>facility,"This paper presents an automatic harmonization method that, for a given melody (sequence of musical notes), generates a sequence of chord symbols in the style of existing data. A typical way is to use hidden Markov models (HMMs) that represent chord transitions on a regular grid (e.g., bar or beat grid). This approach, however, cannot explicitly describe the rhythms, harmonic functions (e.g., tonic, dominant, and subdominant), and the hierarchical structure of chords, which are supposedly important in traditional harmony theories. To solve this, we formulate a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chords incorporating their syntactic functions, (2) a metrical Markov model describing chord rhythms, and (3) a Markov model generating melodies conditionally on a chord sequence. To estimate a variable-length chord sequence for a given melody, we iteratively refine the latent tree structure and the chord symbols and rhythms using a Metropolis-Hastings sampler with split-merge operations. Experimental results show that the proposed method outperformed the HMM-based method in terms of predictive abilities.",JPN,education,Developed economies,"[48.46806, -2.833818]","[-24.375553, 25.696875]","[18.096153, -12.382442, 13.847499]","[-20.0918, 1.6805713, 0.71836966]","[9.946693, 9.296733]","[6.9460487, 3.448845]","[11.466661, 14.412806, -0.6928275]","[9.515939, 8.153753, 12.116274]"
59,Chia-Hao Chung;Yian Chen;Homer H. Chen,Exploiting Playlists for Representation of Songs and Words for Text-Based Music Retrieval.,2017,https://doi.org/10.5281/zenodo.1416436,Chia-Hao Chung+National Taiwan University>TWN>education|KKBOX Inc.>TWN>company;Yian Chen+National Taiwan University>TWN>education|KKBOX Inc.>TWN>company;Homer Chen+National Taiwan University>TWN>education,"As a result of the growth of online music streaming services, a large number of playlists have been created by users and service providers. The title of each playlist provides useful information, such as the theme and listening context, of the songs in the playlist. In this paper, we investigate how to exploit the words extracted from playlist titles for text-based music retrieval. The main idea is to represent songs and words in a common latent space so that the music retrieval is converted to the problem of selecting songs that are the nearest neighbors of the query word in the latent space. Specifically, an unsupervised learning method is proposed to generate a latent representation of songs and words, where the learning objects are the co-occurring songs and words in playlist titles. Five metrics (precision, recall, coherence, diversity, and popularity) are considered for performance evaluation of the proposed method. Qualitative results demonstrate that our method is able to capture the semantic meaning of songs and words, owning to the proximity property of related songs and words in the latent space.",TWN,education,Developing economies,"[-16.074135, 19.820251]","[32.723965, 14.030159]","[-7.345268, 6.341542, -8.854344]","[13.882098, -0.3860776, 13.553765]","[13.84623, 8.111471]","[11.450779, 2.220222]","[13.84802, 14.86413, -1.9204108]","[12.655989, 5.9385877, 12.449587]"
58,Jorge Calvo-Zaragoza;Jose J. Valero-Mas;Antonio Pertusa,End-to-End Optical Music Recognition Using Neural Networks.,2017,https://doi.org/10.5281/zenodo.1418333,Jorge Calvo-Zaragoza+McGill University>CAN>education;Jose J. Valero-Mas+University of Alicante>ESP>education;Antonio Pertusa+University of Alicante>ESP>education,"This work addresses the Optical Music Recognition (OMR) task in an end-to-end fashion using neural networks. The proposed architecture is based on a Recurrent Convolutional Neural Network topology that takes as input an image of a monophonic score and retrieves a sequence of music symbols as output. In the first stage, a series of convolutional filters are trained to extract meaningful features of the input image, and then a recurrent block models the sequential nature of music. The system is trained using a Connectionist Temporal Classification loss function, which avoids the need for a frame-by-frame alignment between the image and the ground-truth music symbols. Experimentation has been carried on a set of 90,000 synthetic monophonic music scores with more than 50 different possible labels. Results obtained depict classification error rates around 2% at symbol level, thus proving the potential of the proposed end-to-end architecture for OMR. The source code, dataset, and trained models are publicly released for reproducible research and future comparison purposes.",CAN,education,Developed economies,"[39.28459, 19.145052]","[-22.35456, -24.295664]","[16.527018, 12.382943, 12.120678]","[-15.4325695, -16.670532, -4.958897]","[8.64443, 6.190805]","[6.496216, -1.0901881]","[10.677995, 11.115863, -0.12356425]","[8.1576605, 4.355429, 9.807634]"
57,Jiakun Fang;David Grunberg;Diane T. Litman;Ye Wang,Discourse Analysis of Lyric and Lyric-Based Classification of Music.,2017,https://doi.org/10.5281/zenodo.1416946,Jiakun Fang+National University of Singapore>SGP>education;David Grunberg+University of Pittsburgh>USA>education;Diane Litman+University of Pittsburgh>USA>education;Ye Wang+National University of Singapore>SGP>education,"Lyrics play an important role in the semantics and the structure of many pieces of music. However, while many existing lyric analysis systems consider each sentence of a given set of lyrics separately, lyrics are more naturally understood as multi-sentence units, where the relations between sentences is a key factor. Here we describe a series of experiments using discourse-based features, which describe the relations between different sentences within a set of lyrics, for several common Music Information Retrieval tasks. We first investigate genre recognition and present evidence that incorporating discourse features allow for more accurate genre classification than single-sentence lyric features do. Similarly, we examine the problem of release date estimation by passing features to classifiers to determine the release period of a particular song, and again determine that an assistance from discourse-based features allow for superior classification relative to single-sentence lyric features alone. These results suggest that discourse-based features are potentially useful for Music Information Retrieval tasks.",SGP,education,Developing economies,"[-32.316944, -19.071632]","[23.819738, -3.838845]","[-12.349613, 10.638965, 17.947433]","[8.311269, 9.859534, 1.1625673]","[12.7401905, 11.575774]","[10.235893, 3.1563687]","[14.196774, 14.708242, 1.4066391]","[11.930557, 6.701392, 11.234579]"
56,Shengchen Li;Simon Dixon;Mark D. Plumbley,Clustering Expressive Timing with Regressed Polynomial Coefficients Demonstrated by a Model Selection Test.,2017,https://doi.org/10.5281/zenodo.1417101,Shengchen Li+Beijing University of Posts and Telecommunications>CHN>education;Simon Dixon+Queen Mary University of London>GBR>education;Mark D. Plumbley+University of Surrey>GBR>education,"Though many past works have tried to cluster expressive timing within a phrase, there have been few attempts to cluster features of expressive timing with constant dimensions regardless of phrase lengths. For example, used as a way to represent expressive timing, tempo curves can be regressed by a polynomial function such that the number of regressed polynomial coefficients remains constant with a given order regardless of phrase lengths. In this paper, clustering the regressed polynomial coefficients is proposed for expressive timing analysis. A model selection test is presented to compare Gaussian Mixture Models (GMMs) fitting regressed polynomial coefficients and fitting expressive timing directly. As there are no expected results of clustering expressive timing, the proposed method is demonstrated by how well the expressive timing are approximated by the centroids of GMMs. The results show that GMMs fitting the regressed polynomial coefficients outperform GMMs fitting expressive timing directly. This conclusion suggests that it is possible to use regressed polynomial coefficients to represent expressive timing within a phrase and cluster expressive timing within phrases of different lengths.",CHN,education,Developing economies,"[19.397438, 29.235676]","[-30.63231, 1.9531116]","[-10.498275, -16.15494, 6.9359336]","[-13.397943, 9.769102, -4.9276996]","[11.557712, 6.5580597]","[5.780657, 1.4480153]","[12.624588, 12.601963, -1.5287731]","[8.13878, 6.4534364, 11.231834]"
55,Feynman T. Liang;Mark Gotham;Matthew Johnson 0003;Jamie Shotton,Automatic Stylistic Composition of Bach Chorales with Deep LSTM.,2017,https://doi.org/10.5281/zenodo.1416208,Feynman Liang+University of Cambridge>GBR>education;Mark Gotham+University of Cambridge>GBR>education;Matthew Johnson+Microsoft>USA>company;Jamie Shotton+Microsoft>USA>company,"This paper presents “BachBot”: an end-to-end automatic composition system for composing and completing music in the style of Bach’s chorales using a deep long short-term memory (LSTM) generative model. We propose a new sequential encoding scheme for polyphonic music and a model for both composition and harmonization which can be efficiently sampled without expensive Markov Chain Monte Carlo (MCMC). Analysis of the trained model provides evidence of neurons specializing without prior knowledge or explicit supervision to detect common music-theoretic concepts such as tonics, chords, and cadences. To assess BachBot’s success, we conducted one of the largest musical discrimination tests on 2336 participants. Among the results, the proportion of responses correctly differentiating BachBot from Bach was only 1% better than random guessing.",GBR,education,Developed economies,"[18.193308, -2.299119]","[-4.7938633, -39.83851]","[-0.16999993, -16.39102, 25.047163]","[-23.148275, 0.16544919, -6.39388]","[10.487715, 8.721186]","[9.165272, 6.130312]","[12.360616, 13.149201, -0.509737]","[9.611562, 5.4582787, 9.146548]"
54,Rachel M. Bittner;Minwei Gu;Gandalf Hernandez;Eric J. Humphrey;Tristan Jehan;Hunter McCurry;Nicola Montecchio,Automatic Playlist Sequencing and Transitions.,2017,https://doi.org/10.5281/zenodo.1417028,Rachel M. Bittner+Spotify Inc.>USA>company;Minwei Gu+Spotify Inc.>USA>company;Gandalf Hernandez+Spotify Inc.>USA>company;Eric J. Humphrey+Spotify Inc.>USA>company;Tristan Jehan+Spotify Inc.>USA>company;P. Hunter McCurry+Spotify Inc.>USA>company;Nicola Montecchio+Spotify Inc.>USA>company,"Professional music curators and DJs artfully arrange and mix recordings together to create engaging, seamless, and cohesive listening experiences, a craft enjoyed by audiences around the world. The average listener, however, lacks both the time and the skill necessary to create comparable experiences, despite access to same source material. As a result, user-generated listening sessions often lack the sophistication popularized by modern artists, e.g. tracks are played in their entirety with little or no thought given to their ordering. To these ends, this paper presents methods for automatically sequencing existing playlists and adding DJ-style crossfade transitions between tracks: the former is modeled as a graph traversal problem, and the latter as an optimization problem. Our approach is motivated by an analysis of listener data on a large music catalog, and subjectively evaluated by professional curators.",USA,company,Developed economies,"[-40.61411, 40.293774]","[36.28642, 22.539885]","[-1.9027425, 32.37969, -3.0321448]","[15.062723, 5.4767513, 23.717936]","[16.238192, 8.1345825]","[12.096064, 1.5702713]","[16.56903, 14.830969, -1.7446119]","[13.111791, 5.1416674, 13.198804]"
53,Jordan B. L. Smith;Elaine Chew,Automatic Interpretation of Music Structure Analyses: A Validated Technique for Post-Hoc Estimation of the Rationale for an Annotation.,2017,https://doi.org/10.5281/zenodo.1415196,Jordan B. L. Smith+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Elaine Chew+Queen Mary University of London>GBR>education,"Annotations of musical structure usually provide a low level of detail: they include boundary locations and section labels, but do not indicate what makes the sections similar or distinct, or what changes in the music at each boundary. For those studying annotated corpora, it would be useful to know the rationale for each annotation, but collecting this information from listeners is burdensome and difficult. We propose a new algorithm for estimating which musical features formed the basis for each part of an annotation. To evaluate our approach, we use a synthetic dataset of music clips, all designed to have ambiguous structure, that was previously used and validated in a psychology experiment. We find that, compared to a previous optimization-based algorithm, our correlation-based approach is better able to predict the rationale for an analysis. Using the best version of our algorithm, we process examples from the SALAMI dataset and demonstrate how we can augment the structure annotation data with estimated rationales, inviting new ways to research and use the data.",JPN,facility,Developed economies,"[0.08732403, 6.446236]","[-1.8768839, 7.398374]","[-5.5570126, -6.0846415, 2.1163387]","[-4.0701456, 2.187754, 1.0256566]","[12.32785, 8.445791]","[8.573927, 2.445379]","[13.009859, 13.856251, -0.8031468]","[10.396118, 6.788759, 11.452061]"
52,Rong Gong;Jordi Pons;Xavier Serra,Audio to Score Matching by Combining Phonetic and Duration Information.,2017,https://doi.org/10.5281/zenodo.1415766,Rong Gong+Universitat Pompeu Fabra>ESP>education;Jordi Pons+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"We approach the singing phrase audio to score matching problem by using phonetic and duration information – with a focus on studying the jingju a cappella singing case. We argue that, due to the existence of a basic melodic contour for each mode in jingju music, only using melodic information (such as pitch contour) will result in an ambiguous matching. This leads us to propose a matching approach based on the use of phonetic and duration information. Phonetic information is extracted with an acoustic model shaped with our data, and duration information is considered with the Hidden Markov Models (HMMs) variants we investigate. We build a model for each lyric path in our scores and we achieve the matching by ranking the posterior probabilities of the decoded most likely state sequences. Three acoustic models are investigated: (i) convolutional neural networks (CNNs), (ii) deep neural networks (DNNs) and (iii) Gaussian mixture models (GMMs). Also, two duration models are compared: (i) hidden semi-Markov model (HSMM) and (ii) post-processor duration model. Results show that CNNs perform better in our (small) audio dataset and also that HSMM outperforms the post-processor duration model.",ESP,education,Developed economies,"[21.135664, -16.576519]","[-7.709825, -20.934896]","[0.36545303, -13.743851, -15.088236]","[2.4434648, -6.3622327, -17.28487]","[10.812169, 5.996013]","[7.691673, 4.3657684]","[11.722632, 12.478684, -1.824411]","[10.172482, 7.3956327, 9.386101]"
51,Adrien Ycart;Emmanouil Benetos,A Study on LSTM Networks for Polyphonic Music Sequence Modelling.,2017,https://doi.org/10.5281/zenodo.1415018,Adrien Ycart+Queen Mary University of London>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education,"Neural networks, and especially long short-term memory networks (LSTM), have become increasingly popular for sequence modelling, be it in text, speech, or music. In this paper, we investigate the predictive power of simple LSTM networks for polyphonic MIDI sequences, using an empirical approach. Such systems can then be used as a music language model which, combined with an acoustic model, can improve automatic music transcription (AMT) performance. As a first step, we experiment with synthetic MIDI data, and we compare the results obtained in various settings, throughout the training process. In particular, we compare the use of a fixed sample rate against a musically-relevant sample rate. We test this system both on synthetic and real MIDI data. Results are compared in terms of note prediction accuracy. We show that the higher the sample rate is, the better the prediction is, because self transitions are more frequent. We suggest that for AMT, a musically-relevant sample rate is crucial in order to model note transitions, beyond a simple smoothing effect.",GBR,education,Developed economies,"[23.247889, -3.532204]","[-10.552472, -33.517887]","[8.797621, 0.35178113, 13.380058]","[-15.135301, -5.2904873, -13.172334]","[9.927698, 8.216668]","[8.482889, 5.588578]","[12.58021, 12.336349, 0.21929888]","[9.343827, 6.140083, 9.21105]"
60,Eduardo Fonseca;Jordi Pons;Xavier Favory;Frederic Font;Dmitry Bogdanov;Andres Ferraro;Sergio Oramas;Alastair Porter;Xavier Serra,Freesound Datasets: A Platform for the Creation of Open Audio Datasets.,2017,https://doi.org/10.5281/zenodo.1417159,"Eduardo Fonseca+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Jordi Pons+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Favory+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Frederic Font+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Andres Ferraro+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sergio Oramas+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Alastair Porter+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Openly available datasets are a key factor in the advancement of data-driven research approaches, including many of the ones used in sound and music computing. In the last few years, quite a number of new audio datasets have been made available but there are still major shortcomings in many of them to have a significant research impact. Among the common shortcomings are the lack of transparency in their creation and the difficulty of making them completely open and sharable. They often do not include clear mechanisms to amend errors and many times they are not large enough for current machine learning needs. This paper introduces Freesound Datasets, an online platform for the collaborative creation of open audio datasets based on principles of transparency, openness, dynamic character, and sustainability. As a proof-of-concept, we present an early snapshot of a large-scale audio dataset built using this platform. It consists of audio samples from Freesound organised in a hierarchy based on the AudioSet Ontology. We believe that building and maintaining datasets following the outlined principles and using open tools and collaborative approaches like the ones presented here will have a significant impact in our research community.",ESP,education,Developed economies,"[-20.482496, 9.084898]","[9.512338, 24.278303]","[-17.72945, -6.252319, -12.358695]","[-5.6218896, 2.892223, 13.616085]","[12.947531, 7.6593065]","[10.173385, 3.945474]","[14.213916, 13.415663, -1.1051708]","[11.600882, 5.9777355, 10.739767]"
50,Ricardo S. Oliveira;Caio Nóbrega;Leandro Balby Marinho;Nazareno Andrade,A Multiobjective Music Recommendation Approach for Aspect-Based Diversification.,2017,https://doi.org/10.5281/zenodo.1417000,Ricardo S. Oliveira+Federal University of Campina Grande>BRA>education;Caio Nóbrega+Federal University of Campina Grande>BRA>education;Leandro B. Marinho+Federal University of Campina Grande>BRA>education;Nazareno Andrade+Federal University of Campina Grande>BRA>education,"Many successful recommendation approaches are based on the optimization of some explicit utility function defined in terms of the misfit between the predicted and the actual items of the user. Although effective, this approach may lead to recommendations that are relevant but obvious and uninteresting. Many approaches investigate this problem by trying to avoid recommendation lists in which items are very similar to each other (aka diversification) with respect to some aspect of the item. However, users may have very different preferences concerning what aspects should be diversified and what should match their past/current preferences. In this paper we take this into consideration by proposing a solution based on multiobjective optimization for generating recommendation lists featuring the optimal balance between the aspects that should be held fixed (maximize similarity with users actual items) and the ones that should be diversified (minimize similarity with other items in the recommendation list). We evaluate our proposed approach on real data from Last.fm and demonstrate its effectiveness in contrast to state-of-the-art approaches.",BRA,education,Developing economies,"[-48.47856, 25.169863]","[41.913784, 15.814536]","[-7.7398753, 28.044641, -9.568917]","[16.08797, 10.424134, 22.074347]","[15.945272, 9.224146]","[12.544427, 1.9036435]","[15.820346, 15.622585, -1.4387649]","[13.497857, 5.2529817, 12.751146]"
73,Audrey Laplante;Timothy D. Bowman;Nawel Aamar,"""I'm at #Osheaga!"": Listening to the Backchannel of a Music Festival on Twitter.",2017,https://doi.org/10.5281/zenodo.1416352,Audrey Laplante+Université de Montréal>CAN>education;Timothy D. Bowman+Wayne State University>USA>education;Nawel Aamar+Université de Montréal>CAN>education,"It has become common practice for audience members to use social media to connect, share, and communicate with each other during events (e.g., sport events, elections, award ceremonies). But how is this backchannel used during a musical event and what does it say about how people engage with the music and the artists performing it? In this paper, we present the result of a study of a dataset composed of 31,140 tweets posted during and around the 10th edition of Osheaga, an important music festival held annually in Montreal. A combination of statistics and qualitative content analysis is used to examine the postings. This allows us to describe the content of these postings (i.e., topics, shared media), the type of message being shared (i.e., opinion, expression, information), and who the authors of these tweets are.",CAN,education,Developed economies,"[-31.031586, 32.326637]","[49.81393, 13.427784]","[-27.715902, 9.373898, -7.961057]","[19.91108, 13.898567, 12.259678]","[14.404563, 8.523013]","[12.394574, 2.3758535]","[14.745799, 14.175023, -1.4818659]","[13.471023, 5.3924294, 12.07118]"
76,Carl Southall;Ryan Stables;Jason Hockman,Automatic Drum Transcription for Polyphonic Recordings Using Soft Attention Mechanisms and Convolutional Neural Networks.,2017,https://doi.org/10.5281/zenodo.1415616,Carl Southall+Birmingham City University>GBR>education;Ryan Stables+Birmingham City University>GBR>education;Jason Hockman+Birmingham City University>GBR>education,"Automatic drum transcription is the process of generating symbolic notation for percussion instruments within audio recordings. To date, recurrent neural network (RNN) systems have achieved the highest evaluation accuracies for both drum solo and polyphonic recordings, however the accuracies within a polyphonic context still remain relatively low. To improve accuracy for polyphonic recordings, we present two approaches to the ADT problem: First, to capture the dynamism of features in multiple time-step hidden layers, we propose the use of soft attention mechanisms (SA) and an alternative RNN configuration containing additional peripheral connections (PC). Second, to capture these same trends at the input level, we propose the use of a convolutional neural network (CNN), which uses a larger set of time-step features. In addition, we propose the use of a bidirectional recurrent neural network (BRNN) in the peak-picking stage. The proposed systems are evaluated along with two state-of-the-art ADT systems in five evaluation scenarios, including a newly-proposed evaluation methodology designed to assess the generalisability of ADT systems. The results indicate that all of the newly proposed systems achieve higher accuracies than the state-of-the-art RNN systems for polyphonic recordings and that the additional BRNN peak-picking stage offers slight improvement in certain contexts.",GBR,education,Developed economies,"[27.477123, -45.683052]","[-34.85991, -14.961604]","[18.473906, -22.968184, 4.134611]","[-0.75954914, 10.670077, -19.733694]","[7.624292, 7.1842713]","[8.262565, 4.620004]","[10.344599, 11.47724, 1.0440278]","[9.010966, 6.9959126, 9.522899]"
96,Louis Bigo;Mathieu Giraud;Richard Groult;Nicolas Guiomard-Kagan;Florence Levé,Sketching Sonata Form Structure in Selected Classical String Quartets.,2017,https://doi.org/10.5281/zenodo.1415020,"Louis Bigo+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Mathieu Giraud+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Richard Groult+MIS, Université de Picardie Jules Verne, Amiens>FRA>education;Nicolas Guiomard-Kagan+MIS, Université de Picardie Jules Verne, Amiens>FRA>education;Florence Levé+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education","Many classical works from 18th and 19th centuries are sonata forms, exhibiting a piece-level tonal path through an exposition, a development and a recapitulation and involving two thematic zones as well as other elements. The computational music analysis of scores with such a large-scale structure is a challenge for the MIR community and should gather different analysis techniques. We propose first steps in that direction, combining analysis features on symbolic scores on patterns, harmony, and other elements into a structure estimated by a Viterbi algorithm on a Hidden Markov Model. We test this strategy on a set of first movements of Haydn and Mozart string quartets. The proposed computational analysis strategy finds some pertinent features and sketches the sonata form structure in some pieces that have a simple sonata form.",FRA,education,Developed economies,"[13.753283, 7.0141683]","[-14.993375, 16.149424]","[3.3590968, -7.5215626, 9.3007345]","[-13.212494, -3.3902144, 3.8534405]","[11.0766535, 7.762432]","[8.2362585, 2.224049]","[12.698625, 12.7123575, -0.504612]","[9.943716, 6.764093, 11.819251]"
95,Andreas Jansson;Eric J. Humphrey;Nicola Montecchio;Rachel M. Bittner;Aparna Kumar;Tillman Weyde,Singing Voice Separation with Deep U-Net Convolutional Networks.,2017,https://doi.org/10.5281/zenodo.1414934,"Andreas Jansson+City, University of London>GBR>education|Spotify>USA>company;Eric Humphrey+Spotify>USA>company;Nicola Montecchio+Spotify>USA>company;Rachel Bittner+Spotify>USA>company;Aparna Kumar+Spotify>USA>company;Tillman Weyde+City, University of London>GBR>education","The decomposition of a music audio signal into its vocal and backing track components is analogous to image-to-image translation, where a mixed spectrogram is transformed into its constituent sources. We propose a novel application of the U-Net architecture — initially developed for medical imaging — for the task of source separation, given its proven capacity for recreating the fine, low-level detail required for high-quality audio reproduction. Through both quantitative evaluation and subjective assessment, experiments demonstrate that the proposed algorithm achieves state-of-the-art performance.",GBR,education,Developed economies,"[-1.4199635, -41.937016]","[-41.168983, -33.85451]","[26.9504, 8.516689, -8.678114]","[-15.413085, -7.576843, -27.71493]","[9.136047, 10.7088585]","[6.773536, 5.857046]","[10.98276, 14.4572935, 1.3431169]","[9.654702, 8.288315, 8.836099]"
94,Cheng-i Wang;Gautham J. Mysore;Shlomo Dubnov,Re-Visiting the Music Segmentation Problem with Crowdsourcing.,2017,https://doi.org/10.5281/zenodo.1415944,Cheng-i Wang+UCSD>USA>education|Adobe Research>USA>company;Gautham J. Mysore+Adobe Research>USA>company;Shlomo Dubnov+UCSD>USA>education,"Identifying boundaries in music structural segmentation is a well studied music information retrieval problem. The goal is to develop algorithms that automatically identify segmenting time points in music that closely matches human annotated data. The annotation itself is challenging due to its subjective nature, such as the degree of change that constitutes a boundary, the location of such boundaries, and whether a boundary should be assigned to a single time frame or a range of frames. Existing datasets have been annotated by small number of experts and the annotators tend to be constrained to specific definitions of segmentation boundaries. In this paper, we re-examine the annotation problem. We crowdsource the problem to a large number of annotators and present an analysis of the results. Our preliminary study suggests that although there is a correlation to existing datasets, this form of annotations reveals additional information such as stronger vs. weaker boundaries, gradual vs. sudden boundaries, and the difference in perception of boundaries between musicians and non-musicians. The study suggests that it could be worth re-defining certain aspects of the boundary identification in music structural segmentation problem with a broader definition.",USA,education,Developed economies,"[-8.043325, 5.2340174]","[-1.3542603, 6.443367]","[3.769357, -0.47178972, -2.643906]","[-2.2580404, 2.0924978, 0.29730022]","[12.1683655, 8.554275]","[8.536269, 2.7148416]","[13.04773, 14.652809, -0.08837018]","[10.487667, 6.9581156, 11.434978]"
93,Eelco van der Wel;Karen Ullrich,Optical Music Recognition with Convolutional Sequence-to-Sequence Models.,2017,https://doi.org/10.5281/zenodo.1415664,Eelco van der Wel+University of Amsterdam>NLD>education|University of Amsterdam>Unknown>Unknown;Karen Ullrich+University of Amsterdam>NLD>education|University of Amsterdam>Unknown>Unknown,"Optical Music Recognition (OMR) is an important technology within Music Information Retrieval. Deep learning models show promising results on OMR tasks, but symbol-level annotated data sets of sufficient size to train such models are not available and difficult to develop. We present a deep learning architecture called a Convolutional Sequence-to-Sequence model to both move towards an end-to-end trainable OMR pipeline, and apply a learning process that trains on full sentences of sheet music instead of individually labeled symbols. The model is trained and evaluated on a human generated data set, with various image augmentations based on real-world scenarios. This data set is the first publicly available set in OMR research with sufficient size to train and evaluate deep learning models. With the introduced augmentations a pitch recognition accuracy of 81% and a duration accuracy of 94% is achieved, resulting in a note level accuracy of 80%. Finally, the model is compared to commercially available methods, showing a large improvements over these applications.",NLD,education,Developed economies,"[38.980892, 19.304863]","[-21.375452, -24.740171]","[16.080019, 12.800397, 10.581448]","[-15.584873, -17.99455, -4.472134]","[8.660128, 6.256004]","[6.502008, -1.0836478]","[10.7068615, 11.108997, -0.11686095]","[8.161164, 4.3003426, 9.869423]"
92,Jorge Calvo-Zaragoza;Gabriel Vigliensoni;Ichiro Fujinaga,"One-Step Detection of Background, Staff Lines, and Symbols in Medieval Music Manuscripts with Convolutional Neural Networks.",2017,https://doi.org/10.5281/zenodo.1417493,Jorge Calvo-Zaragoza+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility|McGill University>CAN>education;Gabriel Vigliensoni+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility|McGill University>CAN>education;Ichiro Fujinaga+Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility|McGill University>CAN>education,"One of the most complex stages of optical music recognition workflows is the detection and isolation of musical symbols. Traditionally, this goal is achieved by performing preprocesses of binarization and staff-line removal. However, these are commonly performed using heuristics that do not generalize widely when applied to different types of documents such as medieval scores. In this paper we propose an effective and generalizable approach to address this problem in one step. Our proposal classifies each pixel of the image among background, staff lines, and symbols using supervised learning techniques, namely convolutional neural networks. Experiments on a set of medieval music pages proved that the proposed approach is very accurate, achieving a performance upwards of 90% and outperforming common ensembles of binarization and staff-line removal algorithms.",CAN,facility,Developed economies,"[33.62032, 21.535757]","[-21.814163, 41.57136]","[19.828144, 5.0839386, 6.2072425]","[-16.271006, -21.135134, -0.47262534]","[9.063035, 6.690051]","[6.572434, -0.91145253]","[11.181132, 11.991557, -0.070686854]","[7.943442, 4.102263, 10.20645]"
91,Jordan B. L. Smith;Masataka Goto,Multi-Part Pattern Analysis: Combining Structure Analysis and Source Separation to Discover Intra-Part Repeated Sequences.,2017,https://doi.org/10.5281/zenodo.1417685,Jordan B. L. Smith+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Structure is usually estimated as a single-level phenomenon with full-texture repeats and homogeneous sections. However, structure is actually multi-dimensional: in a typical piece of music, individual instrument parts can repeat themselves in independent ways, and sections can be homogeneous with respect to several parts or only one part. We propose a novel MIR task, multi-part pattern analysis, that requires the discovery of repeated patterns within instrument parts. To discover repeated patterns in individual voices, we propose an algorithm that applies source separation and then tailors the structure analysis to each estimated source, using a novel technique to resolve transitivity errors. Creating ground truth for this task by hand would be infeasible for a large corpus, so we generate a synthetic corpus from MIDI files. We synthesize audio and produce measure-by-measure descriptions of which instruments are active and which repeat themselves exactly. Lastly, we present a set of appropriate evaluation metrics, and use them to compare our approach to a set of baselines.",JPN,facility,Developed economies,"[16.545233, 24.094576]","[-11.64516, -12.8324585]","[-0.72860706, -16.418509, 10.677283]","[-4.9045672, -14.705169, -5.2650437]","[11.553214, 7.4590797]","[7.310093, 2.1998832]","[12.45126, 13.215825, -0.6708732]","[9.462575, 6.5610747, 10.699835]"
90,Eric J. Humphrey;Nicola Montecchio;Rachel M. Bittner;Andreas Jansson;Tristan Jehan,Mining Labeled Data from Web-Scale Collections for Vocal Activity Detection in Music.,2017,https://doi.org/10.5281/zenodo.1417769,"Eric J. Humphrey+Spotify>USA>company;Nicola Montecchio+Spotify>USA>company;Rachel Bittner+Spotify>USA>company|Music, Audio & Research Lab (MARL)>USA>education;Andreas Jansson+Spotify>USA>company|City University>GBR>education;Tristan Jehan+Spotify>USA>company","This work demonstrates an approach to generating strongly labeled data for vocal activity detection by pairing instrumental versions of songs with their original mixes. Though such pairs are rare, we find ample instances in a massive music collection for training deep convolutional networks at this task, achieving state of the art performance with a fraction of the human effort required previously. Our error analysis reveals two notable insights: imperfect systems may exhibit better temporal precision than human annotators, and should be used to accelerate annotation; and, machine learning from mined data can reveal subtle biases in the data source, leading to a better understanding of the problem itself. We also discuss future directions for the design and evolution of benchmarking datasets to rigorously evaluate AI systems.",USA,company,Developed economies,"[-9.39093, -33.570343]","[-27.723959, -35.261715]","[15.625854, 14.695926, -12.044407]","[-6.1305094, -5.121153, -19.904652]","[10.243442, 11.1515465]","[8.171019, 5.1277366]","[11.466625, 15.210992, 0.62698615]","[9.91848, 6.956196, 8.979867]"
89,Georgi Dzhambazov;Andre Holzapfel;Ajay Srinivasamurthy;Xavier Serra,Metrical-Accent Aware Vocal Onset Detection in Polyphonic Audio.,2017,https://doi.org/10.5281/zenodo.1415086,"Georgi Dzhambazov+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Andre Holzapfel+KTH Royal Institute of Technology>SWE>education;Ajay Srinivasamurthy+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","The goal of this study is the automatic detection of onsets of the singing voice in polyphonic audio recordings. Starting with a hypothesis that the knowledge of the current position in a metrical cycle (i.e. metrical accent) can improve the accuracy of vocal note onset detection, we propose a novel probabilistic model to jointly track beats and vocal note onsets. The proposed model extends a state of the art model for beat and meter tracking, in which a-priori probability of a note at a specific metrical accent interacts with the probability of observing a vocal note onset. We carry out an evaluation on a varied collection of multi-instrument datasets from two music traditions (English popular music and Turkish makam) with different types of metrical cycles and singing styles. Results confirm that the proposed model reasonably improves vocal note onset detection accuracy compared to a baseline model that does not take metrical position into account.",ESP,education,Developed economies,"[5.2357807, -33.5114]","[-22.19685, -8.965124]","[4.777743, -22.609003, -10.609231]","[-4.218772, 4.2664113, -14.3839445]","[10.268822, 5.0177264]","[5.847514, 2.5133526]","[10.311609, 13.359783, -1.383557]","[8.162902, 7.3092375, 10.84399]"
88,Alexandros Tsaptsinos,Lyrics-Based Music Genre Classification Using a Hierarchical Attention Network.,2017,https://doi.org/10.5281/zenodo.1417241,Alexandros Tsaptsinos+Stanford University>USA>education,"Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure—in which words combine to form lines, lines form segments, and segments form a complete song—we adapt a hierarchical attention network (HAN) to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres.",USA,education,Developed economies,"[-31.542048, -16.553087]","[-8.1378, -25.99922]","[-14.903371, 8.727147, 15.24856]","[4.673341, 1.6721568, -21.538252]","[12.917684, 11.1522875]","[8.881581, 4.765032]","[14.083913, 14.399245, 1.4401757]","[10.48697, 6.751316, 9.39393]"
74,Léopold Crestel;Philippe Esling;Lena Heng;Stephen McAdams,A Database Linking Piano and Orchestral MIDI Scores with Application to Automatic Projective Orchestration.,2017,https://doi.org/10.5281/zenodo.1416204,Léopold Crestel+IRCAM>FRA>facility|McGill University>CAN>education;Philippe Esling+IRCAM>FRA>facility|McGill University>CAN>education;Lena Heng+McGill University>CAN>education;Stephen McAdams+McGill University>CAN>education,"This article introduces the Projective Orchestral Database (POD), a collection of MIDI scores composed of pairs linking piano scores to their corresponding orchestrations. To the best of our knowledge, this is the first database of its kind, which performs piano or orchestral prediction, but more importantly which tries to learn the correlations between piano and orchestral scores. Hence, we also introduce the projective orchestration task, which consists in learning how to perform the automatic orchestration of a piano score. We show how this task can be addressed using learning methods and also provide methodological guidelines in order to properly use this database.",FRA,facility,Developed economies,"[36.051483, -1.5818743]","[-10.50254, 5.9704947]","[11.662083, -6.758704, 21.209558]","[2.7652457, -20.777212, 4.5951347]","[10.027653, 7.0171084]","[8.559252, 2.7537334]","[12.281809, 11.517401, -0.56820506]","[9.625952, 6.145341, 10.96893]"
87,Karim M. Ibrahim;David Grunberg;Kat Agres;Chitralekha Gupta;Ye Wang,Intelligibility of Sung Lyrics: A Pilot Study.,2017,https://doi.org/10.5281/zenodo.1414730,"Karim M. Ibrahim+National University of Singapore>SGP>education;David Grunberg+National University of Singapore>SGP>education;Kat Agres+Institute of High Performance Computing, A*STAR>SGP>facility;Chitralekha Gupta+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education","We propose a system to automatically assess the intelligibility of sung lyrics. We are particularly interested in being able to identify songs which are intelligible to second language learners, as such individuals often sing along the song to help them learn their second language, but this is only helpful if the song is intelligible enough for them to understand. As no automatic system for identifying ‘intelligible’ songs currently exists, songs for second language learners are generally selected by hand, a time-consuming and onerous process. We conducted an experiment in which test subjects, all of whom are learning English as a second language, were presented with 100 excerpts of songs drawn from five different genres. The test subjects listened to and transcribed the excerpts and the intelligibility of each excerpt was assessed based on average transcription accuracy across subjects. Excerpts that were more accurately transcribed on average were considered to be more intelligible than those less accurately transcribed on average. We then tested standard acoustic features to determine which were most strongly correlated with intelligibility. Our final system classifies the intelligibility of the excerpts and achieves 66% accuracy for 3 classes of intelligibility.",SGP,education,Developing economies,"[-32.44038, -26.12696]","[11.920038, -24.432697]","[8.630277, 21.657644, 3.4850183]","[5.9567533, 14.600739, 1.5420777]","[11.56964, 11.828371]","[9.358108, 2.840941]","[12.61255, 15.797918, 1.0539675]","[11.461495, 6.643905, 11.024973]"
85,Iris Yuping Ren;Hendrik Vincent Koops;Anja Volk;Wouter Swierstra,In Search of the Consensus Among Musical Pattern Discovery Algorithms.,2017,https://doi.org/10.5281/zenodo.1417105,Iris Yuping Ren+Utrecht University>NLD>education;Hendrik Vincent Koops+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education;Wouter Swierstra+Utrecht University>NLD>education,"Patterns are an essential part of music and there are many different algorithms that aim to discover them. Based on the improvements brought by using data fusion methods to find the consensus of algorithms on other MIR tasks, we hypothesize that fusing the output from musical pattern discovery algorithms will improve the pattern discovery results. In this paper, we explore two methods to combine the pattern output from ten state-of-the-art algorithms using two datasets. Both provide human-annotated patterns as ground truth. We show that finding the consensus among the output of different musical pattern discovery algorithms is challenging for two reasons: First, the number of patterns found by the algorithms exceeds patterns in human annotations by several orders of magnitude, with little agreement on what constitutes a pattern. Second, the algorithms perform inconsistently across different pieces. We show that algorithms lack a consensus with each other. Therefore, it is difficult to harness the collective wisdom of the algorithms to find ground truth patterns. The main contribution of this paper is a meta-analysis of the (dis)similarities among pattern discovery algorithms’ output and using the output in two fusion methods. Furthermore, we discuss the implication of our results for the MIREX task.",NLD,education,Developed economies,"[14.5227785, 21.645905]","[2.6831446, 18.553484]","[-0.7689455, -10.467589, 7.7160177]","[-2.4984374, -12.173362, 7.620424]","[11.585186, 7.811493]","[9.166849, 1.4242706]","[12.7867, 13.418078, -0.5569481]","[10.698818, 6.0944233, 11.4029255]"
84,Vytaute Kedyte;Maria Panteli;Tillman Weyde;Simon Dixon,Geographical Origin Prediction of Folk Music Recordings from the United Kingdom.,2017,https://doi.org/10.5281/zenodo.1417991,Vytaute Kedyte+City University of London>GBR>education;Maria Panteli+Queen Mary University of London>GBR>education;Tillman Weyde+City University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Field recordings from ethnomusicological research since the beginning of the 20th century are available today in large digitised music archives. The application of music information retrieval and data mining technologies can aid large-scale data processing leading to a better understanding of the history of cultural exchange. In this paper we focus on folk and traditional music from the United Kingdom and study the correlation between spatial origins and musical characteristics. In particular, we investigate whether the geographical location of music recordings can be predicted solely from the content of the audio signal. We build a neural network that takes as input a feature vector capturing musical aspects of the audio signal and predicts the latitude and longitude of the origins of the music recording. We explore the performance of the model for different sets of features and compare the prediction accuracy between geographical regions of the UK. Our model predicts the geographical coordinates of music recordings with an average error of less than 120 km. The model can be used in a similar manner to identify the origins of recordings in large unlabelled music collections and reveal patterns of similarity in music from around the world.",GBR,education,Developed economies,"[-29.48017, 16.825565]","[23.535723, -0.89673275]","[-21.484604, -8.03934, 1.7072568]","[10.406604, 6.2673616, 1.5986155]","[13.849786, 8.824544]","[9.716512, 2.7399511]","[14.094934, 14.744877, -0.85216767]","[11.397462, 6.755702, 11.570304]"
83,Yifei Teng;Anny Zhao;Camille Goudeseune,Generating Nontrivial Melodies for Music as a Service.,2017,https://doi.org/10.5281/zenodo.1416336,Yifei Teng+University of Illinois>USA>education;Anny Zhao+University of Illinois>USA>education;Camille Goudeseune+University of Illinois>USA>education,"We present a hybrid neural network and rule-based system that generates pop music. Music produced by pure rule-based systems often sounds mechanical. Music produced by machine learning sounds better, but still lacks hierarchical temporal structure. We restore temporal hierarchy by augmenting machine learning with a temporal production grammar, which generates the music’s overall structure and chord progressions. A compatible melody is then generated by a conditional variational recurrent autoencoder. The autoencoder is trained with eight-measure segments from a corpus of 10,000 MIDI files, each of which has had its melody track and chord progressions identified heuristically. The autoencoder maps melody into a multi-dimensional feature space, conditioned by the underlying chord progression. A melody is then generated by feeding a random sample from that space to the autoencoder’s decoder, along with the chord progression generated by the grammar. The autoencoder can make musically plausible variations on an existing melody, suitable for recurring motifs. It can also reharmonize a melody to a new chord progression, keeping the rhythm and contour. The generated music compares favorably with that generated by other academic and commercial software designed for the music-as-a-service industry.",USA,education,Developed economies,"[10.7334585, -4.4463863]","[-7.3434353, -39.49242]","[10.138245, 9.466787, 4.170797]","[-23.20504, 2.9156976, -7.765049]","[10.539915, 9.485354]","[9.241425, 6.179276]","[11.565218, 15.0096655, -0.891345]","[9.690623, 5.4196215, 9.050026]"
82,Jonas Langhabel;Robert Lieck;Marc Toussaint;Martin Rohrmeier,Feature Discovery for Sequential Prediction of Monophonic Music.,2017,https://doi.org/10.5281/zenodo.1418249,Jonas Langhabel+TU Berlin>DEU>education;Robert Lieck+University of Stuttgart>DEU>education|TU Dresden>DEU>education;Marc Toussaint+University of Stuttgart>DEU>education|TU Dresden>DEU>education;Martin Rohrmeier+TU Dresden>DEU>education|EPFL>CHE>education,"Learning a model for sequential prediction of symbolic music remains an open challenge. An important special case is the prediction of pitch sequences based on a corpus of monophonic music. We contribute to this line of research in two respects: (1) Our models improve the state-of-the-art performance. (2) Our method affords learning interpretable models by discovering an explicit set of relevant features. We discover features using the PULSE learning framework, which repetitively suggests new candidate features using a generative operation and selects features while optimizing the underlying model. Defining a domain-specific generative operation allows to combine multiple music-theoretically motivated features in a unified model and to control their interaction on a fine-grained level. We evaluate our models on a set of benchmark corpora of monophonic chorales and folk songs, outperforming previous work. Finally, we discuss the characteristics of the discovered features from a musicological perspective, giving concrete examples.",DEU,education,Developed economies,"[-10.941362, -1.5632126]","[-12.293433, -33.60468]","[-3.9144478, -0.9900094, 10.604138]","[-13.36398, -2.3062556, -12.148471]","[11.48778, 8.560318]","[9.092796, 5.405911]","[13.056551, 13.348079, 0.096972615]","[9.901523, 5.936465, 9.419559]"
81,Frank Zalkow;Christof Weiß;Meinard Müller,Exploring Tonal-Dramatic Relationships in Richard Wagner's Ring Cycle.,2017,https://doi.org/10.5281/zenodo.1415760,Frank Zalkow+International Audio Laboratories Erlangen>DEU>facility;Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Richard Wagner’s cycle Der Ring des Nibelungen, consisting of four music dramas, constitutes a comprehensive work of high importance for Western music history. In this paper, we indicate how MIR methods can be applied to explore this large-scale work with respect to tonal properties. Our investigations are based on a data set that contains 16 audio recordings of the entire Ring as well as extensive annotations including measure positions, singer activities, and leitmotif regions. As a basis for the tonal analysis, we make use of common audio features, which capture local chord and scale information. Employing a cross-version approach, we show that global histogram representations can reflect certain tonal relationships in a robust way. Based on our annotations, a musicologist may easily select and compare passages associated with dramatic aspects, for example, the appearance of specific characters or the presence of particular leitmotifs. Highlighting and investigating such passages may provide insights into the role of tonality for the dramatic conception of Wagner’s Ring. By giving various concrete examples, we indicate how our approach may open up new ways for exploring large musical corpora in an intuitive and interactive way.",DEU,facility,Developed economies,"[5.2958784, 24.65074]","[-17.618814, 13.403851]","[-12.87327, -13.48796, -3.9796596]","[-10.395636, 2.1414497, 2.1912804]","[12.674604, 7.1084647]","[8.271393, 2.2663033]","[13.205159, 13.521655, -1.7632046]","[10.174467, 6.8149867, 11.892922]"
80,Katherine M. Kinnaird,Examining Musical Meaning in Similarity Thresholds.,2017,https://doi.org/10.5281/zenodo.1417721,Katherine M. Kinnaird+Brown University>USA>education,"Many approaches to Music Information Retrieval tasks rely on correctly determining if two segments of a given musical recording are repeats of each other. Repetitions in recordings are rarely exact, and identifying the appropriate threshold for these pairwise decisions is crucial for tuning MIR algorithms. However, current approaches for determining and reporting this threshold parameter are devoid of contextual meaning and interpretations, which makes comparing previous results difficult and which requires access to specific datasets. This paper highlights weaknesses in current approaches to choosing similarity thresholds, provides a framework using the proportion of orthogonal musical change to tie thresholds back to feature spaces with the cosine dissimilarity measure, and introduces new research possibilities given a music-centered approach for selecting similarity thresholds.",USA,education,Developed economies,"[-1.4065084, 12.91266]","[25.525555, 7.7123923]","[-7.4400496, 3.944621, 3.0699875]","[9.959167, 1.5745217, 8.932167]","[12.802707, 9.468205]","[10.327562, 2.164871]","[13.346585, 15.283045, -0.73929834]","[11.955176, 6.419291, 12.2922945]"
79,Hoon Heo;Hyunwoo J. Kim;Wan Soo Kim;Kyogu Lee,Cover Song Identification with Metric Learning Using Distance as a Feature.,2017,https://doi.org/10.5281/zenodo.1416556,Hoon Heo+Seoul National University>KOR>education;Hyunwoo J. Kim+University of Wisconsin–Madison>USA>education;Wan Soo Kim+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"Most of cover song identification algorithms are based on the pairwise (dis)similarity between two songs which are represented by harmonic features such as chroma, and therefore the choice of a distance measure and a feature has a significant impact on performance. Furthermore, since the similarity measure is query-dependent, it cannot represent an absolute distance measure. In this paper, we present a novel approach to tackle the cover song identification problem from a new perspective. We first construct a set of core songs, and represent each song in a high-dimensional space where each dimension indicates the pairwise distance between the given song and the other in the pre-defined core set. There are several advantages to this. First, using a number of reference songs in the core set, we make the most of relative distances to many other songs. Second, as all songs are transformed into the same high-dimensional space, kernel methods and metric learning are exploited for distance computation. Third, our approach does not depend on the computation method for the pairwise distance, and thus can use any existing algorithms. Experimental results confirm that the proposed approach achieved a large performance gain compared to the state-of-the-art methods.",KOR,education,Developing economies,"[7.7581444, 43.3126]","[27.689667, -10.904133]","[4.6609125, 11.864224, -22.30767]","[18.04338, -3.068057, 2.471584]","[16.110165, 11.082637]","[10.284665, 2.6450396]","[12.8792515, 17.315683, -0.36727542]","[11.824652, 6.706968, 11.915568]"
78,Hyungui Lim;Seungyeon Rhyu;Kyogu Lee,Chord Generation from Symbolic Melody Using BLSTM Networks.,2017,https://doi.org/10.5281/zenodo.1417327,Hyungui Lim+Seoul National University>KOR>education;Seungyeon Ryu+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"Generating a chord progression from a monophonic melody is a challenging problem because a chord progression requires a series of layered notes played simultaneously. This paper presents a novel method of generating chord sequences from a symbolic melody using bidirectional long short-term memory (BLSTM) networks trained on a lead sheet database. To this end, a group of feature vectors composed of 12 semitones is extracted from the notes in each bar of monophonic melodies. In order to ensure that the data shares uniform key and duration characteristics, the key and the time signatures of the vectors are normalized. The BLSTM networks then learn from the data to incorporate the temporal dependencies to produce a chord progression. Both quantitative and qualitative evaluations are conducted by comparing the proposed method with the conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8% and 11.4% performance increase from the other models, respectively. User studies further confirm that the chord sequences generated by the proposed method are preferred by listeners.",KOR,education,Developing economies,"[50.291946, -0.64214635]","[-37.127583, 20.804585]","[18.745777, -15.423816, 16.57027]","[-23.410648, -0.2016983, -1.8258567]","[7.493893, 8.645438]","[6.088182, 3.9647825]","[12.250335, 10.831352, 1.4171181]","[9.5062, 8.907874, 12.209731]"
77,Chih-Wei Wu;Alexander Lerch,Automatic Drum Transcription Using the Student-Teacher Learning Paradigm with Unlabeled Music Data.,2017,https://doi.org/10.5281/zenodo.1415904,Chih-Wei Wu+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"Automatic drum transcription is a sub-task of automatic music transcription that converts drum-related audio events into musical notation. While noticeable progress has been made in the past by combining pattern recognition methods with audio signal processing techniques, the major limitation of many state-of-the-art systems still originates from the difficulty of obtaining a meaningful amount of annotated data to support the data-driven algorithms. In this work, we address the challenge of insufficiently labeled data by exploring the possibility of utilizing unlabeled music data from online resources. Specifically, a student neural network is trained using the labels generated from multiple teacher systems. The performance of the model is evaluated on a publicly available dataset. The results show the general viability of using unlabeled music data to improve the performance of drum transcription systems.",USA,education,Developed economies,"[29.768463, -44.684776]","[-37.47714, -16.018925]","[22.78921, -21.70464, 6.5401654]","[3.0025136, 13.118916, -21.280338]","[7.593284, 7.164539]","[8.417379, 4.4848332]","[10.33567, 11.477043, 1.0315554]","[9.254591, 7.14798, 9.686485]"
86,Ajay Srinivasamurthy;Andre Holzapfel;Xavier Serra,Informed Automatic Meter Analysis of Music Recordings.,2017,https://doi.org/10.5281/zenodo.1415688,"Ajay Srinivasamurthy+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Andre Holzapfel+KTH Royal Institute of Technology>SWE>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Automatic meter analysis aims to annotate a recording of a metered piece of music with its metrical structure. This analysis subsumes correct estimation of the type of meter, the tempo, and the alignment of the metrical structure with the music signal. Recently, Bayesian models have been successfully applied to several of meter analysis tasks, but depending on the musical context, meter analysis still poses significant challenges. In this paper, we investigate if there are benefits to automatic meter analysis from additional a priori information about the metrical structure of music. We explore informed automatic meter analysis, in which varying levels of prior information about the metrical structure of the music piece is available to analysis algorithms. We formulate different informed meter analysis tasks and discuss their practical applications, with a focus on Indian art music. We then adapt state of the art Bayesian meter analysis methods to these tasks and evaluate them on corpora of Indian art music. The experiments show that the use of additional information aids meter analysis and improves automatic meter analysis performance, with significant gains for analysis of downbeats.",ESP,education,Developed economies,"[9.848448, 16.95541]","[-26.302309, -8.272477]","[-12.141481, -11.087353, 1.8350778]","[-4.234518, 15.627213, -9.823416]","[12.0422535, 6.471731]","[5.67592, 2.0955641]","[12.613037, 13.414049, -1.6514438]","[7.9824896, 6.9810634, 11.227746]"
49,Andrea Cogliati;Zhiyao Duan,A Metric for Music Notation Transcription Accuracy.,2017,https://doi.org/10.5281/zenodo.1415830,Andrea Cogliati+University of Rochester>USA>education|University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education|University of Rochester>USA>education,"Automatic music transcription aims at transcribing musical performances into music notation. However, most existing transcription systems only focus on parametric transcription, i.e., they output a symbolic representation in absolute terms, showing frequency and absolute time (e.g., a piano-roll representation), but not in musical terms, with spelling distinctions (e.g., A♭ versus G♯) and quantized meter. Recent attempts at producing full music notation output have been hindered by the lack of an objective metric to measure the adherence of the results to the ground truth music score, and had to rely on time-consuming human evaluation by music theorists. In this paper, we propose an edit distance, similar to the Levenshtein Distance used for measuring the difference between two sequences, typically strings of characters. The metric treats a music score as a sequence of sets of musical objects, ordered by their onsets. The metric reports the differences between two music scores based on twelve aspects: barlines, clefs, key signatures, time signatures, notes, note spelling, note durations, stem directions, groupings, rests, rest duration, and staff assignment. We also apply a linear regression model to the metric in order to predict human evaluations on a dataset of short music excerpts automatically transcribed into music notation.",USA,education,Developed economies,"[24.153435, -7.9934115]","[-7.889601, -13.888946]","[13.981541, -0.9645745, 7.9635563]","[-4.3900833, -11.384361, -7.25464]","[10.426576, 7.305813]","[7.0500126, 2.3358908]","[11.922653, 12.424998, -0.52391165]","[9.029317, 6.6781, 10.829527]"
61,Carlos Eduardo Cancino Chacón;Maarten Grachten;Kat Agres,From Bach to the Beatles: The Simulation of Human Tonal Expectation Using Ecologically-Trained Predictive Models.,2017,https://doi.org/10.5281/zenodo.1416886,"Carlos Cancino-Chacón+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Maarten Grachten+Johannes Kepler University>AUT>education;Kat Agres+Institute of High Performance Computing, A*STAR>SGP>facility","Tonal structure is in part conveyed by statistical regularities between musical events, and research has shown that computational models reflect tonal structure in music by capturing these regularities in schematic constructs like pitch histograms. Of the few studies that model the acquisition of perceptual learning from musical data, most have employed self-organizing models that learn a topology of static descriptions of musical contexts. Also, the stimuli used to train these models are often symbolic rather than acoustically faithful representations of musical material. In this work we investigate whether sequential predictive models of musical memory (specifically, recurrent neural networks), trained on audio from commercial CD recordings, induce tonal knowledge in a similar manner to listeners (as shown in behavioral studies in music perception). Our experiments indicate that various types of recurrent neural networks produce musical expectations that clearly convey tonal structure. Furthermore, the results imply that although implicit knowledge of tonal structure is a necessary condition for accurate musical expectation, the most accurate predictive models also use other cues beyond the tonal structure of the musical context.",AUT,facility,Developed economies,"[-2.404949, -18.479992]","[-4.7283163, -33.549637]","[1.4099776, -9.030139, 28.764568]","[-16.973753, 8.990025, -17.01125]","[11.08194, 9.0196705]","[8.653368, 5.5630937]","[12.548093, 13.538721, -0.25314504]","[9.542315, 5.9183064, 9.417393]"
47,Chitralekha Gupta;David Grunberg;Preeti Rao;Ye Wang,Towards Automatic Mispronunciation Detection in Singing.,2017,https://doi.org/10.5281/zenodo.1418073,Chitralekha Gupta+National University of Singapore>SGP>education;David Grunberg+National University of Singapore>SGP>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education;Ye Wang+National University of Singapore>SGP>education,"A tool for automatic pronunciation evaluation of singing is desirable for those learning a second language. However, efforts to obtain pronunciation rules for such a tool have been hindered by a lack of data; while many spoken-word datasets exist that can be used in developing the tool, there are relatively few sung-lyrics datasets for such a purpose. In this paper, we demonstrate a proof-of-principle for automatic pronunciation evaluation in singing using a knowledge-based approach with limited data in an automatic speech recognition (ASR) framework. To demonstrate our approach, we derive mispronunciation rules specific to South-East Asian English accents in singing based on a comparative study of the pronunciation error patterns in singing versus speech. Using training data restricted to American English speech, we evaluate different methods involving the deduced L1-specific (native language) rules for singing. In the absence of L1 phone models, we incorporate the derived pronunciation variations in the ASR framework via a novel approach that combines acoustic models for sub-phonetic segments to represent the missing L1 phones. The word-level assessment achieved by the system on singing and speech is similar, indicating that it is a promising scheme for realizing a full-fledged pronunciation evaluation system for singing in future.",SGP,education,Developing economies,"[-6.010872, -34.844345]","[-30.751745, -40.706364]","[18.914724, 12.467568, -12.627023]","[5.5907235, -7.5399504, -19.328485]","[9.916551, 11.086471]","[7.804511, 4.567728]","[11.279823, 15.210443, 0.71783537]","[10.393006, 7.4326706, 9.137536]"
22,Peter van Kranenburg;Geert Maessen,Comparing Offertory Melodies of Five Medieval Christian Chant Traditions.,2017,https://doi.org/10.5281/zenodo.1415508,Peter van Kranenburg+Utrecht University>NLD>education|Meertens Institute>Unknown>Unknown;Geert Maessen+Unknown>Unknown>Unknown,"In this study, we compare the melodies of five medieval chant traditions: Gregorian, Old Roman, Milanese, Beneventan, and Mozarabic. We present a newly created dataset containing several hundreds of offertory melodies, which are the longest and most complex within the total body of chant melodies. For each tradition, we train n-gram language models on a representation of the chants as sequence of chromatic intervals. By computing perplexities of the melodies, we get an indication of the relations between the traditions, revealing the melodies of the Gregorian tradition as most diverse. Next, we perform a classification experiment using global features of the melodies. The choice of features is informed by expert knowledge. We use properties of the intervallic content of the melodies, and properties of the melismas, revealing that significant differences exist between the traditions. For example, the Gregorian melodies contain less step-wise intervals compared to the other repertoires. Finally, we train a classifier on the perplexities as computed with the n-gram models, resulting in a very reliable classifier.",NLD,education,Developed economies,"[11.355742, -0.12594065]","[7.7150593, -2.5581987]","[15.520488, -0.59702724, -22.25367]","[-1.8087791, 9.735305, 2.1909325]","[11.212954, 10.061213]","[8.61741, 1.972095]","[11.979236, 15.312074, -0.9669479]","[9.913784, 6.859397, 12.242549]"
21,Zhengshan Shi;Kumaran Arul;Julius O. Smith,Modeling and Digitizing Reproducing Piano Rolls.,2017,https://doi.org/10.5281/zenodo.1416634,Zhengshan Shi+Stanford University>USA>education|CCRMA>USA>facility;Kumaran Arul+Stanford University>USA>education;Julius O. Smith+Stanford University>USA>education|CCRMA>USA>facility,"Reproducing piano rolls are among the early music storage mediums, preserving fine details of a piano or organ performance on a continuous roll of paper with holes punched onto them. While early acoustic recordings suffer from poor quality sound, reproducing piano rolls benefit from the fidelity of a live piano for playback, and capture all features of a performance in what amounts to an early digital data format. However, due to limited availability of well maintained playback instruments and the condition of fragile paper, rolls have remained elusive and generally inaccessible for study. This paper proposes methods for modeling and digitizing reproducing piano rolls. Starting with an optical scan, we convert the raw image data into the MIDI file format by applying histogram-based image processing and building computational models of the musical expressions encoded on the rolls. Our evaluations show that MIDI emulations from our computational models are accurate on note level and proximate the musical expressions when compared with original playback recordings.",USA,education,Developed economies,"[34.87092, 4.4904785]","[-26.736603, 33.196774]","[22.972393, 1.1268291, 21.26212]","[-9.655397, -15.853633, -2.1773877]","[10.298269, 6.921391]","[6.6128044, 0.2793963]","[12.272946, 11.711446, -0.8697539]","[8.261874, 5.1374035, 10.706179]"
20,Brian McFee;Juan Pablo Bello,Structured Training for Large-Vocabulary Chord Recognition.,2017,https://doi.org/10.5281/zenodo.1414880,Brian McFee+New York University>USA>education|Center for Data Science>USA>facility;Juan Pablo Bello+New York University>USA>education|Music and Audio Research Laboratory>USA>facility,"Automatic chord recognition systems operating in the large-vocabulary regime must overcome data scarcity: certain classes occur much less frequently than others, and this presents a significant challenge when estimating model parameters. While most systems model the chord recognition task as a (multi-class) classification problem, few attempts have been made to directly exploit the intrinsic structural similarities between chord classes. In this work, we develop a deep convolutional-recurrent model for automatic chord recognition over a vocabulary of 170 classes. To exploit structural relationships between chord classes, the model is trained to produce both the time-varying chord label sequence as well as binary encodings of chord roots and qualities. This binary encoding directly exposes similarities between related classes, allowing the model to learn a more coherent representation of simultaneous pitch content. Evaluations on a corpus of 1217 annotated recordings demonstrate substantial improvements compared to previous models.",USA,education,Developed economies,"[57.77446, -4.8833327]","[-33.972782, 22.142786]","[26.41514, -10.272297, 21.220213]","[-27.64322, -2.4248466, -0.07838293]","[6.6746554, 8.705098]","[5.9448895, 3.831998]","[11.902473, 10.323451, 2.1384606]","[9.681105, 9.10293, 12.321396]"
19,Corentin Louboutin;Frédéric Bimbot,Modeling the Multiscale Structure of Chord Sequences Using Polytopic Graphs.,2017,https://doi.org/10.5281/zenodo.1415186,Corentin Louboutin+Université Rennes 1>FRA>education;Frédéric Bimbot+CNRS - UMR 6074>FRA>facility,"Chord sequences are an essential source of information in a number of MIR tasks. However, beyond the sequential nature of musical content, relations and dependencies within a music segment can be more efficiently modeled as a graph. Polytopic Graphs have been recently introduced to model music structure so as to account for multiscale relationships between events located at metrically homologous instants. In this paper, we focus on the description of chord sequences and we study a specific set of graph configurations, called Primer Preserving Permutations (PPP). For sequences of 16 chords, PPPs account for 6 different latent systems of relations, corresponding to 6 main structural patterns (Prototypical Carrier Sequences or PCS). Observed chord sequences can be viewed as distorted versions of these PCS and the corresponding optimal PPP is estimated by minimizing a description cost over the latent relations. After presenting the main concepts of this approach, the article provides a detailed study of PPPs across a corpus of 727 chord sequences annotated from the RWC POP database (100 pop songs). Our results illustrate both qualitatively and quantitatively the potential of the proposed model for capturing long-term multiscale structure in musical data, which remains a challenge in computational music modeling and in Music Information Retrieval.",FRA,education,Developed economies,"[52.878048, -2.2750373]","[-25.089462, 18.647568]","[23.9736, -16.863173, 15.773721]","[-20.83238, -2.774283, 4.483119]","[6.9909997, 8.725919]","[7.185622, 3.1660342]","[12.044326, 10.489003, 1.909688]","[9.691025, 8.0830765, 12.118531]"
18,H. G. Ranjani;Deepak Paramashivan;Thippur V. Sreenivas,Quantized Melodic Contours in Indian Art Music Perception: Application to Transcription.,2017,https://doi.org/10.5281/zenodo.1417567,Ranjani H. G.+Indian Institute of Science>IND>education|Indian Institute of Science>IND>facility;Deepak Paramashivan+University of Alberta>CAN>education;Thippur V. Sreenivas+Indian Institute of Science>IND>education|Indian Institute of Science>IND>facility,"R¯agas in Indian Art Music have a ﬂorid dynamism associated with them. Owing to their inherent structural intricacies, the endeavor of mapping melodic contours to musical notation becomes cumbersome. We explore the potential of mapping, through quantization of melodic contours and listening test of synthesized music, to capture the nuances of r¯agas. We address both Hindustani and Carnatic music forms of Indian Art Music. Two quantization schemes are examined using stochastic models of melodic pitch. We attempt to quantify the salience of r¯aga perception from reconstructed melodic contours. Perception experiments verify that much of the r¯aga nuances inclusive of the gamaka (subtle ornamentation) structures can be retained by sampling and quantizing critical points of melodic contours. Further, we show application of this result to automatically transcribe melody of Indian Art Music.",IND,education,Developing economies,"[6.8655524, 1.4508071]","[2.9675138, -18.263372]","[6.9564114, 5.19699, -8.424339]","[14.451469, -14.506494, -6.7466283]","[11.164982, 10.2600355]","[7.452705, 1.2582228]","[11.789829, 15.513169, -1.0910078]","[8.982013, 7.116092, 12.561647]"
17,François Pachet;Alexandre Papadopoulos;Pierre Roy,Sampling Variations of Sequences for Structured Music Generation.,2017,https://doi.org/10.5281/zenodo.1416588,François Pachet+Sony CSL Paris>FRA>company;Alexandre Papadopoulos+UPMC Univ Paris 06>FRA>education;Pierre Roy+Sony CSL Paris>FRA>company,"Recently, machine-learning techniques have been successfully used for the generation of complex artifacts such as music or text. However, these techniques are still unable to capture and generate artifacts that are convincingly structured. In particular, musical sequences do not exhibit pattern structure, as typically found in human composed music. We present an approach to generate structured sequences, based on a mechanism for sampling efficiently variations of musical sequences. Given an input sequence and a statistical model, this mechanism uses belief propagation to sample a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity function defined by Mongeau and Sankoff. We then show how this mechanism can be used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem. We illustrate our approach with a convincingly structured generated lead sheet in the style of the Beatles.",FRA,company,Developed economies,"[21.16783, 4.203939]","[-4.6907725, -42.160847]","[4.660253, 0.6372249, 20.786541]","[-23.802513, -2.198653, -9.889119]","[10.167435, 8.464265]","[9.204364, 6.316755]","[13.185508, 11.945057, 0.113743156]","[9.578435, 5.3628774, 9.063335]"
16,Jeroen Peperkamp;Klaus Hildebrandt;Cynthia C. S. Liem,A Formalization of Relative Local Tempo Variations in Collections of Performances.,2017,https://doi.org/10.5281/zenodo.1415052,Jeroen Peperkamp+Delft University of Technology>NLD>education;Klaus Hildebrandt+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education,"Multiple performances of the same piece share similarities, but also show relevant dissimilarities. With regard to the latter, analyzing and quantifying variations in collections of performances is useful to understand how a musical piece is typically performed, how naturally sounding new interpretations could be rendered, or what is peculiar about a particular performance. However, as there is no formal ground truth as to what these variations should look like, it is a challenge to provide and validate analysis methods for this. In this paper, we focus on relative local tempo variations in collections of performances. We propose a way to formally represent relative local tempo variations, as encoded in warping paths of aligned performances, in a vector space. This enables using statistics for analyzing tempo variations in collections of performances. We elaborate the computation and interpretation of the mean variation and the principal modes of variation. To validate our analysis method despite the absence of a ground truth, we present results on artificially generated data, representing several categories of local tempo variations. Finally, we show how our method can be used for analyzing real-world data and discuss potential applications.",NLD,education,Developed economies,"[42.369347, -26.230227]","[-28.786747, 5.1215596]","[-2.9343073, -30.870445, 1.0150285]","[-9.188578, 6.5711164, -3.1669192]","[11.4449625, 4.4848666]","[5.894624, 1.3148427]","[10.975348, 13.285081, -2.8499465]","[8.246021, 6.269174, 11.323436]"
15,Richard Vogl;Matthias Dorfer;Gerhard Widmer;Peter Knees,Drum Transcription via Joint Beat and Drum Modeling Using Convolutional Recurrent Neural Networks.,2017,https://doi.org/10.5281/zenodo.1415136,"Richard Vogl+Institute of Software Technology & Interactive Systems, Vienna University of Technology>AUT>education|Dept. of Computational Perception, Johannes Kepler University Linz>AUT>education;Matthias Dorfer+Dept. of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Dept. of Computational Perception, Johannes Kepler University Linz>AUT>education;Peter Knees+Institute of Software Technology & Interactive Systems, Vienna University of Technology>AUT>education","Existing systems for automatic transcription of drum tracks from polyphonic music focus on detecting drum instrument onsets but lack consideration of additional meta information like bar boundaries, tempo, and meter. We address this limitation by proposing a system which has the capability to detect drum instrument onsets along with the corresponding beats and downbeats. In this design, the system has the means to utilize information on the rhythmical structure of a song which is closely related to the desired drum transcript. To this end, we introduce and compare different architectures for this task, i.e., recurrent, convolutional, and recurrent-convolutional neural networks. We evaluate our systems on two well-known data sets and an additional new data set containing both drum and beat annotations. We show that convolutional and recurrent-convolutional neural networks perform better than state-of-the-art methods and that learning beats jointly with drums can be beneficial for the task of drum detection.",AUT,education,Developed economies,"[27.887539, -45.95357]","[-34.1979, -14.695115]","[20.085993, -22.66596, 3.2031257]","[-2.0869489, 10.893826, -18.832518]","[7.5921764, 7.1985016]","[8.072113, 4.562303]","[10.340698, 11.486125, 1.0475566]","[8.740747, 7.0091734, 9.658704]"
14,Keunwoo Choi;György Fazekas;Mark B. Sandler;Kyunghyun Cho,Transfer Learning for Music Classification and Regression Tasks.,2017,https://doi.org/10.5281/zenodo.1418015,Keunwoo Choi+Queen Mary University of London>GBR>education;György Fazekas+Queen Mary University of London>GBR>education;Mark Sandler+Queen Mary University of London>GBR>education;Kyunghyun Cho+New York University>USA>education,"In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features.",GBR,education,Developed economies,"[-17.45455, -11.353796]","[-23.291079, -37.405857]","[3.3834455, 2.7310236, 10.286974]","[-4.8276567, 0.51384896, -19.908278]","[11.305186, 9.348185]","[9.770843, 4.6394753]","[13.205519, 13.107199, 0.9310429]","[10.909714, 6.5238705, 9.046597]"
48,Shuo Zhang;Rafael Caro Repetto;Xavier Serra,Understanding the Expressive Functions of Jingju Metrical Patterns Through Lyrics Text Mining.,2017,https://doi.org/10.5281/zenodo.1416608,Shuo Zhang+Universitat Pompeu Fabra>ESP>education;Rafael Caro Repetto+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"The emotional content of jingju (aka Beijing or Peking opera) arias is conveyed through pre-defined metrical patterns known as banshi, each of them associated with a specific expressive function. In this paper, we first report the work on a comprehensive corpus of jingju lyrics that we built, suitable for text mining and text analysis in a data-driven framework. Utilizing this corpus, we propose a novel approach to study the expressive functions of banshi by applying text analysis techniques on lyrics. First we apply topic modeling techniques to jingju lyrics text documents grouped at different levels according to the banshi they are associated with. We then experiment with several different document vector representations of lyrics in a series of document classification experiments. The topic modeling results showed that sentiment polarity (positive or negative) is better distinguished between different shengqiang-banshi (a more fine grained partition of banshi) than banshi alone, and we are able to achieve high accuracy scores in classifying lyrics documents into different banshi categories. We discuss the technical and musicological implications and possible future improvements.",ESP,education,Developed economies,"[-32.188606, -30.60868]","[57.373962, -5.062204]","[4.9074154, 22.315577, 0.8318752]","[15.409904, 25.44143, 9.672427]","[11.541732, 11.590483]","[12.862178, 3.7530928]","[12.520927, 15.789516, 0.7758018]","[14.159327, 5.305905, 10.898887]"
12,Bochen Li;Karthik Dinesh;Gaurav Sharma 0001;Zhiyao Duan,Video-Based Vibrato Detection and Analysis for Polyphonic String Music.,2017,https://doi.org/10.5281/zenodo.1417249,Bochen Li+University of Rochester>USA>education;Karthik Dinesh+University of Rochester>USA>education;Gaurav Sharma+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"In music performance, vibrato is an important artistic effect, where slight variations in pitch are introduced to add expressiveness and warmth. Automatic vibrato detection and analysis, although well studied for monophonic music, has rarely been explored for polyphonic music, because of the challenge in multi-pitch analysis. We propose a video-based approach for detecting and analyzing vibrato in polyphonic string music. Specifically, we capture the fine motion of the left hand of string players through optical flow analysis of video frames. We explore two methods. The first uses a feature extraction and SVM classification pipeline, and the second is an unsupervised technique based on autocorrelation analysis of the principal motion component. The proposed methods are compared with audio-only methods applied to individual instrument tracks separated from original audio mixture using the score. Experiments show that the proposed video-based methods achieve a significantly higher vibrato detection accuracy than the audio-based methods especially in high polyphony cases. Further experiments also demonstrate the utility of the approach in vibrato rate and extent analysis.",USA,education,Developed economies,"[51.310467, 18.004986]","[-37.95568, 10.394793]","[10.706193, -29.209057, 14.387273]","[-17.349539, 15.997313, -5.910689]","[10.846763, 7.1828594]","[6.311108, 2.811406]","[11.6081295, 13.08475, -0.24746911]","[8.806488, 7.3792386, 10.801946]"
11,Matthias Dorfer;Andreas Arzt;Gerhard Widmer,Learning Audio-Sheet Music Correspondences for Score Identification and Offline Alignment.,2017,https://doi.org/10.5281/zenodo.1417807,Matthias Dorfer+Johannes Kepler University Linz>AUT>education;Andreas Arzt+Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"This work addresses the problem of matching short excerpts of audio with their respective counterparts in sheet music images. We show how to employ neural network-based cross-modality embedding spaces for solving the following two sheet music-related tasks: retrieving the correct piece of sheet music from a database when given a music audio as a search query; and aligning an audio recording of a piece with the corresponding images of sheet music. We demonstrate the feasibility of this in experiments on classical piano music by five different composers (Bach, Haydn, Mozart, Beethoven and Chopin), and additionally provide a discussion on why we expect multi-modal neural networks to be a fruitful paradigm for dealing with sheet music and audio at the same time.",AUT,education,Developed economies,"[19.984213, -10.66259]","[-22.469135, -30.48835]","[0.178742, -11.63039, -11.221189]","[-11.810609, -15.563938, -14.111284]","[10.789338, 6.417011]","[8.736348, 5.127858]","[12.027784, 12.438311, -1.5005453]","[9.922255, 6.4377112, 8.9572315]"
10,Meijun Liu;Xiao Hu 0001;Markus Schedl,"Artist Preferences and Cultural, Socio-Economic Distances Across Countries: A Big Data Perspective.",2017,https://doi.org/10.5281/zenodo.1417193,Meijun Liu+The University of Hong Kong>HKG>education;Xiao Hu+The University of Hong Kong>HKG>education;Markus Schedl+Johannes Kepler University Linz>AUT>education,"Users in different countries may have different music preferences, possibly due to geographical, economic, linguistic, and cultural factors. Revealing the relationship between music preference and cultural socio-economic differences across countries is of great importance for music information retrieval in a cross-country or cross-cultural context. Existing works are usually based on small samples in one or several countries or take only one or two socio-economic aspects into account. To bridge the gap, this study makes use of a large-scale music listening dataset, LFM-1b with more than one billion music listening logs, to explore possible associations between a variety of cultural and socio-economic measurements and artist preferences in 20 countries. From a big data perspective, the results reveal: 1) there is a highly uneven distribution of preferred artists across countries; 2) the linguistic differences among these countries are positively associated with the distances in artist preferences; 3) country differences in three of the six cultural dimensions considered in this study have positive influences on the difference of artist preferences among the countries; and 4) geographical and economic distances among the countries have no significant relationship with their artist preferences.",HKG,education,Developing economies,"[-41.515755, 10.98771]","[45.650192, 14.437441]","[-27.114819, 8.051873, 7.9595265]","[15.667839, 13.527968, 15.190022]","[14.420866, 9.879345]","[12.799651, 2.0934618]","[15.09587, 14.829612, -0.2514078]","[13.564065, 5.028301, 12.057722]"
9,Gabriel Vigliensoni;Ichiro Fujinaga,The Music Listening Histories Dataset.,2017,https://doi.org/10.5281/zenodo.1417499,Gabriel Vigliensoni+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"""We introduce the Music Listening Histories Dataset (MLHD), a large-scale collection of music listening events assembled from more than 27 billion time-stamped logs extracted from Last.fm. The logs are organized in the form of listening histories per user, and have been conveniently preprocessed and cleaned. Attractive features of the MLHD are the self-declared metadata provided by users at the moment of registration whose identities have been anonymized, MusicBrainz identifiers for the music entities in each of the logs that allows for an easy linkage to other existing resources, and a set of user profiling features designed to describe aspects of their music listening behavior and activity. We describe the process of assembling the dataset, its content, its demographic characteristics, and discuss about the possible uses of this collection, which, currently, is the largest research dataset of this kind in the field.""",CAN,education,Developed economies,"[-22.966818, 11.556764]","[42.155117, 21.400475]","[-13.182769, 17.19809, -13.200404]","[11.012907, 9.473302, 13.721201]","[13.461347, 7.535592]","[12.426913, 1.5891593]","[14.540522, 14.014514, -1.5163469]","[13.148979, 5.0015554, 12.401853]"
8,Dmitry Bogdanov;Xavier Serra,Quantifying Music Trends and Facts Using Editorial Metadata from the Discogs Database.,2017,https://doi.org/10.5281/zenodo.1416376,Dmitry Bogdanov+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown;Xavier Serra+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown,"While a vast amount of editorial metadata is being actively gathered and used by music collectors and enthusiasts, it is often neglected by music information retrieval and musicology researchers. In this paper we propose to explore Discogs, one of the largest databases of such data available in the public domain. Our main goal is to show how large-scale analysis of its editorial metadata can raise questions and serve as a tool for musicological research on a number of example studies. The metadata that we use describes music releases, such as albums or EPs. It includes information about artists, tracks and their durations, genre and style, format (such as vinyl, CD, or digital files), year and country of each release. Using this data we study correlations between different genre and style labels, assess their specificity and analyze typical track durations. We estimate trends in prevalence of different genres, styles, and formats across different time periods. In our analysis of styles we use electronic music as an example. Our contribution also includes the tools we developed for our analysis and the generated datasets that can be re-used by MIR researchers and musicologists.",ESP,education,Developed economies,"[-27.608046, 15.20581]","[45.995174, 7.029927]","[-24.159277, 5.648862, -5.291282]","[12.889115, 10.708291, 6.963019]","[14.180678, 9.209315]","[11.712428, 2.7569437]","[14.968926, 13.97615, -1.0292103]","[12.82306, 5.4524508, 11.634513]"
7,T. J. Tsai;Steven K. Tjoa;Meinard Müller,Make Your Own Accompaniment: Adapting Full-Mix Recordings to Match Solo-Only User Recordings.,2017,https://doi.org/10.5281/zenodo.1417018,"TJ Tsai+Harvey Mudd College>USA>education;Steven K. Tjoa+Galvanize, Inc.>USA>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility","We explore the task of generating an accompaniment track for a musician playing the solo part of a known piece. Unlike previous work in real-time accompaniment, we focus on generating the accompaniment track in an off-line fashion by adapting a full-mix recording (e.g. a professional CD recording or Youtube video) to match the user’s tempo preferences. The input to the system is a set of recorded passages of a solo part played by the user (e.g. solo part in a violin concerto). These recordings are contiguous segments of music where the soloist part is active. Based on this input, the system identifies the corresponding passages within a full-mix recording of the same piece (i.e. contains both solo and accompaniment parts), and these passages are temporally warped to run synchronously to the solo-only recordings. The warped passages can serve as accompaniment tracks for the user to play along with at a tempo that matches his or her ability or desired interpretation. As the main technical contribution, we introduce a segmental dynamic time warping algorithm that simultaneously solves both the passage identification and alignment problems. We demonstrate the effectiveness of the proposed system on a pilot data set for classical violin.",USA,education,Developed economies,"[16.284357, 0.79787534]","[-16.190151, -16.058002]","[6.7255063, 12.764489, 23.451506]","[2.8329089, -23.98067, -2.1421626]","[10.498639, 8.757063]","[5.9196706, 1.0173827]","[13.542094, 12.192885, -0.13718002]","[8.06281, 6.0581193, 11.096001]"
6,Justin Salamon;Rachel M. Bittner;Jordi Bonada;Juan J. Bosch;Emilia Gómez;Juan Pablo Bello,An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets.,2017,https://doi.org/10.5281/zenodo.1415588,"Justin Salamon+Music and Audio Research Laboratory, New York University>USA>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rachel M. Bittner+Music and Audio Research Laboratory, New York University>USA>education;Jordi Bonada+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Juan J. Bosch+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Juan Pablo Bello+Music and Audio Research Laboratory, New York University>USA>education","Generating continuous f0 annotations for tasks such as melody extraction and multiple f0 estimation typically involves running a monophonic pitch tracker on each track of a multitrack recording and manually correcting any estimation errors. This process is labor intensive and time consuming, and consequently existing annotated datasets are very limited in size. In this paper we propose a framework for automatically generating continuous f0 annotations without requiring manual refinement: the estimate of a pitch tracker is used to drive an analysis/synthesis pipeline which produces a synthesized version of the track. Any estimation errors are now reflected in the synthesized audio, meaning the tracker’s output represents an accurate annotation. Analysis is performed using a wide-band harmonic sinusoidal modeling algorithm which estimates the frequency, amplitude and phase of every harmonic, meaning the synthesized track closely resembles the original in terms of timbre and dynamics. Finally the synthesized track is automatically mixed back into the multitrack. The framework can be used to annotate multitrack datasets for training learning-based algorithms. Furthermore, we show that algorithms evaluated on the automatically generated/annotated mixes produce results that are statistically indistinguishable from those they produce on the original, manually annotated, mixes. We release a software library implementing the proposed framework, along with new datasets for melody, bass and multiple f0 estimation.",USA,education,Developed economies,"[17.719988, 34.731167]","[-6.489965, -12.819662]","[-20.098417, -14.062432, 7.9256]","[3.230788, -15.060838, -11.1207075]","[12.459571, 6.441962]","[6.8427763, 2.6997464]","[14.164411, 12.747505, -0.82888997]","[9.028288, 7.487952, 10.667968]"
5,Rachel M. Bittner;Brian McFee;Justin Salamon;Peter Li;Juan Pablo Bello,Deep Salience Representations for F0 Estimation in Polyphonic Music.,2017,https://doi.org/10.5281/zenodo.1417937,"Rachel M. Bittner+Music and Audio Research Laboratory, New York University>USA>education|Center for Data Science, New York University>USA>education;Brian McFee+Music and Audio Research Laboratory, New York University>USA>education|Center for Data Science, New York University>USA>education;Justin Salamon+Music and Audio Research Laboratory, New York University>USA>education|Center for Data Science, New York University>USA>education;Peter Li+Music and Audio Research Laboratory, New York University>USA>education|Center for Data Science, New York University>USA>education;Juan P. Bello+Music and Audio Research Laboratory, New York University>USA>education|Center for Data Science, New York University>USA>education","Estimating fundamental frequencies in polyphonic music remains a notoriously difficult task in Music Information Retrieval. While other tasks, such as beat tracking and chord recognition have seen improvement with the application of deep learning models, little work has been done to apply deep learning methods to fundamental frequency related tasks including multi-f0 and melody tracking, primarily due to the scarce availability of labeled data. In this work, we describe a fully convolutional neural network for learning salience representations for estimating fundamental frequencies, trained using a large, semi-automatically generated f0 dataset. We demonstrate the effectiveness of our model for learning salience representations for both multi-f0 and melody tracking in polyphonic audio, and show that our models achieve state-of-the-art performance on several multi-f0 and melody datasets. We conclude with directions for future research.",USA,education,Developed economies,"[1.1474022, -21.908096]","[-27.205896, -32.334515]","[7.3446927, 0.49653232, 7.8405514]","[-8.507664, -7.8230953, -18.936665]","[10.782114, 9.087139]","[7.9741106, 5.248066]","[12.418063, 12.946439, 0.42625415]","[9.564396, 6.978882, 8.903432]"
4,Marius Miron;Jordi Janer;Emilia Gómez,Monaural Score-Informed Source Separation for Classical Music Using Convolutional Neural Networks.,2017,https://doi.org/10.5281/zenodo.1416498,Marius Miron+Universitat Pompeu Fabra>ESP>education;Jordi Janer+Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Universitat Pompeu Fabra>ESP>education,"Score information has been shown to improve music source separation when included into non-negative matrix factorization (NMF) frameworks. Recently, deep learning approaches have outperformed NMF methods in terms of separation quality and processing time, and there is scope to extend them with score information. In this paper, we propose a score-informed separation system for classical music that is based on deep learning. We propose a method to derive training features from audio files and the corresponding coarsely aligned scores for a set of classical music pieces. Additionally, we introduce a convolutional neural network architecture (CNN) with the goal of estimating time-frequency masks for source separation. Our system is trained with synthetic renditions derived from the original scores and can be used to separate real-life performances based on the same scores, provided a coarse audio-to-score alignment. The proposed system achieves better performance (SDR and SIR) and is less computationally intensive than a score-informed NMF system on a dataset comprising Bach chorales.",ESP,education,Developed economies,"[5.7642646, -46.211964]","[-37.433064, -32.528114]","[30.792356, 0.16511267, -2.7886446]","[-10.049468, -10.237752, -25.301708]","[8.415438, 10.075953]","[6.8152504, 5.6957445]","[11.0132885, 13.6368685, 1.6607037]","[9.624186, 8.318387, 8.936751]"
3,Rafael Caro Repetto;Xavier Serra,A Collection of Music Scores for Corpus Based Jingju Singing Research.,2017,https://doi.org/10.5281/zenodo.1416346,"Rafael Caro Repetto+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","The MIR research on jingju (also known as Beijing or Peking opera) music has taken audio as the main source of information. Music scores are an important resource for the musicological research of this tradition, but no machine readable ones have been available for computational analysis. In order to explore the potential of symbolic score data for jingju music research, we have expanded the Comp-Music Jingju Music Corpus, which contains mostly audio, with a collection of 92 machine readable scores, for a total of 897 melodic lines. Since our purpose is the study of jingju singing in terms of its musical system elements, we have selected the arias used as examples in reference jingju music textbooks. The collection is accompanied by scores metadata, curated annotations per score and melodic line, and a set of software tools for extracting statistical information from it. All the gathered data and developed software are available for research purposes. In this paper we first discuss the culture specific concepts that are needed for understanding the contents of the collection, followed by a detailed description of it. We then present a series of computational analyses performed on the scores and discuss some musicological findings.",ESP,education,Developed economies,"[-6.8811765, -26.862495]","[-2.2602446, 14.064144]","[9.92692, 9.618441, -17.190197]","[-1.4126656, 9.10843, 6.8031626]","[10.6966305, 10.899033]","[8.333097, 1.7803646]","[11.649043, 15.384517, 0.1232565]","[9.816407, 6.4262047, 11.853551]"
2,Kaustuv Kanti Ganguli;Preeti Rao,Towards Computational Modeling of the Ungrammatical in a Raga Performance.,2017,https://doi.org/10.5281/zenodo.1417349,Kaustuv Kanti Ganguli+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"Raga performance allows for considerable flexibility in interpretation of the raga grammar in order to incorporate elements of creativity via improvisation. It is therefore of much interest in pedagogy to understand what ungrammaticality might mean in the context of a given raga, and possibly develop means to detect this in an audio recording of the raga performance. One prominent notion is that ungrammaticality is considered to occur only when the performer “treads” on another, possibly allied, raga in a listener’s perception. With this view, we consider modeling the technical boundary of a raga as that which separates it from another raga that is closest to it in its distinctive features. We wish to find computational models that can indicate ungrammaticality using a data-driven estimation of the model parameters; i.e. the raga performances of great artists are used to obtain representations that discriminate most between same and different raga performances. We choose a well-known pair of allied ragas (Deshkar and Bhupali in north Indian classical music) for an empirical study of computational representations for the distinctive attributes of tonal hierarchy and melodic shape of a chosen common descending phrase.",IND,education,Developing economies,"[3.4524045, -3.4310882]","[3.8547091, -19.162369]","[7.0375457, 3.6617005, -22.478325]","[14.059509, -13.599362, -4.4305897]","[11.214926, 10.591723]","[7.4826627, 1.1619316]","[11.495536, 15.082446, -1.6998602]","[8.898769, 7.013204, 12.663193]"
23,Cheng-Zhi Anna Huang;Tim Cooijmans;Adam Roberts;Aaron C. Courville;Douglas Eck,Counterpoint by Convolution.,2017,https://doi.org/10.5281/zenodo.1416370,"Cheng-Zhi Anna Huang+MILA, Université de Montréal>CAN>education|Google Brain>USA>company;Tim Cooijmans+MILA, Université de Montréal>CAN>education|Google Brain>USA>company;Adam Roberts+Google Brain>USA>company;Aaron Courville+MILA, Université de Montréal>CAN>education;Douglas Eck+Google Brain>USA>company","Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition. Our model is an instance of orderless NADE [36], which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample quality, which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from [40] yields better samples than ancestral sampling, based on both log-likelihood and human evaluation.",CAN,education,Developed economies,"[-4.547544, -16.986149]","[-3.726753, -42.399387]","[2.5442472, -27.065706, 14.312229]","[-25.237122, -2.4141304, -9.20463]","[11.046908, 9.290266]","[9.205386, 6.142136]","[12.685652, 13.191609, 0.7770233]","[9.550573, 5.4465256, 9.190364]"
24,David M. Weigl;Kevin R. Page,"A Framework for Distributed Semantic Annotation of Musical Score: ""Take It to the Bridge!"".",2017,https://doi.org/10.5281/zenodo.1415894,David M. Weigl+University of Oxford>GBR>education;Kevin R. Page+University of Oxford>GBR>education,"Music notation expresses performance instructions in a way commonly understood by musicians, but printed paper parts are limited to encodings of static, a priori knowledge. In this paper we present a platform for multi-way communication between collaborating musicians through the dynamic modification of digital parts: the Music Encoding and Linked Data (MELD) framework for distributed real-time annotation of digital music scores. MELD users and software agents create semantic annotations of music concepts and relationships, which are associated with musical structure specified by the Music Encoding Initiative schema (MEI). Annotations are expressed in RDF, allowing alternative music vocabularies (e.g., popular vs. classical music structures) to be applied. The same underlying framework retrieves, distributes, and processes information that addresses semantically distinguishable music elements. Further knowledge is incorporated from external sources through the use of Linked Data. The RDF is also used to match annotation types and contexts to rendering actions which display the annotations upon the digital score. Here, we present a MELD implementation and deployment which augments the digital music scores used by musicians in a group performance, collaboratively changing the sequence within and between pieces in a set list.",GBR,education,Developed economies,"[-30.50257, 8.907414]","[14.8888035, 40.652004]","[-16.336115, 4.0612984, 2.226836]","[-0.47933477, -4.2315626, 27.383047]","[14.015521, 9.215571]","[10.731729, 0.0052403915]","[14.759615, 13.769402, -0.774428]","[11.938535, 5.119395, 11.623093]"
13,Nick Gang;Blair Kaneshiro;Jonathan Berger;Jacek P. Dmochowski,Decoding Neurally Relevant Musical Features Using Canonical Correlation Analysis.,2017,https://doi.org/10.5281/zenodo.1417137,Nick Gang+Stanford University>USA>education;Blair Kaneshiro+Stanford University>USA>education;Jonathan Berger+Stanford University>USA>education;Jacek P. Dmochowski+City College of New York>USA>education,"Music Information Retrieval (MIR) has been dominated by computational approaches. The possibility of leveraging neural systems via brain-computer interfaces is an alternative approach to annotating music. Here we test this idea by measuring correlations between musical features and brain responses in a statistically optimal fashion. Using an extensive dataset of electroencephalographic (EEG) responses to a variety of natural music stimuli, we employed Canonical Correlation Analysis to identify spatial EEG components that track temporal stimulus components. We found multiple statistically significant dimensions of stimulus-response correlation (SRC) for all songs studied. The temporal filters that maximize correlation with the neural response highlight harmonics and subharmonics of that song’s beat frequency, with different harmonics emphasized by different components. The most stimulus-driven component of the EEG has an anatomically plausible, symmetric frontocentral topography that is preserved across stimuli. Our results suggest that different neural circuits encode different temporal hierarchies of natural music. Moreover, as techniques for decoding EEG advance, it may be possible to automatically label music via brain-computer interfaces that capture neural responses that are then translated into stimulus annotations.",USA,education,Developed economies,"[-14.455111, -0.40291828]","[-0.4508, 24.192617]","[-4.243307, 3.534185, 15.236844]","[-1.0223981, 16.923351, 10.654902]","[12.337208, 8.880261]","[10.902157, 4.4150915]","[13.51119, 13.745752, -0.36149454]","[11.780211, 4.7535257, 10.581862]"
26,Hendrik Schreiber;Meinard Müller,A Post-Processing Procedure for Improving Music Tempo Estimates Using Supervised Learning.,2017,https://doi.org/10.5281/zenodo.1415046,Hendrik Schreiber+tagtraum industries incorporated>DEU>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Tempo estimation is a fundamental problem in music information retrieval and has been researched extensively. One problem still unsolved is the tendency of tempo estimation algorithms to produce results that are wrong by a small number of known factors (so-called octave errors). We propose a method that uses supervised learning to predict such tempo estimation errors. In a post-processing step, these predictions can then be used to correct an algorithm’s tempo estimates. While being simple and relying only on a small number of features, our proposed method significantly increases accuracy for state-of-the-art tempo estimation methods.",DEU,company,Developed economies,"[39.61477, -25.439415]","[-31.281687, -5.7929854]","[-1.223351, -27.81466, 0.66988295]","[-8.148922, 10.666089, -10.9746685]","[11.504281, 4.500867]","[5.053794, 1.8204077]","[10.9155655, 13.330046, -2.8474007]","[7.3467097, 6.8846855, 10.969406]"
25,Junichi Suzuki;Tetsuro Kitahara,A Music Player with Song Selection Function for a Group of People.,2017,https://doi.org/10.5281/zenodo.1414868,Jun’ichi Suzuki+Nihon University>JPN>education|Nihon University>JPN>education;Tetsuro Kitahara+Nihon University>JPN>education|Nihon University>JPN>education,"There are often situations in which a group of people gather and listen to the same songs. However, majority of existing studies related to music information retrieval (MIR) have focused on personalization for individual users, and there have been only a few studies related to MIR intended for a group of people. Here, we present an Android music player with a music selection function for people who are listening to the same songs in the same place. We assume that each user owns his/her favorite songs on his/her Android device. Once a group of users gathers each user can launch this player on his/her smartphone. Then, the player running on each device starts to communicate with other devices via Bluetooth. Information about songs stored in every device, along with the playback history, is collected to a device referred to as the master device. Then, the master device estimates each user’s preference for every song based on playback history and music similarity. The master device then extracts songs that are highly preferred and sends a command to start playback to the devices storing these songs. Our experimental results demonstrate the successful estimation of music preferences based on music similarity.",JPN,education,Developed economies,"[-28.872229, 26.314833]","[37.77224, 18.420141]","[-11.97383, 15.343371, -17.692316]","[13.089983, 4.3525577, 16.441164]","[14.734555, 7.941931]","[12.282914, 1.8283257]","[14.449906, 14.810007, -2.229759]","[13.194954, 5.20073, 12.785076]"
46,Jordi Pons;Rong Gong;Xavier Serra,Score-Informed Syllable Segmentation for A Cappella Singing Voice with Convolutional Neural Networks.,2017,https://doi.org/10.5281/zenodo.1415632,Jordi Pons+Universitat Pompeu Fabra>ESP>education;Rong Gong+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"This paper introduces a new score-informed method for the segmentation of jingju a cappella singing phrase into syllables. The proposed method estimates the most likely sequence of syllable boundaries given the estimated syllable onset detection function (ODF) and its score. Throughout the paper, we first examine the jingju syllables structure and propose a definition of the term “syllable onset”. Then, we identify which are the challenges that jingju a cappella singing poses. Further, we investigate how to improve the syllable ODF estimation with convolutional neural networks (CNNs). We propose a novel CNN architecture that allows to efficiently capture different time-frequency scales for estimating syllable onsets. Besides, we propose using a score-informed Viterbi algorithm – instead of thresholding the onset function–, because the available musical knowledge we have (the score) can be used to inform the Viterbi algorithm to overcome the identified challenges. The proposed method outperforms the state-of-the-art in syllable segmentation for jingju a cappella singing. We further provide an analysis of the segmentation errors which points possible research directions.",ESP,education,Developed economies,"[-7.1814265, -38.378048]","[-31.384354, -38.349567]","[24.286257, 13.983336, -12.369007]","[1.3485893, -7.7415485, -17.880157]","[9.79628, 11.053203]","[7.726895, 4.6038866]","[11.1920805, 15.070217, 0.8345578]","[10.080529, 7.336034, 9.246373]"
45,Ryo Nishikimi;Eita Nakamura;Masataka Goto;Katsutoshi Itoyama;Kazuyoshi Yoshii,Scale- and Rhythm-Aware Musical Note Estimation for Vocal F0 Trajectories Based on a Semi-Tatum-Synchronous Hierarchical Hidden Semi-Markov Model.,2017,https://doi.org/10.5281/zenodo.1416330,Ryo Nishikimi+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education|RIKEN AIP>JPN>facility,"This paper presents a statistical method that estimates a sequence of musical notes from a vocal F0 trajectory. Since the onset times and F0s of sung notes are considerably deviated from the discrete tatums and pitches indicated in a musical score, a score model is crucial for improving time-frequency quantization of the F0s. We thus propose a hierarchical hidden semi-Markov model (HHSMM) that combines a score model representing the rhythms and pitches of musical notes with musical scales with an F0 model representing time-frequency deviations from a note sequence specified by a score. In the score model, musical scales are generated stochastically. Note pitches are then generated according to the scales and note onsets are generated according to a Markov process defined on the tatum grid. In the F0 model, onset deviations, smooth note-to-note F0 transitions, and F0 deviations are generated stochastically and added to the note sequence. Given an F0 trajectory, our method estimates the most likely sequence of musical notes while giving more importance on the score model than the F0 model. Experimental results showed that the proposed method outperformed an HMM-based method having no models of scales and rhythms.",JPN,education,Developed economies,"[-5.0305595, -30.834036]","[-16.104202, -6.403845]","[15.4938, 6.3035407, -11.060078]","[1.1031189, -4.1814246, -12.377514]","[10.124522, 10.575055]","[6.3782635, 2.8762558]","[11.375414, 14.624478, 0.30249873]","[8.644009, 7.4293375, 10.864102]"
44,Jianyu Fan;Kivanç Tatar;Miles Thorogood;Philippe Pasquier,Ranking-Based Emotion Recognition for Experimental Music.,2017,https://doi.org/10.5281/zenodo.1417475,Jianyu Fan+Simon Fraser University>CAN>education;Kıvanç Tatar+Simon Fraser University>CAN>education;Miles Thorogood+Simon Fraser University>CAN>education;Philippe Pasquier+Simon Fraser University>CAN>education,"Emotion recognition is an open problem in Affective Computing the field. Music emotion recognition (MER) has challenges including variability of musical content across genres, the cultural background of listeners, reliability of ground truth data, and the modeling human hearing in computational domains. In this study, we focus on experimental music emotion recognition. First, we present a music corpus that contains 100 experimental music clips and 40 music clips from 8 musical genres. The dataset (the music clips and annotations) is publicly available at: http://metacreation.net/project/emusic/. Then, we present a crowdsourcing method that we use to collect ground truth via ranking the valence and arousal of music clips. Next, we propose a smoothed RankSVM (SRSVM) method. The evaluation has shown that the SRSVM outperforms four other ranking algorithms. Finally, we analyze the distribution of perceived emotion of experimental music against other genres to demonstrate the difference between genres.",CAN,education,Developed economies,"[-59.18812, -0.8482722]","[48.267998, -6.619079]","[-27.539423, 21.506897, 5.6689453]","[12.505444, 22.215294, 3.5074573]","[14.044439, 12.965479]","[12.957856, 4.060284]","[16.03358, 14.38648, 1.7974306]","[14.045223, 5.227234, 10.417749]"
43,Norbert Schnell;Diemo Schwarz;Joseph Larralde;Riccardo Borghesi,"PiPo, a Plugin Interface for Afferent Data Stream Processing Operators.",2017,https://doi.org/10.5281/zenodo.1416912,Norbert Schnell+IRCAM-CNRS-UPMC>FRA>facility;Diemo Schwarz+IRCAM-CNRS-UPMC>FRA>facility;Joseph Larralde+IRCAM-CNRS-UPMC>FRA>facility;Riccardo Borghesi+IRCAM-CNRS-UPMC>FRA>facility,"We present PiPo, a plugin API for data stream processing with applications in interactive audio processing and music information retrieval as well as potentially other domains of signal processing. The development of the API has been motivated by our recurrent need to use a set of signal processing modules that extract low-level descriptors from audio and motion data streams in the context of different authoring environments and end-user applications. The API is designed to facilitate both, the development of modules and the integration of modules or module graphs into applications. It formalizes the processing of streams of multidimensional data frames which may represent regularly sampled signals as well as time-tagged events or numeric annotations. As we found it sufficient for the processing of incoming (i.e. afferent) data streams, PiPo modules have a single input and output and can be connected to sequential and parallel processing paths. After laying out the context and motivations, we present the concept and implementation of the PiPo API with a set of modules that allow for extracting low-level descriptors from audio streams. In addition, we describe the integration of the API into host environments such as Max, Juce, and OpenFrameworks.",FRA,facility,Developed economies,"[10.681212, 34.106724]","[7.4928913, 29.360067]","[-13.28407, -13.525415, -19.11019]","[-4.2919693, -6.07201, 13.583164]","[11.878213, 7.100505]","[10.364663, 1.3816144]","[12.953286, 12.956297, -0.947605]","[11.227594, 5.5937905, 11.5354395]"
42,Andreas Arzt;Gerhard Widmer,Piece Identification in Classical Piano Music Without Reference Scores.,2017,https://doi.org/10.5281/zenodo.1417673,Andreas Arzt+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"In this paper we describe an approach to identify the name of a piece of piano music, based on a short audio excerpt of a performance. Given only a description of the pieces in text format (i.e. no score information is provided), a reference database is automatically compiled by acquiring a number of audio representations (performances of the pieces) from internet sources. These are transcribed, preprocessed, and used to build a reference database via a robust symbolic fingerprinting algorithm, which in turn is used to identify new, incoming queries. The main challenge is the amount of noise that is introduced into the identification process by the music transcription algorithm and the automatic (but possibly suboptimal) choice of performances to represent a piece in the reference database. In a number of experiments we show how to improve the identification performance by increasing redundancy in the reference database and by using a preprocessing step to rate the reference performances regarding their suitability as a representation of the pieces in question. As the results show this approach leads to a robust system that is able to identify piano music with high accuracy – without any need for data annotation or manual data preparation.",AUT,education,Developed economies,"[34.43997, 8.55938]","[2.0428305, 10.115944]","[22.393871, 7.5691643, 20.71012]","[1.93487, -16.164644, 1.8985022]","[10.352988, 6.859734]","[8.3781185, 0.5542647]","[12.153974, 11.966214, -1.0760412]","[10.078252, 5.7716665, 12.490835]"
41,Eita Nakamura;Kazuyoshi Yoshii;Haruhiro Katayose,Performance Error Detection and Post-Processing for Fast and Accurate Symbolic Music Alignment.,2017,https://doi.org/10.5281/zenodo.1414940,Eita Nakamura+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University|RIKEN AIP>JPN>education|Unknown>Unknown>Unknown;Haruhiro Katayose+Kwansei Gakuin University>JPN>education,"This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typically have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by performance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned signals, to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in local regions around performance errors. To remove the dependence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously proposed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results.",JPN,education,Developed economies,"[17.573452, -10.568922]","[-14.590025, -12.096857]","[-2.4365485, -14.577381, -7.1301003]","[-0.54776734, -17.549938, -5.328703]","[11.027944, 6.502838]","[6.452703, 1.0263182]","[12.052106, 12.640171, -1.528421]","[8.397782, 6.1938267, 10.812402]"
39,David R. W. Sears;Andreas Arzt;Harald Frostel;Reinhard Sonnleitner;Gerhard Widmer,Modeling Harmony with Skip-Grams.,2017,https://doi.org/10.5281/zenodo.1416196,David R. W. Sears+Johannes Kepler University>AUT>education;Andreas Arzt+Johannes Kepler University>AUT>education;Harald Frostel+Johannes Kepler University>AUT>education;Reinhard Sonnleitner+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"String-based (or viewpoint) models of tonal harmony often struggle with data sparsity in pattern discovery and prediction tasks, particularly when modeling composite events like triads and seventh chords, since the number of distinct n-note combinations in polyphonic textures is potentially enormous. To address this problem, this study examines the efficacy of skip-grams in music research, an alternative viewpoint method developed in corpus linguistics and natural language processing that includes sub-sequences of n events (or n-grams) in a frequency distribution if their constituent members occur within a certain number of skips. Using a corpus consisting of four datasets of Western classical music in symbolic form, we found that including skip-grams reduces data sparsity in n-gram distributions by (1) minimizing the proportion of n-grams with negligible counts, and (2) increasing the coverage of contiguous n-grams in a test corpus. What is more, skip-grams significantly outperformed contiguous n-grams in discovering conventional closing progressions (called cadences).",AUT,education,Developed economies,"[26.891064, 23.37737]","[1.7425221, 13.257241]","[-4.082222, -11.585125, 33.47504]","[8.317914, -13.890074, 3.1986516]","[9.788884, 9.25423]","[8.48638, 1.5594748]","[11.688131, 14.048267, -0.9313532]","[9.93827, 6.6450562, 12.303679]"
38,Li-Chia Yang;Szu-Yu Chou;Yi-Hsuan Yang,MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation.,2017,https://doi.org/10.5281/zenodo.1415990,Li-Chia Yang+Academia Sinica>TWN>education|Research Center for IT Innovation>Unknown>Unknown;Szu-Yu Chou+Academia Sinica>TWN>education|Research Center for IT Innovation>Unknown>Unknown;Yi-Hsuan Yang+Academia Sinica>TWN>education|Research Center for IT Innovation>Unknown>Unknown,"Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google’s MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet’s melodies are reported to be much more interesting.",TWN,education,Developing economies,"[24.874498, 4.150777]","[-9.361296, -39.740044]","[5.562723, 3.6643145, 24.791994]","[-23.744686, 1.7983254, -13.686471]","[10.105975, 8.313653]","[8.611492, 6.566747]","[13.199934, 11.716536, 0.29076892]","[9.587754, 5.658213, 8.370295]"
37,Michaël Defferrard;Kirell Benzi;Pierre Vandergheynst;Xavier Bresson,FMA: A Dataset for Music Analysis.,2017,https://doi.org/10.5281/zenodo.1414728,"Michaël Defferrard+LTS2, EPFL>CHE>education|SCSE, NTU>SGP>education;Kirell Benzi+LTS2, EPFL>CHE>education;Pierre Vandergheynst+LTS2, EPFL>CHE>education;Xavier Bresson+SCSE, NTU>SGP>education","We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community’s growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma.",CHE,education,Developed economies,"[-18.623201, 14.518818]","[16.559368, 31.524803]","[-12.956388, -1.8545325, -12.494359]","[-2.6346633, 3.2282646, 14.3889475]","[13.465232, 7.708697]","[10.341577, 3.7148035]","[14.077703, 13.878193, -1.4949573]","[11.669936, 5.6941586, 11.046471]"
36,Akira Maezawa,Fast and Accurate: Improving a Simple Beat Tracker with a Selectively-Applied Deep Beat Identification.,2017,https://doi.org/10.5281/zenodo.1415520,Akira Maezawa+Yamaha Corporation>JPN>company,"In music applications, audio beat tracking is a central component that requires both speed and accuracy, but a fast beat tracker typically has many beat phase errors, while an accurate one typically requires more computation. This paper achieves a fast tracking speed and a low beat phase error by applying a slow but accurate beat phase detector at only the most informative spots in a given song, and interpolating the rest by a fast tatum-level tracker. We present (1) a framework for selecting a small subset of the tatum indices that information-theoretically best describes the beat phases of the song, (2) a fast HMM-based beat tracker for tatum tracking, and (3) an accurate but slow beat detector using a deep neural network (DNN). The evaluations demonstrate that the proposed DNN beat phase detection halves the beat phase error of the HMM-based tracker and enables a 98% decrease in the required number of DNN invocations without dropping the accuracy.",JPN,company,Developed economies,"[34.526157, -33.592213]","[-30.262335, -13.39568]","[8.105713, -29.991434, -6.904103]","[-8.250084, 14.880814, -16.492107]","[10.490288, 4.2410655]","[4.9793987, 2.4075487]","[10.098767, 12.86923, -2.226337]","[7.604049, 6.9805455, 10.314437]"
40,Steven Losorelli;Duc T. Nguyen;Jacek P. Dmochowski;Blair Kaneshiro,NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music.,2017,https://doi.org/10.5281/zenodo.1417917,"Steven Losorelli+Center for the Study of Language and Information, Stanford University>USA>education|Center for Computer Research in Music and Acoustics, Stanford University>USA>education;Duc T. Nguyen+Center for the Study of Language and Information, Stanford University>USA>education|Center for Computer Research in Music and Acoustics, Stanford University>USA>education;Jacek P. Dmochowski+City College of New York>USA>education;Blair Kaneshiro+Center for the Study of Language and Information, Stanford University>USA>education|Center for Computer Research in Music and Acoustics, Stanford University>USA>education","Understanding human perception of music is foundational to many research topics in Music Information Retrieval (MIR). While the field of MIR has shown a rising interest in the study of brain responses, access to data remains an obstacle. Here we introduce the Naturalistic Music EEG Dataset—Tempo (NMED-T), an open dataset of electrophysiological and behavioral responses collected from 20 participants who heard a set of 10 commercially available musical works. Song stimuli span various genres and tempos, and all contain electronically produced beats in duple meter. Preprocessed and aggregated responses include dense-array EEG and sensorimotor synchronization (tapping) responses, behavioral ratings of the songs, and basic demographic information. These data, along with illustrative analysis code, are published in Matlab format. Raw EEG and tapping data are also made available. In this paper we describe the construction of the dataset, present results from illustrative analyses, and document the format and attributes of the published data. This dataset facilitates reproducible research in neuroscience and cognitive MIR, and points to several possible avenues for future studies on human processing of naturalistic music.",USA,education,Developed economies,"[-48.740948, -8.188932]","[-0.30817485, 24.1515]","[-12.308621, 21.274033, 19.848679]","[-0.6902462, 16.127567, 11.125028]","[11.648267, 4.6586967]","[10.926736, 4.3372107]","[10.960485, 13.575677, -2.7284064]","[11.763581, 4.7197566, 10.750844]"
35,Xiao Hu 0001;Kahyun Choi;Yun Hao;Sally Jo Cunningham;Jin Ha Lee;Audrey Laplante;David Bainbridge 0001;J. Stephen Downie,Exploring the Music Library Association Mailing List: A Text Mining Approach.,2017,https://doi.org/10.5281/zenodo.1415824,Xiao Hu+University of Hong Kong>HKG>education;Kahyun Choi+University of Illinois>USA>education;Yun Hao+University of Illinois>USA>education;Sally Jo Cunningham+University of Waikato>NZL>education;Jin Ha Lee+University of Washington>USA>education;Audrey Laplante+Université de Montréal>CAN>education;David Bainbridge+University of Waikato>NZL>education;J. Stephen Downie+University of Illinois>USA>education,"Music librarians and people pursuing music librarianship have exchanged emails via the Music Library Association Mailing List (MLA-L) for decades. The list archive is an invaluable resource to discover new insights on music information retrieval from the perspective of the music librarian community. This study analyzes a corpus of 53,648 emails posted on MLA-L from 2000 to 2016 by using text mining and quantitative analysis methods. In addition to descriptive analysis, main topics of discussions and their trends over the years are identified through topic modeling. We also compare messages that stimulated discussions to those that did not. Inspection of semantic topics reveals insights complementary to previous topic analyses of other Music Information Retrieval (MIR) related resources.",HKG,education,Developing economies,"[-19.594116, 33.913185]","[29.439127, 33.720947]","[-18.847076, 7.7195616, -15.5059595]","[5.7167864, 5.0369167, 19.931593]","[14.556761, 7.7471094]","[11.949615, 0.68880826]","[14.6147375, 14.424131, -2.235977]","[12.260403, 4.37668, 12.197299]"
34,Christopher J. Tralie,Early MFCC and HPCP Fusion for Robust Cover Song Identification.,2017,https://doi.org/10.5281/zenodo.1417331,Christopher J. Tralie+Duke University>USA>education,"While most schemes for automatic cover song identification have focused on note-based features such as HPCP and chord profiles, a few recent papers surprisingly showed that local self-similarities of MFCC-based features also have classification power for this task. Since MFCC and HPCP capture complementary information, we design an unsupervised algorithm that combines normalized, beat-synchronous blocks of these features using cross-similarity fusion before attempting to locally align a pair of songs. As an added bonus, our scheme naturally incorporates structural information in each song to fill in alignment gaps where both feature sets fail. We show a striking jump in performance over MFCC and HPCP alone, achieving a state of the art mean reciprocal rank of 0.87 on the Covers80 dataset. We also introduce a new medium-sized hand designed benchmark dataset called “Covers 1000,” which consists of 395 cliques of cover songs for a total of 1000 songs, and we show that our algorithm achieves an MRR of 0.9 on this dataset for the first correctly identified song in a clique. We provide the precomputed HPCP and MFCC features, as well as beat intervals, for all songs in the Covers 1000 dataset for use in further research.",USA,education,Developed economies,"[7.757064, 45.308193]","[26.728273, -11.293473]","[3.2559488, 14.820083, -25.295246]","[16.616835, -2.721674, 1.3145448]","[16.078709, 11.0994835]","[10.203706, 2.7221663]","[12.848174, 17.395144, -0.29892704]","[11.735473, 6.7228093, 11.765078]"
33,Aggelos Gkiokas;Vassilios Katsouros,Convolutional Neural Networks for Real-Time Beat Tracking: A Dancing Robot Application.,2017,https://doi.org/10.5281/zenodo.1417737,"Aggelos Gkiokas+Institute for Language and Speech Processing, Athena Research and Innovation Center>GRC>education;Vassilis Katsouros+Institute for Language and Speech Processing, Athena Research and Innovation Center>GRC>education","In this paper a novel approach that adopts Convolutional Neural Networks (CNN) for the Beat Tracking task is proposed. The proposed architecture involves 2 convolutional layers with the CNN filter dimensions corresponding to time and band frequencies, in order to learn a Beat Activation Function (BAF) from a time-frequency representation. The output of each convolutional layer is computed only over the past values of the previous layer, to enable the computation of the BAF in an online fashion. The output of the CNN is post-processed by a dynamic programming algorithm in combination with a bank of resonators for calculating the salient rhythmic periodicities. The proposed method has been designed to be computationally efficient in order to be embedded on a dancing NAO robot application, where the dance moves of the choreography are synchronized with the beat tracking output. The proposed system was submitted to the Signal Processing Cup Challenge 2017 and ranked among the top third algorithms.",GRC,education,Developed economies,"[34.899685, -34.991127]","[-30.2321, -11.913336]","[10.6860895, -31.731997, -6.404809]","[-10.244696, 14.320891, -15.485632]","[10.448475, 4.2110915]","[5.0148387, 2.3046393]","[10.062069, 12.854184, -2.2853978]","[7.5790014, 6.9160223, 10.439216]"
32,Johan Pauwels;Ken O'Hanlon;György Fazekas;Mark B. Sandler,Confidence Measures and Their Applications in Music Labelling Systems Based on Hidden Markov Models.,2017,https://doi.org/10.5281/zenodo.1418155,"Johan Pauwels+Centre for Digital Music, Queen Mary University of London>GBR>education;Ken O’Hanlon+Centre for Digital Music, Queen Mary University of London>GBR>education;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark B. Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","Inspired by previous work on confidence measures for tempo estimation in loops, we explore ways to add confidence measures to other music labelling tasks. We start by reflecting on the reasons why the work on loops was successful and argue that it is an example of the ideal scenario in which it is possible to define a confidence measure independently of the estimation algorithm. This requires additional domain knowledge not used by the estimation algorithm, which is rarely available. Therefore we move our focus to defining confidence measures for hidden Markov models, a technique used in multiple music information retrieval systems and beyond. We propose two measures that are oblivious to the specific labelling task, trading off performance for computational requirements. They are experimentally validated by means of a chord estimation task. Finally, we have a look at alternative uses of confidence measures, besides those applications that require a high precision rather than a high recall, such as most query retrievals.",GBR,education,Developed economies,"[-0.3079925, -0.524101]","[-34.01043, -4.683479]","[-19.110722, -12.615452, 19.573444]","[-12.252734, 14.533927, -9.742904]","[11.478412, 7.920345]","[5.104342, 1.9186585]","[12.486944, 13.733061, -0.14565487]","[7.4397235, 6.9814844, 11.277878]"
31,Kristen Masada;Razvan C. Bunescu,Chord Recognition in Symbolic Music Using Semi-Markov Conditional Random Fields.,2017,https://doi.org/10.5281/zenodo.1418343,Kristen Masada+Ohio University>USA>education|Ohio University>USA>education;Razvan Bunescu+Ohio University>USA>education|Ohio University>USA>education,"Chord recognition is a fundamental task in the harmonic analysis of tonal music, in which music is processed into a sequence of segments such that the notes in each segment are consistent with a corresponding chord label. We propose a machine learning model for chord recognition that uses semi-Markov Conditional Random Fields (semi-CRFs) to perform a joint segmentation and labeling of symbolic music. One benefit of using a semi-Markov model is that it enables the utilization of segment-level features, such as segment purity and chord coverage, that capture the extent to which the events in an entire segment of music are compatible with a candidate chord label. Correspondingly, we develop a rich set of segment-level features for a semi-CRF model that also incorporates the likelihood of a large number of chord-to-chord transitions. Evaluations on a dataset of Bach chorales and a corpus of theme and variations for piano by Beethoven and Mozart show that the proposed semi-CRF model outperforms a discriminatively trained Hidden Markov Model (HMM) that does sequential labeling of sounding events, thus demonstrating the suitability of semi-Markov models for joint segmentation and labeling of music.",USA,education,Developed economies,"[56.133884, -4.6591988]","[-31.59105, 17.209826]","[29.135275, -13.39021, 20.12789]","[-21.324886, -5.5166183, 0.9351054]","[6.604822, 8.708027]","[6.5017724, 3.52485]","[11.88879, 10.394852, 2.0351295]","[9.749684, 8.61876, 12.011427]"
30,Siddharth Gururani;Alexander Lerch,Automatic Sample Detection in Polyphonic Music.,2017,https://doi.org/10.5281/zenodo.1418331,Siddharth Gururani+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"The term ‘sampling’ refers to the usage of snippets or loops from existing songs or sample libraries in new songs, mashups, or other music productions. The ability to automatically detect sampling in music is, for instance, beneficial for studies tracking artist influences geographically and temporally. We present a method based on Non-negative Matrix Factorization (NMF) and Dynamic Time Warping (DTW) for the automatic detection of a sample in a pool of songs. The method comprises of two processing steps: first, the DTW alignment path between NMF activations of a song and query sample is computed. Second, features are extracted from this path and used to train a Random Forest classifier to detect the presence of the sample. The method is able to identify samples that are pitch shifted and/or time stretched with approximately 63% F-measure. We evaluate this method against a new publicly available dataset of real-world sample and song pairs.",USA,education,Developed economies,"[6.7566543, -18.86576]","[24.00075, -19.01237]","[15.18674, -2.203105, -4.481433]","[18.932705, -10.101147, 0.92237884]","[9.339884, 7.3966684]","[7.1154327, 0.5250307]","[11.5577345, 12.812153, -0.104770094]","[8.688099, 5.726645, 11.235835]"
29,Krish Narang;Preeti Rao,Acoustic Features for Determining Goodness of Tabla Strokes.,2017,https://doi.org/10.5281/zenodo.1416274,Krish Narang+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"The tabla is an essential component of the Hindustani classical music ensemble and therefore a popular choice with musical instrument learners. Early lessons typically target the mastering of individual strokes from the inventory of bols (spoken syllables corresponding to the distinct strokes) via training in the required articulatory gestures on the right and left drums. Exploiting the close links between the articulation, acoustics and perception of tabla strokes, this paper presents a study of the different timbral qualities that correspond to the correct articulation and to identified common misarticulations of the different bols. We present a dataset created out of correctly articulated and distinct categories of misarticulated strokes, all perceptually verified by an expert. We obtain a system that automatically labels a recording as a good or bad sound, and additionally identifies the precise nature of the misarticulation with a view to providing corrective feedback to the player. We find that acoustic features that are sensitive to the relatively small deviations from the good sound due to poorly articulated strokes are not necessarily the features that have proved successful in the recognition of strokes corresponding to distinct tabla bols as required for music transcription.",IND,education,Developing economies,"[33.72885, -47.973297]","[-24.965609, 8.886388]","[26.753235, -19.09742, 4.0376496]","[2.0213141, 7.823477, -17.688347]","[7.760067, 7.847113]","[8.017868, 4.306911]","[11.059002, 11.485444, 1.1789953]","[9.070604, 7.0195546, 10.033534]"
28,Jia-Ling Syue;Li Su;Yi-Ju Lin;Pei-Ching Li;Yen-Kuang Lu;Yu-Lin Wang;Alvin W. Y. Su,Accurate Audio-to-Score Alignment for Expressive Violin Recordings.,2017,https://doi.org/10.5281/zenodo.1416188,Jia-Ling Syue+National Cheng-Kung University>TWN>education;Li Su+Academia Sinica>TWN>education;Yi-Ju Lin+National Cheng-Kung University>TWN>education;Pei-Ching Li+National Cheng-Kung University>TWN>education;Yen-Kuang Lu+National Cheng-Kung University>TWN>education;Yu-Lin Wang+National Cheng-Kung University>TWN>education;Alvin W. Y. Su+National Cheng-Kung University>TWN>education,"An audio-to-score alignment system adaptive to various playing styles and techniques, and also with high accuracy for onset/offset annotation is the key step toward advanced research on automatic music expression analysis. Technical barriers include the processing of overlapped notes, repeated note sequences, and silence. Most of these characteristics vary with expressions. In this paper, the audio-to-score alignment problem of expressive violin performance is addressed. We propose a two-stage alignment system composed of the dynamic time warping (DTW) algorithm, simulation of overlapped sustain notes, background noise model, silence detection, and refinement process, to better capture the onset. More importantly, we utilize the non-negative matrix factorization (NMF) method for synthesis of the reference signal in order to deal with highly diverse timbre in real-world performance. A dataset of annotated expressive violin recordings in which each piece is played with various expressive musical terms is used. The optimal choice of basic parameters considered in conventional alignment systems, such as features, distance functions in DTW, synthesis methods for the reference signal, and energy ratios, is analyzed. Different settings on different expressions are compared and discussed. Results show that the proposed methods notably improve the conventional DTW-based alignment method.",TWN,education,Developing economies,"[17.495998, -15.387961]","[-18.799505, -14.870104]","[3.347747, -15.113297, -6.9466243]","[0.17037332, -20.845394, -6.4499593]","[9.975469, 6.527892]","[6.102408, 0.7477106]","[11.508368, 12.364727, -0.8432882]","[8.118118, 5.857568, 10.84443]"
27,Venkata Subramanian Viraraghavan;Rangarajan Aravind;Hema A. Murthy,A Statistical Analysis of Gamakas in Carnatic Music.,2017,https://doi.org/10.5281/zenodo.1417837,"Venkata Subramanian Viraraghavan+TCS Research and Innovation>IND>company|Indian Institute of Technology, Madras>IND>education;R Aravind+Indian Institute of Technology, Madras>IND>education;Hema A Murthy+Indian Institute of Technology, Madras>IND>education","Carnatic Music, a form of classical music prevalent in South India, has a central concept called r¯agas, deﬁned as melodic scales and/or a set of characteristic melodic phrases. These deﬁnitions also account for the continuous pitch movement in gamakas and micro-tonal adjustments to pitch values. In this paper, we present several statistics of gamakas to arrive at a model of Carnatic music. We draw upon the two-component model of Carnatic Music, which splits it into a slowly varying ‘stage’ and a detail, called ‘dance’. Based on the statistics, we propose slightly altered def- initions of two similar components called constant-pitch notes and transients. An automated implementation of these deﬁnitions is used in collecting statistics from 84 concert renditions. We then suggest that the constant-pitch notes and tran- sients can be considered as context and detail respectively of the r¯aga, but add that both are necessary for deﬁning the r¯aga. This is veriﬁed by performing listening tests on only the constant-pitch notes and transients independently.",IND,company,Developing economies,"[5.9166117, -0.76470464]","[1.5247207, -17.941788]","[9.781497, 0.5968562, -16.234684]","[12.598065, -14.5973, -8.117558]","[11.318824, 10.171131]","[7.371239, 1.2937226]","[11.72674, 15.130298, -1.364698]","[8.904816, 7.1660705, 12.473133]"
43,Daniel Stoller;Sebastian Ewert;Simon Dixon,Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation,2018,https://doi.org/10.5281/zenodo.1492417,Daniel Stoller+Queen Mary University of London>GBR>education;Sebastian Ewert+Spotify>USA>company;Simon Dixon+Queen Mary University of London>GBR>education,"Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyperparameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a stateof-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.",GBR,education,Developed economies,"[10.410368, -47.075344]","[-40.953346, -34.707726]","[31.909075, -0.3483685, -8.400164]","[-16.28278, -7.856436, -25.710073]","[8.555612, 10.109816]","[6.8011336, 5.9206657]","[11.021116, 13.858132, 1.5067171]","[9.657817, 8.271552, 8.809663]"
42,Mathieu Andreux;Stéphane Mallat,Music Generation and Transformation with Moment Matching-Scattering Inverse Networks,2018,https://doi.org/10.5281/zenodo.1492415,Mathieu Andreux+Ecole normale supérieure>FRA>education|CNRS>FRA>facility|PSL Research University>FRA>education;Stéphane Mallat+Ecole normale supérieure>FRA>education|CNRS>FRA>facility|PSL Research University>FRA>education,"We introduce a Moment Matching-Scattering Inverse Network (MM-SIN) to generate and transform musical sounds. The MM-SIN generator is similar to a variational autoencoder or an adversarial network. However, the encoder or the discriminator are not learned, but computed with a scattering transform defined from prior information on sparse time-frequency audio properties. The generator is trained by jointly minimizing the reconstruction loss of an inverse problem, and a generation loss which computes a distance over scattering moments. It has a similar causal architecture as a WaveNet and provides a simpler mathematical model related to time-frequency decompositions. Numerical experiments demonstrate that this MMSIN generates new realistic musical signals. It can transform low-level musical attributes such as pitch with a linear transformation in the embedding space of scattering coefficients.",FRA,education,Developed economies,"[22.303022, 1.8652492]","[-22.480637, -48.9471]","[0.7944299, -0.14691569, 16.797462]","[-22.503113, -5.401156, -22.564034]","[10.195357, 8.518631]","[8.057812, 6.531377]","[13.22461, 12.023655, 0.22263592]","[9.536794, 6.1980715, 8.152301]"
41,Jan Schlüter;Bernhard Lehner,Zero-Mean Convolutions for Level-Invariant Singing Voice Detection,2018,https://doi.org/10.5281/zenodo.1492413,Jan Schlüter+Austrian Research Institute for Artificial Intelligence>AUT>facility;Bernhard Lehner+Johannes Kepler University Linz>AUT>education,"State-of-the-art singing voice detectors are based on classifiers trained on annotated examples. As recently shown, such detectors have an important weakness: Since singing voice is correlated with sound level in training data, classifiers learn to become sensitive to input magnitude, and give different predictions for the same signal at different sound levels. Starting from a Convolutional Neural Network (CNN) trained on logarithmic-magnitude mel spectrogram excerpts, we eliminate this dependency by forcing each first-layer convolutional filter to be zero-mean – that is, to have its coefficients sum to zero. In contrast to four other methods – data augmentation, instance normalization, spectral delta features, and per-channel energy normalization (PCEN) – that we evaluated on a largescale public dataset, zero-mean convolutions achieve perfect sound level invariance without any impact on prediction accuracy or computational requirements. We assume that zero-mean convolutions would be useful for other machine listening tasks requiring robustness to level changes.",AUT,facility,Developed economies,"[-7.114878, -36.502094]","[-33.98052, -40.343132]","[22.77457, 9.864232, -14.388664]","[-0.5845826, -10.883961, -20.599154]","[9.764178, 11.072362]","[7.6455927, 4.961707]","[11.07768, 15.0465355, 0.8228362]","[10.04002, 7.3875957, 8.924483]"
40,Carl Southall;Ryan Stables;Jason Hockman,Improving Peak-picking Using Multiple Time-step Loss Functions,2018,https://doi.org/10.5281/zenodo.1492409,Carl Southall+Birmingham City University>GBR>education;Ryan Stables+Birmingham City University>GBR>education;Jason Hockman+Birmingham City University>GBR>education,"The majority of state-of-the-art methods for music information retrieval (MIR) tasks now utilise deep learning methods reliant on minimisation of loss functions such as cross entropy. For tasks that include framewise binary classification (e.g., onset detection, music transcription) classes are derived from output activation functions by identifying points of local maxima, or peaks. However, the operating principles behind peak picking are different to that of the cross entropy loss function, which minimises the absolute difference between the output and target values for a single frame. To generate activation functions more suited to peak-picking, we propose two versions of a new loss function that incorporates information from multiple time-steps: 1) multi-individual, which uses multiple individual time-step cross entropies; and 2) multi-difference, which directly compares the difference between sequential time-step outputs. We evaluate the newly proposed loss functions alongside standard cross entropy in the popular MIR tasks of onset detection and automatic drum transcription. The results highlight the effectiveness of these loss functions in the improvement of overall system accuracies for both MIR tasks. Additionally, directly comparing the output from sequential time-steps in the multidifference approach achieves the highest performance.",GBR,education,Developed economies,"[-20.736012, -23.15201]","[-24.939886, -29.1833]","[-4.577417, -23.640112, -9.367711]","[-8.509561, -3.614134, -13.795]","[10.628229, 5.0711813]","[8.699818, 5.0683084]","[10.661893, 13.264408, -1.7744292]","[9.810035, 6.50943, 8.893786]"
39,Jakob Abeßer;Stefan Balke;Meinard Müller,Improving Bass Saliency Estimation using Transfer Learning and Label Propagation,2018,https://doi.org/10.5281/zenodo.1492411,Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Stefan Balke+International Audio Laboratories Erlangen>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>education,"In this paper, we consider two methods to improve an algorithm for bass saliency estimation in jazz ensemble recordings which are based on deep neural networks. First, we apply label propagation to increase the amount of training data by transferring pitch labels from our labeled dataset to unlabeled audio recordings using a spectral similarity measure. Second, we study in several transfer learning experiments, whether isolated note recordings can be beneficial for pre-training a model which is later fine-tuned on ensemble recordings. Our results indicate that both strategies can improve the performance on bass saliency estimation by up to five percent in accuracy.",DEU,facility,Developed economies,"[-0.11256798, -21.835709]","[-32.791126, -29.689507]","[5.2872972, 1.2893071, 8.64321]","[-12.804929, -7.607422, -19.160973]","[11.0645, 9.279553]","[7.6391745, 5.239881]","[12.9229555, 13.073101, 0.82701486]","[9.507792, 7.2235255, 8.995402]"
34,Filip Korzeniowski;Gerhard Widmer,Genre-Agnostic Key Classification With Convolutional Neural Networks,2018,https://doi.org/10.5281/zenodo.1492399,Filip Korzeniowski+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility,"We propose modifications to the model structure and training procedure to a recently introduced Convolutional Neural Network for musical key classification. These modifications enable the network to learn a genre-independent model that performs better than models trained for specific music styles, which has not been the case in existing work. We analyse this generalisation capability on three datasets comprising distinct genres. We then evaluate the model on a number of unseen data sets, and show its superior performance compared to the state of the art. Finally, we investigate the model's performance on short excerpts of audio. From these experiments, we conclude that models need to consider the harmonic coherence of the whole piece when classifying the local key of short segments of audio.",AUT,education,Developed economies,"[-32.233997, -10.201611]","[-26.560806, -33.540222]","[-21.829906, 5.102594, 18.776012]","[-4.4823775, -6.9298115, -17.45596]","[13.116447, 10.887433]","[8.123882, 5.1132083]","[14.028627, 14.338839, 1.3822676]","[9.811404, 6.913578, 9.19545]"
37,Sungheon Park;Taehoon Kim;Kyogu Lee;Nojun Kwak,Music Source Separation Using Stacked Hourglass Networks,2018,https://doi.org/10.5281/zenodo.1492405,Sungheon Park+Seoul National University>KOR>education;Taehoon Kim+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education;Nojun Kwak+Seoul National University>KOR>education,"In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks.",KOR,education,Developing economies,"[8.386796, -45.708687]","[-38.362064, -33.739864]","[31.3434, -1.9624274, -5.3949337]","[-13.464288, -9.041605, -26.733372]","[8.323267, 9.995726]","[6.863905, 5.813736]","[11.021386, 13.616805, 1.6778406]","[9.659588, 8.277611, 8.850042]"
36,Reinier de Valk;Tillman Weyde,Deep Neural Networks with Voice Entry Estimation Heuristics for Voice Separation in Symbolic Music Representations,2018,https://doi.org/10.5281/zenodo.1492403,"Reinier de Valk+Jukedeck Ltd.>GBR>company;Tillman Weyde+City, University of London>GBR>education","In this study we explore the use of deep feedforward neural networks for voice separation in symbolic music representations. We experiment with different network architectures, varying the number and size of the hidden layers, and with dropout. We integrate two voice entry estimation heuristics that estimate the entry points of the individual voices in the polyphonic fabric into the models. These heuristics serve to reduce error propagation at the beginning of a piece, which, as we have shown in previous work, can seriously hamper model performance. The models are evaluated on the 48 fugues from Johann Sebastian Bach's The Well-Tempered Clavier and his 30 inventions—a dataset that we curated and make publicly available. We find that a model with two hidden layers yields the best results. Using more layers does not lead to a significant performance improvement. Furthermore, we find that our voice entry estimation heuristics are highly effective in the reduction of error propagation, improving performance significantly. Our best-performing model outperforms our previous models, where the difference is significant, and, depending on the evaluation metric, performs close to or better than the reported state of the art.",GBR,company,Developed economies,"[1.8303471, -47.394886]","[-35.476917, -33.668793]","[25.643072, 5.0037103, -0.029217737]","[-13.005962, -11.653045, -22.509726]","[8.643388, 10.33604]","[7.130392, 5.5779786]","[10.923755, 14.014464, 1.5478886]","[9.607963, 7.933411, 9.034223]"
35,Lukas Tuggener;Ismail Elezi;Jürgen Schmidhuber;Thilo Stadelmann,Deep Watershed Detector for Music Object Recognition,2018,https://doi.org/10.5281/zenodo.1492401,"Lukas Tuggener+ZHAW Datalab, Zurich University of Applied Sciences>CHE>education|Faculty of Informatics, Università della Svizzera italiana>CHE>education;Ismail Elezi+ZHAW Datalab, Zurich University of Applied Sciences>CHE>education|Ca’ Foscari University of Venice>ITA>education;Jürgen Schmidhuber+Faculty of Informatics, Università della Svizzera italiana>CHE>education;Thilo Stadelmann+ZHAW Datalab, Zurich University of Applied Sciences>CHE>education","Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD's ability to work with synthetic scores equally well as with handwritten music.",CHE,education,Developed economies,"[-8.425278, -11.230123]","[-18.329348, -24.359526]","[8.05962, -2.4865713, 2.5742135]","[-16.076818, -22.993895, -4.74001]","[11.665956, 8.919232]","[6.5250816, -1.0513948]","[12.749762, 13.953029, 0.4594004]","[8.040129, 4.150686, 10.018889]"
44,Melissa R. McGuirl;Katherine M. Kinnaird;Claire Savard;Erin H. Bugbee,SE and SNL diagrams: Flexible data structures for MIR,2018,https://doi.org/10.5281/zenodo.1492419,Melissa R. McGuirl+Brown University>USA>education;Katherine M. Kinnaird+Brown University>USA>education;Claire Savard+University of Michigan>USA>education;Erin H. Bugbee+Brown University>USA>education,"to interpret. The matrix-based representations commonly used in MIR tasks are often difficult This work introduces start-end (SE) diagrams and start(normalized)length (SNL) diagrams, two novel structure-based representations for sequential music data. Inspired by methods from topological data analysis, both SE and SNL diagrams come equipped with efficiently computable and stable metrics. Utilizing SE or SNL diagrams as input, we address the cover song task for score-based data with high accuracy. While both representations are concisely defined and flexible, SNL diagrams in particular address issues introduced by commonly used resampling methods.",USA,education,Developed economies,"[-9.345655, 56.666412]","[3.7272105, 21.566559]","[-37.661278, -3.255005, -5.8200383]","[-11.702917, 13.000324, 16.570282]","[13.603105, 4.7234535]","[9.252892, 1.32532]","[14.987511, 11.151237, -1.4397367]","[10.541499, 5.744547, 10.991346]"
38,Ethan Manilow;Prem Seetharaman;Bryan Pardo,The Northwestern University Source Separation Library,2018,https://doi.org/10.5281/zenodo.1492407,Ethan Manilow+Northwestern University>USA>education;Prem Seetharaman+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"Audio source separation is the process of isolating individual sonic elements from a mixture or auditory scene. We present the Northwestern University Source Separation Library, or nussl for short. nussl (pronounced 'nuzzle') is an open-source, object-oriented audio source separation library implemented in Python. nussl provides implementations for many existing source separation algorithms and a platform for creating the next generation of source separation algorithms. By nature of its design, nussl easily allows new algorithms to be benchmarked against existing algorithms on established data sets and facilitates development of new variations on algorithms. Here, we present the design methodologies in nussl, two experiments using it, and use nussl to showcase benchmarks for some algorithms contained within.",USA,education,Developed economies,"[6.2747655, -49.532818]","[-41.13281, -29.552319]","[35.283863, -0.29732132, -3.1030667]","[-9.125383, -4.929906, -26.822723]","[8.406111, 10.161851]","[6.577851, 5.592242]","[11.007885, 13.70741, 1.74262]","[9.755086, 8.513234, 9.248177]"
45,Cory McKay;Julie Cumming;Ichiro Fujinaga,JSYMBOLIC 2.2: Extracting Features from Symbolic Music for use in Musicological and MIR Research,2018,https://doi.org/10.5281/zenodo.1492421,Cory McKay+McGill University>CAN>education;Julie E. Cumming+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"jSymbolic is an open-source platform for extracting features from symbolic music. These features can serve as inputs to machine learning algorithms, or they can be analyzed statistically to derive musicological insights.  jSymbolic implements 246 unique features, comprising 1497 different values, making it by far the most extensive symbolic feature extractor to date. These features are designed to be applicable to a diverse range of musics, and may be extracted from both symbolic music files as a whole and from windowed subsets of them. Researchers can also use jSymbolic as a platform for developing and distributing their own bespoke features, as it has an easily extensible plug-in architecture.  In addition to implementing 135 new unique features, version 2.2 of jSymbolic places a special focus on functionality for avoiding biases associated with how symbolic music is encoded. In addition, new interface elements and documentation improve convenience, ease-of-use and accessibility to researchers with diverse ranges of technical expertise. jSymbolic now includes a GUI, command-line interface, API , flexible configuration file format, extensive manual and detailed tutorial.  The enhanced effectiveness of jSymbolic 2.2's features is demonstrated in two sets of experiments: 1) genre classification and 2) Renaissance composer attribution.",CAN,education,Developed economies,"[14.753461, 17.54892]","[-0.3482746, 28.60059]","[-6.6620464, -7.6598325, 15.269595]","[-7.264254, -1.2548424, 12.888677]","[13.037251, 5.7343006]","[9.818523, 1.807063]","[14.224138, 11.952561, -1.2445625]","[10.855775, 5.9275002, 11.3873205]"
56,Gabriel Meseguer-Brocal;Alice Cohen-Hadria;Geoffroy Peeters,"DALI: A Large Dataset of Synchronized Audio, Lyrics and notes, Automatically Created using Teacher-student Machine Learning Paradigm.",2018,https://doi.org/10.5281/zenodo.1492443,"Gabriel Meseguer-Brocal+Ircam Lab, CNRS, Sorbonne Université>FRA>facility;Alice Cohen-Hadria+Ircam Lab, CNRS, Sorbonne Université>FRA>facility;Geoffroy Peeters+Ircam Lab, CNRS, Sorbonne Université>FRA>facility","The goal of this paper is twofold. First, we introduce DALI, a large and rich multimodal dataset containing 5358 audio tracks with their time-aligned vocal melody notes and lyrics at four levels of granularity. The second goal is to explain our methodology where dataset creation and learning models interact using a teacher-student machine learning paradigm that benefits each other. We start with a set of manual annotations of draft time-aligned lyrics and notes made by non-expert users of Karaoke games. This set comes without audio. Therefore, we need to find the corresponding audio and adapt the annotations to it. To that end, we retrieve audio candidates from the Web. Each candidate is then turned into a singing-voice probability over time using a teacher, a deep convolutional neural network singing-voice detection system (SVD), trained on cleaned data. Comparing the time-aligned lyrics and the singing-voice probability, we detect matches and update the time-alignment lyrics accordingly. From this, we obtain new audio sets. They are then used to train new SVD students used to perform again the above comparison. The process could be repeated iteratively. We show that this allows to progressively improve the performances of our SVD and get better audiomatching and alignment.",FRA,facility,Developed economies,"[-22.036667, 5.9712186]","[-28.177164, -41.819695]","[-24.340103, 1.2204266, 6.1172256]","[5.2644467, -6.2392054, -22.000906]","[12.752411, 7.7065973]","[8.029387, 4.7626543]","[14.22356, 13.4362545, -0.9572102]","[10.477592, 7.301443, 9.051111]"
47,Xiao Hu;Fanjie Li;Jeremy T. D. Ng,On the Relationships between Music-induced Emotion and Physiological Signals,2018,https://doi.org/10.5281/zenodo.1492425,Xiao Hu+The University of Hong Kong>HKG>education;Fanjie Li+Sichuan University>CHN>education;Jeremy T. D. Ng+The University of Hong Kong>HKG>education,"to optimize emotion-aware music retrieval.  Emotion-aware music information retrieval (MIR) has been difficult due to the subjectivity and temporality of emotion responses to music. Physiological signals are regarded as related to emotion and thus could potentially be exploited in emotion-aware music discovery. This study explored the possibility of using physiological signals to detect users' emotion responses to music, with consideration of individual characteristics (personality, music preferences, etc.). A user experiment was conducted with 23 participants who searched for music in a novel MIR system. Users' listening behaviors and self-reported emotion responses to a total of 628 music pieces were collected. During music listening, a series of peripheral physiological signals (e.g., heart rate, skin conductance) were recorded from participants unobtrusively using a researchgrade wearable wristband. A set of features in the time- and frequency- domains were extracted from the physiological signals and analyzed using statistical and machine learning methods. Results reveal 1) significant differences in some physiological features between positive and negative arousal and mood categories, and 2) effective classification of emotion responses based on physiological signals for some individuals. The findings can contribute to further improvement of emotion-aware intelligent MIR systems exploiting physiological signals as an objective and personalized input.",HKG,education,Developing economies,"[-60.91693, 3.7518234]","[54.430763, -7.391864]","[-25.42912, 25.529036, -0.41896424]","[9.4050865, 19.029898, 6.4438853]","[14.012139, 12.72742]","[13.096241, 3.9508011]","[16.193516, 14.461817, 1.8036319]","[14.09705, 4.9129243, 10.53316]"
48,Rémi Delbouys;Romain Hennequin;Francesco Piccoli;Jimena Royo-Letelier;Manuel Moussallam,Music Mood Detection Based on Audio and Lyrics with Deep Neural Net,2018,https://doi.org/10.5281/zenodo.1492427,R´emi Delbouys+Deezer>FRA>company;Romain Hennequin+Deezer>FRA>company;Francesco Piccoli+Deezer>FRA>company;Jimena Royo-Letelier+Deezer>FRA>company;Manuel Moussallam+Deezer>FRA>company,"1.1 Related work We consider the task of multimodal music mood prediction based on the audio signal and the lyrics of a track. We reproduce the implementation of traditional feature engineering based approaches and propose a new model based on deep learning. We compare the performance of both approaches on a database containing 18,000 tracks with associated valence and arousal values and show that our approach outperforms classical models on the arousal detection task, and that both approaches perform equally on the valence prediction task. We also compare the a posteriori fusion with fusion of modalities optimized simultaneously with each unimodal model, and observe a significant improvement of valence prediction. We release part of our database for comparison purposes.",FRA,company,Developed economies,"[-53.620613, 2.206604]","[49.878468, -10.159745]","[-18.078772, 22.475498, 5.825943]","[9.760809, 25.998474, 4.4127564]","[13.444092, 12.5934925]","[12.996428, 4.3077254]","[16.127823, 14.932384, 1.4763391]","[14.210248, 5.0321794, 10.146122]"
49,Emilia Parada-Cabaleiro;Maximilian Schmitt;Anton Batliner;Simone Hantke;Giovanni Costantini;Klaus Scherer;Bjoern Schuller,Identifying Emotions in Opera Singing: Implications of Adverse Acoustic Conditions,2018,https://doi.org/10.5281/zenodo.1492429,Emilia Parada-Cabaleiro+University of Augsburg>DEU>education;Maximilian Schmitt+University of Augsburg>DEU>education;Anton Batliner+University of Augsburg>DEU>education;Simone Hantke+Technische Universität München>DEU>education;Giovanni Costantini+University of Rome Tor Vergata>ITA>education;Klaus Scherer+University of Geneva>CHE>education;Björn W. Schuller+Imperial College London>GBR>education,"The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners' and machines' identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners' gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion.",DEU,education,Developed economies,"[-61.674816, 5.56207]","[50.87292, -17.143757]","[-24.289173, 26.974232, -3.0093622]","[5.8924346, 20.413372, 8.565844]","[14.095982, 12.80085]","[12.73505, 4.2772164]","[16.229137, 14.50615, 1.7911364]","[13.863235, 4.777862, 10.291502]"
50,Renato Panda;Ricardo Malheiro;Rui Pedro Paiva,Musical Texture and Expressivity Features for Music Emotion Recognition,2018,https://doi.org/10.5281/zenodo.1492431,"Renato Panda+Centre for Informatics and Systems, University of Coimbra>PRT>education;Ricardo Malheiro+Centre for Informatics and Systems, University of Coimbra>PRT>education;Rui Pedro Paiva+Centre for Informatics and Systems, University of Coimbra>PRT>education","We present a set of novel emotionally-relevant audio features to help improving the classification of emotions in audio music. First, a review of the state-of-the-art regarding emotion and music was conducted, to understand how the various music concepts may influence human emotions. Next, well known audio frameworks were analyzed, assessing how their extractors relate with the studied musical concepts. The intersection of this data showed an unbalanced representation of the eight musical concepts. Namely, most extractors are low-level and related with tone color, while musical form, musical texture and expressive techniques are lacking. Based on this, we developed a set of new algorithms to capture information related with musical texture and expressive techniques, the two most lacking concepts. To validate our work, a public dataset containing 900 30-second clips, annotated in terms of Russell's emotion quadrants was created. The inclusion of our features improved the F1-score obtained using the best 100 features by 8.6% (to 76.0%), using support vector machines and 20 repetitions of 10-fold cross-validation.",PRT,education,Developed economies,"[-59.365883, -0.5941149]","[50.84276, -8.97475]","[-27.075087, 23.21352, 6.3813396]","[9.21464, 23.166616, 5.0291004]","[14.039328, 12.920398]","[13.021287, 4.2242246]","[16.086855, 14.422888, 1.8241626]","[14.138028, 4.9891934, 10.295931]"
51,André Ofner;Sebastian Stober,Shared Generative Representation of Auditory Concepts and EEG to Reconstruct Perceived and Imagined Music,2018,https://doi.org/10.5281/zenodo.1492433,André Ofner+University of Potsdam>DEU>education;Sebastian Stober+University of Potsdam>DEU>education,"Retrieving music information from brain activity is a challenging and still largely unexplored research problem. In this paper we investigate the possibility to reconstruct perceived and imagined musical stimuli from electroencephalography (EEG) recordings based on two datasets. One dataset contains multichannel EEG of subjects listening to and imagining rhythmical patterns presented both as sine wave tones and short looped spoken utterances. These utterances leverage the well-known speech-to-song illusory transformation which results in very catchy and easy to reproduce motifs. A second dataset provides EEG recordings for the perception of 10 full length songs. Using a multi-view deep generative model we demonstrate the feasibility of learning a shared latent representation of brain activity and auditory concepts, such as rhythmical motifs appearing across different instrumentations. Introspection of the model trained on the rhythm dataset reveals disentangled rhythmical and timbral features within and across subjects. The model allows continuous interpolation between representations of different observed variants of the presented stimuli. By decoding the learned embeddings we were able to reconstruct both perceived and imagined music. Stimulus complexity and the choice of training data shows strong effect on the reconstruction quality.",DEU,education,Developed economies,"[-14.150191, -10.351762]","[-11.0539, -49.54641]","[-1.5054741, 2.5446904, 14.209417]","[-15.7688265, -0.7612932, -16.375284]","[12.0478115, 8.912757]","[8.645141, 6.3386874]","[13.531486, 13.334558, -0.25860888]","[9.794049, 5.7682414, 8.780333]"
52,Renan de Padua;Verônica Oliveira de Carvalho;Solange Rezende;Diego Furtado Silva,Exploring Musical Relations Using Association Rule Networks,2018,https://doi.org/10.5281/zenodo.1492435,"Renan de Padua+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education|Data Science Team, Itaú-Unibanco>BRA>company;Verônica Oliveira de Carvalho+Instituto de Geociências e Ciências Exatas – Universidade Estadual Paulista>BRA>education;Solange de Oliveira Rezende+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education;Diego Furtado Silva+Departamento de Computação – Universidade Federal de São Carlos>BRA>education","Music information retrieval (MIR) has been gaining increasing attention in both industry and academia. While many algorithms for MIR rely on assessing feature subsequences, the user normally has no resources to interpret the significance of these patterns. Interpreting the relations between these temporal patterns and some aspects of the assessed songs can help understanding not only some algorithms' outcomes but the kind of patterns which better defines a set of similarly labeled recordings. In this work, we present a novel method to assess these relations, constructing an association rule network from temporal patterns obtained by a simple quantization process. With an empirical evaluation, we illustrate how we can use our method to explore these relations in a varied set of data and labels.",BRA,education,Developing economies,"[-27.01654, 8.420891]","[30.38, 0.7890455]","[-8.153058, 5.3852487, 9.087761]","[15.473108, 6.828554, 1.7079705]","[13.348041, 9.518229]","[10.496714, 2.9867783]","[13.863984, 14.745384, -0.48196018]","[12.034979, 6.2614164, 11.275348]"
4,Miguel A. Román;Antonio Pertusa;Jorge Calvo-Zaragoza,An End-to-end Framework for Audio-to-Score Music Transcription on Monophonic Excerpts,2018,https://doi.org/10.5281/zenodo.1492337,Miguel A. Román+University of Alicante>ESP>education;Antonio Pertusa+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+Universitat Politècnica de València>ESP>education,"In this work, we present an end-to-end framework for audio-to-score transcription. To the best of our knowledge, this is the first automatic music transcription approach which obtains directly a symbolic score from audio, instead of performing separate stages for piano-roll estimation (pitch detection and note tracking), meter detection or key estimation. The proposed method is based on a Convolutional Recurrent Neural Network architecture directly trained with pairs of spectrograms and their corresponding symbolic scores in Western notation. Unlike standard pitch estimation methods, the proposed architecture does not need the music symbols to be aligned with their audio frames thanks to a Connectionist Temporal Classification loss function. Training and evaluation were performed using a large dataset of short monophonic scores (incipits) from the RISM collection, that were synthesized to get the ground-truth data. Although there is still room for improvement, most musical symbols were correctly detected and the evaluation results validate the proposed approach. We believe that this end-to-end framework opens new avenues for automatic music transcription.",ESP,education,Developed economies,"[24.808748, -9.459839]","[-25.712713, -26.257189]","[15.493789, -0.4705795, 10.92788]","[-10.735801, -7.379267, -12.230291]","[9.797326, 7.7251563]","[7.9726934, 5.3871617]","[12.000428, 12.493687, -0.10499723]","[9.114631, 6.558616, 9.065767]"
54,Christof Weiss;Stefan Balke;Jakob Abeßer;Meinard Müller,Computational Corpus Analysis: A Case Study on Jazz Solos,2018,https://doi.org/10.5281/zenodo.1492439,"Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Stefan Balke+International Audio Laboratories Erlangen>DEU>facility;Jakob Abeßer+Semantic Music Technologies Group, Fraunhofer IDMT>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility","For musicological studies on large corpora, the compilation of suitable data constitutes a time-consuming step. In particular, this is true for high-quality symbolic representations that are generated manually in a tedious process. A recent study on Western classical music has shown that musical phenomena such as the evolution of tonal complexity over history can also be analyzed on the basis of audio recordings. As our first contribution, we transfer this corpus analysis method to jazz music using the Weimar Jazz Database, which contains high-level symbolic transcriptions of jazz solos along with the audio recordings. Second, we investigate the influence of the input representation type on the corpus-level observations. In our experiments, all representation types led to qualitatively similar results. We conclude that audio recordings can build a reasonable basis for conducting such type of corpus analysis.",DEU,facility,Developed economies,"[7.636867, 13.923883]","[-13.884133, 10.405474]","[-8.325508, -4.3607492, 30.349775]","[-10.005117, 9.936595, 4.5403943]","[10.75971, 9.589237]","[7.7956934, 2.8060722]","[12.006437, 14.557419, -0.63394845]","[9.719256, 6.338544, 11.721948]"
55,Pasquale Lisena;Konstantin Todorov;Cécile Cecconi;Françoise Leresche;Isabelle Canno;Frédéric Puyrenier;Martine Voisin;Thierry Le Meur;Raphaël Troncy,Controlled Vocabularies for Music Metadata,2018,https://doi.org/10.5281/zenodo.1492441,Pasquale Lisena+EURECOM>FRA>education|LIRMM>FRA>education|Philharmonie de Paris>FRA>facility|Bibliotheque nationale de France>FRA>facility|Radio France>FRA>company;Konstantin Todorov+LIRMM>FRA>education;Cécile Cecconi+Philharmonie de Paris>FRA>facility;Françoise Leresche+Bibliotheque nationale de France>FRA>facility;Isabelle Canno+Radio France>FRA>company;Frédéric Puyrenier+Bibliotheque nationale de France>FRA>facility;Martine Voisin+Radio France>FRA>company;Thierry Le Meur+Philharmonie de Paris>FRA>facility;Raphaël Troncy+EURECOM>FRA>education,"We present a set of music-specific controlled vocabularies, formalized using Semantic Web languages, describing topics like musical genres, keys, or medium of performance. We have collected a number of existing vocabularies in various formats, converted them to SKOS and performed the interconnection of their equivalent terms. In addition, novel vocabularies, not available online before, have been designed by an editorial team. Next to multilingual labels and definitions, we provide hierarchical relations as well as links to external resources. We also show the application of those vocabularies for the production of vector embeddings, allowing for the calculation of distances between keys or between instruments.",FRA,education,Developed economies,"[-24.35631, 35.525627]","[15.262755, 38.838264]","[-22.749458, 0.51423055, -8.709378]","[-2.1199577, -2.732292, 27.992823]","[14.219382, 8.980349]","[10.78505, -0.17867199]","[15.064105, 13.610983, -1.1607144]","[12.113882, 5.1505113, 11.606304]"
33,Francisco Castellanos;Jorge Calvo-Zaragoza;Gabriel Vigliensoni;Ichiro Fujinaga,Document Analysis of Music Score Images with Selectional Auto-Encoders,2018,https://doi.org/10.5281/zenodo.1492397,"Francisco J. Castellanos+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+PRHLT Research Center, Universitat Politècnica de València>ESP>facility;Gabriel Vigliensoni+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education","The document analysis of music score images is a key step in the development of successful Optical Music Recognition systems. The current state of the art considers the use of deep neural networks trained to classify every pixel of the image according to the image layer it belongs to. This process, however, involves a high computational cost that prevents its use in interactive machine learning scenarios. In this paper, we propose the use of a set of deep selectional auto-encoders, implemented as fully-convolutional networks, to perform image-to-image categorizations. This strategy retains the advantages of using deep neural networks, which have demonstrated their ability to perform this task, while dramatically increasing the efficiency by processing a large number of pixels in a single step. The results of an experiment performed with a set of high-resolution images taken from Medieval manuscripts successfully validate this approach, with a similar accuracy to that of the state of the art but with a computational time orders of magnitude smaller, making this approach appropriate for being used in interactive applications.",ESP,education,Developed economies,"[20.604244, -8.977091]","[-21.737001, -25.37869]","[21.818089, 20.789106, -1.2888458]","[-16.762478, -17.569508, -2.216249]","[10.652539, 6.5540886]","[6.5065036, -1.075718]","[12.279818, 12.277365, -1.5026903]","[8.124182, 4.2424297, 9.935419]"
57,Eric Humphrey;Simon Durand;Brian McFee,OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition,2018,https://doi.org/10.5281/zenodo.1492445,Eric J. Humphrey+Spotify>USA>company;Simon Durand+Spotify>USA>company;Brian McFee+New York University>USA>education,"Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music information retrieval. While there has been significant progress in developing predictive models for this and related classification tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset contains 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the dataset was sampled and annotated, and compare its characteristics to similar, previous data-sets. Finally, we present experimental results and baseline model performance to motivate future work.",USA,company,Developed economies,"[11.911655, -25.193117]","[-11.062713, 40.80069]","[16.326727, -7.2738476, 2.4560754]","[6.617156, 22.51729, -8.873878]","[8.828566, 7.085093]","[9.246675, 4.3039064]","[11.03068, 12.4473715, 0.34447697]","[10.732907, 6.1870246, 10.301114]"
46,Louis Bigo;Laurent Feisthauer;Mathieu Giraud;Florence Levé,Relevance of Musical Features for Cadence Detection,2018,https://doi.org/10.5281/zenodo.1492423,"Louis Bigo+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Laurent Feisthauer+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Mathieu Giraud+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Florence Levé+MIS, Université de Picardie Jules Verne>FRA>education","Cadences, as breaths in music, are felt by the listener or studied by the theorist by combining harmony, melody, texture and possibly other musical aspects. We formalize and discuss the significance of 44 cadential features, correlated with the occurrence of cadences in scores. These features describe properties at the arrival beat of a cadence and its surroundings, but also at other onsets heuristically identified to pinpoint chords preparing the cadence. The representation of each beat of the score as a vector of cadential features makes it possible to reformulate cadence detection as a classification task. An SVM classifier was run on two corpora from Bach and Haydn totaling 162 perfect authentic cadences and 70 half cadences. In these corpora, the classifier correctly identified more than 75% of perfect authentic cadences and 50% of half cadences, with low false positive rates. The experiment results are consistent with common knowledge that classification is more complex for half cadences than for authentic cadences.",FRA,education,Developed economies,"[13.17707, 2.9690611]","[9.9895735, -3.378537]","[-4.692005, -20.92424, 15.1445]","[-0.112800345, -7.89403, 1.430495]","[11.629855, 6.687462]","[8.495024, 2.6775887]","[12.316031, 14.584436, -0.8051909]","[10.078815, 7.14997, 11.879096]"
32,Jorge Calvo-Zaragoza;David Rizo,Camera-PrIMuS: Neural End-to-End Optical Music Recognition on Realistic Monophonic Scores,2018,https://doi.org/10.5281/zenodo.1492395,Jorge Calvo-Zaragoza+PRHLT Research Center>ESP>facility;David Rizo+Universidad de Alicante>ESP>education,"The optical music recognition (OMR) field studies how to automate the process of reading the musical notation present in a given image. Among its many uses, an interesting scenario is that in which a score captured with a camera is to be automatically reproduced. Recent approaches to OMR have shown that the use of deep neural networks allows important advances in the field. However, these approaches have been evaluated on images with ideal conditions, which do not correspond to the previous scenario. In this work, we evaluate the performance of an end-to-end approach that uses a deep convolutional recurrent neural network (CRNN) over non-ideal image conditions of music scores. Consequently, our contribution also consists of Camera-PrIMuS, a corpus of printed monophonic scores of real music synthetically modified to resemble camera-based realistic scenarios, involving distortions such as irregular lighting, rotations, or blurring. Our results confirm that the CRNN is able to successfully solve the task under these conditions, obtaining an error around 2% at music-symbol level, thereby representing a groundbreaking piece of research towards useful OMR systems.",ESP,facility,Developed economies,"[39.242283, 18.538494]","[-22.178291, -24.525917]","[16.311758, 10.951711, 12.690785]","[-16.312073, -16.523516, -4.378942]","[8.726698, 6.248547]","[6.4808917, -1.1044997]","[10.738862, 11.127454, -0.12980737]","[8.1507225, 4.2848554, 9.871115]"
53,Hendrik Schreiber;Meinard Müller,A Crowdsourced Experiment for Tempo Estimation of Electronic Dance Music,2018,https://doi.org/10.5281/zenodo.1492437,Hendrik Schreiber+tagtraum industries incorporated>DEU>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>education,"Relative to other datasets, state-of-the-art tempo estimation algorithms perform poorly on the GiantSteps Tempo dataset for electronic dance music (EDM). In order to investigate why, we conducted a large-scale, crowdsourced experiment involving 266 participants from two distinct groups. The quality of the collected data was evaluated with regard to the participants' input devices and background. In the data itself we observed significant tempo ambiguities, which we attribute to annotator subjectivity and tempo instability. As a further contribution, we then constructed new annotations consisting of tempo distributions for each track. Using these annotations, we reevaluated two recent state-of-the-art tempo estimation systems achieving significantly improved results. The main conclusions of this investigation are that current tempo estimation systems perform better than previously thought and that evaluation quality needs to be improved. The new crowdsourced annotations will be released for evaluation purposes.",DEU,company,Developed economies,"[38.237812, -24.690952]","[-32.511898, -3.4687412]","[-2.4628036, -25.397816, -1.823331]","[-11.229779, 11.808759, -8.282211]","[11.577473, 4.6626697]","[5.1432233, 1.691002]","[10.993463, 13.419465, -2.789792]","[7.411462, 6.6705465, 11.042454]"
30,Tim Crawford;Golnaz Badkobeh;David Lewis,Searching Page-Images of Early Music Scanned with OMR: A Scalable Solution Using Minimal Absent Words,2018,https://doi.org/10.5281/zenodo.1492391,"Tim Crawford+Goldsmiths, University of London>GBR>education;Golnaz Badkobeh+Goldsmiths, University of London>GBR>education;David Lewis+Oxford eResearch Centre>GBR>facility","We define three retrieval tasks requiring efficient search of the musical content of a collection of ~32k pageimages of 16th-century music to find: duplicates; pages with the same musical content; pages of related music.  The images are subjected to Optical Music Recognition (OMR), introducing inevitable errors. We encode pages as strings of diatonic pitch intervals, ignoring rests, to reduce the effect of such errors. We extract indices comprising lists of two kinds of 'word'. Approximate matching is done by counting the number of common words between a query page and those in the collection.  The two word-types are (a) normal ngrams and (b) minimal absent words (MAWs). The latter have three important properties for our purpose: they can be built and searched in linear time, the number of MAWs generated tends to be smaller, and they preserve the structure and order of the text, obviating the need for expensive sorting operations.  We show that retrieval performance of MAWs is comparable with ngrams, but with a marked speed improvement. We also show the effect of word length on retrieval. Our results suggest that an index of MAWs of mixed length provides a good method for these tasks which is scalable to larger collections.",GBR,education,Developed economies,"[-14.598815, 26.16666]","[13.993114, 17.781128]","[-5.525513, 7.5953383, -19.323479]","[8.629694, -7.2587767, 8.565369]","[13.771961, 7.625403]","[9.357581, 0.6243023]","[13.748621, 14.347822, -2.2395806]","[10.869835, 6.034453, 13.167557]"
58,Chih-Wei Wu;Alexander Lerch,From Labeled to Unlabeled Data – On the Data Challenge in Automatic Drum Transcription,2018,https://doi.org/10.5281/zenodo.1492447,Chih-Wei Wu+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"Automatic Drum Transcription (ADT), like many other music information retrieval tasks, has made progress in the past years through the integration of machine learning and audio signal processing techniques. However, with the increasing popularity of data-hungry approaches such as deep learning, the insufficient amount of data becomes more and more a challenge that concerns the generality of the resulting models and the validity of the evaluation. To address this challenge in ADT, this paper first examines the existing labeled datasets and how representative they are of the research problem. Next, possibilities of using unlabeled data to improve general ADT systems are explored. Specifically, two paradigms that harness information from unlabeled data, namely feature learning and student-teacher learning, are applied to two major types of ADT systems. All systems are evaluated on four different drum datasets. The results highlight the necessity of more and larger annotated datasets and indicate the feasibility of exploiting unlabeled data for improving ADT systems.",USA,education,Developed economies,"[29.3299, -44.43737]","[-37.554234, -15.999602]","[21.646692, -20.4821, 6.6723886]","[3.0887063, 12.628485, -21.395086]","[7.5773005, 7.1503673]","[8.499691, 4.58072]","[10.360756, 11.422399, 1.0739112]","[9.285275, 7.0938544, 9.575721]"
7,Carl Southall;Ryan Stables;Jason Hockman,Player Vs Transcriber: A Game Approach To Data Manipulation For Automatic Drum Transcription,2018,https://doi.org/10.5281/zenodo.1492343,Carl Southall+Birmingham City University>GBR>education;Ryan Stables+Birmingham City University>GBR>education;Jason Hockman+Birmingham City University>GBR>education,"State-of-the-art automatic drum transcription (ADT) approaches utilise deep learning methods reliant on timeconsuming manual annotations and require congruence between training and testing data. When these conditions are not held, they often fail to generalise. We propose a game approach to ADT, termed player vs transcriber (PvT), in which a player model aims to reduce transcription accuracy of a transcriber model by manipulating training data in two ways. First, existing data may be augmented, allowing the transcriber to be trained using recordings with modified timbres. Second, additional individual recordings from sample libraries are included to generate rare combinations. We present three versions of the PvT model: AugExist, which augments pre-existing recordings; AugAddExist, which adds additional samples of drum hits to the AugExist system; and Generate, which generates training examples exclusively from individual drum hits from sample libraries. The three versions are evaluated alongside a state-of-the-art deep learning ADT system using two evaluation strategies. The results demonstrate that including the player network improves the ADT performance and suggests that this is due to improved generalisability. The results also indicate that although the Generate model achieves relatively low results, it is a viable choice when annotations are not accessible.",GBR,education,Developed economies,"[29.878132, -43.777946]","[-37.745106, -14.694337]","[21.615463, -20.120842, 8.455373]","[1.5523412, 11.24998, -19.999899]","[7.567121, 7.132846]","[8.456561, 4.5012336]","[10.357499, 11.39307, 1.0859623]","[9.137034, 7.089328, 9.588222]"
8,Nathaniel Condit-Schultz;Yaolong Ju;Ichiro Fujinaga,A Flexible Approach to Automated Harmonic Analysis: Multiple Annotations of Chorales by Bach and Prætorius,2018,https://doi.org/10.5281/zenodo.1492345,Nathaniel Condit-Schultz+McGill University>CAN>education;Yaolong Ju+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Despite being a core component of Western music theory, harmonic analysis remains a subjective endeavor, resistant automation. This subjectivity arises from disagreements regarding, among other things, the interpretation of contrapuntal figures, the set of ""legal"" harmonies, and how harmony relates to more abstract features like tonal function. In this paper, we provide a formal specification of harmonic analysis. We then present a novel approach to computational harmonic analysis: rather than computing harmonic analyses based on one specific set of rules, we compute all possible analyses which satisfy only basic, uncontroversial constraints. These myriad interpretations can later be filtered to extract preferred analyses; for instance, to forbid 7th chords or to prefer analyses with fewer non-chord tones. We apply this approach to two concrete musical datasets: existing encodings of 371 chorales by J.S. Bach and new encodings of 200 chorales by M. Prætorius. Through an online API users can filter and download numerous harmonic interpretations of these 571 chorales. This dataset will serve as a useful resource in the study of harmonic/functional progression, voice-leading, and the relationship between melody and harmony, and as a stepping stone towards automated harmonic analysis of more complex music.",CAN,education,Developed economies,"[17.931686, -2.4261203]","[-19.611303, 18.01905]","[-0.27508926, -15.249289, 25.78257]","[-18.16587, -6.2803316, 7.226101]","[10.434904, 8.935658]","[7.7816653, 2.182041]","[12.185691, 13.567878, -0.7413092]","[10.001069, 7.8747683, 12.496679]"
9,Tejaswinee Kelkar;Udit Roy;Alexander Refsum Jensenius,Evaluating a Collection of Sound-Tracing Data of Melodic Phrases,2018,https://doi.org/10.5281/zenodo.1492347,Tejaswinee Kelkar+University of Oslo>NOR>education;Udit Roy+Unknown>Unknown>Unknown;Alexander Refsum Jensenius+University of Oslo>NOR>education,"Melodic contour, the 'shape' of a melody, is a common way to visualize and remember a musical piece. The purpose of this paper is to explore the building blocks of a future 'gesture-based' melody retrieval system. We present a dataset containing 16 melodic phrases from four musical styles and with a large range of contour variability. This is accompanied by full-body motion capture data of 26 participants performing sound-tracing to the melodies. The dataset is analyzed using canonical correlation analysis (CCA), and its neural network variant (Deep CCA), to understand how melodic contours and sound tracings relate to each other. The analyses reveal non-linear relationships between sound and motion. The link between pitch and verticality does not appear strong enough for complex melodies. We also find that descending melodic contours have the least correlation with tracings.",NOR,education,Developed economies,"[2.0115702, 19.629469]","[5.0082145, -7.5647964]","[2.8207574, 7.906183, -3.2915094]","[7.514795, -8.355968, -0.46013403]","[12.122259, 9.807977]","[7.9463725, 1.5064653]","[12.570537, 15.552703, -0.864731]","[9.530925, 6.9453998, 12.445465]"
10,Dogac Basaran;Slim Essid;Geoffroy Peeters,Main Melody Estimation with Source-Filter NMF and CRNN,2018,https://doi.org/10.5281/zenodo.1492349,"Dogac Basaran+CNRS, Ircam Lab, Sorbonne Université>FRA>education;Slim Essid+Télécom ParisTech, Université Paris Saclay>FRA>education;Geoffroy Peeters+CNRS, Ircam Lab, Sorbonne Université>FRA>education","Estimating the main melody of a polyphonic audio recording remains a challenging task. We approach the task from a classification perspective and adopt a convolutional recurrent neural network (CRNN) architecture that relies on a particular form of pretraining by source-filter nonnegative matrix factorisation (NMF). The source-filter NMF decomposition is chosen for its ability to capture the pitch and timbre content of the leading voice/instrument, providing a better initial pitch salience than standard timefrequency representations. Starting from such a musically motivated representation, we propose to further enhance the NMF-based salience representations with CNN layers, then to model the temporal structure by an RNN network and to estimate the dominant melody with a final classification layer. The results show that such a system achieves state-of-the-art performance on the MedleyDB dataset without any augmentation methods or large training sets.",FRA,education,Developed economies,"[6.733744, -12.522694]","[-31.673237, -32.999493]","[15.629792, 8.441859, -3.8925219]","[-9.3389, -10.333175, -23.107244]","[10.133486, 9.911527]","[7.661419, 5.445031]","[11.006164, 14.923147, -0.42013162]","[9.671143, 7.1884875, 8.849668]"
11,Tsung-Ping Chen;Li Su,Functional Harmony Recognition of Symbolic Music Data with Multi-task Recurrent Neural Networks,2018,https://doi.org/10.5281/zenodo.1492351,Tsung-Ping Chen+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown;Li Su+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown,"Previous works on chord recognition mainly focus on chord symbols but overlook other essential features that matter in musical harmony. To tackle the functional harmony recognition problem, we compile a new professionally annotated dataset of symbolic music encompassing not only chord symbols, but also various interrelated chord functions such as key modulation, chord inversion, secondary chords, and chord quality. We further present a novel holistic system in functional harmony recognition; a multi-task learning (MTL) architecture is implemented with the recurrent neural network (RNN) to jointly model chord functions in an end-to-end scenario. Experimental results highlight the capability of the proposed recognition system, and a promising improvement of the system by employing multi-task learning instead of single-task learning. This is one attempt to challenge the end-to-end chord recognition task from the perspective of functional harmony so as to uncover the grand structure ruling the flow of musical sound. The dataset and the source code of the proposed system is announced at https://github.com/ Tsung-Ping/functional-harmony.",TWN,education,Developing economies,"[14.22784, 12.674709]","[-35.77676, 23.82988]","[-0.84901583, -4.3179297, 11.849459]","[-28.653769, -2.6010249, -3.6653597]","[11.650073, 7.1242704]","[5.862941, 3.8935704]","[13.3648205, 12.41642, -0.9161982]","[9.608055, 9.169732, 12.2747345]"
12,Hendrik Schreiber;Meinard Müller,A Single-step Approach to Musical Tempo Estimation using a Convolutional Neural Network,2018,https://doi.org/10.5281/zenodo.1492353,Hendrik Schreiber+tagtraum industries incorporated>USA>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>education,"We present a single-step musical tempo estimation system based solely on a convolutional neural network (CNN). Contrary to existing systems, which typically first identify onsets or beats and then derive a tempo, our system estimates the tempo directly from a conventional melspectrogram in a single step. This is achieved by framing tempo estimation as a multi-class classification problem using a network architecture that is inspired by conventional approaches. The system's CNN has been trained with the union of three datasets covering a large variety of genres and tempi using problem-specific data augmentation techniques. Two of the three ground-truths are novel and will be released for research purposes. As input the system requires only 11.9 s of audio and is therefore suitable for local as well as global tempo estimation. When used as a global estimator, it performs as well as or better than other state-of-the-art algorithms. Especially the exact estimation of tempo without tempo octave confusion is significantly improved. As local estimator it can be used to identify and visualize tempo drift in musical performances.",USA,company,Developed economies,"[40.350708, -25.06355]","[-32.085403, -11.009765]","[1.5818143, -28.784304, 1.4211874]","[-7.1272645, 10.751825, -16.00731]","[11.544611, 4.3173046]","[4.9812584, 2.2270842]","[10.750578, 13.398343, -2.8419468]","[7.557486, 6.9369583, 10.446523]"
13,Magdalena Fuentes;Brian McFee;Hélène C. Crayencour;Slim Essid;Juan Pablo Bello,Analysis of Common Design Choices in Deep Learning Systems for Downbeat Tracking,2018,https://doi.org/10.5281/zenodo.1492355,"Magdalena Fuentes+L2S, CNRS-Univ. Paris-Sud-CentraleSup´elec>FRA>education|LTCI, T´el´ecom ParisTech, Univ. Paris-Saclay>FRA>education;Brian McFee+Music and Audio Research Laboratory, New York University>USA>education|Center of Data Science, New York University>USA>education;Hélène C. Crayencour+L2S, CNRS-Univ. Paris-Sud-CentraleSup´elec>FRA>education;Slim Essid+LTCI, T´el´ecom ParisTech, Univ. Paris-Saclay>FRA>education;Juan P. Bello+Music and Audio Research Laboratory, New York University>USA>education|Center of Data Science, New York University>USA>education","Downbeat tracking consists of annotating a piece of musical audio with the estimated position of the first beat of each bar. In recent years, increasing attention has been paid to applying deep learning models to this task, and various architectures have been proposed, leading to a significant improvement in accuracy. However, there are few insights about the role of the various design choices and the delicate interactions between them. In this paper we offer a systematic investigation of the impact of largely adopted variants. We study the effects of the temporal granularity of the input representation (i.e. beat-level vs tatum-level) and the encoding of the networks outputs. We also investigate the potential of convolutional-recurrent networks, which have not been explored in previous downbeat tracking systems. To this end, we exploit a state-of-the-art recurrent neural network where we introduce those variants, while keeping the training data, network learning parameters and postprocessing stages fixed. We find that temporal granularity has a significant impact on performance, and we analyze its interaction with the encoding of the networks outputs.",FRA,education,Developed economies,"[37.289925, -36.28092]","[-31.430923, -14.294498]","[12.096229, -30.490433, -2.3937452]","[-8.068703, 12.804803, -18.824041]","[10.450673, 4.151959]","[4.9829025, 2.594812]","[10.075934, 12.806784, -2.2995496]","[7.7499895, 6.8986835, 10.120308]"
14,Andrew McLeod;Mark Steedman,Meter Detection and Alignment of MIDI Performance,2018,https://doi.org/10.5281/zenodo.1492357,Andrew McLeod+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education;Mark Steedman+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education,"Metrical alignment is an integral part of any complete automatic music transcription (AMT) system. In this paper, we present an HMM for both detecting the metrical structure of given live performance MIDI data, and aligning that structure with the underlying notes. The model takes as input only a list of the notes present in a performance, and labels bars, beats, and sub beats in time. We also present an incremental algorithm which can perform inference on the model efficiently using a modified Viterbi search. We propose a new metric designed for the task, and using it, we show that our model achieves state-of-the-art performance on a corpus of metronomically aligned MIDI data, as well as a second corpus of live performance MIDI data. The code for the model described in this paper is available at https://www.github.com/apmcleod/met-align.",GBR,education,Developed economies,"[38.87829, -2.1901681]","[-13.691181, -12.330557]","[10.145228, -11.098644, 23.612871]","[-2.0076237, -16.407412, -5.51821]","[10.579968, 6.513534]","[6.4828057, 1.0977736]","[12.41065, 11.616182, -0.95356256]","[8.452405, 6.167574, 10.819784]"
15,Dasaem Jeong;Taegyun Kwon;Juhan Nam,A Timbre-based Approach to Estimate Key Velocity from Polyphonic Piano Recordings,2018,https://doi.org/10.5281/zenodo.1492359,Dasaem Jeong+KAIST>KOR>education;Taegyun Kwon+KAIST>KOR>education;Juhan Nam+KAIST>KOR>education,"Estimating the key velocity of each note from polyphonic piano music is a highly challenging task. Previous work addressed the problem by estimating note intensity using a polyphonic note model. However, they are limited because the note intensity is vulnerable to various factors in a recording environment. In this paper, we propose a novel method to estimate the key velocity focusing on timbre change which is another cue associated with the key velocity. To this end, we separate individual notes of polyphonic piano music using non-negative matrix factorization (NMF) and feed them into a neural network that is trained to discriminate the timbre change according to the key velocity. Combining the note intensity from the separated notes with the statistics of the neural network prediction, the proposed method estimates the key velocity in the dimension of MIDI note velocity. The evaluation on Saarland Music Data and the MAPS dataset shows promising results in terms of robustness to changes in the recording environment.",KOR,education,Developing economies,"[35.741802, -3.7614586]","[-31.083778, -25.549366]","[9.791094, -13.118338, 15.067258]","[-7.530384, -12.947613, -24.686403]","[10.346648, 7.261778]","[6.3304853, 4.972841]","[12.14078, 12.016924, -0.28742927]","[9.387103, 8.543145, 9.63907]"
16,Francesco Bigoni;Sofia Dahl,Timbre Discrimination for Brief Instrument Sounds,2018,https://doi.org/10.5281/zenodo.1492361,Francesco Bigoni+Aalborg University>DNK>education;Soﬁa Dahl+Aalborg University>DNK>education,"Timbre discrimination, even for very brief sounds, allows identification and separation of different sound sources. The existing literature on the effect of duration on timbre recognition shows high performance for remarkably short time window lengths, but does not address the possible effect of musical training. In this study, we applied an adaptive procedure to investigate the effect of musical training on individual thresholds for instrument identification. A timbre discrimination task consisting of a 4-alternative forced choice (4AFC) of brief instrument sounds with varying duration was assigned to 16 test subjects using an adaptive staircase method. The effect of musical training has been investigated by dividing the participants into two groups: musicians and non-musicians. The experiment showed lowest thresholds for the guitar sound and highest for the violin sound, with a high overall performance level, but no significant difference between the two groups. It is suggested that the test subjects adjust the weightings of the perceptual dimensions of timbre according to different degrees of acoustic degradation of the stimuli, which are evaluated both by plotting extracted audio features in a feature space and by considering the timbral specificities of the four instruments.",DNK,education,Developed economies,"[9.55157, -28.67183]","[-44.223267, 2.0215507]","[17.778902, -11.489115, -2.615134]","[10.834418, 13.561111, -15.905168]","[8.877736, 7.502841]","[8.840417, 3.7588882]","[11.076109, 12.722651, 0.64574254]","[10.510984, 7.4435544, 10.27331]"
31,Alexander Pacha;Jorge Calvo-Zaragoza,Optical Music Recognition in Mensural Notation with Region-based Convolutional Neural Networks,2018,https://doi.org/10.5281/zenodo.1492393,"Alexander Pacha+Institute of Visual Computing and Human-Centered Technology, TU Wien>AUT>education;Jorge Calvo-Zaragoza+PRHLT Research Center, Universitat Politècnica de València>ESP>facility","In this work, we present an approach for the task of optical music recognition (OMR) using deep neural networks. Our intention is to simultaneously detect and categorize musical symbols in handwritten scores, written in mensural notation. We propose the use of region-based convolutional neural networks, which are trained in an end-toend fashion for that purpose. Additionally, we make use of a convolutional neural network that predicts the relative position of a detected symbol within the staff, so that we cover the entire image-processing part of the OMR pipeline. This strategy is evaluated over a set of 60 ancient scores in mensural notation, with more than 15000 annotated symbols belonging to 32 different classes. The results reflect the feasibility and capability of this approach, with a weighted mean average precision of around 76% for symbol detection, and over 98% accuracy for predicting the position.",AUT,education,Developed economies,"[41.82657, 21.575457]","[-20.819426, -23.165657]","[20.289633, 14.196229, 14.967673]","[-15.784722, -20.756018, -1.9509988]","[8.631166, 6.0689883]","[6.56162, -1.0144179]","[10.542977, 10.96072, -0.28267792]","[8.014461, 4.138805, 10.0690155]"
17,Yun-Ning Hung;Yi-Hsuan Yang,Frame-level Instrument Recognition by Timbre and Pitch,2018,https://doi.org/10.5281/zenodo.1492363,"Yun-Ning Hung+Research Center for IT Innovation, Academia Sinica>TWN>facility;Yi-Hsuan Yang+Research Center for IT Innovation, Academia Sinica>TWN>facility","Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at https://biboamy. github.io/instrument-recognition/.",TWN,facility,Developing economies,"[10.547408, -25.407364]","[-29.298237, -29.094763]","[17.797686, -9.32516, -0.478997]","[-10.357623, -9.188846, -16.92218]","[8.804696, 7.1416254]","[7.972267, 5.0441647]","[11.040348, 12.569678, 0.38958398]","[9.413564, 6.986691, 9.041215]"
19,Daniel Harasim;Martin Rohrmeier;Timothy J. O'Donnell,A Generalized Parsing Framework for Generative Models of Harmonic Syntax,2018,https://doi.org/10.5281/zenodo.1492367,"Daniel Harasim+École Polytechnique Fédérale de Lausanne>CHE>education|Institut für Kunst- und Musikwissenschaft, TU Dresden>DEU>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education|Institut für Kunst- und Musikwissenschaft, TU Dresden>DEU>education;Timothy J. O’Donnell+McGill University>CAN>education","Modeling the structure of musical pieces constitutes a central research problem for music information retrieval, music generation, and musicology. At the present, models of harmonic syntax face challenges on the tasks of detecting local and higher-level modulations (most previous models assume a priori knowledge of key), computing connected parse trees for long sequences, and parsing sequences that do not end with tonic chords, but in turnarounds. This paper addresses those problems by proposing a new generative formalism Probabilistic Abstract Context-Free Grammars (PACFGs) to address these issues, and presents variants of standard parsing algorithms that efficiently enumerate all possible parses of long chord sequences and to estimate their probabilities. PACFGs specifically allow for structured non-terminal symbols in rich and highly flexible feature spaces. The inference procedure moreover takes advantage of these abstractions by sharing probability mass between grammar rules over joint features. The paper presents a model of the harmonic syntax of Jazz using this formalism together with stochastic variational inference to learn the probabilistic parameters of a grammar from a corpus of Jazz-standards. The PACFG model outperforms the standard context-free approach while reducing the number of free parameters and performing key finding on the fly.",CHE,education,Developed economies,"[25.333221, 22.542587]","[-19.658781, 24.1049]","[-4.0494847, -9.447584, 30.159113]","[-18.246367, 0.570205, 2.9945047]","[9.89679, 9.2031145]","[7.1995063, 3.3295004]","[11.703383, 14.048387, -0.95628387]","[9.512401, 7.8752065, 12.15961]"
20,Peter M. C. Harrison;Marcus T. Pearce,An Energy-based Generative Sequence Model for Testing Sensory Theories of Western Harmony,2018,https://doi.org/10.5281/zenodo.1492369,Peter M. C. Harrison+Queen Mary University of London>GBR>education;Marcus T. Pearce+Queen Mary University of London>GBR>education,"The relationship between sensory consonance and Western harmony is an important topic in music theory and psychology. We introduce new methods for analysing this relationship, and apply them to large corpora representing three prominent genres of Western music: classical, popular, and jazz music. These methods centre on a generative sequence model with an exponential-family energy-based form that predicts chord sequences from continuous features. We use this model to investigate one aspect of instantaneous consonance (harmonicity) and two aspects of sequential consonance (spectral distance and voice-leading distance). Applied to our three musical genres, the results generally support the relationship between sensory consonance and harmony, but lead us to question the high importance attributed to spectral distance in the psychological literature. We anticipate that our methods will provide a useful platform for future work linking music psychology to music theory.",GBR,education,Developed economies,"[28.139536, 23.718924]","[-23.33299, 16.794977]","[-0.8516475, -9.956786, 31.88576]","[-22.402613, -5.899379, 7.7698107]","[9.884537, 9.215858]","[7.685545, 2.4002986]","[11.6880455, 14.022765, -0.9346385]","[10.1841755, 8.020638, 12.532889]"
21,Shun-Yao Shih;Heng-Yu Chi,"Automatic, Personalized, and Flexible Playlist Generation using Reinforcement Learning",2018,https://doi.org/10.5281/zenodo.1492371,Shun-Yao Shih+National Taiwan University>TWN>education;Heng-Yu Chi+KKBOX Inc.>TWN>company,"Songs can be well arranged by professional music curators to form a riveting playlist that creates engaging listening experiences. However, it is time-consuming for curators to timely rearrange these playlists for fitting trends in future. By exploiting the techniques of deep learning and reinforcement learning, in this paper, we consider music playlist generation as a language modeling problem and solve it by the proposed attention language model with policy gradient. We develop a systematic and interactive approach so that the resulting playlists can be tuned flexibly according to user preferences. Considering a playlist as a sequence of words, we first train our attention RNN language model on baseline recommended playlists. By optimizing suitable imposed reward functions, the model is thus refined for corresponding preferences. The experimental results demonstrate that our approach not only generates coherent playlists automatically but is also able to flexibly recommend personalized playlists for diversity, novelty and freshness.",TWN,education,Developing economies,"[-39.84711, 39.817028]","[39.229816, 21.77973]","[-0.22054, 29.762096, -3.79683]","[18.091547, 5.8753676, 20.27316]","[16.18291, 8.146711]","[12.26878, 1.76578]","[16.566648, 14.826817, -1.7449065]","[13.335366, 5.1569734, 13.011225]"
22,Philippe Esling;Axel Chemla--Romeu-Santos;Adrien Bitton,"Bridging Audio Analysis, Perception and Synthesis with Perceptually-regularized Variational Timbre Spaces",2018,https://doi.org/10.5281/zenodo.1492373,Philippe Esling+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility;Axel Chemla–Romeu-Santos+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility;Adrien Bitton+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility,"Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception.",FRA,facility,Developed economies,"[11.316435, -31.4343]","[-11.33364, -47.259842]","[7.421956, 6.825788, 18.406675]","[-15.564443, 2.0916703, -16.920599]","[9.783337, 8.55323]","[8.689601, 6.4259825]","[12.747679, 12.280019, 0.52418697]","[9.700607, 5.786911, 8.45138]"
23,Rachel Manzelli;Vijay Thakkar;Ali Siahkamari;Brian Kulis,Conditioning Deep Generative Raw Audio Models for Structured Automatic Music,2018,https://doi.org/10.5281/zenodo.1492375,Rachel Manzelli+Boston University>USA>education;Vijay Thakkar+Boston University>USA>education;Ali Siahkamari+Boston University>USA>education;Brian Kulis+Boston University>USA>education,"Existing automatic music generation approaches that feature deep learning can be broadly classified into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind's WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work.",USA,education,Developed economies,"[24.594505, 2.740827]","[-8.726126, -40.01449]","[7.937792, 2.3475842, 23.071709]","[-22.191057, 2.2806497, -10.797433]","[10.111404, 8.565261]","[9.104922, 6.302648]","[13.119952, 11.928371, 0.3738985]","[9.652983, 5.4629564, 8.872187]"
24,Hao-Wen Dong;Yi-Hsuan Yang,Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation,2018,https://doi.org/10.5281/zenodo.1492377,Hao-Wen Dong+Academia Sinica>TWN>education|Research Center for IT Innovation>Unknown>Unknown;Yi-Hsuan Yang+Academia Sinica>TWN>education|Research Center for IT Innovation>Unknown>Unknown,"It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binaryvalued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binaryvalued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445. github.io/bmusegan/.",TWN,education,Developing economies,"[24.881721, 4.031257]","[-15.402932, -48.2839]","[6.468525, 4.26685, 25.392271]","[-24.715319, 1.1316592, -14.869049]","[10.003271, 8.398144]","[8.407313, 6.601076]","[13.142398, 11.777394, 0.42311314]","[9.555045, 5.8116446, 8.233114]"
25,Christopher Tralie,Cover Song Synthesis by Analogy,2018,https://doi.org/10.5281/zenodo.1492381,Christopher J. Tralie+Duke University>USA>education,"In this work, we pose and address the following ""cover song analogies"" problem: given a song A by artist 1 and a cover song A' of this song by artist 2, and given a different song B by artist 1, synthesize a song B' which is a cover of B in the style of artist 2. Normally, such a polyphonic style transfer problem would be quite challenging, but we show how the cover songs example constrains the problem, making it easier to solve. First, we extract the longest common beat-synchronous subsequence between A and A', and we time stretch the corresponding beat intervals in A' so that they align with A. We then derive a version of joint 2D convolutional NMF, which we apply to the constant-Q spectrograms of the synchronized segments to learn a translation dictionary of sound templates from A to A'. Finally, we apply the learned templates as filters to the song B, and we mash up the translated filtered components into the synthesized song B' using audio mosaicing. We showcase our algorithm on several examples, including a synthesized cover version of Michael Jackson's ""Bad"" by Alien Ant Farm, learned from the latter's ""Smooth Criminal"" cover.",USA,education,Developed economies,"[9.163375, 39.96005]","[-52.277752, -27.69083]","[-2.7567487, 11.271243, -23.376038]","[-11.295193, -17.475767, -24.777208]","[15.927298, 10.951943]","[6.37456, 4.866551]","[12.88257, 17.255392, -0.38178682]","[9.609847, 8.280093, 9.787372]"
26,Yujia Yan;Ethan Lustig;Joseph VanderStel;Zhiyao Duan,Part-invariant Model for Music Generation and Harmonization,2018,https://doi.org/10.5281/zenodo.1492383,Yujia Yan+University of Rochester>USA>education;Ethan Lustig+University of Rochester>USA>education;Joseph Vander Stel+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"Automatic music generation has been gaining more attention in recent years. Existing approaches, however, are mostly ad hoc to specific rhythmic structures or instrumentation layouts, and lack music-theoretic rigor in their evaluations. In this paper, we present a neural language (music) model that tries to model symbolic multi-part music. Our model is part-invariant, i.e., it can process/generate any part (voice) of a music score consisting of an arbitrary number of parts, using a single trained model. For better incorporating structural information of pitch spaces, we use a structured embedding matrix to encode multiple aspects of a pitch into a vector representation. The generation is performed by Gibbs Sampling. Meanwhile, our model directly generates note spellings to make outputs human-readable. We performed objective (grading) and subjective (listening) evaluations by recruiting music theorists to compare the outputs of our algorithm with those of music students on the task of bassline harmonization (a traditional pedagogical task). Our experiment shows that errors of our algorithm and students are differently distributed, and the range of ratings for generated pieces overlaps with students' to varying extents for our three provided basslines. This experiment suggests some future research directions.",USA,education,Developed economies,"[19.582634, 3.5145395]","[-12.200593, -31.583658]","[0.8523058, 0.33569226, 19.083937]","[-14.769792, -1.7958255, -5.3552895]","[10.176339, 8.84496]","[8.884982, 5.756488]","[13.230462, 12.029363, 0.15427029]","[9.502576, 5.822941, 9.518447]"
27,David Sears;Filip Korzeniowski;Gerhard Widmer,Evaluating Language Models of Tonal Harmony,2018,https://doi.org/10.5281/zenodo.1492385,David R. W. Sears+Texas Tech University>USA>education;Filip Korzeniowski+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most stateof-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types.",USA,education,Developed economies,"[26.944378, 24.01271]","[-7.791282, -31.478773]","[-3.2427487, -9.762587, 33.57353]","[-20.532484, -1.403859, -3.349194]","[9.8467045, 9.273378]","[8.75885, 5.5609837]","[11.713074, 14.081318, -0.93663293]","[9.438408, 5.9465604, 9.488758]"
28,Bochen Li;Akira Maezawa;Zhiyao Duan,Skeleton Plays Piano: Online Generation of Pianist Body Movements from MIDI Performance,2018,https://doi.org/10.5281/zenodo.1492387,Bochen Li+University of Rochester>USA>education;Akira Maezawa+Yamaha Corporation>JPN>company;Zhiyao Duan+University of Rochester>USA>education,"Generating expressive body movements of a pianist for a given symbolic sequence of key depressions is important for music interaction, but most existing methods cannot incorporate musical context information and generate movements of body joints that are further away from the fingers such as head and shoulders. This paper addresses such limitations by directly training a deep neural network system to map a MIDI note stream and additional metric structures to a skeleton sequence of a pianist playing a keyboard instrument in an online fashion. Experiments show that (a) incorporation of metric information yields in 4% smaller error, (b) the model is capable of learning the motion behavior of a specific player, and (c) no significant difference between the generated and real human movements is observed by human subjects in 75% of the pieces.",USA,education,Developed economies,"[34.330193, -0.12593634]","[-28.227713, -22.300375]","[12.983068, -3.5640004, 22.48308]","[-10.707189, -13.009992, -9.639316]","[10.010701, 7.2843933]","[8.072182, 5.6637864]","[12.416429, 11.567918, -0.4207894]","[8.9727, 6.1818676, 9.244644]"
29,Jan Hajič jr.;Matthias Dorfer;Gerhard Widmer;Pavel Pecina,Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets,2018,https://doi.org/10.5281/zenodo.1492389,Jan Hajič jr.+Charles University>CHE>education;Matthias Dorfer+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education;Pavel Pecina+Charles University>CHE>education,"Detecting music notation symbols is the most immediate unsolved subproblem in Optical Music Recognition for musical manuscripts. We show that a U-Net architecture for semantic segmentation combined with a trivial detector already establishes a high baseline for this task, and we propose tricks that further improve detection performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81. Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach.",CHE,education,Developed economies,"[33.203663, 29.756443]","[-20.115877, -23.138382]","[5.3773694, -21.011143, 19.783598]","[-14.38944, -21.434189, -2.9384968]","[9.660177, 6.307237]","[6.5350738, -1.0807521]","[12.648866, 11.311001, -1.4743285]","[7.9909463, 4.111854, 10.079054]"
18,Hiroaki Tsushima;Eita Nakamura;Katsutoshi Itoyama;Kazuyoshi Yoshii,Interactive Arrangement of Chords and Melodies Based on a Tree-Structured Generative Model,2018,https://doi.org/10.5281/zenodo.1492365,Hiroaki Tsushima+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"We describe an interactive music composition system that assists a user in refining chords and melodies by generating chords for melodies (harmonization) and vice versa (melodization). Since these two tasks have been dealt with independently, it is difficult to jointly estimate chords and melodies that are optimal in both tasks. Another problem is developing an interactive GUI that enables a user to partially update chords and melodies by considering the latent tree structure of music. To solve these problems, we propose a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chord symbols, (2) a metrical Markov model for chord boundaries, (3) a Markov model for melody pitches, and (4) a metrical Markov model for melody onsets. The harmonic functions (syntactic roles) and repetitive structure of chords are learned by the PCFG. Any variables specified by a user can be optimized or sampled in a principled manner according to a unified posterior distribution. For improved melodization, a long short-term memory (LSTM) network can also be used. The subjective experimental result showed the effectiveness of the proposed system.",JPN,education,Developed economies,"[48.745724, -0.450742]","[-24.480677, 25.63903]","[18.176529, -16.512156, 13.913683]","[-20.599487, 1.7529354, 0.6367989]","[9.994793, 8.760066]","[6.9719605, 3.4499404]","[12.849636, 11.668628, 0.07031607]","[9.517763, 8.204117, 12.1305]"
59,Qingyang Xi;Rachel Bittner;Johan Pauwels;Xuzhou Ye;Juan Pablo Bello,GuitarSet: A Dataset for Guitar Transcription,2018,https://doi.org/10.5281/zenodo.1492449,"Qingyang Xi+Music and Audio Research Lab, New York University>USA>education;Rachel M. Bittner+Music and Audio Research Lab, New York University>USA>education;Johan Pauwels+Center for Digital Music, Queen Mary University of London>GBR>education;Xuzhou Ye+Music and Audio Research Lab, New York University>USA>education;Juan P. Bello+Music and Audio Research Lab, New York University>USA>education","The guitar is a popular instrument for a variety of reasons, including its ability to produce polyphonic sound and its musical versatility. The resulting variability of sounds, however, poses significant challenges to automated methods for analyzing guitar recordings. As data driven methods become increasingly popular for difficult problems like guitar transcription, sets of labeled audio data are highly valuable resources. In this paper we present GuitarSet, a dataset that provides high quality guitar recordings alongside rich annotations and metadata. In particular, by recording guitars using a hexaphonic pickup, we are able to not only provide recordings of the individual strings but also to largely automate the expensive annotation process. The dataset contains recordings of a variety of musical excerpts played on an acoustic guitar, along with time-aligned annotations of string and fret positions, chords, beats, downbeats, and playing style. We conclude with an analysis of new challenges presented by this data, and see that it is interesting for a wide variety of tasks in addition to guitar transcription, including performance analysis, beat/downbeat tracking, and chord estimation.",USA,education,Developed economies,"[42.644226, -7.781958]","[-41.566883, -5.206892]","[21.342724, -7.770824, 9.129461]","[-14.94281, -8.158706, -3.5535176]","[8.019156, 8.423389]","[7.2436132, 4.1250916]","[11.804924, 11.369179, 1.0246502]","[9.108386, 6.708387, 10.170568]"
0,Roman B. Gebhardt;Michael Stein;Athanasios Lykartsis,A Confidence Measure For Key Labelling,2018,https://doi.org/10.5281/zenodo.1492333,Roman B. Gebhardt+TU Berlin>DEU>education|Native Instruments GmbH>DEU>company;Athanasios Lykartsis+TU Berlin>DEU>education;Michael Stein+Native Instruments GmbH>DEU>company,"We present a new measure for automatically estimating the confidence of musical key classification. Our approach leverages the degree of harmonic information held within a musical audio signal (its ""keyness"") as well as the steadiness of local key detections across the its duration (its ""stability""). Using this confidence measure, musical tracks which are likely to be misclassified, i.e. those with low confidence, can then be handled differently from those analysed by standard, fully automatic key detection methods. By means of a listening test, we demonstrate that our developed features significantly correlate with listeners' ratings of harmonic complexity, steadiness and the uniqueness of key. Furthermore, we demonstrate that tracks which are incorrectly labelled using an existing key detection system obtain low confidence values. Finally, we introduce a new method called ""root note heuristics"" for the special treatment of tracks with low confidence. We show that by applying these root note heuristics, key detection results can be improved for minimalistic music.",DEU,education,Developed economies,"[29.98956, 16.694374]","[-7.4143844, -3.461552]","[-20.22139, -13.381299, 18.61614]","[6.0465198, -6.5954156, -6.056558]","[11.11149, 7.600768]","[7.4119573, 2.6656356]","[12.296758, 13.31457, -0.058535997]","[10.193556, 8.232549, 11.82759]"
61,Julia Wilkins;Prem Seetharaman;Alison Wahl;Bryan Pardo,VocalSet: A Singing Voice Dataset,2018,https://doi.org/10.5281/zenodo.1492453,Julia Wilkins+Northwestern University>USA>education|Ithaca College>USA>education;Prem Seetharaman+Northwestern University>USA>education;Alison Wahl+Ithaca College>USA>education;Bryan Pardo+Northwestern University>USA>education,"We present VocalSet, a singing voice dataset of a capella singing. Existing singing voice datasets either do not capture a large range of vocal techniques, have very few singers, or are single-pitch and devoid of musical context. VocalSet captures not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts. VocalSet has recordings of 10.1 hours of 20 professional singers (11 male, 9 female) performing 17 different different vocal techniques. This data will facilitate the development of new machine learning models for singer identification, vocal technique identification, singing generation and other related applications. To illustrate this, we establish baseline results on vocal technique classification and singer identification by training convolutional network classifiers on VocalSet to perform these tasks.",USA,education,Developed economies,"[-5.0012355, -37.96135]","[-30.647072, -42.406723]","[21.524012, 12.641588, -8.470424]","[3.62542, -9.46053, -20.912285]","[9.875904, 10.973682]","[7.7747884, 4.766539]","[11.303284, 14.997128, 0.89243406]","[10.351633, 7.421793, 8.943288]"
60,Emilia Parada-Cabaleiro;Maximilian Schmitt;Anton Batliner;Bjoern Schuller,Musical-Linguistic Annotations of Il Lauro Secco,2018,https://doi.org/10.5281/zenodo.1492451,Emilia Parada-Cabaleiro+University of Augsburg>DEU>education;Maximilian Schmitt+University of Augsburg>DEU>education;Anton Batliner+University of Augsburg>DEU>education;Björn W. Schuller+University of Augsburg>DEU>education|Imperial College London>GBR>education,"In madrigals, The Italian madrigal, a polyphonic secular a cappella composition of the 16th century, is characterised by a strong musical-linguistic relationship, which has made it an icon of the 'Renaissance humanism'. lyrical meaning is mimicked by the music, through the utilisation of a composition technique known as madrigalism. The synergy between Renaissance music and poetry makes madrigals of great value to musicologists, linguists, and historians—thus, it is a promising repertoire for computational musicology. However, the application of computational techniques for automatic detection of madrigalisms within scores of such repertoire is limited by the lack of annotations to refer to. In this regard, we present 30 madrigals of the anthology Il Lauro Secco encoded in two symbolic formats, MEI and **kern, with hand-encoded annotations of madrigalisms. This work aims to encourage the development of algorithms for madrigalism detection, a composition procedure typical of early music, but still underrepresented in music information retrieval research.",DEU,education,Developed economies,"[3.5376527, 10.145974]","[-16.780872, 29.093042]","[-14.020231, -2.3406754, 1.0955292]","[-17.336082, -17.129786, 3.808797]","[12.97861, 8.701536]","[7.5186324, 0.38679332]","[13.493185, 14.010771, -1.3325517]","[9.4693575, 6.018871, 11.976749]"
88,Christine Bauer;Markus Schedl,Investigating Cross-Country Relationship between Users' Social Ties and Music Mainstreaminess,2018,https://doi.org/10.5281/zenodo.1492507,Christine Bauer+Johannes Kepler University Linz>AUT>education;Markus Schedl+Johannes Kepler University Linz>AUT>education,"We investigate the complex relationship between the factors (i) preference for music mainstream, (ii) social ties in an online music platform, and (iii) demographics. We define (i) on a global and a country level, (ii) by several network centrality measures such as Jaccard index among users' connections, closeness centrality, and betweenness centrality, and (iii) by country and age information. Using the LFM-1b dataset of listening events of Last.fm users, we are able to uncover country-dependent differences in consumption of mainstream music as well as in user behavior with respect to social ties and users' centrality. We could identify that users inclined to mainstream music tend to have stronger connections than the group of less mainstreamy users. Furthermore, our analysis revealed that users typically have less connections within a country than cross-country ones, with the first being stronger social ties, though. Results will help building better user models of listeners and in turn improve personalized music retrieval and recommendation algorithms.",AUT,education,Developed economies,"[-37.081104, 23.47341]","[45.085228, 14.1966]","[-20.369072, 21.14635, -7.101474]","[15.586448, 11.628266, 15.242258]","[15.136385, 9.007907]","[12.807755, 1.9533334]","[15.309273, 15.114932, -1.182802]","[13.5546665, 5.023347, 12.140478]"
89,Kosetsu Tsukuda;Satoru Fukayama;Masataka Goto,Listener Anonymizer: Camouflaging Play Logs to Preserve User's Demographic Anonymity,2018,https://doi.org/10.5281/zenodo.1492509,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"When a user signs up with an online music service, she is often requested to register her demographic attributes such as age, gender, and nationality. Even if she does not input such information, it has been reported that user attributes can be predicted with high accuracy by using her play log. How can users enjoy music when using an online music service while preserving their demographic anonymity? To solve this problem, we propose a system called Listener Anonymizer. Listener Anonymizer monitors the user's play log. When it detects that her confidential attributes can be predicted, it selects songs that can decrease the prediction accuracy and recommends them to her. The user can camouflage her play logs by playing these songs to preserve her demographic anonymity. Since such songs do not always match her music taste, selecting as few songs as possible that can effectively anonymize her attributes is required. Listener Anonymizer realizes this by selecting songs based on feature ablation analysis. Our experimental results using Last.fm play logs showed that Listener Anonymizer was able to preserve anonymity with fewer songs than a method that randomly selected songs.",JPN,facility,Developed economies,"[-43.972057, 34.280827]","[41.18431, 20.32131]","[-10.5090275, 29.85608, 0.6525442]","[12.154818, 8.720067, 15.120257]","[15.462358, 8.495887]","[12.5443945, 1.64861]","[15.572478, 14.766413, -1.4257432]","[13.333345, 4.9841595, 12.506668]"
90,Elad Liebman;Corey N. White;Peter Stone,On the Impact of Music on Decision Making in Cooperative Tasks,2018,https://doi.org/10.5281/zenodo.1492511,Elad Liebman+The University of Texas at Austin>USA>education|The University of Texas at Austin>USA>education;Corey N. White+Missouri Western State University>USA>education;Peter Stone+The University of Texas at Austin>USA>education,"Numerous studies have demonstrated that mood affects emotional and cognitive processing. Previous work has established that music-induced mood can measurably alter people's behavior in different contexts. However, the nature of how decision-making is affected by music in social settings hasn't been sufficiently explored. The goal of this study is to examine which aspects of people's decision making in inter-social tasks are affected when exposed to music. For this purpose, we devised an experiment in which people drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results indicate that music indeed alters people's behavior with respect to this social task. To further understand the correspondence between auditory features and decision making, we have also studied how individual aspects of music affected response patterns.",USA,education,Developed economies,"[-37.105682, 17.550022]","[58.636234, 1.0614206]","[-12.891427, 18.92725, -0.052794635]","[5.7674675, 20.13104, 15.493211]","[14.994562, 8.971871]","[13.233265, 2.9021459]","[15.057845, 15.128199, -1.1647091]","[13.577879, 4.426801, 11.202957]"
91,Emmanouil Krasanakis;Emmanouil Schinas;Symeon Papadopoulos;Yiannis Kompatsiaris;Pericles Mitkas,VenueRank: Identifying Venues that Contribute to Artist Popularity,2018,https://doi.org/10.5281/zenodo.1492513,Emmanouil Krasanakis+CERTH-ITI>GRC>facility;Emmanouil Schinas+CERTH-ITI>GRC>facility|Aristotle University of Thessaloniki>GRC>education;Symeon Papadopoulos+CERTH-ITI>GRC>facility;Yiannis Kompatsiaris+CERTH-ITI>GRC>facility;Pericles Mitkas+Aristotle University of Thessaloniki>GRC>education,"An important problem in the live music industry is finding venues that help expose artists to wider audiences. However, it is often difficult to obtain live music audience data to tackle this task. In this work, we investigate whether important venues can instead be inferred through social media data. Our approach consists of employing bipartite graph ranking algorithms to help discover important venues in artist-venue graphs mined from Facebook. We use both well-established algorithms, such as BiRank, and a modification of their common iterative scheme that avoids the impact of possibly erroneous heuristics to the ranking, which we call VenueRank. Resulting venue ranks are compared to those obtained from feature extraction for predicting the most listened artists and large listener increments in Spotify. This comparison yields high correlation between venue importance for listener prediction and bipartite graph ranking algorithms, with VenueRank found more robust against overfitting.",GRC,facility,Developed economies,"[-40.910095, 12.530073]","[43.93382, 12.313451]","[-28.451618, 7.003112, 4.9383283]","[22.977753, 3.7637262, 12.707246]","[14.7296505, 9.619607]","[12.312553, 2.2603583]","[15.144967, 14.836517, -0.5891675]","[13.38184, 5.435375, 12.203495]"
92,Eva Zangerle;Martin Pichl,The Many Faces of Users: Modeling Musical Preference,2018,https://doi.org/10.5281/zenodo.1492515,Eva Zangerle+Universität Innsbruck>AUT>education;Martin Pichl+Universität Innsbruck>AUT>education,"User models that capture the musical preferences of users are central for many tasks in music information retrieval and music recommendation, yet, it has not been fully explored and exploited. To this end, the musical preferences of users in the context of music recommender systems have mostly been captured in collaborative filtering-based approaches. Alternatively, users can be characterized by their average listening behavior and hence, by the mean values of a set of content descriptors of tracks the users listened to. However, a user may listen to highly different tracks and genres. Thus, computing the average of all tracks does not capture the user's listening behavior well. We argue that each user may have many different preferences that depend on contextual aspects (e.g., listening to classical music when working and hard rock when doing sports) and that user models should account for these different sets of preferences. In this paper, we provide a detailed analysis and evaluation of different user models that describe a user's musical preferences based on acoustic features of tracks the user has listened to.",AUT,education,Developed economies,"[-41.816097, 20.232279]","[38.508656, 16.836311]","[-11.730069, 20.63471, -3.8529844]","[15.924814, 6.395687, 16.606445]","[14.900301, 8.84227]","[12.521392, 1.8657576]","[14.91125, 15.152871, -1.131178]","[13.506683, 5.1921663, 12.645709]"
93,Jiyoung Park;Jongpil Lee;Jangyeon Park;Jung-Woo Ha;Juhan Nam,Representation Learning of Music Using Artist Labels,2018,https://doi.org/10.5281/zenodo.1492517,Jiyoung Park+NAVER Corp.>KOR>company;Jongpil Lee+KAIST>KOR>education;Jangyeon Park+NAVER Corp.>KOR>company;Jung-Woo Ha+NAVER Corp.>KOR>company;Juhan Nam+KAIST>KOR>education,"In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models.",KOR,company,Developing economies,"[-16.31025, -4.8215017]","[-22.685387, -36.487488]","[0.65885824, 10.917275, 8.868452]","[-6.10402, -1.029106, -18.588236]","[11.799733, 9.110586]","[9.763846, 4.6308427]","[13.722116, 13.121513, 0.31172034]","[10.900131, 6.480165, 9.106029]"
94,Gabriele Medeot;Srikanth Cherla;Katerina Kosta;Matt McVicar;Samer Abdallah;Marco Selvi;Ed Newton-Rex;Kevin Webster,StructureNet: Inducing Structure in Generated Melodies,2018,https://doi.org/10.5281/zenodo.1492519,Gabriele Medeot+Jukedeck Ltd.>GBR>company;Srikanth Cherla+Jukedeck Ltd.>GBR>company;Katerina Kosta+Jukedeck Ltd.>GBR>company;Matt McVicar+Jukedeck Ltd.>GBR>company;Samer Abdallah+Jukedeck Ltd.>GBR>company;Marco Selvi+Jukedeck Ltd.>GBR>company;Ed Newton-Rex+Jukedeck Ltd.>GBR>company;Kevin Webster+Imperial College London>GBR>education,"We present the StructureNet - a recurrent neural network for inducing structure in machine-generated compositions. This model resides in a musical structure space and works in tandem with a probabilistic music generation model as a modifying agent. It favourably biases the probabilities of those notes that result in the occurrence of structural elements it has learnt from a dataset. It is extremely flexible in that it is able to work with any such probabilistic model, it works well when training data is limited, and the types of structure it can be made to induce are highly customisable. We demonstrate through our experiments on a subset of the Nottingham dataset that melodies generated by a recurrent neural network based melody model are indeed more structured in the presence of the StructureNet.",GBR,company,Developed economies,"[10.696339, -5.833601]","[-6.7446933, -39.35283]","[10.555196, 6.8570056, 2.6648417]","[-22.286997, 1.9234258, -8.811878]","[10.436604, 9.524412]","[9.237955, 6.059517]","[11.427254, 14.994409, -0.89366835]","[9.702169, 5.42031, 9.072059]"
95,Diego Furtado Silva;Felipe Falcão;Nazareno Andrade,Summarizing and Comparing Music Data and Its Application on Cover Song Identification,2018,https://doi.org/10.5281/zenodo.1492521,Diego Furtado Silva+Universidade Federal de São Carlos>BRA>education;Felipe Vieira Falcão+Universidade Federal de Campina Grande>BRA>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education,"While there is a multitude of music information retrieval algorithms that have distance functions as their core procedure, comparing the similarity between recordings is a costly procedure. At the same, the recent growth of digital music repositories makes necessary the development of novel time- and memory-efficient algorithms to deal with music data. One particularly interesting idea on the literature is transforming the music data into reduced representations, improving the memory usage and reducing the time necessary to assess the similarity. However, these techniques usually add other issues, such as an expensive preprocessing or a reduced retrieval performance. In this paper, we propose a novel method to summarize a recording in small snippets based on its self-similarity information. Besides, we present a simple way to compare other recordings to these summaries. We demonstrate, in the scenario of cover song identification, that our method is more than one order of magnitude faster than state-of-the-art adversaries, at the same time that the retrieval performance is not affected significantly. Additionally, our method is incremental, which allows the easy and fast update of the database when a new song needs to be inserted into the retrieval system.",BRA,education,Developing economies,"[7.325183, 43.61888]","[22.352594, 10.305713]","[3.498315, 13.220589, -22.42381]","[13.739177, -3.045467, 4.831126]","[16.082373, 11.124315]","[10.167018, 2.0924397]","[12.86071, 17.31857, -0.34769106]","[11.593425, 6.4346166, 12.297905]"
96,Wei Tsung Lu;Li Su,Transferring the Style of Homophonic Music Using Recurrent Neural Networks and Autoregressive Model,2018,https://doi.org/10.5281/zenodo.1492523,"Wei-Tsung Lu+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown;Li Su+Institute of Information Science, Academia Sinica>TWN>education","Utilizing deep learning techniques to generate musical contents has caught wide attention in recent years. Within this context, this paper investigates a specific problem related to music generation, music style transfer. This practical problem aims to alter the style of a given music piece from one to another while preserving the essence of that piece, such as melody and chord progression. In particular, we discuss the style transfer of homophonic music, composed of a predominant melody part and an accompaniment part, where the latter is modified through Gibbs sampling on a generative model combining recurrent neural networks and autoregressive models. Both objective and subjective test experiment are performed to assess the performance of transferring the style of an arbitrary music piece having a homophonic texture into two different distinct styles, Bachs chorales and Jazz.",TWN,education,Developing economies,"[22.562904, -2.8388245]","[-5.3794684, -40.410385]","[6.0464606, -0.37646654, 12.549145]","[-23.69534, -0.15254922, -8.08289]","[9.941002, 8.396079]","[9.2631855, 6.2493057]","[12.657735, 12.38806, 0.39181754]","[9.608957, 5.417491, 9.098388]"
97,Gino Brunner;Andres Konrad;Yuyi Wang;Roger Wattenhofer,MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer,2018,https://doi.org/10.5281/zenodo.1492525,Gino Brunner+ETH Zurich>CHE>education;Andres Konrad+ETH Zurich>CHE>education;Yuyi Wang+ETH Zurich>CHE>education;Roger Wattenhofer+ETH Zurich>CHE>education,"We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.",CHE,education,Developed economies,"[37.79907, 0.88055754]","[-8.888322, -41.61214]","[6.8715982, -6.8599024, 23.649033]","[-19.195143, 1.106171, -10.137572]","[10.304074, 7.534852]","[8.97647, 6.3296795]","[12.868756, 11.610067, -0.3879732]","[9.434182, 5.582422, 8.755305]"
98,Saumitra Mishra;Bob L. Sturm;Simon Dixon,Understanding a Deep Machine Listening Model Through Feature Inversion,2018,https://doi.org/10.5281/zenodo.1492527,Saumitra Mishra+Queen Mary University of London>GBR>education;Bob L. Sturm+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Methods for interpreting machine learning models can help one understand their global and/or local behaviours, and thereby improve them. In this work, we apply a global analysis method to a machine listening model, which essentially inverts the features generated in a model back into an interpretable form like a sonogram. We demonstrate this method for a state-of-the-art singing voice detection model. We train up-convolutional neural networks to invert the feature generated at each layer of the model. The results suggest that the deepest fully connected layer of the model does not preserve temporal and harmonic structures, but that the inverted features from the deepest convolutional layer do. Moreover, a qualitative analysis of a large number of inputs suggests that the deepest layer in the model learns a decision function as the information it preserves depends on the class label associated with an input.",GBR,education,Developed economies,"[-11.820861, -11.326543]","[-34.387985, -40.21761]","[10.925405, 8.643668, 12.092048]","[-1.1998234, -11.778483, -20.640818]","[11.614364, 9.316841]","[7.6852617, 5.090956]","[13.352088, 13.198398, 0.63188964]","[9.945871, 7.2393184, 9.007416]"
99,Tian Cheng;Satoru Fukayama;Masataka Goto,Comparing RNN Parameters for Melodic Similarity,2018,https://doi.org/10.5281/zenodo.1492529,Tian Cheng+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Melodic similarity is an important task in the Music Information Retrieval (MIR) domain, with promising applications including query by example, music recommendation and visualisation. Most current approaches compute the similarity between two melodic sequences by comparing their local features (distance between pitches, intervals, etc.) or by comparing the sequences after aligning them. In order to find a better feature representing global characteristics of a melody, we propose to represent the melodic sequence of each musical piece by the parameters of a generative Recurrent Neural Network (RNN) trained on its sequence. Because the trained RNN can generate the identical melodic sequence of each piece, we can expect that the RNN parameters contain the temporal information within the melody. In our experiment, we first train an RNN on all melodic sequences, and then use it as an initialisation to train an individual RNN on each melodic sequence. The similarity between two melodies is computed by using the distance between their individual RNN parameters. Experimental results showed that the proposed RNN-based similarity outperformed the baseline similarity obtained by directly comparing melodic sequences.",JPN,facility,Developed economies,"[-0.021021578, 18.054157]","[-1.6658254, -30.56731]","[0.04351685, 8.723041, 1.8179579]","[-0.59037286, 5.81056, -24.9606]","[12.329339, 9.798048]","[9.044978, 5.1952143]","[12.751668, 15.517835, -0.7884093]","[9.944695, 6.180507, 9.356268]"
100,Mathieu Lagrange;Mathias Rossignol;Grégoire Lafay,Visualization of Audio Data Using Stacked Graphs,2018,https://doi.org/10.5281/zenodo.1492531,"Mathieu Lagrange+LS2N, CNRS, École Centrale de Nantes>FRA>education|LS2N, CNRS>FRA>facility;Mathias Rossignol+LS2N, CNRS, École Centrale de Nantes>FRA>education|LS2N, CNRS>FRA>facility;Grégoire Lafay+LS2N, CNRS, École Centrale de Nantes>FRA>education|LS2N, CNRS>FRA>facility","In this paper, we study the benefit of considering stacked graphs to display audio data. Thanks to a careful use of layering of the spectral information, the resulting display is both concise and intuitive. Compared to the spectrogram display, it allows the reader to focus more on the temporal aspect of the time/frequency decomposition while keeping an abstract view of the spectral information. The use of such a display is validated using two perceptual experiments that demonstrate the potential of the approach. The first considers the proposed display to perform an identification task of the musical instrument and the second considers the proposed display to evaluate the technical level of a musical performer. Both experiments show the potential of the display and potential applications scenarios in musical training are discussed.",FRA,education,Developed economies,"[-13.679428, 33.107998]","[-27.24651, 11.14794]","[-10.995039, 10.683317, -25.996784]","[-9.192177, -7.352028, -0.5732846]","[13.628662, 6.869938]","[7.9358225, 2.6191127]","[13.90599, 13.519659, -2.382688]","[9.649132, 7.272205, 11.3737135]"
101,Klaus Frieler;Frank Höger;Martin Pfleiderer;Simon Dixon,Two Web Applications for Exploring Melodic Patterns in Jazz Solos,2018,https://doi.org/10.5281/zenodo.1492533,Klaus Frieler+University of Music “Franz Liszt” Weimar>DEU>education;Frank Höger+University of Music “Franz Liszt” Weimar>DEU>education;Martin Pfliederer+University of Music “Franz Liszt” Weimar>DEU>education;Simon Dixon+Queen Mary University of London>GBR>education,"This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project ""Dig That Lick"" is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The first one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans.",DEU,education,Developed economies,"[6.96459, 14.234816]","[6.875147, 21.008171]","[-9.630186, -3.127632, 31.136152]","[-12.494371, 10.966488, 8.190705]","[10.943517, 9.680657]","[7.506217, 3.2761972]","[12.117921, 14.785426, -0.71933436]","[9.561365, 6.2243, 11.579572]"
102,Matthias Dorfer;Florian Henkel;Gerhard Widmer,"Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game",2018,https://doi.org/10.5281/zenodo.1492535,"Matthias Dorfer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Florian Henkel+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility","Score following is the process of tracking a musical performance (audio) with respect to a known symbolic representation (a score). We start this paper by formulating score following as a multimodal Markov Decision Process, the mathematical foundation for sequential decision making. Given this formal definition, we address the score following task with state-of-the-art deep reinforcement learning (RL) algorithms such as synchronous advantage actor critic (A2C). In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. All this behavior is learned entirely from scratch, based on a weak and potentially delayed reward signal that indicates to the agent how close it is to the correct position in the score. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images.",AUT,education,Developed economies,"[0.84504527, -10.306928]","[1.9528055, -34.272755]","[21.651503, 25.31479, -3.6120918]","[3.2064607, 8.808664, -28.370817]","[10.802925, 6.550644]","[8.623625, 5.7947726]","[12.218389, 12.484691, -1.5091708]","[9.668669, 6.106453, 9.045621]"
103,Olivier Gouvert;Thomas Oberlin;Cédric Févotte,Matrix Co-Factorization for Cold-Start Recommendation,2018,https://doi.org/10.5281/zenodo.1492537,"Olivier Gouvert+IRIT, Université de Toulouse>FRA>education|CNRS>FRA>facility;Thomas Oberlin+IRIT, Université de Toulouse>FRA>education|CNRS>FRA>facility;Cédric Févotte+IRIT, Université de Toulouse>FRA>education|CNRS>FRA>facility","Song recommendation from listening counts is now a classical problem, addressed by different kinds of collaborative filtering (CF) techniques. Among them, Poisson matrix factorization (PMF) has raised a lot of interest, since it seems well-suited to the implicit data provided by listening counts. Additionally, it has proven to achieve state-ofthe-art performance while being scalable to big data. Yet, CF suffers from a critical issue, usually called cold-start problem: the system cannot recommend new songs, i.e., songs which have never been listened to. To alleviate this, one should complement the listening counts with another modality. This paper proposes a multi-modal extension of PMF applied to listening counts and tag labels extracted from the Million Song Dataset. In our model, every song is represented by the same activation pattern in each modality but with possibly different scales. As such, the method is not prone to the cold-start problem, i.e., it can learn from a single modality when the other one is not informative. Our model is symmetric (it equally uses both modalities) and we evaluate it on two tasks: new songs recommendation and tag labeling.",FRA,education,Developed economies,"[-48.366585, 29.051708]","[39.552082, 13.295075]","[-4.3534894, 22.778444, -12.0193615]","[19.353596, 7.468175, 14.919438]","[15.879367, 9.419056]","[12.599593, 2.1516683]","[15.789242, 15.648755, -1.3276399]","[13.627572, 5.4584055, 12.642496]"
3,Stefan Lattner;Maarten Grachten;Gerhard Widmer,A Predictive Model for Music based on Learned Interval Representations,2018,https://doi.org/10.5281/zenodo.1492331,"Stefan Lattner+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Maarten Grachten+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Gerhard Widmer+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company","Connectionist sequence models (e.g., RNNs) applied to musical sequences suffer from two known problems: First, they have strictly ""absolute pitch perception"". Therefore, they fail to generalize over musical concepts which are commonly perceived in terms of relative distances between pitches (e.g., melodies, scale types, modes, cadences, or chord types). Second, they fall short of capturing the concepts of repetition and musical form. In this paper we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network which learns and operates on interval representations of musical sequences. The relative pitch modeling increases generalization and reduces sparsity in the input data. Furthermore, it can learn sequences of copy-and-shift operations (i.e. chromatically transposed copies of musical fragments)—a promising capability for learning musical repetition structure. We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably. Furthermore, we show that the relative pitch processing of the RGAE naturally facilitates the learning and the generation of sequences of copy-and-shift operations, wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural network on this task.",AUT,education,Developed economies,"[-12.78911, -2.1076531]","[-6.806321, -33.981346]","[-2.6618302, 9.445618, 14.785979]","[-17.063677, 8.508384, -13.319748]","[11.462488, 8.656093]","[8.604922, 5.6532516]","[13.2577095, 13.327598, -0.1282559]","[9.466108, 6.0500255, 9.183679]"
2,Tristan Carsault;Jerome Nika;Philippe Esling,Using Musical Relationships Between Chord Labels in Automatic Chord Extraction Tasks,2018,https://doi.org/10.5281/zenodo.1492329,"Tristan Carsault+Ircam>FRA>facility|CNRS>FRA>facility|Sorbonne Université>FRA>education;Jérôme Nika+Ircam>FRA>facility|CNRS>FRA>facility|Sorbonne Université>FRA>education|L3i Lab, University of La Rochelle>FRA>education;Philippe Esling+Ircam>FRA>facility|CNRS>FRA>facility|Sorbonne Université>FRA>education","Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors.",FRA,facility,Developed economies,"[53.368935, -4.877451]","[-33.48569, 23.36458]","[25.755793, -13.455163, 14.196753]","[-29.162947, -4.0669127, -0.80001557]","[6.8178177, 8.67828]","[5.9642286, 3.8121212]","[11.959099, 10.43588, 2.005744]","[9.727331, 8.994451, 12.3165045]"
1,Filip Korzeniowski;Gerhard Widmer,Improved Chord Recognition by Combining Duration and Harmonic Language Models,2018,https://doi.org/10.5281/zenodo.1492335,Filip Korzeniowski+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility,"Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model—to be applied on chord sequences—and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results.",AUT,education,Developed economies,"[55.411274, -5.3761635]","[-36.234898, 21.115015]","[26.315004, -12.637222, 18.131062]","[-24.183657, -1.8215334, -2.6572971]","[6.670509, 8.665019]","[5.970755, 3.9176695]","[11.885782, 10.338059, 2.131575]","[9.518376, 9.1536045, 12.247245]"
6,Curtis Hawthorne;Erich Elsen;Jialin Song;Adam Roberts;Ian Simon;Colin Raffel;Jesse Engel;Sageev Oore;Douglas Eck,Onsets and Frames: Dual-Objective Piano Transcription,2018,https://doi.org/10.5281/zenodo.1492341,Curtis Hawthorne+Google Brain>USA>company;Erich Elsen+Google Brain>USA>company;Jialin Song+Google Brain>USA>company;Adam Roberts+Google Brain>USA>company;Ian Simon+Google Brain>USA>company;Colin Raffel+Google Brain>USA>company;Jesse Engel+Google Brain>USA>company;Sageev Oore+Google Brain>USA>company;Douglas Eck+Google Brain>USA>company,"We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.",USA,company,Developed economies,"[31.833162, -5.2037783]","[-27.24537, -27.182428]","[15.386677, -5.121875, 17.794847]","[-7.9423, -7.88615, -12.428324]","[9.72201, 7.3029833]","[7.934743, 5.382001]","[12.088267, 11.426675, -0.2322394]","[9.133535, 6.76234, 9.086351]"
86,Stefan Lattner;Maarten Grachten;Gerhard Widmer,Learning Interval Representations from Polyphonic Music Sequences,2018,https://doi.org/10.5281/zenodo.1492503,"Stefan Lattner+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Maarten Grachten+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Gerhard Widmer+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company","Many music theoretical constructs (such as scale types, modes, cadences, and chord types) are defined in terms of pitch intervals—relative distances between pitches. Therefore, when computer models are employed in music tasks, it can be useful to operate on interval representations rather than on the raw musical surface. Moreover, interval representations are transposition-invariant, valuable for tasks like audio alignment, cover song detection and music structure analysis. We employ a gated autoencoder to learn fixed-length, invertible and transposition-invariant interval representations from polyphonic music in the symbolic domain and in audio. An unsupervised training method is proposed yielding an organization of intervals in the representation space which is musically plausible. Based on the representations, a transposition-invariant self-similarity matrix is constructed and used to determine repeated sections in symbolic music and in audio, yielding competitive results in the MIREX task ""Discovery of Repeated Themes and Sections"".",AUT,education,Developed economies,"[-12.79108, -2.162819]","[-6.8595853, -34.063663]","[-2.1854835, 10.144338, 14.728005]","[-16.4847, 8.272627, -13.098275]","[11.404104, 8.715757]","[8.723973, 5.598505]","[13.144145, 13.165035, 0.09629369]","[9.49646, 6.0027776, 9.154965]"
85,Yin-Jyun Luo;Li Su,Learning Domain-Adaptive Latent Representations of Music Signals Using Variational Autoencoders,2018,https://doi.org/10.5281/zenodo.1492501,"Yin-Jyun Luo+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown;Li Su+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown","In this paper, we tackle the problem of domain-adaptive representation learning for music processing. Domain adaptation is an approach aiming to eliminate the distributional discrepancy of the modeling data, so as to transfer learnable knowledge from one domain to another. With its great success in the fields of computer vision and natural language processing, domain adaptation also shows great potential in music processing, for music is essentially a highly-structured semantic system having domaindependent information. Our proposed model contains a Variational Autoencoder (VAE) that encodes the training data into a latent space, and the resulting latent representations along with its model parameters are then reused to regularize the representation learning of the downstream task where the data are in the other domain. The experiments on cross-domain music alignment, namely an audioto-MIDI alignment, and a monophonic-to-polyphonic music alignment of singing voice show that the learned representations lead to better higher alignment accuracy than that using conventional features. Furthermore, a preliminary experiment on singing voice source separation, by regarding the mixture and the voice as two distinct domains, also demonstrates the capability to solve music processing problems from the perspective of domain-adaptive representation learning.",TWN,education,Developing economies,"[-12.938509, -7.1846323]","[-12.186099, -47.11042]","[4.5828733, 9.458652, 16.024406]","[-15.269342, 3.7714097, -16.460812]","[10.899182, 8.752278]","[8.537805, 6.198715]","[13.269171, 12.795009, 0.4528078]","[9.767539, 5.9028034, 8.495981]"
87,Louis Spinelli;Josephine Lau;Liz Pritchard;Jin Ha Lee,Influences on the Social Practices Surrounding Commercial Music Services: A Model for Rich Interactions,2018,https://doi.org/10.5281/zenodo.1492505,Louis Spinelli+University of Washington>USA>education;Josephine Lau+University of Washington>USA>education;Liz Pritchard+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"Music can play an important role in social experiences and interactions. Technologies in-use affect these experiences and interactions and as they continue to evolve, social behaviors and norms surrounding them also evolve. In this paper, we explore the social aspects of commercial music services through focus group observation and interview data. We seek to better understand how existing services are used for social music practices and can be improved. We identified 9 social practices and 24 influences surrounding commercial music services. Based on the user data, we created a model of these practices and influences that provides a lens through which social experiences surrounding commercial music services can be understood. An understanding of these social practices within their contextual ecosystem help inform what influences should be considered when designing new technologies. Our findings include the identification of: the underlying relationships between practices and their influences; practices and influences that inform the weight of relationships in social networks; social norms to be considered when designing social features; influences that add additional insight to previously observed behaviors; and a detailed explanation of how music selection and listening practices can be supported by commercial music services.",USA,education,Developed economies,"[-36.462406, 24.914007]","[41.47376, 32.80659]","[-21.425407, 20.15542, -9.15958]","[7.910911, 16.329184, 19.50685]","[15.207166, 8.608671]","[12.962964, 0.98786515]","[15.27891, 15.064563, -1.5554328]","[13.392109, 4.212907, 11.9434805]"
83,Jordi Pons;Oriol Nieto;Matthew Prockup;Erik M. Schmidt;Andreas F. Ehmann;Xavier Serra,End-to-end Learning for Music Audio Tagging at Scale,2018,https://doi.org/10.5281/zenodo.1492497,"Jordi Pons+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Oriol Nieto+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Matthew Prockup+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Erik Schmidt+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Andreas Ehmann+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company","The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogrambased ones in large-scale data scenarios.",ESP,education,Developed economies,"[-40.03826, -2.3418844]","[-21.445005, -38.26658]","[-11.204986, 14.841203, 8.445638]","[-8.3694, 0.43359375, -20.599838]","[14.5057535, 10.640325]","[9.559871, 5.014041]","[15.684209, 14.017832, 0.18590552]","[10.737199, 6.396704, 8.869603]"
62,Chris Donahue;Huanru Henry Mao;Julian McAuley,The NES Music Database: A multi-instrumental dataset with expressive performance attributes,2018,https://doi.org/10.5281/zenodo.1492455,Chris Donahue+UC San Diego>USA>education|UC San Diego>USA>education|UC San Diego>USA>education;Huanru Henry Mao+UC San Diego>USA>education;Julian McAuley+UC San Diego>USA>education,"Existing research on music generation focuses on composition, but often ignores the expressive performance characteristics required for plausible renditions of resultant pieces. In this paper, we introduce the Nintendo Entertainment System Music Database (NES-MDB), a large corpus allowing for separate examination of the tasks of composition and performance. NES-MDB contains thousands of multi-instrumental songs composed for playback by the compositionally-constrained NES audio synthesizer. For each song, the dataset contains a musical score for four instrument voices as well as expressive attributes for the dynamics and timbre of each voice. Unlike datasets comprised of General MIDI files, NES-MDB includes all of the information needed to render exact acoustic performances of the original compositions. Alongside the dataset, we provide a tool that renders generated compositions as NESstyle audio by emulating the device's audio processor. Additionally, we establish baselines for the tasks of composition, which consists of learning the semantics of composing for the NES synthesizer, and performance, which involves finding a mapping between a composition and realistic expressive attributes.",USA,education,Developed economies,"[-16.826061, 8.438113]","[-13.0345, -30.51146]","[-12.316003, -8.254701, -9.808556]","[-11.802095, -1.1507422, -4.794789]","[12.92944, 7.2952385]","[8.462871, 5.97253]","[13.809466, 13.311859, -1.1974757]","[9.851987, 5.784011, 9.915385]"
63,Vsevolod Eremenko;Emir Demirel;Baris Bozkurt;Xavier Serra,Audio-Aligned Jazz Harmony Dataset for Automatic Chord Transcription and Corpus-based Research,2018,https://doi.org/10.5281/zenodo.1492457,"Vsevolod Eremenko+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emir Demirel+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Baris Bozkurt+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","In this paper we present a new dataset of time-aligned jazz harmony transcriptions. This dataset is a useful resource for content-based analysis, especially for training and evaluating chord transcription algorithms. Most of the available chord transcription datasets only contain annotations for rock and pop, and the characteristics of jazz, such as the extensive use of seventh chords, are not represented. Our dataset consists of annotations of 113 tracks selected from ""The Smithsonian Collection of Classic Jazz"" and ""Jazz: The Smithsonian Anthology,"" covering a range of performers, subgenres, and historical periods. Annotations were made by a jazz musician and contain information about the meter, structure, and chords for entire audio tracks. We also present evaluation results of this dataset using stateof-the-art chord estimation algorithms that support seventh chords. The dataset is valuable for jazz scholars interested in corpus-based research. To demonstrate this, we extract statistics for symbolic data and chroma features from the audio tracks.",ESP,education,Developed economies,"[49.190422, -5.111372]","[-28.185827, 21.614565]","[20.890257, -9.2402525, 16.753502]","[-24.292452, -2.1633348, 4.2439375]","[7.387949, 8.826412]","[6.906646, 3.4357283]","[11.987007, 10.770355, 1.7241322]","[9.774771, 8.380839, 12.35063]"
64,Julie Cumming;Cory McKay;Jonathan Stuchbery;Ichiro Fujinaga,Methodologies for Creating Symbolic Corpora of Western Music Before 1600,2018,https://doi.org/10.5281/zenodo.1492459,Julie E. Cumming+McGill University>CAN>education;Cory McKay+Marianopolis College>CAN>education;Jonathan Stuchbery+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"The creation of a corpus of compositions in symbolic formats is an essential step for any project in systematic research. There are, however, many potential pitfalls, especially in early music, where scores are edited in different ways: variables include clefs, note values, types of barline, and editorial accidentals. Different score editors and optical music recognition software have their own ways of storing and exporting musical data. Choice of software and file formats, and their various parameters, can thus unintentionally bias data, as can decisions on how to interpret potentially ambiguous markings in original sources. This becomes especially problematic when data from different corpora are combined for computational processing, since observed regularities and irregularities may in fact be linked with inconsistent corpus collection methodologies, internal and external, rather than the underlying music.  This paper proposes guidelines, templates, and workflows for the creation of consistent early music corpora, and for detecting encoding biases in existing corpora. We have assembled a corpus of Renaissance duos as a sample implementation, and present machine learning experiments demonstrating how inconsistent or naïve encoding methodologies for corpus collection can distort results.",CAN,education,Developed economies,"[19.906935, 17.351748]","[-7.2067266, 17.58014]","[-6.957638, -12.0282545, 21.39328]","[-7.6572585, 8.501315, 8.199225]","[11.860667, 7.0977874]","[8.586242, 1.4302337]","[13.459593, 12.26518, -1.2384245]","[9.959848, 6.0865593, 12.048965]"
65,Venkata Viraraghavan;Rangarajan Aravind;Hema Murthy,Precision of Sung Notes in Carnatic Music,2018,https://doi.org/10.5281/zenodo.1492461,"Venkata Subramanian Viraraghavan+TCS Research and Innovation>IND>company|Indian Institute of Technology, Madras>IND>education;Hema A Murthy+Indian Institute of Technology, Madras>IND>education;R Aravind+Indian Institute of Technology, Madras>IND>education","Carnatic music is replete with continuous pitch movement called gamakas and can be viewed as consisting of constant-pitch notes (CPNs) and transients. The stationary points (STAs) of transients – points where the pitch curve changes direction – also carry melody information. In this paper, the precision of sung notes in Carnatic music is studied in detail by treating CPNs and STAs separately. There is variation among the nineteen musicians considered, but on average, the precision of CPNs increases exponentially with duration and settles at about 10 cents for CPNs longer than 0.5 seconds. For analyzing STAs, in contrast to Western music, r¯aga (melody) information is found to be necessary, and errors in STAs show a significantly larger standard deviation of about 60 cents. To corroborate these observations, the music was automatically transcribed and re-synthesized using CPN and STA information using two interpolation techniques. The results of perceptual tests clearly indicate that the grammar is highly flexible. We also show that the precision errors are not due to poor pitch tracking, singer deficiencies or delay in auditory feedback.",IND,company,Developing economies,"[7.784881, -0.8586398]","[0.58643985, -17.747356]","[10.172557, 1.8259321, -11.918693]","[11.430464, -14.383026, -8.688291]","[11.115428, 10.243798]","[7.2103224, 1.7497729]","[11.696738, 15.220495, -1.2190996]","[8.9627695, 7.337131, 11.974611]"
66,Kyungyun Lee;Keunwoo Choi;Juhan Nam,Revisiting Singing Voice Detection: A quantitative review and the future outlook,2018,https://doi.org/10.5281/zenodo.1492463,Kyungyun Lee+KAIST>KOR>education;Keunwoo Choi+Spotify Inc.>USA>company;Juhan Nam+KAIST>KOR>education,"Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval. Although several proposed algorithms have shown high performances, we argue that there is still room for improving the singing voice detection system. In order to identify the area of improvement, we first perform an error analysis on three recent singing voice detection systems. Based on the analysis, we design novel methods to test the systems on multiple sets of internally curated and generated data to further examine the pitfalls, which are not clearly revealed with the currently available datasets. From the experiment results, we also propose several directions towards building a more robust singing voice detector.",KOR,education,Developing economies,"[-6.9301248, -35.902813]","[8.779457, -24.291992]","[20.850897, 10.2585335, -13.479649]","[8.604718, -3.1521785, -18.161793]","[9.8944235, 11.086494]","[8.706462, 3.656027]","[11.215384, 15.122232, 0.7662653]","[10.795202, 7.5999093, 9.825327]"
67,Andrew Demetriou;Andreas Jansson;Aparna Kumar;Rachel Bittner,Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners,2018,https://doi.org/10.5281/zenodo.1492465,Andrew Demetriou+Spotify Inc.>USA>company;Andreas Jansson+Spotify Inc.>USA>company;Aparna Kumar+Spotify Inc.>USA>company;Rachel M. Bittner+Spotify Inc.>USA>company,"In music information retrieval, we often make assertions about what features of music are important to study, one of which is vocals. While the importance of vocals in music preference is both intuitive and anticipated by psychological theory, we have not found any survey studies that confirm this commonly held assertion. We address two questions: (1) what components of music are most salient to people's musical taste, and (2) how do vocals rank relative to other components of music, in regards to whether people like or dislike a song. Lastly, we explore the aspects of the voice that listeners find important. Two surveys of Spotify users were conducted. The first gathered open-format responses that were then card-sorted into semantic categories by the team of researchers. The second asked respondents to rank the semantic categories derived from the first survey. Responses indicate that vocals were a salient component in the minds of listeners. Further, vocals ranked high as a self-reported factor for a listener liking or disliking a track, among a statistically significant ranking of musical attributes. In addition, we open several new interesting problem areas that have yet to be explored in MIR.",USA,company,Developed economies,"[-40.125668, 18.118746]","[42.2625, 27.481709]","[-13.778059, 25.483547, -2.5995126]","[7.6884294, 15.225279, 13.875972]","[15.087993, 9.207146]","[12.689818, 1.3412529]","[14.957017, 15.391508, -1.0912348]","[13.239391, 4.566223, 11.516163]"
68,Wei Tsung Lu;Li Su,Vocal Melody Extraction with Semantic Segmentation and Audio-symbolic Domain Transfer Learning,2018,https://doi.org/10.5281/zenodo.1492467,"Wei-Tsung Lu+Institute of Information Science, Academia Sinica>TWN>facility|Unknown>Unknown>Unknown;Li Su+Institute of Information Science, Academia Sinica>TWN>facility|Unknown>Unknown>Unknown","The melody extraction problem is analogue to semantic segmentation on a time-frequency image, in which every pixel on the image is classified as a part of a melody object or not. Such an approach can benefit from a signal processing method that helps to enhance the true pitch contours on an image, and, a music language model with structural information on large-scale symbolic music data to be transfer into an audio-based model. In this paper, we propose a novel melody extraction system, using a deep convolutional neural network (DCNN) with dilated convolution as the semantic segmentation tool. The candidate pitch contours on the time-frequency image are enhanced by combining the spectrogram and cepstral-based features. Moreover, an adaptive progressive neural network is employed to transfer the semantic segmentation model in the symbolic domain to the one in the audio domain. This paper makes an attempt to bridge the semantic gaps between signal-level features and perceived melodies, and between symbolic data and audio data. Experiments show competitive accuracy of the proposed method on various datasets.",TWN,facility,Developing economies,"[3.5661347, -13.501985]","[-28.791616, -33.645496]","[18.705925, 7.21574, -1.3053857]","[-4.837353, -10.521803, -18.492313]","[9.982681, 10.024747]","[7.9143195, 5.1851754]","[11.00494, 14.901261, -0.41617092]","[9.789014, 7.029529, 8.9611435]"
84,Romain Hennequin;Jimena Royo-Letelier;Manuel Moussallam,Audio Based Disambiguation of Music Genre Tags,2018,https://doi.org/10.5281/zenodo.1492499,Romain Hennequin+Deezer R&D>FRA>company;Jimena Royo-Letelier+Deezer R&D>FRA>company;Manuel Moussallam+Deezer R&D>FRA>company,"In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres. We show that such embeddings can be used for disambiguating genre tags (identification of different labels for the same genre, tag translation from a tag system to another, inference of hierarchical taxonomies on these genre tags). These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system. We show empirically that they makes it possible to retrieve the original taxonomy of a tag system, spot duplicates tags and translate tags from a tag system to another.",FRA,company,Developed economies,"[-37.214897, 1.5934896]","[35.665066, -4.482535]","[-15.164477, 10.039613, 7.0681796]","[19.392303, 11.380656, 2.0111089]","[13.878423, 10.087143]","[11.013714, 3.7043347]","[15.023853, 14.117977, -0.08889313]","[12.580945, 6.2905774, 10.892062]"
70,Iris Yuping Ren;Anja Volk;Wouter Swierstra;Remco Veltkamp,Analysis by Classification: A Comparative Study of Annotated and Algorithmically Extracted Patterns in Symbolic Music Data,2018,https://doi.org/10.5281/zenodo.1492471,Iris Yuping Ren+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education;Wouter Swierstra+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"Musical patterns are salient passages that repeatedly appear in music. Such passages are vital for compression, classification and prediction tasks in MIR, and algorithms employing different techniques have been proposed to find musical patterns automatically. Human-annotated patterns have been collected and used to evaluate pattern discovery algorithms, e.g., in the Discovery of Repeated Themes &amp; Sections MIREX task. However, state-of-the-art algorithms are not yet able to reproduce human-annotated patterns. To understand what gives rise to the discrepancy between algorithmically extracted patterns and human-annotated patterns, we use jSymbolic to extract features from patterns, visualise the feature space using PCA and perform a comparative analysis using classification techniques. We show that it is possible to classify algorithmically extracted patterns, human-annotated patterns and randomly sampled passages. This implies: (a) Algorithmically extracted patterns possess different properties than human-annotated patterns (b) Algorithmically extracted patterns have different structures than randomly sampled passages (c) Human-annotated patterns contain more information than randomly sampled passages despite subjectivity involved in the annotation process. We further discover that rhythmic features are of high importance in the classification process, which should influence future research on automatic pattern discovery.",NLD,education,Developed economies,"[13.660568, 15.956602]","[2.5862455, 18.792315]","[-4.5817876, -7.6214743, 12.9097395]","[-3.3684552, -11.894962, 7.9448147]","[11.851309, 7.239527]","[9.261743, 1.3897232]","[13.336524, 12.513838, -0.97124076]","[10.674121, 5.9242554, 11.338654]"
71,Christoph Finkensiep;Markus Neuwirth;Martin Rohrmeier,Generalized Skipgrams for Pattern Discovery in Polyphonic Streams,2018,https://doi.org/10.5281/zenodo.1492473,Christoph Finkensiep+École Polytechnique Fédérale de Lausanne>CHE>education;Markus Neuwirth+École Polytechnique Fédérale de Lausanne>CHE>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"The discovery of patterns using a minimal set of assumptions constitutes a central challenge in the modeling of polyphonic music and complex streams in general. Skipgrams have been found to be a powerful model for capturing semi-local dependencies in sequences of entities when dependencies may not be directly adjacent (see, for instance, the problems of modeling sequences of words or letters in computational linguistics). Since common skipgrams define locality based on indices, they can only be applied to a single stream of non-overlapping entities. This paper proposes a generalized skipgram model that allows arbitrary cost functions (defining locality), efficient filtering, recursive application (skipgrams over skipgrams), and memory efficient streaming. Further, a sampling mechanism is proposed that flexibly controls runtime and output size. These generalizations and optimizations make it possible to employ skipgrams for the discovery of repeated patterns of close, nonsimultaneous events or notes. The extensions to the skipgram model provided here do not only apply to musical notes but to any list of entities that is monotonic with respect to a given cost function.",CHE,education,Developed economies,"[15.834783, 22.621067]","[8.355174, 17.833513]","[-0.94495785, -13.800868, 9.282156]","[8.125301, -12.61099, 4.2294817]","[11.579021, 7.3859253]","[8.81302, 1.0409815]","[12.611892, 13.088138, -0.7114002]","[10.438866, 6.486979, 13.143197]"
69,Michael Barone;Karim Ibrahim;Chitralekha Gupta;Ye Wang,Empirically Weighting the Importance of Decision Factors for Singing Preference,2018,https://doi.org/10.5281/zenodo.1492469,Michael Mustaine+National University of Singapore>SGP>education;Karim M. Ibrahim+National University of Singapore>SGP>education;Chitralekha Gupta+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"Although music cognition and music information retrieval have many common areas of research interest, relatively little work utilizes a combination of signal- and humancentric approaches when assessing complex cognitive phenomena. This work explores the importance of four cognitive decision-making factors (familiarity, genre preference, ease of vocal reproducibility, and overall preference) influence in the perception of ""singability"", how attractive a song is to sing. In Experiment One, we develop a model to validate and empirically determine to what degree these factors are important when evaluating its singability. Results indicate that evaluations of how these four factors impact singability strongly correlate with pairwise evaluations (ρ = 0.692, p &lt; 0.0001), supporting the notion that singability is a measurable cognitive process. Experiment Two examines the degree to which timbral and rhythmic features contribute to singability. Regression and random forest analysis find that some selected features are more significant than others. We discuss the method we use to empirically assess the complex decisions, and provide a preliminary exploration regarding what acoustic features may motivate these choices.",SGP,education,Developing economies,"[-41.203472, 20.426374]","[49.33534, -16.453514]","[-13.407426, 21.428642, -3.1230562]","[4.3790946, 18.218384, 9.473182]","[15.1036005, 9.1205225]","[12.3547125, 4.066862]","[15.027097, 15.307972, -1.1058478]","[13.502103, 4.7936563, 10.606883]"
73,Takumi Takahashi;Satoru Fukayama;Masataka Goto,Instrudive: A Music Visualization System Based on Automatically Recognized Instrumentation,2018,https://doi.org/10.5281/zenodo.1492477,Takumi Takahashi+University of Tsukuba>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"A music visualization system called Instrudive is presented that enables users to interactively browse and listen to musical pieces by focusing on instrumentation. Instrumentation is a key factor in determining musical sound characteristics. For example, a musical piece performed with vocals, electric guitar, electric bass, and drums can generally be associated with pop/rock music but not with classical or electronic. Therefore, visualizing instrumentation can help listeners browse music more efficiently. Instrudive visualizes musical pieces by illustrating instrumentation with multi-colored pie charts and displays them on a map in accordance with the similarity in instrumentation. Users can utilize three functions. First, they can browse musical pieces on a map by referring to the visualized instrumentation. Second, they can interactively edit a playlist that showing the items to be played later. Finally, they can discern the temporal changes in instrumentation and skip to a preferable part of a piece with a multi-colored graph. The instruments are identified using a deep convolutional neural network that has four convolutional layers with different filter shapes. Evaluation of the proposed model against conventional and state-of-the-art methods showed that it has the best performance.",JPN,education,Developed economies,"[-13.345226, 34.13893]","[10.487916, 34.892376]","[-12.440935, 10.363951, -22.796509]","[6.3184023, -8.020306, 24.069962]","[13.470116, 6.900572]","[10.748541, 1.1835011]","[13.884762, 13.427994, -2.1923857]","[11.307927, 5.3018675, 12.4002695]"
82,Simon Waloschek;Aristotelis Hadjakos,Driftin' Down the Scale: Dynamic Time Warping in the Presence of Pitch Drift and Transpositions,2018,https://doi.org/10.5281/zenodo.1492495,Simon Waloschek+Detmold University of Music>DEU>education;Aristotelis Hadjakos+Detmold University of Music>DEU>education,"Recordings of a cappella music often exhibit significant pitch drift. This drift may accumulate over time to a total transposition of several semitones, which renders the canonical 2-dimensional Dynamic Time Warping (DTW) useless. We propose Transposition-Aware Dynamic Time Warping (TA-DTW), an approach that introduces a 3rd dimension to DTW. Steps in this dimension represent changes in transposition. Paired with suitable input features, TA-DTW computes an optimal alignment path between a symbolic score and a corresponding audio recording in the presence of pitch drift or arbitrary transpositions.",DEU,education,Developed economies,"[52.48588, 9.732218]","[-18.191158, -16.074884]","[11.040224, -17.61944, -16.94162]","[2.386566, -22.493364, -6.6607327]","[10.501455, 5.5522494]","[6.078139, 0.6681177]","[11.1258745, 12.938942, -1.5600965]","[8.075274, 5.808725, 10.884309]"
72,Igor Vatolkin;Günter Rudolph,Comparison of Audio Features for Recognition of Western and Ethnic Instruments in Polyphonic Mixtures,2018,https://doi.org/10.5281/zenodo.1492475,Igor Vatolkin+TU Dortmund>DEU>education|TU Dortmund>DEU>education;Günter Rudolph+TU Dortmund>DEU>education|TU Dortmund>DEU>education,"Studies on instrument recognition are almost always restricted to either Western or ethnic music. Only little work has been done to compare both musical worlds. In this paper, we analyse the performance of various audio features for recognition of Western and ethnic instruments in chords. The feature selection is done with the help of a minimum redundancy - maximum relevance strategy and a multi-objective evolutionary algorithm. We compare the features found to be the best for individual categories and propose a novel strategy based on non-dominated sorting to evaluate and select trade-off features which may contribute as best as possible to the recognition of individual and all instruments.",DEU,education,Developed economies,"[6.0570617, -22.642496]","[14.45797, -3.7207592]","[18.725325, -3.6895282, -4.9967194]","[11.038065, 1.9093971, -3.3184109]","[8.865222, 7.381257]","[9.062283, 3.1770449]","[11.155827, 12.717392, 0.5354729]","[10.826143, 7.200931, 10.826848]"
80,Anna Aljanaki;Mohammad Soleymani,A Data-driven Approach to Mid-level Perceptual Musical Feature Modeling,2018,https://doi.org/10.5281/zenodo.1492491,Anna Aljanaki+Johannes Kepler University>AUT>education;Mohammad Soleymani+University of Geneva>CHE>education,"Musical features and descriptors could be coarsely divided into three levels of complexity. The bottom level contains the basic building blocks of music, e.g., chords, beats and timbre. The middle level contains concepts that emerge from combining the basic blocks: tonal and rhythmic stability, harmonic and rhythmic complexity, etc. High-level descriptors (genre, mood, expressive style) are usually modeled using the lower level ones. The features belonging to the middle level can both improve automatic recognition of high-level descriptors, and provide new music retrieval possibilities. Mid-level features are subjective and usually lack clear definitions. However, they are very important for human perception of music, and on some of them people can reach high agreement, even though defining them and therefore, designing a hand-crafted feature extractor for them can be difficult. In this paper, we derive the mid-level descriptors from data. We collect and release a dataset 1 of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation. We then compare several approaches to predicting these descriptors from spectrograms using deep-learning. We also demonstrate the usefulness of these mid-level features using music emotion recognition as an application.",AUT,education,Developed economies,"[-14.122334, 1.3133155]","[12.131865, -5.007235]","[-4.754848, 5.350679, 17.690716]","[7.3690796, 5.1607785, -2.258755]","[11.226297, 8.360607]","[9.014499, 2.8853445]","[13.364175, 13.156881, -0.35568395]","[10.713862, 6.8948836, 11.138527]"
79,Vinod Subramanian;Alexander Lerch,Concert Stitch: Organization and Synchronization of Crowd Sourced Recordings,2018,https://doi.org/10.5281/zenodo.1492489,Vinod Subramanian+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"The number of audience recordings of concerts on the internet has exploded with the advent of smartphones. This paper proposes a method to organize and align these recordings in order to create one or more complete renderings of the concert. The process comprises two steps: first, using audio fingerprints to represent the recordings, identify overlapping segments, and compute an approximate alignment using a modified Dynamic Time Warping (DTW) algorithm and second, applying a cross-correlation around the approximate alignment points in order to improve the accuracy of the alignment. The proposed method is compared to two baseline systems using approaches previously proposed for similar tasks. One baseline cross-correlates the audio fingerprints directly without DTW. The second baseline replaces the audio fingerprints with pitch chroma in the DTW algorithm. A new dataset annotating real-world data obtained from the Live Music Archive is presented and used for evaluation of the three systems.",USA,education,Developed economies,"[-7.4362593, 3.4160624]","[-17.327105, -15.91731]","[-2.2179058, -5.0035973, -7.5611296]","[2.585584, -22.279116, -4.589054]","[11.998862, 7.4943037]","[6.0671797, 0.63649064]","[12.745836, 13.484125, -1.1455115]","[8.001604, 5.8695493, 10.948177]"
81,Jimena Royo-Letelier;Romain Hennequin;Viet-Anh Tran;Manuel Moussallam,Disambiguating Music Artists at Scale with Audio Metric Learning,2018,https://doi.org/10.5281/zenodo.1492493,Jimena Royo-Letelier+Deezer>FRA>company;Romain Hennequin+Deezer>FRA>company;Viet-Anh Tran+Deezer>FRA>company;Manuel Moussallam+Deezer>FRA>company,"We address the problem of disambiguating large scale catalogs through the definition of an unknown artist clustering task. We explore the use of metric learning techniques to learn artist embeddings directly from audio, and using a dedicated homonym artists dataset, we compare our method with a recent approach that learn similar embeddings using artist classifiers. While both systems have the ability to disambiguate unknown artists relying exclusively on audio, we show that our system is more suitable in the case when enough audio data is available for each artist in the train dataset. We also propose a new negative sampling method for metric learning that takes advantage of side information such as music genre during the learning phase and shows promising results for the artist clustering task.",FRA,company,Developed economies,"[-36.22314, 2.3122509]","[34.21444, 3.8628325]","[-4.2060227, 12.312882, 5.2528415]","[13.83681, 7.0023303, 5.8311305]","[13.273408, 9.729999]","[11.353226, 2.9422622]","[14.054185, 14.40322, -0.08656965]","[12.871135, 6.2263927, 11.885605]"
77,Andreas Arzt;Stefan Lattner,Audio-to-Score Alignment using Transposition-invariant Features,2018,https://doi.org/10.5281/zenodo.1492485,"Andreas Arzt+Institute of Computational Perception, Johannes Kepler University>AUT>education;Stefan Lattner+Sony Computer Science Laboratories (CSL)>FRA>company","Audio-to-score alignment is an important pre-processing step for in-depth analysis of classical music. In this paper, we apply novel transposition-invariant audio features to this task. These low-dimensional features represent local pitch intervals and are learned in an unsupervised fashion by a gated autoencoder. Our results show that the proposed features are indeed fully transposition-invariant and enable accurate alignments between transposed scores and performances. Furthermore, they can even outperform widely used features for audio-to-score alignment on 'untransposed data', and thus are a viable and more flexible alternative to well-established features for music alignment and matching.",AUT,education,Developed economies,"[20.185232, -15.44028]","[-20.160425, -17.475042]","[0.30199763, -14.432406, -12.839155]","[-3.7450318, -21.817387, -6.7538424]","[10.864666, 6.031294]","[6.1818, 0.6756942]","[11.722668, 12.501631, -1.768665]","[8.270993, 5.8562956, 10.676997]"
76,Katherine M. Kinnaird,Aligned Sub-Hierarchies: A Structure-based Approach to the Cover Song Task,2018,https://doi.org/10.5281/zenodo.1492483,Katherine M. Kinnaird+Brown University>USA>education,"Extending previous structure-based approaches to the song comparison tasks such as the fingerprint and cover song tasks, this paper introduces the aligned sub-hierarchies (AsH) representation. Built by applying a post-processing technique to the aligned hierarchies of a song, the AsH representation is the set of unique aligned hierarchies for repeats (called AHR) encoded in the original aligned hierarchies of the whole song. Effectively each AHR within AsH is a section of the aligned hierarchies for the original song. Like aligned hierarchies, the AsH representation can be embedded into a classification space with a natural metric that makes inter-song comparisons based on sections of the songs. Experiments addressing a version of the cover song task on score-based data using AsH as the basis of inter-song comparison demonstrate potential of AsH-based approaches for MIR tasks.",USA,education,Developed economies,"[9.14401, 36.756046]","[2.6623478, 21.882854]","[-2.7291725, 7.836501, -24.756893]","[-9.860793, 13.803113, 14.967857]","[11.9191675, 7.9943595]","[9.394939, 1.4003545]","[12.931274, 13.788511, -0.54284215]","[10.653288, 5.705354, 11.081507]"
75,Juan S. Gómez;Jakob Abeßer;Estefanía Cano,"Jazz Solo Instrument Classification with Convolutional Neural Networks, Source Separation, and Transfer Learning",2018,https://doi.org/10.5281/zenodo.1492481,Juan S. Gómez+Fraunhofer IDMT>DEU>facility;Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Estefanía Cano+Fraunhofer IDMT>DEU>facility,"Predominant instrument recognition in ensemble recordings remains a challenging task, particularly if closelyrelated instruments such as alto and tenor saxophone need to be distinguished. In this paper, we build upon a recentlyproposed instrument recognition algorithm based on a hybrid deep neural network: a combination of convolutional and fully connected layers for learning characteristic spectral-temporal patterns. We systematically evaluate harmonic/percussive and solo/accompaniment source separation algorithms as pre-processing steps to reduce the overlap among multiple instruments prior to the instrument recognition step. For the particular use-case of solo instrument recognition in jazz ensemble recordings, we further apply transfer learning techniques to fine-tune a previously trained instrument recognition model for classifying six jazz solo instruments. Our results indicate that both source separation as pre-processing step as well as transfer learning clearly improve recognition performance, especially for smaller subsets of highly similar instruments.",DEU,facility,Developed economies,"[7.8075814, -25.88395]","[-33.178574, -29.43724]","[19.377728, -1.2599514, 1.6483531]","[-11.992152, -8.423215, -19.944332]","[8.749904, 7.7078705]","[7.6175933, 5.2161965]","[11.429074, 12.722115, 0.835676]","[9.419139, 7.33423, 9.065223]"
74,Siddharth Gururani;Cameron Summers;Alexander Lerch,Instrument Activity Detection in Polyphonic Music using Deep Neural Networks,2018,https://doi.org/10.5281/zenodo.1492479,"Siddharth Gururani+Center for Music Technology, Georgia Institute of Technology>USA>education;Cameron Summers+Gracenote>USA>company;Alexander Lerch+Center for Music Technology, Georgia Institute of Technology>USA>education","Although instrument recognition has been thoroughly research, recognition in polyphonic music still faces challenges. While most research in polyphonic instrument recognition focuses on predicting the predominant instruments in a given audio recording, instrument activity detection represents a generalized problem of detecting the presence or activity of instruments in a track on a fine-grained temporal scale. We present an approach for instrument activity detection in polyphonic music with temporal resolution ranging from one second to the track level. This system allows, for instance, to retrieve specific areas of interest such as guitar solos. Three classes of deep neural networks are trained to detect up to 18 instruments. The architectures investigated in this paper are: multi-layer perceptrons, convolutional neural networks, and convolutional-recurrent neural networks. An in-depth evaluation on publicly available multi-track datasets using methods such as AUC-ROC and Label Ranking Average Precision highlights different aspects of the model performance and indicates the importance of using multiple evaluation metrics. Furthermore, we propose a new visualization to discuss instrument confusion in a multi-label scenario.",USA,education,Developed economies,"[11.061669, -21.61358]","[-29.683027, -28.760834]","[16.121029, -2.6157002, 0.89607424]","[-10.975922, -9.825145, -17.409307]","[8.681393, 7.344488]","[8.033879, 4.857264]","[11.136316, 12.529161, 0.5153091]","[9.3703, 7.08265, 9.158516]"
78,Chitralekha Gupta;Rong Tong;Haizhou Li;Ye Wang,Semi-supervised Lyrics and Solo-singing Alignment,2018,https://doi.org/10.5281/zenodo.1492487,Chitralekha Gupta+National University of Singapore>SGP>education;Rong Tong+Alibaba Inc. Singapore R&D Center>SGP>company;Haizhou Li+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"We propose a semi-supervised algorithm to align lyrics to the corresponding singing vocals. The proposed method transcribes and aligns lyrics to solo-singing vocals using the imperfect transcripts from an automatic speech recognition (ASR) system and the published lyrics. The ASR provides time alignment between vocals and hypothesized lyrical content, while the non-aligned published lyrics correct the hypothesized lyrical content. The effectiveness of the proposed method is validated through three experiments. First, a human listening test shows that 73.32% of our automatically aligned sentence-level transcriptions are correct. Second, the automatically aligned sung segments are used for singing acoustic model adaptation, which reduces the word error rate (WER) of automatic transcription of sung lyrics from 72.08% to 37.15% in an open test. Third, another iteration of decoding and model adaptation increases the amount of reliably decoded segments from 44.40% to 91.96% and further reduces the WER to 36.32%. The proposed framework offers an automatic way to generate reliable alignments between lyrics and solosinging. A large-scale solo-singing and lyrics aligned corpus can be derived with the proposed method, which will be beneficial for music and singing voice related research.",SGP,education,Developing economies,"[-28.791473, -34.261658]","[-9.9944725, -21.480354]","[8.686647, 18.25107, -2.1705997]","[2.7847965, -4.786084, -20.551418]","[11.051139, 11.700278]","[7.9112706, 4.502445]","[11.965962, 15.613942, 1.0860329]","[10.416855, 7.350742, 9.14545]"
5,Andrew McLeod;Mark Steedman,Evaluating Automatic Polyphonic Music Transcription,2018,https://doi.org/10.5281/zenodo.1492339,Andrew McLeod+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education;Mark Steedman+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education,"Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a timefrequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce M V 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation— for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H.",GBR,education,Developed economies,"[25.377556, -8.089736]","[-7.206301, -12.947934]","[13.868184, -2.398374, 9.473015]","[3.72872, -12.867374, -9.23434]","[9.65759, 7.6805882]","[6.9513235, 2.4403782]","[11.882163, 12.4110985, -0.11111704]","[9.108211, 7.264413, 11.057207]"
107,Jie Hwan Lee;Hyeong-Seok Choi;Kyogu Lee,Audio Query-based Music Source Separation,2019,https://doi.org/10.5281/zenodo.3527954,Jie Hwan Lee+Seoul National University>KOR>education;Hyeong-Seok Choi+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"In recent years, music source separation has been one of the most intensively studied research areas in music information retrieval. Improvements in deep learning lead to a big progress in music source separation performance. However, most of the previous studies are restricted to separating a few limited number of sources, such as vocals, drums, bass, and other. In this study, we propose a network for audio query-based music source separation that can explicitly encode the source information from a query signal regardless of the number and/or kind of target signals. The proposed method consists of a Query-net and a Separator: given a query and a mixture, the Query-net encodes the query into the latent space, and the Separator estimates masks conditioned by the latent vector, which is then applied to the mixture for separation. The Separator can also generate masks using the latent vector from the training samples, allowing separation in the absence of a query. We evaluate our method on the MUSDB18 dataset, and experimental results show that the proposed method can separate multiple sources with a single network. In addition, through further investigation of the latent space we demonstrate that our method can generate continuous outputs via latent vector interpolation.",KOR,education,Developing economies,"[7.603607, -45.61365]","[-38.68704, -31.443851]","[30.115196, -2.2617137, -3.8832533]","[-10.931644, -8.215192, -26.360054]","[8.367193, 10.026462]","[6.824055, 5.762455]","[11.044826, 13.634669, 1.6365346]","[9.648638, 8.357487, 9.039626]"
108,Daniel MacKinlay;Zdravko Botev,Mosaic Style Transfer Using Sparse Autocorrelograms,2019,https://doi.org/10.5281/zenodo.3527956,Dan MacKinlay+UNSW Sydney>AUS>education;Zdravko Botev+UNSW Sydney>AUS>education,"We introduce a novel mosaic synthesis algorithm for musical style transfer using the autocorrelogram as a feature map. We decompose the autocorrelogram feature map sparsely in a decaying sinusoid basis, using that decomposition as an interpolation scheme in feature space. This efficiently provides gradient information in the mosaicing optimization, including of the challenging time-scale parameters which are usually intractable for discretely sampled signals. The required calculations are straightforward to parallelize on vector-processing hardware. Our implementation of the method provides good quality output and novel musical effects in example tasks by itself and can also be integrated into alternative mosaicing methods.",AUS,education,Developed economies,"[9.890268, -34.890163]","[-22.712637, -52.949112]","[23.205038, 7.3098426, 8.058579]","[-25.278507, -7.1451783, -15.159016]","[9.487061, 8.479782]","[8.342684, 6.3106785]","[12.382595, 12.216646, 0.5902749]","[9.529124, 5.949412, 8.562318]"
109,Juheon Lee;Seohyun Kim;Kyogu Lee,Automatic Choreography Generation with Convolutional Encoder-decoder Network,2019,https://doi.org/10.5281/zenodo.3527958,Juheon Lee+Seoul National University>KOR>education;Seohyun Kim+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"Automatic choreography generation is a challenging task because it often requires an understanding of two abstract concepts - music and dance - which are realized in the two different modalities, namely audio and video, respectively. In this paper, we propose a music-driven choreography generation system using an auto-regressive encoder-decoder network. To this end, we first collect a set of multimedia clips that include both music and corresponding dance motion. We then extract the joint coordinates of the dancer from video and the mel-spectrogram of music from audio and train our network using music-choreography pairs as input. Finally, a novel dance motion is generated at the inference time when only music is given as an input. We performed a user study for a qualitative evaluation of the proposed method, and the results show that the proposed model is able to generate musically meaningful and natural dance movements given an unheard song.",KOR,education,Developing economies,"[26.521194, 1.2235311]","[-13.368525, -41.730717]","[11.252386, 2.5244327, 24.661842]","[-16.050798, 4.3061457, -10.938037]","[10.1869955, 7.9937787]","[8.692814, 6.429637]","[12.892892, 11.835071, 0.0532189]","[9.4047, 5.6308036, 8.597908]"
110,Fu Zih-Sing;Li Su,Hierarchical Classification Networks for Singing Voice Segmentation and Transcription,2019,https://doi.org/10.5281/zenodo.3527960,Zih-Sing Fu+National Taiwan University>TWN>education;Li Su+Academia Sinica>TWN>education,"Identifying the onset and offset time of a musical note is a challenging step for singing voice transcription, as the soft onset/offset, portamento, and vibrato phenomena are rich in singing voice signals. In this work, we investigate how to utilize local data representation with pattern recognition for onset and offset detection of singing voice. We consider onset and offset detection as a hierarchical classification problem, where every local data representation as input is classified into one of all the possible event states in monophonic singing, namely the silence, activation, and transition states, and the transition state is further classified into the onset and offset states. An objective function based on this hierarchical taxonomy nicely guides the model to capture the complicated temporal dynamics of note sequences. Multi-channel data representations containing spectral differences and pitch saliency are employed to reflect the patterns of note transition in singing voice signals. The proposed method implemented with residual networks provides improved performance over prior art in onset and offset detection. Moreover, by integrating with a pitch detection framework, the proposed method also outperforms previous singing voice transcription methods. This result emphasizes the importance of note segmentation in singing voice transcription.",TWN,education,Developing economies,"[-6.483892, -37.992935]","[-22.337656, -8.948771]","[24.079681, 12.168228, -11.326703]","[-3.3613045, 4.9563136, -14.741687]","[9.838816, 10.9939375]","[5.868321, 2.6499083]","[11.230827, 15.010215, 0.8653689]","[8.167587, 7.471002, 10.740011]"
111,Dasaem Jeong;Taegyun Kwon;Yoojin Kim;Kyogu Lee;Juhan Nam,VirtuosoNet: A Hierarchical RNN-based System for Modeling Expressive Piano Performance,2019,https://doi.org/10.5281/zenodo.3527962,Dasaem Jeong+KAIST>KOR>education;Taegyun Kwon+KAIST>KOR>education;Yoojin Kim+KAIST>KOR>education;Kyogu Lee+Seoul National University>KOR>education;Juhan Nam+KAIST>KOR>education,"In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models. We also share the dataset we used for the experiment.",KOR,education,Developing economies,"[32.224957, -2.003097]","[-28.370037, -22.867714]","[13.945396, -1.8394449, 20.311462]","[-9.918363, -12.397487, -10.364089]","[10.003512, 7.4436197]","[8.047459, 5.6778092]","[12.49952, 11.645392, -0.27315828]","[9.01602, 6.252366, 9.04323]"
104,Akira Maezawa;Kazuhiko Yamamoto;Takuya Fujishima,Rendering Music Performance With Interpretation Variations Using Conditional Variational RNN,2019,https://doi.org/10.5281/zenodo.3527948,Akira Maezawa+Yamaha Corporation>JPN>company;Kazuhiko Yamamoto+Yamaha Corporation>JPN>company;Takuya Fujishima+Yamaha Corporation>JPN>company,"Capturing and generating a wide variety of musical expression is important in music performance rendering, but current methods fail to model such a variation.  This paper presents a music performance rendering method that could explicitly model differences in interpretations for a given piece of music.  Conditional variational auto-encoder is used to jointly train, conditioned on the music score, an encoder from performance to a latent code and a decoder from the latent code to music performance.  Evaluation demonstrates the method is capable of generating a wide variety of human-like expressive music performances as the latent code is varied.",JPN,company,Developed economies,"[25.931667, -0.20172264]","[-12.960815, -40.980667]","[8.706988, 4.803109, 18.173168]","[-15.865781, 2.6533673, -11.446109]","[9.970436, 8.303798]","[8.692785, 6.3619123]","[12.906333, 12.086053, 0.2147896]","[9.390522, 5.6610193, 8.597511]"
0,Wenqin Chen;Jessica Keast;Jordan Moody;Corinne Moriarty;Felicia Villalobos;Virtue Winter;Xueqi Zhang;Xuanqi Lyu;Elizabeth Freeman;Jessie Wang;Sherry Cai;Katherine Kinnaird,Data Usage in MIR: History & Future Recommendations,2019,https://doi.org/10.5281/zenodo.3527733,Wenqin Chen+Smith College>USA>education;Jessica Keast+Smith College>USA>education;Jordan Moody+Smith College>USA>education;Corinne Moriarty+Smith College>USA>education;Felicia Villalobos+Smith College>USA>education;Virtue Winter+Smith College>USA>education;Xueqi Zhang+Smith College>USA>education;Xuanqi Lyu+Smith College>USA>education;Elizabeth Freeman+Smith College>USA>education;Jessie Wang+Smith College>USA>educationSherry Cai+Smith College>USA>education;Katherine M. Kinnaird+Smith College>USA>education,"The MIR community faces unique challenges in terms of data access, due in large part to country-specific copyright laws. As a result, there is an emerging divide in the MIR research community between labs that have access to music through large companies with abundant funds, and independent labs at smaller institutions who do not have such expansive access. This paper explores how independent researchers have worked to overcome limitations of access to music data without contributing to the crisis of reproducibility.  Acknowledging that there is no single solution for every data access problem that smaller labs face, we propose a number of possibilities for how the MIR community can bridge the gap between advancements from large companies and those within academia. As MIR looks towards the next 20 years, democratizing and expanding access to MIR research and music data is critical. Future solutions could include a  distributed MIREX system, an API designed for MIR researchers, and community-led advocacy to stakeholders.",USA,education,Developed economies,"[-6.8965507, 56.32182]","[20.942495, 42.01387]","[-34.2507, -2.62605, -7.332456]","[-3.0628467, 3.7476575, 18.269726]","[13.647811, 4.8165135]","[11.404918, 0.33303687]","[15.097488, 11.177613, -1.5235823]","[11.793563, 4.718874, 11.830256]"
76,Hadrien Foroughmand;Geoffroy Peeters,Deep-Rhythm for Global Tempo Estimation in Music,2019,https://doi.org/10.5281/zenodo.3527890,Hadrien Foroughmand+IRCAM Lab - CNRS - Sorbonne Université>FRA>education;Geoffroy Peeters+LTCI - Télécom Paris - Institut Polytechnique de Paris>FRA>education,"It has been shown that the harmonic series at the tempo frequency of the onset-strength-function of an audio signal accurately describes its rhythm pattern and can be used to perform tempo or rhythm pattern estimation. Recently, in the case of multi-pitch estimation, the depth of the input layer of a convolutional network has been used to represent the harmonic series of pitch candidates. We use a similar idea here to represent the harmonic series of tempo candidates. We propose the Harmonic-Constant-Q-Modulation which represents, using a 4D-tensors, the harmonic series of  modulation frequencies (considered as tempo frequencies) in several acoustic frequency bands over time. This representation is used as input to a convolutional network which is trained to estimate tempo or rhythm pattern classes. Using a large number of datasets, we evaluate the performance of our approach and compare it with previous approaches.  We show that it slightly increases Accuracy-1 for tempo estimation but not the average-mean-Recall for rhythm pattern recognition.",FRA,education,Developed economies,"[40.034645, -24.535254]","[-32.616173, -10.046094]","[0.77403414, -26.688599, 2.1562855]","[-7.464166, 9.265627, -15.442192]","[11.496023, 4.390557]","[4.951092, 2.1433487]","[10.873486, 13.247892, -2.917074]","[7.488425, 7.0159936, 10.58263]"
75,Lucas Maia;Magdalena Fuentes;Luiz Biscainho;Martín Rocamora;Slim Essid,SAMBASET: A Dataset of Historical Samba de Enredo Recordings for Computational Music Analysis,2019,https://doi.org/10.5281/zenodo.3527888,"Lucas S. Maia+Federal University of Rio de Janeiro>BRA>education|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Magdalena Fuentes+L2S, CNRS–Université Paris-Sud–CentraleSupélec>FRA>education;Luiz W. P. Biscainho+Federal University of Rio de Janeiro>BRA>education|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Martín Rocamora+Universidad de la República>URY>education;Slim Essid+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education","In the last few years, several datasets have been released to meet the requirements of ""hungry"" yet promising data-driven approaches in modern music technology research. Since, for historical reasons, most investigations conducted in the field still revolve around music of the so-called ""Western"" tradition, the corresponding data, methodology and conclusions carry a strong cultural bias. Music of non-""Western"" background, whenever present, is usually underrepresented, poorly labeled, or even mislabeled, the exception being projects that aim at specifically describing such music. In this paper we present SAMBASET, a dataset of Brazilian samba music that contains over 40 hours of historical and modern samba de enredo commercial recordings. To the best of our knowledge, this is the first dataset of this genre. We describe the collection of metadata (e.g. artist, composer, release date) and outline our semiautomatic approach to the challenging task of annotating beats in this large dataset, which includes the assessment of the performance of state-of-the-art beat tracking algorithms for this specific case. Finally, we present a study on tempo and beat tracking that illustrates SAMBASET's value, and we comment on other tasks for which it could be used.",BRA,education,Developing economies,"[-18.253796, 13.302607]","[-20.009512, 10.629195]","[-12.919725, -4.2602444, -12.58974]","[-3.974329, 12.017879, -1.1303865]","[13.29737, 7.5932884]","[5.8562403, 1.8098586]","[14.0625, 13.730318, -1.4825951]","[8.184344, 6.6791377, 11.458303]"
74,Jonggwon Park;Kyoyun Choi;Sungwook Jeon;Dokyun Kim;Jonghun Park,A Bi-Directional Transformer for Musical Chord Recognition,2019,https://doi.org/10.5281/zenodo.3527886,Jonggwon Park+Seoul National University>KOR>education;Kyoyun Choi+Seoul National University>KOR>education;Sungwook Jeon+Seoul National University>KOR>education;Dokyun Kim+Seoul National University>KOR>education;Jonghun Park+Seoul National University>KOR>education,"Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model.  In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance.",KOR,education,Developing economies,"[53.91472, -8.678803]","[-36.053036, 22.213327]","[27.693947, -8.1259775, 13.633846]","[-26.800234, -1.2917151, -2.830006]","[6.8176947, 8.59267]","[5.8903418, 3.8725631]","[11.999641, 10.361696, 2.0208707]","[9.614612, 9.170704, 12.256325]"
73,Michael Taenzer;Jakob Abeßer;Stylianos I. Mimilakis;Christof Weiss;Meinard Müller,Investigating CNN-based Instrument Family Recognition for Western Classical Music Recordings,2019,https://doi.org/10.5281/zenodo.3527884,Michael Taenzer+Fraunhofer IDMT>DEU>facility;Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Stylianos I. Mimilakis+Fraunhofer IDMT>DEU>facility;Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Hanna Lukashevich+Fraunhofer IDMT>DEU>facility,"Western classical music comprises a rich repertoire composed for different ensembles. Often, these ensembles consist of instruments from one or two of the families woodwinds, brass, piano, vocals, and strings. In this paper, we consider the task of automatically recognizing instrument families from music recordings. As one main contribution, we investigate the influence of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models. We report on experiments using three datasets of monotimbral recordings covering different levels of timbral complexity: isolated notes, isolated melodies, and polyphonic pieces. While data augmentation and the normalization of spectral patches turned out to be beneficial, pre-processing strategies such as logarithmic compression and channel-energy normalization did not lead to substantial improvements. Furthermore, our cross-dataset experiments indicate the necessity of further optimization routines such as domain adaptation.",DEU,facility,Developed economies,"[11.275244, -22.418596]","[-33.39586, -26.790438]","[17.64704, -3.4012063, 1.7366899]","[-14.76095, -8.828982, -15.650678]","[8.75633, 7.203613]","[8.152641, 4.852919]","[11.095902, 12.438011, 0.42725483]","[9.713674, 7.2309713, 9.471652]"
72,Bochen Li;Aparna Kumar,Query by Video: Cross-modal Music Retrieval,2019,https://doi.org/10.5281/zenodo.3527882,Bochen Li+University of Rochester>USA>education;Aparna Kumar+Spotify>USA>company,"Cross-modal retrieval learns the relationship between the two types of data in a common space so that an input from one modality can retrieve data from a different modality. We focus on modeling the relationship between two highly diverse data, music and real-world videos. We learn cross-modal embeddings using a two-stream network trained with music-video pairs. Each branch takes one modality as the input and it is constrained with emotion tags. Then the constraints allow the cross-modal embeddings to be learned with significantly fewer music-video pairs. To retrieve music for an input video, the trained model ranks tracks in the music database by cross-modal distances to the query video. Quantitative evaluations show high accuracy of audio/video emotion tagging when evaluated on each branch independently and high performance for cross-modal music retrieval. We also present cross-modal music retrieval experiments on Spotify music using user-generated videos from Instagram and Youtube as queries, and subjective evaluations show that the proposed model can retrieve relevant music. We present the music retrieval results at: http://www.ece.rochester.edu/~bli23/projects/query.html.",USA,education,Developed economies,"[-9.109129, 23.884224]","[42.094257, -8.245757]","[-5.696784, 13.69242, -15.025079]","[20.08044, 18.965773, 1.4061033]","[13.278836, 8.131623]","[10.221574, 4.6657534]","[13.680373, 14.349798, -1.5213246]","[11.252245, 6.2708282, 9.437841]"
71,Ruihan Yang;Dingsu Wang;Ziyu Wang;Tianyao Chen;Junyan Jiang;Gus Xia,Deep Music Analogy Via Latent Representation Disentanglement,2019,https://doi.org/10.5281/zenodo.3527880,"Ruihan Yang+Music X Lab, NYU Shanghai>CHN>education|Machine Learning Department, Carnegie Mellon University>USA>education;Dingsu Wang+Music X Lab, NYU Shanghai>CHN>education;Ziyu Wang+Music X Lab, NYU Shanghai>CHN>education;Tianyao Chen+Music X Lab, NYU Shanghai>CHN>education;Junyan Jiang+Music X Lab, NYU Shanghai>CHN>education|Machine Learning Department, Carnegie Mellon University>USA>education;Gus Xia+Music X Lab, NYU Shanghai>CHN>education","Analogy-making is a key method for computer algorithms to generate both natural and creative music pieces. In general, an analogy is made by partially transferring the music abstractions, i.e., high-level representations and their relationships, from one piece to another; however, this procedure requires disentangling music representations, which usually takes little effort for musicians but is non-trivial for computers. Three sub-problems arise: extracting latent representations from the observation, disentangling the representations so that each part has a unique semantic interpretation, and mapping the latent representations back to actual music. In this paper, we contribute an explicitly-constrained variational autoencoder (EC^2-VAE) as a unified solution to all three sub-problems. We focus on disentangling the pitch and rhythm representations of 8-beat music clips conditioned on chords. In producing music analogies, this model helps us to realize the imaginary situation of ""what if"" a piece is composed using a different pitch contour, rhythm pattern, or chord progression by borrowing the representations from other pieces. Finally, we validate the proposed disentanglement method using objective measurements and evaluate the analogy examples by a subjective study.",CHN,education,Developing economies,"[-10.129068, -7.62286]","[-10.312858, -46.35019]","[2.5616453, 7.2479053, 20.467169]","[-17.567387, 2.4298584, -16.114435]","[11.241138, 8.93374]","[8.748137, 6.483468]","[13.412974, 12.736869, 0.4739605]","[9.679879, 5.6968203, 8.499557]"
70,Ondřej Cífka;Umut Simsekli;Gael Richard,Supervised Symbolic Music Style Translation Using Synthetic Data,2019,https://doi.org/10.5281/zenodo.3527878,Ondřej Cífka+Télécom Paris>FRA>education|Institut Polytechnique de Paris>FRA>education;Umut Şimşekli+Télécom Paris>FRA>education|Institut Polytechnique de Paris>FRA>education;Gaël Richard+Télécom Paris>FRA>education|Institut Polytechnique de Paris>FRA>education,"Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style. More recently, several attempts have been made to extend such approaches to music (both symbolic and audio) in order to enable transforming musical style in a similar manner. In this study, we focus on symbolic music with the goal of altering the 'style' of a piece while keeping its original 'content'. As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of 'aligned' data (i.e. the same musical piece played in multiple styles), we develop the first fully supervised algorithm for this task. At the core of our approach lies a synthetic data generation scheme which allows us to produce virtually unlimited amounts of aligned data, and hence avoid the above issue. In view of this data generation scheme, we propose an encoder-decoder model for translating symbolic music accompaniments between a number of different styles. Our experiments show that our models, although trained entirely on synthetic data, are capable of producing musically meaningful accompaniments even for real (non-synthetic) MIDI recordings.",FRA,education,Developed economies,"[18.437387, 13.578541]","[-10.12105, -41.705315]","[-2.9878283, -6.6530404, 20.488182]","[-19.611118, 3.2269742, -11.175998]","[11.506074, 7.330855]","[8.882154, 6.572405]","[13.45442, 12.188257, -0.76363975]","[9.520276, 5.487406, 8.679532]"
69,Jiawen Huang;Alexander Lerch,Automatic Assessment of Sight-reading Exercises,2019,https://doi.org/10.5281/zenodo.3527876,Jiawen Huang+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education,"Sight-reading requires a musician to decode, process, and perform a musical score quasi-instantaneously and without rehearsal. Due to the complexity of this task, it is difficult to assess the proficiency of a sight-reading performance, and it is even more challenging to model its human assessment. This study aims at evaluating and identifying effective features for automatic assessment of sight-reading performance. The evaluated set of features comprises task-specific, hand-crafted, and interpretable features designed to represent various aspect of sight-reading performance covering parameters such as intonation, timing, dynamics, and score continuity. The most relevant features are identified by Principal Component Analysis and forward feature selection. For context, the same features are also applied to the assessment of rehearsed student music performances and compared across different assessment categories. The results show potential of automatic assessment models for sight-reading and the relevancy of different features as well as the contribution of different feature groups to different assessment categories.",USA,education,Developed economies,"[18.000244, -6.033201]","[-40.61008, 1.6301786]","[-3.6588545, -28.170088, 16.780605]","[-11.361141, 19.94528, -3.5141609]","[10.556767, 6.650267]","[8.096991, 3.6973531]","[12.193382, 12.237664, -1.2095529]","[9.597955, 6.502654, 10.535789]"
68,Meinard Müller;Frank Zalkow,FMP Notebooks: Educational Material for Teaching and Learning Fundamentals of Music Processing,2019,https://doi.org/10.5281/zenodo.3527872,Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Frank Zalkow+International Audio Laboratories Erlangen>DEU>facility,"In this paper, we introduce a novel collection of educational material for teaching and learning fundamentals of music processing (FMP) with a particular focus on the audio domain. This collection, referred to as FMP notebooks, discusses well-established topics in Music Information Retrieval (MIR) as motivating application scenarios. The FMP notebooks provide detailed textbook-like explanations of central techniques and algorithms in combination with Python code examples that illustrate how to implement the theory. All components including the introductions of MIR scenarios, illustrations, sound examples, technical concepts, mathematical details, and code examples are integrated into a consistent and comprehensive framework based on Jupyter notebooks. The FMP notebooks are suited for studying the theory and practice, for generating educational material for lectures, as well as for providing baseline implementations for many MIR tasks, thus addressing students, teachers, and researchers.",DEU,facility,Developed economies,"[-23.196688, 37.8166]","[14.085661, 28.859396]","[-19.130283, -0.81360507, -0.53484833]","[1.0663621, 3.4764614, 18.209538]","[14.369812, 8.138866]","[10.68458, 0.7145308]","[14.655297, 13.875674, -1.9191399]","[11.342957, 4.8398824, 12.021016]"
67,Oriol Nieto;Matthew McCallum;Matthew Davies;Andrew Robertson;Adam Stark;Eran Egozy,"The Harmonix Set: Beats, Downbeats, and Functional Segment Annotations of Western Popular Music",2019,https://doi.org/10.5281/zenodo.3527870,"Oriol Nieto+Pandora Media, Inc.>USA>company;Matthew McCallum+Pandora Media, Inc.>USA>company;Matthew E. P. Davies+INESC TEC>PRT>education;Andrew Robertson+Ableton AG>DEU>company;Adam Stark+MI·MU>GBR>company;Eran Egozy+MIT>USA>education","We introduce the Harmonix set: a collection of annotations of beats, downbeats, and functional segmentation for over 900 full tracks that covers a wide range of western popular music. Given the variety of annotated music information types in this set, and how strongly these three types of data are typically intertwined, we seek to foster research that focuses on multiple retrieval tasks at once. The dataset in- cludes additional metadata such as MusicBrainz identifiers to support the linking of the dataset to third-party informa- tion or audio data when available. We describe the method- ology employed in acquiring this set, including the annota- tion process and song selection. In addition, an initial data exploration of the annotations and actual dataset content is conducted. Finally, we provide a series of baselines of the Harmonix set with reference beat-trackers, downbeat estimation, and structural segmentation algorithms.",USA,company,Developed economies,"[7.873419, 17.230558]","[-3.4083812, 6.703922]","[-10.531753, -8.16809, 0.6098358]","[-1.2708124, 0.0710072, 1.3779695]","[12.701172, 7.674465]","[8.7102375, 2.7176647]","[13.124606, 13.835762, -1.4606143]","[10.593689, 6.734345, 11.16608]"
77,Junyan Jiang;Ke Chen;Wei Li;Gus Xia,Large-vocabulary Chord Transcription Via Chord Structure Decomposition,2019,https://doi.org/10.5281/zenodo.3527892,Junyan Jiang+Fudan University>CHN>education|NYU Shanghai>CHN>education|Carnegie Mellon University>USA>education;Ke Chen+Fudan University>CHN>education|NYU Shanghai>CHN>education;Wei Li+Fudan University>CHN>education;Gus Xia+NYU Shanghai>CHN>education,"While audio chord recognition systems have acquired considerable accuracy on small vocabularies (e.g., major/minor chords), the large-vocabulary chord recognition problem still remains unsolved. This problem hinders the practical usages of audio recognition systems. The difficulty mainly lies in the intrinsic long-tail distribution of chord qualities, and most chord qualities have too few samples for model training.  In this paper, we propose a new model for audio chord recognition under a huge chord vocabulary. The core concept is to decompose any chord label into a set of musically meaningful components (e.g., triad, bass, seventh), each with a much smaller vocabulary compared to the size of the overall chord vocabulary. A multitask classifier is then trained to recognize all the components given the audio feature, and then labels of individual components are reassembled to form the final chord label. Experiments show that the proposed system not only achieves state-of-the-art results on traditional evaluation metrics but also performs well on a large vocabulary.",CHN,education,Developing economies,"[51.078674, -4.4878144]","[-33.50377, 21.818521]","[22.304617, -12.28928, 16.599388]","[-27.460499, -3.372372, 0.3902737]","[7.013724, 8.730877]","[5.937234, 3.7892234]","[11.977031, 10.61754, 1.8286942]","[9.711682, 9.053515, 12.322896]"
66,Emilia Parada-Cabaleiro;Anton Batliner;Björn Schuller,A Diplomatic Edition of Il Lauro Secco: Ground Truth for OMR of White Mensural Notation,2019,https://doi.org/10.5281/zenodo.3527868,"Emilia Parada-Cabaleiro+University of Augsburg>DEU>education|Instituto Complutense de Ciencias Musicales (ICCMU)>ESP>education;Anton Batliner+University of Augsburg>DEU>education|GLAM – Group on Language, Audio & Music, Imperial College London>GBR>education;Björn Schuller+University of Augsburg>DEU>education|GLAM – Group on Language, Audio & Music, Imperial College London>GBR>education","Early musical sources in white mensural notation—the most common notation in European printed music during the Renaissance—are nowadays preserved by libraries worldwide trough digitalisation. Still, the application of music information retrieval to this repertoire is restricted by the use of digitalisation techniques which produce an uncodified output. Optical Music Recognition (OMR) automatically generates a symbolic representation of image-based musical content, thus making this repertoire reachable from the computational point of view; yet, further improvements are often constricted by the limited ground truth available. We address this lacuna by presenting a symbolic representation in original notation of Il Lauro Secco, an anthology of Italian madrigals in white mensural notation. For musicological analytic purposes, we encoded the repertoire in **mens and MEI formats; for OMR ground truth, we automatically codified the repertoire in agnostic and semantic formats, via conversion from the **mens files.",DEU,education,Developed economies,"[22.443039, 18.69655]","[-17.040287, 29.650799]","[-7.6113853, -16.701784, 21.947119]","[-16.460255, -18.364967, 4.582825]","[11.578972, 6.836129]","[7.28276, -0.0039846348]","[13.294272, 12.1577635, -1.4894116]","[9.354147, 5.8130198, 11.925003]"
64,Anders Elowsson;Anders Friberg,Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN,2019,https://doi.org/10.5281/zenodo.3527864,Anders Elowsson+KTH Royal Institute of Technology>SWE>education;Anders Friberg+KTH Royal Institute of Technology>SWE>education,"This paper presents a convolutional neural network (CNN) that uses input from a polyphonic pitch estimation system to predict perceived minor/major modality in music audio. The pitch activation input is structured to allow the first CNN layer to compute two pitch chromas focused on different octaves. The following layers perform harmony analysis across chroma and time scales. Through max pooling across pitch, the CNN becomes invariant with regards to the key class (i.e., key disregarding mode) of the music. A multilayer perceptron combines the modality activation output with spectral features for the final prediction. The study uses a dataset of 203 excerpts rated by around 20 listeners each, a small challenging data size requiring a carefully designed parameter sharing. With an R2 of about 0.71, the system clearly outperforms previous systems as well as individual human listeners. A final ablation study highlights the importance of using pitch activations processed across longer time scales, and using pooling to facilitate invariance with regards to the key class.",SWE,education,Developed economies,"[12.972661, -15.858873]","[-27.458687, -31.59203]","[9.848979, 3.7838478, 10.107459]","[-7.0962286, -7.998277, -17.209131]","[10.867116, 8.449543]","[8.058177, 5.2768784]","[12.83488, 13.10611, 0.40052524]","[9.561763, 6.9339533, 8.86962]"
63,Sathwik Tejaswi Madhusudhan;Girish Chowdhary,DeepSRGM - Sequence Classification and Ranking in Indian Classical Music Via Deep Learning,2019,https://doi.org/10.5281/zenodo.3527862,Sathwik Tejaswi Madhusudhan+University of Illinois>USA>education|University of Illinois>USA>education;Girish Chowdhary+University of Illinois>USA>education|University of Illinois>USA>education,"A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre-possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on theComp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music database that are closely related to the presented query sequence.",USA,education,Developed economies,"[-14.085416, -13.04352]","[-2.987989, -30.149618]","[5.7473917, 20.749996, 17.451004]","[-0.890162, 6.5953856, -21.867191]","[11.736273, 9.843856]","[8.7788315, 5.0834937]","[13.441651, 13.446515, 0.8504186]","[9.761991, 6.3897934, 9.240646]"
62,Lamtharn Hantrakul;Jesse Engel;Adam Roberts;Chenjie Gu;Lamtharn Hantrakul,Fast and Flexible Neural Audio Synthesis,2019,https://doi.org/10.5281/zenodo.3527860,Lamtharn Hantrakul+Google Brain>USA>company;Jesse Engel+Google Brain>USA>company;Adam Roberts+Google Brain>USA>company;Chenjie Gu+Google DeepMind>USA>company,"Autoregressive neural networks, such as WaveNet, have opened up new avenues for expressive audio synthesis. High-quality speech synthesis utilizes detailed linguistic features for conditioning, but comparable levels of control have yet to be realized for neural synthesis of musical instruments. Here, we demonstrate an autoregressive model capable of synthesizing realistic audio that closely follows fine-scale temporal conditioning for loudness and fundamental frequency. We find the appropriate choice of conditioning features and architectures improves both the quantitative accuracy of audio resynthesis and qualitative responsiveness to creative manipulation of conditioning. While large autoregressive models generate audio much slower than real-time, we achieve these results with a more efficient WaveRNN model, opening the door for exploring real-time interactive audio synthesis with neural networks.",USA,company,Developed economies,"[15.2442, -32.634586]","[-20.463497, -49.026035]","[25.456726, 1.1609411, 8.850103]","[-24.514105, -3.9804857, -20.570902]","[9.53808, 8.532274]","[8.092773, 6.7006025]","[12.498976, 12.171074, 0.629027]","[9.532421, 5.9859796, 7.963882]"
61,Zhengshan Shi;Craig Sapp;Kumaran Arul;Jerry  McBride;Julius Smith,SUPRA: Digitizing the Stanford University Piano Roll Archive,2019,https://doi.org/10.5281/zenodo.3527858,Zhengshan Shi+Stanford University>USA>education;Craig Stuart Sapp+Stanford University>USA>education;Kumaran Arul+Stanford University>USA>education;Jerry McBride+Stanford University>USA>education;Julius O. Smith III+Stanford University>USA>education,"This paper describes the digitization process of a large collection of historical piano roll recordings held in the Stanford University Piano Roll Archive (SUPRA), which has resulted in an initial dataset of 478 performances of pianists from the early twentieth century transcribed to MIDI format. The process includes scanning paper rolls, digitizing the hole punches, and translating the pneumatic expression codings into MIDI format to create expressive performance files. We offer derivative files from each step of this process, including a high resolution image of the roll, a ""raw"" MIDI file of hole data, an ""expressive"" MIDI file that translates hole data into dynamics, and an audio file rendering of the expressive MIDI file on a digital piano sample. This provides digital access to the rolls for researchers in a flexible, searchable online database. We currently offer an initial dataset, ""SUPRA-RW"" from a selection of ""red Welte""-type rolls in the SUPRA. This dataset provides roll scans and MIDI transcriptions of important historical piano performances, many being made available widely for the first time.",USA,education,Developed economies,"[34.82827, 4.699269]","[-26.696224, 33.23101]","[23.719048, 1.7625263, 20.654898]","[-9.997432, -15.960034, -2.0170512]","[10.433364, 6.929868]","[6.737594, 0.26667216]","[12.331018, 11.879883, -1.1303921]","[8.3441305, 5.1429625, 10.688584]"
60,Estefania Cano;Scott Beveridge,Microtiming Analysis in Traditional Shetland Fiddle Music,2019,https://doi.org/10.5281/zenodo.3527856,Estefanía Cano+Fraunhofer IDMT>DEU>facility;Scott Beveridge+Songquito>USA>company,"This work aims to characterize microtiming variations in traditional Shetland fiddle music.  These microtiming variations dictate the rhythmic flow of a performed melody, and contribute, among other things, to the suitability of this music as an accompaniment to dancing. In the context of Shetland fiddle music, these microtiming variations are often referred to as lilt.  Using a corpus of 27 traditional fiddle tunes from the Shetland Isles, we examine inter-beat timing deviations, as well as inter-onset timing deviations of eighth note sequences.  Results show a number of distinct inter-beat and inter-onset rhythmic patterns that may characterize lilt, as well as idiosyncratic patterns for each performer.  This paper presents a first step towards the use of Music Information Retrieval (MIR) techniques for modelling lilt in traditional Scottish fiddle music, and highlights its implications in the field of ethnomusicology.",DEU,facility,Developed economies,"[19.095036, -24.66211]","[-19.138506, 8.951233]","[3.1320074, -18.43162, -0.6779539]","[-5.885343, 13.411257, 0.61291355]","[11.280759, 5.5795994]","[6.3854003, 1.5633909]","[11.568889, 13.18231, -1.9933827]","[8.554181, 6.6100416, 11.9379835]"
59,Shuhei Tsuchida;Satoru Fukayama;Masahiro Hamasaki;Masataka Goto,"AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing",2019,https://doi.org/10.5281/zenodo.3527854,Shuhei Tsuchida+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masahiro Hamasaki+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We describe the AIST Dance Video Database (AIST Dance DB), a shared database containing original street dance videos with copyright-cleared dance music. Although dancing is highly related to dance music and dance information can be considered an important aspect of music information, research on dance information processing has not yet received much attention in the Music Information Retrieval (MIR) community. We therefore developed the AIST Dance DB as the first large-scale shared database focusing on street dances to facilitate research on a variety of tasks related to dancing to music. It consists of 13,939 dance videos covering 10 major dance genres as well as 60 pieces of dance music composed for those genres. The videos were recorded by having 40 professional dancers (25 male and 15 female) dance to those pieces. We carefully designed this database so that it can cover both solo dancing and group dancing as well as both basic choreography moves and advanced moves originally choreographed by each dancer. Moreover, we used multiple cameras surrounding a dancer to simultaneously shoot from various directions. The AIST Dance DB will foster new MIR tasks such as dance-motion genre classification, dancer identification, and dance-technique estimation. We propose a dance-motion genre-classification task and developed four baseline methods of identifying dance genres of videos in this database. We evaluated these methods by extracting dancer body motions and training their classifiers on the basis of long short-term memory (LSTM) recurrent neural network models and support-vector machine (SVM) models.",JPN,facility,Developed economies,"[-26.018372, -23.640894]","[-26.655136, -14.34765]","[-8.8183775, -25.440964, -6.2819724]","[-12.206731, 13.705068, -19.084564]","[11.638724, 5.187662]","[5.3793006, 2.1113808]","[11.527372, 13.735569, -2.3247902]","[7.8994775, 6.597596, 10.568284]"
58,Arthur Flexer;Taric Lallai,Can We Increase Inter- and Intra-Rater Agreement in Modeling General Music Similarity?,2019,https://doi.org/10.5281/zenodo.3527852,Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Taric Lallai+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"We present a pilot study on ways to increase inter- and intra-rater agreement in quantification of general similarity between pieces of music. By using a more controlled group of human subjects and carefully curating song material, we try to increase overall agreement between raters concerning the perceived general similarity of songs. Repeated conduction of the experiment with a two week lag shows that intra-rater agreement is higher than inter-rater agreement. Analysis of the results and interviews with test subjects suggests that the genre of songs was a major factor in judging similarity between songs. We discuss the impacts of our results on evaluation of respective machine learning models and question the validity of experiments on general music similarity.",AUT,facility,Developed economies,"[-3.6305063, 10.089034]","[30.804457, 7.5740805]","[-8.985473, 10.804236, 4.5243387]","[9.79749, 5.6217194, 8.743556]","[13.073325, 9.506564]","[11.180243, 2.3395307]","[13.635171, 15.29311, -0.66481215]","[12.59639, 6.478656, 12.563579]"
57,Sebastian Böck;Matthew Davies;Peter Knees,Multi-Task Learning of Tempo and Beat: Learning One to Improve the Other,2019,https://doi.org/10.5281/zenodo.3527850,Sebastian Böck+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|TU Wien>AUT>education;Matthew E.P. Davies+INESC TEC>PRT>facility;Peter Knees+TU Wien>AUT>education,"In this paper, we propose a multi-task learning approach for simultaneous tempo estimation and beat tracking of musical audio. The system shows state-of-the-art performance for both tasks on a wide range of data, but has another fundamental advantage: due to its multi-task nature, it is not only able to exploit the mutual information of both tasks by learning a common, shared representation, but can also improve one by learning only from the other. The multi-task learning is achieved by globally aggregating the skip connections of a beat tracking system built around temporal convolutional networks, and feeding them into a tempo classification layer. The benefit of this approach is investigated by the inclusion of training data for which tempo-only annotations are available, and which is shown to provide improvements in beat tracking accuracy.",AUT,facility,Developed economies,"[39.018658, -30.154528]","[-31.295807, -11.22883]","[4.422146, -26.633932, 0.29718783]","[-8.002115, 12.275784, -15.494607]","[11.451123, 4.371563]","[4.9611673, 2.3006926]","[10.819709, 13.369107, -2.8530698]","[7.5578837, 6.8804026, 10.370816]"
56,Folgert Karsdorp;Peter Kranenburg;Enrique Manjavacas,Learning Similarity Metrics for Melody Retrieval,2019,https://doi.org/10.5281/zenodo.3527848,Folgert Karsdorp+KNAW Meertens Instituut>NLD>facility;Peter van Kranenburg+KNAW Meertens Instituut>NLD>facility|Utrecht University>NLD>education;Enrique Manjavacas+University of Antwerp>BEL>education,"Similarity measures are indispensable in music information retrieval. In recent years, various proposals have been made for measuring melodic similarity in symbolically encoded scores. Many of these approaches are ultimately based on a dynamic programming approach such as sequence alignment or edit distance, which has various drawbacks. First, the similarity scores are not necessarily metrics and are not directly comparable. Second, the algorithms are mostly first-order and of quadratic time-complexity, and finally, the features and weights need to be defined precisely. We propose an alternative approach which employs deep neural networks for end-to-end similarity metric learning. We contrast and compare different recurrent neural architectures (LSTM and GRU) for representing symbolic melodies as continuous vectors, and demonstrate how duplet and triplet loss functions can be employed to learn compact distributional representations of symbolic music in an induced melody space. This approach is contrasted with an alignment-based approach. We present results for the Meertens Tune Collections, which consists of a large number of vocal and instrumental monophonic pieces from Dutch musical sources, spanning five centuries, and demonstrate the robustness of the learned similarity metrics.",NLD,facility,Developed economies,"[-2.777381, 17.290243]","[-1.7676839, -30.407393]","[-1.8658594, 10.288264, -0.17203559]","[-0.7335525, 6.1934905, -24.48349]","[12.859221, 9.407592]","[8.907936, 5.144034]","[13.257173, 15.23588, -0.83785504]","[9.915585, 6.383901, 9.21229]"
55,Adrien Ycart;Daniel Stoller;Emmanouil Benetos,A Comparative Study of Neural Models for Polyphonic Music Sequence Transduction,2019,https://doi.org/10.5281/zenodo.3527846,"Adrien Ycart+Centre for Digital Music, Queen Mary University of London>GBR>education;Daniel Stoller+Centre for Digital Music, Queen Mary University of London>GBR>education;Emmanouil Benetos+Centre for Digital Music, Queen Mary University of London>GBR>education","Automatic transcription of polyphonic music remains a challenging task in the field of Music Information Retrieval. One under-investigated point is the post-processing of time-pitch posteriograms into binary piano rolls. In this study, we investigate this task using a variety of neural network models and training procedures.  We introduce an adversarial framework, that we compare against more traditional training losses. We also propose the use of binary neuron outputs and compare them to the usual real-valued outputs in both training frameworks. This allows us to train networks directly using the F-measure as training objective. We evaluate these methods using two kinds of transduction networks and two different multi-pitch detection systems, and compare the results against baseline note-tracking methods on a dataset of classical piano music. Analysis of results indicates that (1) convolutional models improve results over baseline models, but no improvement is reported for recurrent models; (2) supervised losses are superior to adversarial ones; (3) binary neurons do not improve results; (4) cross-entropy loss results in better or equal performance compared to the F-measure loss.",GBR,education,Developed economies,"[23.155424, -3.124726]","[-26.468714, -28.16556]","[7.689965, -1.276606, 13.432384]","[-8.748284, -6.7864575, -13.700761]","[9.841877, 8.222458]","[7.924494, 5.3530383]","[12.462899, 12.223256, 0.24175584]","[9.274717, 6.7790284, 8.9382305]"
65,Harsh Verma;John Thickstun,Convolutional Composer Classification,2019,https://doi.org/10.5281/zenodo.3527866,Harsh Verma+University of Washington>USA>education;John Thickstun+University of Washington>USA>education,"This paper investigates end-to-end learnable models for attributing composers to musical scores. We introduce several pooled, convolutional architectures for this task and draw connections between our approach and classical learning approaches based on global and n-gram features. We evaluate models on a corpus of 2,500 scores from the KernScores collection, authored by a variety of composers spanning the Renaissance era to the early 20th century. This corpus has substantial overlap with the corpora used in several previous, smaller studies; we compare our results on subsets of the corpus to these previous works.",USA,education,Developed economies,"[-5.282473, -15.688424]","[-8.084611, 13.422069]","[17.007961, 7.1386657, 19.343079]","[-10.281856, 5.518754, 5.3062563]","[11.147482, 9.232817]","[9.3060875, 4.134805]","[12.866161, 13.112312, 0.82251406]","[10.219432, 6.3315997, 10.251585]"
78,Yichao Zhou;Wei Chu;Sam Young;Xin Chen,"BandNet: A Neural Network-based, Multi-Instrument Beatles-Style MIDI Music Composition Machine",2019,https://doi.org/10.5281/zenodo.3527894,Yichao Zhou+Snap Inc.>USA>company;Wei Chu+Snap Inc.>USA>company;Sam Young+Snap Inc.>USA>company;Xin Chen+Snap Inc.>USA>company,"In this paper, we propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' music and generate full songs in the style of the Beatles with little human intervention. In the learning stage, a sequence of stylistically uniform, multiple-channel music samples was modeled by an RNN. In the composition stage, a short clip of randomly-generated music was used as a seed for the RNN to start music score prediction. To form structured music, segments of generated music from different seeds were concatenated together. To improve the quality and structure of the generated music, we integrated music theory knowledge into the model, such as controlling the spacing of gaps in the vocal melody, normalizing the timing of chord changes, and requiring notes to be related to the song's key (C major, for example). This integration improved the quality of the generated music as verified by a professional composer. We also conducted a subjective listening test that showed our generated music was close to original music by the Beatles in terms of style similarity, professional quality, and interestingness.",USA,company,Developed economies,"[39.45086, -6.031339]","[-6.721346, -39.3782]","[7.9803095, -5.0624456, 17.54472]","[-22.145552, 1.8808612, -7.797422]","[9.74621, 8.210914]","[9.25011, 6.064505]","[12.565268, 11.930262, 0.3416798]","[9.72248, 5.4076357, 9.173221]"
79,Jin Ha Lee;Liz Pritchard;Chris Hubbles,Can We Listen To It Together?: Factors Influencing Reception of Music Recommendations and Post-Recommendation Behavior,2019,https://doi.org/10.5281/zenodo.3527896,Jin Ha Lee+University of Washington>USA>education;Liz Pritchard+University of Washington>USA>education;Chris Hubbles+University of Washington>USA>education,"Few prior studies on music recommendations investigate the context in which users receive the recommendations, and what impact the recommendation has on the user. In this paper, we aim to better understand the factors that affect people's decisions as to whether they choose to listen to music recommendations and how the recommendations impact their music-listening behaviors. We conducted an online survey asking about people's past experiences on giving and receiving music recommendations. We found that in addition to the aesthetic qualities of music and the respondent's taste, expectations regarding the delivery (e.g., timing, persistence) of the recommendations, familiarity, trust in the recommender's abilities, and the rationale for suggestions were important factors. We discuss the implications for the design of music recommenders based on the findings, including better rationale for and accessibility of recommended music, improved saving options, and more targeted delivery at specific times. The data also suggests disparities in how people wish to receive music recommendations and what will influence them to listen to recommendations, versus how they would like to offer recommendations to others. In addition, the findings highlight the importance of music recommendations in people's existing social relationships and their role in building/improving new relationships.",USA,education,Developed economies,"[-41.750744, 23.455482]","[41.90618, 29.773071]","[-15.053364, 22.629934, -6.9401245]","[10.818673, 13.721381, 18.44004]","[15.499367, 9.081469]","[12.959023, 1.2231371]","[15.40196, 15.385391, -1.3474625]","[13.511735, 4.420282, 12.16403]"
80,Jong Wook Kim;Juan Bello,Adversarial Learning for Improved Onsets and Frames Music Transcription,2019,https://doi.org/10.5281/zenodo.3527898,Jong Wook Kim+New York University>USA>education;Juan Pablo Bello+New York University>USA>education,"Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements in transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the time-frequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both frame-level and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis.",USA,education,Developed economies,"[26.496702, -4.7107453]","[-25.463852, -28.817663]","[9.746474, -4.531219, 13.764694]","[-8.732333, -5.0480113, -13.786709]","[9.6781025, 7.993033]","[8.526776, 5.0975657]","[12.143739, 11.841807, 0.2115984]","[9.597936, 6.693717, 8.863572]"
103,I-Chieh Wei;Chih-Wei Wu;Li Su,Generating Structured Drum Pattern Using Variational Autoencoder and Self-similarity Matrix,2019,https://doi.org/10.5281/zenodo.3527946,"I-Chieh Wei+Institute of Information Science, Academia Sinica>TWN>education|Netflix, Inc.>USA>company;Chih-Wei Wu+Netflix, Inc.>USA>company;Li Su+Institute of Information Science, Academia Sinica>TWN>education","Drum pattern generation is a task that focuses on the rhythmic aspect of music and aims at generating percussive sequences. With the advancement of machine learning techniques, several models have been proven useful in producing compelling results. However, one of the main challenges is to generate structurally cohesive sequences. In this study, a drum pattern generation model based on Variational Autoencoders (VAEs) is presented; Specifically, the proposed model is built to generate symbolic drum patterns given an accompaniment that consists of melodic sequences. A self-similarity matrix (SSM) is incorporated in the process for encapsulating structural information. Both the objective evaluation and the subjective listening test highlight the model's capability of creating musically meaningful transitions on structural boundaries.",TWN,education,Developing economies,"[26.35836, -47.86159]","[-6.347282, -43.968143]","[22.393047, -21.728283, 0.042335186]","[-20.662745, 6.9480014, -9.41429]","[7.741329, 7.011152]","[9.055382, 6.3359227]","[10.225704, 11.633083, 0.9826132]","[9.399714, 5.5503526, 8.870369]"
102,Elena Epure;Anis Khlif;Romain Hennequin,Leveraging knowledge bases and parallel annotations for music genre translation,2019,https://doi.org/10.5281/zenodo.3527944,Elena V. Epure+Deezer R&D>FRA>company;Anis Khlif+Deezer R&D>FRA>company;Romain Hennequin+Deezer R&D>FRA>company,"Prevalent efforts have been put in automatically inferring genres of musical items. Yet, the propose solutions often rely on simplifications and fail to address the diversity and subjectivity of music genres. Accounting for these has, though, many benefits for aligning knowledge sources, integrating data and enriching musical items with tags. Here, we choose a new angle for the genre study by seeking to predict what would be the genres of musical items in a target tag system, knowing the genres assigned to them within source tag systems. We call this a translation task and identify three cases: 1) no common annotated corpus between source and target tag systems exists, 2) such a large corpus exists, 3) only few common annotations exist. We propose the related solutions: a knowledge-based translation modeled as taxonomy mapping, a statistical translation modeled with maximum likelihood logistic regression; a hybrid translation modeled with maximum a posteriori logistic regression with priors given by the knowledge-based translation. During evaluation, the solutions fit well the identified cases and the hybrid translation is systematically the most effective w.r.t. multilabel  classification metrics. This is a first attempt to unify genre tag systems by leveraging both representation and interpretation diversity.",FRA,company,Developed economies,"[-32.729546, 7.564369]","[36.354824, -4.5557866]","[-16.843166, 6.919655, 5.6115375]","[21.203117, 11.128605, 1.4311069]","[13.902747, 9.602506]","[11.144499, 3.6115797]","[14.806029, 13.86977, -0.5153341]","[12.792237, 6.344047, 11.075097]"
101,Flavio Figueiredo;Nazareno Andrade,Quantifying Disruptive Influence in the AllMusic Guide,2019,https://doi.org/10.5281/zenodo.3527940,Flavio Figueiredo+Universidade Federal de Minas Gerais>BRA>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education,"Understanding how influences shape musical creation provides rich insight into cultural trends. As such, there have been several efforts to create quantitative complex network methods that support the analysis of influence networks among artists in a music corpus. We contribute to this body of work by examining how disruption happens in a corpus about music influence from the All Music Guide. A disruptive artist is one that creates a new stream of influences; this artist builds on prior efforts, but influences subsequent artists that do not build on the same prior efforts. We leverage methods devised to study disruption in Science and Technology, and apply them to the context of music creation. Our results point that such methods identify innovative artists, and that disruption is often uncorrelated with network centrality.",BRA,education,Developing economies,"[-39.44526, 14.626721]","[46.254787, 9.049992]","[-30.297457, 9.580847, 1.7889227]","[13.150104, 11.796547, 10.055837]","[14.61182, 9.30918]","[12.217253, 2.7426856]","[14.934273, 14.791707, -0.83962554]","[13.223861, 5.5282044, 11.735871]"
100,Jordan Smith;Yuta Kawasaki;Masataka Goto,Unmixer: An Interface for Extracting and Remixing Loops,2019,https://doi.org/10.5281/zenodo.3527938,Jordan B. L. Smith+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Yuta Kawasaki+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"To create their art, remix artists would like to have segmented stem tracks at their disposal; that is, isolated instances of the loops and sounds that the original composer used to create a track. We present Unmixer, a web service that will analyze and extract loops from any audio uploaded by a user. The loops are presented in an interface that allows users to immediately remix the loops; if users upload multiple tracks, they can easily create mash-ups with the loops, which are automatically matched in tempo. To analyze the audio, we use a recently-proposed method of source separation based on the nonnegative Tucker decomposition of the spectrum. To reduce interference among the extracted loops, we propose an extra factorization step with a sparseness constraint and demonstrate that it improves the source separation result. We also propose a method for selecting the best instances of the extracted loops and demonstrate its effectiveness in an evaluation. Both of these improvements are incorporated into the backend of the interface. Finally, we discuss the feedback collected in a set of user evaluations.",JPN,facility,Developed economies,"[26.62647, 8.05543]","[-44.610332, -28.277033]","[5.9090986, -3.110927, 25.486706]","[-10.658712, -10.67726, -29.213543]","[10.371542, 8.01432]","[6.3960037, 5.347347]","[13.289884, 11.825033, -0.21406399]","[9.718084, 8.567994, 9.398334]"
99,Taketo Akama,Controlling Symbolic Music Generation based on Concept Learning from Domain Knowledge,2019,https://doi.org/10.5281/zenodo.3527936,Taketo Akama+Sony Computer Science Laboratories>JPN>company,"Machine learning allows automatic construction of generative models for music. However, they are learned from only the succession of notes itself without explicitly employing domain knowledge of musical concepts such as rhythm, contour, and fragmentation &amp; consolidation. We approximate such musical domain knowledge as a function, and feed it into our model. Then, two decoupled spaces are learned: the extraction space that captures the target concept, and the residual space that captures the remainder. For monophonic symbolic music, our model exhibits high decoupling/modeling performance.  Controllability in generation is improved: (i) our interpolation enables concept-aware flexible control over blending two musical fragments, and (ii) our variation generation enables users to make concept-aware adjustable variations.",JPN,company,Developed economies,"[18.946411, 11.876945]","[-9.069343, -41.8753]","[-2.8950515, -4.1739907, 22.67788]","[-19.783733, 0.87054384, -11.399261]","[10.63346, 8.000731]","[9.102321, 6.4115014]","[13.4278145, 11.992424, -0.43857327]","[9.605158, 5.425411, 8.732789]"
98,Changhong Wang;Emmanouil Benetos;Vincent Lostanlen;Elaine Chew,Adaptive Time-Frequency Scattering for Periodic Modulation Recognition in Music Signals,2019,https://doi.org/10.5281/zenodo.3527934,"Changhong Wang+Centre for Digital Music, Queen Mary University of London>GBR>education;Emmanouil Benetos+Centre for Digital Music, Queen Mary University of London>GBR>education;Vincent Lostanlen+Music and Audio Research Laboratory, New York University>USA>education;Elaine Chew+CNRS-UMR9912/STMS IRCAM>FRA>facility","Vibratos, tremolos, trills, and flutter-tongue are techniques frequently found in vocal and instrumental music. A common feature of these techniques is the periodic modulation in the time--frequency domain. We propose a representation based on time--frequency scattering to model the inter-class variability for fine discrimination of these periodic modulations. Time--frequency scattering is an instance of the scattering transform, an approach for building invariant, stable, and informative signal representations. The proposed representation is calculated around the wavelet subband of maximal acoustic energy, rather than over all the wavelet bands. To demonstrate the feasibility of this approach, we build a system that computes the representation as input to a machine learning classifier. Whereas previously published datasets for playing technique analysis focus primarily on techniques recorded in isolation, for ecological validity, we create a new dataset to evaluate the system. The dataset, named CBF-periDB, contains full-length expert performances on the Chinese bamboo flute that have been thoroughly annotated by the players themselves. We report F-measures of 99% for flutter-tongue, 82% for trill, 69% for vibrato, and 51% for tremolo detection, and provide explanatory visualisations of scattering coefficients for each of these techniques.",GBR,education,Developed economies,"[34.445328, -19.694284]","[-11.05324, -1.5946953]","[22.1476, -9.616168, -11.969453]","[8.139797, 3.5843766, -11.020365]","[9.175746, 9.113766]","[8.922627, 3.600589]","[11.117838, 13.584944, -0.023612363]","[10.445376, 7.4134245, 10.363068]"
97,Philippe Rigaux;Nicolas Travers,Scalable Searching and Ranking  for Melodic Pattern Queries,2019,https://doi.org/10.5281/zenodo.3527932,"Philippe Rigaux+Cedric Lab, CNAM>FRA>education;Nicolas Travers+Leonard de Vinci, Research Center>FRA>facility","We present the design and implementation of a scalable search engine for large Digital Score Libraries. It covers the core features expected from an information retrieval system. Music representation is pre-processed, simplified and normalized. Collections are searched for scores that match a melodic pattern, results are ranked on their similarity with the pattern, and matching fragments are finally identified on the fly. Moreover, all these features are designed to be integrated in a standard search engine and thus benefit from the horizontal scalability of such systems. Our method is fully implemented, and relies on Elasticsearch for collection indexing. We describe its main components, report and study its performances.",FRA,education,Developed economies,"[2.9564984, 20.572832]","[18.441044, 18.965696]","[2.9529138, 6.7790956, -6.4866014]","[7.8148026, -6.4437027, 14.684868]","[11.724541, 9.864413]","[10.098978, 0.7418196]","[12.369913, 15.553658, -0.98999345]","[11.327507, 5.5916553, 12.872745]"
96,Cheng-Zhi Anna Huang;Curtis Hawthorne;Adam Roberts;Monica  Dinculescu;James Wexler;Leon Hong;Jacob Howcroft,Approachable Music Composition with Machine Learning at Scale,2019,https://doi.org/10.5281/zenodo.3527930,Cheng-Zhi Anna Huang+Google Brain Magenta>USA>company|Google Brain Pair>USA>company|Google Doodle>USA>company;Curtis Hawthorne+Google Brain Magenta>USA>company|Google Brain Pair>USA>company|Google Doodle>USA>company;Adam Roberts+Google Brain Magenta>USA>company|Google Brain Pair>USA>company|Google Doodle>USA>company;Monica Dinculescu+Google Brain Magenta>USA>company|Google Brain Pair>USA>company|Google Doodle>USA>company;James Wexler+Google Brain Magenta>USA>company|Google Brain Pair>USA>company|Google Doodle>USA>company;Leon Hong+Google Brain Magenta>USA>company|Google Brain Pair>USA>company|Google Doodle>USA>company;Jacob Howcroft+Google Brain Magenta>USA>company|Google Brain Pair>USA>company|Google Doodle>USA>company,"To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach.  For users to input melodies, we designed a simplified sheet-music based interface.  To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations.  We also reduced the model download size to approximately 400KB through post-training weight quantization.  We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers.  In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries.  Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper.  We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.",USA,company,Developed economies,"[15.367089, -2.0785522]","[-18.205584, -40.841732]","[6.7118187, 10.94378, 7.232326]","[-12.060739, 5.1979723, -24.29951]","[10.804101, 8.702959]","[9.282931, 5.6107564]","[13.033849, 12.524008, 0.22581491]","[10.331991, 5.757811, 9.024906]"
95,Li Li;Tomoki Toda;Kazuho Morikawa;Kazuhiro Kobayashi;Shoji Makino,Improving Singing Aid System for Laryngectomees With Statistical Voice Conversion and VAE-SPACE,2019,https://doi.org/10.5281/zenodo.3527928,Li Li+University of Tsukuba>JPN>education|Nagoya University>JPN>education;Tomoki Toda+Nagoya University>JPN>education;Kazuho Morikawa+Nagoya University>JPN>education;Kazuhiro Kobayashi+Nagoya University>JPN>education;Shoji Makino+University of Tsukuba>JPN>education,"This paper proposes an improved singing aid system for laryngectomees that converts electrolaryngeal (EL) speech produced using an electrolarynx to a more naturally sounding singing voice. Although the previously proposed system employing a noise suppression process and a rule-based pitch control approach has achieved preliminary success in converting EL speech into a singing voice, there are still two major limitations. First, the converted singing voice still sounds mechanical and unnatural owing to the adverse impacts of spectrograms extracted from EL speeches, also making the effect of pitch control limited. Second, the capability and flexibility of the rule-based pitch control in modeling various singing styles are insufficient, causing the converted singing voices to lack variety. To address these limitations, this paper proposes an improved system that uses 1) a statistical voice conversion approach to convert spectrograms extracted from EL speeches into those of natural speeches and 2) a deep generative model-based approach called VAE-SPACE for pitch modification, which generates pitch patterns in a data-driven manner instead of following manually designed rules. The experimental results revealed that 1) the conversion of spectrograms was effective in improving the naturalness of singing voices, and 2) the statistical pitch control approach was able to achieve comparable results with the rule-based approach, which was very carefully designed to be specialized in singing.",JPN,education,Developed economies,"[-3.0506701, -36.671486]","[-32.05259, -45.05413]","[22.774605, 6.589017, -11.848666]","[5.800342, -12.175164, -19.888607]","[9.593038, 10.921121]","[7.601054, 4.630687]","[11.110001, 14.959974, 0.92100435]","[10.215043, 7.5217476, 8.941387]"
94,George Sioros;Guilherme Câmara;Anne Danielsen,Mapping Timing Strategies in Drum Performance,2019,https://doi.org/10.5281/zenodo.3527926,"George Sioros+RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion>NOR>facility;Guilherme Schmidt Câmara+RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion>NOR>facility;Anne Danielsen+University of Oslo>NOR>education","How do drummers express different timing styles? We conducted an experiment in which we asked twenty-two professional drummers to perform a simple rhythmic pattern while listening to a metronome. Here, we investigate the strategies they employed to express three different instructed timing profiles for the same pattern: ""on"", ""pushed"" and ""laidback"". Our analysis of the recordings follows three stages. First, we compute sixteen binary features that capture the micro-timing relations of the kick, snare and hi-hat drum on-sets, between each other and with regards to the metrical grid. Second, we construct a micro-timing profile (mtP) for every performance by averaging the binary features across the recording. An mtP codifies the frequency with which the various features were found in a performance. Third, through a ""similarity profiles"" hierarchical clustering analysis, we identify groups of recordings with significant similarities in their mtPs. We found distinct strategies to express each intended timing profile that employ specific combinations of relations between the instruments and with regards to the meter. Finally, we created a map that summarizes the main characteristics of the strategies and their relations using a phylogenetic tree visualization.",NOR,facility,Developed economies,"[27.024685, -42.58907]","[-19.92813, 4.941401]","[22.063904, -17.467812, 2.7777023]","[2.4721727, 16.680244, -9.294974]","[7.674398, 7.1682415]","[6.2037086, 1.7019267]","[10.329843, 11.555759, 1.0251408]","[8.114722, 6.5654526, 11.726812]"
93,Thassilo Gadermaier;Gerhard Widmer,A Study of Annotation and Alignment Accuracy for Performance Comparison in Complex Orchestral Music,2019,https://doi.org/10.5281/zenodo.3527924,"Thassilo Gadermaier+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility","Quantitative analysis of commonalities and differences between recorded music performances is an increasingly common task in computational musicology. A typical scenario involves manual annotation of different recordings of the same piece along the time dimension, for comparative analysis of, e.g., the musical tempo, or for mapping other performance-related information between performances. This can be done by manually annotating one reference performance, and then automatically synchronizing other performances, using audio-to-audio alignment algorithms. In this paper we address several questions related to those tasks. First, we analyze different annotations of the same musical piece, quantifying timing deviations between the respective human annotators. A statistical evaluation of the marker time stamps will provide (a) an estimate of the expected timing precision of human annotations and (b) a ground truth for subsequent automatic alignment experiments. We then carry out a systematic evaluation of different audio features for audio-to-audio alignment, quantifying the degree of alignment accuracy that can be achieved, and relate this to the results from the annotation study.",AUT,education,Developed economies,"[16.163225, -13.422137]","[-18.290028, -13.596493]","[-3.6759949, -11.050474, -6.5749855]","[-0.95544, -21.963726, -2.9734912]","[11.09257, 6.488312]","[5.9409876, 1.0306581]","[12.142902, 12.761239, -1.5807041]","[8.097768, 6.0817013, 11.014745]"
92,Timothy de Reuse;Ichiro Fujinaga,Pattern Clustering in Monophonic Music by Learning a Non-Linear Embedding From Human Annotations,2019,https://doi.org/10.5281/zenodo.3527922,"Timothy de Reuse+Centre for Interdisciplinary Research in Music Media and Technology, McGill University>CAN>education;Ichiro Fujinaga+Centre for Interdisciplinary Research in Music Media and Technology, McGill University>CAN>education","Musical pattern discovery algorithms find instances of repetition in symbolic music, allowing for some user-specifiable amount of variation between identified repetitions; however, they can yield an intractably large number of discovered patterns when allowing for even small amounts of variation. This is commonly addressed by defining some heuristic notion of pattern significance, and returning only the most significant patterns. This paper develops a method of pattern discovery that models human judgement of what constitutes a significant pattern by incorporating annotations of repeated patterns, avoiding the need to design heuristics.  We take pattern discovery as a clustering task, where the input is a set of passages of monophonic music, represented as vectors of extracted features, and the output clusters correspond to discovered patterns. The human annotations are used to train a neural network to learn a low-dimensional embedding of the feature space that maps passages of music close together when they are occurrences of the same ground-truth pattern. The results of this approach match up with the annotations significantly better than the results of an approach using clustering without subspace learning. We provide examples of the types of patterns that this method tends to discover and discuss its feasibility and practicality as a tool for extracting useful information about repetitive structure in music.",CAN,education,Developed economies,"[-8.803916, 0.53517026]","[2.987352, 17.854097]","[-0.4313003, 15.185071, 3.9456358]","[-1.6780702, -11.728389, 6.7473545]","[12.394374, 8.85026]","[9.024004, 1.3705103]","[13.274208, 14.4594145, -0.22363302]","[10.5512905, 6.693428, 12.496902]"
91,Thomas Low;Christian Hentschel;Sayantan Polley;Anustup Das;Harald Sack;Andreas Nurnberger;Sebastian Stober,The ISMIR Explorer - A Visual Interface for Exploring 20 Years of ISMIR Publications,2019,https://doi.org/10.5281/zenodo.3527920,"Thomas Low+Otto von Guericke University Magdeburg>DEU>education;Christian Hentschel+Hasso Plattner Institute for IT Systems Engineering, University of Potsdam>DEU>education;Sayantan Polley+Otto von Guericke University Magdeburg>DEU>education;Anustup Das+Otto von Guericke University Magdeburg>DEU>education;Harald Sack+FIZ Karlsruhe – Leibniz Institute for Information Infrastructure>DEU>facility;Andreas Nürnberger+Otto von Guericke University Magdeburg>DEU>education;Sebastian Stober+Otto von Guericke University Magdeburg>DEU>education","Ever since the first International Symposium on Music Information Retrieval in 2000, the proceedings have been made publicly available to interested researchers. After 20 years of annual conferences and workshops, this number has grown to an impressive amount of almost 2,000 papers. When restricted to linear search and retrieval in a document collection of this size, it becomes inherently hard to identify topics, related work and trends in scientific research. Therefore, this paper presents and evaluates a map-based user interface for exploring 20 years of ISMIR publications. The interface visualizes k-nearest neighbor subsets of semantically similar papers. Users may jump from one neighborhood to the next by selecting another paper from the current subset. Through animated transitions between local k-nn maps, the interface creates the impression of panning a large global map. Evaluations results of a small user study suggest that users are able to discover interesting links between papers. Due to its generic approach, the interface is easily applicable to other document collections as well. The search interface and its source code will be made publicly available.",DEU,education,Developed economies,"[-13.883514, 46.06854]","[24.252605, 19.256336]","[-29.270607, -8.834609, -2.0019543]","[10.544053, -4.8585677, 19.223824]","[13.79935, 5.7182965]","[11.151239, 1.1937292]","[15.139711, 12.001308, -1.7268565]","[12.111365, 5.4387283, 12.964304]"
90,Yin-Jyun Luo;Kat Agres;Dorien Herremans,Learning Disentangled Representations of Timbre and Pitch for Musical Instrument Sounds Using Gaussian Mixture Variational Autoencoders,2019,https://doi.org/10.5281/zenodo.3527918,"Yin-Jyun Luo+Singapore University of Technology and Design>SGP>education|Institute of High Performance Computing, A*STAR>SGP>facility;Kat Agres+Institute of High Performance Computing, A*STAR>SGP>facility|Yong Siew Toh Conservatory of Music, National University of Singapore>SGP>education;Dorien Herremans+Singapore University of Technology and Design>SGP>education|Institute of High Performance Computing, A*STAR>SGP>facility","In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model's efficacy using latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high F-scores when tested on our synthesized sounds, which verifies the model's performance of controllable realistic timbre/pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single encoder-decoder architecture, which is evaluated by measuring the shift in the posterior of instrument classification. Our in-depth evaluation confirms the model's ability to successfully disentangle timbre and pitch.",SGP,education,Developing economies,"[10.115022, -30.181248]","[-11.169509, -47.66717]","[6.2994065, 9.263788, 17.34431]","[-16.363125, 1.0686214, -17.796833]","[10.322531, 8.55934]","[8.561038, 6.440764]","[13.024352, 12.482669, 0.49908406]","[9.637397, 5.818316, 8.430748]"
89,Rachel Bittner;Juan Jose Bosch,Generalized Metrics for Single-f0 Estimation Evaluation,2019,https://doi.org/10.5281/zenodo.3527916,Rachel Bittner+Spotify>USA>company|Unknown>Unknown>Unknown;Juan J. Bosch+Spotify>USA>company|Unknown>Unknown>Unknown,"Single-f0 estimation methods, including pitch trackers and melody estimators, have historically been evaluated using a set of common metrics which score estimates frame-wise in terms of pitch and voicing accuracy. ""Voicing"" refers to whether or not a pitch is active, and has historically been regarded as a binary value. However, this has limitations because it is often ambiguous whether a pitch is present or absent, making a binary choice difficult for humans and algorithms alike. For example, when a source fades out or reverberates, the exact point where the pitch is no longer present is unclear. Many single-f0 estimation algorithms select a threshold for when a pitch is active or not, and different choices of threshold drastically affect the results of standard metrics. In this paper, we present a refinement on the existing single-f0 metrics, by allowing the estimated voicing to be represented as a continuous likelihood, and introducing a weighting on frame level pitch accuracy, which considers the energy of the source producing the f0 relative to the energy of the rest of the signal. We compare these metrics experimentally with the previous metrics using a number of algorithms and datasets and discuss the fundamental differences. We show that, compared to the previous metrics, our proposed metrics allow threshold-independent algorithm comparisons.",USA,company,Developed economies,"[41.514267, -17.516441]","[-6.3692517, -11.872917]","[2.4579475, -22.84299, 11.599693]","[2.8520505, -14.451459, -12.970738]","[8.906211, 8.594715]","[6.8022556, 2.5624597]","[11.192804, 13.371661, -0.2995012]","[9.002939, 7.6451216, 10.8063965]"
88,Miguel Roman;Antonio Pertusa;Jorge Calvo-Zaragoza,A Holistic Approach to Polyphonic Music Transcription with Neural Networks,2019,https://doi.org/10.5281/zenodo.3527914,Miguel A. Román+University of Alicante>ESP>education;Antonio Pertusa+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"We present a framework based on neural networks to extract music scores directly from polyphonic audio in an end-to-end fashion. Most previous Automatic Music Transcription (AMT) methods seek a piano-roll representation of the pitches, that can be further transformed into a score by incorporating tempo estimation, beat tracking, key estimation or rhythm quantization. Unlike these methods, our approach generates music notation directly from the input audio in a single stage. For this, we use a Convolutional Recurrent Neural Network (CRNN) with Connectionist Temporal Classification (CTC) loss function which does not require annotated alignments of audio frames with the score rhythmic information. We trained our model using as input Haydn, Mozart, and Beethoven string quartets and Bach chorales synthesized with different tempos and expressive performances. The output is a textual representation of four-voice music scores based on **kern format. Although the proposed approach is evaluated in a simplified scenario, results show that this model can learn to transcribe scores directly from audio signals, opening a promising avenue towards complete AMT.",ESP,education,Developed economies,"[25.102043, -5.371682]","[-25.746323, -26.391739]","[9.326556, -2.204882, 12.188137]","[-10.847728, -7.4892426, -12.956682]","[9.583517, 8.048793]","[8.052514, 5.2824545]","[12.129937, 12.17143, 0.20291905]","[9.056108, 6.5907187, 9.12162]"
87,So Yeon Park;Audrey Laplante;Jin Ha Lee;Blair Kaneshiro,Tunes Together: Perception and Experience of Collaborative Playlists,2019,https://doi.org/10.5281/zenodo.3527912,So Yeon Park+Stanford University>USA>education;Audrey Laplante+Université de Montréal>CAN>education;Jin Ha Lee+University of Washington>USA>education;Blair Kaneshiro+Stanford University>USA>education,"Music is well established as a means of social connection. In the age of streaming platforms, personalized playlists and recommendations are popular topics in music information retrieval. We bring the focus of music enjoyment back to social connection and examine how technologies can enhance interpersonal relationships, specifically through the context of the collaborative playlist (CP). We conducted an exploratory study of CP users and non-users (N=65) and examined speculative and experienced purposes and outcomes of CPs, as well as general perspectives on music and social connectedness. We derived a CP Framework with three purposes - Practical, Cognitive, and Social - and two connotations - Utility and Orientation. Both users and non-users shared similar perspectives on music-related activities and CP user outcomes. Projected and actual CP purposes differed between groups, however, as did perception of music's role in connectedness in recent years. These results highlight the importance of music-based social interactions for both groups.",USA,education,Developed economies,"[-40.800545, 35.456654]","[41.099007, 31.581326]","[-7.9019065, 26.508894, -3.4683352]","[9.044332, 14.827837, 19.141008]","[15.822248, 8.542878]","[12.999906, 1.0583862]","[16.062508, 14.962537, -1.5265126]","[13.447049, 4.2716312, 12.014081]"
86,Nathaniel Condit-Schultz;Claire Arthur,humdrumR: a New Take on an Old Approach to Computational Musicology,2019,https://doi.org/10.5281/zenodo.3527910,Nathaniel Condit-Schultz+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education;Claire Arthur+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education,"Musicology research is a fundamentally humanistic endeavor. However, despite the productive work of a small niche of humanities-trained computational musicologists, most cutting-edge digital music research is pursued by scholars whose primary training is scientific or computational, not humanistic. This unfortunate situation is prolonged, at least in part, by the daunting barrier that computer coding presents to humanities scholars with no technical training. In this paper, we present humdrumR (""hum-drummer""), a software package designed to afford computational musicology research for both advanced and novice computer coders. Humdrum is a powerful and influential existing computational musicology framework, including the humdrum syntax—a flexible text data format with tens of thousands of extant scores available (Kern Scores)—and the Bash-based humdrum toolkit. HumdrumR is a modern replacement for the humdrum toolkit, based in the data-analysis/statistical programming language R. By combining the flexibility and transparency of the humdrum syntax with the powerful data analysis tools and concise syntax of R, humdrumR offers an appealing new approach to would-be computational musicologists. HumdrumR leverages R's powerful metaprogramming capabilities to create an extremely expressive and composable syntax, allowing novices to achieve usable analyses quickly while avoiding many coding concepts that are commonly challenging for beginners.",USA,education,Developed economies,"[2.2962658, 5.8999453]","[-2.1669233, 31.16982]","[-7.1538196, -4.7625055, 6.015316]","[-12.024058, -2.4695663, 13.984494]","[12.328946, 7.9954467]","[9.796004, 0.70867217]","[13.193623, 13.61162, -0.9054054]","[10.407608, 5.482224, 11.748729]"
85,Jean-Francois Ducher;Philippe  Esling,Folded CQT RCNN For Real-time Recognition of Instrument Playing Techniques,2019,https://doi.org/10.5281/zenodo.3527908,Jean-François Ducher+IRCAM (UMR9912 STMS)>FRA>facility;Philippe Esling+IRCAM (UMR9912 STMS)>FRA>facility,"In the past years, deep learning has produced state-of-the-art performance in timbre and instrument classification. However, only a few models currently deal with the recognition of advanced Instrument Playing Techniques (IPT). None of them have a real-time approach of this problem. Furthermore, most studies rely on a single sound bank for training and testing. Their methodology provides no assurance as to the generalization of their results to other sounds. In this article, we extend state-of-the-art convolutional neural networks to the classification of IPTs. We build the first IPT corpus from independent sound banks, annotate it with the JAMS standard and make it freely available. Our models yield consistently high accuracies on a homogeneous subset of this corpus. However, only a proper taxonomy of IPTs and specifically defined input transforms offer proper resilience when addressing the ""minus-1db"" methodology, which assesses the ability of the models to generalize. In particular, we introduce a novel Folded Constant Q-Transform adjusted to the requirements of IPT classification. Finally we discuss the use of our classifier in real-time.",FRA,facility,Developed economies,"[48.776436, -13.9428425]","[-30.498642, -29.938463]","[24.796349, -6.9744444, 4.2036886]","[-8.793483, -10.913671, -18.036865]","[8.610696, 7.432649]","[7.8604097, 4.856974]","[11.221633, 12.362518, 0.72860193]","[9.237342, 7.0320683, 9.17325]"
84,Stefan Lattner;Monika Dörfler;Andreas Arzt,Learning Complex Basis Functions for Invariant Representations of Audio,2019,https://doi.org/10.5281/zenodo.3527906,Stefan Lattner+Sony Computer Science Laboratories (CSL)>FRA>company;Monika Dörfler+University Vienna>AUT>education;Andreas Arzt+JKU Linz>AUT>education,"Learning features from data has shown to be more successful than using hand-crafted features for many machine learning tasks. In music information retrieval (MIR), features learned from windowed spectrograms are highly variant to transformations like transposition or time-shift. Such variances are undesirable when they are irrelevant for the respective MIR task. We propose an architecture called Complex Autoencoder (CAE) which learns features invariant to orthogonal transformations. Mapping signals onto complex basis functions learned by the CAE results in a transformation-invariant ""magnitude space"" and a transformation-variant ""phase space"". The phase space is useful to infer transformations between data pairs. When exploiting the invariance-property of the magnitude space, we achieve state-of-the-art results in audio-to-score alignment and in repeated section discovery for audio. A PyTorch implementation of the CAE, including the repeated section discovery method is available online.",FRA,company,Developed economies,"[-15.160434, -8.53722]","[-20.522718, -18.260883]","[4.9977336, 14.953295, 9.012499]","[-4.704765, -21.662394, -7.6071362]","[11.497758, 8.950205]","[8.684495, 4.8774247]","[13.453826, 13.129668, 0.3561443]","[9.507729, 6.418763, 9.298782]"
83,Mark Gotham;Matthew Ireland,"Taking Form: A Representation Standard, Conversion Code, and Example Corpora for Recording, Visualizing, and Studying Analyses of Musical Form",2019,https://doi.org/10.5281/zenodo.3527904,Mark Gotham+Cornell University>USA>education|University of Cambridge>GBR>education;Matthew T. Ireland+University of Cambridge>GBR>education,"We report on new specification standards for representing human analyses of musical form which enable musicians to represent their analytical view of a piece either on the score (where an encoded version is available) or on a spreadsheet. Both of these representations are simple, intuitive, and highly human-readable. Further, we provide code for converting between these formats, as well as a nested bracket representation adopted from computational linguistics which, in turn, can be visualised in familiar tree diagrams to provide 'at a glance' introductions to works. Finally, we provide an initial corpus of analyses/annotations in these formats, report on the practicalities of amassing them, and offer tools for automatic comparison of the works in the corpus based on the content and structure of the annotations. We intend for this resource to be useful to computational musicologists, enabling study of form at scale, and also useful pedagogically to all teachers, students, and appreciators of music from whom projects of this kind can be rather disconnected. The code and corpus can be found at https://github.com/MarkGotham/Taking-Form.",USA,education,Developed economies,"[16.695667, 11.342244]","[-5.546252, 29.198519]","[0.100535, -12.192529, 19.998634]","[-14.016696, -3.0493605, 14.927552]","[11.211229, 7.2674885]","[9.265175, 1.027725]","[13.143186, 12.215929, -0.7442274]","[10.212342, 5.7206464, 11.822808]"
82,Chris Donahue;Huanru Henry Mao;Yiting Ethan Li;Garrison Cottrell;Julian McAuley,LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training,2019,https://doi.org/10.5281/zenodo.3527902,Chris Donahue+UC San Diego>USA>education;Huanru Henry Mao+UC San Diego>USA>education;Yiting Ethan Li+UC San Diego>USA>education;Garrison W. Cottrell+UC San Diego>USA>education;Julian McAuley+UC San Diego>USA>education,"We are interested in the task of generating multi-instrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation—here we adapt it to the multi-instrumental setting. Transformers are complex, high-dimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-voice scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music. Despite differences between the two corpora, we find that this pre-training procedure improves both quantitative and qualitative performance for our primary task.",USA,education,Developed economies,"[24.387718, 6.040857]","[-13.151309, -31.223324]","[3.7740326, 1.1940515, 25.583807]","[-13.172002, -2.2463791, -5.8531938]","[10.206829, 8.497985]","[8.58292, 5.619109]","[13.280789, 11.823488, 0.24262412]","[9.488833, 5.9545307, 9.64664]"
81,Andre Holzapfel;Emmanouil Benetos,Automatic Music Transcription and Ethnomusicology: a User Study,2019,https://doi.org/10.5281/zenodo.3527900,Andre Holzapfel+KTH Royal Institute of Technology>SWE>education;Emmanouil Benetos+Queen Mary University of London>GBR>education,"Converting an acoustic music signal into music notation using a computer program has been at the forefront of music information research for several decades, as a task referred to as automatic music transcription (AMT). However, current AMT research is still constrained to system development followed by quantitative evaluations; it is still unclear whether the performance of AMT methods is considered sufficient to be used in the everyday practice of music scholars. In this paper, we propose and carry out a user study on evaluating the usefulness of automatic music transcription in the context of ethnomusicology. As part of the study, we recruited 16 participants who were asked to transcribe short musical excerpts either from scratch or using the output of an AMT system as a basis. We collect and analyze quantitative measures such as transcription time and effort, and a range of qualitative feedback from study participants, which includes user needs, criticisms of AMT technologies, and links between perceptual and quantitative evaluations on AMT outputs. The results show no quantitative advantage of using AMT, but important indications regarding appropriate user groups and evaluation measures are provided.",SWE,education,Developed economies,"[-28.015362, -0.5796915]","[-11.768062, 31.08087]","[-23.382696, -9.580938, -4.3898683]","[-4.935281, 11.982673, 13.673536]","[14.110919, 8.694201]","[7.249862, 2.273805]","[14.404912, 14.547448, -1.3091954]","[9.191708, 6.7028227, 10.882385]"
54,Christoph Finkensiep;Richard Widdess;Martin Rohrmeier,Modelling the Syntax of North Indian Melodies with a Generalized Graph Grammar,2019,https://doi.org/10.5281/zenodo.3527844,Christoph Finkensiep+École Polytechnique Fédérale de Lausanne>CHE>education;Richard Widdess+SOAS University of London>GBR>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"Hierarchical models of music allow explanation of highly complex musical structure based on the general principle of recursive elaboration and a small set of orthogonal operations. Recent approaches to melodic elaboration have converged to a representation based on intervals, which allows the elaboration of pairs of notes. However, two problems remain: First, an interval-first representation obscures one-sided operations like neighbor notes. Second, while models of Western melody styles largely agree on step-wise operations such as neighbors and passing notes, larger intervals are either attributed to latent harmonic properties or left unexplained. This paper presents a grammar for melodies in North Indian raga music, showing not only that recursively applied neighbor and passing note operations underlie this style as well, but that larger intervals are generated as generalized neighbors, based on the tonal hierarchy of the underlying scale structure. The notion of a generalized neighbor is not restricted to ragas but can be transferred to other musical styles, opening new perspectives on latent structure behind melodies and music in general. The presented grammar is based on a graph representation that allows one to express elaborations on both notes and intervals, unifying and generalizing previous graph- and tree-based approaches.",CHE,education,Developed economies,"[9.48431, -1.7434659]","[3.6205683, -19.781061]","[6.955379, 12.622003, 1.6146307]","[13.166694, -14.641779, -4.1649766]","[11.180274, 10.003784]","[7.469306, 1.1974024]","[11.820214, 15.268806, -1.005213]","[8.959444, 7.0807176, 12.608503]"
53,Adrien Ycart;Andrew McLeod;Emmanouil Benetos;Kazuyoshi Yoshii,Blending Acoustic and Language Model Predictions for Automatic Music Transcription,2019,https://doi.org/10.5281/zenodo.3527842,Adrien Ycart+Queen Mary University of London>GBR>education;Andrew McLeod+Kyoto University>JPN>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"In this paper, we introduce a method for converting an input probabilistic piano roll (the output of a typical multi-pitch detection model) into a binary piano roll. The task is an important step for many automatic music transcription systems with the goal of converting an audio recording into some symbolic format. Our model has two components: an LSTM-based music language model (MLM) which can be trained on any MIDI data, not just that aligned with audio; and a blending model used to combine the probabilities of the MLM with those of the input probabilistic piano roll given by an acoustic multi-pitch detection model, which must be trained on (a comparably small amount of) aligned data. We use scheduled sampling to make the MLM robust to noisy sequences during testing. We analyze the performance of our model on the MAPS dataset using two different timesteps (40ms and 16th-note), comparing it against a strong baseline hidden Markov model with a training method not used before for the task to our knowledge. We report a statistically significant improvement over HMM decoding in terms of notewise F-measure with both timesteps, with 16th note timesteps improving further compared to 40ms timesteps.",GBR,education,Developed economies,"[26.401665, -6.747665]","[-12.3114805, -10.165522]","[12.591275, -1.3344413, 11.941515]","[-0.9855563, -13.851988, -6.080281]","[9.611299, 7.829231]","[6.7550282, 3.0193353]","[11.997462, 12.060932, 0.12346925]","[8.78677, 6.769615, 10.349315]"
52,Mason Bretan;Larry Heck,Self-Supervised Methods for Learning Semantic Similarity in Music,2019,https://doi.org/10.5281/zenodo.3527840,Mason Bretan+Samsung Research America>USA>company|Samsung Research America>USA>company;Larry Heck+Samsung Research America>USA>company|Samsung Research America>USA>company,"Neural networks have been used to learn a latent ""musical space"" or ""embedding"" to encode meaningful features and provide a method of measuring semantic similarity between two musical passages. An ideal embedding is one that both captures features useful for downstream tasks and conforms to a distribution suitable for sampling and meaningful interpolation. We present two new methods for learning musical embeddings that leverage context while simultaneously imposing a shape on the feature space distribution via backpropagation using an adversarial component. We focus on the symbolic domain and target short polyphonic musical units consisting of 40 note sequences. The goal is to project these units into a continuous low dimensional space that has semantic relevance. We evaluate relevance based on the learned features' abilities to complete various musical tasks and show improvement over baseline models including variational autoencoders, adversarial autoencoders, and deep structured semantic models. We use a dataset consisting of classical piano and demonstrate the robustness of our methods across multiple input representations.",USA,company,Developed economies,"[-7.8109856, 12.487123]","[-21.31198, -32.33082]","[-1.7843966, 14.071148, 0.59891075]","[-11.682693, -3.8340764, -17.280657]","[12.614415, 9.173702]","[8.985883, 5.21606]","[13.456516, 14.661623, -0.32325906]","[10.004956, 6.29123, 9.001256]"
51,Felipe Falcão;Barış Bozkurt;Xavier Serra;Nazareno Andrade;Ozan Baysal,A Dataset of Rhythmic Pattern Reproductions and Baseline Automatic Assessment System,2019,https://doi.org/10.5281/zenodo.3527838,"Felipe Falcão+Universidade Federal de Campina Grande>BRA>education;Baris Bozkurt+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Izmir Demokrasi University>TUR>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education;Ozan Baysal+Istanbul Technical University>TUR>education","This work presents a novel dataset comprised of audio and jury evaluations for rhythmic pattern reproduction performances by students applying for a conservatory. Data was collected in-loco during entrance exams where students were asked to imitated a set of rhythmic patterns played by teachers. In addition to the pass or fail grades provided by the members of the jury during the exam sessions, a subset of the data was also evaluated by external annotators on a 4-level scale. A baseline automatic assessment system is presented to demonstrate the usefulness of the dataset. Preliminary results deliver an accuracy of 76% for a simple pass/fail logistic regression classifier and a mean average error of 0.59 for a linear regression grade estimator. The implementation is also made publicly available to serve as baseline for alternative assessments systems that may leverage the dataset.",BRA,education,Developing economies,"[43.621464, 7.549313]","[-42.01997, 7.1956563]","[-6.2731066, -23.341658, 4.6918535]","[-12.428054, 19.030252, -0.17281653]","[12.02615, 5.4596825]","[7.9593306, 3.6706305]","[11.344384, 14.128441, -2.1507905]","[9.431574, 6.201848, 10.508822]"
23,Stefan Balke;Matthias Dorfer;Luis Carvalho;Andreas Arzt;Gerhard Widmer,Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music Retrieval,2019,https://doi.org/10.5281/zenodo.3527782,"Stefan Balke+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Matthias Dorfer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Luis Carvalho+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Andreas Arzt+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility","Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks.",AUT,education,Developed economies,"[-7.264808, 21.258375]","[-23.01596, -31.236595]","[-1.4385976, 14.105074, -13.304154]","[-12.113664, -14.440125, -15.806986]","[13.200822, 8.184932]","[8.585035, 5.2520137]","[13.461734, 14.570666, -1.6083734]","[9.8083105, 6.466755, 8.936021]"
22,Berit Janssen;Tom Collins;Iris Yuping Ren,Algorithmic Ability to Predict the Musical Future: Datasets and Evaluation,2019,https://doi.org/10.5281/zenodo.3527780,"Berit Janssen+Utrecht University>NLD>education;Tom Collins+University of York>GBR>education|Music Artificial Intelligence Algorithms, Inc.>USA>company;Iris Yuping Ren+Utrecht University>NLD>education","Music prediction and generation have been of recurring interest in the field of music informatics: many models that emulate listeners' musical expectancies, or that produce novel musical content have been introduced over the past few decades. So far, these models have mostly been evaluated in isolation, following diverse evaluation strategies. Our paper provides an overview of the new MIREX task Patterns for Prediction. We introduce a dataset, which contains monophonic and polyphonic data, both in symbolic and audio representations. We suggest a standardized evaluation procedure to compare algorithmic musical predictions. We compare two neural network models to a baseline model and show that algorithmic approaches can correctly predict about a third of a monophonic segment, and around half of a polyphonic segment, with one of the neural network models achieving best results. However, other approaches to algorithmic music prediction are needed to achieve a more rounded picture of the potential of state-of-the-art methods of music prediction.",NLD,education,Developed economies,"[-15.88948, 0.9740432]","[-11.176559, -34.30815]","[-5.038236, 7.166085, 14.299874]","[-14.43341, -3.2252216, -13.823079]","[11.798341, 8.765932]","[9.010509, 5.519012]","[13.541163, 13.47088, -0.30825162]","[9.842975, 5.8980694, 9.314087]"
21,Jonathan Driedger;Hendrik Schreiber;Bas de Haas;Meinard Müller,Towards Automatically Correcting Tapped Beat Annotations for Music Recordings,2019,https://doi.org/10.5281/zenodo.3527778,Jonathan Driedger+Chordify>Unknown>company;Hendrik Schreiber+International Audio Laboratories Erlangen>DEU>education;W. Bas de Haas+Chordify>Unknown>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>education,"A common method to create beat annotations for music recordings is to let a human annotator tap along with them. However, this method is problematic due to the limited human ability to temporally align taps with audio cues for beats accurately. In order to create accurate beat annotations, it is therefore typically necessary to manually correct the recorded taps in a subsequent step, which is a cumbersome task. In this work we aim to automate this correction step by ""snapping"" the taps to close-by audio cues - a strategy that is often used by beat tracking algorithms to refine their beat estimates. The main contributions of this paper can be summarized as follows. First, we formalize the automated correction procedure mathematically. Second, we introduce a novel visualization method that serves as a tool to analyze the results of the correction procedure for potential errors. Third, we present a new dataset consisting of beat annotations for 101 music recordings. Fourth, we use this dataset to perform a listening experiment as well as a quantitative study to show the effectiveness of our snapping procedure.",Unknown,company,Unknown,"[36.241776, -29.010563]","[-28.193417, -1.3480867]","[1.5847387, -27.441105, -7.332586]","[-7.412627, 11.268312, -5.441597]","[11.188833, 4.8648763]","[5.3740015, 1.5561296]","[11.083588, 13.134876, -2.225283]","[7.5055118, 6.6123223, 11.085616]"
20,Jon Gillick;Carmine-Emanuele Cella;David Bamman,Estimating Unobserved Audio Features for Target-Based Orchestration,2019,https://doi.org/10.5281/zenodo.3527776,"Jon Gillick+University of California, Berkeley>USA>education|University of California, Berkeley>USA>education;Carmine-Emanuele Cella+University of California, Berkeley>USA>education;David Bamman+University of California, Berkeley>USA>education","Target-based assisted orchestration can be thought of as the process of searching for optimal combinations of sounds to match a target sound, given a database of samples, a similarity metric, and a set of constraints.  A typical solution to this problem is a proposed orchestral score where candidate scores are ranked by similarity in some feature space between the target sound and the mixture of audio samples in the database corresponding to the notes in the score; in the orchestral setting, valid scores may contain dozens of instruments sounding simultaneously.  Generally, target-based assisted orchestration systems consist of a combinatorial optimization algorithm and a constraint solver that are jointly optimized to find valid solutions.  A key step in the optimization involves generating a large number of combinations of sounds from the database and then comparing the features of each mixture of sounds with the target sound.  Because of the high computational cost required to synthesize a new audio file and then compute features for every combination of sounds, in practice, existing systems instead estimate the features of each new mixture using precomputed features of the individual source files making up the combination.  Currently, state of the art systems use a simple linear combination to make these predictions, even if the features in use are not themselves linear.  In this work, we explore neural models for estimating the features of a mixture of sounds from the features of the component sounds, finding that standard features can be estimated with accuracy significantly better than that of the methods currently used in assisted orchestration systems.  We present quantitative comparisons and discuss the implications of our findings for target-based orchestration problems.",USA,education,Developed economies,"[-11.781682, -13.699317]","[-10.431021, 5.925604]","[13.82857, 3.6353858, 9.678697]","[3.4263465, -21.31117, 5.0710006]","[11.257853, 8.704054]","[8.75582, 2.8941443]","[13.0352, 13.211797, 0.3164115]","[10.419057, 6.801597, 10.893953]"
19,Keunwoo Choi;Kyunghyun Cho,Deep Unsupervised Drum Transcription,2019,https://doi.org/10.5281/zenodo.3527774,Keunwoo Choi+Spotify>USA>company;Kyunghyun Cho+New York University>USA>education|Facebook AI Research>USA>company,"We introduce DrummerNet, a drum transcription system that is trained in an unsupervised manner. DrummerNet does not require any ground-truth transcription and, with the data-scalability of deep neural networks, learns from a large unlabeled dataset. In DrummerNet, the target drum signal is first passed to a (trainable) transcriber, then reconstructed in a (fixed) synthesizer according to the transcription estimate. By training the system to minimize the distance between the input and the output audio signals, the transcriber learns to transcribe without ground truth transcription. Our experiment shows that DrummerNet performs favorably compared to many other recent drum transcription systems, both supervised and unsupervised.",USA,company,Developed economies,"[28.45518, -44.73724]","[-36.56657, -15.593318]","[19.427277, -21.532696, 6.5535417]","[0.64310074, 12.777651, -21.298067]","[7.5674043, 7.213654]","[8.370939, 4.552601]","[10.3597555, 11.426455, 1.073639]","[9.099855, 7.113288, 9.581678]"
18,Thomas Parmer;Yong-Yeol Ahn,Evolution of the Informational Complexity of Contemporary Western Music,2019,https://doi.org/10.5281/zenodo.3527772,Thomas Parmer+Indiana University>USA>education|Unknown>Unknown>Unknown;Yong-Yeol Ahn+Indiana University>USA>education|Unknown>Unknown>Unknown,"We measure the complexity of songs in the Million Song Dataset (MSD) in terms of pitch, timbre, loudness, and rhythm to investigate their evolution from 1960 to 2010. By comparing the Billboard Hot 100 with random samples, we find that the complexity of popular songs tends to be more narrowly distributed around the mean, supporting the idea of an inverted U-shaped relationship between complexity and hedonistic value. We then examine the temporal evolution of complexity, reporting consistent changes across decades, such as a decrease in average loudness complexity since the 1960s, and an increase in timbre complexity overall but not for popular songs. We also show, in contrast to claims that popular songs sound more alike over time, that they are not more similar than they were 50 years ago in terms of pitch or rhythm, although similarity in timbre shows distinctive patterns across eras and similarity in loudness has been increasing. Finally, we show that musical genres can be differentiated by their distinctive complexity profiles.",USA,education,Developed economies,"[-0.24878272, 9.107671]","[19.48807, 0.7767081]","[-6.7309284, -0.4937504, 4.368437]","[8.227096, 9.303992, 6.1633244]","[13.019431, 8.801385]","[10.320754, 2.5542972]","[13.552485, 14.543028, -0.97163254]","[11.6799345, 7.0294175, 12.772968]"
17,Andreas Katsiavalos;Tom Collins;Bret Battey,An Initial Computational Model for Musical Schemata Theory,2019,https://doi.org/10.5281/zenodo.3527768,"Andreas Katsiavalos+De Montfort University>GBR>education;Tom Collins+University of York>GBR>education|Music Artificial Intelligence Algorithms, Inc.>USA>company;Bret Battey+De Montfort University>GBR>education","Musical schemata theory entails the classification of subphrase-length progressions in melodic, harmonic and metric feature-sets as named entities (e.g., `Romanesca', `Meyer', `Cadence', etc.), where a musical schema is characterized by factors such as music content and form, position and tonal function within phrase structure, and interrelation with other schemata. To examine and automate the task of musical schemata classification, we developed a novel musical schemata classifier. First, we tested methods for exact and approximate matching of user-defined schemata prototypes, to establish the notions of identity and similarity between composite music patterns. Next, we examined methods for schemata prototype extraction from collections of same-labelled annotated examples, performing training and testing sessions similar to supervised learning approaches. The performance of the above tasks was verified using the same annotated dataset of 40 keyboard sonata excerpts from pre-Classical and Classical periods. Our evaluation of the classifier sheds light on: (a)~ability to parse and interpret music information, (b)~similarity methods for composite music patterns, (c)~categorization methods for polyphonic music.",GBR,education,Developed economies,"[3.100844, 4.5756936]","[-10.139172, 16.978235]","[-7.064213, -8.298064, 5.599262]","[-7.5824704, -3.4685585, 5.748548]","[12.001749, 8.0556555]","[8.750472, 2.1656022]","[13.16149, 13.567795, -0.83146024]","[10.311301, 6.6549597, 11.862511]"
16,Gabriel Meseguer Brocal;Geoffroy Peeters,Conditioned-U-Net: Introducing a Control Mechanism in the U-Net for Multiple Source Separations,2019,https://doi.org/10.5281/zenodo.3527766,"Gabriel Meseguer-Brocal+Ircam Lab, CNRS, Sorbonne Université>FRA>education;Geoffroy Peeters+Institut Polytechnique de Paris, Téléccom Paris>FRA>education","Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically trained for a single task, e.g. a particular instrument isolation. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net) which adds a control mechanism to the standard U-Net. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The C-U-Net decides the instrument to isolate according to a one-hot-encoding input vector. The input vector is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost.",FRA,education,Developed economies,"[10.157588, -47.748184]","[-41.422527, -35.216816]","[33.34596, -1.6749402, -7.5957546]","[-17.683153, -7.8451433, -25.582884]","[8.48096, 10.067756]","[6.763465, 5.891627]","[10.98243, 13.804009, 1.5824995]","[9.656281, 8.296015, 8.80121]"
15,Brian McFee;Katherine Kinnaird,Improving Structure Evaluation Through Automatic Hierarchy Expansion,2019,https://doi.org/10.5281/zenodo.3527764,Brian McFee+New York University>USA>education;Katherine M. Kinnaird+Smith College>USA>education,"Structural segmentation is the task of partitioning a recording into non-overlapping time intervals, and labeling each segment with an identifying marker such as A, B, or verse. Hierarchical structure annotation expands this idea to allow an annotator to segment a song with multiple levels of granularity. While there has been recent progress in developing evaluation criteria for comparing two hierarchical annotations of the same recording, the existing methods have known deficiencies when dealing with inexact label matchings and sequential label repetition.  In this article, we investigate methods for automatically enhancing structural annotations by inferring (and expanding) hierarchical information from the segment labels. The proposed method complements existing techniques for comparing hierarchical structural annotations by coarsening or refining labels with variation markers to either collapse similarly labeled segments together, or separate identically labeled segments from each other. Using the multi-level structure annotations provided in the SALAMI dataset, we demonstrate that automatic hierarchy expansion allows structure comparison methods to more accurately assess similarity between annotations.",USA,education,Developed economies,"[16.165573, 31.62058]","[-0.9226715, 7.668557]","[-18.4037, -17.028015, 4.891743]","[-2.2700064, 3.3608572, 2.6311553]","[11.843246, 7.8709636]","[8.5711775, 2.5761802]","[12.890973, 13.939943, -0.5462818]","[10.481834, 6.81804, 11.397816]"
14,Kento Watanabe;Masataka Goto,"Query-by-Blending: A Music Exploration System Blending Latent Vector Representations of Lyric Word, Song Audio, and Artist",2019,https://doi.org/10.5281/zenodo.3527762,Kento Watanabe+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents Query-by-Blending, a novel music exploration system that enables users to find unfamiliar music content by flexibly combining three musical aspects: lyric word, song audio, and artist. Although there are various systems for music retrieval based on the similarity between songs or artists and for music browsing based on visualized songs, it is still difficult to explore unfamiliar content by flexibly combining multiple musical aspects. Query-by-Blending overcomes this difficulty by representing each of the aspects as a latent vector representation (called a ""flavor"" in this paper) that is a distinctive quality felt to be characteristic of a given word/song/artist. By giving a lyric word as a query, for example, a user can find songs and artists whose flavors are similar to the flavor of the query word.  Moreover, by giving a query combining (blending) lyric-word and song-audio flavors, the user can interactively explore unfamiliar content containing the blended flavor. This multi-aspect blending was achieved by constructing a novel vector space model into which all of the lyric words, song audio tracks, and artist IDs of a collection can be embedded. In our experiments, we embedded 14,505 lyric words, 433,936 songs, and 44,696 artists into the same shared vector space and found that the system can appropriately calculate similarities between different aspects and blend flavors to find related lyric words, songs, and artists.",JPN,facility,Developed economies,"[-25.052872, 13.67783]","[31.817743, 14.35792]","[3.664153, 18.538387, -6.652962]","[15.445539, -0.21570942, 14.089186]","[13.725329, 8.4542465]","[11.498297, 2.0908182]","[13.895153, 14.915072, -1.6702951]","[12.544375, 5.873555, 12.5219345]"
13,Simon Waloschek;Aristotelis Hadjakos;Alexander Pacha,Identification and Cross-Document Alignment of Measures in Music Score Images,2019,https://doi.org/10.5281/zenodo.3527760,Simon Waloschek+Detmold University of Music>DEU>education;Aristotelis Hadjakos+Detmold University of Music>DEU>education;Alexander Pacha+TU Wien>AUT>education,"In the course of editing musical works, musicologists regularly compare multiple sources of the same musical piece, such as composers' autographs, handwritten copies, and various prints. For efficient comparison, cross-source navigation is essential, enabling to quickly jump back and forth between multiple sources without losing the current musical position. In practice, measures are first annotated by hand in the individual source images and then related to each other. Our approach automates this time-consuming and error-prone process with the help of deep learning. For this purpose, we train a neural network that automatically finds bounding boxes of all measures in images. A second network is trained to compute the similarity between two measures to determine if they have the same musical content and should, therefore, be linked for navigation. Sequences of outputs from the second network are matched using Dynamic Time Warping to provide the final proposal of measure relationships, so-called concordances. In addition to cross-source navigation, the results can be used to spot structural differences across the sources which are essential for editorial work, so that musicologists can focus more on analytical tasks.",DEU,education,Developed economies,"[20.07835, -10.12594]","[-21.320318, -30.612303]","[-0.63057715, -11.081742, -11.582086]","[-14.108703, -15.516641, -13.2171755]","[10.749383, 6.4589295]","[8.813799, 5.010566]","[12.126986, 12.338301, -1.5457411]","[10.013887, 6.432163, 8.981436]"
12,Lorenzo Porcaro;Emilia Gomez,20 Years of Playlists: A Statistical Analysis on Popularity and Diversity,2019,https://doi.org/10.5281/zenodo.3527758,"Lorenzo Porcaro+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gomez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Joint Research Centre, European Commission>ESP>facility","Grouping songs together, according to music preferences, mood or other characteristics, is an activity which reflects personal listening behaviours and tastes. In the last two decades, due to the increasing size of music catalogues accessible and to improvements of recommendation algorithms, people have been exposed to new ways for creating playlists. In this work, through the statistical analysis of more than 400K playlists from four datasets, created in different temporal and technological contexts, we aim to understand if it is possible to extract information about the evolution of humans strategies for playlist creation. We focus our analysis on two driving concepts of the Music Information Retrieval literature: popularity and diversity.",ESP,education,Developed economies,"[-41.869995, 35.534687]","[38.32401, 20.158997]","[-8.036544, 28.672754, -4.8139253]","[14.049153, 7.2459006, 19.775032]","[15.860342, 8.65544]","[12.470408, 1.58769]","[16.024572, 15.045139, -1.4641507]","[13.331952, 4.9827476, 12.684672]"
11,Mark Gotham;Dmitri Tymoczko;Michael Cuthbert,The RomanText Format: A Flexible and Standard Method for Representing Roman Numerial Analyses,2019,https://doi.org/10.5281/zenodo.3527756,Dmitri Tymoczko+Princeton University>USA>education;Mark Gotham+Cornell University>USA>education;Michael Scott Cuthbert+M.I.T.>USA>education;Christopher Ariza+Independent>Unknown>Unknown,"Roman numeral analysis has been central to the Western musician's toolkit since its emergence in the early nineteenth century: it is an extremely popular method for recording subjective analytical decisions about the chords and keys implied by a passage of music. Disagreements about these judgments have led to extensive theoretical debates and ongoing controversies. Such debates are exacerbated by the absence of a pubic corpus of expert Roman numeral analyses, and by the more fundamental lack of an agreed-upon, computer-readable syntax in which those analyses might be expressed. This paper specifies such a standard, along with an associated code library in music21, and a preliminary set of example corpora. To frame the project, we review some of the motivations for doing harmonic analysis, some reasons why it resists automation, and some prospective uses for our tools.",USA,education,Developed economies,"[23.562025, 17.785439]","[-18.607487, 17.895182]","[-5.5940084, -16.531706, 20.559969]","[-16.333033, -5.0076303, 8.582379]","[11.524165, 6.763478]","[7.868032, 2.0634148]","[13.277153, 12.051969, -1.3753309]","[9.992341, 7.7333603, 12.513855]"
10,Yu-Fen Huang;Tsung-Ping Chen;Nikki Moran;Simon Coleman;Li Su,Identifying Expressive Semantics in Orchestral Conducting Kinematics,2019,https://doi.org/10.5281/zenodo.3527754,Yu-Fen Huang+Academia Sinica>TWN>education;Tsung-Ping Chen+Academia Sinica>TWN>education;Nikki Moran+University of Edinburgh>GBR>education;Simon Coleman+University of Edinburgh>GBR>education;Li Su+Academia Sinica>TWN>education,"Existing kinematic research on orchestral conducting movement contributes to beat-tracking and the delivery of performance dynamics. Methodologically, such movement cues have been treated as distinct, isolated events. Yet as practicing musicians and music pedagogues know, conductors' expressive instructions are highly flexible and dependent on the musical context. We seek to demonstrate an approach to search for effective descriptors to express musical features in conducting movement in a valid music context, and to extract complex expressive semantics from elementary conducting kinematic variations. This study therefore proposes a multi-task learning model to jointly identify dynamic, articulation, and phrasing cues from conducting kinematics. A professional conducting movement dataset is compiled using a high-resolution motion capture system. The ReliefF algorithm is applied to select significant features from conducting movement, and recurrent neural network (RNN) is implemented to identify multiple movement cues. The experimental results disclose key elements in conducting movement which communicate musical expressiveness; the results also highlight the advantage of multi-task learning in the complete musical context over single-task learning. To the best of our knowledge, this is the first attempt to use recurrent neural network to explore multiple semantic expressive cuing in conducting movement kinematics.",TWN,education,Developing economies,"[8.419508, 10.562913]","[-35.298626, -10.595698]","[-2.2758424, -17.101864, 3.8963232]","[-18.411213, 10.187572, -6.3688297]","[12.11457, 7.257297]","[8.055667, 4.006018]","[12.834442, 13.504685, -1.2747067]","[9.009914, 6.39474, 9.8274555]"
9,Guillaume Doras;Geoffroy Peeters,Cover Detection Using Dominant Melody Embeddings,2019,https://doi.org/10.5281/zenodo.3527752,"Guillaume Doras+Sacem & Ircam Lab, CNRS, Sorbonne Université>FRA>education;Geoffroy Peeters+Telecom Paris, Institut Polytechnique de Paris>FRA>education","Automatic cover detection -- the task of finding in an audio database all the covers of one or several query tracks -- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use.  In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -- that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.",FRA,education,Developed economies,"[9.690679, 42.632]","[27.234549, -14.187666]","[-0.31292477, 12.618329, -24.838646]","[19.128689, -3.6617725, -1.0266323]","[16.076351, 11.116024]","[10.1782, 2.9362164]","[12.872583, 17.342049, -0.34924996]","[11.751869, 6.596759, 11.516307]"
8,Rachel Bittner;Magdalena Fuentes;David Rubinstein;Andreas Jansson;Keunwoo Choi;Thor Kell,mirdata: Software for Reproducible Usage of Datasets,2019,https://doi.org/10.5281/zenodo.3527750,"Rachel M. Bittner+Spotify>USA>company;Magdalena Fuentes+L2S, CNRS–Univ. Paris-Sud–CentraleSupélec>FRA>education|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;David Rubinstein+Spotify>USA>company;Andreas Jansson+Spotify>USA>company;Keunwoo Choi+Spotify>USA>company;Thor Kell+Spotify>USA>company","There are a number of efforts in the MIR community towards increased reproducibility, such as creating more open datasets, publishing code, and the use of common software libraries, e.g. for evaluation. However, when it comes to datasets, there is usually little guarantee that researchers are using the exact same data in the same way, which among other issues, makes comparisons of different methods on the ""same"" datasets problematic.  In this paper, we first show how (often unknown) differences in datasets can lead to significantly different experimental results. We propose a solution to these problems in the form of an open source library, mirdata, which handles datasets in their current distribution modes, but controls for possible variability. In particular, it contains tools which: (1) validate if the user's data (e.g. audio, annotations) is consistent with a canonical version of the dataset; (2) load annotations in a consistent manner; (3) download or give instructions for obtaining data; and (4) make it easy to perform track metadata-specific analysis.",USA,company,Developed economies,"[-5.3022485, 55.09644]","[20.67404, 43.508587]","[-35.56986, -4.717502, -7.9294443]","[-4.512849, 5.701169, 17.847244]","[13.44793, 5.0675526]","[11.410065, 0.42840433]","[14.875907, 11.480486, -1.3464231]","[11.687692, 4.8102546, 11.632997]"
7,Thitaree Tanprasert;Teerapat Jenrungrot;Meinard Müller;Timothy Tsai,MIDI-Sheet Music Alignment Using Bootleg Score Synthesis,2019,https://doi.org/10.5281/zenodo.3527748,Thitaree Tanprasert+Harvey Mudd College>USA>education;Teerapat Jenrungrot+Harvey Mudd College>USA>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>education;TJ Tsai+Harvey Mudd College>USA>education,"MIDI-sheet music alignment is the task of finding correspondences between a MIDI representation of a piece and its corresponding sheet music images.  Rather than using optical music recognition to bridge the gap between sheet music and MIDI, we explore an alternative approach: projecting the MIDI data into pixel space and performing alignment in the image domain.  Our method converts the MIDI data into a crude representation of the score that only contains rectangular floating notehead blobs, a process we call bootleg score synthesis.  Furthermore, we project sheet music images into the same bootleg space by applying a deep watershed notehead detector and filling in the bounding boxes around each detected notehead.  Finally, we align the bootleg representations using a simple variant of dynamic time warping.  On a dataset of 68 real scanned piano scores from IMSLP and corresponding MIDI performances, our method achieves a 97.3% accuracy at an error tolerance of one second, outperforming several baseline systems that employ optical music recognition.",USA,education,Developed economies,"[18.667364, -11.7434]","[-13.035055, -16.06618]","[-1.2637717, -12.394423, -8.832171]","[-5.600202, -20.797487, -0.63126016]","[10.881369, 6.4532475]","[6.4089913, 0.26504186]","[12.011373, 12.575229, -1.5272985]","[8.113462, 5.2041426, 10.788582]"
106,Jeffrey Ens;Philippe Pasquier,Quantifying Musical Style: Ranking Symbolic Music based on Similarity to a Style,2019,https://doi.org/10.5281/zenodo.3527952,Jeff Ens+Simon Fraser University>CAN>education;Philippe Pasquier+Simon Fraser University>CAN>education,"Modelling human perception of musical similarity is critical for the evaluation of generative music systems, musicological research, and many Music Information Retrieval tasks. Although human similarity judgments are the gold standard, computational analysis is often preferable, since results are often easier to reproduce, and computational methods are much more scalable. Moreover, computation based approaches can be calculated quickly and on demand, which is a prerequisite for use with an online system. We propose StyleRank, a method to measure the similarity between a MIDI file and an arbitrary musical style delineated by a collection of MIDI files. MIDI files are encoded using a novel set of features and an embedding is learned using Random Forests. Experimental evidence demonstrates that StyleRank is highly correlated with human perception of stylistic similarity, and that it is precise enough to rank generated samples based on their similarity to the style of a corpus. In addition, similarity can be measured with respect to a single feature, allowing specific discrepancies between generated samples and a particular musical style to be identified.",CAN,education,Developed economies,"[-0.2627241, 14.7762785]","[23.560354, 5.7790675]","[-3.4744773, 3.841318, 0.49525404]","[13.564486, 1.0249159, 8.755048]","[12.826084, 9.4611435]","[9.991194, 2.2889962]","[13.137892, 15.272668, -0.6773602]","[11.631379, 6.6444373, 12.449432]"
5,"Alexander Pacha;Jorge Calvo-Zaragoza;Jan Hajič, jr.",Learning Notation Graph Construction for Full-Pipeline Optical Music Recognition,2019,https://doi.org/10.5281/zenodo.3527744,Alexander Pacha+TU Wien>AUT>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education;Jan Hajiˇc jr.+Charles University>CHE>education,"Optical Music Recognition (OMR) promises great benefits to Music Information Retrieval by reducing the costs of making sheet music available in a symbolic format. Recent advances in deep learning have turned typical OMR obstacles into clearly solvable problems, especially the stages that visually process the input image, such as staff line removal or detection of music-notation objects. However, merely detecting objects is not enough for retrieving the actual content, as music notation is a configurational writing system where the semantic of a primitive is defined by its relationship to other primitives. Thus, OMR systems must employ a notation assembly stage to infer such relationships among the detected objects. So far, this stage has been addressed by devising a set of predefined rules or grammars, which hardly generalize well. In this work, we formulate the notation assembly stage from a set of detected primitives as a machine learning problem. Our notation assembly is modeled as a graph that stores syntactic relationships among primitives, which allows us to capture the configuration of symbols in a music-notation document. Our results over the handwritten sheet music corpus MUSCIMA++ show 95.2% precision, 96.0% recall, and an F-score of 95.6% in establishing the correct syntactic relationships. When inferring relationships on data from a music object detector, the model achieves 93.2% precision, 91.5% recall and an F-score of 92.3%.",AUT,education,Developed economies,"[42.319176, 20.017244]","[-19.528765, -23.517282]","[22.344095, 16.411295, 13.635783]","[-13.883376, -20.691368, -4.3807383]","[8.63301, 6.0416565]","[6.5628843, -1.0119407]","[10.649007, 11.036856, -0.20482093]","[8.074525, 4.190986, 9.977849]"
4,Jeong Choi;Jongpil Lee;Jiyoung Park;Juhan Nam,Zero-shot Learning for Audio-based Music Classification and Tagging,2019,https://doi.org/10.5281/zenodo.3527741,Jeong Choi+KAIST>KOR>education|NAVER Corp.>KOR>company;Jongpil Lee+KAIST>KOR>education;Jiyoung Park+NAVER Corp.>KOR>company;Juhan Nam+KAIST>KOR>education,"Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where auxiliary information about the labels is used to unveil the relationship between each other.  In this work, we investigate the zero-shot learning in music domain and organize two different setups of auxiliary information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations.  Considering a music track is usually multi-labeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning.  Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in music domain.",KOR,education,Developing economies,"[-39.554874, -2.6692357]","[34.140602, -1.2176237]","[-10.644237, 13.418326, 8.550812]","[20.071941, 13.901191, -0.86707866]","[14.464903, 10.633814]","[10.392053, 4.3091645]","[15.564275, 14.078987, 0.17372337]","[11.964959, 6.1444583, 10.248082]"
3,Johan Pauwels;Ken O'Hanlon;Emilia Gomez;Mark B. Sandler,20 Years of Automatic Chord Recognition from Audio,2019,https://doi.org/10.5281/zenodo.3527739,"Johan Pauwels+Centre for Digital Music, Queen Mary University of London>GBR>education;Ken O’Hanlon+Centre for Digital Music, Queen Mary University of London>GBR>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Mark B. Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","In 1999, Fujishima published ""Realtime Chord Recognition of Musical Sound: a System using Common Lisp Music"". This paper kickstarted an active research topic that has been popular in and around the ISMIR community. The field of Automatic Chord Recognition (ACR) has evolved considerably from early knowledge-based systems towards data-driven methods, with neural network approaches arguably being central to current ACR research. Nonetheless, many of its core issues were already addressed or referred to in the Fujishima paper. In this paper, we review those twenty years of ACR according to these issues. We furthermore attempt to frame current directions in the field in order to establish some perspective for future research.",GBR,education,Developed economies,"[54.777958, -6.634317]","[-34.22092, 19.488808]","[28.15124, -10.840694, 15.3847885]","[-27.462019, -6.59901, -1.1316212]","[6.8288803, 8.597643]","[6.058407, 3.7186189]","[11.894236, 10.397323, 2.0289521]","[9.680475, 9.041494, 12.227939]"
2,Peter Knees;Markus Schedl;Masataka Goto,Intelligent User Interfaces for Music Discovery: The Past 20 Years and What's to Come,2019,https://doi.org/10.5281/zenodo.3527737,Peter Knees+TU Wien>AUT>education;Markus Schedl+Johannes Kepler University Linz>AUT>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Providing means to assist the user in finding music is one of the original motivations underlying the research field known as Music Information Retrieval (MIR). Therefore, already the first edition of ISMIR in the year 2000 called for papers addressing the topic of ""User interfaces for music IR"". Since then, the way humans interact with technology to access and listen to music has substantially changed, not least driven by the advances of MIR and related research fields such as machine learning and recommender systems.   In this paper, we reflect on the evolution of MIR-driven user interfaces for music browsing and discovery over the past two decades. We argue that three major developments have transformed and shaped user interfaces during this period, each connected to a phase of new listening practices: first, connected to personal music collections, intelligent audio processing and content description algorithms that facilitate the automatic organization of repositories and finding music according to sound qualities; second, connected to collective web platforms, the exploitation of user-generated metadata pertaining to semantic descriptions; and third, connected to streaming services, the collection of online music interaction traces on a large scale and their exploitation in recommender systems.  We review and contextualize work from ISMIR and related venues from all three phases and extrapolate current developments to outline possible scenarios of music recommendation and listening interfaces of the future.",AUT,education,Developed economies,"[-20.833643, 28.759449]","[35.791813, 17.903954]","[-16.127472, 5.998954, -19.322514]","[10.517703, 4.952152, 17.637213]","[14.349358, 7.401446]","[12.1863985, 1.6146486]","[14.308527, 14.168892, -2.4052227]","[12.890931, 5.065931, 12.501459]"
1,Alexander Lerch;Claire Arthur;Ashis Pati;Siddharth Gururani,Music Performance Analysis: A Survey,2019,https://doi.org/10.5281/zenodo.3527735,Alexander Lerch+Georgia Institute of Technology>USA>education;Claire Arthur+Georgia Institute of Technology>USA>education;Ashis Pati+Georgia Institute of Technology>USA>education;Siddharth Gururani+Georgia Institute of Technology>USA>education,"Music Information Retrieval (MIR) tends to focus on the analysis of audio signals. Often, a single music recording is used as representative of a ""song"" even though different performances of the same song may reveal different properties. A performance is distinct in many ways from a (arguably more abstract) representation of a ""song,"" ""piece,"" or musical score. The characteristics of the (recorded) performance -as opposed to the score or musical idea- can have a major impact on how a listener perceives music. The analysis of music performance, however, has been traditionally only a peripheral topic for the MIR research community. This paper surveys the field of Music Performance Analysis (MPA) from various perspectives, discusses its significance to the field of MIR, and points out opportunities for future research in this field.",USA,education,Developed economies,"[-15.361983, 7.025669]","[14.26474, 32.622154]","[-9.031391, -8.949158, -9.326348]","[-0.7325917, 9.399449, 14.5438595]","[12.732637, 7.495066]","[11.473518, 0.78525]","[13.6352415, 13.403579, -1.3532358]","[11.767257, 4.7541313, 11.719808]"
24,Thomas Nuttall;Miguel García-Casado;Víctor Núñez-Tarifa;Rafael Caro Repetto;Xavier Serra,Contributing to New Musicological Theories with Computational Methods: The Case of Centonization in Arab-Andalusian Music,2019,https://doi.org/10.5281/zenodo.3527784,"Thomas Nuttall+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Miguel García Casado+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Víctor Núñez Tarifa+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rafael Caro Repetto+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Arab-Andalusian music was formed in the medieval Islamic territories of Iberian Peninsula, drawing on local traditions and assuming Arabic influences. The expert performer and researcher of the Moroccan tradition of this music, Amin Chaachoo, is developing a theory, whose last formulation was recently published in La Mu-sique Hispano-Arabe, al-Ala (2016), which argues that centonization, a melodic composition technique used in Gregorian chant, was also utilized for the creation of this repertoire. In this paper we aim to contribute to Chaachoo's theory by means of tf-idf analysis. A highorder n-gram model is applied to a corpus of 149 prescriptive transcriptions of heterophonic recordings, representing each as an unordered multiset of patterns. Computing the tf-idf statistic of each pattern in this corpus provides a means by which we can rank and compare motivic content across nawabāt, distinct musical forms of the tradition. For each nawba, an empirical comparison is made between patterns identified as significant via our approach and those proposed by Chaachoo. Ultimately we observe considerable agreement between the two pattern sets and go further in proposing new, unique and as yet undocumented patterns that occur at least as frequently and with at least as much importance as those in Chaachoo's proposals.",ESP,education,Developed economies,"[1.5785656, 7.5439277]","[7.5871954, -1.7144042]","[-7.725834, -3.6849878, 3.9672158]","[-2.8143265, 10.215281, 3.5748653]","[12.41248, 8.880001]","[8.133289, 1.5123557]","[12.71523, 14.5635, -1.1174438]","[9.64715, 6.696139, 12.250229]"
105,Yaolong Ju;Samuel Howes;Cory McKay;Nathaniel Condit-Schultz;Jorge Calvo-Zaragoza;Ichiro Fujinaga,An Interactive Workflow for Generating Chord Labels for Homorhythmic Music in Symbolic Formats,2019,https://doi.org/10.5281/zenodo.3527950,"Yaolong Ju+Schulich School of Music, McGill University>CAN>education;Samuel Howes+Schulich School of Music, McGill University>CAN>education;Cory McKay+Marianopolis College>CAN>education;Nathaniel Condit-Schultz+Georgia Institute of Technology>USA>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education;Ichiro Fujinaga+Schulich School of Music, McGill University>CAN>education","Automatic harmonic analysis is challenging: rule-based models cannot account for every possible edge case, and manual annotation is expensive and sometimes inconsistent, undermining the training and evaluation of machine learning models. We present an interactive workflow to address these problems, and test it on Bach chorales. First, a rule-based model was used to generate preliminary, consistent chord labels in order to pre-train three machine learning models. These four models were grouped into an ensemble that generated chord labels by voting, achieving 91.4% accuracy on a reserved test set. A domain expert then corrected only those chords that the ensemble did not agree on unanimously (20.9% of the generated labels). Finally, we used these corrected annotations to re-train the machine learning models, and the resulting ensemble attained an accuracy of 93.5% on the reserved test set, a 24.4% reduction in the number of errors. This versatile interactive workflow can either work in a fully automatic way, or can capitalize on relatively minimal human involvement to generate higher-quality chord labels. It combines the consistency of rule-based models with the nuance of manual analysis to generate relatively inexpensive high-quality ground truth for training effective machine learning models.",CAN,education,Developed economies,"[51.365967, 0.69698083]","[-26.338783, 16.001007]","[18.658676, -15.613928, 19.446146]","[-19.926386, -8.886936, 3.7287586]","[7.6181574, 8.48194]","[6.511136, 3.5942283]","[12.278201, 10.858837, 1.3414625]","[9.849622, 8.392856, 12.320361]"
25,Quentin Lemaire;Andre Holzapfel,Temporal Convolutional Networks for Speech and Music Detection in Radio Broadcast,2019,https://doi.org/10.5281/zenodo.3527786,Quentin Lemaire+KTH Royal Institute of Technology>SWE>education;Andre Holzapfel+KTH Royal Institute of Technology>SWE>education,"The task of speech and music detection aims at the automatic annotation of potentially overlapping speech and music segments in audio recordings. This meta-data extraction process has important applications in royalty collection in broadcast audio. This study focuses on  deep neural network architectures made to process sequential data, and a series of recent architectures that have not yet been applied for this task are evaluated, extended and compared with a state-of-the-art architecture. Moreover, different training strategies are evaluated, and we demonstrate the advantages of a step-wise procedure that facilitates the combination heterogeneous datasets. The study shows that Temporal Convolution Network (TCN) architectures can outperform state-of-the-art architectures, and that especially the novel extension of non-causal TCN introduced in this paper leads to a significant improvement in the accuracy.",SWE,education,Developed economies,"[-9.190156, -11.542588]","[-25.28056, -37.261517]","[9.710146, -1.7718688, 3.8279827]","[-4.8158803, -2.9416335, -20.673216]","[11.9553385, 9.631512]","[8.75721, 4.80354]","[13.361369, 13.59631, 0.95753855]","[10.360061, 6.7635803, 8.916247]"
27,Jonathan Donier,Community-Based Cover Song Detection,2019,https://doi.org/10.5281/zenodo.3527790,Marc Sarfati+Spotify>USA>company;Anthony Hu+Spotify>USA>company;Jonathan Donier+Spotify>USA>company,"Audio-based cover song detection has received much attention in the MIR community in the recent years. To date, the most popular formulation of the problem has been to compare the audio signals of two tracks and to make a binary decision based on this information only. However, leveraging additional signals might be key if one wants to solve the problem at an industrial scale. In this paper, we introduce an ensemble-based method that approaches the problem from a many-to-many perspective.  Instead of considering pairs of tracks in isolation, we consider larger sets of potential versions for a given composition, and create and exploit the graph of relationships between these tracks. We show that this can result in a significant improvement in performance, in particular when the number of existing versions of a given composition is large.",USA,company,Developed economies,"[9.136898, 43.81025]","[27.12385, -14.160641]","[1.7052721, 10.9377165, -25.15294]","[18.386238, -3.1726515, -1.282979]","[16.092093, 11.1213455]","[10.067597, 2.7677882]","[12.877267, 17.367498, -0.35181075]","[11.645156, 6.6510367, 11.605904]"
50,Reinier de Valk;Ryaan Ahmed;Tim Crawford,JosquIntab: A Dataset for Content-based Computational Analysis of Music in Lute Tablature,2019,https://doi.org/10.5281/zenodo.3527836,"Reinier de Valk+Goldsmiths, University of London>GBR>education;Ryaan Ahmed+MIT>USA>education;Tim Crawford+Goldsmiths, University of London>GBR>education","An enormous corpus of music for the lute, spanning some two and half centuries, survives today. Unlike other musical corpora from the same period, this corpus has undergone only limited musicological study. The main reason for this is that it is written down exclusively in lute tablature, a prescriptive form of notation that is difficult to understand for non-specialists as it reveals little structural information. In this paper we present JosquIntab, a dataset of automatically created enriched diplomatic transcriptions in MIDI and MEI format of 64 sixteenth-century lute intabulations, instrumental arrangements of vocal compositions. Such a dataset enables large-scale content-based computational analysis of music in lute tablature hitherto impossible. We describe the dataset, the mapping algorithm used to create it, as well as a method to quantitatively evaluate the degree of arrangement (goodness of fit) of an intabulation. Furthermore, we present two use cases, demonstrating the usefulness of the dataset for both music information retrieval and musicological research. We make the dataset, the source code, and an implementation of the mapping algorithm, runnable as a command line tool, publicly available.",GBR,education,Developed economies,"[-16.821768, 13.408562]","[-7.2308364, 15.770808]","[-10.504633, -4.5311313, -11.703823]","[-6.6156955, 9.025743, 5.2272286]","[13.228811, 7.401612]","[8.362923, 1.3772297]","[13.879479, 13.69928, -1.6156317]","[9.745687, 6.1509476, 12.1187935]"
49,Saebyul Park;Taegyun Kwon;Jongpil Lee;Jeounghoon Kim;Juhan Nam,A Cross-Scape Plot Representation for Visualizing Symbolic Melodic Similarity,2019,https://doi.org/10.5281/zenodo.3527834,Saebyul Park+KAIST>KOR>education;Taegyun Kwon+KAIST>KOR>education;Jongpil Lee+KAIST>KOR>education;Jeounghoon Kim+KAIST>KOR>education;Juhan Nam+KAIST>KOR>education,"Symbolic melodic similarity is based on measuring a pairwise distance between two songs from diverse perspectives. The distance is usually summarized as a single value for song retrieval. This obscures observing the details of similarity patterns within the two songs. In this paper, we propose a cross-scape plot representation to visualize multi-scaled melody similarity between two symbolic music. The cross-scape plot is computed by stacking up a minimum local distance between two segments from each of the two songs. As the layer goes up, the segment size increases and it computes incrementally more long-term distances. This hierarchical representation allows for capturing the location and length of similar segments between two songs in a visually intuitive manner. We show the effectiveness of the cross-scape plot by evaluating it on examples from folk music collections with similarity-based categories and plagiarism cases.",KOR,education,Developing economies,"[0.5720303, 20.851805]","[16.487238, 6.024212]","[0.98476744, 2.6863897, -0.37092307]","[6.480494, -2.061649, 5.23489]","[12.262696, 9.590282]","[9.429364, 1.7803826]","[12.755486, 15.438012, -0.9051306]","[11.170169, 6.875928, 12.824713]"
48,Xiao Hu;Ying Que;Noriko Kando;Wenwei Lian,Analyzing User Interactions with Music Information Retrieval System: An Eye-tracking Approach,2019,https://doi.org/10.5281/zenodo.3527832,Xiao Hu+University of Hong Kong>HKG>education;Ying Que+University of Hong Kong>HKG>education;Noriko Kando+National Institute of Informatics>JPN>facility;Wenwei Lian+University of Hong Kong>HKG>education,"There has been little research considering eye movement as a measure when assessing user interactions with music information retrieval (MIR) systems, whereas many studies have adopted conventional user-centered measures such as user effectiveness and user perception. To bridge this research gap, this study investigates users' eye movement patterns and measures with two music retrieval tasks and two interface presentation modes. A user experiment was conducted with 16 participants whose eye movement and mouse click behaviors were recorded through professional eye trackers. Through analyzing visual patterns of eye gazes and movements as well as various metrics in prominent Areas of Interest (AOI), it is found that users' eye movement behaviors were related to task type. Besides, the results also disclosed that some eye movement metrics were related to both user effective-ness and user perception, and influenced by user characteristics. It is also found that some eye movement and user effectiveness metrics can be used to predict user perception. This study allows researchers to gain a deeper insight into user interactions with MIR systems from the perspective of eye movement measure.",HKG,education,Developing economies,"[-22.440083, 24.519997]","[36.049965, 32.508923]","[-14.169165, 11.872657, -10.863646]","[3.5942488, 11.602298, 17.22673]","[14.503828, 7.9066997]","[12.330187, 0.8523656]","[14.387927, 14.827665, -2.120768]","[12.783012, 4.368853, 12.107194]"
47,Maarten Grachten;Emmanuel Deruty;Alexandre Tanguy,Auto-adaptive Resonance Equalization using Dilated Residual Networks,2019,https://doi.org/10.5281/zenodo.3527830,Maarten Grachten+Sony CSL>FRA>company;Emmanuel Deruty+Sony CSL>FRA>company;Alexandre Tanguy+Yascore>FRA>company,"In music and audio production, attenuation of spectral resonances is an important step towards a technically correct result. In this paper we present a two-component system to automate the task of resonance equalization. The first component is a dynamic equalizer that automatically detects resonances and offers to attenuate them by a user-specified factor. The second component is a deep neural network that predicts the optimal attenuation factor based on the windowed audio. The network is trained and validated on empirical data gathered from an experiment in which sound engineers choose their preferred attenuation factors for a set of tracks. We test two distinct network architectures for the predictive model and find that a dilated residual network operating directly on the audio signal is on a par with a network architecture that requires a prior audio feature extraction stage. Both architectures predict human-preferred resonance attenuation factors significantly better than a baseline approach.",FRA,company,Developed economies,"[32.580902, -20.536942]","[-23.948978, -49.17725]","[24.04625, -5.942946, -12.852362]","[-22.452917, -6.372867, -23.974146]","[9.332104, 9.357152]","[7.7187037, 5.6911397]","[11.430848, 13.504925, 0.73773146]","[9.45111, 6.823541, 8.6501]"
46,Laurent Feisthauer;Louis Bigo;Mathieu Giraud,Modeling and Learning Structural Breaks in Sonata Forms,2019,https://doi.org/10.5281/zenodo.3527828,Laurent Feisthauer+Université de Lille>FRA>education;Louis Bigo+Université de Lille>FRA>education;Mathieu Giraud+Université de Lille>FRA>education,"Expositions of Sonata Forms are structured towards two cadential goals, one being the Medial Caesura (MC). The MC is a gap in the musical texture between the Transition zone (TR) and the Secondary thematic zone (S). It appears as a climax of energy accumulation initiated by the TR, dividing the Exposition in two parts.   We introduce high-level features relevant to formalize this energy gain and to identify MCs. These features concern rhythmic, harmonic and textural aspects of the music and characterize either the MC, its preparation or the texture contrast between TR and S. They are used to train a LSTM neural network on a corpus of 27 movements of string quartets written by Mozart. The model correctly locates the MCs on 14 movements within a leave-one-piece-out validation strategy. We discuss these results and how the network manages to model such structural breaks.",FRA,education,Developed economies,"[13.964171, 6.5635347]","[-14.62366, 16.147507]","[3.1043813, -5.8821673, 9.818428]","[-14.381223, -3.9236538, 2.9844854]","[10.979208, 7.829819]","[8.5188465, 2.5941236]","[12.640492, 12.6566725, -0.40952346]","[9.896792, 6.69379, 11.5248]"
45,Brian Bemman;David Meredith,Backtracking Search Heuristics for Solving the All-partition Array Problem,2019,https://doi.org/10.5281/zenodo.3527826,Brian Bemman+Aalborg University>DNK>education;David Meredith+Aalborg University>DNK>education,"Recent efforts to model the compositional processes of Milton Babbitt have yielded a number of computationally challenging problems. One of these problems, known as the \textit{all-partition array problem}, is a particularly hard variant of set covering, and several different approaches, including mathematical optimization, constraint satisfaction, and greedy backtracking, have been proposed for solving it. Of these previous approaches, only constraint programming has led to a successful solution. Unfortunately, this solution is expensive in terms of computation time. We present here two new search heuristics and a modification to a previously proposed heuristic, that, when applied to a greedy backtracking algorithm, allow the all-partition array problem to be solved in a practical running time. We demonstrate the success of our heuristics by solving for three different instances of the problem found in Babbitt's music, including one previously solved with constraint programming and one Babbitt himself was unable to solve. Use of the new heuristics allows each instance of the problem to be solved more quickly than was possible with previous approaches.",DNK,education,Developed economies,"[25.926626, 37.56956]","[21.717623, -33.766453]","[-5.330655, -10.50212, -28.078571]","[21.051199, -18.422869, -1.5037018]","[11.327721, 8.137882]","[8.828975, 1.1656214]","[12.720984, 13.004591, -0.073851585]","[10.431448, 6.586352, 12.973013]"
44,Lucas Ferreira;Jim Whitehead,Learning to Generate Music With Sentiment,2019,https://doi.org/10.5281/zenodo.3527824,"Lucas N. Ferreira+University of California, Santa Cruz>USA>education|University of California, Santa Cruz>USA>education;Jim Whitehead+University of California, Santa Cruz>USA>education|University of California, Santa Cruz>USA>education","Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy.  A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous.",USA,education,Developed economies,"[-43.183487, -12.536654]","[-10.492685, -39.47491]","[-0.6974953, 7.894861, 29.188776]","[-23.415623, 4.268271, -13.692341]","[10.442204, 8.712672]","[9.039391, 6.4495864]","[13.470441, 12.060115, 0.30644038]","[9.684307, 5.436501, 8.70161]"
43,Rainer Kelz;Gerhard Widmer,Towards Interpretable Polyphonic Transcription with Invertible Neural Networks,2019,https://doi.org/10.5281/zenodo.3527822,Rainer Kelz+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University Linz>AUT>education,"We explore a novel way of conceptualising the task of polyphonic music transcription, using so-called invertible neural networks. Invertible models unify both discriminative and generative aspects in one function, sharing one set of parameters. Introducing invertibility enables the practitioner to directly inspect what the discriminative model has learned, and exactly determine which inputs lead to which outputs. For the task of transcribing polyphonic audio into symbolic form, these models may be especially useful as they allow us to observe, for instance, to what extent the concept of single notes could be learned from a corpus of polyphonic music alone (which has been identified as a serious problem in recent research). This is an entirely new approach to audio transcription, which first of all necessitates some groundwork. In this paper, we begin by looking at the simplest possible invertible transcription model, and then thoroughly investigate its properties. Finally, we will take first steps towards a more sophisticated and capable version. We use the task of piano transcription, and specifically the MAPS dataset, as a basis for these investigations.",AUT,facility,Developed economies,"[24.956799, -5.3781786]","[-11.699036, -35.41587]","[8.545333, -2.9562201, 10.921197]","[-14.420901, -5.125284, -16.02987]","[9.554269, 7.9684634]","[8.502889, 5.544275]","[11.978152, 12.125202, 0.26893038]","[9.470609, 6.324821, 9.03018]"
42,Christos Koutlis;Manos Schinas;Vasiliki Gkatziaki;Symeon Papadopoulos;Yiannis  Kompatsiaris,Data-Driven Song Recognition Estimation Using Collective Memory Dynamics Models,2019,https://doi.org/10.5281/zenodo.3527820,Christos Koutlis+CERTH-ITI>GRC>facility;Manos Schinas+CERTH-ITI>GRC>facility;Vasiliki Gkatziaki+CERTH-ITI>GRC>facility;Symeon Papadopoulos+CERTH-ITI>GRC>facility;Yiannis Kompatsiaris+CERTH-ITI>GRC>facility,"Cultural products such as music tracks intend to be appreciated and recognized by a portion of the audience. However, no matter how highly recognized a song might be at the beginning of its life, its recognition will inevitably and progressively decay. The mechanism that governs this decreasing trajectory could be modelled as a forgetting curve or a collective memory decay process. Here, we propose a composite model, termed T-REC, that involves chart data, YouTube views, Spotify popularity of tracks and forgetting curve dynamics with the purpose of estimating song recognition levels. We also present a comparative study, involving state-of-the-art and baseline models based on ground truth data from a survey that we conducted regarding the recognition level of 100 songs in Sweden. Our method is found to perform best among this ensemble of models. A remarkable finding of our study pertains to the role of the number of weeks a song remains in the charts, which is found to be a major factor for the accurate estimation of the song recognition level.",GRC,facility,Developed economies,"[2.5018249, -5.658652]","[43.965702, 19.194359]","[9.330071, 13.297883, -2.488824]","[19.901304, 11.032983, 19.357744]","[11.321514, 9.425722]","[12.284013, 2.1424181]","[12.08414, 14.662784, -0.2007048]","[13.377077, 5.6377788, 12.488622]"
41,Dmitry Bogdanov;Alastair Porter;Hendrik Schreiber;Julián Urbano;Sergio Oramas,"The AcousticBrainz Genre Dataset: Multi-Source, Multi-Level, Multi-Label, and Large-Scale",2019,https://doi.org/10.5281/zenodo.3527818,"Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Alastair Porter+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Hendrik Schreiber+tagtraum industries incorporated>USA>company;Julián Urbano+Multimedia Computing Group, Delft University of Technology>NLD>education;Sergio Oramas+Pandora>USA>company","This paper introduces the AcousticBrainz Genre Dataset, a large-scale collection of hierarchical multi-label genre annotations from different metadata sources. It allows researchers to explore how the same music pieces are annotated differently by different communities following their own genre taxonomies, and how this could be addressed by genre recognition systems. Genre labels for the dataset are sourced from both expert annotations and crowds, permitting comparisons between strict hierarchies and folksonomies. Music features are available via the AcousticBrainz database. To guide research, we suggest a concrete research task and provide a baseline as well as an evaluation method. This task may serve as an example of the development and validation of automatic annotation algorithms on complementary datasets with different taxonomies and coverage. With this dataset, we hope to contribute to developments in content-based music genre recognition as well as cross-disciplinary studies on genre metadata analysis.",ESP,education,Developed economies,"[-21.20217, 9.904866]","[32.52175, -2.1748872]","[-17.575092, -5.5434084, -14.633101]","[16.806377, 11.813847, 1.6273708]","[13.073674, 7.790255]","[10.989176, 3.4931197]","[14.311571, 13.73755, -1.2523329]","[12.507712, 6.154028, 11.066434]"
40,Sebastian Rosenzweig;Frank Scherbaum;Meinard Müller,Detecting Stable Regions in Frequency Trajectories for Tonal Analysis of Traditional Georgian Vocal Music,2019,https://doi.org/10.5281/zenodo.3527816,Sebastian Rosenzweig+International Audio Laboratories Erlangen>DEU>facility;Frank Scherbaum+University of Potsdam>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"While Georgia has a long history of orally transmitted polyphonic singing, there is still an ongoing controversial discussion among ethnomusicologists on the tuning system underlying this type of music. First attempts have been made to analyze tonal properties (e. g., harmonic and melodic intervals) based on fundamental frequency (F0) trajectories. One major challenge in F0-based tonal analysis is introduced by unstable regions in the trajectories due to pitch slides and other frequency fluctuations. In this paper, we describe two approaches for detecting stable regions in frequency trajectories: the first algorithm uses morphological operations inspired by image processing, and the second one is based on suitably defined binary time–frequency masks. To avoid undesired distortions in subsequent analysis steps, both approaches keep the original F0-values unmodified, while only removing F0-values in unstable trajectory regions. We evaluate both approaches against manually annotated stable regions and discuss their potential in the context of interval analysis for traditional three-part Georgian singing.",DEU,facility,Developed economies,"[-6.6753206, -31.085787]","[-2.6021702, -13.474384]","[14.266117, 9.0218, -12.678497]","[10.06024, -11.258162, -12.240481]","[10.302892, 10.785303]","[6.871121, 2.2373898]","[11.402539, 14.959516, 0.35221463]","[8.967119, 7.688139, 11.491198]"
39,Ashis Pati;Alexander Lerch;Gaëtan Hadjeres,Learning to Traverse Latent Spaces for Musical Score Inpainting,2019,https://doi.org/10.5281/zenodo.3527814,"Ashis Pati+Center for Music Technology, Georgia Institute of Technology>USA>education;Alexander Lerch+Center for Music Technology, Georgia Institute of Technology>USA>education;Gaëtan Hadjeres+Sony CSL>FRA>company","Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models.",USA,education,Developed economies,"[-13.540832, -4.4425173]","[-10.476837, -44.091637]","[2.0102277, 10.181836, 14.969048]","[-17.735046, 0.9984871, -14.499053]","[11.435249, 8.806172]","[8.799931, 6.425177]","[13.398841, 13.043805, 0.13885434]","[9.662243, 5.6419687, 8.604881]"
38,Daniel Harasim;Timothy O'Donnell;Martin Rohrmeier,Harmonic Syntax in Time: Rhythm Improves Grammatical Models of Harmony,2019,https://doi.org/10.5281/zenodo.3527812,Daniel Harasim+École Polytechnique Fédérale de Lausanne>CHE>education;Timothy J. O’Donnell+McGill University>CAN>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"Music is hierarchically structured, both in how it is perceived by listeners and how it is composed. Such structure can be captured elegantly using probabilistic grammatical models similar to those used to study natural language. They address the complexity of the structure using abstract categories in a recursive formalism. Most existing grammatical models of musical structure focus on one single dimension of music--such as melody, harmony, or rhythm. While these grammar models often work well on short musical excerpts, accurate analysis of longer pieces requires taking into account the constraints from multiple domains of structure. The present paper proposes abstract product grammars--a formalism which integrates multiple dimensions of musical structure into a single grammatical model--along with efficient parsing and inference algorithms for this formalism. We use this model to study the combination of hierarchically-structured harmonic syntax and hierarchically-structured rhythmic information. The latter is modeled by a novel grammar of rhythm that is capable of expressing temporal regularities in musical phrases. It integrates grouping structure and meter. The combined model of harmony and rhythm outperforms both single-dimension models in computational experiments. All models are trained and evaluated on a treebank of hand-annotated Jazz standards.",CHE,education,Developed economies,"[25.266249, 22.46647]","[-19.311407, 23.976751]","[-5.021565, -8.809507, 30.667738]","[-17.654604, 0.85818434, 3.5456483]","[9.904503, 9.202189]","[7.3012805, 3.1399517]","[11.686242, 14.085701, -1.0690849]","[9.4277, 7.522524, 12.213061]"
37,Furkan Yesiler;Chris Tralie;Albin Correya;Diego Furtado Silva;Philip Tovstogan;Emilia Gomez;Xavier Serra,Da-TACOS: A Dataset for Cover Song Identification and Understanding,2019,https://doi.org/10.5281/zenodo.3527810,"Furkan Yesiler+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Chris Tralie+Ursinus College>USA>education;Albin Correya+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Diego F. Silva+Universidade Federal de São Carlos>BRA>education;Philip Tovstogan+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Joint Research Centre, European Commission>ESP>facility;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","This paper focuses on Cover Song Identification (CSI), an important research challenge in content-based Music Information Retrieval (MIR). Although the task itself is interesting and challenging for both academia and industry scenarios, there are a number of limitations for the advancement of current approaches. We specifically address two of them in the present study. First, the number of publicly available datasets for this task is limited, and there is no publicly available benchmark set that is widely used among researchers for comparative algorithm evaluation. Second, most of the algorithms are not publicly shared and reproducible, limiting the comparison of approaches. To overcome these limitations we propose Da-TACOS, a DaTAset for COver Song Identification and Understanding, and two frameworks for feature extraction and benchmarking to facilitate reproducibility. Da-TACOS contains 25K songs represented by unique editorial metadata plus 9 low- and mid-level features pre-computed with open source libraries, and is divided into two subsets. The Cover Analysis subset contains audio features (e.g. key, tempo) that can serve to study how musical characteristics vary for cover songs. The Benchmark subset contains the set of features that have been frequently used in CSI research, e.g. chroma, MFCC, beat onsets etc. Moreover, we provide initial benchmarking results of a selected number of state-of-the-art CSI algorithms using our dataset, and for reproducibility, we share a GitHub repository containing the feature extraction and benchmarking frameworks.",ESP,education,Developed economies,"[6.659062, 43.55886]","[22.921509, 11.411597]","[5.1383696, 13.465692, -21.059677]","[15.954175, -2.2475617, 2.8989074]","[16.104881, 11.14112]","[10.201483, 2.6282]","[12.910228, 17.2615, -0.39433277]","[11.666346, 6.52058, 11.747386]"
36,Eva Zangerle;Michael Vötter;Ramona Huber;Yi-Hsuan Yang,Hit Song Prediction: Leveraging Low- and High-Level Audio Features,2019,https://doi.org/10.5281/zenodo.3527808,Eva Zangerle+University of Innsbruck>AUT>education;Ramona Huber+University of Innsbruck>AUT>education;Michael Vötter+University of Innsbruck>AUT>education;Yi-Hsuan Yang+Academia Sinica>TWN>education,"Assessing the potential success of a given song based on its acoustic characteristics is an important task in the music industry. This task has mostly been approached from an internal perspective, utilizing audio descriptors to predict the success of a given song, where either low- or high-level audio features have been utilized separately. In this work, we aim to jointly exploit low- and high-level audio features and model the prediction as a regression task. Particularly, we make use of a wide and deep neural network architecture that allows for jointly exploiting low- and high-level features. Furthermore, we enrich the set of features with information about the release year of tracks. We evaluate our approach based on the Million Song Dataset and characterize a song as a hit if it is contained in the Billboard Hot 100 at any point in time. Our findings suggest that the proposed approach is able to outperform baseline approaches as well as approaches utilizing low- or high-level features individually. Furthermore, we find that incorporating the release year as well as features describing mood and vocals of a song improve prediction results.",AUT,education,Developed economies,"[-38.852905, -8.186472]","[-17.09233, -32.499268]","[-7.0834045, 21.248829, 9.500499]","[-11.5058775, -3.876508, -14.0791645]","[12.914333, 10.320068]","[9.098575, 5.1154566]","[13.791249, 13.888663, 0.75903106]","[9.899207, 6.4696836, 9.260368]"
35,John Thickstun;Zaid Harchaoui;Dean Foster;Sham Kakade,Coupled Recurrent Models for Polyphonic Music Composition,2019,https://doi.org/10.5281/zenodo.3527806,John Thickstun+University of Washington>USA>education;Zaid Harchaoui+University of Washington>USA>education;Dean P. Foster+Amazon>USA>company;Sham M. Kakade+University of Washington>USA>education|Amazon>USA>company,"This paper introduces a novel recurrent model for music composition that is tailored to the structure of polyphonic music. We propose an efficient new conditional probabilistic factorization of musical scores, viewing a score as a collection of concurrent, coupled sequences: i.e. voices. To model the conditional distributions, we borrow ideas from both convolutional and recurrent neural models; we argue that these ideas are natural for capturing music's pitch invariances, temporal structure, and polyphony. We train models for single-voice and multi-voice composition on 2,300 scores from the KernScores dataset.",USA,education,Developed economies,"[22.826908, -3.3204873]","[-9.99545, -31.613583]","[7.493909, 0.38144344, 13.348808]","[-15.588385, 1.783422, -6.582849]","[10.173281, 8.324745]","[8.861618, 5.462403]","[12.710247, 12.405864, 0.19299425]","[9.635735, 6.0690527, 9.422014]"
34,Morteza Behrooz;Sarah Mennicken;Jennifer Thom;Rohit Kumar;Henriette Cramer,Augmenting Music Listening Experiences on Voice Assistants,2019,https://doi.org/10.5281/zenodo.3527804,Morteza Behrooz+University of California Santa Cruz>USA>education;Sarah Mennicken+Spotify>USA>company;Jennifer Thom+Spotify>USA>company;Rohit Kumar+Unknown>Unknown>Unknown;Henriette Cramer+Spotify>USA>company,"Voice interfaces have rapidly gained popularity, introducing the opportunity for new ways to explore new interaction paradigms for music. However, most interactions with music in current consumer voice devices are still relatively transactional; primarily allowing for keyword-based commands and basic content playback controls. They are less likely to contextualize content or support content discovery beyond what users think to ask for. We present an approach to dynamically augment the voice-based music experience with background information using story generation techniques. Our findings indicate that augmentation can have positive effects on voice-based music experiences, given the right user context and mindset.",USA,education,Developed economies,"[-24.030582, 0.91146994]","[3.7091901, 25.127945]","[5.8008103, 15.462014, -10.371751]","[-17.815603, 12.021636, 11.902073]","[10.569028, 11.078592]","[8.721271, 6.1418257]","[11.58961, 15.3727665, 0.57505316]","[10.1307745, 5.618756, 8.914293]"
33,Kyungyun Lee;Juhan Nam,Learning a Joint Embedding Space of Monophonic and Mixed Music Signals for Singing Voice,2019,https://doi.org/10.5281/zenodo.3527802,Kyungyun Lee+KAIST>KOR>education|KAIST>Unknown>Unknown;Juhan Nam+KAIST>KOR>education|KAIST>Unknown>Unknown,"Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and singer-based music retrieval in both the in-domain and cross-domain tasks.",KOR,education,Developing economies,"[3.1036088, -35.709484]","[5.976739, -27.046782]","[24.054424, 15.762263, -4.5029383]","[-2.2282941, -2.202986, -24.08208]","[9.977657, 10.455353]","[9.536974, 4.258105]","[11.436318, 14.545459, 0.84019077]","[10.921248, 7.093545, 9.550219]"
32,Andrew Wiggins;Youngmoo Kim,Guitar Tablature Estimation with a Convolutional Neural Network,2019,https://doi.org/10.5281/zenodo.3527800,Andrew Wiggins+Drexel University>USA>education|Unknown>Unknown>Unknown;Youngmoo Kim+Drexel University>USA>education|Unknown>Unknown>Unknown,"Guitar tablature is a popular notation guitarists use to learn and share music. As it stands, most tablatures are created by an experienced guitarist taking the time and effort to annotate a song. As the process is time consuming and requires expertise, we are interested in automating this task. Previous approaches to automatic tablature transcription break the problem into two steps: 1) polyphonic pitch estimation, followed by 2) tablature fingering arrangement. Using a convolutional neural network (CNN) model, we can jointly solve both steps by learning a mapping directly from audio data to tablature. The model can simultaneously leverage physical playability constraints and differences in string timbres implicit in the data to determine the actual fingerings being used by the guitarist. We propose TabCNN, a CNN for estimating guitar tablature from audio of a solo acoustic guitar performance. We train and test our network using microphone recordings from the GuitarSet dataset, and TabCNN outperforms a state-of-the-art multipitch estimation algorithm. We also introduce a set of metrics to evaluate guitar tablature estimation.",USA,education,Developed economies,"[45.63874, -9.217129]","[-42.506935, -9.216153]","[24.653864, -10.977222, 7.213244]","[-18.479643, -7.989809, -5.9942546]","[7.7207065, 8.175004]","[7.6294565, 4.656476]","[11.548768, 11.278376, 1.3187091]","[9.078821, 6.725712, 9.533294]"
31,Christof Weiss;Sebastian J. Schlecht;Sebastian Rosenzweig;Meinard Müller,Towards Measuring Intonation Quality of Choir Recordings: A Case Study on Bruckner's Locus Iste,2019,https://doi.org/10.5281/zenodo.3527798,Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Sebastian J. Schlecht+International Audio Laboratories Erlangen>DEU>facility;Sebastian Rosenzweig+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Unaccompanied vocal music is a central part of Western art music, yet it requires excellent skills for singers to achieve proper intonation. In this paper, we analyze intonation deficiencies by introducing an intonation cost measure that can be computed from choir recordings and may help to assess the singers' intonation quality. With our approach, we measure the deviation between the recording's local salient frequency content and an adaptive reference grid based on the equal-tempered scale. The adaptivity introduces invariance of the local intonation measure to global intonation drifts. In our experiments, we compute this measure for several recordings of Anton Bruckner's choir piece Locus Iste. We demonstrate the robustness of the proposed measure by comparing scenarios of different complexity regarding the availability of aligned scores and multi-track recordings, as well as the number of singers per part. Even without using score information, our cost measure shows interesting trends, thus indicating the potential of our method for real-world applications.",DEU,facility,Developed economies,"[-1.6778727, -27.167765]","[-4.069068, -16.692896]","[14.736554, 1.8482165, -17.084446]","[7.464686, -18.00741, -10.36918]","[10.414716, 10.431626]","[6.8708267, 2.044227]","[11.335069, 15.180255, -0.06610757]","[8.824772, 7.56843, 11.534939]"
30,Go Shibata;Ryo Nishikimi;Eita Nakamura;Kazuyoshi Yoshii,"Statistical Music Structure Analysis Based on a Homogeneity-, Repetitiveness-, and Regularity-Aware Hierarchical Hidden Semi-Markov Model",2019,https://doi.org/10.5281/zenodo.3527796,Go Shibata+Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education;Ryo Nishikimi+Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education|Kyoto University>JPN>education,"This paper describes a music structure analysis method that splits music audio signals into meaningful segments such as musical sections and clusters them. In this task, how to model the four fundamental aspects of musical sections, i.e., homogeneity, repetitiveness, novelty, and regularity, in a unified way is still an open problem. Here we propose a solid statistical approach based on a homogeneity-, repetitiveness-, and regularity-aware hierarchical hidden semi-Markov model. The higher-level semi-Markov chain represents a sequence of sections that tend to have regularly spaced boundaries. The timbral features in each section are assumed to follow emission distributions that are homogeneous over time. The lower-level left-to-right Markov chain in each section represents a chord sequence whose sequential order is constrained to be a repetition of a chord sequence in another section of the same cluster. The whole model can be trained unsupervisedly based on Bayesian sparse learning where unnecessary sections automatically degenerate. The proposed method outperformed representative methods in segmentation and clustering accuracies with estimated sections having similar statistical properties as the ground truth data.",JPN,education,Developed economies,"[-1.2088132, 1.1255679]","[-23.842999, -14.530846]","[0.3282449, -5.6601944, 2.6331742]","[-1.2922786, -2.83007, -8.287303]","[11.693636, 8.187886]","[6.929899, 3.665418]","[12.669273, 13.697523, -0.31668618]","[9.948141, 7.977439, 11.333907]"
29,Tsung-Ping Chen;Li Su,Harmony Transformer: Incorporating Chord Segmentation into Harmony Recognition,2019,https://doi.org/10.5281/zenodo.3527794,Tsung-Ping Chen+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown;Li Su+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown,"Musical harmony analysis is usually a process of unfolding and interpreting the hierarchical structure of music. Computational approaches to such structural analysis are still challenging, owing to the fact that the boundary between different harmonic states (such as chord functions) is not explicitly defined in the audio or symbolic music data. It is a novel approach to improve chord recognition by jointly identifying chord change using end-to-end sequence learning. In this paper, we propose the Harmony Transformer, a multi-task music harmony analysis model aiming to improve chord recognition through incorporating chord segmentation into the recognition process. The integration of chord segmentation and chord recognition is implemented with the Transformer, a deep sequential learning model yielding fruitful results in the field of natural language processing. A non-autoregressive decoding framework is also adopted here in aid of concatenating the two highly correlated tasks. Experiments of both chord symbol recognition and functional harmony recognition on audio and symbolic datasets demonstrate that explicitly learning the hierarchical structural information of musical data can facilitate and improve the harmony recognition.",TWN,education,Developing economies,"[53.983547, -8.51231]","[-35.746075, 23.831188]","[29.055853, -8.495552, 14.095654]","[-28.98689, -2.2804232, -3.484602]","[6.809678, 8.599347]","[5.894065, 3.8717086]","[11.920604, 10.383391, 2.0856123]","[9.614025, 9.154846, 12.266533]"
28,Magdalena Fuentes;Lucas Maia;Martín Rocamora;Luiz Biscainho;Helene-Camille Crayencour;Slim Essid;Juan Bello,Tracking Beats and Microtiming in Afro-Latin American Music Using Conditional Random Fields and Deep Learning,2019,https://doi.org/10.5281/zenodo.3527792,"Magdalena Fuentes+L2S, CNRS–Univ. Paris-Sud–CentraleSupélec>FRA>education|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Lucas S. Maia+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education|Federal University of Rio de Janeiro>BRA>education;Martín Rocamora+Universidad de la República>URY>education;Luiz W. P. Biscainho+Federal University of Rio de Janeiro>BRA>education;Hélène C. Crayencour+L2S, CNRS–Univ. Paris-Sud–CentraleSupélec>FRA>education;Slim Essid+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Juan P. Bello+Music and Audio Research Laboratory, New York University>USA>education","Events in music frequently exhibit small-scale temporal deviations (microtiming), with respect to the underlying regular metrical grid. In some cases, as in music from the Afro-Latin American tradition, such deviations appear systematically, disclosing their structural importance in rhythmic and stylistic configuration. In this work we explore the idea of automatically and jointly tracking beats and microtiming in timekeeper instruments of Afro-Latin American music, in particular Brazilian samba and Uruguayan candombe. To that end, we propose a language model based on conditional random fields that integrates beat and onset likelihoods as observations. We derive those activations using deep neural networks and evaluate its performance on manually annotated data using a scheme adapted to this task. We assess our approach in controlled conditions suitable for these timekeeper instruments, and study the microtiming profiles' dependency on genre and performer, illustrating promising aspects of this technique towards a more comprehensive understanding of these music traditions.",FRA,education,Developed economies,"[36.963577, -38.243336]","[-28.551632, -8.863212]","[14.168103, -29.657446, -5.7136226]","[-7.120015, 16.168875, -9.402649]","[10.4027815, 4.216534]","[5.678129, 1.9959754]","[10.238339, 12.732109, -2.2403705]","[7.9782543, 6.79952, 11.192708]"
26,Shreyan Chowdhury;Andreu Vall Portabella;Verena Haunschmid;Gerhard Widmer,Towards Explainable Music Emotion Recognition: The Route via Mid-level Features,2019,https://doi.org/10.5281/zenodo.3527788,"Shreyan Chowdhury+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Andreu Vall+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Verena Haunschmid+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education","Emotional aspects play an important part in our interaction with music. However, modeling this aspect in MIR systems has been notoriously challenging since emotion is an inherently abstract and subjective experience, thus making it difficult to quantify or predict in the first place, and to make sense of the predictions in the next. In an attempt to create a model that can give a musically meaningful and intuitive explanation for its prediction, we propose a VGG-style deep neural network that learns to predict emotional characteristics of a musical piece together with (and based on) human-interpretable, mid-level perceptual features. We compare this to predicting emotion directly with an identical network that does not take into account the mid-level features, and observe that the cost of going through the mid-level features is surprisingly low, on average. The design of our network allows us to visualize the effects of perceptual features on individual emotion predictions, and we argue that the small loss in performance in going through the mid-level features is justified by the gain in explainability of the predictions.",AUT,education,Developed economies,"[-59.69909, -1.6228788]","[48.858524, -11.652584]","[-28.609356, 22.841757, 7.358931]","[6.645844, 24.897234, 3.0066395]","[14.090354, 12.970832]","[12.911093, 4.487407]","[15.919586, 14.2922535, 1.7929213]","[14.097515, 4.920247, 10.041908]"
6,Siddharth Gururani;Mohit Sharma;Alexander Lerch,An Attention Mechanism for Musical Instrument Recognition,2019,https://doi.org/10.5281/zenodo.3527746,"Siddharth Gururani+Center for Music Technology, Georgia Institute of Technology>USA>education;Mohit Sharma+Center for Music Technology, Georgia Institute of Technology>USA>education;Alexander Lerch+Center for Music Technology, Georgia Institute of Technology>USA>education","While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, such as MedleyDB, have strong per-frame instrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or 'attend to') specific time segments in the audio relevant to each instrument label leading to interpretable results.",USA,education,Developed economies,"[10.192484, -23.82969]","[-29.649553, -28.444029]","[20.092913, -6.882533, 0.95464677]","[-11.745669, -10.1208105, -16.429762]","[8.807002, 7.089647]","[8.238284, 4.9860315]","[10.9875765, 12.442662, 0.36561447]","[9.459717, 6.9215193, 9.158222]"
113,Federico Simonetta;Carlos Eduardo Cancino-Chacón;Stavros Ntalampiras;Gerhard Widmer,A Convolutional Approach to Melody Line Identification in Symbolic Scores,2019,https://doi.org/10.5281/zenodo.3527966,Federico Simonetta+University of Milano>ITA>education;Carlos Cancino-Chacón+Austrian Research Institute for Artificial Intelligence>AUT>facility;Stavros Ntalampiras+University of Milano>ITA>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|Austrian Research Institute for Artificial Intelligence>AUT>facility,"In many musical traditions, the melody line is of primary significance in a piece. Human listeners can readily distinguish melodies from accompaniment; however, making this distinction given only the written score – i.e. without listening to the music performed – can be a difficult task. Solving this task is of great importance for both Music Information Retrieval and musicological applications.  In this paper, we propose an automated approach to identifying the most salient melody line in a symbolic score. The backbone of the method consists of a convolutional neural network (CNN) estimating the probability that each note in the score (more precisely: each pixel in a piano roll encoding of the score) belongs to the melody line. We train and evaluate the method on various datasets, using manual annotations where available and solo instrument parts where not. We also propose a method to inspect the CNN and to analyze the influence exerted by notes on the prediction of other notes; this method can be applied whenever the output of a neural network has the same size as the input.",ITA,education,Developed economies,"[10.701118, -12.72687]","[-15.90708, -30.611435]","[1.7104061, -9.825665, 13.716315]","[-15.918829, -12.724994, -13.634534]","[11.482402, 7.1133327]","[8.667555, 5.1538243]","[12.913464, 12.447929, -1.0046642]","[9.73455, 6.495547, 9.231771]"
112,Daniel Yang;Thitaree Tanprasert;Teerapat Jenrungrot;Mengyi Shan;Timothy Tsai,MIDI Passage Retrieval Using Cell Phone Pictures of Sheet Music,2019,https://doi.org/10.5281/zenodo.3527964,Daniel Yang+Harvey Mudd College>USA>education;Thitaree Tanprasert+Harvey Mudd College>USA>education;Teerapat Jenrungrot+Harvey Mudd College>USA>education;Mengyi Shan+Harvey Mudd College>USA>education;TJ Tsai+Harvey Mudd College>USA>education,"This paper investigates a cross-modal retrieval problem in which a user would like to retrieve a passage of music from a MIDI file by taking a cell phone picture of a physical page of sheet music.  While audio-sheet music retrieval has been explored by a number of works, this scenario is novel in that the query is a cell phone picture rather than a digital scan.  To solve this problem, we introduce a mid-level feature representation called a bootleg score which explicitly encodes the rules of Western musical notation.  We convert both the MIDI and the sheet music into bootleg scores using deterministic rules of music and classical computer vision techniques for detecting simple geometric shapes.  Once the MIDI and cell phone image have been converted into bootleg scores, we estimate the alignment using dynamic programming.  The most notable characteristic of our system is that it does test-time adaptation and has no trainable weights at all--only a set of about 30 hyperparameters.  On a dataset containing 1000 cell phone pictures taken of 100 scores of classical piano music, our system achieves an F measure score of .869 and outperforms baseline systems based on commercial optical music recognition software.",USA,education,Developed economies,"[32.818405, 6.2530737]","[-12.778434, -16.390522]","[22.38959, 4.3351774, 16.10096]","[-6.559427, -20.596203, -0.5129511]","[10.5015545, 6.875014]","[6.5273504, 0.15285113]","[12.382543, 11.895775, -1.1618482]","[8.150384, 5.110424, 10.814664]"
33,Ethan Manilow;Gordon Wichern;Jonathan LeRoux,Hierarchical musical instrument separation,2020,https://doi.org/10.5281/zenodo.4245448,Ethan Manilow+Northwestern University>USA>education;Gordon Wichern+Mitsubishi Electric Research Laboratories (MERL)>USA>company;Jonathan Le Roux+Mitsubishi Electric Research Laboratories (MERL)>USA>company,"Many sounds that humans encounter are hierarchical in nature; a piano note is one of many played during a performance, which is one of many instruments in a band, which might be playing in a bar with other noises occurring. Inspired by this, we re-frame the musical source separation problem as hierarchical, combining similar instruments together at certain levels and separating them at other levels. This allows us to deconstruct the same mixture in multiple ways, depending on the appropriate level of the hierarchy for a given application. In this paper, we present various methods for hierarchical musical instrument separation, with some methods focusing on separating specific instruments (like guitars) and other methods that determine what to separate based on a user-supplied audio example. We additionally show that separating all hierarchy levels is possible even when training data is limited at fine-grained levels of the hierarchy.",USA,education,Developed economies,"[7.204892, -42.465046]","[-40.769253, -26.915085]","[25.262009, -3.3837454, -1.9994539]","[-10.037381, -4.8459215, -31.256067]","[8.39177, 9.840952]","[6.586472, 5.4331045]","[10.989088, 13.377121, 1.5165579]","[9.792182, 8.497562, 9.48089]"
34,Rohit M A;Vinutha T P;Preeti Rao,Structural segmentation of Dhrupad vocal bandish audio based on tempo,2020,https://doi.org/10.5281/zenodo.4245522,Rohit M A+Indian Institute of Technology Bombay>IND>education;Vinutha T P+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"A Dhrupad vocal concert comprises a composition section that is interspersed with improvised episodes of increased rhythmic activity involving the interaction between the vocals and the percussion. Tracking the changing rhythmic density, in relation to the underlying metric tempo of the piece, thus facilitates the detection and labeling of the improvised sections in the concert structure. This work concerns the automatic detection of the musically relevant rhythmic densities as they change in time across the bandish (composition) performance. An annotated dataset of Dhrupad bandish concert sections is presented. We implement a CNN-based system, trained to detect local tempo relationships, and follow it with temporal smoothing. We also employ audio source separation as a pre-processing step to the detection of the individual surface densities of the vocals and the percussion. This helps us obtain the complete musical description of the concert sections in terms of capturing the changing rhythmic interaction of the two performers.",IND,education,Developing economies,"[0.18689886, -4.916991]","[-24.932564, -10.06587]","[8.263264, -6.896505, -1.0271708]","[-1.1818819, 10.882119, -14.48389]","[11.422231, 8.302566]","[5.1874423, 2.1244497]","[12.205104, 14.293823, 0.094779745]","[7.687475, 6.944891, 10.560934]"
35,Matthew Davies;Magdalena Fuentes;João Fonseca;Luis Aly;Marco Jerónimo;Filippo Bonini Baraldi,Moving in time: Computational analysis of microtiming in Maracatu de baque solto,2020,https://doi.org/10.5281/zenodo.4245554,Matthew E. P. Davies+University of Coimbra>PRT>education|INESC TEC>PRT>facility;Magdalena Fuentes+New York University>USA>education;João Fonseca+University of Porto>PRT>education|INESC TEC>PRT>facility;Luís Aly+University of Porto>PRT>education|INESC TEC>PRT>facility;Marco Jerónimo+University of Porto>PRT>education|INESC TEC>PRT>facility;Filippo Bonini Baraldi+Ethnomusicology Institute (INET-md)>PRT>facility|Centre de Recherche en Ethnomusicologie (CREM-LESC)>FRA>facility,"""Maracatu de baque solto"" is a Carnival performance combining music, poetry, and dance, occurring in the Zona da Mata Norte region of Pernambuco (Northeast Brazil). Maracatu percussive music is strongly repetitive, and is played as loud and as fast as possible. Both from an MIR and ethnomusicological perspective this makes a complex musical scene to analyse and interpret. In this paper we focus on the extraction of microtiming profiles towards the longer term goal of understanding how rhythmic performance in Maracatu is used to promote health and well-being. To conduct this analysis we use a set of recordings acquired with contact microphones which minimise the interference between performers. Our analysis reveals that the microtiming profiles differ substantially from those observed in more widely studied South American music. In particular, we highlight the presence of dynamic microtiming profiles as well as the importance of the choice of time-keeper instrument, which dictates how the performances can be understood. Throughout this work, we emphasize the importance of a multidisciplinary approach in which MIR, audio engineering, and ethnomusicology must interact to provide meaningful insight about this music.",PRT,education,Developed economies,"[19.12629, -24.76499]","[-20.490517, 8.363818]","[3.0589538, -19.70614, 0.280873]","[-5.7564735, 15.106057, -0.84379554]","[11.353833, 5.4407983]","[6.026731, 1.7216259]","[11.494796, 13.155769, -2.1199062]","[8.309185, 6.6936812, 11.769949]"
36,Elena V. Epure;Guillaume Salha;Romain Hennequin,Multilingual music genre embeddings for effective cross-lingual music item annotation,2020,https://doi.org/10.5281/zenodo.4245556,Elena V. Epure+Deezer Research>FRA>company;Guillaume Salha+Deezer Research>FRA>company;Romain Hennequin+Deezer Research>FRA>company,"Annotating music items with music genres is crucial for music recommendation and information retrieval, yet challenging given that music genres are subjective concepts. Recently, in order to explicitly consider this subjectivity, the annotation of music items was modeled as a translation task: predict for a music item its music genres within a target vocabulary or taxonomy (tag system) from a set of music genre tags originating from other tag systems. However, without a parallel corpus, previous solutions could not handle tag systems in other languages, being limited to the English-language only. Here, by learning multilingual music genre embeddings, we enable cross-lingual music genre translation without relying on a parallel corpus. First, we apply compositionality functions on pre-trained word embeddings to represent multi-word tags. Second, we adapt the tag representations to the music domain by leveraging multilingual music genres graphs with a modified retrofitting algorithm. Experiments show that our method: 1) is effective in translating music genres across tag systems in multiple languages (English, French and Spanish); 2) outperforms the previous baseline in an English-language multi-source translation task.",FRA,company,Developed economies,"[-32.934517, 7.5374312]","[36.30171, -4.6202736]","[-16.208534, 6.770936, 6.868073]","[20.855986, 11.572883, 1.4246798]","[13.926245, 9.691171]","[11.041258, 3.6858022]","[14.83059, 13.921032, -0.45873547]","[12.667004, 6.3140445, 10.928113]"
37,Robert Lieck;Martin Rohrmeier,Modelling hierarchical key structure with pitch scapes,2020,https://doi.org/10.5281/zenodo.4245558,Robert Lieck+Digital and Cognitive Musicology Lab>CHE>facility|EPFL>CHE>education;Martin Rohrmeier+Digital and Cognitive Musicology Lab>CHE>facility|EPFL>CHE>education,"Musical form and syntax in Western classical music are hierarchically organised on different timescales. One of the most important features of this structure is the organisation of modulations between different keys throughout a piece. Music theoretical research has established taxonomies of prototypical modulation plans for different modes and musical forms. However, these prototypes still require empirical validation based on quantitative statistical methods and cannot be retrieved automatically so far.In this paper, we present a novel method to infer prototypical modulation plans from musical corpora. A modulation plan is formalised as a transposition-invariant probabilistic model over the underlying pitch class distributions based on a hierarchical pitch scape representation. Prototypical modulation plans can be learned in an unsupervised manner by training a mixture model (similar to a Gaussian mixture model) on the data, so that different prototypes appear as distinct clusters.We evaluate our approach by performing hierarchical clustering on a corpus of more than 150 Baroque pieces, with the extracted clusters showing excellent agreement with the most common prototypes postulated in music theory. Our method bears a great potential for modelling, analysis and discovery of hierarchical key structure and prototypes in corpora across a broad range of musical styles. An accompanying library is available at: github.com/robert-lieck/pitchscapes.",CHE,facility,Developed economies,"[28.217371, -17.91346]","[-13.365228, 13.629228]","[12.043766, -16.597935, 6.806644]","[-9.040323, -2.6012444, 2.628311]","[10.017241, 5.9842644]","[7.747487, 2.5519671]","[11.337527, 13.290949, -0.61259604]","[10.109663, 7.590289, 11.950214]"
38,Christoph Finkensiep;Ken Déguernel;Markus Neuwirth;Martin Rohrmeier,Voice-leading schema recognition using rhythm and pitch features,2020,https://doi.org/10.5281/zenodo.4245482,Christoph Finkensiep+École Polytechnique Fédérale de Lausanne>CHE>education;Ken Déguernel+École Polytechnique Fédérale de Lausanne>CHE>education;Markus Neuwirth+Anton Bruckner University Linz>AUT>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"Musical schemata constitute important structural building blocks used across historical styles and periods.They consist of two or more melodic lines that are combined to form specific successions of intervals. This paper tackles the problem of recognizing voice-leading schemata in polyphonic music.Since schema types and subtypes can be realized in a wide variety of ways on the musical surface,finding schemata in an automated fashion is a challenging task.To perform schema inference we employ a skipgram model that computes schema candidates, which are then classified using a binary classifier on musical features related to pitch and rhythm.This model is evaluated on a novel dataset of schema annotations in Mozart's piano sonatas produced byexpert annotators, which is published alongside this paper.The features are chosen to encode music-theoretically predicted properties of schema instances.We assess the relevance of each feature for the classification task, thus contributing to the theoretical understanding of complex musical objects.",CHE,education,Developed economies,"[26.22713, -17.948006]","[-10.335001, 17.009844]","[18.718163, -14.804544, -8.5717945]","[-7.8059196, -3.8968756, 5.5303645]","[9.750048, 10.531149]","[8.432574, 2.1086264]","[10.980884, 13.494088, -0.58353984]","[10.092638, 6.7774878, 11.869622]"
39,Ayush Patwari;Nicholas Kong;Jun Wang;Ullas Gargi;Michele Covell,Semantically meaningful attributes from co-listen embeddings for playlist exploration and expansion,2020,https://doi.org/10.5281/zenodo.4245486,Ayush Patwari+YouTube Music>USA>company;Nicholas Kong+YouTube Music>USA>company;Jun Wang+YouTube Music>USA>company;Ullas Gargi+YouTube Music>USA>company;Michele Covell+Google Research>USA>company;Aren Jansen+Google Research>USA>company,"Audio embeddings of musical similarity are often used for music recommendations and autoplay discovery. These embeddings are typically learned using co-listen data to train a deep neural network, to provide consistent tripletloss distances. Instead of directly using these co-listen–based embeddings, we explore making recommendations based on a second, smaller embedding space of human-intelligible musical attributes. To do this, we use the co-listen–based audio embeddings as inputs to small attribute classifiers, trained on a small hand-labeled dataset. These classifiers map from the original embedding space to a new interpretable attribute coordinate system that provides a more useful distance measure for downstream applications. The attributes and attribute embeddings allow us to provide a search interface and more intelligible recommendations for music curators. We examine the relative performance of these two embedding spaces (the co-listen–audio embedding and the attribute embedding) for the mathematical separation of thematic playlists. We also report on the usefulness of recommendations from the attribute-embedding space to human curators for automatically extending thematic playlists.",USA,company,Developed economies,"[-42.6784, 38.754784]","[35.512035, 10.272937]","[-4.8724713, 31.057598, 0.24292558]","[23.469597, 6.264174, 15.953458]","[16.07876, 8.246056]","[11.628645, 2.7946289]","[16.40105, 14.814504, -1.6403471]","[13.22657, 6.085945, 12.218453]"
40,Bruno Di Giorgi;Matthias Mauch;Mark Levy,Downbeat tracking with tempo invariant convolutional neural networks,2020,https://doi.org/10.5281/zenodo.4245408,Bruno Di Giorgi+Apple Inc.>USA>company;Matthias Mauch+Apple Inc.>USA>company;Mark Levy+Apple Inc.>USA>company,"The human ability to track musical downbeats is robust to changes in tempo, and it extends to tempi never previously encountered.  We propose a deterministic time-warping operation that enables this skill in a convolutional neural network (CNN) by allowing the network to learn rhythmic patterns independently of tempo.  Unlike conventional deep learning approaches, which learn rhythmic patterns at the tempi present in the training dataset, the patterns learned in our model are tempo-invariant, leading to better tempo generalisation and more efficient usage of the network capacity.We test the generalisation property on a synthetic dataset created by rendering the Groove MIDI Dataset using FluidSynth, split into a training set containing the original performances and a test set containing tempo-scaled versions rendered with different SoundFonts (test-time augmentation).The proposed model generalises nearly perfectly to unseen tempi (F-measure of 0.89 on both training and test sets), whereas a comparable conventional CNN achieves similar accuracy only for the training set (0.89) and drops to 0.54 on the test set.The generalisation advantage of the proposed model extends to real music, as shown by results on the GTZAN and Ballroom datasets.",USA,company,Developed economies,"[36.58579, -35.71631]","[-32.426636, -11.767711]","[10.835795, -30.802568, -3.8644717]","[-7.1321864, 10.674852, -17.237818]","[10.45961, 4.151671]","[4.9672785, 2.3459089]","[10.063026, 12.798262, -2.3018703]","[7.6746, 6.8583384, 10.271382]"
41,Francesco Foscarin;Andrew McLeod;Philippe Rigaux;Florent Jacquemard;Masahiko Sakai,ASAP: A dataset of aligned scores and performances for piano transcription,2020,https://doi.org/10.5281/zenodo.4245490,Francesco Foscarin+CNAM>FRA>education;Andrew McLeod+EPFL>CHE>education;Philippe Rigaux+CNAM>FRA>education;Florent Jacquemard+INRIA>FRA>facility|Nagoya University>JPN>education;Masahiko Sakai+Nagoya University>JPN>education,"In this paper we present Aligned Scores and Performances (ASAP): a new dataset of 222 digital musical scores aligned with 1068 performances (more than 92 hours) of Western classical piano music.The scores are provided as paired MusicXML files and quantized MIDI files, and the performances as paired MIDI files and partially as audio recordings.Scores and performances are aligned with downbeat, beat, time signature, and key signature annotations. ASAP has been obtained thanks to a new annotation workflow that combines score analysis andalignment algorithms, with the goal of reducing the time for manual annotation. The dataset itself is, to our knowledge, the largest that includes an alignment of music scores to MIDI and audio performance data. As such, it is a useful resource for a wide variety of MIR applications, from those that target the complete audio-to-score Automatic Music Transcription task, to others that target more specific aspects (e.g., key signature estimation and beat or downbeat tracking from both MIDI and audio representations).",FRA,education,Developed economies,"[31.532232, -3.8956587]","[-20.123747, -12.59041]","[16.413988, -3.3184192, 18.075779]","[-4.2003493, -16.38548, -2.4901612]","[9.760909, 7.204158]","[6.537549, 0.9758481]","[12.098286, 11.452006, -0.2771775]","[8.633632, 5.8456974, 10.716181]"
42,Aayush Surana;Yash Goyal;Manish Shrivastava;Suvi H Saarikallio;Vinoo Alluri,Tag2Risk: Harnessing social music tags for characterizing depression risk,2020,https://doi.org/10.5281/zenodo.4245450,"Aayush Surana+International Institute of Information Technology, Hyderabad>IND>education|University of Jyväskylä>FIN>education;Yash Goyal+International Institute of Information Technology, Hyderabad>IND>education;Manish Shrivastava+International Institute of Information Technology, Hyderabad>IND>education;Suvi Saarikallio+University of Jyväskylä>FIN>education;Vinoo Alluri+International Institute of Information Technology, Hyderabad>IND>education","Musical preferences have been considered a mirror of the self. In this age of Big Data, online music streaming services allow us to capture ecologically valid music listening behavior and provide a rich source of information to identify several user-specific aspects. Studies have shown musical engagement to be an indirect representation of internal states including internalized symptomatology and depression. The current study aims at unearthing patterns and trends in the individuals at risk for depression as it manifests in naturally occurring music listening behavior. Mental-well being scores, musical engagement measures, and listening histories of Last.fm users (N=541) were acquired. Social tags associated with each listener's most popular tracks were analyzed to unearth the mood/emotions and genres associated with the users. Results revealed that social tags prevalent in the users at risk for depression were predominantly related to emotions depicting Sadness associated with genre tags representing neo-psychedelic-, avant garde-, dream-pop. This study will open up avenues for an MIR-based approach to characterizing and predicting risk for depression which can be helpful in early detection and additionally provide bases for designing music recommendations accordingly.",IND,education,Developing economies,"[-49.475487, 3.3342931]","[54.02982, 0.7241772]","[-19.833355, 18.26303, 1.3382827]","[9.816085, 19.102192, 16.222784]","[13.647017, 12.240714]","[13.124732, 3.2922251]","[15.901021, 14.810854, 1.0936077]","[13.942153, 4.833554, 11.049096]"
43,Gabriel Meseguer Brocal;Geoffroy Peeters,Content based singing voice source separation via strong conditioning using aligned phonemes,2020,https://doi.org/10.5281/zenodo.4245560,Gabriel Meseguer-Brocal+Ircam/CNRS/SU>FRA>education;Geoffroy Peeters+Institut Polytechnique de Paris>FRA>education,"Informed source separation has recently gained renewed interest with the introduction of neural networks and the availability of large multitrack datasets containing both the mixture and the separated sources.These approaches use prior information about the target source to improve separation.Historically, Music Information Retrieval ({MIR}) researchers have focused primarily on score-informed source separation, but more recent approaches explore lyrics-informed source separation.However, because of the lack of multitrack datasets with time-aligned lyrics, models use weak conditioning with the non-aligned lyrics.In this paper, we present a multimodal multitrack dataset with lyrics aligned in time at the word level with phonetic information as well as explore strong conditioning using the aligned phonemes.Our model follows a {U-Net} architecture and takes as input both the magnitude spectrogram of a musical mixture and a matrix with aligned phoneme information.The phoneme matrix is embedded to obtain the parameters that control Feature-wise Linear Modulation ({FiLM}) layers.These layers condition the {U-Net} feature maps to adapt the separation process to the presence of different phonemes via affine transformations.We show that phoneme conditioning can be successfully applied to improve singing voice source separation.",FRA,education,Developed economies,"[-1.6439731, -43.92739]","[-37.510723, -33.930813]","[26.642996, 8.648083, -5.488363]","[-12.456479, -9.992359, -25.263172]","[9.032177, 10.687005]","[6.9081035, 5.744181]","[10.888466, 14.556465, 1.3961493]","[9.72182, 8.251666, 8.924768]"
44,Shunit Haviv Hakimi;Nadav Bhonker;Ran El-Yaniv,BebopNet: Deep neural models for personalized jazz improvisations,2020,https://doi.org/10.5281/zenodo.4245562,Shunit Haviv Hakimi+Technion – Israel Institute of Technology>ISR>education;Nadav Bhonker+Technion – Israel Institute of Technology>ISR>education;Ran El-Yaniv+Technion – Israel Institute of Technology>ISR>education,"A major bottleneck in the evaluation of music generation is that music appreciation is a highly subjective matter. When considering an average appreciation as an evaluation metric, user studies can be helpful. The challenge of generating personalized content, however, has been examined only rarely in the literature. In this paper, we address generation of personalized music and propose a novel pipeline for music generation that learns and optimizes user-specific musical taste. We focus on the task of symbol-based, monophonic, harmony-constrained jazz improvisations. Our personalization pipeline begins with BebopNet, a music language model trained on a corpus of jazz improvisations by Bebop giants. BebopNet is able to generate improvisations based on any given chord progression. We then assemble a personalized dataset, labeled by a specific user, and train a user-specific metric that reflects this user's unique musical taste. Finally, we employ a personalized variant of beam-search with BebopNet to optimize the generated jazz improvisations for that user. We present an extensive empirical study in which we apply this pipeline to extract individual models as implicitly defined by several human listeners. Our approach enables an objective examination of subjective personalized models whose performance is quantifiable. The results indicate that it is possible to model and optimize personal jazz preferences and offer a foundation for future research in personalized generation of art. We also briefly discuss opportunities, challenges, and questions that arise from our work, including issues related to creativity.",ISR,education,Developing economies,"[16.290312, 3.4387357]","[6.251157, 23.165081]","[-5.354856, -0.9573771, 29.233778]","[-14.474398, 13.725713, 6.0545664]","[10.531799, 8.914113]","[9.6592, 5.502119]","[12.991171, 12.626314, -0.31393963]","[10.39446, 5.6006994, 9.175019]"
45,Arianne N. van Nieuwenhuijsen;John Ashley Burgoyne;Frans Wiering;Mick Sneekes,A simple method for user-driven music thumbnailing,2020,https://doi.org/10.5281/zenodo.4245410,Arianne N. van Nieuwenhuijsen+Utrecht University>NLD>education;John Ashley Burgoyne+University of Amsterdam>NLD>education;Frans Wiering+Utrecht University>NLD>education;Mick Sneekes+Utrecht University>NLD>education,"More and more music is becoming available digitally, increasing the need to navigate through large numbers of audio tracks easily. One approach for improving the browsing experience is music thumbnailing: the procedure of finding a continuous fragment that can represent the whole musical piece. This paper proposes a human-centred approach to creating thumbnails based on listeners' perception, directly asking listeners to identify the most characteristic fragment. We carried out a user study to assign representativeness scores to multiple fragments from a selection of popular music tracks. To strengthen the results, we performed a replication of the same user study with new participants and a different set of music. Thereafter, we used audio features, the segmentation algorithm, and participants' overall familiarity with the songs to predict representativeness scores. The results suggest that neither segmentation nor familiarity have a significant impact on users' thumbnail preferences: even segments with starting points that pay no regard to song structure can be suitable thumbnails. Three high-level audio characteristics, however, do impact the perceived representativeness of a fragment: Raw Intensity, Melodic Conventionality, and Conventionally of Intensity. Based on these findings, we propose a new, easy-to-apply method for music thumbnailing.",NLD,education,Developed economies,"[-12.11826, 31.55319]","[32.3265, 25.115362]","[-8.982654, 13.068557, -21.530882]","[4.835355, 10.995835, 10.794755]","[13.849726, 7.20711]","[11.447542, 1.6666981]","[13.887159, 14.043277, -2.2578897]","[12.520217, 5.333681, 12.7158165]"
46,Jada E Watson,Programming inequality: Gender representation on Canadian country radio (2005-2019),2020,https://doi.org/10.5281/zenodo.4245452,Jada Watson+University of Ottawa>CAN>education,"In May 2015, a consultant for country radio revealed a decades' long practice of limiting space for songs by female artists. He encouraged program directors to avoid playing songs by women back-to-back and advocated for programming their songs at 13-15% of station playlists. His words sparked debate within the industry and drew attention to growing inequalities on radio and within the genre. The majority of these discussions have centered on US country radio, with limited attention to the growing imbalance on the format in Canada. While country format radio in both countries subscribe to a practice of gender-based programming, Canadian program directors are governed by the federal Broadcasting Act, which regulates dissemination of Canadian content. Using metadata extracted from one of the main radio monitoring services – Mediabase, this paper examines gender-related trends on Canadian country format radio between 2005 and 2019. Through data-driven analysis of Mediabase's weekly re-ports, this paper shows declining representation of songs by women on Canadian country radio and addresses the impact of Canadian content regulations on this process.",CAN,education,Developed economies,"[-47.281235, 15.880903]","[45.60949, 23.910555]","[-9.112937, 30.010773, 6.2740374]","[14.573499, 16.362688, 17.770803]","[15.232009, 8.70256]","[12.717046, 1.5856938]","[15.351581, 14.808017, -1.1902758]","[13.392721, 4.781458, 12.335125]"
47,Matan Gover;Philippe Depalle,Score-informed source separation of choral music,2020,https://doi.org/10.5281/zenodo.4245412,Matan Gover+McGill University>CAN>education;Philippe Depalle+McGill University>CAN>education,"Choral music recordings are a particularly challenging target for source separation due to the choral blend and the inherent acoustical complexity of the 'choral timbre'. Due to the scarcity of publicly available multi-track choir recordings, we create a dataset of synthesized Bach chorales. We apply data augmentation to alter the chorales so that they more faithfully represent music from a broader range of choral genres. For separation we employ Wave-U-Net, a time-domain convolutional neural network (CNN) originally proposed for vocals and accompaniment separation. We show that Wave-U-Net outperforms a baseline implemented using score-informed NMF (non-negative matrix factorization). We introduce score-informed Wave-U-Net to incorporate the musical score into the separation process. We experiment with different score conditioning methods and show that conditioning on the score leads to improved separation results. We propose a 'score-guided' model variant in which separation is guided by the score alone, bypassing the need to specify the identity of the extracted source. Finally, we evaluate our models (trained on synthetic data only) on real choir recordings and find that in the absence of a large training set of real recordings, NMF still performs better than Wave-U-Net in this setting. To our knowledge, this paper is the first to study source separation of choral music.",CAN,education,Developed economies,"[5.160899, -46.84999]","[-36.86369, -32.644234]","[31.788074, 1.6883782, -1.941211]","[-10.216706, -10.067188, -24.154963]","[8.425056, 10.160412]","[7.0133147, 5.7208333]","[10.928424, 13.7266865, 1.7525878]","[9.597426, 8.025791, 8.865204]"
48,Cynthia C. S. Liem;Chris Mostert,Can't trust the feeling? How open data reveals unexpected behavior of high-level music descriptors,2020,https://doi.org/10.5281/zenodo.4245414,Cynthia C. S. Liem+Delft University of Technology>NLD>education;Chris Mostert+Delft University of Technology>NLD>education,"Copyright restrictions prevent the widespread sharing of commercial music audio. Therefore, the availability of resharable pre-computed music audio features has become critical. In line with this, the AcousticBrainz platform offers a dynamically growing, open and community-contributed large-scale resource of locally computed low-level and high-level music descriptors. Beyond enabling research reuse, the availability of such an open resource allows for renewed reflection on the music descriptors we have at hand: while they were validated to perform successfully under lab conditions, they now are being run 'in the wild'. Their response to these more ecological conditions can shed light on the degree to which they truly had construct validity. In this work, we seek to gain further understanding into this, by analyzing high-level classifier-based music descriptor output in AcousticBrainz. While no hard ground truth is available on what the true value of these descriptors should be, some oracle information can still be derived, relying on semantic redundancies between several descriptors, and multiple feature submissions being available for the same recording. We report on multiple unexpected patterns found in the data, indicating that the descriptor values should not be taken as absolute truth, and hinting at directions for more comprehensive descriptor testing that are overlooked in common machine learning evaluation and quality assurance setups.",NLD,education,Developed economies,"[-22.165789, 17.51762]","[8.808513, 26.644056]","[-12.429259, 11.166385, -3.4866302]","[-3.7142136, -4.8670125, 16.404068]","[14.004563, 8.379582]","[9.578928, 2.9078708]","[14.270515, 14.779096, -1.5192552]","[11.143163, 6.366765, 10.963424]"
49,Mengyi Shan;Timothy Tsai,Improved handling of repeats and jumps in audio-sheet image synchronization,2020,https://doi.org/10.5281/zenodo.4245358,Mengyi Shan+Harvey Mudd College>USA>education;TJ Tsai+Harvey Mudd College>USA>education,"This paper studies the problem of automatically generating Youtube piano score following videos given an audio recording and raw sheet music images.  Whereas previous works focus on synthetic sheet music where the data has been cleaned and preprocessed, we instead focus on developing a system that can cope with the messiness of raw, unprocessed sheet music PDFs from IMSLP.  We investigate how well existing systems cope with real scanned sheet music, filler pages and unrelated pieces or movements, and discontinuities due to jumps and repeats.  We find that a significant bottleneck in system performance is handling jumps and repeats correctly.  In particular, we find that a previously proposed Jump DTW algorithm does not perform robustly when jump locations are unknown a priori.  We propose a novel alignment algorithm called Hierarchical DTW that can handle jumps and repeats even when jump locations are not known.  It first performs alignment at the feature level on each sheet music line, and then performs a second alignment at the segment level.  By operating at the segment level, it is able to encode domain knowledge about how likely a particular jump is.  Through carefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we show that Hierarachical DTW significantly outperforms Jump DTW in handling various types of jumps.",USA,education,Developed economies,"[30.411142, 8.923913]","[-14.853796, -16.944601]","[6.30037, -13.922357, -14.9423275]","[-3.983723, -23.075018, -3.6489866]","[10.911836, 6.078561]","[6.1868277, 0.5841446]","[12.010768, 12.260142, -1.8234255]","[8.1311455, 5.7175293, 10.821664]"
50,Shahan Nercessian,Zero-shot singing voice conversion,2020,https://doi.org/10.5281/zenodo.4245370,"Shahan Nercessian+iZotope, Inc.>USA>company","In this paper, we propose the use of speaker embedding networks to perform zero-shot singing voice conversion, and suggest two architectures for its realization.  The use of speaker embedding networks not only enables the capability to adapt to new voices on-the-fly, but also allows for model training on unlabeled data.  This not only facilitates the collection of suitable singing voice data, but also allows networks to be pretrained on large speech corpora before being refined on singing voice datasets, improving network generalization.  We illustrate the effectiveness of the proposed zero-shot singing voice conversion algorithms by both qualitative and quantitative means.",USA,company,Developed economies,"[-0.47086006, -37.123264]","[-29.596588, -44.31213]","[24.19701, 4.897535, -13.07017]","[1.5292345, -10.391022, -23.733706]","[9.546271, 10.895137]","[7.7406735, 4.7933]","[11.010962, 14.813908, 0.9856668]","[10.31484, 7.4122796, 8.826787]"
51,Fabrizio Pedersoli;Masataka Goto,Dance beat tracking from visual information alone,2020,https://doi.org/10.5281/zenodo.4245456,Fabrizio Pedersoli+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We propose and explore the novel task of dance beat tracking, which can be regarded as a fundamental topic in the Dance Information Retrieval (DIR) research field. Dance beat tracking aims at detecting musical beats from a dance video by using its visual information without using its audio information (i.e., dance music). The visual analysis of dances is important to achieve general machine understanding of dances, not limited to dance music. As a sub-area of Music Information Retrieval (MIR) research, DIR also shares similar goals with MIR and needs to extract various high-level semantics from dance videos. While audio-based beat tracking has been thoroughly studied in MIR, there has not been visual-based beat tracking for dance videos.We approach dance beat tracking as a time series classification problem and conduct several experiments using a Temporal Convolutional Neural Network (TCN) using the AIST Dance Video Database. We evaluate the proposed solution considering different data splits based on either ""dancer"" or ""music"". Moreover, we propose a periodicity-based loss that considerably improves the overall beat tracking performance according to several evaluation metrics.",JPN,facility,Developed economies,"[33.77745, -34.516174]","[-27.02649, -14.091628]","[9.609774, -31.223457, -8.593316]","[-11.2802, 13.456938, -18.729761]","[10.473643, 4.289582]","[5.0886254, 2.2223094]","[10.19655, 12.901643, -2.236957]","[7.686698, 6.769966, 10.536524]"
52,Ke Chen;Cheng-i Wang;Taylor Berg-Kirkpatrick;Shlomo Dubnov,Music SketchNet: Controllable music generation via factorized representations of pitch and rhythm,2020,https://doi.org/10.5281/zenodo.4245372,"Ke Chen+CREL>USA>facility|UC San Diego>USA>education;Cheng-i Wang+Smule, Inc>USA>company;Taylor Berg-Kirkpatrick+UC San Diego>USA>education;Shlomo Dubnov+CREL>USA>facility|UC San Diego>USA>education","Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete monophonic musical pieces, conditioned on surrounding context, and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architectures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process.",USA,facility,Developed economies,"[22.108034, 7.070097]","[-10.52087, -44.088333]","[1.3038088, 1.6348042, 22.942923]","[-17.559982, 0.3636772, -14.215964]","[10.367656, 8.435555]","[8.770232, 6.401901]","[13.407065, 11.815499, 0.17860919]","[9.706615, 5.6395273, 8.602923]"
53,Jin Ha Lee;Anh Thu Nguyen,How music fans shape commercial music services: A case study of BTS and ARMY,2020,https://doi.org/10.5281/zenodo.4245564,Jin Ha Lee+University of Washington>USA>education;Anh Thu Nguyen+University of Washington>USA>education,"Much of the existing research on user aspects in the music information retrieval field tends to focus on general user needs or behavior related to music information seeking, music listening and sharing, or other use of commercial music services. However, we have a limited understanding of the personal and social contexts of music fans who enthusiastically support musicians and are often avid users of commercial music services. In this study, we aim to better understand the contextual complexities surrounding music fans through a case study of the group BTS and its fan community, ARMY. In particular, we are interested in discovering factors that influence the interactions of music fans with music services, especially in the current environment where the prevalence of social media and other tools/technologies influences musical enjoyment. Through virtual ethnography and content analysis, we identified four factors that affect music fans' interactions with commercial music services: 1) perception of music genres, 2) participatory fandom, 3) desire for agency and transparency, and 4) importance of non-musical factors. The discussion of each aspect is followed by design implications for commercial music services to consider.",USA,education,Developed economies,"[-36.37147, 25.131367]","[41.79919, 31.78703]","[-22.456987, 19.402428, -9.733048]","[9.5788145, 16.477695, 18.01387]","[15.17794, 8.511598]","[12.951402, 1.0112793]","[15.261535, 15.041333, -1.6638284]","[13.381966, 4.2436643, 11.952881]"
54,Avriel C Epps-Darling;Henriette Cramer;Romain Takeo Bouyer,Artist gender representation in music streaming,2020,https://doi.org/10.5281/zenodo.4245416,Avriel Epps-Darling+Harvard University>USA>education;Romain Takeo Bouyer+Spotify>USA>company;Henriette Cramer+Spotify>USA>company,"This study examines gender representation in current music streaming, utilizing one of the world's largest streaming services. First, we found listeners generally stream fewer female or mixed-gender creator groups than male artists, with differences per genre. Second, while still relatively low, we found that recommendation-based streaming has a slightly higher proportion of female creators than ""organic"" listening (i.e., tracks that are not recommended by editors or algorithms). Third, we examined streaming data from 200,000 US users to determine the proportion of female artists in organic and recommended streams over a 28-day period and the relationship between recommended streams and users' future organic listening. The proportion of female artists in recommended streaming appears predictive of the proportion of female artists in organic streaming; these effects are moderated by gender and age. Fourth, this study also samples creators across different popularity levels, seeing more female and multi-gender groups at lower levels than in the middle tiers. However, solo female artists are better represented again in the superstars category, suggesting influence of selected superstars and genres. We conclude by discussing potential avenues in algorithmic auditing.",USA,education,Developed economies,"[-46.429234, 15.907379]","[45.540497, 23.956642]","[-9.737238, 29.10706, 4.4946637]","[13.729674, 15.998794, 17.733248]","[15.221814, 8.717873]","[12.831715, 1.4285681]","[15.352331, 14.840377, -1.1513819]","[13.418042, 4.7069383, 12.369558]"
55,Filip Korzeniowski;Oriol Nieto;Matthew McCallum;Minz Won;Sergio Oramas;Erik Schmidt,Mood classification using listening data,2020,https://doi.org/10.5281/zenodo.4245488,"Filip Korzeniowski+Pandora Media LLC.>USA>company;Oriol Nieto+Pandora Media LLC.>USA>company;Matthew C. McCallum+Pandora Media LLC.>USA>company;Minz Won+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sergio Oramas+Pandora Media LLC.>USA>company;Erik M. Schmidt+Netflix Inc.>USA>company","The mood of a song is a highly relevant feature for exploration and recommendation in large collections of music. These collections tend to require automatic methods for predicting such moods. In this work, we show that listening-based features outperform content-based ones when classifying moods: embeddings obtained through matrix factorization of listening data appear to be more informative of a track mood than embeddings based on its audio content. To demonstrate this, we compile a subset of the Million Song Dataset, totalling 67k tracks, with expert annotations of 188 different moods collected from AllMusic. Our results on this novel dataset not only expose the limitations of current audio-based models, but also aim to foster further reproducible research on this timely topic",USA,company,Developed economies,"[-54.773518, 2.0311828]","[50.196304, -4.0556016]","[-19.9041, 23.566534, 8.140561]","[14.075565, 19.441582, 6.3367825]","[13.479424, 12.614808]","[12.968211, 3.708985]","[16.174438, 14.9091835, 1.5836741]","[14.12733, 5.2421103, 10.739743]"
32,Andrea Vaglio;Romain Hennequin;Manuel Moussallam;Gael Richard;Florence d'Alché-Buc,Multilingual lyrics-to-audio alignment,2020,https://doi.org/10.5281/zenodo.4245484,"Andrea Vaglio+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education|Deezer R&D>FRA>company;Romain Hennequin+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education|Deezer R&D>FRA>company;Manuel Moussallam+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Gaël Richard+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Florence d’Alché-Buc+LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education","Lyrics-to-audio alignment methods have recently reported impressive results, opening the door to practical applications such as karaoke and within song navigation. However, most studies focus on a single language - usually English - for which annotated data are abundant. The question of their ability to generalize to other languages, especially in low (or even zero) training resource scenarios has been so far left unexplored. In this paper, we address the lyrics-to-audio alignment task in a generalized multilingual setup. More precisely, this investigation presents the first (to the best of our knowledge) attempt to create a language-independent lyrics-to-audio alignment system. Building on a RNN model trained with a CTC algorithm, we study the relevance of different intermediate representations, either character or phoneme, along with several strategies to design a training set. The evaluation is conducted on multiple languages with a varying amount of data available, from plenty to zero. Results show that learning from diverse data and using a universal phoneme set as an intermediate representation yield the best generalization performances.",FRA,education,Developed economies,"[-27.788765, -34.274426]","[-10.690488, -22.626692]","[10.953465, 18.777699, -1.8145634]","[2.969085, -2.3428404, -20.64165]","[11.209467, 11.790689]","[8.334012, 4.617558]","[12.1838875, 15.818172, 1.1412237]","[10.538935, 7.092957, 9.179792]"
31,Lisa Kawai;Philippe Esling;Tatsuya Harada,Attributes-aware deep music transformation,2020,https://doi.org/10.5281/zenodo.4245520,Lisa Kawai+The University of Tokyo>JPN>education;Philippe Esling+IRCAM>FRA>Unknown;Tatsuya Harada+The University of Tokyo>JPN>education|RIKEN>JPN>facility,"Recent machine learning techniques have enabled a large variety of novel music generation processes. However, most approaches do not provide any form of interpretable control over musical attributes, such as pitch and rhythm. Obtaining control over the generation process is critically important for its use in real-life creative setups. Nevertheless, this problem remains arduous, as there are no known functions nor differentiable approximations to transform symbolic music with control of musical attributes.In this work, we propose a novel method that enables attributes-aware music transformation from any set of musical annotations, without requiring complicated derivative implementation. By relying on an adversarial confusion criterion on given musical annotations, we force the latent space of a generative model to abstract from these features. Then, reintroducing these features as conditioning to the generative function, we obtain a continuous control over them. To demonstrate our approach, we rely on sets of musical attributes computed by the jSymbolic library as annotations and conduct experiments that show that our method outperforms previous methods in control. Finally, comparing correlations between attributes and the transformed results show that our method can provide explicit control over any continuous or discrete annotation.",JPN,education,Developed economies,"[23.385391, 0.91099423]","[-10.986083, -41.99718]","[7.127421, 6.1927886, 12.810168]","[-20.277905, 1.685278, -13.00921]","[10.877117, 8.846083]","[9.040896, 6.4916186]","[13.350928, 12.612679, 0.48715192]","[9.60265, 5.438454, 8.636425]"
30,Yudhik Agrawal;Samyak Jain;Emily Carlson;Petri Toiviainen;Vinoo Alluri,Towards multimodal MIR: Predicting individual differences from music-induced movement,2020,https://doi.org/10.5281/zenodo.4245368,"Yudhik Agrawal+Cognitive Science Lab, International Institute of Information Technology>IND>education;Samyak Jain+Cognitive Science Lab, International Institute of Information Technology>IND>education;Emily Carlson+University of Jyväskylä>FIN>education;Petri Toiviainen+University of Jyväskylä>FIN>education;Vinoo Alluri+Cognitive Science Lab, International Institute of Information Technology>IND>education","As the field of Music Information Retrieval grows, it is important to take into consideration the multi-modality of music and how aspects of musical engagement such as movement and gesture might be taken into account. Bodily movement is universally associated with music and reflective of important individual features related to music preference such as personality, mood, and empathy. Future multimodal MIR systems may benefit from taking these aspects into account. The current study addresses this by identifying individual differences, specifically Big Five personality traits, and scores on the Empathy and Systemizing Quotients (EQ/SQ) from participants' free dance movements. Our model successfully explored the unseen space for personality as well as EQ, SQ, which has not previously been accomplished for the latter. R2 scores for personality, EQ, and SQ were 76.3%, 77.1%, and 86.7% respectively. As a follow-up, we investigated which bodily joints were most important in defining these traits. We discuss how further research may explore how the mapping of these traits to movement patterns can be used to build a more personalized, multi-modal recommendation system, as well as potential therapeutic applications.",IND,education,Developing economies,"[-13.844623, 54.030098]","[9.338701, 45.130264]","[-34.12866, 1.686301, 1.6535391]","[-3.1441147, 15.549079, 14.496445]","[13.249576, 5.39956]","[11.543912, 0.9142673]","[14.624567, 11.733728, -1.3389764]","[11.821305, 4.553535, 11.426609]"
29,Ziyu Wang;Yiyi Zhang;Yixiao Zhang;Junyan Jiang;Ruihan Yang;Gus Xia;Junbo Zhao,PianoTree VAE: Structured representation learning for polyphonic music,2020,https://doi.org/10.5281/zenodo.4245446,Ziyu Wang+Music X Lab>USA>facility|New York University>USA>education;Yiyi Zhang+Center for Data Science>USA>education|New York University>USA>education;Yixiao Zhang+Music X Lab>USA>facility|New York University>USA>education;Junyan Jiang+Music X Lab>USA>facility|New York University>USA>education;Ruihan Yang+Music X Lab>USA>facility|New York University>USA>education;Junbo Zhao (Jake)+Computer Science Department>CHN>education|Zhejiang University>CHN>education;Gus Xia+Music X Lab>USA>facility|New York University>USA>education,"The dominant approach for music representation learning involves the deep unsupervised model family variational autoencoder (VAE). However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning. The experiments prove the validity of the PianoTree VAE via (i)-semantically meaningful latent code for polyphonic segments; (ii)-more satisfiable reconstruction aside of decent geometry learned in the latent space; (iii)-this model's benefits to the variety of the downstream music generation.",USA,facility,Developed economies,"[21.65863, -4.8697505]","[-10.212905, -45.469177]","[3.8223867, 8.075587, 12.310837]","[-16.983557, 3.7535632, -14.3284]","[10.893688, 8.45117]","[8.751115, 6.432505]","[13.029385, 12.769837, 0.17197986]","[9.549795, 5.563784, 8.560016]"
5,Michael Krause;Frank Zalkow;Christof Weiss;Meinard Müller,Classifying leitmotifs in recordings of operas by Richard Wagner,2020,https://doi.org/10.5281/zenodo.4245472,Michael Krause+International Audio Laboratories Erlangen>DEU>facility;Frank Zalkow+International Audio Laboratories Erlangen>DEU>facility;Julia Zalkow+International Audio Laboratories Erlangen>DEU>facility;Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"From the 19th century on, several composers of Western opera made use of leitmotifs (short musical ideas referring to semantic entities such as characters, places, items, or feelings) for guiding the audience through the plot and illustrating the events on stage. A prime example of this compositional technique is Richard Wagner's four-opera cycle Der Ring des Nibelungen. Across its different occurrences in the score, a leitmotif may undergo considerable musical variations. The concrete leitmotif instances in an audio recording are subject to acoustic variability. Our paper approaches the task of classifying such leitmotif instances in audio recordings. As our main contribution, we conduct a case study on a dataset covering 16 recorded performances of the Ring with annotations of ten central leitmotifs, leading to 2403 occurrences and 38448 instances in total. We build a neural network classification model and evaluate its ability to generalize across different performances and leitmotif occurrences. Our findings demonstrate the possibilities and limitations of leitmotif classification in audio recordings and pave the way towards the fully automated detection of leitmotifs in music recordings.",DEU,facility,Developed economies,"[4.7347655, 25.59792]","[-17.827744, 13.259795]","[-12.589374, -11.203092, -3.0673065]","[-10.358076, 2.5579748, 1.4269512]","[12.554981, 7.1869817]","[8.269757, 2.4204254]","[13.1312065, 13.4930105, -1.6771059]","[9.964367, 6.6435876, 11.410318]"
6,Timothy Tsai;Kevin Ji,Composer style classification of piano sheet music images using language model pretraining,2020,https://doi.org/10.5281/zenodo.4245398,TJ Tsai+Harvey Mudd College>USA>education;Kevin Ji+Harvey Mudd College>USA>education,"This paper studies composer style classification of piano sheet music images.  Previous approaches to the composer classification task have been limited by a scarcity of data.  We address this issue in two ways: (1) we recast the problem to be based on raw sheet music images rather than a symbolic music format, and (2) we propose an approach that can be trained on unlabeled data.  Our approach first converts the sheet music image into a sequence of musical ``words"" based on the bootleg feature representation, and then feeds the sequence into a text classifier.  We show that it is possible to significantly improve classifier performance by first training a language model on a set of unlabeled data, initializing the classifier with the pretrained language model weights, and then finetuning the classifier on a small amount of labeled data.  We train AWD-LSTM, GPT-2, and RoBERTa language models on all piano sheet music images in IMSLP.  We find that transformer-based architectures outperform CNN and LSTM models, and pretraining boosts classification accuracy for the GPT-2 model from 46% to 70% on a 9-way classification task.  The trained model can also be used as a feature extractor that projects piano sheet music into a feature space that characterizes compositional style.",USA,education,Developed economies,"[-5.1198897, -15.583064]","[-21.315739, -27.834309]","[17.863153, 6.117685, 19.411757]","[-13.886908, -10.815904, -9.11403]","[10.431452, 7.022817]","[8.355319, 4.7510014]","[12.3215475, 11.8734255, -0.72968864]","[9.470196, 6.3892584, 9.5698185]"
7,Frank Zalkow;Meinard Müller,Using weakly aligned score–audio pairs to train deep chroma models for cross-modal music retrieval,2020,https://doi.org/10.5281/zenodo.4245400,Frank Zalkow+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Many music information retrieval tasks involve the comparison of a symbolic score representation with an audio recording. A typical strategy is to compare score–audio pairs based on a common mid-level representation, such as chroma features. Several recent studies demonstrated the effectiveness of deep learning models that learn task-specific mid-level representations from temporally aligned training pairs. However, in practice, there is often a lack of strongly aligned training data, in particular for real-world scenarios. In our study, we use weakly aligned score–audio pairs for training, where only the beginning and end of a score excerpt is annotated in an audio recording, without aligned correspondences in between. To exploit such weakly aligned data, we employ the Connectionist Temporal Classification (CTC) loss to train a deep learning model for computing an enhanced chroma representation. We then apply this model to a cross-modal retrieval task, where we aim at finding relevant audio recordings of Western classical music, given a short monophonic musical theme in symbolic notation as a query. We present systematic experiments that show the effectiveness of the CTC-based model for this theme-based retrieval task.",DEU,facility,Developed economies,"[-8.839047, 23.616528]","[-24.473402, -31.25337]","[-4.40538, 14.283844, -14.906385]","[-9.924243, -13.693939, -15.74126]","[13.1986265, 8.24118]","[8.581714, 5.1984525]","[13.578601, 14.369014, -1.4742918]","[9.8140745, 6.5291057, 8.8988905]"
8,Daniel Yang;Timothy Tsai,Camera-based piano sheet music identification,2020,https://doi.org/10.5281/zenodo.4245476,Daniel Yang+Harvey Mudd College>USA>education;TJ Tsai+Harvey Mudd College>USA>education,"This paper presents a method for large-scale retrieval of piano sheet music images.  Our work differs from previous studies on sheet music retrieval in two ways.  First, we investigate the problem at a much larger scale than previous studies, using all solo piano sheet music images in the entire IMSLP dataset as a searchable database.  Second, we use cell phone images of sheet music as our input queries, which lends itself to a practical, user-facing application.  We show that a previously proposed fingerprinting method for sheet music retrieval is far too slow for a real-time application, and we diagnose its shortcomings.  We propose a novel hashing scheme called dynamic n-gram fingerprinting that significantly reduces runtime while simultaneously boosting retrieval accuracy.  In experiments on IMSLP data, our proposed method achieves a mean reciprocal rank of 0.85 and an average runtime of 0.98 seconds per query.",USA,education,Developed economies,"[34.070255, 7.660174]","[19.422834, 15.399058]","[22.069002, 6.0001574, 19.384169]","[13.974724, -10.943018, 9.236457]","[10.379392, 6.8858824]","[8.798816, 0.12102027]","[12.224912, 11.914968, -1.1186546]","[10.543447, 5.5427203, 13.059991]"
9,Louis Spinelli;Josephine Lau;Jin Ha Lee,User perceptions underlying social music behavior,2020,https://doi.org/10.5281/zenodo.4245474,Louis Spinelli+University of Washington>USA>education;Josephine Lau+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"While prior studies investigating the social aspects of music provide a landscape of users' various social behaviors around commercial music services (CMS), there remains a lack in understanding of users' perceptions and value judgments underlying these behaviors. Specifically, there is more to learn about what influences and behaviors individual music users perceive as meaningful in social contexts. We used the Q methodology to explore which behaviors and influences are important to CMS users and why. We extracted two factors that explain the two different viewpoints shared by groups of music users, focusing on how they perceive the meaning and value of different social music behavior and interactions. From these findings, we then revise an existing social music coding dictionary and interaction model and offer new CMS design insights.",USA,education,Developed economies,"[-37.630627, 22.270117]","[41.420597, 32.625107]","[-19.15896, 19.743057, -3.8444722]","[8.70528, 16.321728, 19.851751]","[15.168551, 8.911726]","[12.901154, 0.94320047]","[15.18009, 15.22842, -1.352435]","[13.35459, 4.2075667, 11.935263]"
10,Yuchen Yuan;Sho Oishi;Charles Cronin;Daniel Müllensiefen;Quentin Atkinson;Shinya Fujii;Patrick E. Savage,Perceptual vs. automated judgements of music copyright infringement,2020,https://doi.org/10.5281/zenodo.4245364,"Yuchen Yuan+Keio University>JPN>education;Sho Oishi+Keio University>JPN>education;Charles Cronin+George Washington University Law School>USA>education;Daniel Müllensiefen+Goldsmiths, University of London>GBR>education;Quentin Atkinson+University of Auckland>NZL>education;Shinya Fujii+Keio University>JPN>education;Patrick E. Savage+Keio University>JPN>education","Music copyright lawsuits often result in multimillion dollar damage awards or settlements, yet there are few objective guidelines for applying copyright law in in-fringement claims involving musical works. Recent re-search has attempted to develop objective methods based on automated similarity algorithms, but there remains almost no data on the role of perceived similarity in mu-sic copyright decisions despite its crucial role in copy-right law. We collected perceptual data from 20 partici-pants for 17 adjudicated copyright cases from the USA and Japan after editing the disputed sections to contain either full audio, melody only, or lyrics only. Due to the historical emphasis in legal opinions on melody as the key criterion for deciding infringement, we predicted that listening to melody-only versions would result in percep-tual judgements that more closely matched actual past legal decisions. Surprisingly, however, we found no sig-nificant differences between the three conditions, with participants matching past decisions in between 50-60% of cases in all three conditions. Automated algorithms designed to calculate melodic and audio similarity pro-duced comparable results: both algorithms were able to match past decisions with identical accuracy of 71% (12/17 cases). Analysis of cases that were difficult to classify suggests that melody, lyrics, and other factors sometimes interact in complex ways difficult to capture using quantitative metrics. We propose directions for fur-ther investigation of the role of similarity in music copy-right law using larger and more diverse samples of cases and enhanced methods, and adapting our perceptual ex-periment method to avoid relying for ground truth data only on court decisions (which may be subject to selec-tion bias). Our results contribute to important practical debates, such as whether jury members should be allowed to listen to full audio recordings during copyright cases.",JPN,education,Developed economies,"[-2.8348033, 19.267487]","[19.267399, 4.0726976]","[-1.4042224, 2.5808473, 4.6156187]","[9.792399, 3.1679547, 5.476004]","[12.912339, 9.559942]","[10.7162895, 2.2631872]","[13.450256, 15.424165, -0.8356328]","[11.6733675, 6.887306, 12.982268]"
11,Claire Savard;Erin H Bugbee;Melissa R McGuirl;Katherine M. Kinnaird,SuPP & MaPP: Adaptable structure-based representations for MIR tasks,2020,https://doi.org/10.5281/zenodo.4245438,Claire Savard+University of Colorado-Boulder>USA>education;Erin H. Bugbee+Brown University>USA>education;Melissa R. McGuirl+Brown University>USA>education;Katherine M. Kinnaird+Smith College>USA>education,"Accurate and flexible representations of music data are paramount to addressing MIR tasks, yet many of the existing approaches are difficult to interpret or rigid in nature. This work introduces two new song representations for structure-based retrieval methods: Surface Pattern Preservation (SuPP), a continuous song representation, and Matrix Pattern Preservation (MaPP), SuPP's discrete counterpart. These representations come equipped with several user-defined parameters so that they are adaptable for a range of MIR tasks. Experimental results show MaPP as successful in addressing the cover song task on a set of Mazurka scores, with a mean precision of 0.965 and recall of 0.776. SuPP and MaPP also show promise in other MIR applications, such as novel-segment detection and genre classification, the latter of which demonstrates their suitability as inputs for machine learning problems.",USA,education,Developed economies,"[-9.765396, 56.50394]","[3.7665613, 21.589195]","[-38.573326, -3.5621252, -3.9655588]","[-11.221341, 13.394896, 16.967085]","[13.628643, 4.674944]","[9.4524975, 1.3861877]","[15.046731, 11.078252, -1.5230415]","[10.666416, 5.6852045, 10.984229]"
12,Yaolong Ju;Sylvain Margot;Cory McKay;Luke Dahn;Ichiro Fujinaga,Automatic figured bass annotation using the new bach chorales figured bass dataset,2020,https://doi.org/10.5281/zenodo.4245512,Yaolong Ju+McGill University>CAN>education;Sylvain Margot+McGill University>CAN>education;Cory McKay+Marianopolis College>CAN>education;Luke Dahn+The University of Utah>USA>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper focuses on the computational study of figured bass, which remains an under-researched topic in MIR, likely due to a lack of machine-readable datasets. First, we introduce the Bach Chorales Figured Bass dataset (BCFB), a collection of 139 chorales composed by Johann Sebastian Bach that includes both the original music and figured bass annotations encoded in MusicXML, **kern, and MEI formats. We also present a comparative study on automatic figured bass annotation using both rule-based and machine learning approaches, which respectively achieved classification accuracies of 85.3% and 85.9% on BCFB. Finally, we discuss promising areas for MIR research involving figured bass, including automatic harmonic analysis.",CAN,education,Developed economies,"[17.750578, -2.5598989]","[-10.399452, 27.347502]","[0.23297499, -13.959303, 25.695415]","[-8.22866, -12.904383, 8.542344]","[10.6656, 9.063679]","[8.659559, 2.7214746]","[12.151297, 13.547104, -0.53237593]","[10.254592, 6.25232, 10.790812]"
13,Phillip B Kirlin,A corpus-based analysis of syncopated patterns in ragtime,2020,https://doi.org/10.5281/zenodo.4245514,Phillip B. Kirlin+Rhodes College>USA>education,"In this paper, we build on and extend a number of previous studies of rhythmic patterns that occur in ragtime music.  All of these studies have used the RAG-C dataset of approximately 11,000 symbolically-encoded ragtime pieces to identify salient rhythmic patterns in the corpus and  qualify how they are used.  Ragtime music is distinguished from other musical genres by frequent use of syncopation, and previous computational studies have confirmed a number of musicological hypotheses regarding the use of syncopated patterns in ragtime compositions.  In this work, we extend these studies to investigate further questions involving the use of syncopation.  Specifically, we introduce a new methodological framework for processing the RAG-C dataset and confirm that experiments from previous studies obtain similar results using the new methodology.  We investigate the use of the common ``short-long-short'' syncopated pattern in different time periods and present new results detailing its use by three well-known ragtime composers.  We describe how the use of other syncopated patterns has evolved over time and the different distributions of patterns that result from those changes.  Lastly, we present novel results identifying statistically significant patterns in the way composers varied the amount of syncopation in consecutive measures in compositions.",USA,education,Developed economies,"[46.92891, 11.201044]","[-17.352766, 8.670788]","[-12.7324505, -24.99218, 0.013133774]","[-7.9280005, 14.281604, 2.5165658]","[12.02995, 5.3438025]","[6.756494, 1.3759238]","[11.323061, 14.346519, -2.1006713]","[8.70021, 6.71423, 12.275976]"
14,Florian Thalmann;Kazuyoshi Yoshii;Thomas Wilmering;Wiggins Geraint;Mark B. Sandler,A method for analysis of shared structure in large music collections using techniques from genetic sequencing and graph theory,2020,https://doi.org/10.5281/zenodo.4245440,Florian Thalmann+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Thomas Wilmering+Queen Mary University of London>GBR>education;Geraint A. Wiggins+Vrije Universiteit Brussel>BEL>education;Mark B. Sandler+Queen Mary University of London>GBR>education,"While common approaches to automatic structural analysis of music typically focus on individual audio files, our approach collates audio features of large sets of related files in order to find a shared musical temporal structure. The content of each individual file and the differences between them can then be described in relation to this shared structure. We first construct a large similarity graph of temporal segments, such as beats or bars, based on self-alignments and selected pair-wise alignments between the given input files. Part of this graph is then partitioned into groups of corresponding segments using multiple sequence alignment. This partitioned graph is searched for recurring sections which can be organized hierarchically based on their co-occurrence. We apply our approach to discover shared harmonic structure in a dataset containing a large number of different live performances of a number of songs. Our evaluation shows that using the joint information from a number of files has the advantage of evening out the noisiness or inaccuracy of the underlying feature data and leads to a robust estimate of shared musical material.",JPN,education,Developed economies,"[-3.4554813, 2.7499459]","[-0.27000338, 1.3911735]","[-0.23898165, -3.4392693, -0.70321196]","[3.5987422, 0.48032537, -2.2576063]","[12.010513, 8.282448]","[8.12799, 2.6530845]","[12.797493, 13.854386, -0.3985973]","[10.57006, 7.4656115, 11.714684]"
15,Woosung Choi;Minseok Kim;Jaehwa Chung;Daewon Lee;Soonyoung Jung,Investigating U-Nets with various intermediate blocks for spectrogram-based singing voice separation,2020,https://doi.org/10.5281/zenodo.4245404,Woosung Choi+Korea University>KOR>education;Minseok Kim+Korea University>KOR>education;Jaehwa Chung+Korea National Open University>KOR>education;Daewon Lee+Seokyeong University>KOR>education;Soonyoung Jung+Korea University>KOR>education,"Singing Voice Separation (SVS) tries to separate singing voice from a given mixed musical signal. Recently, many U-Net-based models have been proposed for the SVS task, but there were no existing works that evaluate and compare various types of intermediate blocks that can be used in the U-Net architecture. In this paper, we introduce a variety of intermediate spectrogram transformation blocks. We implement U-nets based on these blocks and train them on complex-valued spectrograms to consider both magnitude and phase. These networks are then compared on the SDR metric. When using a particular block composed of convolutional and fully-connected layers, it achieves state-of-the-art SDR on the MUSDB singing voice separation task by a large margin of 0.9 dB. Our code and models are available online.",KOR,education,Developing economies,"[-1.646204, -42.789562]","[-40.95568, -34.570713]","[28.926685, 7.474463, -7.685166]","[-15.956007, -8.03696, -26.359495]","[8.98144, 10.643329]","[6.844476, 5.8888245]","[10.863326, 14.392502, 1.4610453]","[9.653605, 8.241697, 8.755562]"
56,Chang-Bin Jeon;Hyeong-Seok Choi;Kyogu Lee,Exploring aligned lyrics-informed singing voice separation,2020,https://doi.org/10.5281/zenodo.4245526,Chang-Bin Jeon+Seoul National University>KOR>education;Hyeong-Seok Choi+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"In this paper, we propose a method of utilizing aligned lyrics as additional information to improve the performance of singing voice separation. We have combined the highway network-based lyrics encoder into Open-unmix separation network and show that the model trained with the aligned lyrics indeed results in a better performance than the model that was not informed. The question now remains whether the increase of performance is actually due to the phonetic contents that lie in the informed aligned lyrics or not. To this end, we investigated the source of performance increase in multifaceted ways by observing the change of performance when incorrect lyrics were given to the model. Experiment results show that the model can use not only just vocal activity information but also the phonetic contents from the aligned lyrics.",KOR,education,Developing economies,"[-2.2407892, -44.075714]","[-37.12521, -34.42098]","[25.437012, 9.517256, -4.962588]","[-12.992075, -11.18206, -25.625288]","[10.874871, 11.6860075]","[7.0170035, 5.4710984]","[11.018697, 14.697682, 1.4027721]","[9.881388, 8.099578, 9.014496]"
16,Matevž Pesek;Lovro Suhadolnik;Peter Šavli;Matija Marolt,The rhythmic dictator: Does gamification of rhythm dictation exercises help?,2020,https://doi.org/10.5281/zenodo.4245478,Matevž Pesek+University of Ljubljana>SVN>education;Lovro Suhadolnik+University of Ljubljana>SVN>education;Peter Šavli+Conservatory of Music and Ballet Ljubljana>SVN>education;Matija Marolt+University of Ljubljana>SVN>education,"We present the development and evaluation of a gamified rhythmic dictation application for music theory learning. The application's focus is on mobile accessibility and user experience, so it includes intuitive controls for input of rhythmic exercises, a responsive user interface, several gamification elements and a flexible exercise generator. We evaluated the rhythmic dictation application with conservatory-level music theory students through A/B testing, to assess their engagement and performance. The results show a significant impact of the application on the students' exam scores.",SVN,education,Developed economies,"[45.998283, 6.2128606]","[-42.022953, 7.2291584]","[-7.026744, -26.55954, 4.7700186]","[-12.542213, 18.97616, 0.32764333]","[12.058816, 5.354523]","[7.9637465, 3.6748757]","[11.280189, 14.18028, -2.2514148]","[9.502154, 6.025508, 10.552972]"
18,Yunpeng Li;Marco Tagliasacchi;Beat Gfeller;Dominik Roblek,Learning to denoise historical music,2020,https://doi.org/10.5281/zenodo.4245480,Yunpeng Li+Google>USA>company;Beat Gfeller+Google>USA>company;Marco Tagliasacchi+Google>USA>company;Dominik Roblek+Google>USA>company,"We propose an audio-to-audio generative model that learns to denoise old music recordings. Our model internally converts its input into a time-frequency representation by means of a short-time Fourier transform (STFT), and processes the resulting complex spectrogram using a convolutional neural network. The network is trained with both reconstruction and adversarial objectives on a synthetic noisy music dataset, which is created by mixing clean music with real noise samples extracted from quiet segments of old recordings. We evaluate our method quantitatively on held-out test examples of the synthetic dataset, and qualitatively by human rating on samples of actual historical recordings. Our results show that the proposed method is effective in removing noise, while preserving the quality and details of the original music.",USA,company,Developed economies,"[3.4406276, 26.760738]","[-22.541338, -47.428932]","[8.849453, 14.451734, 13.30367]","[-20.34445, -4.0788403, -21.99119]","[12.13106, 7.575139]","[7.9204, 6.5153103]","[13.228768, 13.531715, -1.4752725]","[9.61731, 6.2159243, 8.089141]"
19,Christof Weiss;Stephanie Klauk;Mark R H Gotham;Meinard Müller;Rainer Kleinertz,Discourse not dualism: An interdisciplinary dialogue on sonata form in Beethoven's early piano sonatas,2020,https://doi.org/10.5281/zenodo.4245402,"Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Stephanie Klauk+Institut für Musikwissenschaft, Saarland University>DEU>education;Mark Gotham+Institut für Musikwissenschaft, Saarland University>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Rainer Kleinertz+Institut für Musikwissenschaft, Saarland University>DEU>education","The computational analysis of music has traditionally seen a sharp divide between the ""audio approach"" relying on signal processing and the ""symbolic approach"" based on scores. Likewise, there has also been an unfortunate gap between any such computational endeavour and more traditional approaches as used in historical musicology. In this paper, we take a step towards ameliorating this situation through the application of a computational method for visualizing local key characteristics in audio recordings. We exploit these visualizations of diatonic scale content by discussing their musicological implications, being aware of methodological limitations as for the case of minor keys. As a proof of concept, we use this method for investigating differences between the traditional sonata-form model and selected Beethoven piano sonatas in the context of sonata theory from the end of the 18th century. We consider this scenario as an example for a rewarding dialogue between computer science and historical musicology.",DEU,facility,Developed economies,"[13.543287, 7.3929]","[-16.05194, 14.966494]","[4.415191, -8.185638, 10.541435]","[-12.064566, -0.84431046, 3.1946683]","[11.05387, 7.8021207]","[8.129377, 2.1923234]","[12.753517, 12.747431, -0.5691172]","[10.013156, 6.9218845, 12.047248]"
20,Jennifer Thom;Angela Nazarian;Ruth Brillman;Henriette Cramer;Sarah Mennicken,"""Play music"": User motivations and expectations for non-specific voice queries",2020,https://doi.org/10.5281/zenodo.4245524,"Jennifer Thom+Spotify>USA>company;Angela Nazarian+University of California, Davis>USA>education;Ruth Brillman+Spotify>USA>company;Henriette Cramer+Spotify>USA>company;Sarah Mennicken+Spotify>USA>company","The growing market of voice-enabled devices introduces new types of music search requests that can be more ambiguous than in typed search interfaces as voice assistants can potentially support conversational requests. However, these systems may not be able to fulfill ambiguous requests in a manner that matches the user need.  In this work, we study an example of ambiguous requests which we term as non-specific queries (NSQs), such as ""play music,"" where users ask to stream content using a single utterance that does not specify what content they want to hear.  To better understand user motivations for making NSQs, we conducted semi-structured qualitative interviews with voice users. We observed four themes that structure user perceptions of the benefits and shortcomings of making NSQs: the tradeoff between control and convenience, varying expectations for personalization, the effects of context on expectations, and learned user behaviors. We conclude with implications for how these themes can inform the interaction design of voice search systems in handling non-specific music requests in voice search systems.",USA,company,Developed economies,"[-23.701721, 1.8536953]","[35.92405, 30.346905]","[7.336832, 13.725566, -10.352504]","[4.9868336, 10.838274, 20.419552]","[10.703658, 11.135405]","[12.393756, 0.99210775]","[11.931716, 15.569398, 0.31794396]","[12.856443, 4.573161, 12.304764]"
21,Felipe V Falcão;Nazareno Andrade;Flavio Figueiredo;Diego Furtado Silva;Fabio Morais,Measuring disruption in song similarity networks,2020,https://doi.org/10.5281/zenodo.4245356,Felipe Falcão+Universidade Federal de Campina Grande>BRA>education|Universidade Federal de Minas Gerais>BRA>education|Universidade Federal de São Carlos>BRA>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education|Universidade Federal de Minas Gerais>BRA>education|Universidade Federal de São Carlos>BRA>education;Flávio Figueiredo+Universidade Federal de Minas Gerais>BRA>education;Diego Silva+Universidade Federal de São Carlos>BRA>education;Fabio Morais+Universidade Federal de Campina Grande>BRA>education,"Investigating music with a focus on the similarity relations between songs, albums, and artists plays an important role when trying to understand trends in the history of music genres. In particular, representing these relations as a similarity network allows us to investigate the innovation presented by these entities in a multitude of points-of-view, including disruption. A disruptive object is one that creates a new stream of events, changing the traditional way of how a context usually works. The proper measurement of disruption remains as a task with large room for improvement, and these gaps are even more evident in the music domain, where the topic has not received much attention so far. This work builds on preliminary studies focused on the analysis of music disruption derived from metadata-based similarity networks, demonstrating that the raw audio can augment similarity information. We developed a case study based on a collection of a Brazilian local music tradition called Forró, that emphasizes the analytical and musicological potential of the musical disruption metric to describe and explain a genre trajectory over time.",BRA,education,Developing economies,"[-7.1575384, 11.317371]","[45.348015, 7.995439]","[-5.0107512, 14.249423, -1.9798753]","[13.079864, 10.202485, 9.152605]","[13.34058, 9.26041]","[11.978524, 2.708049]","[13.810165, 14.908802, -0.4919715]","[13.096931, 5.607979, 11.835691]"
22,Axel Marmoret;Jeremy Cohen;Frédéric Bimbot;Nancy Bertin,Uncovering audio patterns in music with Nonnegative Tucker Decomposition for structural segmentation,2020,https://doi.org/10.5281/zenodo.4245552,Axel Marmoret+Univ Rennes>FRA>education|Inria>FRA>facility|CNRS>FRA>facility|IRISA>FRA>facility;Jérémy E. Cohen+Univ Rennes>FRA>education|Inria>FRA>facility|CNRS>FRA>facility|IRISA>FRA>facility;Nancy Bertin+Univ Rennes>FRA>education|Inria>FRA>facility|CNRS>FRA>facility|IRISA>FRA>facility;Frédéric Bimbot+Univ Rennes>FRA>education|Inria>FRA>facility|CNRS>FRA>facility|IRISA>FRA>facility,"Recent work has proposed the use of tensor decomposition to model repetitions and to separate tracks in loop-based electronic music. The present work investigates further on the ability of Nonnegative Tucker Decompositon (NTD) to uncover musical patterns and structure in pop songs in their audio form.Exploiting the fact that NTD tends to express the content of bars as linear combinations of a few patterns, we illustrate the ability of the decomposition to capture and single out repeated motifs in the corresponding compressed space, which can be interpreted from a musical viewpoint. The resulting features also turn out to be efficient for structural segmentation, leading to experimental results on the RWC Pop data set which are potentially challenging state-of-the-art approaches that rely on extensive example-based learning schemes.",FRA,education,Developed economies,"[-4.003098, -1.999858]","[3.3390732, -0.82892615]","[5.929229, -7.475633, 1.2927716]","[-0.052983314, 1.1903596, -8.131893]","[11.56805, 8.251527]","[7.0091515, 4.0203247]","[12.321841, 13.933079, 0.13246574]","[10.507595, 8.421203, 10.860834]"
23,Daniel Harasim;Christoph Finkensiep;Petter Ericson;Timothy J. O'Donnell;Martin Rohrmeier,The jazz Harmony Treebank,2020,https://doi.org/10.5281/zenodo.4245406,Daniel Harasim+École Polytechnique Fédérale de Lausanne>CHE>education;Christoph Finkensiep+École Polytechnique Fédérale de Lausanne>CHE>education;Petter Ericson+École Polytechnique Fédérale de Lausanne>CHE>education;Timothy J. O’Donnell+McGill University>CAN>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"Grammatical models which represent the hierarchical structure of chord sequences have proven very useful in recent analyses of Jazz harmony. A critical resource for building and evaluating such models is a ground-truth database of syntax trees that encode hierarchical analyses of chord sequences. In this paper, we introduce the Jazz Harmony Treebank (JHT),  a dataset of hierarchical analyses of complete Jazz standards. The analyses were created and checked by experts, based on lead sheets from the open iRealPro collection. The JHT is publicly available in JavaScript Object Notation (JSON), a human-understandable and machine-readable format for structured data. We additionally discuss statistical properties of the corpus and present a simple open-source web application for the graphical creation and editing of trees which was developed during the creation of the dataset.",CHE,education,Developed economies,"[23.821455, 22.213383]","[-20.15916, 23.366474]","[-6.634113, -6.3536334, 30.724144]","[-18.911486, 0.8600865, 4.9856]","[10.035168, 9.2736225]","[7.1546345, 3.2486284]","[11.903123, 14.311738, -0.7621596]","[9.563602, 7.965252, 12.33731]"
24,Kento Watanabe;Masataka Goto,A chorus-section detection method for lyrics text,2020,https://doi.org/10.5281/zenodo.4245442,Kento Watanabe+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper addresses the novel task of detecting chorus sections in English and Japanese lyrics text. Although chorus-section detection using audio signals has been studied, whether chorus sections can be detected from text-only lyrics is an open issue. Another open issue is whether patterns of repeating lyric lines such as those appearing in chorus sections depend on language. To investigate these issues, we propose a neural network-based model for sequence labeling. It can learn phrase repetition and linguistic features to detect chorus sections in lyrics text. It is, however, difficult to train this model since there was no dataset of lyrics with chorus-section annotations as there was no prior work on this task. We therefore generate a large amount of training data with such annotations by leveraging pairs of musical audio signals and their corresponding manually time-aligned lyrics; we first automatically detect chorus sections from the audio signals and then use their temporal positions to transfer them to the line-level chorus-section annotations for the lyrics. Experimental results show that the proposed model with the generated data contributes to detecting the chorus sections, that the model trained on Japanese lyrics can detect chorus sections surprisingly well in English lyrics and that patterns of repeating lyric lines are language-independent.",JPN,facility,Developed economies,"[-28.979189, -29.334164]","[-8.23584, -25.894129]","[10.062902, 22.525196, -7.0407276]","[4.800305, 1.0587082, -21.48572]","[11.45612, 11.671371]","[8.813349, 4.4812064]","[12.39452, 15.925039, 0.9605882]","[10.604638, 6.955237, 9.454644]"
25,Ziyu Wang;Ke Chen;Junyan Jiang;Yiyi Zhang;Maoran Xu;Shuqi Dai;Gus Xia,POP909: A pop-song dataset for music arrangement generation,2020,https://doi.org/10.5281/zenodo.4245366,"Ziyu Wang+Music X Lab>USA>facility|New York University>USA>education;Ke Chen+CREL>USA>facility|University of California, San Diego>USA>education;Junyan Jiang+Music X Lab>USA>facility|New York University>USA>education;Yiyi Zhang+Center for Data Science>USA>facility|New York University>USA>education;Maoran Xu+Department of Statistics>USA>education|University of Florida>USA>education;Shuqi Dai+Computer Science Department>USA>education|Carnegie Mellon University>USA>education;Xianbin Gu+Music X Lab>USA>facility|New York University>USA>education;Gus Xia+Music X Lab>USA>facility|New York University>USA>education","Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",USA,facility,Developed economies,"[-20.519386, 5.968606]","[-6.639205, -41.20294]","[3.3746004, 1.0283638, 29.08785]","[-21.620335, -0.84138525, -9.526182]","[10.435172, 8.436515]","[9.165705, 6.1555495]","[13.471152, 11.906626, 0.09773598]","[9.591594, 5.4433813, 9.1460905]"
26,Tsung-Ping Chen;Satoru Fukayama;Masataka Goto;Li Su,Chord jazzification: Learning jazz interpretations of chord symbols,2020,https://doi.org/10.5281/zenodo.4245444,Tsung-Ping Chen+Academia Sinica>TWN>education;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Li Su+Academia Sinica>TWN>education,"Chord symbols, typically notating the root note and the chord quality, are extensively used yet oversimplified representation of tonal harmony and chord progressions in popular music. In spite of its convenience, the chord symbol notation only provides basic information about the chordal configuration, and leaves much room for interpretation. With such limitations, an algorithm generating merely chord symbols is usually insufficient for a wide range of music genres such as jazz. To solve this problem, we propose chord jazzification, a process to generate realistic chord configurations in jazz style. With deep learning approaches, we decompose chord jazzification into coloring and voicing. Coloring concerns the choice of color tones, while voicing concerns the configurations of chords. We also create a new dataset featuring interpretations of chord symbols in pop-jazz compositions. By conducting experiments on the new dataset, we show that 1) the two-stage process outperforms an end-to-end generation approach in modeling chord configurations, and 2) attention-based models are better at capturing the structure of chord sequences in comparison with recurrent neural networks.",TWN,education,Developing economies,"[55.736427, -1.2273402]","[-8.152573, -38.179913]","[22.407505, -13.674747, 21.470835]","[-25.004616, 1.9737341, -10.178242]","[7.1927505, 8.6529875]","[9.185662, 6.1592045]","[12.065032, 10.565784, 1.8088111]","[9.657615, 5.4531913, 9.070883]"
27,Ziyu Wang;Dingsu Wang;Yixiao Zhang;Gus Xia,Learning interpretable representation for controllable polyphonic music generation,2020,https://doi.org/10.5281/zenodo.4245518,Ziyu Wang+NYU Shanghai>USA>education;Dingsu Wang+NYU Shanghai>USA>education;Yixiao Zhang+NYU Shanghai>USA>education;Gus Xia+NYU Shanghai>USA>education,"While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.",USA,education,Developed economies,"[21.834005, 6.328131]","[-11.965934, -44.065662]","[2.373263, 3.607108, 23.38654]","[-20.601269, 0.13758941, -14.767579]","[10.326876, 8.40173]","[8.931907, 6.4917564]","[13.365994, 11.882373, 0.24553627]","[9.59157, 5.520422, 8.574273]"
28,Taketo Akama,Connective fusion: Learning transformational joining of sequences with application to melody creation,2020,https://doi.org/10.5281/zenodo.4245360,Taketo Akama+Sony Computer Science Laboratories>JPN>facility,"We present Connective Fusion, a music generation scheme by transformational joining of two musical sequences for creative purposes. Given two shorter sequences as inputs, our model transforms each of them such that their concatenation is more coherent to form a longer sequence, while each of the transformed shorter sequences retains meaningful similarity with the corresponding input sequence. In short, our model connects and fuses two contextually unrelated sequences in a coherent way. This transformation can be applied iteratively to gradually fuse the input sequences. The style latent space is simultaneously learned, allowing users to control how the two sequences are merged. Our approach comprises two steps of unsupervised learning: a deep generative model with a latent space is learned, followed by adversarial learning of the transformation function in the latent space. We demonstrate the usefulness of our method through the task of melody creation using a symbolic music dataset.",JPN,facility,Developed economies,"[12.595899, -9.657267]","[-9.77285, -42.65617]","[12.057881, 5.2357535, 4.401851]","[-20.1526, 3.5620577, -12.73857]","[10.287173, 9.389376]","[8.940763, 6.5218797]","[11.317964, 14.685358, -0.7765175]","[9.606757, 5.4643483, 8.582953]"
17,Florian Henkel;Rainer Kelz;Gerhard Widmer,Learning to read and follow music in complete score sheet images,2020,https://doi.org/10.5281/zenodo.4245550,Florian Henkel+Johannes Kepler University Linz>AUT>education;Rainer Kelz+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University Linz>AUT>education,"This paper addresses the task of score following in sheet music given as unprocessed images. While existing work either relies on OMR software to obtain a computer-readable score representation, or crucially relies on prepared sheet image excerpts, we propose the first system that directly performs score following in full-page, completely unprocessed sheet images.  Based on incoming audio and a given image of the score, our system directly predicts the most likely position within the page that matches the audio, outperforming current state-of-the-art image-based score followers in terms of alignment precision. We also compare our method to an OMR-based approach and empirically show that it can be a viable alternative to such a system.",AUT,education,Developed economies,"[19.825687, -7.7139134]","[-14.216418, -16.393862]","[22.109184, 23.639013, -2.7672324]","[-4.961632, -21.766676, -2.8812406]","[10.643924, 6.5801044]","[6.3313003, 0.355177]","[12.267805, 12.3064165, -1.4246441]","[8.1563635, 5.4510036, 10.8112955]"
4,Hendrik Schreiber;Frank Zalkow;Meinard Müller,Modeling and estimating local tempo: A case study on Chopin's Mazurkas,2020,https://doi.org/10.5281/zenodo.4245546,Hendrik Schreiber+International Audio Laboratories Erlangen>DEU>facility;Frank Zalkow+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Even though local tempo estimation promises musicological insights into expressive musical performances, it has never received as much attention in the music information retrieval (MIR) research community as either beat tracking or global tempo estimation. One reason for this may be the lack of a generally accepted definition. In this paper, we discuss how to model and measure local tempo in a musically meaningful way using a cross-version dataset of Frédéric Chopin's Mazurkas as a use case. In particular, we explore how tempo stability can be measured and taken into account during evaluation. Comparing existing and newly trained systems, we find that CNN-based approaches can accurately measure local tempo even for expressive classical music, if trained on the target genre. Furthermore, we show that different training–test splits have a considerable impact on accuracy for difficult segments.",DEU,facility,Developed economies,"[44.850506, -24.449888]","[-30.898174, -4.850446]","[-4.009898, -32.133232, 3.1466658]","[-8.991716, 12.441408, -10.36619]","[11.3541, 4.669993]","[5.048099, 1.934316]","[11.052669, 13.128543, -2.6852062]","[7.470282, 6.8453956, 10.722408]"
57,Estefania Cano;Fernando Mora Ángel;Gustavo Adolfo López Gil;José Ricardo Zapata;Antonio Escamilla;Juan Fernando Alzate Londoño;Moises Betancur Pelaez,Sesquialtera in the Colombian bambuco: Perception and estimation of beat and meter,2020,https://doi.org/10.5281/zenodo.4245454,Estefanía Cano+Fraunhofer IDMT>DEU>facility;Fernando Mora-Ángel+Universidad de Antioquia>COL>education;Gustavo A. López Gil+Universidad de Antioquia>COL>education;José R. Zapata+Universidad Pontificia Bolivariana>COL>education;Antonio Escamilla+Universidad Pontificia Bolivariana>COL>education;Juan F. Alzate+Universidad de Antioquia>COL>education;Moisés Betancur+Universidad de Antioquia>COL>education,"The bambuco, one of the national rhythms of Colombia, is characterized by the presence of sesquialteras or the superposition of rhythmic elements from two meters. In this work, we analyze sesquialteras in bambucos from two perspectives. First, we analyze the perception of beat and meter by asking 10 Colombian musicians to perform beat annotations in a dataset of bambucos. Results show  great diversity in the annotations: a total of five different meters or meter combinations were found in the annotations, with each bambuco in the study being annotated in at least two different meters. Second, we perform a beat tracking analysis in a dataset of bambucos with two state-of-the-art algorithms.  Given that the algorithms used in the analysis were designed to deal with the rhythmic regularity of a single meter, it is not surprising that tracking performance is not very high (~42% mean F-measure). However, a deeper analysis of  the onset detection functions used for beat tracking, indicate that there is enough information on the signal level to characterize the bi-metric behavior of bambucos. With this in mind, we highlight possibilities for computational analysis of rhythm in bambucos.",DEU,facility,Developed economies,"[-10.020946, -49.74108]","[-23.725883, 1.0394287]","[-8.141941, -29.632221, 0.94513386]","[-2.5654585, 14.045462, -5.9336333]","[11.454871, 4.5915895]","[5.9067082, 1.728585]","[10.853448, 13.59808, -2.6452534]","[8.044304, 6.840887, 11.624462]"
59,Yucong Jiang,Score following with hidden tempo using a switching state-space model,2020,https://doi.org/10.5281/zenodo.4245528,Yucong Jiang+University of Richmond>USA>education;Christopher Raphael+Indiana University Bloomington>USA>education,"A score-following program traces the notes in a musical score during a performance. This capability is essential to many meaningful applications that synchronize audio with a score in an on-line fashion. Existing algorithms often stumble on certain difficult cases, one of which is piano music. This paper presents a new method to tackle such cases. The method treats tempo as a variable rather than a constant (with constraints), allowing the program to adapt to live performance variations. This is first expressed by a Kalman filter model at the note level, and then by an almost equivalent switching state-space model at the audio frame level. The latter contains both discrete and continuous hidden variables, and is computationally intractable. We show how certain reasonable approximations make the computation manageable. This new method is tested on a dataset of 50 piano excerpts. Compared with a previously established state-of-the-art algorithm, the new method shows more stable and accurate results: it reduces fatal score-following errors, and improves accuracy from 65.0% to 69.1%.",USA,education,Developed economies,"[42.836864, -28.626528]","[-17.782986, -10.560544]","[-0.9060126, -34.389553, -0.78684926]","[1.9396219, -16.651709, -3.7624981]","[11.472982, 4.450704]","[5.9572105, 1.1705132]","[10.951644, 13.2399845, -2.8204446]","[8.109323, 6.296868, 10.917003]"
88,Javier Nistal;Stefan Lattner;Gaël Richard,DrumGAN: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks,2020,https://doi.org/10.5281/zenodo.4245504,Javier Nistal+Sony CSL>FRA>company;Stefan Lattner+Sony CSL>FRA>company;Gaël Richard+Institut Polytechnique de Paris>FRA>education,"Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.",FRA,company,Developed economies,"[23.853868, -40.026775]","[-19.31656, -48.449734]","[17.787142, -14.287861, 5.930489]","[-23.759596, -2.3472762, -19.91894]","[9.66628, 8.388655]","[8.191537, 6.6937475]","[12.984004, 11.72318, 0.5274555]","[9.551459, 5.963403, 8.083602]"
89,Ioannis Petros Samiotis;Sihang Qiu;Andrea Mauri;Cynthia C. S. Liem;Christoph Lofi;Alessandro Bozzon,Microtask crowdsourcing for music score transcriptions: An experiment with error detection,2020,https://doi.org/10.5281/zenodo.4245580,Ioannis Petros Samiotis+Delft University of Technology>NLD>education;Sihang Qiu+Delft University of Technology>NLD>education;Andrea Mauri+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education;Christoph Lofi+Delft University of Technology>NLD>education;Alessandro Bozzon+Delft University of Technology>NLD>education,"Human annotation is still an essential part of modern transcription workflows for digitizing music scores, either as a standalone approach where a single expert annotator transcribes a complete score, or for supporting an automated Optical Music Recognition (OMR) system. Research on human computation has shown the effectiveness of crowdsourcing for scaling out human work by defining a large number of microtasks which can easily be distributed and executed. However, microtask design for music transcription is a research area that remains unaddressed. This paper focuses on the design of a crowdsourcing task to detect errors in a score transcription which can be deployed in either automated or human-driven transcription workflows. We conduct an experiment where we study two design parameters: 1) the size of the score to be annotated and 2) the modality in which it is presented in the user interface. We analyze the performance and reliability of non-specialised crowdworkers on Amazon Mechanical Turk with respect to these design parameters, differentiated by worker experience and types of transcription errors. Results are encouraging, and pave the way for scalable and efficient crowd-assisted music transcription systems.",NLD,education,Developed economies,"[-8.193932, 5.749315]","[-11.929663, 41.48565]","[-7.0667486, 17.219927, 4.254568]","[5.300849, 24.35177, -8.153367]","[12.876459, 9.199626]","[8.974545, 4.340909]","[13.291109, 14.893579, -0.19739707]","[10.183573, 6.0492806, 10.16088]"
90,Pritish Chandna;Helena Cuesta;Emilia Gomez,A deep learning based analysis-synthesis framework for unison singing,2020,https://doi.org/10.5281/zenodo.4245502,"Pritish Chandna+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Helena Cuesta+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+European Commission, Joint Research Centre>ESP>facility","Unison singing is the name given to an ensemble of singers simultaneously singing the same melody and lyrics. While each individual singer in a unison sings the same principle melody, there are slight timing and pitch deviations between the singers, which, along with the ensemble of timbres, give the listener a perceived sense of ""unison"". In this paper, we present a study of unison singing in the context of choirs; utilising some recently proposed deep-learning based methodologies, we analyse the fundamental frequency (F0) distribution of the individual singers in recordings of unison mixtures. Based on the analysis, we propose a system for synthesising a unison signal from an a cappella input and a single voice prototype representative of a unison mixture. We use subjective listening test to evaluate perceptual factors of our proposed system for synthesis, including quality, adherence to the melody as well the degree of perceived unison.",ESP,education,Developed economies,"[-2.4615073, -38.53511]","[-34.872223, -31.264158]","[27.505625, 10.567491, -11.941012]","[-9.252943, -6.8848524, -22.913754]","[9.595819, 10.477843]","[7.2791467, 5.691661]","[11.207209, 14.540879, 0.9287943]","[9.653371, 7.7198153, 8.891116]"
91,Albin Correya;Dmitry Bogdanov;Luis Joglar-Ongay;Xavier Serra,Essentia.js: A JavaScript library for music and audio analysis on the web,2020,https://doi.org/10.5281/zenodo.4245510,"Albin Correya+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Luis Joglar-Ongay+Music Technology Group, Universitat Pompeu Fabra>ESP>education|SonoSuite>ESP>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Open-source software libraries for audio/music analysis and feature extraction have a significant impact on the development of Audio Signal Processing and Music Information Retrieval (MIR) systems. Despite the abundance of such tools on the native computing platforms, there is a lack of an extensive and easy-to-use reference library for audio feature extraction on the Web. In this paper, we present Essentia.js, an open-source JavaScript (JS) library for audio and music analysis on both web clients and JS-based servers. Along with the Web Audio API, it can be used for efficient and robust real-time audio feature extraction on the web browsers. Essentia.js is modular, lightweight, and easy-to-use, deploy, maintain, and integrate into the existing plethora of JS libraries and Web technologies. It is powered by a WebAssembly back-end of the Essentia C++ library, which facilitates a JS interface to a wide range of low-level and high-level audio features. It also provides a higher-level JS API and add-on MIR utility modules along with extensive documentation, usage examples, and tutorials. We benchmark the proposed library on two popular web browsers, Node.js engine, and Android devices, comparing it to the native performance of Essentia and Meyda JS library.",ESP,education,Developed economies,"[-18.992828, 16.08218]","[9.772408, 29.677547]","[-14.972248, -0.5301399, -14.933765]","[-5.7426343, -1.9365095, 16.566565]","[14.002294, 7.552671]","[10.4310875, 1.3244076]","[14.284357, 14.107945, -1.936626]","[11.306515, 5.566511, 11.611225]"
92,Gabriel Oliveira;Mariana Santos;Danilo B Seufitelli;Anisio Lacerda;Mirella M Moro,Detecting collaboration profiles in success-based music genre networks,2020,https://doi.org/10.5281/zenodo.4245534,Gabriel P. Oliveira+Universidade Federal de Minas Gerais>BRA>education;Mariana O. Silva+Universidade Federal de Minas Gerais>BRA>education;Danilo B. Seuﬁtelli+Universidade Federal de Minas Gerais>BRA>education;Anisio Lacerda+Universidade Federal de Minas Gerais>BRA>education;Mirella M. Moro+Universidade Federal de Minas Gerais>BRA>education,"We analyze and identify collaboration profiles in success-based music genre networks. Such networks are built upon data recently collected from both global and regional Spotify weekly charts.  Overall, our findings reveal an increase in the number of distinct successful genres from high-potential markets, pointing out that local repertoire is more important than ever on building the global music ecosystem. We also detect collaboration patterns mapped into four different profiles: Solid, Regular, Bridge and Emerging, wherein the two first depict higher average success. These findings indicate  great opportunities for the music industry by revealing the driving power of inter-genre collaborations within regional and global markets.",BRA,education,Developing economies,"[-38.338524, 11.799107]","[45.767014, 11.314298]","[-25.8303, 6.724307, 2.8541996]","[15.824234, 12.530807, 11.295451]","[14.643916, 9.411862]","[12.376269, 2.6301146]","[15.03888, 14.684372, -0.70783883]","[13.381724, 5.4530406, 11.856466]"
93,Darius Petermann;Pritish Chandna;Helena Cuesta;Jordi Bonada;Emilia Gomez,Deep learning based source separation applied to choir ensembles,2020,https://doi.org/10.5281/zenodo.4245536,"Darius Petermann+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Pritish Chandna+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Helena Cuesta+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Jordi Bonada+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+European Commission, Joint Research Centre>ESP>facility","Choral singing is a widely practiced form of ensemble singing wherein a group of people sing simultaneously in polyphonic harmony. The most commonly practiced setting for choir ensembles consists of four parts; Soprano, Alto, Tenor and Bass (SATB), each with its own range of fundamental frequencies (F0s). The task of source separation for this choral setting entails separating the SATB mixture into the constituent parts. Source separation for musical mixtures is well studied and many deep learning based methodologies have been proposed for the same. However, most of the research has been focused on a typical case which consists in separating vocal, percussion and bass sources from a mixture, each of which has a distinct spectral structure. In contrast, the simultaneous and harmonic nature of ensemble singing leads to high structural similarity and overlap between the spectral components of the sources in a choral mixture, making source separation for choirs a harder task than the typical case. This, along with the lack of an appropriate consolidated dataset has led to a dearth of research in the field so far. In this paper we first assess how well some of the recently developed methodologies for musical source separation perform for the case of SATB choirs. We then propose a novel domain-specific adaptation for conditioning the recently proposed U-Net architecture for musical source separation using the fundamental frequency contour of each of the singing groups and demonstrate that our proposed approach surpasses results from domain-agnostic architectures.",ESP,education,Developed economies,"[3.9382672, -48.391853]","[-35.725998, -30.938395]","[32.037464, 2.1823862, -5.215898]","[-10.806229, -7.4520392, -23.28879]","[8.52917, 10.273811]","[6.997659, 5.7445803]","[10.949767, 13.871537, 1.6254525]","[9.620485, 8.0127325, 8.917]"
94,Carlos Eduardo Cancino-Chacón;Silvan Peter;Shreyan Chowdhury;Anna Aljanaki;Gerhard Widmer,On the characterization of expressive performance in classical music: First results of the con espressione game,2020,https://doi.org/10.5281/zenodo.4245506,"Carlos Cancino-Chacón+Austrian Research Institute for Artificial Intelligence>AUT>facility|RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, University of Oslo>NOR>education;Silvan Peter+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Shreyan Chowdhury+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Anna Aljanaki+Institute of Computer Science, University of Tartu>EST>education;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence>AUT>facility|Institute of Computational Perception, Johannes Kepler University Linz>AUT>education","A piece of music can be expressively performed, or interpreted, in a variety of ways. With the help of an online questionnaire, the Con Espressione Game, we collected some 1,500 descriptions of expressive character relating to 45 performances of 9 excerpts from classical piano pieces, played by different famous pianists. More specifically, listeners were asked to describe, using freely chosen words (preferably: adjectives), how they perceive the expressive character of the different performances. In this paper, we offer a first account of this new data resource for expressive performance research, and provide an exploratory analysis, addressing three main questions: (1) how similarly do different listeners describe a performance of a piece? (2) what are the main dimensions (or axes) for expressive character emerging from this?; and (3) how do measurable parameters of a performance (e.g., tempo, dynamics) and mid- and high-level features that can be predicted by machine learning models (e.g., articulation, arousal) relate to these expressive dimensions? The dataset that we publish along with this paper was enriched by adding hand-corrected score-to-performance alignments, as well as descriptive audio features such as tempo and dynamics curves.",AUT,facility,Developed economies,"[-13.853947, 2.7910173]","[-32.786552, 6.3704057]","[-3.0034344, 7.379829, 19.722385]","[-11.294446, 1.45515, -1.8450093]","[11.196297, 8.001001]","[7.718002, 3.5807538]","[13.317812, 13.065596, -0.90121335]","[9.168549, 6.21935, 10.793709]"
95,Yu Wang;Justin Salamon;Mark Cartwright;Nicholas J. Bryan;Juan P Bello,Few-shot drum transcription in polyphonic music,2020,https://doi.org/10.5281/zenodo.4245384,Yu Wang+New York University>USA>education;Justin Salamon+Adobe Research>USA>company;Mark Cartwright+New York University>USA>education;Nicholas J. Bryan+Adobe Research>USA>company;Juan Pablo Bello+New York University>USA>education,"Data-driven approaches to automatic drum transcription (ADT) are often limited to a predefined, small vocabulary of percussion instrument classes. Such models cannot recognize out-of-vocabulary classes nor are they able to adapt to finer-grained vocabularies. In this work, we address open vocabulary ADT by introducing few-shot learning to the task. We train a Prototypical Network on a synthetic dataset and evaluate the model on multiple real-world ADT datasets with polyphonic accompaniment. We show that, given just a handful of selected examples at inference time, we can match and in some cases outperform a state-of-the-art supervised ADT approach under a fixed vocabulary setting. At the same time, we show that our model can successfully generalize to finer-grained or extended vocabularies unseen during training, a scenario where supervised approaches cannot operate at all. We provide a detailed analysis of our experimental results, including a breakdown of performance by sound class and by polyphony.",USA,education,Developed economies,"[26.35653, -44.52945]","[-38.274647, -14.553971]","[17.0186, -21.12595, 3.8477337]","[2.7453272, 10.502731, -20.070139]","[7.711656, 7.244831]","[8.480262, 4.5107684]","[10.367738, 11.564566, 1.0095155]","[9.188709, 7.1063, 9.6284895]"
96,Ashis Pati;Siddharth Kumar Gururani;Alexander Lerch,dMelodies: A music dataset for disentanglement learning,2020,https://doi.org/10.5281/zenodo.4245382,Ashis Pati+Georgia Institute of Technology>USA>education;Siddharth Gururani+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (? 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised dis- entanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.",USA,education,Developed economies,"[-10.034466, -7.569577]","[-8.368491, -46.97423]","[2.3577852, 8.3792, 19.660995]","[-18.890451, 3.4585083, -18.07539]","[11.374234, 9.020651]","[8.829516, 6.168484]","[13.438445, 12.849825, 0.48499525]","[10.007336, 5.8401055, 8.658034]"
97,Jongpil Lee;Nicholas J. Bryan;Justin Salamon;Zeyu Jin;Juhan Nam,Metric learning vs classification for disentangled music representation learning,2020,https://doi.org/10.5281/zenodo.4245468,Jongpil Lee+KAIST>KOR>education|Adobe Research>USA>company;Nicholas J. Bryan+Adobe Research>USA>company;Justin Salamon+Adobe Research>USA>company;Zeyu Jin+Adobe Research>USA>company;Juhan Nam+KAIST>KOR>education,"Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classification, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning, classification, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classification, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We find that classification-based models are generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging.",KOR,education,Developing economies,"[-9.570589, -6.7964993]","[-7.55644, -47.477417]","[-1.7176192, 11.682656, 4.719318]","[-11.6270485, 4.3527737, -16.93995]","[12.064489, 9.263089]","[9.405213, 5.1419997]","[13.649593, 14.131027, -0.042734608]","[10.288709, 6.124406, 8.85035]"
98,Jiawen Huang;Yun-Ning Hung;Ashis Pati;Siddharth Kumar Gururani;Alexander Lerch,Score-informed networks for music performance assessment,2020,https://doi.org/10.5281/zenodo.4245582,Jiawen Huang+Georgia Institute of Technology>USA>education;Yun-Ning Hung+Georgia Institute of Technology>USA>education;Ashis Pati+Georgia Institute of Technology>USA>education;Siddharth Gururani+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"The assessment of music performances in most cases takes into account the underlying musical score being performed. While there have been several automatic approaches for objective music performance assessment (MPA) based on extracted features from both the performance audio and the score, deep neural network-based methods incorporating score information into MPA models have not yet been investigated. In this paper, we introduce three different models capable of score-informed performance assessment. These are (i) a convolutional neural network that utilizes a simple time-series input comprising of aligned pitch contours and score, (ii) a joint embedding model which learns a joint latent space for pitch contours and scores, and (iii) a distance matrix-based convolutional neural network which utilizes patterns in the distance matrix between pitch contours and musical score to predict assessment ratings. Our results provide insights into the suitability of different architectures and input representations and demonstrate the benefits of score-informed models as compared to score-independent models.",USA,education,Developed economies,"[-16.413437, 5.1639686]","[-31.2102, -35.16204]","[-6.962411, 12.4340105, 17.029783]","[-7.307686, -9.620491, -22.094564]","[13.230459, 7.832384]","[7.6884785, 5.6776104]","[13.878285, 13.614813, -1.3835175]","[9.840247, 7.118228, 8.686624]"
99,Jacopo de Berardinis;Angelo Cangelosi;Eduardo Coutinho,The multiple voices of musical emotions: Source separation for improving music emotion recognition models and their interpretability,2020,https://doi.org/10.5281/zenodo.4245428,Jacopo de Berardinis+University of Manchester>GBR>education;Angelo Cangelosi+University of Manchester>GBR>education;Eduardo Coutinho+University of Liverpool>GBR>education,"Despite the manifold developments in music emotion recognition and related areas, estimating the emotional impact of music still poses many challenges. These are often associated to the complexity of the acoustic codes to emotion and the lack of large amounts of data with robust golden standards. In this paper, we propose a new computational model (EmoMucs) that considers the role of different musical voices in the prediction of the emotions induced by music. We combine source separation algorithms for breaking up music signals into independent song elements (vocals, bass, drums, other) and end-to-end state-of-the-art machine learning techniques for feature extraction and emotion modelling (valence and arousal regression). Through a series of computational experiments on a benchmark dataset using source-specialised models trained independently and different fusion strategies, we demonstrate that EmoMucs outperforms state-of-the-art approaches with the advantage of providing insights into the relative contribution of different musical elements to the emotions perceived by listeners.",GBR,education,Developed economies,"[-60.652878, -2.5769405]","[49.988487, -9.827501]","[-30.569168, 24.107903, 5.8455396]","[9.911079, 24.675795, 4.10973]","[14.041235, 12.942268]","[13.01021, 4.3303885]","[15.908624, 14.356454, 1.9015515]","[14.160436, 5.0391607, 10.19146]"
100,Michael M Michelashvili;Lior Wolf,Hierarchical timbre-painting and articulation generation,2020,https://doi.org/10.5281/zenodo.4245584,Michael Michelashvili+Tel Aviv University>ISR>education|Tel Aviv University>ISR>education;Lior Wolf+Tel Aviv University>ISR>education|Tel Aviv University>ISR>education,"We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.",ISR,education,Developing economies,"[19.959879, 6.100292]","[-21.44917, -48.85881]","[5.7628813, 4.9616923, 20.406275]","[-22.761536, -4.720777, -21.330702]","[10.226857, 8.629618]","[8.102626, 6.6748486]","[13.146115, 11.969185, 0.18593504]","[9.572206, 6.0565605, 8.021894]"
101,Kyle Robinson;Dan Brown;Markus Schedl,User insights on diversity in music recommendation lists,2020,https://doi.org/10.5281/zenodo.4245464,"Kyle Robinson+David R. Cheriton School of Computer Science, University of Waterloo>CAN>education;Dan Brown+David R. Cheriton School of Computer Science, University of Waterloo>CAN>education;Markus Schedl+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education","While many researchers have proposed various ways of quantifying recommendation list diversity, these approaches have had little input from users on their own perceptions and preferences in seeking diversity. Through an exploratory user study, we provide a better understanding of how users view the concept of diversity in music recommendations, and how they might optimise levels of intra-list diversity themselves. In our study, 17 participants interacted with and rated the suggestions from two different recommendation systems. One provided static top-7 collaborative filtering recommendations, and the other provided an interactive slider to re-rank these recommendations based on a continuous diversity scale. We also asked participants a series of free-form questions on music discovery and diversity in semi-structured interviews. User-preferred levels of diversity varied widely both within and between subjects. Although most users agreed that diversity is beneficial in music discovery, they also noted a risk of dissatisfaction from too much diversity. A key finding is that preference for diversification was often linked to user mood. Participants also expressed a clear distinction between diversity within existing preferences, and outside of existing preferences. These ideas of inner and outer diversity are not well defined within the bounds of current diversity metrics, and we discuss their implications.",CAN,education,Developed economies,"[-47.46421, 24.183313]","[41.99444, 16.62406]","[-9.03291, 27.6641, -7.619945]","[15.118763, 10.972295, 20.253654]","[15.895542, 9.041534]","[12.662651, 1.6344011]","[15.862447, 15.506609, -1.4492444]","[13.493401, 5.032594, 12.684855]"
102,Polina Proutskova;Anja Volk;Peyman Heidarian;Gyorgy Fazekas,From music ontology towards ethno-music-ontology,2020,https://doi.org/10.5281/zenodo.4245586,"Polina Proutskova+Center for Digital Music, Queen Mary University of London>GBR>education;Anja Volk+Utrecht University>NLD>education;Peyman Heidarian+The University of Waikato>NZL>education;György Fazekas+Center for Digital Music, Queen Mary University of London>GBR>education","This paper presents exploratory work investigating the suitability of the Music Ontology - the most widely used formal specification of the music domain - for modelling non-Western musical traditions. Four contrasting case studies from a variety of musical cultures are analysed: Dutch folk song research, reconstructive performance of rural Russian traditions, contemporary performance and composition of Persian classical music, and recreational use of a personal world music collection. We propose semantic models describing the respective do- mains and examine the applications of the Music Ontology for these case studies: which concepts can be successfully reused, where they need adjustments, and which parts of the reality in these case studies are not covered by the Mu- sic Ontology. The variety of traditions, contexts and modelling goals covered by our case studies sheds light on the generality of the Music Ontology and on the limits of generalisation ""for all musics"" that could be aspired for on the Semantic Web.",GBR,education,Developed economies,"[-26.397036, 37.55623]","[16.686556, 38.68115]","[-21.695925, 0.99498475, -3.2447145]","[-1.6939247, -1.4452081, 26.013332]","[14.20651, 8.978178]","[10.867096, -0.14803413]","[15.0911665, 13.77582, -1.2621619]","[12.183376, 5.107909, 11.610268]"
103,Antonio R. Parmezan;Diego Furtado Silva;Gustavo Batista,A combination of local approaches for hierarchical music genre classification,2020,https://doi.org/10.5281/zenodo.4245540,Antonio R. S. Parmezan+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education;Diego Furtado Silva+Departamento de Computação – Universidade Federal de São Carlos>BRA>education;Gustavo E. A. P. A. Batista+School of Computer Science and Engineering – University of New South Wales> AUS>education,"Labeling a music recording according to its genre is an intuitive and familiar way to describe its content. Music genres are valuable information, especially for music organization, personalized listening experience, and playlist generation. Automatically classifying music genres is a challenging endeavor due to the inherent ambiguity and subjectivity. Most efforts on music genre classification consider the complete independence between labels. However, music genres typically respect a hierarchical structure based on the influences or origins of each style. Conversely, many of the methods available for hierarchical classification are based on assumptions about the class hierarchy, such as the need for multiple children in each hierarchy's node, which may limit their use in music applications. Also, the local classifier per node approach that would be the most suitable for this scenario is costly regarding time and memory. In this paper, we present two local hierarchical classification approaches and show how to combine them to obtain a single one that is more robust and faithful to the music genre classification scenario. We evaluate our proposal in a music dataset hierarchically labeled with 120 music genres. As shown, compared to state-of-the-art approaches, our approach has a lower computational cost and can achieve competitive performances.",BRA,education,Developing economies,"[-30.525023, -12.623442]","[30.040405, -2.949485]","[-18.540344, 6.0029798, 13.925023]","[15.840821, 8.182424, -0.8422109]","[13.003156, 10.890602]","[10.579037, 3.4468312]","[13.998609, 14.284226, 1.4473529]","[12.284663, 6.4968987, 10.923603]"
104,Jason Smith;Erin Truesdell;Jason Freeman;Brian Magerko;Kristy Boyer;Tom Mcklin,Modeling music and code knowledge to support a co-creative AI agent for education,2020,https://doi.org/10.5281/zenodo.4245386,"Jason Smith+Center for Music Technology, Georgia Institute of Technology>USA>education;Erin J.K. Truesdell+Expressive Machinery Lab, Georgia Institute of Technology>USA>education;Jason Freeman+Center for Music Technology, Georgia Institute of Technology>USA>education;Brian Magerko+Expressive Machinery Lab, Georgia Institute of Technology>USA>education;Kristy Elizabeth Boyer+University of Florida>USA>education;Tom McKlin+The Findings Group>USA>company","EarSketch is an online environment for learning introductory computing concepts through code-driven, sample-based music production. This paper details the design and implementation of a module to perform code and music analyses on projects on the EarSketch platform. This analysis module combines inputs in the form of symbolic metadata, audio feature analysis, and user code to produce comprehensive models of user projects. The module performs a detailed analysis of the abstract syntax tree of a user's code to model use of computational concepts. It uses music information retrieval (MIR) and symbolic metadata to analyze users' musical design choices. These analyses produce a model containing users' coding and musical decisions, as well as qualities of the algorithmic music created by those decisions. The models produced by this module will support future development of CAI, a Co-creative Artificial Intelligence. CAI is designed to collaborate with learners and promote increased competency and engagement with topics in the EarSketch curriculum. Our module combines code analysis and MIR to further the educational goals of CAI and EarSketch and to explore the application of multimodal analysis tools to education.",USA,education,Developed economies,"[-26.300816, 4.3457656]","[-1.1203238, 31.61408]","[-26.709057, -5.4176655, 5.009455]","[-12.148941, -2.5335462, 16.620827]","[12.788221, 7.5250807]","[10.162663, 0.58315015]","[14.427874, 12.92732, -1.1846216]","[10.733275, 5.1906896, 11.724053]"
105,Yun-Ning Hung;Alexander Lerch,Multitask learning for instrument activation aware music source separation,2020,https://doi.org/10.5281/zenodo.4245548,Yun-Ning Hung+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"Music source separation is a core task in music information retrieval which has seen a dramatic improvement in the past years. Nevertheless, most of the existing systems focus exclusively on the problem of source separation itself and ignore the utilization of other~---possibly related---~MIR tasks which could lead to additional quality gains. In this work, we propose a novel multitask structure to investigate using instrument activation information to improve source separation performance. Furthermore, we investigate our system on six independent instruments, a more realistic scenario than the three instruments included in the widely-used MUSDB dataset, by leveraging a combination of the MedleyDB and Mixing Secrets datasets. The results show that our proposed multitask model outperforms the baseline Open-Unmix model on the mixture of Mixing Secrets and MedleyDB dataset while maintaining comparable performance on the MUSDB dataset.",USA,education,Developed economies,"[7.302044, -42.89781]","[-37.091415, -29.062464]","[26.810505, -3.7409475, -1.6107646]","[-12.194792, -1.8421612, -27.18205]","[8.373515, 10.077819]","[6.813516, 5.7441616]","[10.991765, 13.446293, 1.6542222]","[9.666817, 8.273908, 9.174628]"
106,Shih-Lun Wu;Yi-Hsuan Yang,The jazz transformer on the front line: Exploring the shortcomings of AI-composed music through quantitative measures,2020,https://doi.org/10.5281/zenodo.4245390,Shih-Lun Wu+National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown|Academia Sinica>Unknown>Unknown;Yi-Hsuan Yang+National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown|Academia Sinica>Unknown>Unknown,"This paper presents the Jazz Transformer, a generative model that utilizes a neural sequence model called the Transformer-XL for modeling lead sheets of Jazz music. Moreover, the model endeavors to incorporate structural events present in the Weimar Jazz Database (WJazzD) for inducing structures in the generated music. While we are able to reduce the training loss to a low value, our listening test suggests however a clear gap between the average ratings of the generated and real compositions. We therefore go one step further and conduct a series of computational analysis of the generated compositions from different perspectives. This includes analyzing the statistics of the pitch class, grooving, and chord progression, assessing the structureness of the music with the help of the fitness scape plot, and evaluating the model's understanding of Jazz music through a MIREX-like continuation prediction task. Our work presents in an analytical manner why machine-generated music to date still falls short of the artwork of humanity, and sets some goals for future work on automatic composition to further pursue.",TWN,education,Developing economies,"[-24.646057, 6.2558846]","[-6.5897, -36.77102]","[-29.362803, -3.286644, 5.032426]","[-19.743443, 3.3410437, -3.976397]","[12.564152, 7.5186787]","[9.243503, 5.959245]","[14.161157, 12.96453, -1.0988467]","[9.756299, 5.5206165, 9.374099]"
107,Taegyun Kwon;Dasaem Jeong;Juhan Nam,Polyphonic piano transcription using autoregressive multi-state note model,2020,https://doi.org/10.5281/zenodo.4245466,Taegyun Kwon+KAIST>KOR>education;Dasaem Jeong+KAIST>KOR>education|SK Telecom>KOR>company;Juhan Nam+KAIST>KOR>education,"Recent advances in polyphonic piano transcription have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multiple loss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order is learned by an auto-regressive connection within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters.",KOR,education,Developing economies,"[29.80971, -6.309591]","[-27.215206, -24.802431]","[13.783522, -5.399526, 13.852165]","[-9.956099, -9.605615, -11.502036]","[9.686803, 7.379462]","[7.932357, 5.4365954]","[11.915497, 11.629082, -0.14322588]","[8.983628, 6.4502206, 9.042752]"
108,Christopher J Tralie;Elizabeth Dempsey,"Exact, parallelizable dynamic time warping alignment with linear memory",2020,https://doi.org/10.5281/zenodo.4245470,Christopher J. Tralie+Ursinus College>USA>education;Elizabeth Dempsey+Ursinus College>USA>education,"Audio alignment is a fundamental preprocessing step in many MIR pipelines. For two audio clips with M and N frames, respectively, the most popular approach, dynamic time warping (DTW), has O(MN) requirements in both memory and computation, which is prohibitive for frame-level alignments at reasonable rates. To address this, a variety of memory efficient algorithms exist to approximate the optimal alignment under the DTW cost. To our knowledge, however, no exact algorithms exist that are guaranteed to break the quadratic memory barrier.  In this work, we present a divide and conquer algorithm that computes the exact globally optimal DTW alignment using O(M+N) memory. Its runtime is still O(MN), trading off memory for a 2x increase in computation.  However, the algorithm can be parallelized up to a factor of min(M, N) with the same memory constraints, so it can still run more efficiently than the textbook version with an adequate GPU. We use our algorithm to compute exact alignments on a collection of orchestral music, which we use as ground truth to benchmark the alignment accuracy of several popular approximate alignment schemes at scales that were not previously possible.",USA,education,Developed economies,"[52.54931, 10.289162]","[-17.442862, -17.967134]","[9.531287, -18.334177, -18.289656]","[0.019598786, -25.294878, -4.9219837]","[10.608253, 5.638987]","[6.0269084, 0.601904]","[11.238488, 12.848188, -1.6590521]","[8.107935, 5.805682, 10.843429]"
109,Yu-Hua Chen;Yu-Siang Huang;Wen-Yi Hsiao;Yi-Hsuan Yang,Automatic composition of guitar tabs by transformers and groove modeling,2020,https://doi.org/10.5281/zenodo.4245542,Yu-Hua Chen+Taiwan AI Labs>TWN>company|Academia Sinica>Unknown>education|National Taiwan University>Unknown>education;Yu-Hsiang Huang+Taiwan AI Labs>TWN>company|Academia Sinica>Unknown>education|National Taiwan University>Unknown>education;Wen-Yi Hsiao+Taiwan AI Labs>TWN>company|Academia Sinica>Unknown>education|National Taiwan University>Unknown>education;Yi-Hsuan Yang+Taiwan AI Labs>TWN>company|Academia Sinica>Unknown>education|National Taiwan University>Unknown>education,"Recent years have witnessed great progress in using deep learning algorithms to learn to compose music in the form of a MIDI file.  However, whether such algorithms apply equally well to compose guitar tabs, which are quite different from MIDIs, remain relatively unexplored. To address this, we build a model for composing fingerstyle guitar tabs with a neural sequence model architecture called the Transformer-XL. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful fingering (i.e., string-fret combinations), which is important for tabs but not for MIDIs. Second, whether it generates compositions with coherent rhythmic grooving, which is crucial for fingerstyle guitar music. And, finally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence for the promise of deep learning for guitar tab composition, and suggests areas for future study.",TWN,company,Developing economies,"[46.17004, -9.691393]","[-42.634987, -9.331952]","[25.106241, -12.865414, 6.6371894]","[-19.161066, -7.1901374, -6.1111865]","[7.6767883, 8.219415]","[8.095769, 5.4678655]","[11.587307, 11.202648, 1.380068]","[9.068153, 6.2551823, 9.337569]"
110,Taejun Kim;Minsuk Choi;Evan Sacks;Yi-Hsuan Yang;Juhan Nam,A computational analysis of real-world DJ mixes using mix-to-track subsequence alignment,2020,https://doi.org/10.5281/zenodo.4245544,"Taejun Kim+Graduate School of Culture Technology, KAIST>KOR>education|1001Tracklists>USA>company|Research Center for IT Innovation, Academia Sincia>TWN>education;Minsuk Choi+Graduate School of Culture Technology, KAIST>KOR>education|1001Tracklists>USA>company|Research Center for IT Innovation, Academia Sincia>TWN>education;Evan Sacks+1001Tracklists>USA>company;Yi-Hsuan Yang+Research Center for IT Innovation, Academia Sincia>TWN>education;Juhan Nam+Graduate School of Culture Technology, KAIST>KOR>education","A DJ mix is a sequence of music tracks concatenated seamlessly, typically rendered for audiences in a live setting by a DJ on stage. As a DJ mix is produced in a studio or the live version is recorded for music streaming services, computational methods to analyze DJ mixes, for example, extracting track information or understanding DJ techniques, have drawn research interests. Many of previous works are, however, limited to identifying individual tracks in a mix or segmenting it, and the sizes of the datasets are usually small. In this paper, we provide an in-depth analysis of DJ music by aligning a mix to its original music tracks. We set up the subsequence alignment such that the audio features are less sensitive to the tempo or key change of the original track in a mix. This approach provides temporally tight mix-to-track matching from which we can obtain cue-points, transition length, mix segmentation, and musical changes in DJ performance. Using 1,557 mixes from 1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the proposed analysis and show a wide range of statistics, which may elucidate the creative process of DJ music making.",KOR,education,Developing economies,"[17.831064, -39.675552]","[-14.134851, -19.573957]","[8.845731, -25.439129, -15.257652]","[-0.2741714, 14.065393, -14.486969]","[9.82471, 4.6579113]","[6.160776, 0.87751704]","[10.770873, 12.161456, -1.8312047]","[8.011751, 6.339429, 10.735579]"
87,Helena Cuesta;Brian McFee;Emilia Gomez,Multiple F0 estimation in vocal ensembles using convolutional neural networks,2020,https://doi.org/10.5281/zenodo.4245434,"Helena Cuesta+Music Technology Group, Universitat Pompeu Fabra>ESP>education|European Commission, Joint Research Centre>ESP>facility;Brian McFee+Music and Audio Research Lab & Center for Data Science, New York University>USA>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|European Commission, Joint Research Centre>ESP>facility","This paper addresses the extraction of multiple F0 values from polyphonic and a cappella vocal performances using convolutional neural networks (CNNs). We address the major challenges of ensemble singing, i.e., all melodic sources are vocals and singers sing in harmony. We build upon an existing architecture to produce a pitch salience function of the input signal, where the harmonic constant-Q transform (HCQT) and its associated phase differentials are used as an input representation. The pitch salience function is subsequently thresholded to obtain a multiple F0 estimation output. For training, we build a dataset that comprises several multi-track datasets of vocal quartets with F0 annotations.This work proposes and evaluates a set of CNNs for this task in diverse scenarios and data configurations, including recordings with additional reverb. Our models outperform a state-of-the-art method intended for the same music genre when evaluated with an increased F0 resolution, as well as a general-purpose method for multi-F0 estimation. We conclude with a discussion on future research directions.",ESP,education,Developed economies,"[-8.186696, -37.52281]","[-28.038664, -32.55237]","[23.098309, 12.780542, -15.100236]","[-6.7137914, -7.9661245, -18.787754]","[9.918801, 11.021853]","[7.8259225, 5.297091]","[11.341099, 14.940567, 0.83367485]","[9.595606, 7.071412, 8.831659]"
86,Wayne Chi;Prachi Kumar;Suri Yaddanapudi;Suresh Rahul;Umut Isik,Generating music with a self-correcting non-chronological autoregressive model,2020,https://doi.org/10.5281/zenodo.4245578,Wayne Chi+Amazon Web Services>USA>company;Prachi Kumar+Amazon Web Services>USA>company;Suri Yaddanapudi+Amazon Web Services>USA>company;Rahul Suresh+Amazon Web Services>USA>company;Umut Isik+Amazon Web Services>USA>company,"We describe a novel approach for generating music using a self-correcting, non-chronological, autoregressive model. We represent music as a sequence of edit events, each of which denotes either the addition or removal of a note---even a note previously generated by the model. During inference, we generate one edit event at a time using direct ancestral sampling. Our approach allows the model to fix previous mistakes such as incorrectly sampled notes and prevent accumulation of errors which autoregressive models are prone to have. Another benefit of our approach is a finer degree of control during human and AI collaboration as our approach is notewise online. We show through quantitative metrics and human survey evaluation that our approach generates better results than orderless NADE and Gibbs sampling approaches.",USA,company,Developed economies,"[21.235094, -0.6641284]","[-3.6409686, -42.488388]","[3.7578876, -1.5111974, 14.709952]","[-25.95035, -2.2934902, -9.608318]","[10.203226, 8.475142]","[9.067662, 6.184833]","[13.10137, 12.19139, 0.16640669]","[9.503875, 5.464619, 9.177527]"
85,Derek S Cheng;Thorsten Joachims;Douglas Turnbull,Exploring acoustic similarity for novel music recommendation,2020,https://doi.org/10.5281/zenodo.4245500,Derek Cheng+Cornell University>USA>education;Thorsten Joachims+Cornell University>USA>education;Douglas Turnbull+Ithaca College>USA>education,"Most commercial music services rely on collaborative filtering to recommend artists and songs. While this method is effective for popular artists with large fanbases, it can present difficulties for recommending novel, lesser known artists due to a relative lack of user preference data. In this paper, we therefore seek to understand how content-based approaches can be used to more effectively recommend songs from these lesser known artists. Specifically, we conduct a user study to answer three questions. Firstly, do most users agree which songs are most acoustically similar? Secondly, is acoustic similarity a good proxy for how an individual might construct a playlist or recommend music to a friend? Thirdly, if so, can we find acoustic features that are related to human judgments of acoustic similarity? To answer these questions, our study asked 117 test subjects to compare two unknown candidate songs relative to a third known reference song. Our findings show that 1) judgments about acoustic similarity are fairly consistent, 2) acoustic similarity is highly correlated with playlist selection and recommendation, but not necessarily personal preference, and 3) we identify a subset of acoustic features from the Spotify Web API that is particularly predictive of human similarity judgments.",USA,education,Developed economies,"[-46.894455, 25.77225]","[36.699505, 15.06941]","[-6.9776626, 23.722916, -9.167191]","[14.79264, 4.7346787, 14.631025]","[15.788065, 9.371135]","[12.365937, 1.9349113]","[15.6713295, 15.746232, -1.397939]","[13.431444, 5.4329433, 12.708166]"
84,Karim M. Ibrahim;Elena V. Epure;Geoffroy Peeters;Gael Richard,Should we consider the users in contextual music auto-tagging models?,2020,https://doi.org/10.5281/zenodo.4245426,Karim M. Ibrahim+Télécom Paris>FRA>education|Deezer Research>FRA>company;Elena V. Epure+Deezer Research>FRA>company;Geoffroy Peeters+Télécom Paris>FRA>education;Gaël Richard+Télécom Paris>FRA>education,"Music tags are commonly used to describe and categorize music. Various auto-tagging models and datasets have been proposed for the automatic music annotation with tags. However, the past approaches often neglect the fact that many of these tags largely depend on the user, especially the tags related to the context of music listening. In this paper, we address this problem by proposing a user-aware music auto-tagging system and evaluation protocol. Specifically, we use both the audio content and user information extracted from the user listening history to predict contextual tags for a given user/track pair. We propose a new dataset of music tracks annotated with contextual tags per user. We compare our model to the traditional audio-based model and study the influence of user embeddings on the classification quality. Our work shows that explicitly modeling the user listening history into the automatic tagging process could lead to more accurate estimation of contextual tags.",FRA,education,Developed economies,"[-42.325188, -3.9370522]","[39.140697, 0.03326093]","[-15.222803, 15.656929, 13.402317]","[23.488499, 8.096815, 4.327703]","[14.503251, 10.620562]","[11.6140785, 3.316119]","[15.647261, 14.047122, 0.15465315]","[13.0589485, 6.289696, 11.331244]"
60,Sangeun Kum;Jing-Hua Lin;Li Su;Juhan Nam,Semi-supervised learning using teacher-student models for vocal melody extraction,2020,https://doi.org/10.5281/zenodo.4245374,"Sangeun Kum+Graduate School of Culture Technology, KAIST>KOR>education|Institute of Information Science, Academia Sinica>TWN>education;Jing-Hua Lin+Institute of Information Science, Academia Sinica>TWN>education;Li Su+Institute of Information Science, Academia Sinica>TWN>education;Juhan Nam+Graduate School of Culture Technology, KAIST>KOR>education","The lack of labeled data is a major obstacle in many music information retrieval tasks such as melody extraction, where labeling is extremely laborious or costly. Semi-supervised learning (SSL) provides a solution to alleviate the issue by leveraging a large amount of unlabeled data. In this paper, we propose an SSL method using teacher-student models for vocal melody extraction. The teacher model is pre-trained with labeled data and guides the student model to make identical predictions given unlabeled input in a self-training setting. We examine three setups of teacher-student models with different data augmentation schemes and loss functions. Also, considering the scarcity of labeled data in the test phase, we artificially generate large-scale testing data with pitch labels from unlabeled data using an analysis-synthesis method. The results show that the SSL method significantly increases the performance against supervised learning only and the improvement depends on the teacher-student models, the size of unlabeled data, the number of self-training iterations, and other training details. We also find that it is essential to ensure that the unlabeled audio has vocal parts. Finally, we show that the proposed SSL method allows a simple convolutional recurrent neural network model to achieve performance comparable to state-of-the-arts.",KOR,education,Developing economies,"[3.1854079, -13.460938]","[-26.41145, -41.031673]","[17.790943, 7.6930537, 0.0018324424]","[4.697116, -6.755903, -24.43078]","[10.042025, 9.992899]","[8.465283, 4.6831384]","[11.003342, 14.879713, -0.42907736]","[10.40746, 7.095458, 9.035022]"
61,Yin-Jyun Luo;Kin Wai Cheuk;Tomoyasu Nakano;Masataka Goto;Dorien Herremans,Unsupervised disentanglement of pitch and timbre for isolated musical instrument sounds,2020,https://doi.org/10.5281/zenodo.4245532,"Yin-Jyun Luo+Singapore University of Technology and Design (SUTD)>SGP>education|Institute of High Performance Computing, A*STAR>SGP>facility;Kin Wai Cheuk+Singapore University of Technology and Design (SUTD)>SGP>education|Institute of High Performance Computing, A*STAR>SGP>facility;Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Dorien Herremans+Singapore University of Technology and Design (SUTD)>SGP>education|Institute of High Performance Computing, A*STAR>SGP>facility","Disentangling factors of variation aims to uncover latent variables that underlie the process of data generation. In this paper, we propose a framework that achieves unsupervised pitch and timbre disentanglement for isolated musical instrument sounds without relying on data annotations or pre-trained neural networks. Our framework, based on variational auto-encoders, takes as input a spectral frame, and encodes pitch and timbre as categorical and continuous variables, respectively. The input is then reconstructed by combining those variables. Under an unsupervised training setting, a major challenge is that encoders are tasked to capture factors of interest with distinct latent representations, without access to the corresponding ground-truth labels. We therefore introduce auxiliary tasks and objectives which leverage pitch shifting as a strategy to create surrogate labels, thereby encouraging the disentanglement of pitch and timbre. Through an ablation study we analyze the impact of the proposed objectives. The evaluation shows the efficacy of the proposed framework for learning disentangled representations, and verifies its applicability to unsupervised pitch classification and conditional spectral synthesis.",SGP,education,Developing economies,"[9.96013, -29.868746]","[-11.074336, -47.537308]","[7.4833245, 9.975787, 17.747217]","[-16.638262, 1.0673379, -17.201942]","[8.948698, 7.852342]","[8.588505, 6.452484]","[11.173163, 12.771612, 0.7947918]","[9.738069, 5.8042316, 8.456221]"
62,Chitralekha Gupta;Lin Huang;Haizhou Li,Automatic rank-ordering of singing vocals with twin-neural network,2020,https://doi.org/10.5281/zenodo.4245458,Chitralekha Gupta+National University of Singapore>SGP>education;Lin Huang+National University of Singapore>SGP>education;Haizhou Li+National University of Singapore>SGP>education|University of Bremen>DEU>education,"When making judgements, humans are known to be better at choosing a preferred option amongst a small number of options, rather than giving an absolute ranking of all the options. This preference-based judgment rank-ordering method is called Best-Worst Scaling (BWS). Inspired by this concept, we propose a preference-based framework to generate a relative rank-ordering of singing vocals, and therefore, singers. We adopt a twin-neural network (Siamese) that learns to choose a preferred candidate in terms of singing quality between two inputs. With a few such pairwise comparisons, this method generates a relative rank-order of a complete list of singers. Additionally, we incorporate a knowledge-based musically-relevant pitch histogram representation, as a conditioning vector, to provide explicit musical information to the network. The experiments show that this method is able to reliably evaluate singing quality and rank-order singing vocals, independent of the song or the singer. The results suggest that the twin-neural network learns the underlying discerning properties relevant to singing quality, instead of being specific to the content of a song or singer.",SGP,education,Developing economies,"[-4.8106494, -42.45983]","[32.2689, -23.069708]","[27.222075, 12.143955, -8.120453]","[24.804855, -4.8852386, -4.864954]","[9.636965, 10.978892]","[7.9416304, 5.607464]","[11.12533, 14.969454, 1.0975847]","[9.9235525, 6.9828944, 8.720641]"
63,Cheng-Zhi Anna Huang;Hendrik Vincent Koops;Ed Newton-Rex;Monica Dinculescu;Carrie Cai,AI song contest: Human-AI co-creation in songwriting,2020,https://doi.org/10.5281/zenodo.4245530,Cheng-Zhi Anna Huang+Google Brain>USA>company;Hendrik Vincent Koops+RTL Netherlands>NLD>company;Ed Newton-Rex+ByteDance>CHN>company;Monica Dinculescu+Google Brain>USA>company;Carrie J. Cai+Google Brain>USA>company,"Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.",USA,company,Developed economies,"[-25.60904, 5.1316113]","[-15.751716, -39.52141]","[-26.415735, -3.6989408, 6.778413]","[-19.255972, 10.894714, 2.5021474]","[12.819294, 7.5183616]","[9.711242, 5.7077904]","[14.408451, 12.997181, -1.2226055]","[10.295935, 5.345962, 9.353213]"
64,Kosetsu Tsukuda;Masataka Goto,Analysis of song/artist latent features and its application for song search,2020,https://doi.org/10.5281/zenodo.4245538,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"For recommending songs to a user, one effective approach is to represent artists and songs with latent vectors and predict the user's preference toward the songs. Although the latent vectors represent the characteristics of artists and songs well, they have typically been used only for computing the preference score. In this paper, we discuss how we can leverage these vectors for realizing applications that enable users to search for songs from new perspectives. To this end, by embedding song/artist vectors into the same feature space, we first propose two concepts of artist-song relationships: overall similarity and prominent affinity. Overall similarity is the degree to which the characteristics of a song are similar overall to the characteristics of the artist; while prominent affinity is the degree to which a song prominently represents the characteristics of the artist. By using Last.fm play logs for two years, we analyze the characteristics of the concepts. Moreover, based on the analysis results, we propose three applications for song search. Through case studies, we demonstrate that our proposed applications are beneficial for searching for songs according to the users' various search intents.",JPN,facility,Developed economies,"[-25.278587, 13.441092]","[35.198536, 14.551697]","[2.4545245, 17.8096, -6.580864]","[15.046387, 1.373547, 14.584778]","[13.708722, 8.558867]","[12.100679, 2.1337523]","[13.914257, 14.988515, -1.5557377]","[13.309564, 5.547778, 12.566679]"
65,Andrew McLeod;James Owers;Kazuyoshi Yoshii,The MIDI degradation toolkit: Symbolic music augmentation and correction,2020,https://doi.org/10.5281/zenodo.4245566,Andrew McLeod+EPFL>CHE>education;James Owers+University of Edinburgh>GBR>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a ``degraded'' version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difficulty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK's degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community.",CHE,education,Developed economies,"[37.70987, 1.6075555]","[-11.947986, -11.988363]","[4.8822904, -7.5138435, 22.75304]","[-4.705512, -15.403688, -7.819705]","[10.4751, 7.332309]","[6.948722, 2.9534714]","[13.0478325, 11.671502, -0.52864325]","[8.991798, 6.645416, 10.372383]"
66,Hao-Wen Dong;Ke Chen;Julian McAuley;Taylor Berg-Kirkpatrick,MusPy: A toolkit for symbolic music generation,2020,https://doi.org/10.5281/zenodo.4245380,Hao-Wen Dong+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Ke Chen+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Julian McAuley+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Taylor Berg-Kirkpatrick+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy .",USA,education,Developed economies,"[20.990145, 10.867812]","[-4.4820013, 26.308159]","[1.6090562, -3.830298, 20.778915]","[-9.957437, 3.616828, 12.842681]","[10.384229, 8.14019]","[9.711834, 4.1834354]","[13.39282, 11.720265, -0.1694554]","[10.580831, 5.8670955, 10.893205]"
67,Philippe Esling;Théis Bazin;Adrien Bitton;Tristan J. J. Carsault;Ninon Devis,Ultra-light deep MIR by trimming lottery tickets,2020,https://doi.org/10.5281/zenodo.4245492,Philippe Esling+IRCAM - Sorbonne Université>FRA>education|CNRS>FRA>facility;Theis Bazin+IRCAM - Sorbonne Université>FRA>education|CNRS>FRA>facility;Adrien Bitton+IRCAM - Sorbonne Université>FRA>education|CNRS>FRA>facility;Tristan Carsault+IRCAM - Sorbonne Université>FRA>education|CNRS>FRA>facility;Ninon Devis+IRCAM - Sorbonne Université>FRA>education|CNRS>FRA>facility,"Current state-of-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all discriminative tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success. In this paper, we address this issue by developing a new approach based on the recent lottery ticket hypothesis. We modify the original lottery approach to allow for explicitly removing parameters, through structured trimming of entire units, instead of simply masking individual weights. This allows to obtain models which are effectively lighter in terms of size, memory and number of operations.We show that our proposal allows to remove up to 95% of the models parameters without loss of accuracy, leading to ultra-light deep MIR models. We confirm the surprising result that, at smaller compression ratios (removing up to 90% of the network), lighter models consistently outperform their heavier counterpart. We exhibit these results on a large array of MIR tasks including audio classification, pitch recognition, chord extraction, drum transcription and onset estimation. These resulting ultra-light deep models for MIR can run on CPU, and can even fit on embedded devices with minimal degradation of accuracy.",FRA,education,Developed economies,"[-7.993069, 59.31295]","[-18.647547, -37.205166]","[-40.288746, -1.1209813, -7.109486]","[-12.16855, 0.13953088, -20.04002]","[13.620409, 4.6694202]","[9.2735615, 5.1613445]","[15.053339, 11.068073, -1.5025451]","[10.269292, 6.280676, 8.984415]"
68,Gabriel Meseguer Brocal;Rachel Bittner;Simon Durand;Brian Brost,Data cleansing with contrastive learning for vocal note event annotations,2020,https://doi.org/10.5281/zenodo.4245420,Gabriel Meseguer-Brocal+Ircam/CNRS/SU>FRA>education;Rachel Bittner+Spotify>USA>company;Simon Durand+Spotify>USA>company;Brian Brost+Spotify>USA>company,"Data cleansing is a well studied strategy for cleaning erroneous labels in datasets, which has not yet been widely adopted in Music Information Retrieval.Previously proposed data cleansing models do not consider structured (e.g. time varying) labels, such as those common to music data.We propose a novel data cleansing model for time-varying, structured labels which exploits the local structure of the labels, and demonstrate its usefulness for vocal note event annotations in music.Our model is trained in a contrastive learning manner by automatically contrasting likely correct labels pairs against local deformations of them.We demonstrate that the accuracy of a transcription model improves greatly when trained using our proposed data cleaning strategy compared with the accuracy when trained using the original dataset.Additionally we use our model to estimate the annotation error rates in the DALI dataset, and highlight other potential uses for this type of model.",FRA,education,Developed economies,"[-4.5552506, -40.05036]","[-23.640108, -28.073229]","[21.831877, 16.086224, -9.496448]","[-8.595406, 1.1450768, -26.474236]","[9.800299, 11.040555]","[8.429409, 4.822546]","[11.305968, 15.053295, 1.042073]","[9.960152, 6.9598055, 9.128882]"
69,Juan S. Gómez-Cañón;Estefania Cano;Perfecto Herrera;Emilia Gomez,Joyful for you and tender for us: the influence of individual characteristics and language on emotion labeling and classification,2020,https://doi.org/10.5281/zenodo.4245568,"Juan Sebastián Gómez-Cañón+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Estefanía Cano+Songquito UG>DEU>company;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+European Commission, Joint Research Centre>ESP>facility","Tagging a musical excerpt with an emotion label may result in a vague and ambivalent exercise. This subjectivity entangles several high-level music description tasks when the computational models built to address them produce predictions on the basis of a ""ground truth"". In this study, we investigate the relationship between emotions perceived in pop and rock music (mainly in Euro-American styles) and personal characteristics from the listener, using language as a key feature. Our goal is to understand the influence of lyrics comprehension on music emotion perception and use this knowledge to improve Music Emotion Recognition (MER) models. We systematically analyze over 30K annotations of 22 musical fragments to assess the impact of individual differences on agreement, as defined by Krippendorff's coefficient. We employ personal characteristics to form group-based annotations by assembling ratings with respect to listeners' familiarity, preference, lyrics comprehension, and music sophistication. Finally, we study our group-based annotations in a two-fold approach: (1) assessing the similarity within annotations using manifold learning algorithms and unsupervised clustering, and (2) analyzing their performance by training classification models with diverse ""ground truths"". Our results suggest that a) applying a broader categorization of taxonomies and b) using multi-label, group-based annotations based on language, can be beneficial for MER models.",ESP,education,Developed economies,"[-55.143852, -0.8206825]","[47.011505, -3.440496]","[-22.607697, 27.555609, 8.579178]","[16.661394, 22.658401, 3.2545264]","[13.766032, 12.546108]","[12.787983, 3.7435954]","[16.017391, 14.625599, 1.5885596]","[13.897075, 5.306533, 10.64892]"
70,Francisco J. Castellanos;Jorge Calvo-Zaragoza;Jose M. Inesta,A neural approach for full-page optical music recognition of mensural documents,2020,https://doi.org/10.5281/zenodo.4245494,Francisco J. Castellanos+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education;Jose M. Inesta+University of Alicante>ESP>education,"The digitization of the content within musical manuscripts allows the possibility of preserving, disseminating, and exploiting that cultural heritage. The automation of this process has been object of study for a long time in the field of Optical Music Recognition (OMR), with a wide variety of proposed solutions. Currently, there is a tendency to use machine learning strategies based on neural networks because of their high performance and flexibility to adapt to different scenarios by changing only the training data. However, most of the recent literature addresses only specific parts of the traditional OMR workflow such as music object detection or symbol classification. In this paper, we progress one step further by proposing a full-page OMR system for Mensural notation scores that consists of simply two processes, which are enough to extract the symbolic music information from a full page. More precisely, our pipeline uses Selectional Auto-Encoders to extract single staff regions, combined with end-to-end staff-level recognition based on Convolutional Recurrent Neural Networks for retrieving the music notation. The results confirm the adequacy of our method, reporting a successful behavior on two Mensural collections (Capitan and Seils datasets) with a straightforward implementation.",ESP,education,Developed economies,"[41.849586, 21.722267]","[-20.933662, -23.204556]","[20.931015, 12.70542, 14.958411]","[-15.068947, -20.26687, -1.4958831]","[8.635098, 6.102201]","[6.548191, -0.9650532]","[10.599708, 11.018294, -0.2134972]","[8.022941, 4.150198, 10.0831175]"
58,Yu-Fen Huang;Jeng-I Liang;I-CHIEH WEI;Li Su,Joint analysis of mode and playing technique in guqin performance with machine learning,2020,https://doi.org/10.5281/zenodo.4245378,"Yu-Fen Huang+Institute of Information Science, Academia Sinica>TWN>facility|Taipei National University of the Arts>Unknown>education;Jeng-I Liang+Taipei National University of the Arts>Unknown>education;I-Chieh Wei+Institute of Information Science, Academia Sinica>TWN>facility;Li Su+Institute of Information Science, Academia Sinica>TWN>facility","Music is hierarchically structured, in which the global attributes (e.g., the determined tonal structure, musical form) dominate the distribution of local elements (e.g., pitch, playing technique arrangement). Existing methods for instrumental playing technique detection mostly focus on the local features extracted from audio. However, we argue that structural information is critical for both global and local tasks, particularly considering the characteristics of Guqin music. Incorporating mode and playing technique analysis, this study demonstrates that the structural relationship between notes is crucial for detecting mode, and such information also provides extra guidance for the playing technique detection in local-level. In this study, a new dataset is compiled for Guqin performance analysis. The mode detection is achieved by pattern matching, and the predicted results are conjoined with audio features to be inputted into a neural network for playing technique detection. Advanced techniques are developed to optimize the extracted pitch contour from the audio. It is manifest in the results that the global and local features are inter-connected in Guqin music. Our analysis identifies key components affecting the recognition of mode and playing technique, and challenging cases resulting from unique properties in Guqin audio signal are discussed for further research.",TWN,facility,Developing economies,"[49.0148, -26.473701]","[-37.186447, -9.224567]","[10.628534, -21.976954, -0.38775328]","[-6.0464544, 7.0972657, -19.764109]","[10.359597, 5.230999]","[7.20091, 4.0187964]","[10.644641, 12.892109, -1.6205803]","[9.098957, 7.106351, 10.263124]"
71,Fred Bruford;Olivier Lartillot;SKoT McDonald;Mark B. Sandler,Multidimensional similarity modelling of complex drum loops using the GrooveToolbox,2020,https://doi.org/10.5281/zenodo.4245418,"Fred Bruford+Centre for Digital Music, Queen Mary University>GBR>education;Olivier Lartillot+RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, University of Oslo>NOR>education;SKoT McDonald+inMusic Brands inc>USA>company;Mark Sandler+Centre for Digital Music, Queen Mary University>GBR>education","The GrooveToolbox is a new Python library implementing numerous algorithms, both novel and pre-existing, for the analysis of symbolic drum loops, including rhythm features, similarity metrics and microtiming features. As part of the GrooveToolbox we introduce two new metrics of rhythm similarity and four new features for describing the perceptually salient properties of microtiming deviations in drum loops. Based on a two-part perceptual evaluation, we show these four new microtiming features can each correlate to similarity perception, and be used along with rhythm similarity metrics to improve personalized similarity models for complex drum loops. A new measure of structural rhythmic similarity is also shown to correlate more strongly to similarity perception of drum loops than the more commonly used Hamming distance. These results point to the potential application of the GrooveToolbox and its new features in drum loop analysis for intelligent music production tools. The GrooveToolbox may be found at:  https://github.com/fredbru/GrooveToolbox",GBR,education,Developed economies,"[26.443165, -47.817867]","[-19.98325, 4.0729337]","[23.364075, -21.026575, -1.200999]","[2.4731479, 15.376144, -7.5540752]","[7.742052, 7.017057]","[6.1981235, 1.6698631]","[10.151307, 11.683728, 0.9716816]","[8.146168, 6.562967, 11.789168]"
73,HAO HAO TAN;Dorien Herremans,Music FaderNets: Controllable music generation based on high-Level features via low-level feature modelling,2020,https://doi.org/10.5281/zenodo.4245376,Hao Hao Tan+Singapore University of Technology and Design>SGP>education;Dorien Herremans+Singapore University of Technology and Design>SGP>education,"High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate ""sliding faders"" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the ""faders"" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.",SGP,education,Developing economies,"[22.980032, 7.749347]","[-9.694441, -47.935444]","[-0.2668342, 1.4000396, 24.059345]","[-14.848327, 1.6827896, -14.489517]","[10.407157, 8.365439]","[8.689556, 6.355309]","[13.49693, 11.811627, 0.13220365]","[9.771016, 5.667731, 8.514451]"
74,Sebastian Böck;Matthew Davies,"Deconstruct, analyse, reconstruct: How to improve tempo, beat, and downbeat estimation",2020,https://doi.org/10.5281/zenodo.4245498,Sebastian Böck+enliteAI>AUT>company;Matthew E. P. Davies+University of Coimbra>PRT>education,"In this paper, we undertake a critical assessment of a state-of-the-art deep neural network approach for computational rhythm analysis. Our methodology is to deconstruct this approach, analyse its constituent parts, and then reconstruct it. To this end, we devise a novel multi-task approach for the simultaneous estimation of tempo, beat, and downbeat. In particular, we seek to embed more explicit musical knowledge into the design decisions in building the network. We additionally reflect this outlook when training the network, and include a simple data augmentation strategy to increase the network's exposure to a wider range of tempi, and hence beat and downbeat information. Via an in-depth comparative evaluation, we present state-of-the-art results over all three tasks, with performance increases of up to 6% points over existing systems.",AUT,company,Developed economies,"[38.6771, -30.031397]","[-32.118034, -12.66913]","[3.4854407, -26.409517, -1.0342647]","[-6.650448, 12.17531, -17.422363]","[11.426968, 4.395267]","[4.899788, 2.4342744]","[10.739224, 13.3892975, -2.8611546]","[7.6607103, 6.9120126, 10.200011]"
75,Bo-Yu Chen;Jordan B. L. Smith;Yi-Hsuan Yang,Neural loop combiner: Neural network models for assessing the compatibility of loops,2020,https://doi.org/10.5281/zenodo.4245462,Bo-Yu Chen+Academia Sinica>TWN>education|TikTok>GBR>company;Jordan B. L. Smith+TikTok>GBR>company;Yi-Hsuan Yang+Academia Sinica>TWN>education,"Music producers who use loops may have access to thousands in loop libraries, but finding ones that are compatible is a time-consuming process; we hope to reduce this burden with automation. State-of-the-art systems for estimating compatibility, such as AutoMashUpper, are mostly rule-based and could be improved on with machine learning. To train a model, we need a large set of loops with ground truth compatibility values. No such dataset exists, so we extract loops from existing music to obtain positive examples of compatible loops, and propose and compare various strategies for choosing negative examples. For reproducibility, we curate data from the Free Music Archive. Using this data, we investigate two types of model architectures for estimating the compatibility of loops: one based on a Siamese network, and the other a pure convolutional neural network (CNN). We conducted a user study in which participants rated the quality of the combinations suggested by each model, and found the CNN to outperform the Siamese network. Both model-based approaches outperformed the rule-based one. We have opened source the code for building the models and the dataset.",TWN,education,Developing economies,"[38.436615, -6.9975204]","[-33.172634, -22.452394]","[7.7210155, -3.9308848, 19.883919]","[-16.67261, 1.3639417, -22.699213]","[9.779434, 8.395287]","[8.030597, 6.0298305]","[12.692492, 11.894883, 0.27627066]","[9.887431, 6.697133, 8.973066]"
76,Peter Van Kranenburg (Utrecht University;Meertens Institute)*,Rule mining for local boundary detection in melodies,2020,https://doi.org/10.5281/zenodo.4245422,Peter van Kranenburg+Meertens Instituut>NLD>facility,"The task of melodic segmentation is a long-standing MIR task that has not yet been solved. In this paper, a rule mining algorithm is employed to find rule sets that classify notes within their local context as phrase boundaries. Both the discovered rule set and a Random Forest Classifier trained on the same data set outperform previous methods on the task of melodic segmentation of melodies from the Essen Folk Song Collection, the Meertens Tune Collections, and the set of Bach Chorales. By inspecting the rules, some important clues are revealed about what constitutes a melodic phrase boundary, notably a prevalence of rhythm features over pitch features.",NLD,facility,Developed economies,"[6.8809595, -7.459017]","[10.375359, -2.0503123]","[10.249838, 7.449148, -1.3217196]","[-1.6263543, -5.671606, 2.5728743]","[10.627958, 9.874004]","[8.462157, 2.4379106]","[11.420736, 15.186093, -0.8002386]","[10.200137, 7.1495566, 11.919094]"
77,Jaehun Kim;Andrew M. Demetriou;Sandy Manolios;M. Stella Tavella;Cynthia C. S. Liem,"""Butter lyrics over hominy grit"": Comparing audio and psychology-based text features in MIR tasks",2020,https://doi.org/10.5281/zenodo.4245574,Jaehun Kim+Delft University of Technology>NLD>education;Andrew M. Demetriou+Delft University of Technology>NLD>education;Sandy Manolios+Delft University of Technology>NLD>education;M. Stella Tavella+Musixmatch>ITA>company;Cynthia C. S. Liem+Delft University of Technology>NLD>education,"Psychology research has shown that song lyrics are a rich source of data, yet they are often overlooked in the field of MIR compared to audio. In this paper, we provide an initial assessment of the usefulness of features drawn from lyrics for various fields, such as MIR and Music Psychology. To do so, we assess the performance of lyric-based text features on 3 MIR tasks, in comparison to audio features. Specifically, we draw sets of text features from the field of Natural Language Processing and Psychology. Further, we estimate their effect on performance while statistically controlling for the effect of audio features, by using a hierarchical regression statistical model. Lyric-based features show a small but statistically significant effect, that anticipates further research. Implications and directions for future studies are discussed.",NLD,education,Developed economies,"[-12.682478, 53.157898]","[34.432587, -16.64297]","[-33.849396, -1.5398562, 1.8145013]","[4.642425, 13.551027, 5.525598]","[13.3286085, 5.1864886]","[10.150543, 3.1347928]","[14.711241, 11.56334, -1.3675835]","[11.668386, 6.113766, 11.108925]"
78,Bas Cornelissen;Willem Zuidema;John Ashley Burgoyne,Mode classification and natural units in plainchant,2020,https://doi.org/10.5281/zenodo.4245572,"Bas Cornelissen+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education;Willem Zuidema+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education;John Ashley Burgoyne+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education","Many musics across the world are structured around multiple modes, which hold a middle ground between scales and melodies. We study whether we can classify mode in a corpus of 20,865 medieval plainchant melodies from the Cantus database. We revisit the traditional 'textbook' classification approach (using the final, the range and initial note) as well as the only prior computational study we are aware of, which uses pitch profiles. Both approaches work well, but largely reduce modes to scales and ignore their melodic character. Our main contribution is a model that reaches 93–95% F1 score on mode classification, compared to 86–90% using traditional pitch-based musicological methods. Importantly, it reaches 81–83% even when we discard all absolute pitch information and reduce a melody to its contour. The model uses tf–idf vectors and strongly depends on the choice of units: i.e., how the melody is segmented. If we borrow the syllable or word structure from the lyrics, the model outperforms all of our baselines. This suggests that, like language, music is made up of 'natural' units, in our case between the level of notes and complete phrases, a finding that may well be useful in other musics.",NLD,education,Developed economies,"[51.067284, -23.5005]","[8.00115, -3.8031988]","[13.80435, -29.383207, 5.205539]","[-0.83668244, 8.065138, 1.9773262]","[9.965169, 5.498787]","[8.63326, 2.0995617]","[10.776302, 12.849099, -0.91745883]","[10.093553, 6.930237, 12.11418]"
79,Jacob deGroot-Maggetti;Timothy R de Reuse;Laurent Feisthauer;Samuel Howes;Yaolong Ju;Suzuka Kokubu;Sylvain Margot;Néstor Nápoles López;Finn Upham,Data quality matters: Iterative corrections on a corpus of Mendelssohn string quartets and implications for MIR analysis,2020,https://doi.org/10.5281/zenodo.4245460,"Jacob de Groot-Maggetti+Schulich School of Music, McGill University>CAN>education;Timothy de Reuse+Schulich School of Music, McGill University>CAN>education;Laurent Feisthauer+University of Lille>FRA>education;Samuel Howes+Schulich School of Music, McGill University>CAN>education;Yaolong Ju+Schulich School of Music, McGill University>CAN>education;Suzuka Kokubu+Schulich School of Music, McGill University>CAN>education;Sylvain Margot+Schulich School of Music, McGill University>CAN>education;Néstor Nápoles López+Schulich School of Music, McGill University>CAN>education;Finn Upham+Schulich School of Music, McGill University>CAN>education","In this paper, we describe a workflow of successive corrections on Optical Music Recognition (OMR) generated MusicXML files and their respective outputs under Music Information Retrieval tasks. The original OMR-generated files of six Mendelssohn String Quartets were initially corrected by individual members of this interdisciplinary group, then reviewed by others to further standardize the quality and music analysis priorities of the team. Four MIR tasks are applied to each round of corrections on this collection: cadence detection, chord labeling, key finding, and monophonic pattern discovery.We measure changes in the outputs of these four MIR tasks from one round of correction to the next in order to evaluate the impact of corrections. Results show that expert revision is more beneficial to some MIR tasks than to others. The resulting corpus of curated MusicXML files is available as an open-source repository under a Creative Commons Attribution 4.0 International License for further MIR research.",CAN,education,Developed economies,"[-11.102474, 54.834938]","[-16.777784, 36.583286]","[-38.492184, -1.6154779, 0.25887287]","[-7.8334947, -20.133472, 5.9176946]","[13.340132, 5.1159925]","[6.741031, -0.40146917]","[14.750247, 11.397044, -1.3838248]","[8.10644, 4.227649, 10.713958]"
80,Luca Angioloni;Valentijn Borghuis;Lorenzo Brusci;Paolo Frasconi,"CONLON: A pseudo-song generator based on a new pianoroll, Wasserstein autoencoders, and optimal interpolations",2020,https://doi.org/10.5281/zenodo.4245576,Luca Angioloni+Università di Firenze>ITA>education;Tijn Borghuis+Eindhoven University of Technology>NLD>education|Musi-co>Unknown>company;Lorenzo Brusci+Musi-co>Unknown>company|Eindhoven University of Technology>NLD>education;Paolo Frasconi+Università di Firenze>ITA>education,"We introduce CONLON, a pattern-based MIDI generation method that employs a new lossless pianoroll-like data description in which velocities and durations are stored in separate channels. CONLON uses Wasserstein autoencoders as the underlying generative model. Its generation strategy is similar to interpolation, where MIDI pseudo-songs are obtained by concatenating patterns decoded from smooth trajectories in the embedding space, but aims to produce a smooth result in the pattern space by computing optimal trajectories as the solution of a widest-path problem. A set of surveys enrolling 69 professional musicians shows that our system, when trained on datasets of carefully selected and coherent patterns, is able to produce pseudo-songs that are musically consistent and potentially useful for professional musicians. Additional materials can be found at https://paolo-f.github.io/CONLON/ .",ITA,education,Developed economies,"[24.629225, 2.085458]","[-7.543714, -42.196213]","[8.70913, 1.3173027, 21.352287]","[-20.895065, -0.9240398, -10.932039]","[10.10391, 8.32483]","[9.01821, 6.4317403]","[13.011463, 11.800753, 0.24028425]","[9.43865, 5.4500093, 8.783378]"
81,Guillaume Doras;Furkan Yesiler;Joan Serra;Emilia Gomez;Geoffroy Peeters,Combining musical features for cover detection,2020,https://doi.org/10.5281/zenodo.4245424,"Guillaume Doras+Sacem>FRA>company|Ircam, CNRS, Sorbonne Université, STMS Lab>FRA>facility;Furkan Yesiler+Music Technology Group, Universitat Pompeu Fabra>ESP>education|European Commission, Joint Research Centre>ESP>facility;Joan Serrà+Dolby Laboratories>ESP>company;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|European Commission, Joint Research Centre>ESP>facility;Geoffroy Peeters+Telecom Paris, LTCI>FRA>education","Recent works have addressed the automatic cover detection problem from a metric learning perspective. They employ different input representations, aiming to exploit melodic or harmonic characteristics of songs and yield promising performances. In this work, we propose a comparative study of these different representations and show that systems combining melodic and harmonic features drastically outperform those relying on a single input representation. We illustrate how these features complement each other with both quantitative and qualitative analyses. We finally investigate various fusion schemes and propose methods yielding state-of-the-art performances on two publicly-available large datasets.",FRA,company,Developed economies,"[9.4502535, 42.801563]","[28.728872, -12.629437]","[0.7120039, 13.604462, -24.505222]","[18.583494, -4.7878346, 1.9434206]","[16.056137, 11.152967]","[10.279981, 2.87523]","[12.85893, 17.339577, -0.3344193]","[11.807372, 6.65225, 11.722417]"
82,Antonio Ramires;Frederic Font;Dmitry Bogdanov;Jordan B. L. Smith;Yi-Hsuan Yang;Joann Ching;Bo-Yu Chen;Yueh-Kao Wu;Hsu Wei-Han;Xavier Serra,The freesound loop dataset and annotation tool,2020,https://doi.org/10.5281/zenodo.4245430,"António Ramires+Music Technology Group, Universitat Pompeu Fabra>ESP>education|TikTok>GBR>company|Research Center for IT Innovation, Academia Sinica>TWN>facility;Frederic Font+Music Technology Group, Universitat Pompeu Fabra>ESP>education|TikTok>GBR>company|Research Center for IT Innovation, Academia Sinica>TWN>facility;Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education|TikTok>GBR>company|Research Center for IT Innovation, Academia Sinica>TWN>facility;Jordan B. L. Smith+TikTok>GBR>company;Yi-Hsuan Yang+Research Center for IT Innovation, Academia Sinica>TWN>facility;Joann Ching+Research Center for IT Innovation, Academia Sinica>TWN>facility;Bo-Yu Chen+Research Center for IT Innovation, Academia Sinica>TWN>facility;Yueh-Kao Wu+Research Center for IT Innovation, Academia Sinica>TWN>facility;Hsu Wei-Han+Research Center for IT Innovation, Academia Sinica>TWN>facility;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Music loops are essential ingredients in electronic music production, and there is a high demand for pre-recorded loops in a variety of styles. Several commercial and community databases have been created to meet this demand, but most of them are not suitable for research due to their strict licensing. In this paper, we present the Freesound Loop Dataset (FSLD), a new large-scale dataset of music loops annotated by experts. The loops originate from Freesound, a community database of audio recordings released under Creative Commons licenses, so the audio in our dataset may be redistributed. The annotations include instrument, meter, key and genre tags. We describe the methodology used to assemble and annotate the data, and report on the distribution of tags in the data and inter-annotator agreement. We also present to the community an online loop annotator tool that we developed. To illustrate the usefulness of FSLD, we present short case studies on using it to estimate tempo and key, generate new loops, and evaluate a loop separation algorithm. We anticipate that the community will find yet more uses for the data, in applications from automatic loop characterisation to algorithmic composition.",ESP,education,Developed economies,"[17.670454, 34.93291]","[-34.093887, -3.3984177]","[-21.02756, -13.551079, 8.180257]","[-13.660676, 12.246389, -8.792254]","[12.463708, 6.4114556]","[5.053628, 1.5453931]","[14.175981, 12.694271, -0.7558715]","[7.5246015, 6.650119, 11.005675]"
83,Furkan Yesiler;Joan Serra;Emilia Gomez,Less is more: Faster and better music version identification with embedding distillation,2020,https://doi.org/10.5281/zenodo.4245570,"Furkan Yesiler+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Dolby Laboratories>ESP>company|European Commission, Joint Research Centre>ESP>facility;Joan Serrà+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Dolby Laboratories>ESP>company|European Commission, Joint Research Centre>ESP>facility;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Dolby Laboratories>ESP>company|European Commission, Joint Research Centre>ESP>facility","Version identification systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made significant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99% smaller embeddings that, moreover, yield up to a 3% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop.",ESP,education,Developed economies,"[2.2973068, 43.73168]","[29.253786, -15.357104]","[0.1707381, 18.49946, -19.76582]","[15.045584, 18.235409, -8.216985]","[13.73482, 9.463896]","[10.17536, 3.4623268]","[13.145979, 16.573843, -0.60244286]","[11.586144, 6.5763726, 11.144827]"
72,André Ofner;Sebastian Stober,Modeling perception with hierarchical prediction: Auditory segmentation with deep predictive coding locates candidate evoked potentials in EEG,2020,https://doi.org/10.5281/zenodo.4245496,André Ofner+Otto von Guericke University>DEU>education;Sebastian Stober+Otto von Guericke University>DEU>education,"The human response to music combines low-level expectations that are driven by the perceptual characteristics of audio with high-level expectations from the context and the listener's expertise. This paper discusses surprisal based music representation learning with a hierarchical predictive neural network. In order to inspect the cognitive validity of the network's predictions along their time-scales, we use the network's prediction error to segment electroencephalograms (EEG) based on the audio signal. Using the NMED-T dataset on passive natural music listening we explore the automatic segmentation of audio and EEG into events using the suggested model. By averaging only the EEG signal at predicted locations, we were able to visualize auditory evoked potentials connected to local and global musical structures. This indicates the potential of unsupervised predictive learning with deep neural networks as means to retrieve musical structure from audio and as a basis to uncover the corresponding cognitive processes in the human brain.",DEU,education,Developed economies,"[-14.157112, -10.489719]","[-10.401043, -35.19529]","[-1.0121113, 1.7613803, 13.068382]","[-14.857147, -2.126312, -15.18592]","[11.961299, 8.971617]","[9.222443, 5.486897]","[13.435216, 13.295198, -0.16719541]","[10.039131, 5.686161, 9.330823]"
3,Brian Manolovitz;Mitsunori Ogihara,Practical evaluation of repeated recommendations in personalized music discovery,2020,https://doi.org/10.5281/zenodo.4245516,Brian Manolovitz+University of Miami>USA>education;Mitsunori Ogihara+University of Miami>USA>education,"Studies have shown that repeated exposures to novel songs cause an increase in a person's memory and liking. These studies are commonly verified through self-reporting emotion-based surveys. This paper proposes the ""retention rate"" as an additional parameter for evaluation. The ""retention rate"" is one at which the listener revisits the novel items. The authors hypothesize that when a person listens to novel (i.e., both unfamiliar and interesting) pieces of music, the retention rate will be proportional to the number of times the discovery engine suggests the pieces to her, as long as they remain novel. The authors have tested the hypothesis through a six-week human-subject experiment that simulates a real-world listening environment and a follow-up survey. During the experiment period, each subject received, through Discover Weekly in Spotify, suggestions for novel songs up to three times and provided evaluation. One month after the evaluation experiment, the human-subjects answered whether they had revisited the novel songs. Through the analysis of the response and survey data, the researchers conclude that the more times a listener is exposed to a song during the discovery process, the more likely she is to return to the song.",USA,education,Developed economies,"[-46.746754, 25.638777]","[42.02695, 25.939947]","[-8.215248, 24.84032, -9.334586]","[4.5369053, 16.284761, 14.570857]","[15.895434, 9.252126]","[12.668026, 1.1991098]","[15.765154, 15.696011, -1.450779]","[13.15678, 4.4788313, 11.508541]"
114,Martin Rohrmeier,Towards a formalization of musical rhythm,2020,https://doi.org/10.5281/zenodo.4245508,Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"Temporality lies at the very heart of music, and the play with rhythmic and metrical structures constitutes a major device across musical styles and genres. Rhythmic and metrical structure are closely intertwined, particularly in the tonal idiom. While there have been many approaches for modeling musical tempo, beat and meter and their inference, musical rhythm and its complexity have been comparably less explored and formally modeled. The model formulates a generative grammar of symbolic rhythmic musical structure and its internal recursive substructure. The approach characterizes rhythmic groups in alignment with meter in terms of the recursive subdivision of temporal units, as well as dependencies established by recursive operations such as preparation and different kinds of shifting (such as anticipation and delay). The model is formulated in terms of an abstract context-free grammar and applies for monophonic rhythms and harmonic rhythm.",CHE,education,Developed economies,"[7.5487895, 9.431293]","[-22.153334, 6.254187]","[-4.8855453, -18.107914, 2.3427482]","[-1.1776799, 19.05508, -8.527871]","[11.9359455, 5.976235]","[6.462907, 1.8516389]","[11.819129, 13.986962, -1.7662134]","[8.629354, 7.066377, 12.000395]"
1,Keitaro Tanaka;Takayuki Nakatsuka;Ryo Nishikimi;Kazuyoshi Yoshii;Shigeo Morishima,Multi-instrument music transcription based on deep spherical clustering of spectrograms and pitchgrams,2020,https://doi.org/10.5281/zenodo.4245436,Keitaro Tanaka+Waseda University>JPN>education;Takayuki Nakatsuka+Waseda University>JPN>education;Ryo Nishikimi+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Shigeo Morishima+Waseda Research Institute for Science and Engineering>JPN>facility,"This paper describes a clustering-based music transcription method that estimates the piano rolls of arbitrary musical instrument parts from multi-instrument polyphonic music signals. If target musical pieces are always played by particular kinds of musical instruments, a way to obtain piano rolls is to compute the pitchgram (pitch saliency spectrogram) of each musical instrument by using a deep neural network (DNN). However, this approach has a critical limitation that it has no way to deal with musical pieces including undefined musical instruments. To overcome this limitation, we estimate a condensed pitchgram with an existing instrument-independent neural multi-pitch estimator and then separate the pitchgram into a specified number of musical instrument parts with a deep spherical clustering technique. To improve the performance of transcription, we propose a joint spectrogram and pitchgram clustering method based on the timbral and pitch characteristics of musical instruments. The experimental results show that the proposed method can transcribe musical pieces including unknown musical instruments as well as those containing only predefined instruments, at the state-of-the-art transcription accuracy.",JPN,education,Developed economies,"[13.265895, -28.49894]","[-28.59624, -30.22518]","[22.815117, -1.510404, 3.6455903]","[-3.9855826, -12.442611, -15.043884]","[9.23644, 7.6714225]","[7.792733, 5.091336]","[11.627753, 12.577553, 0.35984632]","[9.349168, 7.076196, 9.042791]"
113,Meijun Liu;Eva Zangerle;Xiao Hu;Alessandro Melchiorre;Markus Schedl,"Pandemics, music, and collective sentiment: Evidence from the outbreak of COVID-19",2020,https://doi.org/10.5281/zenodo.4245394,Meijun Liu+The University of Hong Kong>HKG>education;Eva Zangerle+University of Innsbruck>AUT>education;Xiao Hu+The University of Hong Kong>HKG>education;Alessandro Melchiorre+Johannes Kepler University Linz>AUT>education|Linz Institute of Technology (LIT)>AUT>education;Markus Schedl+Johannes Kepler University Linz>AUT>education|Linz Institute of Technology (LIT)>AUT>education,"The COVID-19 pandemic causes a massive global health crisis and produces substantial economic and social distress, which in turn may cause stress and anxiety among people. Real-world events play a key role in shaping collective sentiment in a society. As people listen to music daily everywhere in the world, the sentiment of music being listened to can reflect the mood of the listeners and serve as a measure of collective sentiment. However, the exact relationship between real-world events and the sentiment of music being listened to is not clear. Driven by this research gap, we use the unexpected outbreak of COVID-19 as a natural experiment to explore how users' sentiment of music being listened to evolves before and during the outbreak of the pandemic. We employ causal inference approaches on an extended version of the LFM-1b dataset of listening events shared on Last.fm, to examine the impact of the pandemic on the sentiment of music listened to by users in different countries. We find that, after the first COVID-19 case in a country was confirmed, the sentiment of artists users listened to becomes more negative. This negative effect is pronounced for males while females' music emotion is less influenced by the outbreak of the COVID-19 pandemic. We further find a negative association between the number of new weekly COVID-19 cases and users' music sentiment. Our results provide empirical evidence that public sentiment can be monitored based on collective music listening behaviors, which can contribute to research in related disciplines.",HKG,education,Developing economies,"[-37.687572, 20.440977]","[55.13304, 1.7311355]","[-24.42784, 19.182709, -5.1730876]","[11.504294, 18.131134, 14.923334]","[15.273862, 9.0416975]","[13.19715, 2.6633546]","[15.221512, 15.22616, -1.3130411]","[13.814413, 4.641893, 11.487247]"
111,Sanga Chaki;Pranjal Doshi;Sourangshu Bhattacharya;Prof. Priyadarshi Patnaik,Explaining perceived emotion predictions in music: An attentive approach,2020,https://doi.org/10.5281/zenodo.4245388,"Sanga Chaki+Advanced Technology Development Centre, IIT Kharagpur>IND>education;Pranjal Doshi+Indian Institute of Technology Kharagpur>IND>education;Sourangshu Bhattacharya+Indian Institute of Technology Kharagpur>IND>education;Priyadarshi Patnaik+Indian Institute of Technology Kharagpur>IND>education","Dynamic prediction of perceived emotions of music is a challenging problem with interesting applications. Utilization of relevant context in audio sequence is essential for effective prediction. Existing methods have used LSTMs with modest success. In this work we describe three attentive LSTM based approaches for dynamic emotion prediction from music clips. We validate our models through extensive experimentation on standard dataset annotated with arousal-valence values in continuous time, and choose the best performer. We find that the LSTM based attention models perform better than the state of the art transformers for the dynamic emotion prediction task, both in terms of R2 and Kendall-Tau metrics. We explore individual smaller feature sets in search of a more effective one and to understand how different features contribute to perceived emotion. The spectral features are found to perform at par with the generic ComPare feature set [1]. Through attention map analysis we visualize how attention is distributed over music clips' frames for emotion prediction. It is observed that the models attend to frames which contribute to changes in reported arousal-valence values and chroma to produce better emotion predictions, effectively capturing long-term dependencies.",IND,education,Developing economies,"[-59.820786, 4.140042]","[48.846107, -10.958173]","[-24.044641, 22.781387, -0.12772937]","[8.06822, 25.30034, 2.7959728]","[14.046752, 12.699183]","[12.966246, 4.4044805]","[16.170847, 14.460588, 1.688074]","[14.145199, 4.9336615, 10.084195]"
112,Olivier Lartillot;Fred Bruford,Bistate reduction and comparison of drum patterns,2020,https://doi.org/10.5281/zenodo.4245432,Olivier Lartillot+University of Oslo>NOR>education;Fred Bruford+Queen Mary University>GBR>education,"This paper develops the hypothesis that symbolic drum patterns can be represented in a reduced form as a simple oscillation between two states, a Low state (commonly associated with kick drum events) and a High state (often associated with either snare drum or high hat). Both an onset time and an accent time is associated to each state. The systematic inference of the reduced form is formalized. This enables the specification of a rhythmic structural similarity measure on drum patterns, where reduced patterns are compared through alignment. The two-state representation allows a low computational cost alignment, once the complex topological formalization is fully taken into account. A comparison with the Hamming distance, as well as similarity ratings collected from listeners on a drum loop dataset, indicates that the bistate reduction enables to convey subtle aspects that goes beyond surface-level comparison of rhythmic textures.",NOR,education,Developed economies,"[26.219242, -46.921986]","[-19.809717, 4.569536]","[23.252811, -19.034132, 0.042821072]","[2.9354763, 15.870404, -8.634622]","[7.716653, 7.034873]","[6.2688203, 1.7042238]","[10.107739, 11.706133, 1.0068249]","[8.151651, 6.585401, 11.748481]"
2,Omar A Peracha,Improving polyphonic music models with feature-rich encoding,2020,https://doi.org/10.5281/zenodo.4245396,Omar Peracha+Humtap>USA>company,"This paper explores sequential modelling of polyphonic music with deep neural networks. While recent breakthroughs have focussed on network architecture, we demonstrate that the representation of the sequence can make an equally significant contribution to the performance of the model as measured by validation set loss. By extracting salient features inherent to the training dataset, the model can either be conditioned on these features or trained to predict said features as extra components of the sequences being modelled. We show that training a neural network to predict a seemingly more complex sequence, with extra features included in the series being modelled, can improve overall model performance significantly. We first introduce TonicNet, a GRU-based model trained to initially predict the chord at a given time-step before then predicting the notes of each voice at that time-step, in contrast with the typical approach of predicting only the notes. We then evaluate TonicNet on the canonical JSB Chorales dataset and obtain state-of-the-art results.",USA,company,Developed economies,"[22.4155, -5.4136896]","[-11.00786, -34.02623]","[8.824504, 2.2395494, 10.851192]","[-14.420521, -4.1101365, -13.554216]","[10.631687, 8.405669]","[8.501206, 5.546447]","[12.790654, 12.653661, 0.24961857]","[9.581573, 6.3761215, 9.105797]"
0,Go Shibata;Ryo Nishikimi;Kazuyoshi Yoshii,Music structure analysis based on an LSTM-HSMM hybrid model,2020,https://doi.org/10.5281/zenodo.4245362,Go Shibata+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Ryo Nishikimi+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Kazuyoshi Yoshii+Kyoto University>JPN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"This paper describes a statistical music structure analysis method that splits an audio signal of popular music into musically meaningful sections at the beat level and classifies them into predefined categories such as intro, verse, and chorus, where beat times are assumed to be estimated in advance. A basic approach to this task is to train a recurrent neural network (e.g., long short-term memory (LSTM) network) that directly predicts section labels from acoustic features. This approach, however, suffers from frequent musically unnatural label switching because the homogeneity, repetitiveness, and duration regularity of musical sections are hard to represent explicitly in the network architecture. To solve this problem, we formulate a unified hidden semi-Markov model (HSMM) that represents the generative process of homogeneous mel-frequency cepstrum coefficients, repetitive chroma features, and mel spectra from section labels, where the emission probabilities of mel spectra are computed from the posterior probabilities of section labels predicted by an LSTM. Given these acoustic features, the most likely label sequence can be estimated with Viterbi decoding. The experimental results show that the proposed LSTM-HSMM hybrid model outperformed a conventional HSMM.",JPN,education,Developed economies,"[-2.14059, 0.881168]","[-23.858433, -14.582918]","[0.7375805, -7.0958304, 0.97227263]","[-1.7940538, -2.984226, -8.994805]","[11.753108, 8.314694]","[8.630689, 5.298472]","[12.70417, 13.720599, -0.35036698]","[9.671924, 6.4079533, 9.586621]"
100,Daniel Yang;Kevin Ji;Timothy Tsai,Aligning Unsynchronized Part Recordings to a Full Mix Using Iterative Subtractive Alignment,2021,https://doi.org/10.5281/zenodo.5624563,Daniel Yang+Harvey Mudd College>USA>education;Kevin Ji+Harvey Mudd College>USA>education;TJ Tsai+Harvey Mudd College>USA>education,"This paper explores an application that would enable a group of musicians in quarantine to produce a performance of a chamber work by recording each part in isolation in a completely unsynchronized manner, and then generating a synchronized performance by aligning, time scale modifying, and mixing the individual part recordings. We focus on the main technical challenge of aligning the individual part recordings against a reference ``full mix'' recording containing a performance of the work. We propose an iterative subtractive alignment approach, in which each part recording is aligned against the full mix recording and then subtracted from it. We also explore different feature representations and cost metrics to handle the asymmetrical nature of the part--full mix comparison. We evaluate our proposed approach on two different datasets: one that is a modification of the URMP dataset that presents an idealized setting, and another that contains a small set of piano trio data collected from musicians during the pandemic specifically for this study. Compared to a standard pairwise alignment approach, we find that the proposed approach has strong performance on the URMP dataset and mixed success on the more realistic piano trio data.",USA,education,Developed economies,"[20.171057, -33.945843]","[-15.221781, -19.10748]","[4.410499, -17.694185, -15.915174]","[-0.714417, -25.313686, -0.6719766]","[10.761895, 5.965403]","[6.1227646, 0.7790998]","[11.63538, 12.494637, -1.7451555]","[8.083046, 6.1601768, 10.687316]"
26,Hugo F Flores Garcia;Aldo Aguilar;Ethan Manilow;Bryan Pardo,Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition,2021,https://doi.org/10.5281/zenodo.5624615,Hugo Flores Garcia+Northwestern University>USA>education;Aldo Aguilar+Northwestern University>USA>education;Ethan Manilow+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"Deep learning work on musical instrument recognition has generally focused on instrument classes for which we have abundant data. In this work, we exploit hierarchical relationships between instruments in a few-shot learning setup to enable classification of a wider set of musical instruments, given a few examples at inference. We apply a hierarchical loss function to the training of prototypical networks, combined with a method to aggregate prototypes hierarchically, mirroring the structure of a predefined musical instrument hierarchy. These extensions require no changes to the network architecture and new levels can be easily added or removed. Compared to a non-hierarchical few-shot baseline, our method leads to a significant increase in classification accuracy and significant decrease in mistake severity on instrument classes unseen in training.",USA,education,Developed economies,"[10.977225, -23.302204]","[-31.078648, -28.669256]","[19.256836, -5.0994325, 2.0440114]","[-11.471391, -10.514162, -18.974087]","[8.844785, 7.2247295]","[7.9631147, 4.939101]","[11.073811, 12.508265, 0.3845246]","[9.340122, 7.131925, 9.142563]"
27,Mark R H Gotham;Rainer Kleinertz;Christof Weiss;Meinard Müller;Stephanie Klauk,What if the 'When' Implies the 'What'?: Human harmonic analysis datasets clarify the relative role of the separate steps in automatic tonal analysis,2021,https://doi.org/10.5281/zenodo.5676067,"Mark Gotham+Institut für Musik und Musikwissenschaft, Technische Universität Dortmund>DEU>education|Institut für Musikwissenschaft, Saarland University>DEU>education|International Audio Laboratories Erlangen>DEU>facility;Rainer Kleinertz+Institut für Musikwissenschaft, Saarland University>DEU>education|International Audio Laboratories Erlangen>DEU>facility;Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Stephanie Klauk+Institut für Musikwissenschaft, Saarland University>DEU>education","This paper uses the emerging provision of human harmonic analyses to assess how reliably we can map from knowing only when chords and keys change to a full identification of what those chords and keys are. We do this with a simple implementation of pitch class profile matching methods, partly to provide a benchmark score against which to judge the performance of less readily interpretable machine learning systems, many of which explicitly separate these when and what tasks and provide performance evaluation for these separate stages. Additionally, as this 'oracle'-style, 'perfect' segmentation information will not usually be available in practice, we test the sensitivity of these methods to slight modifications in the position of segment boundaries by introducing deliberate errors. This study examines several corpora. The focus on is symbolic data, though we include one audio dataset for comparison. The code and corpora (of symbolic scores and analyses) are available within: https://github.com/MarkGotham/When-in-Rome",DEU,education,Developed economies,"[27.54742, 20.620295]","[-20.603212, 16.616884]","[-2.7981248, -13.903955, 29.390821]","[-19.012247, -6.7600837, 4.6783767]","[9.669974, 9.173236]","[7.7752633, 2.3941658]","[11.8956785, 13.852376, -0.9319381]","[9.944811, 7.984806, 12.325718]"
28,Juan S. Gómez-Cañón;Estefania Cano;Yi-Hsuan Yang;Perfecto Herrera;Emilia Gomez,Let's agree to disagree: Consensus Entropy Active Learning for Personalized Music Emotion Recognition,2021,https://doi.org/10.5281/zenodo.5624399,"Juan Sebastián Gómez-Cañón+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Estefanía Cano+Songquito UG>DEU>company;Yi-Hsuan Yang+Academia Sinica>TWN>education;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Joint Research Centre, European Commission>ESP>facility","Previous research in music emotion recognition (MER) has tackled the inherent problem of subjectivity through the design of personalized models -- models which predict the emotions that a particular user would perceive from music.   Personalized models are trained in a supervised manner, and are tested exclusively with the annotations provided by a specific user.   While past research has focused on model adaptation or reducing the amount of annotations required from a given user, we propose a novel methodology based on uncertainty sampling and query-by-committee methods, adopting prior knowledge from the agreement of human annotations as an oracle for active learning.   We assume that our disagreements define our personal opinions and should be considered for personalization.   We use the DEAM dataset, the current benchmark dataset for MER, to pre-train our models.   We then use the AMG1608 dataset, the largest MER dataset containing multiple annotations per musical excerpt, to re-train diverse machine learning models using active learning and evaluate personalization.   Our results suggest that our methodology can be beneficial to produce personalized classification algorithms, which exhibit different results depending on the algorithms' complexity.",ESP,education,Developed economies,"[-57.30106, -2.3935]","[46.56319, -5.7194843]","[-23.8901, 18.828266, 6.615336]","[14.817941, 21.995686, 1.5893095]","[13.957772, 12.931023]","[12.736791, 3.9939084]","[15.982097, 14.429345, 1.7171521]","[13.870579, 5.354167, 10.4531555]"
29,Curtis Hawthorne;Ian Simon;Rigel Swavely;Ethan Manilow;Jesse Engel,Sequence-to-Sequence Piano Transcription with Transformers,2021,https://doi.org/10.5281/zenodo.5624461,Curtis Hawthorne+Google Research>USA>company;Ian Simon+Google Research>USA>company;Rigel Swavely+Google Research>USA>company;Ethan Manilow+Google Research>USA>company;Jesse Engel+Google Research>USA>company,"Automatic Music Transcription has seen significant progress in recent years by training custom deep neural networks on large datasets. However, these models have required extensive domain-specific design of network architectures, input/output representations, and complex decoding schemes. In this work, we show that equivalent performance can be achieved using a generic encoder-decoder Transformer with standard decoding methods. We demonstrate that the model can learn to translate spectrogram inputs directly to MIDI-like output events for several transcription tasks. This sequence-to-sequence approach simplifies transcription by jointly modeling audio features and language-like output dependencies, thus removing the need for task-specific architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labeling rather than custom model design.",USA,company,Developed economies,"[32.973984, -6.383003]","[-18.81902, -33.014366]","[15.0634165, -8.130977, 15.471872]","[-10.127784, -3.672847, -10.04872]","[9.749985, 7.2503157]","[8.57609, 5.3333282]","[12.072798, 11.395389, -0.240945]","[9.436435, 6.352634, 9.13166]"
30,Ben Hayes;Charalampos Saitis;Gyorgy Fazekas,Neural Waveshaping Synthesis,2021,https://doi.org/10.5281/zenodo.5624613,Ben Hayes+Queen Mary University of London>GBR>education;Charalampos Saitis+Queen Mary University of London>GBR>education;George Fazekas+Queen Mary University of London>GBR>education,"We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.",GBR,education,Developed economies,"[15.439282, -32.50407]","[-20.701183, -49.10113]","[24.439592, 1.4230261, 9.056845]","[-24.344969, -4.654819, -20.996853]","[9.700265, 8.499482]","[8.028753, 6.6638837]","[12.589525, 12.118466, 0.58198506]","[9.553455, 6.0398283, 7.996374]"
31,Johannes Hentschel;Fabian C. Moss;Markus Neuwirth;Martin A Rohrmeier,A semi-automated workflow paradigm for the distributed creation and curation of expert annotations,2021,https://doi.org/10.5281/zenodo.5624417,Johannes Hentschel+École Polytechnique Fédérale de Lausanne>CHE>education;Fabian C. Moss+École Polytechnique Fédérale de Lausanne>CHE>education;Markus Neuwirth+Anton Bruckner University Linz>AUT>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"The creation and curation of labeled datasets can be an arduous, expensive, and time-consuming task. We introduce a workflow paradigm for remote consensus-building between expert annotators, while considerably reducing the associated administrative overhead through automation. Most music annotation tasks rely heavily on human interpretation and therefore defy the concept of an objective and indisputable ground truth. Thus, our paradigm invites and documents inter-annotator controversy based on a transparent set of analytical criteria, and aims at putting forth the consensual solutions emerging from such deliberations. The workflow that we suggest traces the entire genesis of annotation data, including the relevant discussions between annotators, reviewers, and curators. It adopts a well-proven pattern from collaborative software development, namely distributed version control, and allows for the automation of repetitive maintenance tasks, such as validity checks, message dispatch, or updates of meta- and paradata. To demonstrate the workflow's effectiveness, we introduce one possible implementation through GitHub Actions and showcase its success in creating cadence, phrase, and harmony annotations for a corpus of 36 trio sonatas by Arcangelo Corelli. Both code and annotated scores are freely available and the implementation can be readily used in and adapted for other MIR projects.",CHE,education,Developed economies,"[-26.572699, 42.055424]","[15.852473, 43.66862]","[-21.017675, -12.277874, 5.2952304]","[-8.655098, 8.238889, 16.597366]","[12.562747, 6.483484]","[10.873356, 1.434724]","[14.862788, 13.282857, -0.7863893]","[11.960059, 5.2990775, 11.456813]"
32,Mojtaba Heydari;Frank Cwitkowitz;Zhiyao Duan,"BeatNet: CRNN and Particle Filtering for Online Joint Beat, Downbeat and Meter Tracking",2021,https://doi.org/10.5281/zenodo.5624577,Mojtaba Heydari+University of Rochester>USA>education;Frank Cwitkowitz+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"The online estimation of rhythmic information, such as beat positions, downbeat positions, and meter, is critical for many real-time music applications. Musical rhythm comprises complex hierarchical relationships across time, rendering its analysis intrinsically challenging and at times subjective. Furthermore, systems which attempt to estimate rhythmic information in real-time must be causal and must produce estimates quickly and efficiently. In this work, we introduce an online system for joint beat, downbeat, and meter tracking, which utilizes causal convolutional and recurrent layers, followed by a pair of sequential Monte Carlo particle filters applied during inference. The proposed system does not need to be primed with a time signature in order to perform downbeat tracking, and is instead able to estimate meter and adjust the predictions over time. Additionally, we propose an information gate strategy to significantly decrease the computational cost of particle filtering during the inference step, making the system much faster than previous sampling-based methods. Experiments on the GTZAN dataset, which is unseen during training, show that the system outperforms various online beat and downbeat tracking systems and achieves comparable performance to a baseline offline joint method.",USA,education,Developed economies,"[37.469357, -34.548325]","[-29.220901, -10.648294]","[8.043595, -30.530987, -3.0629148]","[-5.952946, 14.77429, -14.078558]","[10.511134, 4.125087]","[5.129904, 2.2481582]","[10.136411, 12.880071, -2.350994]","[7.6457553, 6.8911963, 10.514433]"
33,Yuki Hiramatsu;Eita Nakamura;Kazuyoshi Yoshii,Joint Estimation of Note Values and Voices for Audio-to-Score Piano Transcription,2021,https://doi.org/10.5281/zenodo.5624411,"Yuki Hiramatsu+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education|The Hakubi Center for Advanced Research>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education|PRESTO, Japan Science and Technology Agency (JST)>JPN>facility","This paper describes an essential improvement of a state-of-the-art automatic piano transcription (APT) system that can transcribe a human-readable symbolic musical score from a piano recording. Whereas estimation of the pitches and onset times of musical notes has been improved drastically thanks to the recent advances of deep learning, estimation of note values and voice labels, which is a crucial component of the APT system, still remains a challenging task. A previous study has revealed that (i) the pitches and onset times of notes are useful but the performed note durations are less informative for estimating the note values and that (ii) the note values and voices have mutual dependency. We thus propose a bidirectional long short-term memory network that jointly estimates note values and voice labels from note pitches and onset times estimated in advance. To improve the robustness against tempo errors, extra notes, and missing notes included in the input data, we investigate data augmentation. The experimental results show the efficacy of multi-task learning and data augmentation, and the proposed method achieved better accuracies than existing methods.",JPN,education,Developed economies,"[30.637062, -4.3983874]","[-26.259426, -25.2891]","[17.225794, -2.3538997, 15.9370365]","[-10.174621, -7.538776, -10.503428]","[9.73592, 7.328628]","[8.035375, 5.3377395]","[11.963057, 11.5951, -0.23932917]","[9.091305, 6.499411, 9.079402]"
34,Yo-Wei Hsiao;Li Su,Learning note-to-note affinity for voice segregation and melody line identification of symbolic music data,2021,https://doi.org/10.5281/zenodo.5624479,Yo-Wei Hsiao+Academia Sinica>TWN>education;Li Su+Academia Sinica>TWN>education,"Voice segregation, melody line identification and other tasks of identifying the horizontal elements of music have been developed independently, although their purposes are similar. In this paper, we propose a unified framework to solve the voice segregation and melody line identification tasks of symbolic music data. To achieve this, a neural network model is trained to learn note-to-note affinity values directly from their contextual notes, in order to represent a music piece as a weighted undirected graph, with the affinity values being the edge weights. Individual voices or streams are then obtained with spectral clustering over the learned graph. Conditioned on minimal prior knowledge, the framework can achieve state-of-the-art performance on both tasks, and further demonstrates strong advantages on simulated real-world symbolic music data with missing notes and asynchronous chord notes.",TWN,education,Developing economies,"[10.746103, -12.741044]","[-14.641482, -26.548132]","[1.4701875, -9.069519, 14.783963]","[-10.998696, -11.492849, 7.0732684]","[11.507246, 7.1233506]","[8.076309, 4.024986]","[12.968559, 12.498834, -0.9562253]","[10.011732, 6.4719386, 10.136749]"
35,Jui-Yang Hsu;Li Su,VOCANO: A note transcription framework for singing voice in polyphonic music,2021,https://doi.org/10.5281/zenodo.5624383,Jui-Yang Hsu+National Taiwan University>TWN>education;Li Su+Academia Sinica>Unknown>Unknown,"High variability of singing voice and insufficiency of note event annotation present a huge bottleneck in singing voice transcription (SVT). In this paper, we present VOCANO, an open-source VOCAl NOte transcription framework built upon robust neural networks with multi-task and semi-supervised learning. Based on a state-of-the-art SVT method, we further consider virtual adversarial training (VAT), a semi-supervised learning (SSL) method for SVT on both clean and accompanied singing voice data, the latter being pre-processed using the singing voice separation (SVS) technique. The proposed framework outperforms the state of the arts on public benchmarks over a wide variety of evaluation metrics. The effects of the types of training models and the sizes of the unlabeled datasets on the performance of SVT are also discussed.",TWN,education,Developing economies,"[0.9642794, -32.763752]","[-29.182781, -41.97579]","[19.319962, 1.6638871, -8.882385]","[5.135665, -8.291284, -22.39384]","[9.708202, 10.494576]","[7.881241, 4.8033504]","[11.140133, 14.5393915, 0.563018]","[10.363289, 7.3166566, 8.938323]"
36,Rujing Huang;Bob L. T. Sturm;Andre Holzapfel,De-centering the West: East Asian Philosophies and the Ethics of Applying Artificial Intelligence to Music,2021,https://doi.org/10.5281/zenodo.5624543,Rujing Huang+KTH Royal Institute of Technology>SWE>education;Bob L. T. Sturm+KTH Royal Institute of Technology>SWE>education;Andre Holzapfel+KTH Royal Institute of Technology>SWE>education,"Questions about the ethical dimensions of artificial intelligence (AI) become more pressing as its applications multiply. While there is a growing literature calling attention to the ethics of AI in general, sector-specific and culturally sensitive approaches remain under-explored. We thus initiate an effort to establish a framework of ethical guidelines for music AI in the context of East Asia, a region whose rapid technological advances are playing a leading role in contemporary geopolitical competition. We draw a connection between technological ethics and non-Western philosophies such as Confucianism, Buddhism, Shintoism, and Daoism. We emphasize interrelations between AI and traditional cultural heritage and values. Drawing on the IEEE Principles of Ethically Aligned Design, we map its proposed ethical principles to East Asian contexts and their respective music ecosystem. In this process of establishing a culturally situated understanding of AI ethics, we see that the seemingly universal concepts of ""human rights"", ""well-being"", and potential ""misuse"" are ultimately fluid and need to be carefully examined in specific cultural contexts.",SWE,education,Developed economies,"[-27.479559, 5.7861266]","[-5.875593, 45.39392]","[-23.21, -5.603655, 6.0156913]","[-20.266048, 11.08985, 6.131919]","[12.921394, 7.425254]","[10.044123, 5.6746984]","[14.400067, 13.136292, -1.3279775]","[10.471634, 5.0233865, 9.707023]"
37,Tun Min Hung;Bo-Yu Chen;Yen Tung Yeh;Yi-Hsuan Yang,A Benchmarking Initiative for Audio-domain Music Generation using the FreeSound Loop Dataset,2021,https://doi.org/10.5281/zenodo.5624409,Tun-Min Hung+Academia Sinica>TWN>education|National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown;Bo-Yu Chen+Academia Sinica>TWN>education|National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown;Yen-Tung Yeh+Academia Sinica>TWN>education|National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown;Yi-Hsuan Yang+Academia Sinica>TWN>education|National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown,"This paper proposes a new benchmark task for generating musical passages in the audio domain by using the drum loops from the FreeSound Loop Dataset, which are publicly re-distributable. Moreover, we use a larger collection of drum loops from Looperman to establish four model-based objective metrics for evaluation, releasing these metrics as a library for quantifying and facilitating the progress of musical audio generation. Under this evaluation framework, we benchmark the performance of three recent deep generative adversarial network (GAN) models we customize to generate loops, including StyleGAN, StyleGAN2, and UNAGAN. We also report a subjective evaluation of these models. Our evaluation shows that the one based on StyleGAN2 performs the best in both objective and subjective metrics.",TWN,education,Developing economies,"[25.241585, 6.092457]","[-17.233337, -48.845325]","[5.3214197, 0.102403335, 26.155256]","[-24.702978, -0.46398, -18.330154]","[10.269805, 8.286511]","[8.283874, 6.6542835]","[13.292581, 11.705426, 0.17363223]","[9.626859, 5.916086, 8.091668]"
38,Hsiao-Tzu Hung;Joann Ching;Seungheon Doh;Nabin Kim;Juhan Nam;Yi-Hsuan Yang,EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation,2021,https://doi.org/10.5281/zenodo.5624519,Hsiao-Tzu Hung+Academia Sinica>TWN>education|National Taiwan University>TWN>education;Joann Ching+Academia Sinica>TWN>education|National Taiwan University>TWN>education;Seungheon Doh+KAIST>KOR>education;Nabin Kim+Georgia Institute of Technology>USA>education;Juhan Nam+KAIST>KOR>education;Yi-Hsuan Yang+Academia Sinica>TWN>education,"<p>While there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced &#39;yee-m&ograve;-pi-uh&#39;) dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the methodology for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks.</p>",TWN,education,Developing economies,"[-61.626846, -0.6066236]","[49.764244, -7.7902546]","[-30.024593, 19.45616, 2.5740232]","[9.989612, 20.340513, 2.8530037]","[13.938641, 12.930875]","[12.9829235, 4.171332]","[16.058937, 14.364272, 1.7500434]","[14.043422, 5.103276, 10.247457]"
39,Kevin Ji;Daniel Yang;Timothy Tsai,Piano Sheet Music Identification Using Marketplace Fingerprinting,2021,https://doi.org/10.5281/zenodo.5624375,Kevin Ji+Harvey Mudd College>USA>education|Harvey Mudd College>USA>education|Harvey Mudd College>USA>education;Daniel Yang+Harvey Mudd College>USA>education|Harvey Mudd College>USA>education|Harvey Mudd College>USA>education;TJ Tsai+Harvey Mudd College>USA>education|Harvey Mudd College>USA>education|Harvey Mudd College>USA>education,"This paper studies the problem of identifying piano sheet music based on a cell phone image of all or part of a physical page. We re-examine current best practices for large-scale sheet music retrieval through an economics perspective. In our analogy, the runtime search is like a consumer shopping in a store. The items on the shelves correspond to fingerprints, and purchasing an item corresponds to doing a fingerprint lookup in the database. From this perspective, we show that previous approaches are extremely inefficient marketplaces in which the consumer has very few choices and adopts an irrational buying strategy. The main contribution of this work is to propose a novel fingerprinting scheme called marketplace fingerprinting. This approach redesigns the system to be an efficient marketplace in which the consumer has many options and adopts a rational buying strategy that explicitly considers the cost and expected utility of each item. We also show that deciding which fingerprints to include in the database poses a type of minimax problem in which the store and the consumer have competing interests. On experiments using all solo piano sheet music images in IMSLP as a searchable database, we show that marketplace fingerprinting substantially outperforms previous approaches and achieves a mean reciprocal rank of 0.905 with sub-second average runtime.",USA,education,Developed economies,"[34.15434, 7.6348763]","[19.46838, 15.337281]","[22.206867, 5.133225, 20.028545]","[13.988442, -12.114306, 9.599969]","[10.41746, 6.901242]","[8.777247, 0.13291594]","[12.205041, 11.923452, -1.0942131]","[10.488956, 5.4562926, 13.053838]"
40,Keunhyoung Kim;Jongpil Lee;Sangeun Kum;Juhan Nam,Learning a cross-domain embedding space of vocal and mixed audio with a structure-preserving triplet loss,2021,https://doi.org/10.5281/zenodo.5625674,Keunhyoung Luke Kim+Neutune Research>KOR>company;Jongpil Lee+Neutune Research>KOR>company;Sangeun Kum+Neutune Research>KOR>company;Juhan Nam+KAIST>KOR>education,"Recent advances of music source separation have achieved high quality of vocal isolation from mix audio. This has paved the way for various applications in the area of music informational retrieval (MIR). In this paper, we propose a method to learn a cross-domain embedding space between isolated vocal and mixed audio for vocal-centric MIR tasks, leveraging a pre-trained music source separation model. Learning the cross-domain embedding was previously attempted with a triplet-based similarity model where vocal and mixed audio are encoded by two different convolutional neural networks. We improve the approach with a structure-preserving triplet loss that exploits not only cross-domain similarity between vocal and mixed audio but also intra-domain similarity within vocal tracks or mix tracks. We learn vocal embedding using a large-scaled dataset and evaluate it in singer identification and query-by-singer tasks. In addition, we use the vocal embedding for vocal-based music tagging and artist classification in transfer learning settings. We show that the proposed model significantly improves the previous cross-domain embedding model, particularly when the two embedding spaces from isolated vocals and mixed audio are concatenated.",KOR,company,Developing economies,"[3.133414, -35.717846]","[5.9343657, -27.121061]","[24.80924, 15.74239, -4.0284133]","[-2.7607417, -1.8904349, -24.068184]","[9.950311, 10.378381]","[9.701515, 4.652773]","[11.37499, 14.46358, 0.79199904]","[10.916579, 6.577792, 9.184774]"
41,Qiuqiang Kong;Yin Cao;Haohe Liu;Keunwoo Choi;Yuxuan Wang,Decoupling Magnitude and Phase Estimation with Deep ResUNet for Music Source Separation,2021,https://doi.org/10.5281/zenodo.5624475,Qiuqiang Kong+ByteDance>CHN>company|University of Surrey>GBR>education;Yin Cao+University of Surrey>GBR>education;Haohe Liu+ByteDance>CHN>company;Keunwoo Choi+ByteDance>CHN>company;Yuxuan Wang+ByteDance>CHN>company,"Deep neural network based methods have been successfully applied to music source separation. They typically learn a mapping from a mixture spectrogram to a set of source spectrograms, all with magnitudes only. This approach has several limitations: 1) its incorrect phase reconstruction degrades the performance, 2) it limits the magnitude of masks between 0 and 1 while we observe that 22% of time-frequency bins have ideal ratio mask values of over~1 in a popular dataset, MUSDB18, 3) its potential on very deep architectures is under-explored. Our proposed system is designed to overcome these. First, we propose to estimate phases by estimating complex ideal ratio masks (cIRMs) where we decouple the estimation of cIRMs into magnitude and phase estimations. Second, we extend the separation method to effectively allow the magnitude of the mask to be larger than~1. Finally, we propose a residual UNet architecture with up to 143 layers. Our proposed system achieves a state-of-the-art MSS result on the MUSDB18 dataset, especially, a SDR of 8.98 dB on vocals, outperforming the previous best performance of 7.24 dB. The source code is available at: https://github.com/bytedance/music_source_separation.",CHN,company,Developing economies,"[9.418778, -46.058025]","[-39.99354, -34.02613]","[29.469206, -0.34284458, -6.9220605]","[-14.698061, -6.431057, -25.628448]","[8.504164, 9.963883]","[6.830492, 5.8913956]","[11.084058, 13.7101145, 1.5579796]","[9.649618, 8.266529, 8.801077]"
42,Filip Korzeniowski;Sergio Oramas;Fabien Gouyon,Artist Similarity Using Graph Neural Networks,2021,https://doi.org/10.5281/zenodo.5625676,Filip Korzeniowski+Pandora Media LLC.>USA>company;Sergio Oramas+Pandora Media LLC.>USA>company;Fabien Gouyon+Pandora Media LLC.>USA>company,"Artist similarity plays an important role in organizing, understanding, and subsequently, facilitating discovery in large collections of music. In this paper, we present a hybrid approach to computing similarity between artists using graph neural networks trained with triplet loss. The novelty of using a graph neural network architecture is to combine the topology of a graph of artist connections with content features to embed artists into a vector space that encodes similarity.  To evaluate the proposed method, we compile the new OLGA dataset, which contains artist similarities from AllMusic, together with content features from AcousticBrainz. With 17,673 artists, this is the largest academic artist similarity dataset that includes content-based features to date.  Moreover, we also showcase the scalability of our approach by experimenting with a much larger proprietary dataset.  Results show the superiority of the proposed approach over current state-of-the-art methods for music similarity.  Finally, we hope that the OLGA dataset will facilitate research on data-driven models for artist similarity.",USA,company,Developed economies,"[-41.597576, 7.139922]","[35.38701, 7.409363]","[-25.84414, 11.349539, 8.527745]","[16.51326, 5.683145, 9.862014]","[14.168104, 10.049524]","[11.754663, 2.6288133]","[14.990663, 14.848574, -0.008691844]","[13.135155, 6.065922, 12.174447]"
43,Jin Ha Lee;Arpita Bhattacharya;Ria Antony;Nicole Santero;Anh Le,"""Finding Home"": Understanding How Music Supports Listeners' Mental Health through a Case Study of BTS",2021,https://doi.org/10.5281/zenodo.5624569,"Jin Ha Lee+University of Washington>USA>education;Arpita Bhattacharya+University of California, Irvine>USA>education;Ria Antony+University of Washington>USA>education;Nicole Santero+University of Nevada, Las Vegas>USA>education;Anh Le+University of Washington>USA>education","The positive impact of music on people's mental health and wellbeing has been well researched in music psychology, but there is a dearth of research exploring the implications of these benefits for the design of commercial music services (CMS). In this paper, we investigate how popular music can support the listener's mental health through a case study of fans of the music group BTS, with a goal of understanding how they perceive and describe the way music is influencing their mental health. We aim to derive specific design implications for CMS to facilitate such support for fans' mental health and wellbeing. Through an online survey of 1190 responses, we identify and discuss the patterns of seven different mood regulations along with major themes on fans' lived experiences of how BTS's music (1) provides comfort, (2) catalyzes self-growth, and (3) facilitates coping. We conclude the study with discussion of four specific suggestions for CMS features incorporating (1) visual elements, (2) non-music media, (3) user-generated content for collective sense-making, and (4) metadata related to mood and lyrical content that can facilitate the mental health support provided by popular music.",USA,education,Developed economies,"[-36.71665, 21.286444]","[42.195454, 32.160713]","[-23.294268, 18.165222, -6.4697022]","[9.407331, 17.623978, 17.234192]","[15.21156, 8.75482]","[13.008975, 1.0484896]","[15.167303, 15.22365, -1.4356598]","[13.455445, 4.306368, 11.7206135]"
44,Harin Lee;Frank Höger;Marc Schönwiesner;Minsu Park;Nori Jacoby,Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood Detection Algorithms,2021,https://doi.org/10.5281/zenodo.5625680,Harin Lee+Max Planck Institute for Human Cognitive and Brain Sciences>DEU>facility|Max Planck Institute for Empirical Aesthetics>DEU>facility|Leipzig University>DEU>education|New York University Abu Dhabi>USA>education;Frank Höger+Max Planck Institute for Empirical Aesthetics>DEU>facility;Marc Schönwiesner+Max Planck Institute for Human Cognitive and Brain Sciences>DEU>facility|Leipzig University>DEU>education;Minsu Park+New York University Abu Dhabi>USA>education;Nori Jacoby+Max Planck Institute for Empirical Aesthetics>DEU>facility,"Do people from different cultural backgrounds perceive the mood in music the same way? How closely do human ratings across different cultures approximate automatic mood detection algorithms that are often trained on corpora of predominantly Western popular music? Analyzing 166 participants' responses from Brazil, South Korea, and the US, we examined the similarity between the ratings of nine categories of perceived moods in music and estimated their alignment with four popular mood detection algorithms. We created a dataset of 360 recent pop songs drawn from major music charts of the countries and constructed semantically identical mood descriptors across English, Korean, and Portuguese languages. Multiple participants from the three countries rated their familiarity, preference, and perceived moods for a given song. Ratings were highly similar within and across cultures for basic mood attributes such as sad, cheerful, and energetic. However, we found significant cross-cultural differences for more complex characteristics such as dreamy and love. To our surprise, the results of mood detection algorithms were uniformly correlated across human ratings from all three countries and did not show a detectable bias towards any particular culture. Our study thus suggests that the mood detection algorithms can be considered as an objective measure at least within the popular music context.",DEU,facility,Developed economies,"[-52.387146, 4.501105]","[54.534264, -2.4573646]","[-17.49693, 29.219898, 6.900001]","[11.974155, 20.99024, 11.688432]","[13.384389, 12.54472]","[13.211367, 3.5235593]","[16.070415, 14.983901, 1.4852642]","[14.231364, 5.004518, 10.889235]"
45,Jordan Lenchitz,Reconsidering quantization in MIR,2021,https://doi.org/10.5281/zenodo.5624645,Jordan Lenchitz+Florida State University>USA>education,"This paper presents a critique of the ubiquity of boilerplate quantizations in MIR research relative to the paucity of engagement with their methodological implications. The wide-ranging consequences of reflexivity on the future of scholarly inquiry combined with the near-universal contemporary recognition of the need to broaden the scope of MIR research invite and merit critical attention. To that end, focusing primarily on twelve-tone equal-tempered pitch and dyadic rhythm models, we explore the practical, cultural, perceptual, historical, and epistemological consequences of these pervasive quantizations. We analyze several case studies of meaningful and successful past research that balanced practicality with methodological validity in order to posit several best practices for both future intercultural studies and research centered on more narrowly constructed corpora. We conclude with a discussion of the dangers of solutionism on the one hand and the self-fulfilling prophecies of status quoism on the other as well as an emphasis on the need for intellectual honesty in metatheoretical discourse.",USA,education,Developed economies,"[-9.676009, 57.2537]","[20.894655, 46.738945]","[-39.058315, -1.1167208, -4.5298915]","[-6.1133075, 10.17904, 20.377949]","[13.637593, 4.6940293]","[11.7585945, 0.29539663]","[15.043347, 11.079841, -1.4864887]","[11.919003, 4.502493, 11.565549]"
46,Liwei Lin;Gus Xia;Qiuqiang Kong;Junyan Jiang,"A unified model for zero-shot music source separation, transcription and synthesis",2021,https://doi.org/10.5281/zenodo.5624623,Liwei Lin+New York University Shanghai>USA>education;Qiuqiang Kong+ByteDance>CHN>company;Junyan Jiang+New York University Shanghai>USA>education;Gus Xia+New York University Shanghai>USA>education,"<p>We propose a unified model for three inter-related tasks: 1) to separate&nbsp;individual sound sources from a mixed music audio, 2) to transcribe&nbsp;each sound source to MIDI notes, and 3) to synthesize&nbsp;new pieces based on the timbre of separated sources. The model is inspired by the fact that when humans listen to music, our minds can not only separate the sounds of different instruments, but also at the same time perceive high-level representations such as score and timbre. To mirror such capability computationally, we designed a pitch-timbre disentanglement module based on a popular encoder-decoder neural architecture for source separation. The key inductive biases are vector-quantization for pitch representation and pitch-transformation invariant for timbre representation. In addition, we adopted a query-by-example method to achieve zero-shot&nbsp;learning, i.e., the model is capable of doing source separation, transcription, and synthesis for unseen&nbsp;instruments. The current design focuses on audio mixtures of two monophonic instruments. Experimental results show that our model outperforms existing multi-task baselines, and the transcribed score serves as a powerful auxiliary for separation tasks.</p>",USA,education,Developed economies,"[7.1128936, -46.056572]","[-39.449276, -35.835243]","[30.527584, -3.6613545, -2.1783786]","[-16.126057, -5.0285115, -23.555828]","[8.3580475, 10.0178995]","[7.034297, 5.97537]","[11.039784, 13.61357, 1.6382295]","[9.605488, 8.139019, 8.729774]"
47,Carlos Lordelo;Emmanouil Benetos;Simon Dixon;Sven Ahlbäck,Pitch-Informed Instrument Assignment using a Deep Convolutional Network with Multiple Kernel Shapes,2021,https://doi.org/10.5281/zenodo.5625682,Carlos Lordelo+Queen Mary University of London>GBR>education|Doremir Music Research AB>SWE>company;Emmanouil Benetos+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education;Sven Ahlbäck+Doremir Music Research AB>SWE>company,"This paper proposes a deep convolutional neural network for performing note-level instrument assignment. Given a polyphonic multi-instrumental music signal along with its ground truth or predicted notes, the objective is to assign an instrumental source for each note. This problem is addressed as a pitch-informed classification task where each note is analysed individually. We also propose to utilise several kernel shapes in the convolutional layers in order to facilitate learning of timbre-discriminative feature maps. Experiments on the MusicNet dataset using 7 instrument classes show that our approach is able to achieve an average F-score of 0.904 when the original multi-pitch annotations are used as the pitch information for the system, and that it also excels if the note information is provided using third-party multi-pitch estimation algorithms. We also include ablation studies investigating the effects of the use of multiple kernel shapes and comparing different input representations for the audio and the note-related information.",GBR,education,Developed economies,"[12.770962, -23.787924]","[-28.559443, -29.696064]","[15.712133, -4.6892185, 3.6270845]","[-9.361929, -8.573943, -16.52391]","[8.708445, 7.1902757]","[7.9408593, 5.1417184]","[11.083245, 12.452001, 0.39287552]","[9.447308, 7.0032964, 8.997693]"
25,Giovanni Gabbolini;Derek Bridge,An interpretable music similarity measure based on path interestingness,2021,https://doi.org/10.5281/zenodo.5624649,Giovanni Gabbolini+University College Cork>IRL>education|Insight Centre for Data Analytics>IRL>facility;Derek Bridge+University College Cork>IRL>education|Insight Centre for Data Analytics>IRL>facility,"We introduce a novel and interpretable path-based music similarity measure. Our similarity measure assumes that items, such as songs and artists, and information about those items are represented in a knowledge graph. We find paths in the graph between a seed and a target item; we score those paths based on their interestingness; and we aggregate those scores to determine the similarity between the seed and the target. A distinguishing feature of our similarity measure is its interpretability. In particular, we can translate the most interesting paths into natural language, so that the causes of the similarity judgements can be readily understood by humans. We compare the accuracy of our similarity measure with other competitive path-based similarity baselines in two experimental settings and with four datasets.  The results highlight the validity of our approach to music similarity, and demonstrate that path interestingness scores can be the basis of an accurate and interpretable similarity measure.",IRL,education,Developed economies,"[-4.9933157, 14.316769]","[33.43575, 8.508844]","[-4.101512, 8.816583, -1.6391248]","[13.890267, 3.7296681, 10.479242]","[13.163979, 9.391962]","[11.511529, 2.4250736]","[13.621211, 15.233909, -0.6286165]","[12.901047, 6.2671895, 12.413757]"
24,Dave Foster;Simon Dixon,Filosax: A Dataset of Annotated Jazz Saxophone Recordings,2021,https://doi.org/10.5281/zenodo.5625643,Dave Foster+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"The Filosax dataset is a large collection of specially commissioned recordings of jazz saxophonists playing with commercially available backing tracks. Five participants each recorded themselves playing the melody, interpreting a transcribed solo and improvising on 48 tracks, giving a total of around 24 hours of audio data. The solos are annotated both as individual note events with physical timing, and as sheet music with a metrical interpretation of the timing. In this paper, we outline the criteria used for choosing and sourcing the repertoire, the recording process and the semi-automatic transcription pipeline. We demonstrate the use of the dataset to analyse musical phenomena such as swing timing and dynamics of typical musical figures, as well as for training a source activity detection system and predicting expressive characteristics. Other potential applications include the modelling of jazz improvisation, performer identification, automatic music transcription, source separation and music generation.",GBR,education,Developed economies,"[9.0658455, 13.534561]","[-34.599197, 2.3072515]","[-9.017714, -5.03356, 27.328289]","[-12.978942, -3.9277189, -1.596199]","[10.798554, 9.440099]","[7.3711276, 3.4253314]","[12.13665, 14.324438, -0.63447815]","[9.15639, 6.5503836, 10.903177]"
23,Francesco Foscarin;Nicolas Audebert;Raphael Fournier-S'Niehotta,PKSpell: Data-Driven Pitch Spelling and Key Signature Estimation,2021,https://doi.org/10.5281/zenodo.5624435,"Francesco Foscarin+CEDRIC (EA4629), CNAM Paris>FRA>education;Nicolas Audebert+CEDRIC (EA4629), CNAM Paris>FRA>education;Raphaël Fournier S’niehotta+CEDRIC (EA4629), CNAM Paris>FRA>education","We present PKSpell: a data-driven approach for the joint estimation of pitch spelling and key signatures from MIDI files. Both elements are fundamental for the production of a full-fledged musical score and facilitate many MIR tasks such as harmonic analysis, section identification, melodic similarity, and search in a digital music library.    We design a deep recurrent neural network model that only requires information readily available in all kinds of MIDI files, including performances, or other symbolic encodings. We release a model trained on the ASAP dataset. Our system can be used with these pre-trained parameters and is easy to integrate into a MIR pipeline. We also propose a data augmentation procedure that helps re-training on small datasets.    PKSpell achieves strong key signature estimation performance on a challenging dataset. Most importantly, this model establishes a new state-of-the-art performance on the MuseData pitch spelling dataset without retraining.",FRA,education,Developed economies,"[26.768824, -18.810524]","[-15.770252, -32.501602]","[18.083757, -16.137796, -9.506384]","[-12.435315, -4.973795, -10.929534]","[9.890553, 5.840171]","[8.546498, 5.318126]","[10.978504, 13.296541, -0.73237264]","[9.322899, 6.297371, 9.304885]"
22,Christoph Finkensiep;Martin A Rohrmeier,Modeling and Inferring Proto-Voice Structure in Free Polyphony,2021,https://doi.org/10.5281/zenodo.5624431,Christoph Finkensiep+École Polytechnique Fédérale de Lausanne>CHE>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"Voice leading is considered to play an important role in the structure of Western tonal music.  However, the explicit voice assignment of a piece (if present at all)  generally does not reflect all phenomena related to voice leading.  Instead, voice-leading phenomena can occur in free textures (e.g., in most keyboard music),  or cut across the explicitly notated voices  (e.g., through implicit polyphony within a single voice).  This paper presents a model of proto-voices,  voice-like structures that encode sequential and vertical relations between notes  without the need to assume explicit voices.  Proto-voices are constructed by recursive combination of primitive structural operations,  such as insertion of neighbor or passing notes,  or horizontalization of simultaneous notes.  Together, these operations give rise to a grammar-like hierarchical system  that can be used to infer the structural fabric of a piece using a chart parsing algorithm.  Such a model can serve as a foundation  for defining higher-level latent entities (such as harmonies or voice-leading schemata),  explicitly linking them to their realizations on the musical surface.",CHE,education,Developed economies,"[3.128192, -31.26335]","[-11.676927, 16.92661]","[21.998024, 12.043585, -0.71361727]","[-8.100614, -5.623398, 4.756977]","[9.626138, 8.100651]","[7.9831724, 1.8655932]","[11.275877, 14.096172, 0.34387028]","[9.673113, 7.1101594, 12.1903925]"
0,Rohit M A;Amitrajit Bhattacharjee;Preeti Rao,Four-way Classification of Tabla Strokes with Models Adapted from Automatic Drum Transcription,2021,https://doi.org/10.5281/zenodo.5624489,Rohit M A+Indian Institute of Technology Bombay>IND>education;Amitrajit Bhattacharjee+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"Motivated by musicological applications of the four-way categorization of tabla strokes, we consider automatic classification methods that are potentially robust to instrument differences. We present a new, diverse tabla dataset suitably annotated for the task. The acoustic correspondence between the tabla stroke categories and the common popular Western drum types motivates us to adapt models and methods from automatic drum transcription. We start by exploring the use of transfer learning on a state-of-the-art pre-trained multiclass CNN drums model. This is compared with 1-way models trained separately for each tabla stroke class. We find that the 1-way models provide the best mean f-score while the drums pre-trained and tabla-adapted 3-way models generalize better for the most scarce target class. To improve model robustness further, we investigate both drums and tabla-specific data augmentation strategies.",IND,education,Developing economies,"[30.105135, -46.531754]","[-39.887516, -12.755168]","[24.497635, -19.988941, 4.085899]","[2.951558, 8.786912, -19.78827]","[7.619255, 7.445614]","[8.259498, 4.388056]","[10.658795, 11.44268, 1.1214129]","[9.143174, 7.037797, 9.650311]"
1,Taketo Akama,A Contextual Latent Space Model: Subsequence Modulation in Melodic Sequence,2021,https://doi.org/10.5281/zenodo.5624425,Taketo Akama+Sony Computer Science Laboratories>JPN>company,"Some generative models for sequences such as music and text allow us to edit only subsequences, given surrounding context sequences, which plays an important part in steering generation interactively. However, editing subsequences mainly involves randomly resampling subsequences from a possible generation space. We propose a contextual latent space model (CLSM) in order for users to be able to explore subsequence generation with a sense of direction in the generation space, e.g., interpolation, as well as exploring variations—semantically similar possible subsequences. A context-informed prior and decoder constitute the generative model of CLSM, and a context position-informed encoder is the inference model. In experiments, we use a monophonic symbolic music dataset, demonstrating that our contextual latent space is smoother in interpolation than baselines, and the quality of generated samples is superior to baseline models. The generation examples are available online.",JPN,company,Developed economies,"[14.086684, -4.4064]","[-9.074877, -44.090782]","[6.5505023, 6.653866, 5.0849605]","[-20.2606, 5.60304, -13.679628]","[10.800439, 9.528665]","[8.929605, 6.494128]","[11.877385, 15.005163, -0.6077669]","[9.590298, 5.5140963, 8.597007]"
2,María Alfaro-Contreras;David Rizo;Jose M. Inesta;Jorge Calvo-Zaragoza,OMR-assisted transcription: a case study with early prints,2021,https://doi.org/10.5281/zenodo.5625663,María Alfaro-Contreras+University of Alicante>ESP>education;David Rizo+University of Alicante>ESP>education;Jose M. Iñesta+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"Most of the musical heritage is only available as physical documents, given that the engraving process was carried out by handwriting or typesetting until the end of the 20th century. Their mere availability as scanned images does not enable tasks such as indexing or editing unless they are transcribed into a structured digital format. Given the cost and time required for manual transcription, Optical Music Recognition (OMR) presents itself as a promising alternative. Quite often, OMR systems show acceptable but not perfect performance, which eventually leaves them out of the transcription process. On the assumption that OMR systems might always make some errors, it is essential that the user corrects the output. This paper contributes to a better understanding of how music transcription is improved by the assistance of OMR systems that include the end-user in the recognition process. For that, we have measured the transcription time of a printed early music work under two scenarios: a manual one and a state-of-the-art OMR-assisted one, with several alternatives each. Our results demonstrate that using OMR remarkably reduces users' effort, even when its performance is far optimal, compared to the fully manual option.",ESP,education,Developed economies,"[33.146095, 30.314764]","[-22.435122, 38.057316]","[6.269951, -20.135122, 21.415928]","[-12.372489, -21.334097, 1.107805]","[9.64789, 6.3044734]","[6.7363224, -0.6915055]","[12.647627, 11.294762, -1.4945117]","[7.8821154, 4.1096725, 10.603982]"
3,Stefan A Baumann,Deeper Convolutional Neural Networks and Broad Augmentation Policies Improve Performance in Musical Key Estimation,2021,https://doi.org/10.5281/zenodo.5624477,Stefan Andreas Baumann+Unknown>Unknown>Unknown,"In recent years, complex convolutional neural network architectures such as the Inception architecture have been shown to offer significant improvements over previous architectures in image classification. So far, little work has been done applying these architectures to music information retrieval tasks, with most models still relying on sequential neural network architectures. In this paper, we adapt the Inception architecture to the specific needs of harmonic music analysis and use it to create a model (InceptionKeyNet) for the task of key estimation. We then show that the resulting model can significantly outperform state-of-the-art single-task models when trained on the same datasets. Additionally, we evaluate a broad range of augmentation methods and find that extending augmentation policies to include a more diverse set of methods further improves accuracy. Finally, we train both the proposed and state-of-the-art single-task models on differently sized training datasets and different augmentation policies and compare the differences in generalization performance.",Unknown,Unknown,Unknown,"[11.526054, -18.63392]","[-25.903088, -34.816772]","[15.5927925, -0.221224, 4.2420797]","[-3.5474322, -5.57881, -18.562979]","[10.953539, 7.5791054]","[8.261468, 5.2128463]","[12.875143, 12.786123, 0.19235158]","[9.90823, 6.8042684, 9.025346]"
4,Axel Berndt,The Music Performance Markup Format and Ecosystem,2021,https://doi.org/10.5281/zenodo.5624429,Axel Berndt+Ostwestfalen-Lippe University of Applied Sciences and Arts>DEU>education|Detmold University of Music>DEU>education,"Music Performance Markup (MPM) is a new XML format that offers a model-based, systematic approach for describing and analysing musical performances. Its foundation is a set of mathematical models that capture the characteristics of performance features such as tempo, rubato, dynamics, articulations, and metrical accentuations. After a brief introduction to MPM, this paper will put the focus on the infrastructure of documentations, software tools and ongoing development activities around the format.",DEU,education,Developed economies,"[-12.474397, 28.315777]","[3.32328, 37.960377]","[-9.177131, -9.129868, -12.2358055]","[-10.212615, -4.026682, 21.75819]","[13.5694475, 7.1442447]","[9.928872, 0.2397666]","[13.990085, 13.413407, -1.8828701]","[10.43043, 5.1077437, 11.758801]"
5,Louis Bigo;David Regnier;Nicolas Martin,Identification of rhythm guitar sections in symbolic tablatures,2021,https://doi.org/10.5281/zenodo.5624513,David Régnier+University of Lille>FRA>education;Nicolas Martin+Arobas Music>FRA>company;Louis Bigo+University of Lille>FRA>education,"Sections of guitar parts in pop/rock songs are commonly described by functional terms including for example rhythm guitar, lead guitar, solo or riff. At a low level, these terms generally involve textural properties, for example whether the guitar tends to play chords or single notes. At a higher level, they indicate the function the guitar is playing relative to other instruments of the ensemble, for example whether the guitar is accompanying in background, or if it is intended to play a part in the foreground. Automatic labelling of instrumental function has various potential applications including the creation of consistent datasets dedicated to the training of generative models that focus on a particular function. In this paper, we propose a computational method to identify rhythm guitar sections in symbolic tablatures. We define rhythm guitar as sections that aim at making the listener perceive the chord progression that characterizes the harmony part of the song. A set of 31 high level features is proposed to predict if a bar in a tablature should be labeled as rhythm guitar or not. These features are used by an LSTM classifier which yields to a F1 score of 0.95 on a dataset of 102 guitar tablatures with manual function annotations. Manual annotations and computed feature vectors are publicly released.",FRA,education,Developed economies,"[46.64401, -9.104336]","[-42.317257, -6.8677373]","[24.973248, -12.350963, 4.8732166]","[-16.474625, -6.448381, -4.00454]","[7.667462, 8.218998]","[7.31019, 4.303765]","[11.610517, 11.19897, 1.3570583]","[9.13968, 6.63008, 10.076358]"
6,Charles Brazier;Gerhard Widmer,On-Line Audio-to-Lyrics Alignment Based on a Reference Performance,2021,https://doi.org/10.5281/zenodo.5625665,"Charles Brazier+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Audio-to-lyrics alignment has become an increasingly active research task in MIR, supported by the emergence of several open-source datasets of audio recordings with word-level lyrics annotations. However, there are still a number of open problems, such as a lack of robustness in the face of severe duration mismatches between audio and lyrics representation; a certain degree of language-specificity caused by acoustic differences across languages; and the fact that most successful methods in the field are not suited to work in real-time. Real-time lyrics alignment (tracking) would have many useful applications, such as fully automated subtitle display in live concerts and opera. In this work, we describe the first real-time-capable audio-to-lyrics alignment pipeline that is able to robustly track the lyrics of different languages, without additional language information. The proposed model predicts, for each audio frame, a probability vector over (European) phoneme classes, using a very small temporal context, and aligns this vector with a phoneme posteriogram matrix computed beforehand from another recording of the same work, which serves as a reference and a proxy to the written-out lyrics. We evaluate our system's tracking accuracy on the challenging genre of classical opera. Finally, robustness to out-of-training languages is demonstrated in an experiment on Jingju (Beijing opera).",AUT,education,Developed economies,"[-27.878426, -34.230106]","[-10.797663, -22.29595]","[10.52099, 19.272741, -0.4204822]","[3.1060345, -2.6402051, -19.824354]","[11.176352, 11.753064]","[6.545545, 1.3804764]","[12.191229, 15.808914, 1.0973387]","[8.618418, 6.2884603, 10.49419]"
7,Aaron Carter-Enyi;Gilad Rabinovitch;Nathaniel Condit-Schultz,Visualizing Intertextual Form with Arc Diagrams: Contour and Schema-based Methods,2021,https://doi.org/10.5281/zenodo.5624583,Aaron Carter-Enyi+Morehouse College>USA>education;Gilad Rabinovitch+Florida State University>USA>education;Nathaniel Condit-Schultz+Georgia Institute of Technology>USA>education,"The visualizations in Wattenberg's Shape of Song (2001) were based on pitch-string matching, but there are many other equivalence classes and similarity relations proposed by music research. This paper applies recent algorithms by Carter-Enyi (2016) and Carter-Enyi and Rabinovitch (2021) with the intention of making arc diagrams more effective for research and teaching. We first draw on Barber's intertextual analysis of Yoruba Oriki, in which tone language texts are circulated through various performances (Barber 1984). Intertextuality is exemplified through a 2018 composition by Nigerian composer Ayo Oluranti, then extended to Dizzy Gillespie's solo in his recording of ""Blue Moon"" (ca. 1952). Example visualizations are produced through an open-source implementation, ATAVizM, which brings together contour theory (Quinn 1997), schema theory (Gjerdingen 2007), and edit distance (Orpen and Huron 1992). Applications to the music of Bach and Mozart demonstrate that an African-centered analytical methodology has utility for music research at large. Computational music research can benefit from analytical approaches that draw upon humanistic theory and are applicable to a variety of musics.",USA,education,Developed economies,"[-9.45286, 40.431248]","[-6.3881283, 28.638552]","[-28.040709, -5.768432, 9.811923]","[-10.983117, 5.41041, 8.97313]","[12.528716, 6.9555893]","[8.158491, 1.5636618]","[14.101447, 12.667162, -1.4111634]","[9.877928, 6.46758, 12.107179]"
8,Francisco J. Castellanos;Antonio-Javier Gallego;Jorge Calvo-Zaragoza,Unsupervised Domain Adaptation for Document Analysis of Music Score Images,2021,https://doi.org/10.5281/zenodo.5624455,Francisco J. Castellanos+University of Alicante>ESP>education;Antonio-Javier Gallego+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"<p>Document analysis is a key step within the typical Optical Music Recognition workflow. It processes an input image to obtain its layered version by extracting the different sources of information. Recently, this task has been formulated as a supervised learning problem, specifically by means of Convolutional Neural Networks due to their high performance and generalization capability. However, the requirement of training data for each new type of document still represents an important drawback. This issue can be palliated through Domain Adaptation (DA), which is the field that aims to adapt the knowledge learned with an annotated collection of data to other domains for which labels are not available. In this work, we combine a DA strategy based on adversarial training with Selectional Auto-Encoders to define an unsupervised framework for document analysis. Our experiments show a remarkable improvement for the layers that depict particular features at each domain, whereas layers that depict common features (such as staff lines) are barely affected by the adaptation process. In the best-case scenario, our method achieves an average relative improvement of around 44%, thereby representing a promising solution to unsupervised document analysis.</p>",ESP,education,Developed economies,"[21.213306, -9.320079]","[-20.1013, -26.151608]","[21.148651, 19.219154, -0.1755228]","[-18.758928, -18.662489, -3.7690568]","[10.74849, 6.4378986]","[6.4605856, -1.1344306]","[12.20807, 12.247774, -1.5342783]","[8.172228, 4.2744374, 9.858538]"
9,Rodrigo Castellon;Chris Donahue;Percy Liang,Codified audio language modeling learns useful representations for music information retrieval,2021,https://doi.org/10.5281/zenodo.5624605,Rodrigo Castellon+Stanford University>USA>education;Chris Donahue+Stanford University>USA>education;Percy Liang+Stanford University>USA>education,"We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically, we explore representations from Jukebox (Dhariwal et al. 2020): a music generation system containing a language model trained on codified audio from 1M songs. To determine if Jukebox's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, emotion recognition, and key detection. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging, suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR.",USA,education,Developed economies,"[-15.134326, 16.317902]","[-17.164953, -36.658234]","[-4.4761147, 14.881404, -11.008256]","[-12.867372, -1.9142649, -19.918177]","[13.260144, 8.473538]","[9.634059, 4.73594]","[13.529951, 14.520456, -1.385184]","[10.508425, 6.160613, 9.4769]"
48,Wei-Tsung Lu;Ju-Chiang Wang;Minz Won;Keunwoo Choi;Xuchen Song,SpecTNT: a Time-Frequency Transformer for Music Audio,2021,https://doi.org/10.5281/zenodo.5624503,"Wei-Tsung Lu+ByteDance>USA>company;Ju-Chiang Wang+ByteDance>USA>company;Minz Won+ByteDance>USA>company|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Keunwoo Choi+ByteDance>USA>company;Xuchen Song+ByteDance>USA>company","Transformers have drawn attention in the MIR field for their remarkable performance shown in natural language processing and computer vision. However, prior works in the audio processing domain mostly use Transformer as a temporal feature aggregator that acts similar to RNNs. In this paper, we propose SpecTNT, a Transformer-based architecture to model both spectral and temporal sequences of an input time-frequency representation. Specifically, we introduce a novel variant of the Transformer-in-Transformer (TNT) architecture. In each SpecTNT block, a spectral Transformer extracts frequency-related features into the frequency class token (FCT) for each frame. Later, the FCTs are linearly projected and added to the temporal embeddings (TEs), which aggregate useful information from the FCTs. Then, a temporal Transformer processes the TEs to exchange information across the time axis. By stacking the SpecTNT blocks, we build the SpecTNT model to learn the representation for music signals. In experiments, SpecTNT demonstrates state-of-the-art performance in music tagging and vocal melody extraction, and shows competitive performance for chord recognition. The effectiveness of SpecTNT and other design choices are further examined through ablation studies.",USA,company,Developed economies,"[40.65287, 3.0630407]","[-19.110403, -31.524733]","[9.82151, -7.0146165, 29.444796]","[-7.028538, -3.3072002, -10.11127]","[10.080283, 7.5921674]","[8.284954, 5.388657]","[12.551451, 11.315763, -0.10582045]","[9.294006, 6.479621, 9.25678]"
10,Chin-Jui Chang;Chun-Yi Lee;Yi-Hsuan Yang,Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding,2021,https://doi.org/10.5281/zenodo.5624625,Chin-Jui Chang+Academia Sinica>TWN>education|Research Center for IT Innovation>TWN>facility;Chun-Yi Lee+National Tsing Hua University>TWN>education;Yi-Hsuan Yang+Taiwan AI Labs>TWN>company,"This paper proposes a new self-attention based model for music score infilling, i.e., to generate a polyphonic music sequence that fills in the gap between given past and future contexts. While existing approaches can only fill in a short segment with a fixed number of notes, or a fixed time span between the past and future contexts, our model can infill a variable number of notes (up to 128) for different time spans. We achieve so with three major technical contributions. First, we adapt XLNet, an autoregressive model originally proposed for unsupervised model pre-training, to music score infilling. Second, we propose a new, musically specialized positional encoding called relative bar encoding that better informs the model of notes' position within the past and future context. Third, to capitalize relative bar encoding, we perform look-ahead onset prediction to predict the onset of a note one time step before predicting the other attributes of the note. We compare our proposed model with two strong baselines and show that our model is superior in both objective and subjective analyses.",TWN,education,Developing economies,"[21.70513, -11.745694]","[-9.292425, -33.623478]","[4.2244725, 8.378976, 8.305568]","[-17.24202, -3.8971531, -12.212146]","[11.134951, 6.7695513]","[8.750711, 5.6826186]","[12.544818, 12.432289, -1.0476544]","[9.681814, 5.952631, 9.263643]"
12,Vincent K.M. Cheung;Hsuan-Kai Kao;Li Su,Semi-supervised violin fingering generation using variational autoencoders,2021,https://doi.org/10.5281/zenodo.5624441,Vincent K.M. Cheung+Academia Sinica>TWN>education;Hsuan-Kai Kao+Academia Sinica>TWN>education;Li Su+Academia Sinica>TWN>education,"There are many ways to play the same note with the fingerboard hand on string instruments such as the violin. Musicians can flexibly adapt their string choice, hand position, and finger placement to maximise expressivity and playability when sounding each note. Violin fingerings therefore serve as important guides in ensuring effective performance, especially for inexperienced players. However, fingering annotations are often missing or only partially available on violin sheet music. Here, we propose a model based on the variational autoencoder that generates violin fingering patterns using only pitch and timing information found on the score. Our model leverages limited existing fingering data with the possibility to learn in a semi-supervised manner. Results indicate that fingering annotations generated by our model successfully imitate the style and preferences of a human performer. We further show its significantly improved performance with semi-supervised learning, and demonstrate our model's ability to match the state-of-the-art in violin fingering pattern generation when trained on only half the amount of labelled data.",TWN,education,Developing economies,"[26.20006, 2.7684367]","[-40.335506, -9.255693]","[8.499389, 5.617442, 23.408802]","[-17.161861, -10.649897, -7.266893]","[9.969635, 8.213013]","[7.6110916, 4.567089]","[12.979943, 11.866969, 0.40364778]","[9.127094, 6.603229, 9.589272]"
13,Keunwoo Choi;Yuxuan Wang,"Listen, Read, and Identify: Multimodal Singing Language Identification of Music",2021,https://doi.org/10.5281/zenodo.5624555,Keunwoo Choi+ByteDance>CHN>company|ByteDance>Unknown>Unknown;Yuxuan Wang+ByteDance>CHN>company|ByteDance>Unknown>Unknown,"We propose a multimodal singing language classification model that uses both audio content and textual metadata. LRID-Net, the proposed model, takes an audio signal and a language probability vector estimated from the metadata and outputs the probabilities of the target languages. Optionally, LRID-Net is facilitated with modality dropouts to handle a missing modality. In the experiment, we trained several LRID-Nets with varying modality dropout configuration and tested them with various combinations of input modalities. The experiment results demonstrate that using multimodal input improves performance. The results also suggest that adopting modality dropout does not degrade the performance of the model when there are full modality inputs while enabling the model to handle missing modality cases to some extent.",CHN,company,Developing economies,"[-9.727297, -31.936403]","[-27.623463, -42.567604]","[14.391574, 13.249182, -15.292197]","[6.7456403, -5.302968, -22.34917]","[10.322806, 11.1618185]","[7.976244, 4.942878]","[11.454549, 15.280298, 0.58793706]","[10.390122, 7.322881, 9.031057]"
14,Shreyan Chowdhury;Gerhard Widmer,On Perceived Emotion in Expressive Piano Performance: Further Experimental Evidence for the Relevance of Mid-level Perceptual Features,2021,https://doi.org/10.5281/zenodo.5624499,"Shreyan Chowdhury+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Despite recent advances in audio content-based music emotion recognition, a question that remains to be explored is whether an algorithm can reliably discern emotional or expressive qualities between different performances of the same piece. In the present work, we analyze several sets of features on their effectiveness in predicting arousal and valence of six different performances (by six famous pianists) of Bach's Well-Tempered Clavier Book 1. These features include low-level acoustic features, score-based features, features extracted using a pre-trained emotion model, and Mid-level perceptual features. We compare their predictive power by evaluating them on several experiments designed to test performance-wise or piece-wise variations of emotion. We find that Mid-level features show significant contribution in performance-wise variation of both arousal and valence - even better than the pre-trained emotion model. Our findings add to the evidence of Mid-level perceptual features being an important representation of musical attributes for several tasks - specifically, in this case, for capturing the expressive aspects of music that manifest as perceived emotion of a musical performance.",AUT,education,Developed economies,"[-63.045273, 3.2651381]","[51.01329, -11.609086]","[-26.877441, 26.37609, 6.8564296]","[7.6776185, 22.016087, 4.6600094]","[13.905243, 12.757175]","[12.918536, 4.317368]","[16.115498, 14.423215, 1.7745215]","[14.027996, 4.860193, 10.180551]"
15,Bas Cornelissen;Willem Zuidema;John Ashley Burgoyne,Cosine Contours: a Multipurpose Representation for Melodies,2021,https://doi.org/10.5281/zenodo.5624531,"Bas Cornelissen+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education;Willem Zuidema+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education;John Ashley Burgoyne+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education","Melodic contour is central to our ability to perceive and produce music. We propose to represent melodic contours as a combination of cosine functions, using the discrete cosine transform. The motivation for this approach is twofold: (1) it approximates a maximally informative contour representation (capturing most of the variation in as few dimensions as possible), but (2) it is nevertheless independent of the specifics of the data sets for which it is used. We consider the relation with principal component analysis, which only meets the first of these requirements. Theoretically, the principal components of a repertoire of random walks are known to be cosines. We find, empirically, that the principal components of melodies also closely approximate cosines in multiple musical traditions. We demonstrate the usefulness of the proposed representation by analyzing contours at three levels (complete songs, melodic phrases and melodic motifs) across multiple traditions in three small case studies.",NLD,education,Developed economies,"[6.615853, -5.126047]","[4.797082, -7.468898]","[7.653336, 5.5319843, -2.838303]","[6.3900948, -7.632097, 0.022273459]","[10.663104, 9.998022]","[8.085471, 1.6011528]","[11.488596, 15.2496395, -0.8695591]","[9.787671, 7.039375, 12.487267]"
16,Shuqi Dai;Zeyu Jin;Celso Gomes;Roger Dannenberg,Controllable deep melody generation via hierarchical music structure representation,2021,https://doi.org/10.5281/zenodo.5625667,Shuqi Dai+Carnegie Mellon University>USA>education;Zeyu Jin+Adobe Inc.>USA>company;Celso Gomes+Adobe Inc.>USA>company;Roger B. Dannenberg+Carnegie Mellon University>USA>education,"Recent advances in deep learning have expanded possibilities to generate music, but generating a customizable full piece of music with consistent long-term structure remains a challenge. This paper introduces MusicFrameworks, a hierarchical music structure representation and a multi-step generative process to create a full-length melody guided by long-term repetitive structure, chord, melodic contour, and rhythm constraints. We first organize the full melody with section and phrase-level structure. To generate melody in each phrase, we generate rhythm and basic melody using two separate transformer-based networks, and then generate the melody conditioned on the basic melody, rhythm and chords in an auto-regressive manner. By factoring music generation into sub-problems, our approach allows simpler models and requires less data. To customize or add variety, one can alter chords, basic melody, and rhythm structure in the music frameworks, letting our networks generate the melody accordingly. Additionally, we introduce new features to encode musical positional information, rhythm patterns, and melodic contours based on musical domain knowledge. A listening test reveals that melodies generated by our method are rated as good as or better than human-composed music in the POP909 dataset about half the time.",USA,education,Developed economies,"[21.262724, 5.7378516]","[-8.272013, -39.028214]","[3.4319372, 3.0419164, 21.621195]","[-23.365446, 2.7148192, -10.141388]","[10.281095, 8.649803]","[9.2359, 6.208209]","[13.245645, 11.783281, 0.24465282]","[9.64563, 5.459367, 8.95692]"
17,Emir Demirel;Sven Ahlbäck;Simon Dixon,MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription,2021,https://doi.org/10.5281/zenodo.5624643,Emir Demirel+Queen Mary University of London>GBR>education;Sven Ahlbäck+Doremir Music Research AB>Unknown>company;Simon Dixon+Queen Mary University of London>GBR>education,"This paper makes several contributions to automatic lyrics transcription (ALT) research. Our main contribution is a novel variant of the Multistreaming Time-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which processes the temporal information using multiple streams in parallel with varying resolutions keeping the network more compact, and thus with a faster inference and an improved recognition rate than having identical TDNN streams. In addition, two novel preprocessing steps prior to training the acoustic model are proposed. First, we suggest using recordings from both monophonic and polyphonic domains during training the acoustic model. Second, we tag monophonic and polyphonic recordings with distinct labels for discriminating non-vocal silence and music instances during alignment. Moreover, we present a new test set with a considerably larger size and a higher musical variability compared to the existing datasets used in ALT literature, while maintaining the gender balance of the singers. Our best performing model sets the state-of-the-art in lyrics transcription by a large margin. For reproducibility, we publicly share the identifiers to retrieve the data used in this paper.",GBR,education,Developed economies,"[-25.50061, -34.09231]","[-27.586182, -39.565205]","[14.278538, 20.288145, -2.8696299]","[1.3094124, -4.5918794, -23.368078]","[11.14181, 11.764954]","[8.081667, 4.909128]","[12.178625, 15.76452, 1.1726675]","[10.2960005, 7.112557, 8.991191]"
18,Hao-Wen Dong;Chris Donahue;Taylor Berg-Kirkpatrick;Julian Mcauley,Towards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music,2021,https://doi.org/10.5281/zenodo.5624447,Hao-Wen Dong+University of California San Diego>USA>education;Chris Donahue+Stanford University>USA>education;Taylor Berg-Kirkpatrick+University of California San Diego>USA>education;Julian McAuley+University of California San Diego>USA>education,"Modern keyboards allow a musician to play multiple instruments at the same time by assigning zones—fixed pitch ranges of the keyboard—to different instruments. In this paper, we aim to further extend this idea and examine the feasibility of automatic instrumentation—dynamically assigning instruments to notes in solo music during performance. In addition to the online, real-time-capable setting for performative use cases, automatic instrumentation can also find applications in assistive composing tools in an offline setting. Due to the lack of paired data of original solo music and their full arrangements, we approach automatic instrumentation by learning to separate parts (e.g., voices, instruments and tracks) from their mixture in symbolic multitrack music, assuming that the mixture is to be played on a keyboard. We frame the task of part separation as a sequential multi-class classification problem and adopt machine learning to map sequences of notes into sequences of part labels. To examine the effectiveness of our proposed models, we conduct a comprehensive empirical evaluation over four diverse datasets of different genres and ensembles—Bach chorales, string quartets, game music and pop music. Our experiments show that the proposed models outperform various baselines. We also demonstrate the potential for our proposed models to produce alternative convincing instrumentations for an existing arrangement by separating its mixture into parts. All source code and audio samples can be found at https://salu133445.github.io/arranger/ .",USA,education,Developed economies,"[16.938248, 13.859106]","[-42.020416, -6.6608524]","[0.09316627, -7.109238, 17.49615]","[-15.605586, -5.402514, -3.5553782]","[11.824918, 7.0188193]","[7.368872, 4.288413]","[13.491087, 12.341921, -0.96109563]","[9.240742, 6.355178, 10.1425085]"
19,Sachinda Edirisooriya;Hao-Wen Dong;Julian Mcauley;Taylor Berg-Kirkpatrick,An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition,2021,https://doi.org/10.5281/zenodo.5625669,Sachinda Edirisooriya+University of California San Diego>USA>education;Hao-Wen Dong+University of California San Diego>USA>education;Julian McAuley+University of California San Diego>USA>education;Taylor Berg-Kirkpatrick+University of California San Diego>USA>education,"Previous work has shown that neural architectures are able to perform optical music recognition (OMR) on monophonic and homophonic music with high accuracy. However, piano and orchestral scores frequently exhibit polyphonic passages, which add a second dimension to the task. Monophonic and homophonic music can be described as homorhythmic, or having a single musical rhythm. Polyphonic music, on the other hand, can be seen as having multiple rhythmic sequences, or voices, concurrently. We first introduce a workflow for creating large-scale polyphonic datasets suitable for end-to-end recognition from sheet music publicly available on the MuseScore forum. We then propose two novel formulations for end-to-end polyphonic OMR---one treating the problem as a type of multi-task binary classification, and the other treating it as multi-sequence detection. Building upon the encoder-decoder architecture and an image encoder proposed in past work on end-to-end OMR, we propose two novel decoder models---FlagDecoder and RNNDecoder---that correspond to the two formulations. Finally, we compare the empirical performance of these end-to-end approaches to polyphonic OMR and observe a new state-of-the-art performance with our multi-sequence detection decoder, RNNDecoder.",USA,education,Developed economies,"[38.2149, 20.010572]","[-22.99923, -23.946909]","[17.833605, 10.329552, 10.871673]","[-14.630334, -16.162403, -5.9550633]","[8.701981, 6.2663956]","[6.4777613, -1.1167527]","[10.670322, 11.203838, -0.107693404]","[8.276183, 4.4891443, 9.737909]"
20,Anders Elowsson;Olivier Lartillot,A Hardanger Fiddle Dataset with Performances Spanning Emotional Expressions and Annotations Aligned using Image Registration,2021,https://doi.org/10.5281/zenodo.5624587,"Anders Elowsson+RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, University of Oslo>NOR>education;Olivier Lartillot+RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, University of Oslo>NOR>education","This paper presents a Hardanger fiddle dataset ""HF1"" with polyphonic performances spanning five different emotional expressions: normal, angry, sad, happy, and tender. The performances thus cover the four quadrants of the activity/valence-space. The onsets and offsets, together with an associated pitch, were human-annotated for each note in each performance by the fiddle players themselves. First, they annotated the normal version. These annotations were then transferred to the expressive performances using music alignment and finally human-verified. Two separate music alignment methods based on image registration were developed for this purpose; a B-spline implementation that produces a continuous temporal transformation curve and a Demons algorithm that produces displacement matrices for time and pitch that also account for local timing variations across the pitch range. Both methods start from an ""Onsetgram"" of onset salience across pitch and time and perform the alignment task accurately. Various settings of the Demons algorithm were further evaluated in an ablation study. The final dataset is around 43 minutes long and consists of 19 734 notes of Hardanger fiddle music, recorded in stereo. The dataset and source code are available online. The dataset will be used in MIR research for tasks involving polyphonic transcription, score alignment, beat tracking, downbeat tracking, tempo estimation, and classification of emotional expressions.",NOR,education,Developed economies,"[-62.41665, 0.7647389]","[-19.233027, -12.3073015]","[-27.07183, 20.615198, -0.91333264]","[-2.7647326, -17.111553, -1.0503995]","[13.920725, 12.745008]","[6.424884, 0.88874793]","[16.114393, 14.393264, 1.7978287]","[8.477117, 5.820075, 10.864734]"
21,Jeffrey Ens;Philippe Pasquier,Building the MetaMIDI Dataset: Linking Symbolic and Audio Musical Data,2021,https://doi.org/10.5281/zenodo.5624567,Jeff Ens+Simon Fraser University>CAN>education;Philippe Pasquier+Simon Fraser University>CAN>education,"We introduce the MetaMIDI Dataset (MMD), a large scale collection of 436,631 MIDI files and metadata. MMD, contains artist and title metadata for 221,504 MIDI files, and genre metadata for 143,868 MIDI files, collected during the web-scraping process. MIDI files in MMD, were matched against a collection of 32,000,000 30-second audio clips retrieved from Spotify, resulting in over 10,796,557 audio-MIDI matches. In addition, we linked 600,142 Spotify tracks with 1,094,901 MusicBrainz recordings to produce a set of 168,032 MIDI files that are matched to the MusicBrainz database. We also provide a set of 53,496 MIDI files using audio-MIDI matches where the derived metadata on Spotify is a fuzzy match to the web-scraped metadata. These links augment many files in the dataset with the extensive metadata available via the Spotify API and the MusicBrainz database. We anticipate that this collection of data will be of great use to MIR researchers addressing a variety of research topics.",CAN,education,Developed economies,"[17.036304, 16.052017]","[17.35714, 32.53973]","[-3.371226, -9.205705, 18.425964]","[-3.0838544, 4.2701597, 15.417931]","[12.073423, 7.1900024]","[11.250596, 0.4008752]","[13.62273, 12.529689, -1.1384462]","[11.810273, 4.96104, 11.699854]"
11,Yi-Wei Chen;Hung-Shin Lee;Yen-Hsing Chen;Hsin-Min Wang,SurpriseNet: Melody Harmonization Conditioning on User-controlled Surprise Contours,2021,https://doi.org/10.5281/zenodo.5624423,"Yi-Wei Chen+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Hung-Shin Lee+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Yen-Hsing Chen+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Hsin-Min Wang+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown","The surprisingness of a song is an essential and seemingly subjective factor in determining whether the listener likes it. With the help of information theory, it can be described as the transition probability of a music sequence modeled as a Markov chain. In this study, we introduce the concept of deriving entropy variations over time, so that the surprise contour of each chord sequence can be extracted. Based on this, we propose a user-controllable framework that uses a conditional variational autoencoder (CVAE) to harmonize the melody based on the given chord surprise indication. Through explicit conditions, the model can randomly generate various and harmonic chord progressions for a melody, and the Spearman's correlation and p-value significance show that the resulting chord progressions match the given surprise contour quite well. The vanilla CVAE model was evaluated in a basic melody harmonization task (no surprise control) in terms of six objective metrics. The results of experiments on the Hooktheory Lead Sheet Dataset show that our model achieves performance comparable to the state-of-the-art melody harmonization model.",TWN,education,Developing economies,"[10.931619, -7.176334]","[-25.436281, 25.721123]","[13.747552, 11.162956, 0.51650465]","[-22.197866, 1.977668, -0.16040124]","[10.338328, 9.636138]","[6.959707, 3.4099147]","[11.496975, 14.914974, -0.7382199]","[9.574318, 8.243694, 12.093945]"
101,Mickael Zehren;Marco Alunno;Paolo Bientinesi,ADTOF: A large dataset of non-synthetic music for automatic drum transcription,2021,https://doi.org/10.5281/zenodo.5624527,Mickaël Zehren+Umeå Universitet>SWE>education;Marco Alunno+Universidad EAFIT Medellín>COL>education;Paolo Bientinesi+Umeå Universitet>SWE>education,"The state-of-the-art methods for drum transcription in the presence of melodic instruments (DTM) are machine learning models trained in a supervised manner, which means that they rely on labeled datasets. The problem is that the available public datasets are limited either in size or in realism, and are thus suboptimal for training purposes. Indeed, the best results are currently obtained via a rather convoluted multi-step training process that involves both real and synthetic datasets. To address this issue, starting from the observation that the communities of rhythm games players provide a large amount of annotated data, we curated a new dataset of crowdsourced drum transcriptions. This dataset contains real-world music, is manually annotated, and is about two orders of magnitude larger than any other non-synthetic dataset, making it a prime candidate for training purposes. However, due to crowdsourcing, the initial annotations contain mistakes. We discuss how the quality of the dataset can be improved by automatically correcting different types of mistakes. When used to train a popular DTM model, the dataset yields a performance that matches that of the state-of-the-art for DTM, thus demonstrating the quality of the annotations.",SWE,education,Developed economies,"[28.58436, -43.610405]","[-37.79994, -13.868135]","[20.911034, -18.380869, 6.4252515]","[3.2372415, 12.055335, -18.894115]","[7.688126, 7.239778]","[8.590592, 4.459915]","[10.39425, 11.552598, 0.9771605]","[9.3104315, 7.0727763, 9.687305]"
49,Néstor Nápoles López;Mark R H Gotham;Ichiro Fujinaga,AugmentedNet: A Roman Numeral Analysis Network with Synthetic Training Examples and Additional Tonal Tasks,2021,https://doi.org/10.5281/zenodo.5624533,Néstor Nápoles López+McGill University>CAN>education|CIRMMT>CAN>facility;Mark Gotham+Universität des Saarlandes>DEU>education;Ichiro Fujinaga+McGill University>CAN>education|CIRMMT>CAN>facility,"AugmentedNet is a new convolutional recurrent neural network for predicting Roman numeral labels. The network architecture is characterized by a separate convolutional block for bass and chromagram inputs. This layout is further enhanced by using synthetic training examples for data augmentation, and a greater number of tonal tasks to solve simultaneously via multitask learning. This paper reports the improved performance achieved by combining these ideas. The additional tonal tasks strengthen the shared representation learned through multitask learning. The synthetic examples, in turn, complement key transposition, which is often the only technique used for data augmentation in similar problems related to tonal music. The name ""AugmentedNet"" speaks to the increased number of both training examples and tonal tasks. We report on tests across six relevant and publicly available datasets: ABC, BPS, HaydnSun, TAVERN, When-in-Rome, and WTC. In our tests, our model outperforms recent methods of functional harmony, such as other convolutional neural networks and Transformer-based models. Finally, we show a new method for reconstructing the full Roman numeral label, based on common Roman numeral classes, which leads to better results compared to previous methods.",CAN,education,Developed economies,"[24.024101, 17.386333]","[-25.163765, -34.30867]","[-3.6258414, -16.705584, 20.077984]","[-2.3423014, -4.8076735, -17.296862]","[11.412399, 6.7486167]","[8.470559, 5.4082465]","[13.277334, 11.99104, -1.2692024]","[9.638856, 6.649745, 9.087142]"
51,"Lizé Masclef, Ninon;Andrea Vaglio;Manuel Moussallam",User-centered evaluation of lyrics-to-audio alignment,2021,https://doi.org/10.5281/zenodo.5625688,"Ninon Lizé Masclef+Deezer Research>FRA>company|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Andrea Vaglio+Deezer Research>FRA>company|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Manuel Moussallam+Deezer Research>FRA>company|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education","<p>The growing interest for Human-centered MIR motivates the development of perceptually-grounded evaluation metrics. Despite remarkable progress of lyrics-to-audio alignment systems in recent years, one thing remaining unresolved is whether the metrics employed to assess their performance are perceptually grounded. Even if a tolerance window for errors was fixed at 0.3s for the MIREX challenge, no experiment was conducted to confer psychological validity to this threshold. Following an interdisciplinary approach, fueled by psychology and musicology insights, we consider the lyrics-to-audio alignment evaluation from a user-centered perspective. In this paper, we call into question the perceptual robustness of the most used metric to evaluate this task. We investigate the perception of audio and lyrics synchrony through two realistic experimental settings inspired from karaoke, and discuss implications for evaluation metrics. The most striking features of these results are the asymmetrical perceptual thresholds of synchrony perception between lyrics and audio, as well as the influence of rhythmic factors on them.</p>",FRA,company,Developed economies,"[-28.122234, -33.873383]","[34.11648, -17.198229]","[9.617222, 20.158468, -0.8169209]","[3.364213, 14.2082, 5.450765]","[11.249185, 11.794081]","[9.933371, 2.9921763]","[12.298094, 15.836221, 1.1332966]","[9.611869, 6.0596366, 10.80559]"
78,Pavan M Seshadri;Alexander Lerch,Improving Music Performance Assessment With Contrastive Learning,2021,https://doi.org/10.5281/zenodo.5624481,Pavan Seshadri+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education,"Several automatic approaches for objective music performance assessment (MPA) have been proposed in the past, however, existing systems are not yet capable of reliably predicting ratings with the same accuracy as professional judges. This study investigates contrastive learning as a potential method to improve existing MPA systems. Contrastive learning is a widely used technique in representation learning to learn a structured latent space capable of separately clustering multiple classes. It has been shown to produce state of the art results for image-based classification problems. We introduce a weighted contrastive loss suitable for regression tasks applied to a convolutional neural network and show that contrastive loss results in performance gains in regression tasks for MPA. Our results show that contrastive-based methods are able to match and exceed SoTA performance for MPA regression tasks by creating better class clusters within the latent space of the neural networks.",USA,education,Developed economies,"[-11.127091, -5.2005405]","[-31.424786, -35.351864]","[6.0149026, 16.499348, 13.982789]","[-6.500285, -10.322402, -22.163744]","[11.440008, 8.571996]","[7.832736, 5.635577]","[13.636964, 13.043116, -0.49191374]","[9.992429, 6.9238257, 8.709157]"
79,Dougal Shakespeare;Camille Roth,Tracing Affordance and Item Adoption on Music Streaming Platforms,2021,https://doi.org/10.5281/zenodo.5625698,Dougal Shakespeare+Centre March Bloch>DEU>facility;Camille Roth+CNRS>FRA>facility,"Popular music streaming platforms offer users a diverse network of content exploration through a triad of affordances: organic, algorithmic and editorial access modes. Whilst offering great potential for discovery, such platform developments also pose the modern user with daily adoption decisions on two fronts: platform affordance adoption and the adoption of recommendations therein. Following a carefully constrained set of Deezer users over a 2-year observation period, our work explores factors driving user behaviour in the broad sense, by differentiating users on the basis of their temporal daily usage, adoption of the main platform affordances, and the ways in which they react to them, especially in terms of recommendation adoption. Diverging from a perspective common in studies on the effects of recommendation, we assume and confirm that users exhibit very diverse behaviours in using and adopting the platform affordances. The resulting complex and quite heteregeneous picture demonstrates that there is no blanket answer for adoption practices of both recommendation features and recommendations.",DEU,facility,Developed economies,"[-32.91795, 25.638636]","[40.92394, 29.880424]","[-18.944227, 17.401604, -16.301014]","[10.960362, 12.799575, 16.96898]","[14.970847, 8.491688]","[12.931909, 1.2665884]","[14.973518, 15.045265, -1.7191166]","[13.472857, 4.3731794, 12.19508]"
80,Zhengshan Shi,Computational analysis and modeling of expressive timing in Chopin's Mazurkas,2021,https://doi.org/10.5281/zenodo.5624515,"Zhengshan Shi+Center for Computer Research in Music and Acoustics, Stanford University>USA>education","Performers' distortion of notated rhythms in a musical score is a significant factor in the production of convincingly expressive music interpretations. Sometimes exaggerated, and sometimes subtle, these distortions are driven by a variety of factors, including schematic features (both structural such as phrase boundaries and surface events such as recurrent rhythmic patterns), as well as relatively rare veridical events that characterize the individuality and uniqueness of a particular piece.   Performers tend to adopt similar pervasive approaches to interpreting schemas, resulting in common performance practices, while often formulating less common approaches to the interpretation of veridical events. Furthermore, some performers choose anomalous interpretations of schemas. We present a machine learning model of expressive performance of Chopin Mazurkas and a critical analysis of the output based upon statistical analyses of the musical scores and of recorded performances. We compare the timings of recorded human performances of selected Mazurkas by Frédéric Chopin with performances of the same works generated by a neural network trained with recorded human performances of the entire corpus. This paper demonstrates that while machine learning succeeds, to some degree, in expressive interpretation of schemata, convincingly capturing performance characteristics remains very much a work in progress.",USA,education,Developed economies,"[44.979275, -24.35361]","[-31.804588, 6.892876]","[-4.4892874, -32.443905, 3.9888859]","[-11.042503, 2.8740706, -0.21784899]","[11.348889, 4.9400916]","[7.7532763, 3.3861532]","[11.214813, 13.103407, -2.497992]","[9.139058, 6.287927, 10.890646]"
81,Nithya Nadig Shikarpur;Asawari Keskar;Preeti Rao,Computational analysis of melodic mode switching in raga performance,2021,https://doi.org/10.5281/zenodo.5625702,Nithya Shikarpur+Indian Institute of Technology Bombay>IND>education;Asawari Keskar+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education,"Melodic mode shifting is a construct used occasionally by skilled artists in a raga performance to enhance it by bringing in temporarily shades of a different raga. In this work, we study a specific North Indian Khyal concert structure known as the Jasrangi jugalbandi where a male and female singer co-perform different ragas in an interactive fashion. The mode-shifted ragas with their relatively displaced assumed tonics comprise the identical set of scale intervals and therefore can be easily confused when performed together. With an annotated dataset based on available concerts by well-known artists, we present an analysis of the performance in terms of the raga characteristics as they are manifested through the interactive engagement. We analyse both the aspects of modal music forms, viz. the pitch distribution, representing tonal hierarchy, and the melodic phrases, across the sequence of singing turns by the two artists with reference to representative individual performances of the corresponding ragas.",IND,education,Developing economies,"[3.9250085, -3.394827]","[3.6050591, -18.907986]","[7.843131, 4.3848453, -21.403702]","[15.1427145, -13.756595, -4.9829283]","[11.136974, 10.572399]","[7.445528, 1.179752]","[11.499841, 15.133867, -1.6277213]","[8.906012, 7.0981317, 12.6205]"
82,Qingwei Song;Qiwei Sun;Dongsheng Guo;Haiyong Zheng,SinTra: Learning an inspiration model from a single multi-track music segment,2021,https://doi.org/10.5281/zenodo.5624355,Qingwei Song+Ocean University of China>CHN>education|Xi’an Jiaotong University>CHN>education;Qiwei Sun+Xi’an Jiaotong University>CHN>education;Dongsheng Guo+Ocean University of China>CHN>education;Haiyong Zheng+Ocean University of China>CHN>education,"In this paper, we propose SinTra, an auto-regressive sequential generative model that can learn from a single multi-track music segment, to generate coherent, aesthetic, and variable polyphonic music of multi-instruments with an arbitrary length of bar. For this task, to ensure the relevance of generated samples and training music, we present a novel pitch-group representation. SinTra, consisting of a pyramid of Transformer-XL with a multi-scale training strategy, can learn both the musical structure and the relative positional relationship between notes of the single training music segment. Additionally, for maintaining the inter-track correlation, we use the convolution operation to process multi-track music, and when decoding, the tracks are independent to each other to prevent interference. We evaluate SinTra with both subjective study and objective metrics. The comparison results show that our framework can learn information from a single music segment more sufficiently than Music Transformer. Also the comparison between SinTra and its variant, i.e., the single-stage SinTra with the first stage only, shows that the pyramid structure can effectively suppress overly-fragmented notes.",CHN,education,Developing economies,"[-16.726883, -5.233448]","[-27.938818, -28.998066]","[-1.5916233, 12.4178705, 9.150321]","[-10.833366, -7.1502395, -16.016287]","[11.766416, 9.010522]","[7.956333, 5.202404]","[13.723329, 13.033574, 0.2600765]","[9.403364, 6.9117317, 9.026902]"
83,Janne Spijkervet;John Ashley Burgoyne,Contrastive Learning of Musical Representations,2021,https://doi.org/10.5281/zenodo.5624573,"Janne Spijkervet+Institute for Logic, Language, and Computation, University of Amsterdam>NLD>education;John Ashley Burgoyne+Institute for Logic, Language, and Computation, University of Amsterdam>NLD>education","While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1% despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper.",NLD,education,Developed economies,"[-12.268013, -5.48729]","[-21.2756, -36.04282]","[4.8801756, 13.601262, 14.13884]","[-8.076152, -0.6202954, -18.075125]","[11.262142, 8.718459]","[9.454858, 4.8868775]","[13.519463, 12.925325, 0.070093505]","[10.621404, 6.481838, 8.992117]"
84,Xiaoheng Sun;Qiqi He;Gao Yongwei;Wei Li,Musical Tempo Estimation Using a Multi-scale Network,2021,https://doi.org/10.5281/zenodo.5624391,Xiaoheng Sun+Fudan University>CHN>education;Qiqi He+Fudan University>CHN>education;Yongwei Gao+Fudan University>CHN>education;Wei Li+Fudan University>CHN>education,"Recently, some single-step systems without onset detection have shown their effectiveness in automatic musical tempo estimation. Following the success of these systems, in this paper we propose a Multi-scale Grouped Attention Network to further explore the potential of such methods. A multi-scale structure is introduced as the overall network architecture where information from different scales is aggregated to strengthen contextual feature learning. Furthermore, we propose a Grouped Attention Module as the key component of the network. The proposed module separates the input feature into several groups along the frequency axis, which makes it capable of capturing long-range dependencies from different frequency positions on the spectrogram. In comparison experiments, the results on public datasets show that the proposed model outperforms existing state-of-the-art methods on Accuracy1.",CHN,education,Developing economies,"[40.031773, -25.078136]","[-31.559635, -9.528324]","[0.91256654, -27.788958, 0.8019184]","[-5.3647857, 10.543894, -14.906855]","[11.467914, 4.406049]","[5.0406723, 2.1785965]","[10.841235, 13.340245, -2.8474061]","[7.4626956, 7.048125, 10.614876]"
85,Pau Torras;Arnau Baró;Lei Kang;Alicia Fornés,On the Integration of Language Models into Sequence to Sequence Architectures for Handwritten Music Recognition,2021,https://doi.org/10.5281/zenodo.5624451,Pau Torras+Universitat Autònoma de Barcelona>ESP>education;Arnau Baró+Universitat Autònoma de Barcelona>ESP>education;Lei Kang+Universitat Autònoma de Barcelona>ESP>education;Alicia Fornés+Universitat Autònoma de Barcelona>ESP>education,"Despite the latest advances in Deep Learning, the recognition of handwritten music scores is still a challenging endeavour. Even though the recent Sequence to Sequence (Seq2Seq) architectures have demonstrated its capacity to reliably recognise handwritten text, their performance is still far from satisfactory when applied to historical handwritten scores. Indeed, the ambiguous nature of handwriting, the non-standard musical notation employed by composers of the time and the decaying state of old paper make these scores remarkably difficult to read, sometimes even by trained humans. Thus, in this work we explore the incorporation of language models into a Seq2Seq-based architecture to try to improve transcriptions where the aforementioned unclear writing produces statistically unsound mistakes, which as far as we know, has never been attempted for this field of research on this architecture. After studying various Language Model integration techniques, the experimental evaluation on historical handwritten music scores shows a significant improvement over the state of the art, showing that this is a promising research direction for dealing with such difficult manuscripts.",ESP,education,Developed economies,"[24.465027, -3.4810965]","[-18.878498, -22.565968]","[10.352172, 1.5799841, 14.419492]","[-13.48095, -19.739784, -6.248903]","[9.78408, 7.8829985]","[6.648271, -1.1272775]","[12.3754015, 12.139855, 0.11045587]","[8.970767, 6.1287885, 9.187524]"
86,Kosetsu Tsukuda;Keisuke Ishida;Masahiro Hamasaki;Masataka Goto,Kiite Cafe: A Web Service for Getting Together Virtually to Listen to Music,2021,https://doi.org/10.5281/zenodo.5624491,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Keisuke Ishida+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masahiro Hamasaki+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"In light of the COVID-19 pandemic making it difficult for people to get together in person, this paper describes a public web service called Kiite Cafe that lets users get together virtually to listen to music. When users listen to music on Kiite Cafe, their experiences are characterized by two architectures: (i) visualization of each user's reactions, and (ii) selection of songs from users' favorite songs. These architectures enable users to feel social connection with others and the joy of introducing others to their favorite songs as if they were together in person to listen to music. In addition, the architectures provide three user experiences: (1) motivation to react to played songs, (2) the opportunity to listen to a diverse range of songs, and (3) the opportunity to contribute as curators. By analyzing the behavior logs of 1,760 Kiite Cafe users over about five months, we quantitatively show that these user experiences can generate various effects (e.g., users react to a more diverse range of songs on Kiite Cafe than when listening alone). We also discuss how our proposed architectures can continue to enrich music listening experiences with others even after the pandemic's resolution.",JPN,facility,Developed economies,"[-31.866734, 29.227856]","[40.52173, 28.894428]","[-22.277138, 13.239644, -15.014814]","[7.6937943, 14.534956, 17.506166]","[14.781156, 7.667779]","[13.031735, 1.1554501]","[14.952079, 14.3843, -2.0844464]","[13.477512, 4.3349223, 11.9702635]"
87,Kosetsu Tsukuda;Masahiro Hamasaki;Masataka Goto,Toward an Understanding of Lyrics-viewing Behavior While Listening to Music on a Smartphone,2021,https://doi.org/10.5281/zenodo.5624633,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masahiro Hamasaki+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Why and how do people view lyrics? Although various lyrics-based systems have been proposed in MIR community, this fundamental question remains unexplored. Better understanding of lyrics viewing behavior would be beneficial for both researchers and music streaming platforms to improve their lyrics-based systems. Therefore, in this paper, we investigate why and how people view lyrics, especially when they listen to music on a smartphone. To answer ""why,"" we conduct a questionnaire-based online user survey involving 206 participants. To answer ""how,"" we analyze over 23 million lyrics request logs sent from the smartphone application of a music streaming service. Our analysis results suggest several reusable insights, including the following: (1) People have high demand for viewing lyrics to confirm what the artist sings, more deeply understand the lyrics, sing the song, and figure out the structure such as verse and chorus. (2) People like to view lyrics after returning home at night and before going to sleep rather than during the daytime. (3) People usually view the same lyrics repeatedly over time. Applying these insights, we also discuss application examples that could enable people to more actively view lyrics and listen to new songs, which would not only diversify and enrich people's music listening experiences but also be beneficial especially for music streaming platforms.",JPN,facility,Developed economies,"[-32.214283, -26.902132]","[40.3441, 27.310867]","[7.685337, 19.913342, 2.815536]","[6.392114, 13.230604, 15.355874]","[11.356508, 11.788187]","[12.766549, 1.2583671]","[12.468918, 15.797534, 1.141854]","[13.354024, 4.4578166, 12.064173]"
77,Simon J Schwär;Sebastian Rosenzweig;Meinard Müller,A Differentiable Cost Measure for Intonation Processing in Polyphonic Music,2021,https://doi.org/10.5281/zenodo.5624601,Simon Schwär+International Audio Laboratories Erlangen>DEU>facility;Sebastian Rosenzweig+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Intonation is the process of choosing an appropriate pitch for a given note in a musical performance. Particularly in polyphonic singing, where all musicians can continuously adapt their pitch, this leads to complex interactions. To achieve an overall balanced sound, the musicians dynamically adjust their intonation considering musical, perceptual, and acoustical aspects. When adapting the intonation in a recorded performance, a sound engineer may have to individually fine-tune the pitches of all voices to account for these aspects in a similar way. In this paper, we formulate intonation adaptation as a cost minimization problem. As our main contribution, we introduce a differentiable cost measure by adapting and combining existing principles for measuring intonation. In particular, our measure consists of two terms, representing a tonal aspect (the proximity to a tonal grid) and a harmonic aspect (the perceptual dissonance between salient frequencies). We show that, combining these two aspects, our measure can be used to flexibly account for different artistic intents while allowing for robust and joint processing of multiple voices in real-time. In an experiment, we demonstrate the potential of our approach for the task of intonation adaptation of amateur choral music using recordings from a publicly available multitrack dataset.",DEU,facility,Developed economies,"[-0.28429592, -27.566872]","[-4.060282, -16.608585]","[14.31361, 2.1911573, -13.460644]","[7.3043537, -17.61702, -10.163889]","[10.484522, 10.341336]","[6.884591, 2.0903554]","[11.389562, 15.138119, -0.11120769]","[8.86032, 7.569641, 11.47837]"
88,Andrea Vaglio;Romain Hennequin;Manuel Moussallam;Gael Richard,The Words Remain the Same: Cover Detection with Lyrics Transcription,2021,https://doi.org/10.5281/zenodo.5624395,"Andrea Vaglio+Deezer R&D>FRA>company|Télécom Paris, Institut Polytechnique de Paris>FRA>education;Romain Hennequin+Deezer R&D>FRA>company|Télécom Paris, Institut Polytechnique de Paris>FRA>education;Manuel Moussallam+Deezer R&D>FRA>company|Télécom Paris, Institut Polytechnique de Paris>FRA>education;Gaël Richard+Télécom Paris, Institut Polytechnique de Paris>FRA>education","Cover detection has gained sustained interest in the scientific community and has recently made significant progress both in terms of scalability and accuracy. However, most approaches are based on the estimation of harmonic and melodic features and neglect lyrics information although it is an important invariant across covers. In this work, we propose a novel approach leveraging lyrics without requiring access to full texts though the use of lyrics recognition on audio. Our approach relies on the fusion of a singing voice recognition framework and a more classic tonal-based cover detection method. To the best of our knowledge, this is the first time that lyrics estimation from audio has been explicitly used for cover detection. Furthermore, we exploit efficient string matching and an approximated nearest neighbors search algorithm which lead to a scalable system which is able to operate on very large databases. Extensive experiments on the largest publicly available cover detection dataset demonstrate the validity of using lyrics information for this task.",FRA,company,Developed economies,"[10.419266, 43.75719]","[28.810638, -12.716152]","[1.5115036, 12.679862, -27.14644]","[19.089655, -5.2760396, 1.4313365]","[16.066065, 11.14811]","[10.344062, 2.8678465]","[12.822936, 17.355661, -0.25470382]","[11.842034, 6.6186347, 11.747426]"
90,Ju-Chiang Wang;Jordan B. L. Smith;Wei-Tsung Lu;Xuchen Song,Supervised Metric Learning For Music Structure Features,2021,https://doi.org/10.5281/zenodo.5624427,Ju-Chiang Wang+ByteDance>CHN>company;Jordan B. L. Smith+ByteDance>CHN>company;Wei-Tsung Lu+ByteDance>CHN>company;Xuchen Song+ByteDance>CHN>company,"Music structure analysis (MSA) methods traditionally search for musically meaningful patterns in audio: homogeneity, repetition, novelty, and segment-length regularity. Hand-crafted audio features such as MFCCs or chromagrams are often used to elicit these patterns. However, with more annotations of section labels (e.g., verse, chorus, bridge) becoming available, one can use supervised feature learning to make these patterns even clearer and improve MSA performance. To this end, we take a supervised metric learning approach: we train a deep neural network to output embeddings that are near each other for two spectrogram inputs if both have the same section type (according to an annotation), and otherwise far apart. We propose a batch sampling scheme to ensure the labels in a training pair are interpreted meaningfully. The trained model extracts features that can be used by existing MSA algorithms. In evaluations with three datasets (HarmonixSet, SALAMI, and RWC), we demonstrate that using the proposed features can improve a traditional MSA algorithm significantly in both intra- and cross-dataset scenarios.",CHN,company,Developing economies,"[-5.9586725, 5.028384]","[-18.44363, -29.359818]","[-2.5240338, 11.681851, 2.8021092]","[-4.1869793, -2.0886993, -14.194906]","[12.237553, 9.266374]","[8.847259, 4.5594287]","[13.480985, 14.283924, -0.07580593]","[10.285547, 6.8680325, 9.632612]"
91,Shiqi Wei;Gus Xia,Learning long-term music representations via hierarchical contextual constraints,2021,https://doi.org/10.5281/zenodo.5624351,Shiqi Wei+Fudan University>CHN>education|New York University Shanghai>USA>education;Gus Xia+New York University Shanghai>USA>education,"<p>Learning symbolic music representations, especially disentangled representations with probabilistic interpretations, has been shown to benefit both music understanding and generation. However, most models are only applicable to short-term music, while learning long-term music representations remains a challenging task. We have seen several studies attempting to learn hierarchical representations directly in an end-to-end manner, but these models have not been able to achieve the desired results and the training process is not stable. In this paper, we propose a novel approach to learn long-term symbolic music representations through contextual constraints. First, we use contrastive learning to pre-train a long-term representation by constraining its difference from the short-term representation (extracted by an off-the-shelf model). Then, we fine-tune the long-term representation by a hierarchical prediction model such that a good long-term representation (e.g., an 8-bar representation) can reconstruct the corresponding short-term ones (e.g., the 2-bar representations within the 8-bar range). Experiments show that our method stabilizes the training and the fine-tuning steps. In addition, the designed contextual constraints benefit both reconstruction and disentanglement, significantly outperforming the baselines.</p>",CHN,education,Developing economies,"[-14.155394, -5.0050125]","[-8.883083, -46.369682]","[2.2591555, 9.847355, 12.650469]","[-18.32343, 3.7141721, -16.93975]","[11.485145, 8.990294]","[8.811095, 6.145309]","[13.447164, 13.05841, 0.21310265]","[9.802529, 5.8730445, 8.763332]"
92,Christof Weiss;Johannes Zeitler;Tim Zunner;Florian Schuberth;Meinard Müller,Learning Pitch-Class Representations from Score-Audio Pairs of Classical Music,2021,https://doi.org/10.5281/zenodo.5624549,"Christof Weiß+International Audio Laboratories Erlangen>DEU>facility|LTCI, Télécom Paris, Institut Polytechnique de Paris>FRA>education;Johannes Zeitler+International Audio Laboratories Erlangen>DEU>facility;Tim Zunner+International Audio Laboratories Erlangen>DEU>facility;Florian Schuberth+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility","Chroma or pitch-class representations of audio recordings are an essential tool in music information retrieval. Traditional chroma features relying on signal processing are often influenced by timbral properties such as overtones or vibrato and, thus, only roughly correspond to the pitch classes indicated by a score. Deep learning provides a promising possibility to overcome such problems but requires large annotated datasets. Previous approaches therefore use either synthetic audio, MIDI-piano recordings, or chord annotations for training. Since these strategies have different limitations, we propose to learn transcription-like pitch-class representations using pre-synchronized score-audio pairs of classical music. We train several CNNs with musically inspired architectures and evaluate their pitch-class estimates for various instrumentations including orchestra, piano, chamber music, and singing. Moreover, we illustrate the learned features' behavior when used as input to a chord recognition system. In all our experiments, we compare cross-validation with cross-dataset evaluation. Obtaining promising results, our strategy shows how to leverage the power of deep learning for constructing robust but interpretable tonal representations.",DEU,facility,Developed economies,"[22.026249, -20.646107]","[-26.44501, -31.235907]","[16.10109, -12.148748, -15.049231]","[-8.082536, -7.3608875, -16.87305]","[10.213894, 6.078297]","[8.13664, 5.204853]","[11.45511, 13.240825, -0.7068025]","[9.599542, 6.854581, 8.902031]"
93,Christof Weiss;Geoffroy Peeters,Training Deep Pitch-Class Representations With a Multi-Label CTC Loss,2021,https://doi.org/10.5281/zenodo.5624359,Christof Weiß+Télécom Paris>FRA>education|Institut Polytechnique de Paris>FRA>education;Geoffroy Peeters+Télécom Paris>FRA>education|Institut Polytechnique de Paris>FRA>education,"Despite the success of end-to-end approaches, chroma (or pitch-class) features remain a useful mid-level representation of music audio recordings due to their direct interpretability. Since traditional chroma variants obtained with signal processing suffer from timbral artifacts such as overtones or vibrato, they do not directly reflect the pitch classes notated in the score. For this reason, training a chroma representation using deep learning (""deep chroma"") has become an interesting strategy. Existing approaches involve the use of supervised learning with strongly aligned labels for which, however, only few datasets are available. Recently, the Connectionist Temporal Classification (CTC) loss, initially proposed for speech, has been adopted to learn monophonic (single-label) pitch-class features using weakly aligned labels based on corresponding score--audio segment pairs. To exploit this strategy for the polyphonic case, we propose the use of a multi-label variant of this CTC loss, the MCTC, and formalize this loss for the pitch-class scenario. Our experiments demonstrate that the weakly aligned approach achieves almost equivalent pitch-class estimates than training with strongly aligned annotations. We then study the sensitivity of our approach to segment duration and mismatch. Finally, we compare the learned features with other pitch-class representations and demonstrate their use for chord and local key recognition on classical music datasets.",FRA,education,Developed economies,"[21.742783, -21.2305]","[-25.265562, -31.073929]","[16.216934, -13.116904, -16.589184]","[-8.838831, -13.108238, -15.79762]","[10.111215, 6.03052]","[8.239124, 5.2157493]","[11.331854, 13.304518, -0.65237826]","[9.577141, 6.7747426, 8.879174]"
94,Daniel Wolff;Remi Mignot;Axel Roebel,Audio Defect Detection in Music with Deep Networks,2021,https://doi.org/10.5281/zenodo.5624545,"Daniel Wolff+IRCAM, CNRS, Sorbonne Université>FRA>education;Rémi Mignot+IRCAM, CNRS, Sorbonne Université>FRA>education;Axel Roebel+IRCAM, CNRS, Sorbonne Université>FRA>education","With increasing amounts of music being digitally transferred from production to distribution, automatic means of determining media quality are needed. Protection mechanisms in digital audio processing tools have not eliminated the need of production entities located downstream the distribution chain to assess audio quality and detect defects inserted further upstream. Such analysis often relies on the received audio and scarce meta-data alone.     Deliberate use of artefacts such as clicks in popular music as well as more recent defects stemming from corruption in modern audio encodings call for data-centric and context-sensitive solutions for detection. We present a convolutional network architecture following end-to-end encoder-decoder configuration to develop detectors for two exemplary audio defects.    A click detector is trained and compared to a traditional signal processing method, with a discussion on context sensitivity. Additional post-processing is used for data augmentation and workflow simulation. The ability of our models to capture variance is explored in a detector for artefacts from decompression of corrupted MP3 compressed audio. For both tasks we describe the synthetic generation of artefacts for controlled detector training and evaluation. We evaluate our detectors on the large open-source Free Music Archive (FMA) and genre-specific datasets.",FRA,education,Developed economies,"[-9.610228, -10.660865]","[-50.84131, -12.213787]","[11.697119, -1.5623767, 2.3362732]","[-5.2904153, -16.803352, -10.403941]","[11.689987, 9.236235]","[7.8542695, 6.215086]","[13.114998, 13.432964, 0.76886296]","[10.01056, 6.4406233, 8.486683]"
95,Minz Won;Keunwoo Choi;Xavier Serra,Semi-supervised Music Tagging Transformer,2021,https://doi.org/10.5281/zenodo.5624405,"Minz Won+Music Technology Group, Universitat Pompeu Fabra>ESP>education|ByteDance>USA>company;Keunwoo Choi+ByteDance>USA>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","We present Music Tagging Transformer that is trained with a semi-supervised approach. The proposed model captures local acoustic characteristics in shallow convolutional layers, then temporally summarizes the sequence of the extracted features using stacked self-attention layers. Through a careful model assessment, we first show that the proposed architecture outperforms the previous state-of-the-art music tagging models that are based on convolutional neural networks under a supervised scheme.    The Music Tagging Transformer is further improved by noisy student training, a semi-supervised approach that leverages both labeled and unlabeled data combined with data augmentation. To our best knowledge, this is the first attempt to utilize the entire audio of the million song dataset.",ESP,education,Developed economies,"[-40.721287, -2.397108]","[-22.722147, -38.465958]","[-12.116541, 14.679044, 10.045486]","[-6.1504836, -0.5808081, -21.543167]","[14.549294, 10.664883]","[9.607549, 4.8125215]","[15.577602, 14.111696, 0.115034625]","[10.774299, 6.5338435, 8.896278]"
96,Minz Won;Justin Salamon;Nicholas J. Bryan;Gautham Mysore;Xavier Serra,Emotion Embedding Spaces for Matching Music to Stories,2021,https://doi.org/10.5281/zenodo.5624483,"Minz Won+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Adobe Research>USA>company;Justin Salamon+Adobe Research>USA>company;Nicholas J. Bryan+Adobe Research>USA>company;Gautham J. Mysore+Adobe Research>USA>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Content creators often use music to enhance their stories, as it can be a powerful tool to convey emotion. In this paper, our goal is to help creators find music to match the emotion of their story. We focus on text-based stories that can be auralized (e.g., books), use multiple sentences as input queries, and automatically retrieve matching music. We formalize this task as a cross-modal text-to-music retrieval problem. Both the music and text domains have existing datasets with emotion labels, but mismatched emotion vocabularies prevent us from using mood or emotion annotations directly for matching. To address this challenge, we propose and investigate several emotion embedding spaces, both manually defined (e.g., valence/arousal) and data-driven (e.g., Word2Vec and metric learning) to bridge this gap. Our experiments show that by leveraging these embedding spaces, we are able to successfully bridge the gap between modalities to facilitate cross modal retrieval.   We show that our method can leverage the well established valence-arousal space, but that it can also achieve our goal via data-driven embedding spaces. By leveraging data-driven embeddings, our approach has the potential of being generalized to other retrieval tasks that require broader or completely different vocabularies.",ESP,education,Developed economies,"[-62.10228, -2.3623993]","[43.026875, -8.01831]","[-30.219433, 25.49043, 2.690669]","[18.992174, 19.675093, 1.745027]","[14.050661, 12.904792]","[12.562576, 3.9255705]","[16.083143, 14.386943, 1.8765982]","[13.829422, 5.381384, 10.441784]"
97,Abudukelimu Wuerkaixi;Christodoulos Benetatos;Zhiyao Duan;Changshui Zhang,CollageNet: Fusing arbitrary melody and accompaniment into a coherent song,2021,https://doi.org/10.5281/zenodo.5624619,Abudukelimu Wuerkaixi+Tsinghua University>CHN>education|Institute for Artificial Intelligence>CHN>education|Beijing National Research Center for Information Science and Technology>CHN>facility;Christodoulos Benetatos+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education;Changshui Zhang+Tsinghua University>CHN>education|Institute for Artificial Intelligence>CHN>education|Beijing National Research Center for Information Science and Technology>CHN>facility,"When writing pop or hip-hop music, musicians sometimes sample from other songs and fuse the samples into their own music. We propose a new task in the symbolic music domain that is similar to the music sampling practice and a  neural network model named CollageNet to fulfill this task. Specifically, given a piece of melody and an irrelevant accompaniment with the same length, we fuse them into harmonic two-track music after some necessary changes to the inputs. Besides, users are involved in the fusion process by providing controls to the amount of changes along several disentangled musical aspects: rhythm and pitch of the melody, and chord and texture of the accompaniment. We conduct objective and subjective experiments to demonstrate the validity of our model. Experimental results confirm that our model achieves significantly higher level of harmony than rule-based and data-driven baseline methods. Furthermore, the musicality of each of the tracks does not deteriorate after the transformation applied by CollageNet, which is also superior to the two baselines.",CHN,education,Developing economies,"[16.428595, 0.50441927]","[-5.5039773, -38.738674]","[6.480848, 10.9289055, 23.793095]","[-20.01383, 1.9948312, -7.7894855]","[10.45472, 8.932509]","[9.410749, 6.156546]","[13.537557, 12.196338, -0.14172192]","[9.804651, 5.334253, 9.212473]"
98,Kazuhiko Yamamoto,Human-in-the-Loop Adaptation for Interactive Musical Beat Tracking,2021,https://doi.org/10.5281/zenodo.5624651,Kazuhiko Yamamoto+YAMAHA Corporation>JPN>company,"In music information retrieval (MIR), beat-tracking is one of the most fundamental and important task. However, a perfect algorithm is difficult to achieve. In addition, there could be a no unique correct answer because what one interprets as a beat differs for each individual. To address this, we propose a novel human-in-the-loop user interface that allows the system to interactively adapt to a specific user and target music. In our system, the user does not need to correct all errors manually, but rather only a small portion of the errors. The system then adapts the internal neural network model to the target, and automatically corrects remaining errors. This is achieved by a novel adaptive runtime self-attention in which the adaptable parameters are intimately integrated as a part of the user interface. It enables both low-cost training using only a local context of the music piece, and by contrast, highly effective runtime adaptation using the global context. We show our framework dramatically reduces the user's effort of correcting beat tracking errors in our experiments.",JPN,company,Developed economies,"[33.224525, -35.813118]","[-27.594236, -17.225111]","[9.356832, -27.927944, -9.230241]","[-13.925302, 12.415769, -14.246763]","[10.355248, 4.3760552]","[8.650225, 5.6875997]","[10.258535, 12.833987, -2.2010384]","[10.073643, 5.7837396, 9.039651]"
99,Daniel Yang;Timothy Tsai,Composer Classification With Cross-Modal Transfer Learning and Musically-Informed Augmentation,2021,https://doi.org/10.5281/zenodo.5625645,Daniel Yang+Harvey Mudd College>USA>education|Harvey Mudd College>USA>education;TJ Tsai+Harvey Mudd College>USA>education|Harvey Mudd College>USA>education,"This paper studies composer style classification of piano sheet music, MIDI, and audio data. We expand upon previous work in three ways. First, we explore several musically motivated data augmentation schemes based on pitch-shifting and random removal of individual notes or groups of notes. We show that these augmentation schemes lead to dramatic improvements in model performance, of a magnitude that exceeds the benefit of pretraining on all solo piano sheet music images in IMSLP. Second, we describe a way to modify previous models in order to enable cross-model transfer learning, in which a model trained entirely on sheet music can be used to perform composer classification of audio or MIDI data. Third, we explore the performance of trained models in a 1-shot learning context, in which the model performs classification among a set of composers that are unseen in training. Our results indicate that models learn a representation of compositional style that generalizes beyond the set of composers used in training.",USA,education,Developed economies,"[-5.7807274, -15.436476]","[-21.302395, -28.10294]","[15.72514, 7.8208313, 18.754541]","[-13.386998, -10.021742, -9.106213]","[11.229213, 9.236687]","[8.345068, 4.9309316]","[13.107921, 13.024665, 0.8496573]","[9.456538, 6.372033, 9.447324]"
89,Ziyu Wang;Gus Xia,MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation,2021,https://doi.org/10.5281/zenodo.5624387,"Ziyu Wang+Music X Lab, NYU Shanghai>USA>education;Gus Xia+Music X Lab, NYU Shanghai>USA>education","BERT has proven to be a powerful language model in natural language processing and established an effective pre-training &amp; fine-tuning methodology. We see that music, as a special form of language, can benefit from such methodology if we carefully handle its highly-structured and polyphonic properties. To this end, we propose MuseBERT and show that: 1) MuseBERT has detailed specification of note attributes and explicit encoding of music relations, without presuming any pre-defined sequential event order, 2) the pre-trained MuseBERT is not merely a language model, but also a controllable music generator, and 3) MuseBERT gives birth to various downstream music generation and analysis tasks with practical value. Experiment shows that the pre-trained model outperforms the baselines in terms of reconstruction likelihood and generation quality. We also demonstrate downstream applications including chord analysis, chord-conditioned texture generation, and accompaniment refinement.",USA,education,Developed economies,"[22.101103, 6.8607345]","[-15.850737, -41.94797]","[1.982995, 2.4517741, 24.11599]","[-20.342371, -4.711977, -11.545468]","[10.425907, 8.374077]","[8.903398, 6.0020313]","[13.515615, 11.813241, 0.091834866]","[9.908841, 5.645748, 8.965791]"
76,Harald Victor Schweiger;Emilia Parada-Cabaleiro;Markus Schedl,Does Track Sequence in User-generated Playlists Matter?,2021,https://doi.org/10.5281/zenodo.5624369,"Harald Schweiger+Institute of Computational Perception, Johannes Kepler University Linz (JKU)>AUT>education;Emilia Parada-Cabaleiro+Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Markus Schedl+Institute of Computational Perception, Johannes Kepler University Linz (JKU)>AUT>education","The extent to which the sequence of tracks in music playlists matters to listeners is a disputed question, nevertheless a very important one for tasks such as music recommendation (e.g., automatic playlist generation or continuation). While several user studies already approached this question, results are largely inconsistent. In contrast, in this paper we take a data-driven approach and investigate 704,166 user-generated playlists of a major music streaming provider. In particular, we study the consistency (in terms of variance) of a variety of audio features and metadata between subsequent tracks in playlists, and we relate this variance to the corresponding variance computed on a position-independent set of tracks.  Our results show that some features vary on average up to 16% less among subsequent tracks in comparison to position-independent pairs of tracks. Furthermore, we show that even pairs of tracks that lie up to 12 positions apart in the playlist are significantly more consistent in several audio features and genres. Our findings yield a better understanding of how users create playlists and will stimulate further progress in sequential music recommenders.",AUT,education,Developed economies,"[-41.280167, 40.18325]","[38.11538, 20.564861]","[-3.8336008, 32.14291, -3.7799418]","[15.246241, 6.5574636, 20.299679]","[16.249641, 8.129158]","[12.380965, 1.6812129]","[16.554468, 14.856714, -1.7216238]","[13.332425, 5.133929, 12.774292]"
75,Pedro Pereira Sarmento;Adarsh Kumar;Cj Carr;Zack Zukowski;Mathieu Barthet;Yi-Hsuan Yang,DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models,2021,https://doi.org/10.5281/zenodo.5624597,Pedro Sarmento+Queen Mary University of London>GBR>education;Adarsh Kumar+Academia Sinica>TWN>education|Indian Institute of Technology Kharagpur>IND>education;CJ Carr+Dadabots>Unknown>company;Zack Zukowski+Dadabots>Unknown>company;Mathieu Barthet+Queen Mary University of London>GBR>education;Yi-Hsuan Yang+Academia Sinica>TWN>education|Taiwan AI Labs>TWN>company,"Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation.",GBR,education,Developed economies,"[42.3022, -7.857408]","[-44.620937, -8.966636]","[22.084007, -6.4150715, 9.867102]","[-15.89368, -5.47497, -6.646078]","[8.053808, 8.372896]","[7.4927607, 4.542233]","[11.835247, 11.380312, 0.9829887]","[9.2066, 6.010048, 9.631147]"
74,Antonia Saravanou;Federico Tomasi;Rishabh Mehrotra;Mounia Lalmas,Multi-Task Learning of Graph-based Inductive Representations of Music Content,2021,https://doi.org/10.5281/zenodo.5624379,Antonia Saravanou+University of Athens>GRC>education;Federico Tomasi+Spotify>USA>company;Rishabh Mehrotra+Spotify>USA>company;Mounia Lalmas+Spotify>USA>company,"Music streaming platforms rely heavily on learning meaningful representations of tracks to surface apt recommendations to users in a number of different use cases. In this work, we consider the task of learning music track representations by leveraging three rich heterogeneous sources of information: (i) organizational information (e.g., playlist co-occurrence), (ii) content information (e.g., audio &amp; acoustics), and (iii) music stylistics (e.g., genre). We advocate for a multi-task formulation of graph representation learning, and propose MUSIG: Multi-task Sampling and Inductive learning on Graphs. MUSIG allows us to derive generalized track representations that combine the benefits offered by (i) the inductive graph based framework, which generates embeddings by sampling and aggregating features from a node's local neighborhood, as well as, (ii) multi-task training of aggregation functions, which ensures the learnt functions perform well on a number of important tasks. We present large scale empirical results for track recommendation for the playlist completion task, and compare different classes of representation learning approaches, including collaborative filtering, word2vec and node embeddings as well as, graph embedding approaches. Our results demonstrate that considering content information (i.e.,audio and acoustic features) is useful and that multi-task supervision helps learn better representations.",GRC,education,Developed economies,"[-15.265424, -4.189746]","[36.378353, 10.506733]","[0.3761826, 8.531831, 9.806871]","[24.805548, 7.023799, 15.359041]","[11.711447, 9.039177]","[11.864357, 2.7401297]","[13.590702, 13.194539, 0.33423504]","[13.26581, 5.962409, 12.283788]"
103,Jingwei Zhao;Gus Xia,AccoMontage: Accompaniment Arrangement via Phrase Selection and Style Transfer,2021,https://doi.org/10.5281/zenodo.5625706,"Jingwei Zhao+Music X Lab, NYU Shanghai>USA>education|Music X Lab, NYU Shanghai>USA>education;Gus Xia+Music X Lab, NYU Shanghai>USA>education|Music X Lab, NYU Shanghai>USA>education","Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines.",USA,education,Developed economies,"[15.532805, 0.20393375]","[-6.147425, -40.67683]","[6.961719, 11.031092, 25.944078]","[-22.845142, 0.05667607, -9.353292]","[10.588604, 8.7569475]","[9.235639, 6.2568216]","[13.519576, 12.176992, -0.1779229]","[9.675927, 5.381201, 9.10158]"
53,Andrew Mcleod;Martin A Rohrmeier,A Modular System for the Harmonic Analysis of Musical Scores using a Large Vocabulary,2021,https://doi.org/10.5281/zenodo.5655391,Andrew McLeod+EPFL>CHE>education|EPFL>CHE>education;Martin Rohrmeier+EPFL>CHE>education|EPFL>CHE>education,"The harmonic analysis of a musical composition is a fundamental step towards understanding its structure. Central to this analysis is the labeling of segments of a piece with chord symbols and local key information. In this work, we propose a modular system for performing such a harmonic analysis, incorporating spelled pitches (i.e., not treating enharmonically equivalent pitches as identical) and using a very large vocabulary of 1540 chords (each with a root, type, and inversion) and 70 keys (with a tonic and mode), leading to a full harmonic characterization similar to Roman numeral analysis. Our system's modular design allows each of its components to model an aspect of harmony at an appropriate level of granularity, and also aids in both flexibility and interpretability. We show that our system improves upon a state-of-the-art model for the task, both on a previously available corpus consisting mostly of pieces from the Classical and Romantic eras of Western music, as well as on a much larger corpus spanning a wider range from the 16th through the 20th centuries.",CHE,education,Developed economies,"[-7.0112967, -4.4968495]","[-19.295927, 18.03488]","[15.16793, -5.793087, -11.0848055]","[-17.66828, -5.6829486, 7.5416036]","[10.437959, 9.153097]","[7.847972, 2.1237571]","[12.372384, 14.001811, -1.1871526]","[9.98517, 7.7835236, 12.485432]"
54,Gianluca Micchi;Katerina Kosta;Gabriele Medeot;Pierre Chanquion,A deep learning method for enforcing coherence in Automatic Chord Recognition,2021,https://doi.org/10.5281/zenodo.5624539,Gianluca Micchi+ByteDance>CHN>company;Katerina Kosta+ByteDance>CHN>company;Gabriele Medeot+ByteDance>CHN>company;Pierre Chanquion+ByteDance>CHN>company,"Deep learning approaches to automatic chord recognition and functional harmonic analysis of symbolic music have improved the state of the art, but they still face a common problem: how to deal with a vast chord vocabulary. The naive approach of writing one output class for each possible chord is hindered by the combinatorial explosion of the output size (~10 million classes). We can reduce this complexity by several orders of magnitude by treating each label (e.g. key or chord quality) independently. However this has been shown to lead to incoherent output labels. To solve this issue we introduce a modified Neural Autoregressive Distribution Estimation (NADE) as the last layer of a Convolutional Recurrent Neural Network. The NADE layer ensures that labels related to the same chord are dependently predicted, therefore enforcing coherence. The experiments showcase the advantage of the new model both in automatic chord recognition and functional harmonic analysis compared to the model that does not include NADE as well as State of the Art models.",CHN,company,Developing economies,"[57.503906, -6.5907965]","[-34.5039, 23.74435]","[28.616453, -7.427308, 18.703707]","[-29.23476, -3.3007593, -1.8894793]","[6.5583415, 8.635267]","[5.8838787, 3.9011514]","[11.880131, 10.2786455, 2.2021246]","[9.583189, 9.223861, 12.301257]"
55,Martin A Miguel;Diego Fernandez Slezak,Modeling beat uncertainty as a 2D distribution of period and phase: a MIR task proposal,2021,https://doi.org/10.5281/zenodo.5624639,Martin A. Miguel+Universidad de Buenos Aires>ARG>education|CONICET-Universidad de Buenos Aires>ARG>facility;Diego Fernández Slezak+Universidad de Buenos Aires>ARG>education|CONICET-Universidad de Buenos Aires>ARG>facility,"This work proposes modeling the beat percept as a 2d probability distribution and its inference from musical stimulus as a new MIR task. We present a methodology for collecting a 2d beat distribution of period and phase from free beat-tapping data from multiple participants. The methodology allows capturing beat-tapping variability both within (e.g.: mid-track beat change) and between annotators (e.g.: participants tap at different phases). The data analysis methodology was tested with simulated beat tracks assessing robustness to tapping variability, mid-tapping beat change and disagreement between annotators. It was also tested on experimental tapping data where the entropy of the estimated beat distributions correlated with tapping difficulty reported by the participants. For the MIR task, we propose using optimal transport as an evaluation criterion for models that estimate the beat distribution from musical stimuli. This criterion provides better scores to beat estimations closer in phase or period to distributions obtained from data. Finally, we present baseline models for the task of estimating the beat distribution. The methodology is presented with aims to enhance the exploration of ambiguity in the beat percept. For example, it exposes if beat uncertainty is related to a pulse that is hard to produce or conflicting interpretations of the beat.",ARG,education,Developing economies,"[-15.1370535, 57.731182]","[-31.508512, -1.3372316]","[-32.06403, 3.910887, -1.7703574]","[-9.543959, 13.861301, -5.329798]","[10.79435, 4.384332]","[5.3386736, 1.5770525]","[10.290716, 12.939959, -2.2880778]","[7.4736333, 6.7391453, 11.243167]"
56,Olof Misgeld;Torbjörn L Gulz;Jūra Miniotaitė;Andre Holzapfel,A case study of deep enculturation and sensorimotor synchronization to real music,2021,https://doi.org/10.5281/zenodo.5624537,Olof Misgeld+KMH Royal College of Music>SWE>education;Torbjörn Gulz+KMH Royal College of Music>SWE>education;Andre Holzapfel+KTH Royal Institute of Technology>SWE>education;J¯ura Miniotait˙e+KTH Royal Institute of Technology>SWE>education,"Synchronization of movement to music is a behavioural capacity that separates humans from most other species. Whereas such movements have been studied using a wide range of methods, only few studies have investigated synchronisation to real music stimuli in a cross-culturally comparative setting. The present study employs beat tracking evaluation metrics and accent histograms to analyze the differences in the ways participants from two cultural groups synchronize their tapping with either familiar or unfamiliar music stimuli. Instead of choosing two apparently remote cultural groups, we selected two groups of musicians that share cultural backgrounds, but that differ regarding the music style they specialize in. The employed method to record tapping responses in audio format facilitates a fine-grained analysis of metrical accents that emerge from the responses. The identified differences between groups are related to the metrical structures inherent to the two musical styles, such as non-isochronicity of the beat, and differences between the groups document the influence of the deep enculturation of participants to their style of expertise. Besides these findings, our study sheds light on a conceptual weakness of a common beat tracking evaluation metric, when applied to human tapping instead of machine generated beat estimations.",SWE,education,Developed economies,"[27.137949, -31.691738]","[-25.083172, 3.2620695]","[-0.8377747, -23.263826, -11.682915]","[-3.9048884, 15.6574955, -3.9668803]","[11.284064, 5.829121]","[5.8528824, 1.4536288]","[12.02035, 12.618745, -2.1196966]","[7.9027104, 6.584987, 11.576086]"
57,Gautam Mittal;Jesse Engel;Curtis Hawthorne;Ian Simon,Symbolic Music Generation with Diffusion Models,2021,https://doi.org/10.5281/zenodo.5624363,"Gautam Mittal+University of California, Berkeley>USA>education;Jesse Engel+Google Brain>USA>company;Curtis Hawthorne+Google Brain>USA>company;Ian Simon+Google Brain>USA>company","Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.",USA,education,Developed economies,"[20.48758, 3.5176084]","[-9.080028, -44.605755]","[2.842094, -0.30175775, 19.146055]","[-19.008194, 4.8838167, -14.686744]","[10.324483, 8.504286]","[8.839371, 6.442294]","[13.315776, 11.883021, 0.020834325]","[9.591933, 5.565457, 8.533175]"
58,Faraaz Nadeem,Learning from Musical Feedback with Sonic the Hedgehog,2021,https://doi.org/10.5281/zenodo.5624541,Faraaz Nadeem+Massachusetts Institute of Technology>USA>education,"Most videogame reinforcement learning (RL) research only deals with the video component of games, even though humans typically play while experiencing both audio and video. In this paper, we aim to bridge this gap in research, and present two main contributions. First, we provide methods for extracting, processing, visualizing, and hearing gameplay audio alongside video. Then, we show that in Sonic The Hedgehog, agents provided with both audio and video can outperform agents with access to only video by 6.6% on a joint training task, and 20.4% on a zero-shot transfer task. We conclude that game audio informs useful decision making, and that audio features are more easily transferable to unseen test levels than video features.",USA,education,Developed economies,"[-11.870116, -4.192006]","[2.472441, -34.71261]","[4.6487536, 14.12639, 16.287235]","[3.715096, 10.066573, -28.783272]","[11.164366, 8.584353]","[8.773154, 5.9146814]","[13.519119, 12.804075, -0.14042981]","[9.799493, 6.0953994, 9.0511]"
59,Javier Nistal;Stefan Lattner;Gaël Richard,DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis With GANs,2021,https://doi.org/10.5281/zenodo.5624507,Javier Nistal+Telecom Paris>FRA>education|Sony Computer Science Laboratories (CSL)>FRA>company;Stefan Lattner+Sony Computer Science Laboratories (CSL)>FRA>company;Gaël Richard+Telecom Paris>FRA>education,"Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called ""soft labels"") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website.",FRA,education,Developed economies,"[26.622822, 4.667383]","[-17.41143, -47.956257]","[8.2556095, 2.7407978, 27.777138]","[-23.063704, 0.25520897, -19.064194]","[9.799323, 8.470607]","[8.300203, 6.699076]","[13.097244, 11.786249, 0.4940817]","[9.586157, 5.820133, 8.088054]"
60,Takehisa Oyama;Ryoto Ishizuka;Kazuyoshi Yoshii,Phase-Aware Joint Beat and Downbeat Estimation Based on Periodicity of Metrical Structure,2021,https://doi.org/10.5281/zenodo.5624517,Takehisa Oyama+Kyoto University>JPN>education;Ryoto Ishizuka+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education|Japan Science and Technology Agency (JST)>JPN>facility,"This paper describes a phase-aware joint beat and downbeat estimation method mainly intended for popular music with a periodic metrical structure and steady tempo. The conventional approach to beat estimation is to train a deep neural network (DNN) that estimates the beat presence probability at each frame. This approach, however, relies heavily on a periodicity-aware post-processing step that detects beat times from the noisy probability sequence. To mitigate this problem, we have designed a DNN that estimates the beat phase at each frame whose period is equal to the beat interval. The estimation losses computed at all frames not limited to a fewer number of beat frames can thus be effectively used for backpropagation-based supervised training, whereas a DNN has conventionally been trained such that it constantly outputs zero at all non-beat frames. The same applies to downbeat estimation. We also modify the post-processing method for the estimated phase sequence. For joint beat and downbeat detection, we investigate multi-task learning architectures that output beat and downbeat phases in this order, in reverse order, and in parallel. The experimental results demonstrate the importance of phase modeling for stable beat and downbeat estimation.",JPN,education,Developed economies,"[36.04881, -33.27945]","[-30.983698, -13.404657]","[8.866892, -26.768232, -4.860873]","[-7.315953, 14.238758, -17.119106]","[10.549951, 4.256702]","[4.8973136, 2.4616587]","[10.1784935, 12.925255, -2.2138147]","[7.650458, 6.948342, 10.219339]"
61,Yuto Ozaki;John M Mcbride;Emmanouil Benetos;Peter Pfordresher;Joren Six;Adam Tierney;Polina Proutskova;Emi Sakai;Haruka Kondo;Haruno Fukatsu;Shinya Fujii;Patrick E. Savage,Agreement Among Human and Automated Transcriptions of Global Songs,2021,https://doi.org/10.5281/zenodo.5624529,"Yuto Ozaki+Keio University>JPN>education;John McBride+Center for Soft and Living Matter, Institute for Basic Science>KOR>facility;Emmanouil Benetos+Queen Mary University of London>GBR>education;Peter Q. Pfordresher+University at Buffalo>USA>education;Joren Six+Ghent University>BEL>education;Adam T. Tierney+Birkbeck, University of London>GBR>education;Polina Proutskova+Queen Mary University of London>GBR>education;Emi Sakai+Unknown>Unknown>Unknown;Haruka Kondo+Keio University>JPN>education;Haruno Fukatsu+Keio University>JPN>education;Shinya Fujii+Keio University>JPN>education;Patrick E. Savage+Keio University>JPN>education","Cross-cultural musical analysis requires standardized symbolic representation of sounds such as score notation. However, transcription into notation is usually conducted manually by ear, which is time-consuming and subjective. Our aim is to evaluate the reliability of existing methods for transcribing songs from diverse societies. We had 3 experts independently transcribe a sample of 32 excerpts of traditional monophonic songs from around the world (half a cappella, half with instrumental accompaniment). 16 songs also had pre-existing transcriptions created by 3 different experts. We compared these human transcriptions against one another and against 10 automatic music transcription algorithms. We found that human transcriptions can be sufficiently reliable (~90% agreement, κ ~.7), but current automated methods are not (&lt;60% agreement, κ &lt;.4). No automated method clearly outperformed others, in contrast to our predictions. These results suggest that improving automated methods for cross-cultural music transcription is critical for diversifying MIR.",JPN,education,Developed economies,"[-22.415483, -32.507034]","[-11.943584, 29.309307]","[18.817434, -0.74409264, 8.114169]","[-5.0407267, 12.194507, 10.81754]","[10.699991, 10.975204]","[7.370995, 2.3237855]","[11.869244, 15.004853, 0.42412025]","[9.466789, 6.5536213, 10.8204565]"
62,Emilia Parada-Cabaleiro;Maximilian Schmitt;Anton Batliner;Bjorn W. Schuller;Markus Schedl,Automatic Recognition of Texture in Renaissance Music,2021,https://doi.org/10.5281/zenodo.5624443,"Emilia Parada-Cabaleiro+Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Maximilian Schmitt+Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg>DEU>education;Anton Batliner+Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg>DEU>education;Björn Schuller+Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg>DEU>education|GLAM – Group on Language, Audio & Music, Imperial College London>GBR>education;Markus Schedl+Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education","Renaissance music constitutes a resource of immense richness for Western culture, as shown by its central role in digital humanities. Yet, despite the advance of computational musicology in analysing other Western repertoires, the use of computer-based methods to automatically retrieve relevant information from Renaissance music, e. g., identifying word-painting strategies such as madrigalisms, is still underdeveloped. To this end, we propose a score-based machine learning approach for the classification of texture in Italian madrigals of the 16th century. Our outcomes indicate that Low Level Descriptors, such as intervals, can successfully convey differences in High Level features, such as texture. Furthermore, our baseline results, particularly the ones from a Convolutional Neural Network, show that machine learning can be successfully used to automatically identify sections in madrigals associated with specific textures from symbolic sources.",AUT,education,Developed economies,"[35.70349, 20.731527]","[-16.524862, 28.361248]","[21.063665, 8.498647, 8.20517]","[-17.805433, -16.603277, 2.3696434]","[8.920122, 6.4944725]","[7.493264, 0.3334114]","[10.947965, 11.512688, -0.08859762]","[9.437327, 5.9626546, 11.851806]"
102,Huan Zhang;Yiliang Jiang;Tao Jiang;Hu Peng,Learn by Referencing: Towards Deep Metric Learning for Singing Assessment,2021,https://doi.org/10.5281/zenodo.5624579,Huan Zhang+Carnegie Mellon University>USA>education;Yiliang Jiang+Tencent Music Entertainment>CHN>company;Tao Jiang+Tencent Music Entertainment>CHN>company;Peng Hu+Tencent Music Entertainment>CHN>company,"The excellence of human singing is an important aspect of subjective, aesthetic perception of music. In this paper, we propose a novel approach to tackle Automatic Singing Assessment (ASA) task through deep metric learning. With the goal of retrieving the commonalities of good singing without explicitly engineering them, we force a triplet model to map perceptually pleasant-sounding singing performance closer to the reference track compared to others, and thus learning a joint embedding space with performance characteristics. Incorporating mid-level representations like spectrogram and chroma, this approach takes advantage of the feature learning ability of neural networks, while using the reference track as an important anchor. On our designed testing set that spans across various styles and techniques, our model outperforms traditional rule-based ASA systems.",USA,education,Developed economies,"[-5.0518684, -39.151894]","[-29.943092, -35.671513]","[23.274157, 14.14259, -7.6461744]","[-6.809023, -7.549091, -21.729511]","[9.806761, 10.913101]","[7.834622, 5.580567]","[11.333074, 14.958695, 0.9143918]","[9.956454, 7.082247, 8.696458]"
63,Ashis Pati;Alexander Lerch,Is Disentanglement enough? On Latent Representations for Controllable Music Generation,2021,https://doi.org/10.5281/zenodo.5624591,Ashis Pati+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education,"Improving controllability or the ability to manipulate one or more attributes of the generated data has become a topic of interest in the context of deep generative models of music. Recent attempts in this direction have relied on learning disentangled representations from data such that the underlying factors of variation are well separated. In this paper, we focus on the relationship between disentanglement and controllability by conducting a systematic study using different supervised disentanglement learning algorithms based on the Variational Auto-Encoder (VAE) architecture. Our experiments show that a high degree of disentanglement can be achieved by using different forms of supervision to train a strong discriminative encoder. However, in the absence of a strong generative decoder, disentanglement does not necessarily imply controllability. The structure of the latent space with respect to the VAE-decoder plays an important role in boosting the ability of a generative model to manipulate different attributes. To this end, we also propose methods and metrics to help evaluate the quality of a latent space with respect to the afforded degree of controllability.",USA,education,Developed economies,"[22.18919, 5.69499]","[-11.488208, -45.346245]","[2.3051717, 6.196333, 21.542395]","[-19.926617, 1.8621408, -16.364576]","[10.43148, 8.476407]","[8.79321, 6.607806]","[13.436223, 11.972417, 0.23266642]","[9.6848345, 5.6246557, 8.465399]"
64,Nicolás Pironio;Diego Fernandez Slezak;Martin A Miguel,Pulse clarity metrics developed from a deep learning beat tracking model,2021,https://doi.org/10.5281/zenodo.5625692,Nicolás Pironio+Universidad de Buenos Aires>ARG>education;Diego Fernández Slezak+Universidad de Buenos Aires>ARG>education|CONICET-Universidad de Buenos Aires>ARG>facility;Martín A. Miguel+Universidad de Buenos Aires>ARG>education|CONICET-Universidad de Buenos Aires>ARG>facility,"In this paper we present novel pulse clarity metrics based on different sections of a state-of-the-art beat tracking model. Said model consists of two sections: a recurrent neural network that estimates beat probabilities for audio and a dynamic Bayesian network (DBN) that determines beat moments from the neural network's output. We obtained pulse clarity metrics by analyzing periodical behavior from neuron activation values and we interpreted the probability distribution computed by the DBN as the model's certainty. To analyze whether the inner workings of the model provide new insight into pulse clarity, we also proposed reference metrics using the output of both networks. We evaluated the pulse clarity metrics over a wide range of stimulus types such as songs and mono-tonal rhythms, obtaining comparable results to previous models. These results suggest that adapting a model from a related task is feasible for the pulse clarity problem. Additionally, results of the evaluation of pulse clarity models on multiple datasets showed that, with some variability, both ours and previous work generalized well beyond their original training datasets.",ARG,education,Developing economies,"[32.741318, -32.076305]","[-29.061306, -13.476762]","[9.670612, -32.476166, -1.456833]","[-7.2718, 17.89293, -17.35518]","[10.561679, 4.256493]","[5.127578, 2.3818822]","[10.14845, 12.897773, -2.251706]","[7.682458, 7.0467186, 10.42449]"
65,Verena Praher;Katharina Prinz;Arthur Flexer;Gerhard Widmer,"On the Veracity of Local, Model-agnostic Explanations in Audio Classification: Targeted Investigations with Adversarial Examples",2021,https://doi.org/10.5281/zenodo.5624471,"Verena Praher+Johannes Kepler University Linz>AUT>education;Katharina Prinz+Johannes Kepler University Linz>AUT>education;Arthur Flexer+Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>facility;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>facility","Local explanation methods such as LIME have become popular in MIR as  tools for generating post-hoc, model-agnostic explanations of a  model's classification decisions. The basic idea is to identify  a small set of human-understandable features of the classified example that are most influential on the classifier's prediction.  These are then presented as an explanation.  Evaluation of such explanations in publications often resorts to  accepting what matches the expectation of a human without actually being able to verify if what the explanation shows is what really caused the model's prediction.  This paper reports on targeted investigations where we try to get more insight into the actual veracity of LIME's explanations in an audio classification task.  We deliberately design adversarial examples for the classifier, in a way that  gives us knowledge about which parts of the input are potentially responsible  for the model's (wrong) prediction. Asking LIME to explain the predictions for these adversaries permits us to study whether local explanations do indeed detect these regions  of interest. We also look at whether LIME is more successful in finding perturbations that are more prominent and easily noticeable for a human.   Our results suggest that LIME does not necessarily manage to identify the most relevant input features and hence it remains unclear whether explanations are useful or even misleading.",AUT,education,Developed economies,"[-31.49154, 0.2563581]","[-35.81877, -41.13404]","[-1.7009223, 19.700312, 18.12168]","[-10.963087, 1.166473, -11.896373]","[12.168228, 9.221844]","[9.210166, 4.909351]","[13.771617, 13.505404, 0.21359666]","[10.370204, 6.3496, 9.462242]"
66,Laure Prétet;Gaël Richard;Geoffroy Peeters,"Is there a ""language of music-video clips"" ? A qualitative and quantitative study",2021,https://doi.org/10.5281/zenodo.5625696,Laure Prétet+Télécom Paris>FRA>education|Bridge.audio>FRA>company;Gaël Richard+Télécom Paris>FRA>education|IP Paris>FRA>education;Geoffroy Peeters+Télécom Paris>FRA>education|IP Paris>FRA>education,"Recommending automatically a video given a music or a music given a video has become an important asset for the audiovisual industry - with user-generated or professional content.  While both music and video have specific temporal organizations, most current works do not consider those and only focus on globally recommending a media.  As a first step toward the improvement of these recommendation systems, we study in this paper the relationship between music and video temporal organization.  We do this for the case of official music videos, with a quantitative and a qualitative approach.  Our assumption is that the movement in the music are correlated to the ones in the video.  To validate this, we first interview a set of internationally recognized music video experts.  We then perform a large-scale analysis of official music-video clips (which we manually annotated into video genres) using MIR description tools (downbeats and functional segments estimation) and Computer Vision tools (shot detection).  Our study confirms that a ""language of music-video clips"" exists; i.e. editors favor the co-occurrence of music and video events using strategies such as anticipation.   It also highlights that the amount of co-occurrence depends on the music and video genres.",FRA,education,Developed economies,"[-15.093434, 35.615788]","[24.504065, 9.459762]","[-14.205248, 14.280988, -24.561827]","[15.591192, 1.7140787, 2.7199085]","[13.140159, 8.879676]","[10.104213, 2.4362247]","[13.613456, 14.69996, -0.939274]","[11.467314, 6.40056, 11.706857]"
67,Gowriprasad R;Venkatesh V;Hema A Murthy;R Aravind;Sri Rama Murty K,Tabla Gharana Recognition from Audio music recordings of Tabla Solo performances,2021,https://doi.org/10.5281/zenodo.5624631,"Gowriprasad R+Indian Institute of Technology, Madras>IND>education;Venkatesh V+Indian Institute of Technology, Hyderabad>IND>education;Hema A Murthy+Indian Institute of Technology, Madras>IND>education;R Aravind+Indian Institute of Technology, Madras>IND>education;Sri Rama Murty K+Indian Institute of Technology, Hyderabad>IND>education","Tabla is a percussion instrument in Hindustani music tradition. Tabla learning and performance in the Indian subcontinent is based on stylistic schools called gharana-s. Each gharana is characterized by its unique style of playing technique, dynamics of tabla strokes, repertoire, compositions, and improvisations. Identifying the gharana from a tabla performance is hence helpful to characterize the performance. This paper addresses the task of automatic gharana recognition from solo tabla recordings. We motivate the problem and present different facets and challenges in the task. We present a comprehensive and diverse collection of over 16 hours of tabla solo recordings for the task. We propose an approach using deep learning models that use a combination of convolutional neural networks (CNN) and long short-term memory (LSTM) networks. The CNNs are used to extract gharana discriminative features from the raw audio data. The LSTM networks are trained to classify the gharana-s by processing the sequence of extracted features from CNNs. Our experiments on gharana recognition include different lengths of audio data and comparison between various aspects of the task. An evaluation demonstrates promising results with the highest recognition accuracy of 93%.",IND,education,Developing economies,"[31.563839, -49.413242]","[-39.957973, -11.730494]","[28.670515, -18.681152, -0.27403617]","[1.0671132, 7.529798, -19.141733]","[7.962905, 7.6871123]","[7.9957666, 4.5613604]","[10.928443, 11.698858, 1.0568671]","[9.065437, 6.930176, 9.581179]"
68,Lindsey Reymore;Emmanuelle Beauvais-Lacasse;Bennett Smith;Stephen Mcadams,Navigating noise: Modeling perceptual correlates of noise-related semantic timbre categories with audio features,2021,https://doi.org/10.5281/zenodo.5624469,Lindsey Reymore+McGill University>CAN>education;Emmanuelle Beauvais-Lacasse+McGill University>CAN>education;Bennett Smith+McGill University>CAN>education;Stephen McAdams+McGill University>CAN>education,"Audio features such as inharmonicity, noisiness, and spectral roll-off have been identified as correlates of ""noisy"" sounds; however, such features are likely involved in the experience of multiple semantic timbre categories of varied meaning and valence. This paper examines the relationships among audio features and the semantic timbre categories raspy/grainy/rough, harsh/noisy, and airy/breathy. Participants (n = 153) rated a random subset of 52 stimuli from a set of 156 ~2-second orchestral instrument sounds from varied instrument families, registers, and playing techniques. Stimuli were rated on the three semantic categories of interest and on perceived playing effort and emotional valence. With an updated version of the Timbre Toolbox (R-2021 A), we extracted 44 summary audio features from the stimuli using spectral and harmonic representations. These features were used as input for various models built to predict mean semantic ratings (raspy/grainy/rough, harsh/noisy, airy/breathy) for each sound. Random Forest models predicting semantic ratings from audio features outperformed Partial Least-Squares Regression models, consistent with previous results suggesting non-linear methods are advantageous in timbre semantic predictions using audio features. In comparing Relative Variable Importance measures from the models among the three semantic categories, results demonstrate that although these related semantic categories are associated in part with overlapping features, they can be differentiated through individual patterns of feature relationships.",CAN,education,Developed economies,"[-17.532751, -0.7666947]","[49.736485, -14.817464]","[-1.6776663, 19.017479, 12.42054]","[3.0554092, 19.481506, 7.444731]","[12.54065, 9.136165]","[12.545135, 4.3008766]","[13.60645, 13.974154, -0.5847954]","[13.724907, 4.795282, 10.290707]"
69,Kyle Robinson;Dan Brown,Quantitative User Perceptions of Music Recommendation List Diversity,2021,https://doi.org/10.5281/zenodo.5624535,"Kyle Robinson+David R. Cheriton School of Computer Science, University of Waterloo>CAN>education;Dan Brown+David R. Cheriton School of Computer Science, University of Waterloo>CAN>education","Diversity is known to play an important role in recommender systems. However, its relationship to users and their satisfaction is not well understood, especially in the music domain. We present a user study: 92 participants were asked to evaluate personalized recommendation lists at varying levels of diversity. Recommendations were generated by two different collaborative filtering methods, and diversified in three different ways, one of which is a simple and novel method based on genre filtering. All diversified lists were recognised by users to be more diverse, and this diversification increased overall recommendation list satisfaction. Our simple filtering approach was also successful at tailoring diversity to some users. Within the collaborative filtering framework, however, we were not able to generate enough diversity to match all user preferences. Our results highlight the need to diversify in music recommendation lists, even when it comes at the cost of ""accuracy"".",CAN,education,Developed economies,"[-47.358547, 24.167065]","[41.927967, 16.47446]","[-9.536059, 26.94836, -7.4572425]","[15.382223, 10.759935, 20.726406]","[15.865216, 9.097842]","[12.673897, 1.7401288]","[15.791369, 15.57914, -1.418514]","[13.530062, 5.130373, 12.708565]"
70,Martin A Rohrmeier;Fabian C. Moss,A Formal Model of Extended Tonal Harmony,2021,https://doi.org/10.5281/zenodo.5624635,"Martin Rohrmeier+Digital and Cognitive Musicology Lab, EPFL>CHE>education;Fabian C. Moss+Digital and Cognitive Musicology Lab, EPFL>CHE>education","Extended tonality is a central system that characterizes the music from the 19th up to the 21st century, including styles like popular music, film music or Jazz. Developing from classical major-minor tonality, the harmonic language of extended tonality forms its own set of rules and regularities, which are a result of the freer combinatoriality of chords within phrases, non-standard chord forms, the emancipation of dissonance, and the loosening of the concept of key. These phenomena posit a challenge for formal, mathematical theory building. The theoretical model proposed in this paper proceeds from Neo-Riemannian and Tonfeld theory, a systematic but informal music-theoretical framework for extended tonality. Our model brings together three fundamental components: the underlying algebraic structure of the Tonnetz, the three basic analytical categories from Tonfeld theory (octatonic and hexatonic collections as well as stacks of fifths), and harmonic syntax in terms of formal language theory. The proposed model is specified to a level of detail that lends itself for implementation and empirical investigation.",CHE,education,Developed economies,"[27.00751, 24.186123]","[-17.90844, 19.633661]","[-2.2509968, -9.938841, 34.044903]","[-17.592226, -8.0659075, 8.954122]","[9.861295, 9.2856245]","[7.762522, 2.1598246]","[11.698568, 14.044483, -0.93743694]","[10.023268, 7.798253, 12.51225]"
71,Simon Rouard;Gaëtan Hadjeres,CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis,2021,https://doi.org/10.5281/zenodo.5624403,Simon Rouard+Sony CSL - CentraleSupélec>FRA>company;Gaëtan Hadjeres+Sony CSL>FRA>company,"In this paper, we propose a novel score-base generative model for unconditional raw audio synthesis.  Our proposal builds upon the latest developments on diffusion process modeling with stochastic differential equations, which already demonstrated promising results on image generation.  We motivate novel heuristics for the choice of the diffusion processes better suited for audio generation, and consider the use of a conditional U-Net to approximate the score function. While previous approaches on diffusion models on audio were mainly designed as speech vocoders in medium resolution, our method termed CRASH (Controllable Raw Audio Synthesis with High-resolution) allows us to generate short percussive sounds in 44.1kHz in a controllable way.  Through extensive experiments, we showcase on a drum sound generation task the numerous sampling schemes offered by our method (unconditional generation, deterministic generation, inpainting, interpolation, variations, class-conditional sampling) and propose the class-mixing sampling, a novel way to generate ""hybrid"" sounds.  Our proposed method offers flexible generation capabilities with lighter and easier-to-train models than GAN-based methods.",FRA,company,Developed economies,"[23.907354, -40.39005]","[-18.809702, -49.094593]","[18.2403, -14.507875, 4.771261]","[-23.218212, -2.833554, -17.487827]","[9.234603, 8.38235]","[8.210067, 6.7375474]","[10.653865, 11.757198, 0.9235623]","[9.608876, 5.9062934, 8.092017]"
72,Luke O Rowe;George Tzanetakis,Curriculum Learning for Imbalanced Classification in Large Vocabulary Automatic Chord Recognition,2021,https://doi.org/10.5281/zenodo.5624463,Luke Rowe+University of Victoria>CAN>education|University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"A problem inherent to the task of large vocabulary automatic chord recognition (ACR) is that the distribution over the chord qualities typically exhibits power-law characteristics. This intrinsic imbalance makes it difficult for ACR systems to learn the rare chord qualities in a large chord vocabulary. While recent ACR systems have exploited the hierarchical relationships that exist between chord qualities, few have attempted to exploit these relationships explicitly to improve the classification of rare chord qualities.    In this paper, we propose a convolutional Transformer model for the task of ACR trained on a dataset of 1217 tracks over a large chord vocabulary consisting of 170 chord types. In order to address the class imbalance of the chord quality distribution, we incorporate the hierarchical relationships between chord qualities into a curriculum learning training scheme that gradually learns the rare and complex chord qualities in the dataset. We show that the proposed convolutional Transformer model achieves state-of-the-art performance on traditional ACR evaluation metrics. Furthermore, we show that the proposed curriculum learning training scheme outperforms existing methods in improving the classification of rare chord qualities.",CAN,education,Developed economies,"[58.35274, -5.013518]","[-33.980175, 22.619112]","[26.751566, -8.832942, 21.945093]","[-28.784033, -1.7244022, 0.305118]","[6.6430845, 8.698429]","[5.901384, 3.8479624]","[11.892735, 10.270516, 2.1752815]","[9.673594, 9.133236, 12.319934]"
73,Justin Salamon;Oriol Nieto;Nicholas J. Bryan,Deep Embeddings and Section Fusion Improve Music Segmentation,2021,https://doi.org/10.5281/zenodo.5624371,Justin Salamon+Adobe Research>USA>company;Oriol Nieto+Adobe Research>USA>company;Nicholas J. Bryan+Adobe Research>USA>company,"Music segmentation algorithms identify the structure of a music recording by automatically dividing it into sections and determining which sections repeat and when. Since the desired granularity of the sections may vary by application, multi-level segmentation produces several levels of segmentation ordered by granularity from one section (the whole song) up to N unique sections, and has proven to be a challenging MIR task. In this work we propose a multi-level segmentation method that leverages deep audio embeddings learned via other tasks. Our approach builds on an existing multi-level segmentation algorithm, replacing manually engineered features with deep embeddings learned through audio classification problems where data are abundant. Additionally, we propose a novel section fusion algorithm that leverages the multi-level segmentation to consolidate short segments at each level in a way that is consistent with the segmentations at lower levels. Through a series of experiments we show that replacing handcrafted features with deep embeddings can lead to significant improvements in multi-level music segmentation performance, and that section fusion further improves the results by cleaning up spurious short sections. We compare our approach to two strong baselines and show that it yields state-of-the-art results.",USA,company,Developed economies,"[-4.161273, -3.595085]","[-4.252548, -26.202814]","[6.379934, -3.3492014, 1.2639836]","[-1.539371, 0.0665574, -15.963467]","[11.709672, 8.483952]","[8.912148, 4.3472295]","[12.660291, 14.030581, 0.2830192]","[10.41501, 6.9420614, 10.5127535]"
50,Vincenzo Madaghiele;Pasquale Lisena;Raphael Troncy,MINGUS: Melodic Improvisation Neural Generator Using Seq2Seq,2021,https://doi.org/10.5281/zenodo.5625684,Vincenzo Madaghiele+EURECOM>FRA>education|EURECOM>FRA>facility;Pasquale Lisena+EURECOM>FRA>education|EURECOM>FRA>facility;Raphaël Troncy+EURECOM>FRA>education|EURECOM>FRA>facility,"Sequence to Sequence (Seq2Seq) approaches have shown good performances in automatic music generation. We introduce MINGUS, a Transformer-based Seq2Seq architecture for modelling and generating monophonic jazz melodic lines.  MINGUS relies on two dedicated embedding models (respectively for pitch and duration) and exploits in prediction features such as chords (current and following), bass line, position inside the measure.   The obtained results are comparable with the state of the art of music generation with neural models, with particularly good performances on jazz music.",FRA,education,Developed economies,"[17.121702, 3.499614]","[-6.6970663, -36.71729]","[-3.7034116, 0.44726473, 29.108877]","[-20.354965, 2.9267604, -4.1914144]","[10.227642, 8.768163]","[9.140763, 6.042231]","[13.174224, 11.904358, 0.080922544]","[9.622196, 5.5322485, 9.12848]"
52,Naotake Masuda;Daisuke Saito,Synthesizer Sound Matching with Differentiable DSP,2021,https://doi.org/10.5281/zenodo.5624609,Naotake Masuda+The University of Tokyo>JPN>education;Daisuke Saito+The University of Tokyo>JPN>education,"While synthesizers have become commonplace in music production, many users find it difficult to control the parameters of a synthesizer to create the intended sound. In order to assist the user, the sound matching task aims to estimate synthesis parameters that produce a sound closest to the query sound. Recently, neural networks have been employed for this task. These neural networks are trained on paired data of synthesis parameters and the corresponding output sound, optimizing a loss of synthesis parameters. However, synthesis parameters are only indirectly correlated with the audio output. Another problem is that query made by the user usually consists of real-world sounds, different from the synthesizer output used during training.   In this paper, we propose a novel approach to the problem of synthesizer sound matching by implementing a basic subtractive synthesizer using differentiable DSP modules. This synthesizer has interpretable controls and is similar to those used in music production. We can then train an estimator network by directly optimizing the spectral similarity of the synthesized output. Furthermore, we can train the network on real-world sounds whose ground-truth synthesis parameters are unavailable. We pre-train the network with parameter loss and fine-tune the model with spectral loss using real-world sounds. We show that the proposed method finds better matches compared to baseline models.",JPN,education,Developed economies,"[14.39634, -34.636738]","[-20.762785, -50.54837]","[29.157778, 1.84003, 8.168379]","[-24.578087, -5.383024, -18.691608]","[9.496031, 8.593055]","[8.089065, 6.728281]","[12.302666, 12.391263, 0.75778115]","[9.53779, 5.9845448, 7.937027]"
64,Marco Pasini;Jan Schlüter,Musika! Fast Infinite Waveform Music Generation,2022,https://doi.org/10.5281/zenodo.7316720,"Marco Pasini+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Jan Schlüter+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education","Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours.",AUT,education,Developed economies,"[21.70239, 3.317881]","[-17.168186, -46.624355]","[5.555195, 1.4658729, 18.987858]","[-21.330574, -1.5321453, -18.758959]","[10.10641, 8.509045]","[8.463504, 6.6294723]","[13.242693, 11.884527, 0.19917275]","[9.632393, 5.769113, 8.216661]"
107,Daniel Szelogowski;Lopamudra Mukherjee;Benjamin Whitcomb,A Novel Dataset and Deep Learning Benchmark for Classical Music Form Recognition and Analysis,2022,https://doi.org/10.5281/zenodo.7316810,Daniel Szelogowski+University of Wisconsin>USA>education;Lopamudra Mukherjee+University of Wisconsin>USA>education;Benjamin Whitcomb+University of Wisconsin>USA>education,"Automated computational analysis schemes for Western classical music analysis based on form and hierarchical structure have not received much attention in the literature so far. One reason, of course, is the paucity of labeled datasets — which, if available, could be used to train machine learning approaches. Dataset curation cannot be crowdsourced; one needs trained musicians to devote sizable effort to carry out such annotations. Further, such an analysis is not simple for beginners; obtaining labeled data that can capture the nuances of a musician's reasoning acquired over years of practice is fraught with challenges. To this end, we provide a system for computational analysis of classical music, both for machine learning and music researchers. First, we introduce a labeled dataset containing 200 classical music pieces annotated by form and phrases. Then, by leveraging this dataset, we show that deep learning-based methods can be used to learn Form Classification as well as Phrase Analysis and Classification, for which few (if any) results have been reported yet. Taken together, we provide the community with a unique dataset as well as a toolkit needed to analyze classical music structure, which can be used or extended to drive applications in both commercial and educational settings.",USA,education,Developed economies,"[-14.047793, -13.043431]","[-9.336748, 13.477976]","[6.727643, 20.633263, 17.331879]","[-12.099091, 5.396375, 4.879655]","[11.751056, 9.640877]","[9.384396, 4.3834286]","[13.4343, 13.321236, 0.7589062]","[10.319876, 6.2486606, 10.173226]"
83,Marcos Acosta;Irmak Bukey;T J Tsai,An Exploration of Generating Sheet Music Images,2022,https://doi.org/10.5281/zenodo.7316758,Marcos Acosta+Harvey Mudd College>USA>education;Irmak Bukey+Pomona College>USA>education;TJ Tsai+Harvey Mudd College>USA>education,"Many previous works in recent years have explored various forms of music generation. These works have focused on generating either raw audio waveforms or symbolic music. In this work, we explore the feasibility of generating sheet music images, which is often the primary form in which musical compositions are notated for other musicians. Using the PrIMuS dataset as a testbed, we explore five different sequence-based approaches for generating lines of sheet music: generating sequences of (a) pixel columns, (b) image patches, (c) visual word tokens, (d) semantic tokens, and (e) XML-based tags. We show sample generated images, discuss the practical challenges and problems with each approach, and give our recommendation on the most promising paths to explore in the future.",USA,education,Developed economies,"[31.648249, 6.3066516]","[-6.221642, -41.903267]","[24.308828, 5.537651, 15.669664]","[-21.371304, -2.2850869, -8.880935]","[10.543299, 6.862995]","[9.114674, 6.2949367]","[12.388487, 11.998949, -1.2509979]","[9.602658, 5.412037, 8.98533]"
82,Romain Loiseau;Baptiste Bouvier;Yann Teytaut;Elliot Vincent;Mathieu Aubry;Loic Landrieu,A Model You Can Hear: Audio Identification with Playable Prototypes,2022,https://doi.org/10.5281/zenodo.7316756,"Romain Loiseau+Ecole des Ponts, Univ Gustave Eiffel, CNRS>FRA>education|INRIA>FRA>facility;Baptiste Bouvier+STMS Lab, UMR 9912 (IRCAM, CNRS, Sorbonne University)>FRA>education;Yann Teytaut+STMS Lab, UMR 9912 (IRCAM, CNRS, Sorbonne University)>FRA>education;Elliot Vincent+INRIA and DIENS (ENS-PSL, CNRS, INRIA)>FRA>facility;Mathieu Aubry+Ecole des Ponts, Univ Gustave Eiffel, CNRS>FRA>education;Loic Landrieu+LASTIG, Univ Gustave Eiffel, IGN, ENSG>FRA>education","Machine learning techniques have proved useful for classifying and analyzing audio content. However, recent methods typically rely on abstract and high-dimensional representations that are difficult to interpret. Inspired by transformation-invariant approaches developed for image and 3D data, we propose an audio identification model based on learnable spectral prototypes. Equipped with dedicated transformation networks, these prototypes can be used to cluster and classify input audio samples from large collections of sounds. Our model can be trained with or without supervision and reaches state-of-the-art results for speaker and instrument identification, while remaining easily interpretable. The code is available at: https://github.com/romainloiseau/a-model-you-can-hear",FRA,education,Developed economies,"[-12.148484, -26.779469]","[-34.509617, -38.8172]","[11.496339, -14.348687, -24.43558]","[-1.5290424, -13.497447, -19.212753]","[13.1456785, 7.473514]","[7.7855334, 4.820693]","[13.91166, 13.639058, -1.3205123]","[9.912008, 7.248867, 9.3452]"
80,Mathilde Abrassart;Guillaume Doras,"And what if two musical versions don't share melody, harmony, rhythm, or lyrics ?",2022,https://doi.org/10.5281/zenodo.7316752,Mathilde Abrassart+Ircam>FRA>facility;Guillaume Doras+Ircam>FRA>facility|Sorbonne Université>FRA>education|CNRS>FRA>facility|STMS Lab>FRA>facility,"Version identification (VI) has seen substantial progress over the past few years. On the one hand, the introduction of the metric learning paradigm has favored the emergence of scalable yet accurate VI systems. On the other hand, using features focusing on specific aspects of musical pieces, such as melody, harmony, or lyrics, yielded interpretable and promising performances. In this work, we build upon these recent advances and propose a metric learning-based system systematically leveraging four dimensions commonly admitted to convey musical similarity between versions: melodic line, harmonic structure, rhythmic patterns, and lyrics. We describe our deliberately simple model architecture, and we show in particular that an approximated representation of the lyrics is an efficient proxy to discriminate between versions and non-versions. We then describe how these features complement each other and yield new state-of-the-art performances on two publicly available datasets. We finally suggest that a VI system using a combination of melodic, harmonic, rhythmic and lyrics features could theoretically reach the optimal performances obtainable on these datasets.",FRA,facility,Developed economies,"[-0.065938815, 12.818173]","[29.286379, -15.183586]","[-3.0985756, 6.2485895, 5.5689387]","[14.126967, 17.758205, -8.267529]","[12.673915, 9.745185]","[10.120235, 3.2684517]","[13.043956, 15.528893, -0.6718568]","[11.510853, 6.623409, 11.314995]"
79,Angelo Cesar Mendes Da Silva;Diego F Silva;Ricardo Marcondes Marcacini,Heterogeneous Graph Neural Network for Music Emotion Recognition,2022,https://doi.org/10.5281/zenodo.7316750,Angelo Cesar Mendes da Silva+Universidade de São Paulo>BRA>education;Diego Furtado Silva+Universidade Federal de São Carlos>BRA>education;Ricardo Marcondes Marcacini+Universidade de São Paulo>BRA>education,"Music emotion recognition has been a growing field of research motivated by the wealth of information that these labels express. Recognition of emotions highlights music's social and psychological functions, extending traditional applications such as style recognition or content similarity. Once musical data are intrinsically multi-modal, exploring this characteristic is usually beneficial. However, building a structure that incorporates different modalities in a unique space to represent the songs is challenging. Integrating information from related instances by learning heterogeneous graph-based representations has achieved state-of-the-art results in multiple tasks. This paper proposes structuring musical features over a heterogeneous network and learning a multi-modal representation using Graph Convolutional Networks with features extracted from audio and lyrics as inputs to handle the music emotion recognition tasks. We show that the proposed learning approach resulted in a representation with greater power to discriminate emotion labels. Moreover, our heterogeneous graph neural network classifier outperforms related works for music emotion recognition.",BRA,education,Developing economies,"[-58.589306, -4.7957125]","[43.71353, -12.300486]","[-26.81965, 19.556356, 10.843506]","[27.762388, 8.390164, 14.653257]","[11.740263, 9.150059]","[8.813393, 5.094442]","[13.779544, 13.277007, 0.40427342]","[9.9429035, 6.065658, 9.579737]"
78,Shuqi Dai;Huiran Yu;Roger B Dannenberg,What is missing in deep music generation? A study of repetition and structure in popular music,2022,https://doi.org/10.5281/zenodo.7316748,Shuqi Dai+Carnegie Mellon University>USA>education;Huiran Yu+Carnegie Mellon University>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education,"Structure is one of the most essential aspects of music, and music structure is commonly indicated through repetition. However, the nature of repetition and structure in music is still not well understood, especially in the context of music generation, and much remains to be explored with Music Information Retrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and American) illustrate important music construction principles: (1) structure exists at multiple hierarchical levels, (2) songs use repetition and limited vocabulary so that individual songs do not follow general statistics of song collections, (3) structure interacts with rhythm, melody, harmony, and predictability, and (4) over the course of a song, repetition is not random, but follows a general trend as revealed by cross-entropy. These and other findings offer challenges as well as opportunities for deep-learning music generation and suggest new formal music criteria and evaluation methods. Music from recent music generation systems is analyzed and compared to human-composed music in our datasets, often revealing striking differences from a structural perspective.",USA,education,Developed economies,"[19.979412, 2.4821944]","[1.4061654, 7.7702947]","[3.235838, 3.7965517, 17.796007]","[-0.82309675, 6.1099534, 4.6071]","[10.276235, 8.593298]","[8.572316, 2.5350468]","[13.348846, 12.168345, 0.27657565]","[10.02858, 6.562793, 11.399486]"
77,Dmitry Bogdanov;Xavier Lizarraga-Seijas;Pablo Alonso-Jiménez;Xavier Serra,MusAV: A dataset of relative arousal-valence annotations for validation of audio models,2022,https://doi.org/10.5281/zenodo.7316746,Dmitry Bogdanov+Universitat Pompeu Fabra>ESP>education;Xavier Lizarraga-Seijas+Universitat Pompeu Fabra>ESP>education;Pablo Alonso-Jiménez+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"We present MusAV, a new public benchmark dataset for comparative validation of arousal and valence (AV) regression models for audio-based music emotion recognition. To gather the ground truth, we rely on relative judgments instead of absolute values to simplify the manual annotation process and improve its consistency. We build MusAV by gathering comparative annotations of arousal and valence on pairs of tracks, using track audio previews and metadata from the Spotify API. The resulting dataset contains 2,092 track previews covering 1,404 genres, with pairwise relative AV judgments by 20 annotators and various subsets of the ground truth based on different levels of annotation agreement. We demonstrate the use of the dataset in an example study evaluating nine models for AV regression that we train based on state-of-the-art audio embeddings and three existing datasets of absolute AV annotations. The results on MusAV offer a view of the performance of the models complementary to the metrics obtained during training and provide insights into the impact of the considered datasets and embeddings on the generalization abilities of the models.",ESP,education,Developed economies,"[-56.741962, 1.8618958]","[47.792698, -6.435292]","[-22.645084, 22.369972, 5.9558096]","[13.66145, 23.080162, 3.266429]","[13.744949, 12.686155]","[12.79437, 4.0254765]","[16.1638, 14.667613, 1.6434401]","[13.986694, 5.2843647, 10.4323]"
76,Ilaria Manco;Emmanouil Benetos;Elio Quinton;George Fazekas,Contrastive Audio-Language Learning for Music,2022,https://doi.org/10.5281/zenodo.7316744,Ilaria Manco+Queen Mary University of London>GBR>education|Universal Music Group>GBR>company;Emmanouil Benetos+Queen Mary University of London>GBR>education|Universal Music Group>GBR>company;Elio Quinton+Universal Music Group>GBR>company;György Fazekas+Queen Mary University of London>GBR>education,"As one of the most intuitive interfaces known to humans, natural language has the potential to mediate many tasks that involve human-computer interaction, especially in application-focused fields like Music Information Retrieval. In this work, we explore cross-modal learning in an attempt to bridge audio and language in the music domain. To this end, we propose MusCALL, a framework for Music Contrastive Audio-Language Learning. Our approach consists of a dual-encoder architecture that learns the alignment between pairs of music audio and descriptive sentences, producing multimodal embeddings that can be used for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to this property, MusCALL can be transferred to virtually any task that can be cast as text-based retrieval. Our experiments show that our method performs significantly better than the baselines at retrieving audio that matches a textual description and, conversely, text that matches an audio query. We also demonstrate that the multimodal alignment capability of our model can be successfully extended to the zero-shot transfer scenario for genre classification and auto-tagging on two public datasets.",GBR,education,Developed economies,"[-11.568579, -5.376675]","[39.388245, -8.32506]","[5.9525046, 14.959646, 13.862303]","[20.93261, 16.998674, -0.42501664]","[11.365743, 8.696379]","[9.926642, 4.8109756]","[13.643259, 12.989598, -0.12625262]","[11.13842, 6.240783, 9.393695]"
75,Tengyu Deng;Eita Nakamura;Kazuyoshi Yoshii,End-to-End Lyrics Transcription Informed by Pitch and Onset Estimation,2022,https://doi.org/10.5281/zenodo.7316742,Tengyu Deng+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education|Japan Science and Technology Agency (JST)>JPN>facility,"This paper presents an automatic lyrics transcription (ALT) method for music recordings that leverages the framewise semitone-level sung pitches estimated in a multi-task learning framework. Compared to automatic speech recognition (ASR), ALT is challenging due to the insufficiency of training data and the variation and contamination of acoustic features caused by singing expressions and accompaniment sounds. The domain adaptation approach has thus recently been taken for updating an ASR model pre-trained from sufficient speech data. In the naive application of the end-to-end approach to ALT, the internal audio-to-lyrics alignment often fails due to the time-stretching nature of singing features. To stabilize the alignment, we make use of the semi-synchronous relationships between notes and characters. Specifically, a convolutional recurrent neural network (CRNN) is used for estimating the semitone-level pitches with note onset times while eliminating the intra- and inter-note pitch variations. This estimate helps an end-to-end ALT model based on connectionist temporal classification (CTC) learn correct audio-to-character alignment and mapping, where the ALT model is trained jointly with the pitch and onset estimation model. The experimental results show the usefulness of the pitch and onset information in ALT.",JPN,education,Developed economies,"[-25.606253, -32.844673]","[-28.136793, -39.76897]","[13.706026, 21.592861, -0.99679804]","[1.8687968, -5.749013, -22.783564]","[11.144898, 11.71692]","[8.025526, 4.8819027]","[12.104101, 15.659344, 1.0857733]","[10.223279, 7.0935273, 8.906485]"
74,Saurjya Sarkar;Emmanouil Benetos;Mark Sandler,EnsembleSet: a new high quality synthesised dataset for chamber ensemble separation,2022,https://doi.org/10.5281/zenodo.7316740,Saurjya Sarkar+Queen Mary University of London>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Mark Sandler+Queen Mary University of London>GBR>education,"Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articulation mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets.",GBR,education,Developed economies,"[4.1755543, -49.001804]","[-38.48957, -29.110106]","[33.506226, 1.8549879, -6.1016903]","[-12.633492, -4.519228, -27.666557]","[8.45262, 10.112023]","[6.900412, 5.74563]","[11.008306, 13.729821, 1.6576129]","[9.590907, 8.16337, 9.211642]"
73,Mojtaba Heydari;Zhiyao Duan,Singing beat tracking with Self-supervised front-end and linear transformers,2022,https://doi.org/10.5281/zenodo.7316738,Mojtaba Heydari+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction. Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features.",USA,education,Developed economies,"[30.551966, -35.730545]","[-29.20209, -15.905218]","[11.42959, -34.956894, -8.215233]","[-11.008841, 11.055787, -15.327232]","[10.312116, 4.3280783]","[5.0487866, 2.517336]","[10.082865, 12.957451, -2.0376782]","[7.883387, 6.999599, 10.179139]"
72,Franco Caspe;Andrew Mcpherson;Mark Sandler,DDX7: Differentiable FM Synthesis of Musical Instrument Sounds,2022,https://doi.org/10.5281/zenodo.7343063,"Franco Caspe+Centre for Digital Music, Queen Mary University of London>GBR>education;Andrew McPherson+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source. On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.",GBR,education,Developed economies,"[14.77465, -33.255577]","[-20.912584, -49.5392]","[27.221369, 0.47513384, 9.31963]","[-23.29881, -5.287809, -19.851921]","[9.452837, 8.558029]","[8.065848, 6.7040086]","[12.366763, 12.296881, 0.71671367]","[9.547874, 6.030559, 7.9925976]"
84,Weixing Wei;Peilin Li;Yi Yu;Wei Li,HPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano Transcription,2022,https://doi.org/10.5281/zenodo.7316762,Weixing Wei+Fudan University>CHN>education|Shanghai Key Laboratory of Intelligent Information Processing>CHN>facility;Peilin Li+Fudan University>CHN>education|Shanghai Key Laboratory of Intelligent Information Processing>CHN>facility;Yi Yu+National Institute of Informatics (NII)>JPN>facility;Wei Li+Fudan University>CHN>education|Shanghai Key Laboratory of Intelligent Information Processing>CHN>facility,"While neural network models are making significant progress in piano transcription, they are becoming more resource-consuming due to requiring larger model size and more computing power. In this paper, we attempt to apply more prior about piano to reduce model size and improve the transcription performance. The sound of a piano note contains various overtones, and the pitch of a key does not change over time. To make full use of such latent information, we propose HPPNet that using the Harmonic Dilated Convolution to capture the harmonic structures and the Frequency Grouped Recurrent Neural Network to model the pitch-invariance over time. Experimental results on the MAESTRO dataset show that our piano transcription system achieves state-of-the-art performance both in frame and note scores (frame F1 93.15%, note F1 97.18%). Moreover, the model size is much smaller than the previous state-of-the-art deep learning models.",CHN,education,Developing economies,"[31.164421, -6.601494]","[-29.780249, -31.641039]","[12.8738165, -4.3178997, 16.232395]","[-6.1301017, -9.949914, -15.071135]","[9.547668, 7.3950334]","[7.8386436, 5.340204]","[11.964185, 11.494844, -0.087318994]","[9.236753, 6.8286896, 8.887236]"
71,Curtis Hawthorne;Ian Simon;Adam Roberts;Neil Zeghidour;Joshua Gardner;Ethan Manilow;Jesse Engel,Multi-instrument Music Synthesis with Spectrogram Diffusion,2022,https://doi.org/10.5281/zenodo.7316734,Curtis Hawthorne+Google Research>USA>company;Ian Simon+Google Research>USA>company;Adam Roberts+Google Research>USA>company;Neil Zeghidour+Google Research>USA>company;Josh Gardner+Google Research>USA>company;Ethan Manilow+Google Research>USA>company;Jesse Engel+Google Research>USA>company,"An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fréchet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.",USA,company,Developed economies,"[13.60152, -30.876024]","[-20.412851, -47.63199]","[25.295475, -0.45876193, 3.9226944]","[-21.518925, -3.6632092, -19.523798]","[9.356728, 8.539521]","[8.120979, 6.6649847]","[12.188872, 12.450399, 0.6750998]","[9.589009, 5.979574, 8.018092]"
69,Michael Zhou;Andrew Mcgraw;Douglas R Turnbull,Towards Quantifying the Strength of Music Scenes Using Live Event Data,2022,https://doi.org/10.5281/zenodo.7316730,Michael Zhou+Columbia University>USA>education;Andrew McGraw+University of Richmond>USA>education;Douglas R. Turnbull+Ithaca College>USA>education,"There are many benefits for a community when there is a vibrant local music scene (e.g., increased mental &amp; physical well-being, increased economic activity) and there are many factors that contribute to an environment in which a live music scene can thrive (e.g., available performance spaces, helpful government policies). In this paper, we explore using an estimate of the live music event rate (LMER) as a rough indicator to measure the strength of a local music scene. We define LMER as the number of music shows per 100,000 people per year and then explore how this indicator is (or is not) correlated with 28 other socioeconomic indicators. To do this, we analyze a set of 308,051 music events from 2019 across 1,139 cities in the United States. Our findings reveal that factors related to transportation (e.g., high walkability), population (high density), economics (high employment rate), age (high proportion of individuals age 20-29), and education (bachelor's degree or higher) are strongly correlated with having a high number of live music events. Conversely, we did not find statistically significant evidence that other indica- tors (e.g., racial diversity) are correlated.",USA,education,Developed economies,"[-15.037517, 8.120654]","[46.60221, 13.538552]","[-7.850729, -7.247568, -9.678285]","[13.977887, 13.977502, 13.993685]","[12.852958, 7.4147224]","[12.879708, 2.0844038]","[13.589725, 13.624938, -1.3614075]","[13.586683, 4.846458, 11.962567]"
68,Chang-Bin Jeon;Kyogu Lee,Towards robust music source separation on loud commercial music,2022,https://doi.org/10.5281/zenodo.7342777,Chang-Bin Jeon+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"Nowadays, commercial music has extreme loudness and heavily compressed dynamic range compared to the past. Yet, in music source separation, these characteristics have not been thoroughly considered, resulting in the domain mismatch between the laboratory and the real world. In this paper, we confirmed that this domain mismatch negatively affect the performance of the music source separation networks. To this end, we first created the out-of-domain evaluation datasets, musdb-L and XL, by mimicking the music mastering process. Then, we quantitatively verify that the performance of the state-of-the-art algorithms significantly deteriorated in our datasets. Lastly, we proposed LimitAug data augmentation method to reduce the domain mismatch, which utilizes an online limiter during the training data sampling process. We confirmed that it not only alleviates the performance degradation on our out-of-domain datasets, but also results in higher performance on in-domain data.",KOR,education,Developing economies,"[7.571342, -45.596188]","[-38.245335, -30.067219]","[29.137157, -2.8271215, -3.3693602]","[-11.864461, -3.598551, -25.916325]","[8.344784, 9.996835]","[6.8953514, 5.7403765]","[11.015224, 13.632609, 1.6642929]","[9.635214, 8.260739, 9.107993]"
67,Peiling Lu;Xu Tan;Botao Yu;Tao Qin;Sheng Zhao;Tie-Yan Liu,MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks,2022,https://doi.org/10.5281/zenodo.7342745,Peiling Lu+Microsoft Research Asia>CHN>facility;Xu Tan+Microsoft Research Asia>CHN>facility;Botao Yu+Nanjing University>CHN>education;Tao Qin+Microsoft Research Asia>CHN>facility;Sheng Zhao+Microsoft Azure Speech>CHN>facility;Tie-Yan Liu+Microsoft Research Asia>CHN>facility,"Human usually composes music by organizing elements according to the musical form to express music ideas. However, for neural network-based music generation, it is difficult to do so due to the lack of labelled data on musical form. In this paper, we develop MeloForm, a system that generates melody with musical form using expert systems and neural networks. Specifically, 1) we design an expert system to generate a melody by developing musical elements from motifs to phrases then to sections with repetitions and variations according to pre-given musical form; 2) considering the generated melody is lack of musical richness, we design a Transformer based refinement model to improve the melody without changing its musical form. MeloForm enjoys the advantages of precise musical form control by expert systems and musical richness learning via neural models. Both subjective and objective experimental evaluations demonstrate that MeloForm generates melodies with precise musical form control with 97.79% accuracy, and outperforms baseline systems in terms of subjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure, thematic, richness and overall quality, without any labelled musical form data. Besides, MeloForm can support various kinds of forms, such as verse and chorus form, rondo form, variational form, sonata form, etc.",CHN,facility,Developing economies,"[10.564766, -9.53951]","[-6.11234, -38.421703]","[13.700562, 8.082844, 4.607594]","[-20.870907, 3.306581, -7.2740784]","[10.325228, 9.459611]","[9.435771, 6.0482388]","[11.300997, 14.718982, -0.8061906]","[9.819385, 5.357059, 9.233386]"
66,Qingqing Huang;Aren Jansen;Joonseok Lee;Ravi Ganti;Judith Yue Li;Daniel P W Ellis,MuLan: A Joint Embedding of Music Audio and Natural Language,2022,https://doi.org/10.5281/zenodo.7316724,Qingqing Huang+Google Research>KOR>company;Aren Jansen+Google Research>KOR>company;Joonseok Lee+Google Research>KOR>company|Seoul National University>KOR>education;Ravi Ganti+Google Research>KOR>company;Judith Yue Li+Google Research>KOR>company;Daniel P. W. Ellis+Google Research>KOR>company,"Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.",KOR,company,Developing economies,"[-18.56831, -3.5152972]","[38.79963, -7.828818]","[-0.17912264, 14.059912, 13.581938]","[21.352827, 15.756438, -0.47101176]","[11.931712, 9.140524]","[10.208478, 4.640218]","[13.826799, 13.308544, 0.17877746]","[11.32503, 6.186808, 9.539724]"
65,Jiafeng Liu;Yuanliang Dong;Zehua Cheng;Xinran Zhang;Xiaobing Li;Feng Yu;Maosong Sun,Symphony Generation with Permutation Invariant Language Model,2022,https://doi.org/10.5281/zenodo.7316722,Jiafeng Liu+Central Conservatory of Music>CHN>education;Yuanliang Dong+Central Conservatory of Music>CHN>education;Zehua Cheng+University of Oxford>GBR>education;Xinran Zhang+Central Conservatory of Music>CHN>education;Xiaobing Li+Central Conservatory of Music>CHN>education;Feng Yu+Central Conservatory of Music>CHN>education;Maosong Sun+Tsinghua University>CHN>education,"In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation.",CHN,education,Developing economies,"[19.941517, 4.479087]","[-13.164435, -37.96251]","[4.8911934, -1.9706535, 18.913164]","[-16.070936, -1.1007541, -8.891688]","[10.195056, 8.549375]","[8.640143, 5.9672275]","[13.199298, 11.889691, 0.08525582]","[9.308528, 5.870067, 9.041326]"
63,Jaidev Shriram;Makarand Tapaswi;Vinoo Alluri,Sonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations,2022,https://doi.org/10.5281/zenodo.7372149,"Jaidev Shriram+International Institute of Information Technology, Hyderabad>IND>education;Makarand Tapaswi+International Institute of Information Technology, Hyderabad>IND>education;Vinoo Alluri+International Institute of Information Technology, Hyderabad>IND>education","Reading, much like music listening, is an immersive experience that transports readers while taking them on an emotional journey. Listening to complementary music has the potential to amplify the reading experience, especially when the music is stylistically cohesive and emotionally relevant. In this paper, we propose the first fully automatic method to build a dense soundtrack for books, which can play high-quality instrumental music for the entirety of the reading duration. Our work employs a unique text processing and music weaving pipeline that determines the context and emotional composition of scenes in a chapter. This allows our method to identify and play relevant excerpts from the soundtrack of the book's movie adaptation. By relying on the movie composer's craftsmanship, our book soundtracks include expert-made motifs and other scene-specific musical characteristics. We validate the design decisions of our approach through a perceptual study. Our readers note that the book soundtrack greatly enhanced their reading experience, due to high immersiveness granted via uninterrupted and style-consistent music, and a heightened emotional state attained via high precision emotion and scene context recognition.",IND,education,Developing economies,"[14.817581, 9.874142]","[54.22778, -12.991848]","[4.9148135, -2.294251, -10.463409]","[5.897366, 20.444027, 1.8979405]","[11.798187, 7.5370827]","[12.803675, 4.454623]","[13.023508, 12.943371, -1.4404669]","[13.874586, 4.9378996, 9.992543]"
62,Nikita Srivatsan;Taylor Berg-Kirkpatrick,Checklist Models for Improved Output Fluency in Piano Fingering Prediction,2022,https://doi.org/10.5281/zenodo.7316716,Nikita Srivatsan+Carnegie Mellon University>USA>education;Taylor Berg-Kirkpatrick+UC San Diego>USA>education,"In this work we present a new approach for the task of predicting fingerings for piano music. While prior neural approaches have often treated this as a sequence tagging problem with independent predictions, we put forward a checklist system, trained via reinforcement learning, that maintains a representation of recent predictions in addition to a hidden state, allowing it to learn soft constraints on output structure. We also demonstrate that by modifying input representations --- which in prior work using neural models have often taken the form of one-hot encodings over individual keys on the piano --- to encode relative position on the keyboard to the prior note instead, we can achieve much better performance. Additionally, we reassess the use of raw per-note labeling precision as an evaluation metric, noting that it does not adequately measure the fluency, i.e. human playability, of a model's output. To this end, we compare methods across several statistics which track the frequency of adjacent finger predictions that while independently reasonable would be physically challenging to perform in sequence, and implement a reinforcement learning strategy to minimize these as part of our training loss. Finally through human expert evaluation, we demonstrate significant gains in performability directly attributable to improvements with respect to these metrics.",USA,education,Developed economies,"[29.85529, -2.6169903]","[-25.79723, -23.981943]","[19.411795, -1.6490928, 18.489626]","[-10.350855, -10.048846, -8.195367]","[9.8551, 7.152157]","[7.679306, 4.380714]","[12.076178, 11.532793, -0.39770162]","[9.130612, 6.2631655, 9.845797]"
61,Nazif Can Tamer;Pedro Ramoneda;Xavier Serra,Violin Etudes: A Comprehensive Dataset for f0 Estimation and Performance Analysis,2022,https://doi.org/10.5281/zenodo.7316714,"Nazif Can Tamer+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Pedro Ramoneda+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Violin performance analysis requires accurate and robust f0 estimates to give feedback on the playing accuracy. Despite the recent advancements in data-driven f0 estimators, their application to performance analysis remains a challenge due to style-specific and dataset-induced biases. In this paper, we address this problem by introducing Violin Etudes, a 27.8-hours violin performance dataset constructed with domain knowledge in instrument pedagogy and a novel automatic f0-labeling paradigm. Experimental results on unseen datasets show that the CREPE f0 estimator trained on Violin Etudes outperforms the widely-used pre-trained version trained on multiple manually-labeled datasets. Further preliminary findings suggest that (i) existing data-driven f0 estimators may overfit to equal temperament, and (ii) iterative re-labeling regularized by our novel Constrained Harmonic Resynthesis method can simultaneously enhance datasets and f0 estimators. Our dataset curation methodology is easily scalable to other instruments owing to the quantity of pedagogical data online. It also supports a range of MIR research directions thanks to the performance difficulty labels from educational institutions.",ESP,education,Developed economies,"[15.114617, -20.020988]","[-38.75776, -5.2628937]","[7.2702184, -14.420153, -4.127655]","[-11.485243, -7.232776, -5.636577]","[9.505595, 6.7505536]","[7.8141003, 4.2393823]","[11.318791, 12.38106, -0.48662975]","[9.311606, 6.380274, 9.991285]"
60,Louis Couturier;Louis Bigo;Florence Leve,A Dataset of Symbolic Texture Annotations in Mozart Piano Sonatas,2022,https://doi.org/10.5281/zenodo.7316712,Louis Couturier+Université de Picardie Jules Verne>FRA>education;Louis Bigo+Université de Lille>FRA>education;Florence Levé+Université de Picardie Jules Verne>FRA>education|Université de Lille>FRA>education,"Musical scores are generally analyzed under different aspects, notably melody, harmony, rhythm, but also through their texture, although this last concept is arguably more delicate to formalize. Symbolic texture depicts how sounding components are organized in the score. It outlines the density of elements, their heterogeneity, role and interactions. In this paper, we release a set of manual annotations for each bar of 9 movements among early piano sonatas by W. A. Mozart, totaling 1164 labels that follow a syntax dedicated to piano score texture. A quantitative analysis of the annotations highlights some characteristic textural features in the corpus. In addition, we present and release the implementation of low-level descriptors of symbolic texture. These descriptors can be correlated with texture annotations and used in different machine-learning tasks. Along with provided data, they offer promising applications in computer assisted music analysis and composition.",FRA,education,Developed economies,"[17.003952, 18.397749]","[-15.015185, 17.847452]","[-7.4029346, -7.4301386, 19.122429]","[-13.163343, -5.987195, 4.9021673]","[11.870387, 7.1666765]","[8.288053, 2.1940389]","[13.440218, 12.381363, -1.0839084]","[10.021504, 6.8139586, 11.838617]"
59,Martha E Thomae Elias;Julie Cumming;Ichiro Fujinaga,Counterpoint Error-Detection Tools for Optical Music Recognition of Renaissance Polyphonic Music,2022,https://doi.org/10.5281/zenodo.7316710,Martha E. Thomae+McGill University>CAN>education;Julie E. Cumming+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper discusses part of a larger project to preserve and increase access to Guatemalan music sources written in mensural notation by using a digitization and music information retrieval (MIR) workflow to obtain both digital images and symbolic scores with editorial corrections. The workflow involves MIR tools such as optical music recognition (OMR), automatic voice alignment for mensural notation, editorial correction software, and computational counterpoint error detection. In this paper, we evaluate whether the use of automatic counterpoint error-detection tools makes the correction process more efficient. The results confirm that marking illegal dissonances in the score following the rules of Renaissance counterpoint indeed makes the process of editorial correction of scribal errors in Renaissance music more efficient by reducing the time taken and improving the accuracy of such corrections. Moreover, marking the illegal dissonances in the score also allowed us to catch OMR errors that had passed through undetected at a previous stage of the workflow.",CAN,education,Developed economies,"[37.478527, 20.15225]","[-17.176945, 36.35702]","[18.927584, 9.136218, 10.299549]","[-8.006263, -19.097284, 5.116346]","[8.690539, 6.2948213]","[6.7367005, -0.4496034]","[10.7371235, 11.2821865, -0.089631364]","[8.090175, 4.2102184, 10.719771]"
58,Yigitcan Özer;Meinard Müller,Source Separation of Piano Concertos with Test-Time Adaptation,2022,https://doi.org/10.5281/zenodo.7316708,Yigitcan Özer+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Music source separation (MSS) aims at decomposing a music recording into constituent sources, such as a lead instrument and the accompaniment. Despite the difficulties in MSS due to the high correlation of musical sources in time and frequency, deep neural networks (DNNs) have led to substantial improvements to accomplish this task. For training supervised machine learning models such as DNNs, isolated sources are required. In the case of popular music, one can exploit open-source datasets which involve multitrack recordings of vocals, bass, and drums. For western classical music, however, isolated sources are generally not available. In this article, we consider the case of piano concertos, which are composed for a pianist typically accompanied by an orchestra. The lack of multitrack recordings makes training supervised machine learning models for the separation of piano and orchestra challenging. To overcome this problem, we generate artificial training material by randomly mixing sections of the solo piano repertoire (e.g., piano sonatas) and orchestral pieces without piano (e.g., symphonies) to train state-of-the-art DNN models for MSS. As our main contribution, we propose a test-time adaptation (TTA) procedure, which exploits random mixtures of the piano-only and orchestra-only parts in the test data to further improve the separation quality.",DEU,facility,Developed economies,"[4.08611, -45.94104]","[-34.644394, -29.766691]","[29.924034, 2.5192397, 0.49110985]","[-11.704484, -8.170565, -21.527582]","[8.323557, 10.159684]","[7.164343, 5.689354]","[10.949888, 13.640084, 1.7138348]","[9.518316, 7.7825184, 8.98261]"
70,Morgan Buisson;Brian Mcfee;Slim Essid;Hélène C.  Crayencour Crayencour,Learning Multi-Level Representations for Hierarchical Music Structure Analysis.,2022,https://doi.org/10.5281/zenodo.7343060,"Morgan Buisson+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Brian McFee+New York University>USA>education|New York University>USA>education;Slim Essid+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Hélène C. Crayencour+CNRS-Univ. Paris-Sud-CentraleSupélec>FRA>education","Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, we explore unsupervised learning of such representations using a contrastive approach operating at different time-scales. We evaluate the proposed system on flat and multi-level music segmentation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity.",FRA,education,Developed economies,"[-0.49881533, 3.052249]","[-4.4411144, -26.065573]","[-1.4518648, -2.69609, 4.6211786]","[-2.1154597, -0.029403487, -15.294498]","[11.914969, 8.578432]","[8.806184, 4.2598176]","[13.103997, 13.592261, -0.3314856]","[10.368534, 7.0750036, 10.612385]"
57,Chris Donahue;John Thickstun;Percy Liang,Melody transcription via generative pre-training,2022,https://doi.org/10.5281/zenodo.7316706,Chris Donahue+Stanford University>USA>education|Stanford University>USA>education|Stanford University>USA>education;John Thickstun+Stanford University>USA>education|Stanford University>USA>education|Stanford University>USA>education;Percy Liang+Stanford University>USA>education|Stanford University>USA>education|Stanford University>USA>education,"Despite the central role that melody plays in music perception, it remains an open challenge in MIR to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in *melody transcription* is building methods which can handle broad audio containing any number of instrument ensembles and musical styles---existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data---we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build a system capable of transcribing human-readable lead sheets directly from music audio.",USA,education,Developed economies,"[8.991304, -10.921261]","[-16.30837, -35.01905]","[14.834085, 5.0385175, 2.6952887]","[-13.962128, -6.524641, -10.39889]","[10.076842, 9.61108]","[8.839746, 5.3980813]","[11.199073, 14.674174, -0.6158943]","[9.872129, 6.079841, 9.490229]"
85,Pedro L T Neves;José Fornari;João B Florindo,Generating music with sentiment using Transformer-GANs,2022,https://doi.org/10.5281/zenodo.7342704,Pedro L. T. Neves+State University of Campinas>BRA>education;Jose Fornari+State University of Campinas>BRA>education;João B. Florindo+State University of Campinas>BRA>education,"The field of Automatic Music Generation has seen significant progress thanks to the advent of Deep Learning. However, most of these results have been produced by unconditional models, which lack the ability to interact with their users, not allowing them to guide the generative process in meaningful and practical ways. Moreover, synthesizing music that remains coherent across longer timescales while still capturing the local aspects that make it sound ``realistic'' or human-like is still challenging. This is due to the large computational requirements needed to work with long sequences of data, and also to limitations imposed by the training schemes that are often employed. In this paper, we propose a generative model of symbolic music conditioned by data retrieved from human sentiment. The model is a Transformer-GAN trained with labels that correspond to different configurations of the valence and arousal dimensions that quantitatively represent human affective states. We try to tackle both of the problems above by employing an efficient linear version of Attention and using a Discriminator both as a tool to improve the overall quality of the generated music and its ability to follow the conditioning signals.",BRA,education,Developing economies,"[-43.186813, -12.545121]","[-10.430414, -39.63458]","[-0.3567832, 8.58833, 28.903767]","[-23.064932, 3.6023383, -13.897519]","[10.261521, 8.764454]","[8.899257, 6.617982]","[13.388773, 11.942287, 0.34787083]","[9.6271305, 5.4713674, 8.510655]"
87,Kyungyun Lee;Gladys Hitt;Emily Terada;Jin Ha Lee,Ethics of Singing Voice Synthesis: Perceptions of Users and Developers,2022,https://doi.org/10.5281/zenodo.7316768,Kyungyun Lee+Gaudio Lab>KOR>facility;Gladys Hitt+University of Washington>USA>education;Emily Terada+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"Singing Voice Synthesis (SVS) has recently garnered much attention as its quality has improved vastly with the use of artificial intelligence (AI), creating many opportunities for supporting music creators and listeners. Recently, there have been growing concerns about ethical issues related to AI development in general, and to AI-based SVS development specifically. Many questions remain unexplored about how to ethically develop and use such technology. In this paper, we investigate the perception of ethical issues related to SVS from the perspectives of two different groups: the general public and developers. We collected 3,075 user comments from YouTube videos showcasing various uses of SVS as part of a mainstream variety show. Additionally, we interviewed six researchers developing SVS technology. Through thematic analysis, we identify and discuss three different aspects related to ethical issues in SVS development, highlighting the similarities and differences between the perspectives of the general public and developers: (1) Use scenarios, (2) Attitudes towards development, and (3) Meaning of Creativity, and (4) Concerns about human rights, intellectual property (IP) and legal issues.",KOR,facility,Developing economies,"[-24.62507, 2.1989195]","[-5.779465, 45.07766]","[6.8100395, 11.701549, -10.52168]","[-19.522783, 11.23006, 5.2011724]","[10.446095, 11.083869]","[10.022462, 5.681338]","[11.797969, 15.489982, 0.42827386]","[10.443629, 5.0649557, 9.657991]"
112,Maximilian Damböck;Richard Vogl;Peter Knees,On the Impact and Interplay of Input Representations and Network Architectures for Automatic Music Tagging,2022,https://doi.org/10.5281/zenodo.7343091,Maximilian Damböck+TU Wien>AUT>education;Richard Vogl+TU Wien>AUT>education;Peter Knees+TU Wien>AUT>education|Georgia Tech>USA>education,"Automatic music tagging systems have once more gained relevance over the last years, not least through their use in applications such as music recommender systems. State-of-the-art systems are based on a variant of convolutional neural networks (CNNs) and use some type of time-frequency audio representation as input, in a fitting combination to predict semantic tags available through expert or crowd-based annotation. In this work we systematically compare five widely used audio input representations (STFT, CQT, Mel spectrograms, MFCCs, and raw audio waveform) using five established convolutional neural network architectures (MusicCNN, VGG16, ResNet, a Squeeze and Excitation Network (SeNet), as well as a newly proposed MusicCNN variant using dilated convolutions) for the task of music tag prediction. Performance of all factor combinations are measured on two distinct tagging datasets, namely MagnaTagATune and MTG Jamendo. A two-way ANOVA shows that both input representation and model architecture significantly impact the classification results. Despite differently sized input representations and practical impact on model training, we find that using STFT as input representations provides the best results overall and on specific tag categories (genre, instrument, mood), while other representations show less consistent behavior in these regards. Furthermore, the proposed dilated convolutional architecture shows significant performance improvements for all input representations except raw waveform.",AUT,education,Developed economies,"[-41.227863, -2.9642084]","[-22.540571, -39.259186]","[-12.781644, 16.602198, 11.077907]","[-6.9296823, 1.0134934, -22.626308]","[14.595263, 10.691478]","[9.658767, 4.786238]","[15.644525, 14.082294, 0.12452682]","[10.851378, 6.5137963, 8.860078]"
111,Yang Qu;Yutian Qin;Lecheng Chao;Hangkai Qian;Ziyu Wang;Gus Xia,Modeling perceptual loudness of piano tone: theory and applications,2022,https://doi.org/10.5281/zenodo.7343094,"Yang Qu+Music X Lab, NYU Shanghai>CHN>education|City University of Hong Kong>HKG>education;Yutian Qin+Music X Lab, NYU Shanghai>CHN>education|New York University>USA>education;Lecheng Chao+Music X Lab, NYU Shanghai>CHN>education;Hangkai Qian+Music X Lab, NYU Shanghai>CHN>education;Ziyu Wang+Music X Lab, NYU Shanghai>CHN>education|Mohamed Bin Zayed University of Artificial Intelligence>ARE>education;Gus Xia+Music X Lab, NYU Shanghai>CHN>education|Mohamed Bin Zayed University of Artificial Intelligence>ARE>education","The relationship between perceptual loudness and physical attributes of sound is an important subject in both computer music and psychoacoustics. Early studies of ""equal-loudness contour"" can trace back to the 1920s and the measured loudness with respect to intensity and frequency has been revised many times since then. However, most studies merely focus on synthesized sound, and the induced theories on natural tones with complex timbre has rarely been justified. To this end, we investigate both theory and applications of natural-tone loudness perception in this paper via modeling piano tone. The theory part contains: 1) an accurate measurement of piano-tone equal-loudness contour of pitches, and 2) a machine-learning model capable of inferring loudness purely based on spectral features trained on human subject measurements. As for the application, we apply our theory to piano control transfer, in which we adjust the MIDI velocities on two different player pianos (in different acoustic environments) to achieve the same perceptual effect. Experiments show that both of our theoretical loudness modeling and the corresponding performance control transfer algorithm significantly outperform their baselines.",CHN,education,Developing economies,"[34.429676, -3.482823]","[-29.851097, -22.783876]","[11.998481, -11.869285, 17.293673]","[-7.4425516, -13.245603, -11.379159]","[10.21173, 7.4497013]","[7.917383, 5.763259]","[12.1460285, 12.100862, -0.3032239]","[9.000774, 6.4360614, 8.790662]"
110,Jingwei Zhao;Gus Xia;Ye Wang,Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation,2022,https://doi.org/10.5281/zenodo.7316816,Jingwei Zhao+National University of Singapore>SGP>education|New York University Shanghai>CHN>education;Gus Xia+New York University Shanghai>CHN>education;Ye Wang+National University of Singapore>SGP>education,"The variational auto-encoder has become a leading framework for symbolic music generation, and a popular research direction is to study how to effectively control the generation process. A straightforward way is to control a model using different conditions during inference. However, in music practice, conditions are usually sequential (rather than simple categorical labels), involving rich information that overlaps with the learned representation. Consequently, the decoder gets confused about whether to listen to the latent representation or the condition, and sometimes just ignores the condition. To solve this problem, we leverage domain adversarial training to disentangle the representation from condition cues for better control. Specifically, we propose a condition corruption objective that uses the representation to denoise a corrupted condition. Minimized by a discriminator and maximized by the VAE encoder, this objective adversarially induces a condition-invariant representation. In this paper, we focus on the task of melody harmonization to illustrate our idea, while our methodology can be generalized to other controllable generative tasks. Demos and experiments show that our methodology facilitates not only condition-invariant representation learning but also higher-quality controllability compared to baselines.",SGP,education,Developing economies,"[24.987293, 3.5169227]","[-11.99849, -45.53973]","[7.231367, 3.5092025, 24.039461]","[-21.431953, 2.5648613, -16.293732]","[10.034097, 8.404206]","[8.762316, 6.637735]","[13.141685, 11.789592, 0.41217405]","[9.622665, 5.596064, 8.381271]"
109,Emmanouil Karystinaios;Gerhard Widmer,Cadence Detection in Symbolic Classical Music using Graph Neural Networks.,2022,https://doi.org/10.5281/zenodo.7316814,"Emmanouil Karystinaios+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Cadences are complex structures that have been driving music from the beginning of contrapuntal polyphony until today. Detecting such structures is vital for numerous MIR tasks such as musicological analysis, key detection, music segmentation, and others. However, automatic cadence detection remains a challenging task mainly because it involves a combination of high-level musical elements like harmony, voice leading, and rhythm. In this work, we present a graph representation of symbolic scores as an intermediate means to solve the cadence detection task. We approach cadence detection as an imbalanced node classification problem using a Graph Convolutional Network. We obtain results that are at least on par with the state of the art, and we present a model capable of making predictions at multiple levels of granularity, from individual notes to beats, thanks to the fine-grained, note-by-note representation. Moreover, our experiments suggest that graph convolution is able to learn non-local features that assist in cadence detection, freeing us from the need of having to devise specialized features that encode non-local context. We argue that this general approach to modeling musical scores and classification tasks has a number of potential advantages, beyond the specific recognition task presented here.",AUT,education,Developed economies,"[13.27602, 3.1383781]","[-14.979963, -28.468195]","[-4.3559656, -19.81611, 15.658829]","[-19.096882, -12.882063, -13.026389]","[11.585799, 6.6989846]","[8.772494, 5.184185]","[13.251603, 12.527503, -1.0566157]","[9.91865, 6.1410956, 9.560051]"
108,Guillem Cortès;Alex Ciurana;Emilio Molina;Marius Miron;Owen Meyers;Joren Six;Xavier Serra,BAF: An audio fingerprinting dataset for broadcast monitoring,2022,https://doi.org/10.5281/zenodo.7372162,"Guillem Cortès+BMAT Licensing S.L.>ESP>company|MTG, Universitat Pompeu Fabra>ESP>education|Epidemic Sound>SWE>company|IPEM, Ghent University>BEL>education;Alex Ciurana+BMAT Licensing S.L.>ESP>company|MTG, Universitat Pompeu Fabra>ESP>education|Epidemic Sound>SWE>company|IPEM, Ghent University>BEL>education;Emilio Molina+BMAT Licensing S.L.>ESP>company|MTG, Universitat Pompeu Fabra>ESP>education|Epidemic Sound>SWE>company|IPEM, Ghent University>BEL>education;Marius Miron+BMAT Licensing S.L.>ESP>company|MTG, Universitat Pompeu Fabra>ESP>education|Epidemic Sound>SWE>company|IPEM, Ghent University>BEL>education;Owen Meyers+BMAT Licensing S.L.>ESP>company|MTG, Universitat Pompeu Fabra>ESP>education|Epidemic Sound>SWE>company|IPEM, Ghent University>BEL>education;Joren Six+BMAT Licensing S.L.>ESP>company|MTG, Universitat Pompeu Fabra>ESP>education|Epidemic Sound>SWE>company|IPEM, Ghent University>BEL>education;Xavier Serra+BMAT Licensing S.L.>ESP>company|MTG, Universitat Pompeu Fabra>ESP>education|Epidemic Sound>SWE>company|IPEM, Ghent University>BEL>education","Audio Fingerprinting (AFP) is a well-studied problem in music information retrieval for various use-cases e.g. content-based copy detection, DJ-set monitoring, and music excerpt identification. However, AFP for continuous broadcast monitoring (e.g. for TV &amp; Radio), where music is often in the background, has not received much attention despite its importance to the music industry. In this paper (1) we present BAF, the first public dataset for music monitoring in broadcast. It contains 74 hours of production music from Epidemic Sound and 57 hours of TV audio recordings. Furthermore, BAF provides cross-annotations with exact matching timestamps between Epidemic tracks and TV recordings. Approximately, 80% of the total annotated time is background music. (2) We benchmark BAF with public state-of-the-art AFP systems, together with our proposed baseline PeakFP: a simple, non-scalable AFP algorithm based on spectral peak matching. In this benchmark, none of the algorithms obtain a F1-score above 47%, pointing out that further research is needed to reach the AFP performance levels in other studied use cases. The dataset, baseline, and benchmark framework are open and available for research.",ESP,company,Developed economies,"[-17.071749, -26.32195]","[25.090786, -20.316654]","[5.6454372, -13.314234, -25.636528]","[18.70579, -10.681186, 3.7902763]","[9.092327, 4.44064]","[8.523904, 0.049893063]","[10.595382, 11.536338, -1.9704114]","[10.271224, 5.293578, 12.767223]"
106,Longshen Ou;Xiangming Gu;Ye Wang,Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription,2022,https://doi.org/10.5281/zenodo.7342796,Longshen Ou+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Xiangming Gu+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Ye Wang+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Automatic speech recognition (ASR) has progressed significantly in recent years due to the emergence of large-scale datasets and the self-supervised learning (SSL) paradigm. However, as its counterpart problem in the singing domain, the development of automatic lyric transcription (ALT) suffers from limited data and degraded intelligibility of sung lyrics. To fill in the performance gap between ALT and ASR, we attempt to exploit the similarities between speech and singing. In this work, we propose a transfer-learning-based ALT solution that takes advantage of these similarities by adapting wav2vec 2.0, an SSL ASR model, to the singing domain. We maximize the effectiveness of transfer learning by exploring the influence of different transfer starting points. We further enhance the performance by extending the original CTC model to a hybrid CTC/attention model. Our method surpasses previous approaches by a large margin on various ALT benchmark datasets. Further experiments show that, with even a tiny proportion of training data, our method still achieves competitive performance.",SGP,education,Developing economies,"[-24.672503, -33.71828]","[-28.144995, -40.24546]","[16.000053, 20.131865, -1.2669812]","[2.5638368, -6.7319007, -23.123394]","[11.09851, 11.713261]","[8.036929, 4.7995005]","[12.126153, 15.717341, 1.1840189]","[10.37931, 7.157886, 8.955467]"
105,Maximilian Mayerl;Stefan Brandl;Günther Specht;Markus Schedl;Eva Zangerle,Verse versus Chorus: Structure-aware Feature Extraction for Lyrics-based Genre Recognition,2022,https://doi.org/10.5281/zenodo.7316806,"Maximilian Mayerl+Leopold-Franzens-Universität Innsbruck>AUT>education;Stefan Brandl+Institute of Computational Perception, Johannes Kepler Universität Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Günther Specht+Leopold-Franzens-Universität Innsbruck>AUT>education;Markus Schedl+Institute of Computational Perception, Johannes Kepler Universität Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Eva Zangerle+Leopold-Franzens-Universität Innsbruck>AUT>education","The aim of lyrics-based genre recognition is to automatically determine the genre of a given song based on its lyrics. Previous approaches for this task have commonly used textual features extracted from the entirety of a song's lyrics, neglecting the inherent structure of lyrics consisting of, for instance, verses and choruses. Therefore, we pose the hypothesis that features extracted from different parts of the lyrics can have significantly different predictive power. To test this hypothesis, we perform a series of experiments to determine whether models trained on features taken from verses and choruses perform differently for genre recognition. Our experiments indeed confirm our hypothesis, showing that generally, using features extracted from verses leads to higher performance than features extracted from choruses. Digging deeper, we found that this is especially true for pop and rap songs. Rock songs show the opposite effect, with features extracted from choruses performing better than those taken from verses.",AUT,education,Developed economies,"[-28.395117, -29.886633]","[24.256462, -4.7690654]","[11.496947, 22.139881, -5.962652]","[7.0441117, 11.105709, 1.0988683]","[11.463223, 11.727694]","[10.205679, 3.1254349]","[12.392334, 15.901744, 1.0403941]","[11.9197645, 6.729545, 11.200009]"
104,Francesco Foscarin;Katharina Hoedt;Verena Praher;Arthur Flexer;Gerhard Widmer,"Concept-Based Techniques for ""Musicologist-Friendly"" Explanations in Deep Music Classifiers",2022,https://doi.org/10.5281/zenodo.7316804,"Francesco Foscarin+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Katharina Hoedt+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Verena Praher+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Arthur Flexer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>facility;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>facility","Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations.",AUT,education,Developed economies,"[-31.417837, 0.777355]","[-13.164963, -33.542168]","[-3.225548, 18.738783, 17.32776]","[-12.055468, -0.15623693, -11.781058]","[12.345218, 9.274332]","[9.250444, 4.8695383]","[13.892245, 13.617168, 0.17853238]","[10.30346, 6.2299237, 9.655928]"
103,Alison B Ma;Alexander Lerch,Representation Learning for the Automatic Indexing of Sound Effects Libraries,2022,https://doi.org/10.5281/zenodo.7316800,Alison B. Ma+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education,"Labeling and maintaining a commercial sound effects library is a time-consuming task exacerbated by databases that continually grow in size and undergo taxonomy updates. Moreover, sound search and taxonomy creation are complicated by non-uniform metadata, an unrelenting problem even with the introduction of a new industry standard, the Universal Category System. To address these problems and overcome dataset-dependent limitations that inhibit the successful training of deep learning models, we pursue representation learning to train generalized embeddings that can be used for a wide variety of sound effects libraries and are a taxonomy-agnostic representation of sound. We show that a task-specific but dataset-independent representation can successfully address data issues such as class imbalance, inconsistent class labels, and insufficient dataset size, outperforming established representations such as OpenL3. Detailed experimental results show the impact of metric learning approaches and different cross-dataset training methods on representational effectiveness.",USA,education,Developed economies,"[-15.21328, -8.552757]","[-20.23419, -37.39048]","[5.972356, 15.365485, 8.667121]","[-9.948464, 1.5322684, -19.095846]","[11.577826, 8.946773]","[9.649991, 4.874902]","[13.540099, 13.134517, 0.25117478]","[10.709696, 6.4014916, 8.959813]"
102,Ashvala Vinay;Alexander Lerch,Evaluating Generative Audio Systems and Their Metrics,2022,https://doi.org/10.5281/zenodo.7343083,Ashvala Vinay+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Center for Music Technology>USA>education,"Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems.",USA,education,Developed economies,"[-3.4705493, -22.1057]","[-17.456928, -50.26656]","[11.924627, 6.434934, 15.573569]","[-25.54487, -1.87055, -17.726059]","[9.922362, 8.649839]","[8.169342, 6.6603546]","[13.079533, 12.344304, 0.4821826]","[9.679175, 5.944515, 8.082605]"
101,Florian Thalmann;Eita Nakamura;Kazuyoshi Yoshii,Tracking the Evolution of a Band's Live Performances over Decades,2022,https://doi.org/10.5281/zenodo.7342596,Florian Thalmann+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"Evolutionary studies have become a dominant thread in the analysis of large audio collections. Such corpora usually consist of musical pieces by various composers or bands and the studies usually focus on identifying general historical trends in harmonic content or music production techniques. In this paper we present a comparable study that examines the music of a single band whose publicly available live recordings span three decades. We first discuss the opportunities and challenges faced when working with single-artist and live-music datasets and introduce solutions for audio feature validation and outlier detection. We then investigate how individual songs vary over time and identify general performance trends using a new approach based on relative feature values, which improves accuracy for features with a large variance. Finally, we validate our findings by juxtaposing them with descriptions posted in online forums by experienced listeners of the band's large following.",JPN,education,Developed economies,"[-13.117166, 8.042464]","[21.031376, -10.252188]","[-4.0316153, -5.4656997, -8.497408]","[17.178356, 10.465993, -4.471358]","[12.597992, 7.253747]","[10.511181, 2.9682658]","[13.146517, 13.531493, -1.4801348]","[11.858724, 6.679788, 11.470358]"
86,Ke Chen;Hao-Wen Dong;Yi Luo;Julian Mcauley;Taylor Berg-Kirkpatrick;Miller Puckette;Shlomo Dubnov,Improving Choral Music Separation through Expressive Synthesized Data from Sampled Instruments,2022,https://doi.org/10.5281/zenodo.7316766,Ke Chen+UC San Diego>USA>education;Hao-Wen Dong+UC San Diego>USA>education;Yi Luo+Tencent AI Lab>CHN>company;Julian McAuley+UC San Diego>USA>education;Taylor Berg-Kirkpatrick+UC San Diego>USA>education;Miller Puckette+UC San Diego>USA>education;Shlomo Dubnov+UC San Diego>USA>education,"Choral music separation refers to the task of extracting tracks of voice parts (e.g., soprano, alto, tenor, and bass) from mixed audio. The lack of datasets has impeded research on this topic as previous work has only been able to train and evaluate models on a few minutes of choral music data due to copyright issues and dataset collection difficulties. In this paper, we investigate the use of synthesized training data for the source separation task on real choral music. We make three contributions: first, we provide an automated pipeline for synthesizing choral music data from sampled instrument plugins within controllable options for instrument expressiveness. This produces an 8.2-hour-long choral music dataset from the JSB Chorales Dataset and one can easily synthesize additional data. Second, we conduct an experiment to evaluate multiple separation models on available choral music separation datasets from previous work. To the best of our knowledge, this is the first experiment to comprehensively evaluate choral music separation. Third, experiments demonstrate that the synthesized choral data is of sufficient quality to improve the model's performance on real choral music datasets. This provides additional experimental statistics and data support for the choral music separation study.",USA,education,Developed economies,"[5.6728015, -44.574707]","[-38.671886, -27.83508]","[27.309433, -0.53276277, -0.9697135]","[-13.099896, -4.6083493, -29.515581]","[8.451586, 9.939925]","[6.810543, 5.676811]","[11.012887, 13.4951935, 1.5308946]","[9.661128, 8.232615, 9.156899]"
100,Xichu Ma;Xiao Liu;Bowen Zhang;Ye Wang,Robust Melody Track Identification in Symbolic Music,2022,https://doi.org/10.5281/zenodo.7342531,Xichu Ma+National University of Singapore>SGP>education;Xiao Liu+National University of Singapore>SGP>education;Bowen Zhang+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"<p>Melody tracks are worthy of special attention in the field of symbolic music information retrieval (MIR) because they contribute more towards music perception than many other musical components. However, many existing symbolic MIR systems neglect melody track identification (MTI) and are thus less effective. Existing MTI methods are also not robust and perform poorly on MIDI files representing music of unusual genres, arrangements, or formats. To address this problem, we propose a CNN-Transformer-based MTI model designed to robustly identify a single melody track for a given MIDI file. As this process can take a sizable amount of time for long songs, we also use a sparse Transformer to speed up attention computation. Our experiments show that our proposed model outperforms state-of-the-art (SOTA) algorithms in accuracy and can also benefit downstream MIR tasks.</p>",SGP,education,Developing economies,"[10.869801, -12.957995]","[-15.786008, -30.739542]","[0.5690843, -10.546367, 14.286224]","[-16.164427, -11.895096, -13.602964]","[11.456765, 6.996082]","[8.720225, 5.234748]","[12.961111, 12.447943, -0.97811735]","[9.725199, 6.1760316, 9.258389]"
98,Pablo Alonso-Jiménez;Xavier Serra;Dmitry Bogdanov,Music Representation Learning Based on Editorial Metadata from Discogs,2022,https://doi.org/10.5281/zenodo.7316790,Pablo Alonso-Jiménez+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education;Dmitry Bogdanov+Universitat Pompeu Fabra>ESP>education,"This paper revisits the idea of music representation learning supervised by editorial metadata, contributing to the state of the art in two ways. First, we exploit the public editorial metadata available on Discogs, an extensive community-maintained music database containing information about artists, releases, and record labels. Second, we use a contrastive learning setup based on COLA, different from previous systems based on triplet loss. We train models targeting several associations derived from the metadata and experiment with stacked combinations of learned representations, evaluating them on standard music classification tasks. Additionally, we consider learning all the associations jointly in a multi-task setup. We show that it is possible to improve the performance of current self-supervised models by using inexpensive metadata commonly available in music collections, producing representations comparable to those learned on classification setups. We find that the resulting representations based on editorial metadata outperform a system trained with music style tags available in the same large-scale dataset, which motivates further research using this type of supervision. Additionally, we give insights on how to preprocess Discogs metadata to build training objectives and provide public pre-trained models.",ESP,education,Developed economies,"[-16.454536, -4.8958974]","[33.854206, -0.7607486]","[-0.25813076, 11.593559, 8.955901]","[18.936583, 13.007324, -1.3302674]","[11.866052, 9.010738]","[10.3891535, 4.1965394]","[13.910301, 13.074921, 0.19744378]","[11.961554, 6.0937357, 10.389579]"
97,Stewart Greenhill;Majid Abdolshah;Vuong Le;Sunil Gupta;Svetha Venkatesh,Semantic Control of Generative Musical Attributes,2022,https://doi.org/10.5281/zenodo.7316788,Stewart Greenhill+Deakin University>AUS>education;Majid Abdolshah+Deakin University>AUS>education;Vuong Le+Deakin University>AUS>education;Sunil Gupta+Deakin University>AUS>education;Svetha Venkatesh+Deakin University>AUS>education,"Deep generative neural networks have been successful in tasks such as composing novel music and rendering expressive performance. Controllability is essential for building creative tools from such models. Recent work in this area has focused on disentangled latent space representations, but this is only part of the solution. Efficient control of semantic attributes must handle non-linearities and holes that occur in latent spaces, whilst minimising unwanted changes to other attributes. This paper introduces SeNT-Gen, a neural traversal algorithm that uses a secondary neural network to model the complex relationships between latent codes and musical attributes. This enables precise editing of semantic attributes that adapts to context. We demonstrate the method using the dMelodies dataset, and show strong performance for several VAE models.",AUS,education,Developed economies,"[20.344542, 7.271868]","[-11.449941, -43.302658]","[-0.49789748, 4.182918, 21.862886]","[-19.914762, 1.6246744, -14.620915]","[10.552721, 8.451291]","[8.907992, 6.5277514]","[13.419322, 12.03553, 0.09038731]","[9.589554, 5.515685, 8.538202]"
96,Jui-Te Wu;Jun-You Wang;Jyh-Shing Roger Jang;Li Su,A unified model for zero-shot singing voice conversion and synthesis,2022,https://doi.org/10.5281/zenodo.7316786,Jui-Te Wu+National Taiwan University>TWN>education|Academia Sinica>TWN>facility;Jun-You Wang+National Taiwan University>TWN>education;Jyh-Shing Roger Jang+National Taiwan University>TWN>education|Academia Sinica>TWN>facility;Li Su+National Taiwan University>TWN>education|Academia Sinica>TWN>facility,"Recent advances in deep learning not only facilitate the implementation of zero-shot singing voice synthesis (SVS) and singing voice conversion (SVC) tasks but also provide the opportunity to unify these two tasks into one generalized model. In this paper, we propose such a model that generate the singing voice of any target singer from any source singing content in either text or audio format. The model incorporates self-supervised joint training of the phonetic encoder and the acoustic encoder, with an audio-to-phoneme alignment process in each training step, such that these encoders map the audio and text data respectively into a shared, temporally aligned, and singer agnostic latent space. The target singer's latent representations encoded at different granularity levels are all trained to match the source latent representations sequentially with the attention mechanisms in the decoding stage. This enables the model to generate unseen target singer's voice with fine-grained resolution from either text or audio sources. Both objective and subjective experiments confirmed that the proposed model is competitive with the state-of-the-art SVC and SVS methods.",TWN,education,Developing economies,"[-0.32335645, -37.017616]","[-29.56743, -44.037804]","[24.263105, 4.479698, -13.891436]","[1.945279, -9.564908, -23.304176]","[9.459741, 10.8128]","[7.8100476, 4.8796935]","[11.05998, 14.749166, 0.9458364]","[10.273624, 7.229221, 8.759839]"
95,Vjosa Preniqi;Kyriaki Kalimeri;Charalampos Saitis,"""More than words"": Linking Music Preferences and Moral Values through Lyrics",2022,https://doi.org/10.5281/zenodo.7343071,"Vjosa Preniqi+Centre for Digital Music, Queen Mary University of London>GBR>education;Kyriaki Kalimeri+ISI Foundation>ITA>company;Charalampos Saitis+Centre for Digital Music, Queen Mary University of London>GBR>education","This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song's overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression ap- proaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (.20 ≤ r ≤ .30) than values of empathy and equality (.08 ≤ r ≤ .11), while basic demographic variables only account for a small part in the models' explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.",GBR,education,Developed economies,"[-34.562443, -29.253063]","[50.625637, 1.0736725]","[3.6236808, 26.514357, 3.4824069]","[17.429148, 26.182838, 5.7538624]","[12.044971, 11.818648]","[12.8751335, 3.6363785]","[12.813074, 15.921695, 1.040884]","[13.88875, 5.256311, 10.768958]"
94,Timothy De Reuse;Ichiro Fujinaga,"A Transformer-Based ""Spellchecker"" for Detecting Errors in OMR Output",2022,https://doi.org/10.5281/zenodo.7316782,Timothy de Reuse+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"The outputs of Optical Music Recognition (OMR) systems require time-consuming human correction. Given that most of the errors induced by OMR processes appear non-musical to humans, we propose that the time to correct errors may be reduced by marking all symbols on a score that are musically unlikely, allowing the human to focus their attention accordingly. Using a dataset of Romantic string quartets, we train a variant of the Transformer network architecture on the task of classifying each symbol of an optically-recognized musical piece in symbolic format as correct or erroneous, based on whether a manual correction of the piece would require an insertion, deletion, or replacement of a symbol at that location. Since we have a limited amount of data with real OMR errors, we employ extensive data augmentation to add errors into training data in a way that mimics how OMR would modify the score. Our best-performing models achieve 99% recall and 50% precision on this error-detection task.",CAN,education,Developed economies,"[33.149242, 31.435886]","[-18.660757, 35.42674]","[3.9181452, -21.239214, 22.334959]","[-10.730522, -17.613117, 3.524882]","[9.708943, 6.1937995]","[6.551992, -0.6438536]","[12.670152, 11.27634, -1.5533634]","[8.043377, 4.209343, 10.556702]"
93,Stefan Lattner,SampleMatch: Drum Sample Retrieval by Musical Context,2022,https://doi.org/10.5281/zenodo.7316780,Stefan Lattner+Sony Computer Science Laboratories (CSL)>FRA>company,"Modern digital music production typically involves combining numerous acoustic elements to compile a piece of music. Important types of such elements are drum samples, which determine the characteristics of the percussive components of the piece. Artists must use their aesthetic judgement to assess whether a given drum sample fits the current musical context. However, selecting drum samples from a potentially large library is tedious and may interrupt the creative flow. In this work, we explore the automatic drum sample retrieval based on aesthetic principles learned from data. As a result, artists can rank the samples in their library by fit to some musical context at different stages of the production process (i.e., by fit to incomplete song mixtures). To this end, we use contrastive learning to maximize the score of drum samples originating from the same song as the mixture. We conduct a listening test to determine whether the human ratings match the automatic scoring function. We also perform objective quantitative analyses to evaluate the efficacy of our approach.",FRA,company,Developed economies,"[23.887068, -46.257847]","[-17.328522, 3.974555]","[18.611084, -18.826427, -1.5498638]","[5.238316, 13.073711, -12.31836]","[7.9247365, 7.0395365]","[8.509899, 3.9867067]","[10.237376, 11.746623, 0.8866475]","[9.642697, 7.2042108, 10.336712]"
92,Noah Schaffer;Boaz Cogan;Ethan Manilow;Max Morrison;Prem Seetharaman;Bryan Pardo,Music Separation Enhancement with Generative Modeling,2022,https://doi.org/10.5281/zenodo.7316778,"Noah Schaffer+Interactive Audio Lab, Northwestern University>USA>education|Descript, Inc.>USA>company;Boaz Cogan+Interactive Audio Lab, Northwestern University>USA>education|Descript, Inc.>USA>company;Ethan Manilow+Interactive Audio Lab, Northwestern University>USA>education|Descript, Inc.>USA>company;Max Morrison+Interactive Audio Lab, Northwestern University>USA>education|Descript, Inc.>USA>company;Prem Seetharaman+Descript, Inc.>USA>company;Bryan Pardo+Interactive Audio Lab, Northwestern University>USA>education","Despite phenomenal progress in recent years, state-of-the-art music separation systems produce source estimates with significant perceptual shortcomings, such as adding extraneous noise or removing harmonics. We propose a post-processing model (the Make it Sound Good (MSG) post-processor) to enhance the output of music source separation systems. We apply our post-processing model to state-of-the-art waveform-based and spectrogram-based music source separators, including a separator unseen by MSG during training. Our analysis of the errors produced by source separators shows that waveform models tend to introduce more high-frequency noise, while spectrogram models tend to lose transients and high frequency content. We introduce objective measures to quantify both kinds of errors and show MSG improves the source reconstruction of both kinds of errors. Crowdsourced subjective evaluations demonstrate that human listeners prefer source estimates of bass and drums that have been post-processed by MSG.",USA,education,Developed economies,"[6.4640594, -44.289406]","[-39.29967, -27.845503]","[27.397106, -0.9044391, -3.216972]","[-13.78557, -6.3545313, -29.622276]","[8.448298, 9.983454]","[6.6952424, 5.713925]","[11.083216, 13.672422, 1.5760181]","[9.70719, 8.365607, 9.231109]"
91,Peter Knees;Bruce Ferwerda;Andreas Rauber;Sebastian Strumbelj;Annabel Resch;Laurenz Tomandl;Valentin Bauer;Fung Yee Tang;Josip Bobinac;Amila Ceranic;Riad Dizdar,A Reproducibility Study on User-centric MIR Research and Why it is Important,2022,https://doi.org/10.5281/zenodo.7316776,Peter Knees+TU Wien>AUT>education|Georgia Institute of Technology>USA>education;Bruce Ferwerda+Jönköping University>SWE>education;Andreas Rauber+TU Wien>AUT>education;Sebastian Strumbelj+TU Wien>AUT>education;Annabel Resch+TU Wien>AUT>education;Laurenz Tomandl+TU Wien>AUT>education;Valentin Bauer+TU Wien>AUT>education;Fung Yee Tang+TU Wien>AUT>education;Josip Bobinac+TU Wien>AUT>education;Amila Ceranic+TU Wien>AUT>education;Riad Dizdar+TU Wien>AUT>education,"Reproducibility of results is a central pillar of scientific work. In music information retrieval research, this is widely acknowledged and practiced by the community by re-implementing algorithms and re-validating machine learning experiments. In this paper, we argue for an increased need to also reproduce the results and findings of user studies, including qualitative work, especially since these often lay the foundations and serve as justification for choices taken in algorithmic design and optimization criteria. As an example, we attempt to reproduce the study by Kim et al. presented in the RecSys (2020) paper ''Do Channels Matter? Illuminating Interpersonal Influence on Music Recommendations.'' By repeating this study on how interpersonal relationships can affect a user's assessment of music recommendations on a new sample of n=142 participants, we can largely confirm and support the validity of the original results. At the same time, we extend the analysis and also observe differences with regards to adoption rates between different channels as well as different factors that influences the adoption rate. From this specific reproducibility study, we conclude that potential cultural differences should be accounted for more explicitly in future studies and that systems development should be more explicitly connected to its intended target audience.",AUT,education,Developed economies,"[-8.2634535, 56.520863]","[37.469486, 26.985115]","[-35.48682, -2.2777476, -5.645804]","[9.811514, 8.308045, 17.653614]","[13.73111, 4.6218104]","[12.4418955, 1.4975321]","[15.08032, 11.105523, -1.4777867]","[13.107831, 4.839215, 12.547388]"
90,Katerina Kosta;Wei Tsung Lu;Gabriele Medeot;Pierre Chanquion,A deep learning method for melody extraction from a polyphonic symbolic music representation,2022,https://doi.org/10.5281/zenodo.7402960,Katerina Kosta+ByteDance>USA>company;Wei Tsung Lu+ByteDance>USA>company;Gabriele Medeot+ByteDance>USA>company;Pierre Chanquion+ByteDance>USA>company,"The task of identifying melodic lines in a polyphonic piece is a known active research topic in the symbolic and audio domain. Its importance has attracted the interest of researchers focusing on Music Information Retrieval and musicological applications and achieving high results is a common goal in industrial applications. The distinction of a melody in a written score can be a challenging task, however improvements have been reported in recent years using deep learning methods. In this paper, we present a lightweight deep bidirectional LSTM model for identifying the most salient melodic line of a music piece using handcrafted features without requiring the input score to be separated into multiple parts. We evaluate our model to measure the effectiveness of several data augmentation techniques and to compare performance to other state-of-the-art models. We also identify the features importance and evaluate their incremental contribution on the model performance using evaluation metrics. Results on the POP909 dataset show that our model approximates or outperforms current state of the art models trained on the same dataset, based on different implemented metrics and observations.",USA,company,Developed economies,"[14.179947, 13.008251]","[-15.970564, -32.418694]","[-0.76937693, -5.539548, 12.874459]","[-12.444642, -4.067533, -12.023332]","[11.692701, 7.0573907]","[8.8865, 5.303839]","[13.294368, 12.387402, -0.93506396]","[9.793426, 6.42522, 9.170856]"
89,Yigitcan Özer;Matej Ištvánek;Vlora Arifi-Müller;Meinard Müller,Using Activation Functions for Improving Measure-Level Audio Synchronization,2022,https://doi.org/10.5281/zenodo.7342932,Yigitcan Özer+International Audio Laboratories Erlangen>DEU>facility|Brno University of Technology>CHE>education;Matˇej Ištvánek+Brno University of Technology>CHE>education;Vlora Ariﬁ-Müller+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Audio synchronization aims at aligning multiple recordings of the same piece of music. Traditional synchronization approaches are often based on dynamic time warping using chroma features as an input representation. Previous work has shown how one can integrate onset cues into this pipeline for improving the alignment's temporal accuracy. Furthermore, recent work based on deep neural networks has led to significant improvements for learning onset, beat, and downbeat activation functions. However, for music with soft onsets and abrupt tempo changes, these functions may be unreliable, leading to unstable results. As the main contribution of this paper, we introduce a combined approach that integrates activation functions into the synchronization pipeline. We show that this approach improves the temporal accuracy thanks to the activation cues while inheriting the robustness of the traditional synchronization approach. Conducting experiments based on string quartet recordings, we evaluate our combined approach where we transfer measure annotations from a reference recording to a target recording.",DEU,facility,Developed economies,"[22.894754, -30.7264]","[-21.132309, -14.820993]","[-0.45878342, -19.252148, -16.881428]","[-1.2579614, -23.980423, -8.124369]","[11.031582, 5.756856]","[5.7257004, 1.2114061]","[11.80485, 12.400524, -2.0647209]","[7.7669015, 6.3928947, 10.734459]"
88,Takuya Takahashi;Mathieu Barthet,Emotion-driven Harmonisation And Tempo Arrangement of Melodies Using Transfer Learning,2022,https://doi.org/10.5281/zenodo.7316770,Takuya Takahashi+Queen Mary University of London>GBR>education;Mathieu Barthet+Queen Mary University of London>GBR>education,"We propose and assess deep learning models for harmonic and tempo arrangement generation given melodies and emotional constraints. A dataset of 4000 symbolic scores and emotion labels was gathered by expanding the HTPD3 dataset with mood tags from last.fm and allmusic.com. We explore how bi-directional LSTM and Transformer encoder architectures can learn relationships between symbolic melodies, chord progressions, tempo, and expressed emotions, with and without a transfer learning strategy leveraging symbolic music data without emotion labels. Three emotion annotation summarisation methods based on the Arousal/Valence (AV) representation are compared: Emotion Average, Emotion Surface, and Emotion Category. 20 participants (average age: 30.2, 7 females and 13 males from Japan) rated how well generated accompaniments matched melodies (musical coherence) as well as perceived emotions for 75 arrangements corresponding to combinations of models and emotion summarisation methods. Musical coherence and match between target and perceived emotions were highest when melodies were encoded using a BLSTM model with transfer learning. The proposed method generates emotion-driven harmonic/tempo arrangements in a fast way, a keen advantage compared to state of the art. Applications of this work include AI-based composition assistant and live interactive music systems for entertainment such as video games.",GBR,education,Developed economies,"[9.805361, -7.763561]","[52.86209, -11.567858]","[11.282138, 12.381322, 2.8075833]","[8.404816, 20.804428, 0.60755527]","[10.378969, 9.564832]","[12.837245, 4.4050107]","[11.422991, 14.913756, -0.82115656]","[13.956071, 5.011829, 10.056391]"
99,Chih-Pin Tan;Alvin W Y Su;Yi-Hsuan Yang,Melody Infilling with User-Provided Structural Context,2022,https://doi.org/10.5281/zenodo.7316792,Chih-Pin Tan+National Cheng Kung University>TWN>education|Academia Sinica>TWN>facility|Taiwan AI Labs>TWN>company;Alvin W. Y. Su+National Cheng Kung University>TWN>education|Academia Sinica>TWN>facility|Taiwan AI Labs>TWN>company;Yi-Hsuan Yang+National Cheng Kung University>TWN>education|Academia Sinica>TWN>facility|Taiwan AI Labs>TWN>company,"This paper proposes a novel Transformer-based model for music score infilling, to generate a music passage that fills in the gap between given past and future contexts. While existing infilling approaches can generate a passage that connects smoothly locally with the given contexts, they do not take into account the musical form or structure of the music and may therefore generate overly smooth results. To address this issue, we propose a structure-aware conditioning approach that employs a novel attention-selecting module to supply user-provided structure-related information to the Transformer for infilling. With both objective and subjective evaluations, we show that the proposed model can harness the structural information effectively and generate melodies in the style of pop of higher quality than the two existing structure-agnostic infilling models.",TWN,education,Developing economies,"[9.907623, -5.58313]","[-4.179466, -37.497543]","[12.0602455, 8.238583, 1.5084633]","[-6.529805, 0.037618477, -1.0170969]","[10.55877, 9.606938]","[9.096546, 6.027605]","[11.489709, 15.078882, -0.86257046]","[9.783344, 5.40781, 9.073442]"
56,Franca Bittner;Marcel Gonzalez;Maike L Richter;Hanna Lukashevich;Jakob Abeßer,Multi-pitch Estimation meets Microphone Mismatch: Applicability of Domain Adaptation,2022,https://doi.org/10.5281/zenodo.7342840,Franca Bittner+Fraunhofer IDMT>DEU>facility|Semantic Music Technologies Group>DEU>facility;Marcel Gonzalez+Fraunhofer IDMT>DEU>facility|Semantic Music Technologies Group>DEU>facility;Maike Richter+Fraunhofer IDMT>DEU>facility|Semantic Music Technologies Group>DEU>facility;Hanna Lukashevich+Fraunhofer IDMT>DEU>facility|Semantic Music Technologies Group>DEU>facility;Jakob Abeûer+Fraunhofer IDMT>DEU>facility|Semantic Music Technologies Group>DEU>facility,"The performance of machine learning (ML) models is known to be affected by discrepancies between training (source) and real-world (target) data distributions. This problem is referred to as domain shift and is commonly approached using domain adaptation (DA) methods. As one relevant scenario, automatic piano transcription algorithms in music learning applications potentially suffer from domain shift since pianos are recorded in different acoustic conditions using various devices. Yet, most currently available datasets for piano transcription only cover ideal recording situations with high-quality microphones. Consequently, a transcription model trained on these datasets will face a mismatch between source and target data in real-world scenarios. To address this issue, we employ a recently proposed dataset which includes annotated piano recordings covering typical real-life recording settings for a piano learning application on mobile devices. We first quantify the influence of the domain shift on the performance of a deep learning-based piano multi-pitch estimation (MPE) algorithm. Then, we employ and evaluate four unsupervised DA methods to reduce domain shift. Our results show that the studied MPE model is surprisingly robust to domain shift in microphone mismatch scenarios and the DA methods do not notably improve the transcription performance.",DEU,facility,Developed economies,"[24.245747, -23.316721]","[-27.792284, -37.840412]","[15.038885, -16.96822, -15.195356]","[-4.6593885, -6.7824345, -13.57874]","[9.929823, 5.6807346]","[8.121844, 5.085746]","[10.834108, 13.1442995, -0.8456281]","[9.669836, 6.694596, 8.874877]"
81,Genís Plaja-Roglans;Marius Miron;Xavier Serra,A diffusion-inspired training strategy for singing voice extraction in the waveform domain,2022,https://doi.org/10.5281/zenodo.7316754,Genís Plaja-Roglans+Universitat Pompeu Fabra>ESP>education;Marius Miron+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"Notable progress in music source separation has been achieved using multi-branch networks that operate on both temporal and spectral domains. However, such networks tend to be complex and heavy-weighted. In this work, we tackle the task of singing voice extraction from polyphonic music signals in an end-to-end manner using an approach inspired by the training procedure of denoising diffusion models. We perform unconditional signal modelling to gradually convert an input mixture signal to the corresponding singing voice or accompaniment. We use fewer parameters than the state-of-the-art models while operating on the waveform domain, bypassing phase-related problems. More concisely, we train a non-causal WaveNet using a diffusion-inspired strategy improving the said network for singing voice extraction and obtaining performance comparable to the end-to-end state-of-the-art on MUSDB18. We further report results on a non-MUSDB-overlapping version of MedleyDB and the multi-track audio of the Saraga Carnatic dataset showing good generalization, and run perceptual tests of our approach. Code, models, and audio examples are made available.",ESP,education,Developed economies,"[-2.035268, -40.153923]","[-39.499165, -32.769886]","[22.422426, 8.822794, -9.327496]","[-12.27328, -6.9283013, -24.598003]","[9.420572, 10.862143]","[6.9023747, 5.853597]","[10.990351, 14.83935, 1.0905284]","[9.675485, 8.140468, 8.894608]"
54,Chitralekha Gupta;Yize Wei;Zequn Gong;Purnima Kamath;Zhuoyao Li;Lonce Wyse,Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures,2022,https://doi.org/10.5281/zenodo.7342809,Chitralekha Gupta+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Yize Wei+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Zequn Gong+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Purnima Kamath+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Zhuoyao Li+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Lonce Wyse+National University of Singapore>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Standard evaluation metrics such as the Inception score and Fréchet Audio Distance provide a general audio quality distance metric between the synthesized audio and reference clean audio. However, the sensitivity of these metrics to variations in the statistical parameters that define an audio texture is not well studied. In this work, we provide a systematic study of the sensitivity of some of the existing audio quality evaluation metrics to parameter variations in audio textures. Furthermore, we also study three more potentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram matrix based distance, (b) an Accumulated Gram metric using a summarized version of the Gram matrices, and (c) a cochlear-model based statistical features metric. These metrics use deep features that summarize the statistics of any given audio texture, thus being inherently sensitive to variations in the statistical parameters that define an audio texture. We study and evaluate the sensitivity of existing standard metrics as well as Gram matrix and cochlear-model based metrics in response to control-parameter variations for audio textures across a wide range of texture and parameter types, and validate with subjective evaluation. We find that each of the metrics is sensitive to different sets of texture-parameter types. This is the first step towards investigating objective metrics for assessing parameter sensitivity in audio textures.",SGP,education,Developing economies,"[-3.6573825, -21.950462]","[-17.241272, -51.091156]","[11.98621, 6.944231, 13.978074]","[-3.2409027, 2.9725251, -31.564869]","[11.524018, 9.222559]","[8.1466465, 6.6198654]","[13.177177, 13.101513, 0.6447545]","[9.864946, 5.996472, 8.19591]"
24,Johannes Imort;Giorgio Fabbro;Marco A Martinez Ramirez;Stefan Uhlich;Yuichiro Koyama;Yuki Mitsufuji,Distortion Audio Effects: Learning How to Recover the Clean Signal,2022,https://doi.org/10.5281/zenodo.7316634,Johannes Imort+RWTH Aachen University>DEU>education;Giorgio Fabbro+Sony Europe B.V.>DEU>company;Marco A. Martínez-Ramírez+Sony Group Corporation>JPN>company;Stefan Uhlich+Sony Europe B.V.>DEU>company;Yuichiro Koyama+Sony Group Corporation>JPN>company;Yuki Mitsufuji+Sony Group Corporation>JPN>company,"Given the recent advances in music source separation and automatic mixing, removing audio effects in music tracks is a meaningful step toward developing an automated remixing system. This paper focuses on removing distortion audio effects applied to guitar tracks in music production. We explore whether effect removal can be solved by neural networks designed for source separation and audio effect modeling.Our approach proves particularly effective for effects that mix the processed and clean signals. The models achieve better quality and significantly faster inference compared to state-of-the-art solutions based on sparse optimization. We demonstrate that the models are suitable not only for declipping but also for other types of distortion effects. By discussing the results, we stress the usefulness of multiple evaluation metrics to assess different aspects of reconstruction in distortion effect removal.",DEU,education,Developed economies,"[-11.511611, -23.97421]","[-40.852478, -31.09779]","[7.793303, -5.440063, -21.199087]","[-12.1264925, -9.824592, -29.507019]","[8.789021, 9.577359]","[6.645339, 5.631754]","[11.175419, 13.513429, 1.0499914]","[9.653412, 8.404474, 9.170348]"
23,Francisco C. F. Almeida;Gilberto Bernardes;Christof Weiss,Mid-level Harmonic Audio Features for Musical Style Classification,2022,https://doi.org/10.5281/zenodo.7316632,Francisco Almeida+University of Porto>PRT>education;Gilberto Bernardes+University of Porto>PRT>education;Christof Weiû+International Audio Laboratories Erlangen>DEU>facility,"The extraction of harmonic information from musical audio is fundamental for several music information retrieval tasks. In this paper, we propose novel harmonic audio features based on the perceptually-inspired tonal interval vector space, computed as the Fourier transform of chroma vectors. Our contribution includes mid-level features for musical dissonance, chromaticity, dyadicity, triadicity, diminished quality, diatonicity, and whole-toneness. Moreover, we quantify the perceptual relationship between short- and long-term harmonic structures, tonal dispersion, harmonic changes, and complexity. Beyond the computation on fixed-size windows, we propose a context-sensitive harmonic segmentation approach. We assess the robustness of the new harmonic features in style classification tasks regarding classical music periods and composers. Our results align with, slightly outperforming, existing features and suggest that other musical properties than those in state-of-the-art literature are partially captured. We discuss the features regarding their musical interpretation and compare the different feature groups regarding their effectiveness for discriminating classical music periods and composers.",PRT,education,Developed economies,"[-24.103922, -15.479653]","[-5.871163, 0.72279185]","[-10.91761, 0.9030612, 19.713585]","[7.848578, -3.7785861, -2.9515967]","[12.40068, 10.633147]","[7.8468328, 2.4540381]","[13.327693, 14.003552, 1.1724733]","[10.29844, 7.903084, 11.98342]"
22,Junyan Jiang;Daniel Chin;Yixiao Zhang;Gus Xia,Learning Hierarchical Metrical Structure Beyond Measures,2022,https://doi.org/10.5281/zenodo.7316630,"Junyan Jiang+Music X Lab, NYU Shanghai>USA>education|MBZUAI>Unknown>Unknown|Centre for Digital Music, QMUL>GBR>education;Daniel Chin+Music X Lab, NYU Shanghai>USA>education|MBZUAI>Unknown>Unknown|Centre for Digital Music, QMUL>GBR>education;Yixiao Zhang+Music X Lab, NYU Shanghai>USA>education|MBZUAI>Unknown>Unknown|Centre for Digital Music, QMUL>GBR>education;Gus Xia+Music X Lab, NYU Shanghai>USA>education|MBZUAI>Unknown>Unknown|Centre for Digital Music, QMUL>GBR>education","Music contains hierarchical structures beyond beats and measures. While hierarchical structure annotations are helpful for music information retrieval and computer musicology, such annotations are scarce in current digital music databases. In this paper, we explore a data-driven approach to automatically extract hierarchical metrical structures from scores. We propose a new model with a Temporal Convolutional Network-Conditional Random Field (TCN-CRF) architecture. Given a symbolic music score, our model takes in an arbitrary number of voices in a beat-quantized form, and predicts a 4-level hierarchical metrical structure from downbeat-level to section-level. We also annotate a dataset using RWC-POP MIDI files to facilitate training and evaluation. We show by experiments that the proposed method performs better than the rule-based approach under different orchestration settings. We also perform some simple musicological analysis on the model predictions. All demos, datasets and pre-trained models are publicly available on Github.",USA,education,Developed economies,"[15.169333, 31.867374]","[-10.857602, -31.016554]","[-19.85013, -18.524782, 5.4287806]","[-14.025095, 1.7714053, -7.28002]","[12.241553, 9.071867]","[8.980279, 5.2275515]","[13.291607, 14.283709, -0.2687831]","[9.862648, 6.088239, 9.580819]"
21,Yueh-Kao Wu;Ching-Yu Chiu;Yi-Hsuan Yang,Jukedrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE,2022,https://doi.org/10.5281/zenodo.7316628,Yueh-Kao Wu+Academia Sinica>TWN>facility;Ching-Yu Chiu+National Cheng Kung University>TWN>education;Yi-Hsuan Yang+Taiwan AI Labs>TWN>company,"This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio- domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",TWN,facility,Developing economies,"[24.056433, -41.16672]","[-32.751465, -18.40832]","[18.597311, -14.543643, 3.2159405]","[-4.1584044, 12.448977, -21.893188]","[8.050493, 7.377109]","[8.226896, 4.460243]","[10.475723, 11.716772, 0.91709703]","[8.709158, 6.930639, 9.664117]"
20,Karim M. Ibrahim;Elena V. Epure;Geoffroy Peeters;Gaël Richard,Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts,2022,https://doi.org/10.5281/zenodo.7342816,"Karim M. Ibrahim+Télécom Paris, Institut Polytechnique de Paris>FRA>education|Deezer Research>FRA>company;Elena V. Epure+Deezer Research>FRA>company;Geoffroy Peeters+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Gaël Richard+Télécom Paris, Institut Polytechnique de Paris>FRA>education","As music has become more available especially on music streaming platforms, people have started to have distinct preferences to fit to their varying listening situations, also known as context. Hence, there has been a growing interest in considering the user's situation when recommending music to users. Previous works have proposed personalized auto-taggers to infer situation-related tags from music content and user's global listening preferences. However, in a practical music retrieval system, these context-aware auto-tagger could be only used by assuming that the context class is explicitly provided by the user. In this work, for designing a fully automatised music retrieval system, we propose to disambiguate the user's listening information from stream data. Namely, we propose a system which can generate a situational playlist for a user at a certain time first by leveraging personalized music auto-taggers, and second by automatically inferring the user's situation from stream data (e.g. device, network) and user's general profile information (e.g. age). Experiments show that such a personalized context-aware music retrieval system is feasible, but the performance suffers in the case of new users, new tracks or when the number of context classes increases.",FRA,education,Developed economies,"[-39.108437, 0.8937603]","[34.456886, 16.630884]","[-17.98703, 15.670828, 12.207975]","[16.343706, -1.8326674, 18.411385]","[14.505706, 10.269444]","[11.855782, 2.0015697]","[15.385606, 14.150349, -0.20665157]","[12.966691, 5.4779406, 12.622627]"
19,Seungyeon Rhyu;Sarah Kim;Kyogu Lee,Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning,2022,https://doi.org/10.5281/zenodo.7342916,Seungyeon Rhyu+Seoul National University>KOR>education;Sarah Kim+Krust Universe>KOR>company;Kyogu Lee+Seoul National University>KOR>education,"We propose a system for rendering a symbolic piano performance with flexible musical expression. It is necessary to actively control musical expression for creating a new music performance that conveys various emotions or nuances. However, previous approaches were limited to following the composer's guidelines of musical expression or dealing with only a part of the musical attributes. We aim to disentangle the entire musical expression and structural attribute of piano performance using a conditional VAE framework. It stochastically generates expressive parameters from latent representations and given note structures. In addition, we employ self-supervised approaches that force the latent variables to represent target attributes. Finally, we leverage a two-step encoder and decoder that learn hierarchical dependency to enhance the naturalness of the output. Experimental results show that our system can stably generate performance parameters relevant to the given musical scores, learn disentangled representations, and control musical attributes independently of each other.",KOR,education,Developing economies,"[33.144302, -0.9523643]","[-12.766164, -40.66899]","[14.5671015, -0.9838794, 23.134914]","[-16.291021, 1.5920128, -11.58018]","[10.023672, 7.244248]","[8.644445, 6.3408866]","[12.396074, 11.579299, -0.40858784]","[9.330371, 5.7232895, 8.6714945]"
18,Jingwei Zhao;Gus Xia;Ye Wang,Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention,2022,https://doi.org/10.5281/zenodo.7316622,"Jingwei Zhao+National University of Singapore>SGP>education|Integrative Sciences and Engineering Programme, NUS Graduate School>SGP>education;Gus Xia+New York University Shanghai>CHN>education|Music X Lab, NYU Shanghai>CHN>education;Ye Wang+National University of Singapore>SGP>education|Institute of Data Science, NUS>SGP>education","We propose Beat Transformer, a novel Transformer encoder architecture for joint beat and downbeat tracking. Different from previous models that track beats solely based on the spectrogram of an audio mixture, our model deals with demixed spectrograms with multiple instrument channels. This is inspired by the fact that humans perceive metrical structures from richer musical contexts, such as chord progression and instrumentation. To this end, we develop a Transformer model with both time-wise attention and instrument-wise attention to capture deep-buried metrical cues. Moreover, our model adopts a novel dilated self-attention mechanism, which achieves powerful hierarchical modelling with only linear complexity. Experiments demonstrate a significant improvement in demixed beat tracking over the non-demixed version. Also, Beat Transformer achieves up to 4% point improvement in downbeat tracking accuracy over the TCN architectures. We further discover an interpretable attention pattern that mirrors our understanding of hierarchical metrical structures.",SGP,education,Developing economies,"[33.467854, -33.4426]","[-30.95622, -17.575165]","[7.0395303, -33.227596, -7.341136]","[-7.361897, 11.75917, -21.383795]","[10.406277, 4.1934505]","[5.011527, 2.6855392]","[10.169599, 12.840781, -2.2394505]","[8.155131, 6.860131, 9.806504]"
17,Ellie Bean Abrams;Eva Muñoz Vidal;Claire Pelofi;Pablo Ripollés,Retrieving musical information from neural data: how cognitive features enrich acoustic ones,2022,https://doi.org/10.5281/zenodo.7343078,"Ellie Bean Abrams+Music and Audio Research Laboratory, New York University>USA>education|Center for Language, Music, and Emotion, New York University>USA>education|Department of Psychology, New York University>USA>education;Eva Muñoz Vidal+Music and Audio Research Laboratory, New York University>USA>education|Center for Language, Music, and Emotion, New York University>USA>education|Department of Psychology, New York University>USA>education;Claire Pelofi+Music and Audio Research Laboratory, New York University>USA>education|Center for Language, Music, and Emotion, New York University>USA>education|Department of Psychology, New York University>USA>education;Pablo Ripollés+Music and Audio Research Laboratory, New York University>USA>education|Center for Language, Music, and Emotion, New York University>USA>education|Department of Psychology, New York University>USA>education","Various features – from low-level acoustics, to higher-level statistical regularities, to memory associations – contribute to the experience of musical enjoyment and pleasure. Recent work suggests that musical surprisal, that is, the unexpectedness of a musical event given its context, may directly predict listeners' experiences of pleasure and enjoyment during music listening. Understanding how surprisal shapes listeners' preferences for certain musical pieces has implications for music recommender systems, which are typically content- (both acoustic or semantic) or metadata-based. Here we test a recently developed computational algorithm, called Dynamic-Regularity Extraction (D-REX), that uses Bayesian inference to predict the surprisal that humans experience while listening to music. We demonstrate that the brain tracks musical surprisal as modeled by D-REX by conducting a decoding analysis on the neural signal (collected through magnetoencephalography) of participants listening to music. Thus, we demonstrate the validity of a computational model of musical surprisal, which may remarkably inform the next generation of recommender systems. In addition, we present an open-source neural dataset which will be available for future research to foster approaches combining MIR with cognitive neuroscience, an approach we believe will be a key strategy in characterizing people's reactions to music.",USA,education,Developed economies,"[-14.47588, -0.41227058]","[32.757328, 11.735768]","[-3.09295, 3.299976, 15.133568]","[0.28540793, 19.277002, 12.083781]","[12.207502, 8.869125]","[10.851609, 4.256135]","[13.540655, 13.471946, -0.37480074]","[11.870734, 5.0011554, 10.637161]"
16,Simeon Rau;Frank Heyen;Stefan Wagner;Michael Sedlmair,Visualization for AI-Assisted Composing,2022,https://doi.org/10.5281/zenodo.7316618,"Simeon Rau+VISUS, University of Stuttgart>DEU>education;Frank Heyen+VISUS, University of Stuttgart>DEU>education;Stefan Wagner+ISTE, University of Stuttgart>DEU>education;Michael Sedlmair+VISUS, University of Stuttgart>DEU>education","We propose a visual approach for interactive, AI-assisted composition that serves as a compromise between fully automatic and fully manual composition. Instead of generating a whole piece, the AI takes on the role of an assistant that generates short melodies for the composer to choose from and adapt. In an iterative process, the composer queries the AI for continuations or alternative fill-ins, chooses a suggestion, and adds it to the piece. As listening to many suggestions would take time, we explore different ways to visualize them, to allow the composer to focus on the most interesting-looking melodies. We also present the results of a qualitative evaluation with five composers.",DEU,education,Developed economies,"[-9.606091, 40.243214]","[-6.2792444, 40.02129]","[-27.664612, -4.8564076, 8.6539545]","[-22.866074, 8.106651, -1.0203981]","[12.707787, 7.287615]","[9.607511, 6.060681]","[14.197766, 12.854856, -1.2969525]","[10.053466, 5.2291474, 9.681801]"
15,Daiyu Zhang;Ju-Chiang Wang;Katerina Kosta;Jordan B. L. Smith;Shicen Zhou,Modeling the rhythm from lyrics for melody generation of pop songs,2022,https://doi.org/10.5281/zenodo.7316616,Daiyu Zhang+ByteDance>CHN>company;Ju-Chiang Wang+ByteDance>CHN>company;Katerina Kosta+ByteDance>CHN>company;Jordan B. L. Smith+ByteDance>CHN>company;Shicen Zhou+ByteDance>CHN>company,"Creating a pop song melody according to pre-written lyrics is a typical practice for composers. A computational model of how lyrics are set as melodies is important for automatic composition systems, but an end-to-end lyric-to-melody model would require enormous amounts of paired training data. To mitigate the data constraints, we adopt a two-stage approach, dividing the task into lyric-to-rhythm and rhythm-to-melody modules. However, the lyric-to-rhythm task is still challenging due to its multimodality. In this paper, we propose a novel lyric-to-rhythm framework that includes part-of-speech tags to achieve better text-setting, and a Transformer architecture designed to model long-term syllable-to-note associations. For the rhythm-to-melody task, we adapt a proven chord-conditioned melody Transformer, which has achieved state-of-the-art results. Experiments for Chinese lyric-to-melody generation show that the proposed framework is able to model key characteristics of rhythm and pitch distributions in the dataset, and in a subjective evaluation, the melodies generated by our system were rated as similar to or better than those of a state-of-the-art alternative.",CHN,company,Developing economies,"[17.95838, 5.5416126]","[-9.809837, -26.543652]","[1.7950333, 3.8360424, 29.78677]","[3.445043, -0.4171406, -23.384214]","[10.307272, 8.701489]","[8.954133, 4.6997976]","[13.301084, 11.902921, 0.10981642]","[10.699964, 6.7914333, 9.328866]"
14,Yen-Tung Yeh;Yi-Hsuan Yang;Bo-Yu Chen,Exploiting Pre-trained Feature Networks for Generative Adversarial Networks in Audio-domain Loop Generation,2022,https://doi.org/10.5281/zenodo.7316614,Yen-Tung Yeh+Academia Sinica>TWN>education|National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown;Bo-Yu Chen+Academia Sinica>TWN>education|National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown;Yi-Hsuan Yang+Academia Sinica>TWN>education|National Taiwan University>TWN>education|Taiwan AI Labs>Unknown>Unknown,"While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",TWN,education,Developing economies,"[25.89757, 4.8875604]","[-17.24145, -48.783546]","[7.0551767, 1.8069682, 26.438303]","[-24.659027, 0.07437824, -18.48352]","[9.949067, 8.305472]","[8.255107, 6.696206]","[13.156042, 11.717821, 0.48959392]","[9.602579, 5.906824, 8.060092]"
13,Daiki Naruse;Tomoyuki Takahata;Yusuke Mukuta;Tatsuya Harada,Pop Music Generation with Controllable Phrase Lengths,2022,https://doi.org/10.5281/zenodo.7316612,Daiki Naruse+The University of Tokyo>JPN>education;Tomoyuki Takahata+The University of Tokyo>JPN>education;Yusuke Mukuta+The University of Tokyo>JPN>education|RIKEN>JPN>facility;Tatsuya Harada+The University of Tokyo>JPN>education|RIKEN>JPN>facility,"Research on music generation using deep learning has attracted more attention; in particular, Transformer-based models have succeeded in generating coherent musical pieces. Recently, an increasing number of studies have focused on phrases that are smaller musical units, and several studies have addressed phrase-level control. In this study, we propose a method for sequentially generating a piece that enables the control of each phrase length and, consequently, the length of the entire piece. We added PHRASE and a new event, BAR COUNTDOWN, which indicates the number of bars remaining in the phrase, to the existing event-based music representations. To reflect user input indicating the phrase lengths of the piece being generated, we used an autoregressive generation model that adds these two events to the generated event-token sequence based on the user input and uses it as input for the next time step. Subjective listening tests revealed that the pieces generated by our methods possessed designated phrase lengths and ended naturally at the determined length.",JPN,education,Developed economies,"[18.057758, 5.631177]","[-13.109567, -36.32243]","[1.5651183, 3.6841714, 28.75285]","[-17.413733, -3.5553277, -9.598617]","[10.372224, 8.562264]","[8.860071, 5.851262]","[13.417259, 11.791462, 0.13909952]","[9.473613, 5.7365212, 9.1503315]"
12,Jaehun Kim;Cynthia C. S. Liem,The power of deep without going deep? A study of HDPGMM music representation learning,2022,https://doi.org/10.5281/zenodo.7316610,Jaehun Kim+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education,"In the previous decade, Deep Learning (DL) has proven to be one of the most effective machine learning methods to tackle a wide range of Music Information Retrieval (MIR) tasks. It offers highly expressive learning capacity that can fit any music representation needed for MIR-relevant downstream tasks. However, it has been criticized for sacrificing interpretability. On the other hand, the Bayesian nonparametric (BN) approach promises similar positive properties as DL, such as high flexibility, while being robust to overfitting and preserving interpretability. Therefore, the primary motivation of this work is to explore the potential of Bayesian nonparametric models in comparison to DL models for music representation learning. More specifically, we assess the music representation learned from the Hierarchical Dirichlet Process Gaussian Mixture Model (HDPGMM), an infinite mixture model based on the Bayesian nonparametric approach, to MIR tasks, including classification, auto-tagging, and recommendation. The experimental result suggests that the HDPGMM music representation can outperform DL representations in certain scenarios, and overall comparable.",NLD,education,Developed economies,"[-12.052763, -8.627594]","[28.917507, 3.3005216]","[6.899891, 9.599257, 12.694035]","[24.532667, -1.1560373, 3.3577363]","[11.489462, 9.17273]","[10.669021, 3.8703346]","[13.440168, 13.019497, 0.55181473]","[10.284118, 5.996384, 8.891341]"
11,Anil Venkatesh;Viren Sachdev,Detecting Symmetries of All Cardinalities With Application to Musical 12-Tone Rows,2022,https://doi.org/10.5281/zenodo.7316608,Anil Venkatesh+Adelphi University>USA>education;Viren Sachdev+Adelphi University>USA>education,"Popularized by Arnold Schoenberg in the mid-20th century, the method of twelve-tone composition produces musical compositions based on one or more orderings of the equal-tempered chromatic scale. The work of twelve-tone composers is famously challenging to traditional Western tonal and structural sensibilities; even so, group theoretic approaches have determined that 10% of certain composers' works contain a highly unusual classical symmetry of music. We extend this result by revealing many symmetries that were previously undetected in the works of Schoenberg, Webern, and Berg. Our approach is computational rather than group theoretic, scanning each composition for symmetries of many different cardinalities. Thus, we capture partial symmetries that would be overlooked by more formal means. Moreover, our methods are applicable beyond the narrow scope of twelve-tone composition. We achieve our results by first extending the group-theoretic notion of symmetry to encompass shorter motives that may be repeated and reprised in a given composition, and then comparing the incidence of these symmetries between the work of composers and the space of all possible 12-tone rows. We present four candidate hierarchies of symmetry and show that in each model, between 75% and 95% of actual compositions contained high levels of internal symmetry.",USA,education,Developed economies,"[9.480099, 20.671581]","[-17.017466, 19.302835]","[-5.5989065, 16.037619, 9.888548]","[-16.02932, -8.300654, 8.180894]","[12.364773, 8.668674]","[7.846596, 2.1366239]","[12.705126, 14.107299, -0.6924102]","[10.043188, 7.675024, 12.510615]"
10,Eunjin Choi;Yoonjin Chung;Seolhee Lee;Jongik Jeon;Taegyun Kwon;Juhan Nam,YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations,2022,https://doi.org/10.5281/zenodo.7316606,Eunjin Choi+KAIST>KOR>education;Yoonjin Chung+KAIST>KOR>education;Seolhee Lee+KAIST>KOR>education;Jong Ik Jeon+KAIST>KOR>education;Taegyun Kwon+KAIST>KOR>education;Juhan Nam+KAIST>KOR>education,"Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB.",KOR,education,Developing economies,"[-62.149086, -0.09195062]","[49.40932, -7.4138107]","[-28.189594, 19.567049, 1.0741365]","[10.380213, 19.580902, 1.9229707]","[13.908209, 12.764604]","[12.900779, 4.1611037]","[16.04541, 14.312758, 1.682999]","[13.959948, 5.1889224, 10.239196]"
9,Yuqiang Li;Shengchen Li;George Fazekas,How Music features and Musical Data Representations Affect Objective Evaluation of Music Composition: A Review of CSMT Data Challenge 2020,2022,https://doi.org/10.5281/zenodo.7316604,Yuqiang Li+Xi'an Jiaotong-Liverpool University>CHN>education;Shengchen Li+Xi'an Jiaotong-Liverpool University>CHN>education;George Fazekas+Queen Mary University of London>GBR>education,"Tools and methodologies for distinguishing computer-generated melodies from human-composed melodies have a broad range of applications from detecting copyright infringement through the evaluation of generative music systems to facilitating transparent and explainable AI. This paper reviews a data challenge on distinguishing computer-generated melodies from human-composed melodies held in association with the Conference on Sound and Music Technology (CSMT) in 2020. An investigation of the submitted systems and the results are presented first. Besides the structure of the proposed models, the paper investigates two important factors that were identified as contributors to good model performance: the specific music features and the music representation used. Through an analysis of the submissions, important melody-related music features have been identified. Encoding or representation of the music in the context of neural network modes has also been found to significantly impact system performance through an experiment where the top-ranked system was re-implemented with different input representations for comparison purposes. Besides demonstrating the feasibility of developing an objective music composition evaluation system, the investigation presented in this paper also reveals some important limitations of current music composition systems opening opportunities for future work in the community.",CHN,education,Developing economies,"[-18.07564, 5.4191318]","[-5.8411126, 42.61629]","[-17.859781, 1.1462862, 6.09782]","[-19.950901, 8.337519, 1.9032922]","[13.173973, 8.599981]","[9.760742, 5.857546]","[13.982548, 13.732325, -0.7078974]","[10.239961, 5.1414948, 9.561193]"
8,Elio Quinton,Equivariant self-supervision for musical tempo estimation,2022,https://doi.org/10.5281/zenodo.7316602,Elio Quinton+Universal Music Group>USA>company,"Self-supervised methods have emerged as a promising avenue for representation learning in the recent years since they alleviate the need for labeled datasets, which are scarce and expensive to acquire. Contrastive methods are a popular choice for self-supervision in the audio domain, and typically provide a learning signal by forcing the model to be invariant to some transformations of the input. These methods, however, require measures such as negative sampling or some form of regularisation to be taken to prevent the model from collapsing on trivial solutions. In this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data. We derive a simple loss function that prevents the network from collapsing on a trivial solution during training, without requiring any form of regularisation or negative sampling.Our experiments show that it is possible to learn meaningful representations for tempo estimation by solely relying on equivariant self-supervision, achieving performance comparable with supervised methods on several benchmarks. As an added benefit, our method only requires moderate compute resources and therefore remains accessible to a wide research community.",USA,company,Developed economies,"[40.928936, -25.288977]","[-21.02382, -35.309734]","[-0.7533348, -28.979862, 2.4346511]","[-8.316843, -1.6717066, -16.606337]","[11.510261, 4.4124117]","[9.397926, 4.9054303]","[10.90623, 13.3165455, -2.8985827]","[10.536287, 6.4159365, 8.900169]"
7,Da-Yi Wu;Wen-Yi Hsiao;Fu-Rong Yang;Oscar D Friedman;Warren Jackson;Scott Bruzenak;Yi-Wen Liu;Yi-Hsuan Yang,DDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation,2022,https://doi.org/10.5281/zenodo.7316600,Da-Yi Wu+Academia Sinica>TWN>education;Wen-Yi Hsiao+Taiwan AI Labs>TWN>Unknown;Fu-Rong Yang+National Tsing Hua Univ.>TWN>education;Oscar Friedman+470 Music Group>TWN>company;Warren Jackson+PARC>USA>company;Scott Bruzenak+470 Music Group>TWN>company;Yi-Wen Liu+National Tsing Hua Univ.>TWN>education;Yi-Hsuan Yang+Academia Sinica>TWN>education|Taiwan AI Labs>TWN>Unknown,"A vocoder is a conditional audio generation model that converts acoustic features such as mel-spectrograms into waveforms. Taking inspiration from Differentiable Digital Signal Processing (DDSP), we propose a new vocoder named SawSing for singing voices. SawSing synthesizes the harmonic part of singing voices by filtering a sawtooth source signal with a linear time-variant finite impulse response filter whose coefficients are estimated from the input mel-spectrogram by a neural network. As this approach enforces phase continuity, SawSing can generate singing voices without the phase-discontinuity glitch of many existing vocoders. Moreover, the source-filter assumption provides an inductive bias that allows SawSing to be trained on a small amount of data. Our evaluation shows that SawSing converges much faster and outperforms state-of-the-art generative adversarial network- and diffusion-based vocoders in a resource-limited scenario with only 3 training recordings and a 3-hour training time.",TWN,education,Developing economies,"[0.047815036, -36.16595]","[-21.113344, -47.001522]","[25.597778, 5.4251194, -15.560201]","[-20.52224, -5.0413785, -19.553545]","[9.484386, 10.788986]","[7.85841, 6.455073]","[10.974219, 14.69743, 0.9780117]","[9.601863, 6.2484694, 8.055847]"
6,Marcel A Vélez Vásquez;John Ashley Burgoyne,Tailed U-Net: Multi-Scale Music Representation Learning,2022,https://doi.org/10.5281/zenodo.7316596,Marcel A. Vélez Vásquez+University of Amsterdam>NLD>education;John Ashley Burgoyne+University of Amsterdam>NLD>education,"Self-supervised learning has steadily been gaining traction in recent years. In music information retrieval (MIR), one promising recent application of self-supervised learning is the CLMR framework (contrastive learning of musical representations). CLMR has shown good performance, achieving results on par with state-of-the-art end-to-end classification models, but it is strictly an encoding framework. It suffers the characteristic limitation of any encoder that it cannot explicitly combine multi-timescale information, whereas a characteristic feature of human audio perception is that we tend to perceive all frequencies simultaneously. To this end, we propose a generalization of CLMR that learns to extract and explicitly combine representations across different frequency resolutions, which we coin the tailed U-Net (TUNe). TUNe architectures combine multi-timescale information during a decoding phase, similar to U-Net architectures used in computer vision and source separation, but have a tail added to reduce sample-level information to a smaller pre-defined number of representation dimensions. The size of the decoding phase is a hyperparameter, and in the case of a zero-layer decoding phase, TUNe reduces to CLMR. The best TUNe architectures, however, require less training time to match CLMR performance, have superior transfer learning performance, and are competitive with state-of-the-art models even at dramatically reduced dimensionalities.",NLD,education,Developed economies,"[-14.553692, -5.608611]","[-20.332653, -33.736736]","[3.290985, 9.504391, 11.032995]","[-8.237346, -1.3524349, -14.141887]","[11.403644, 8.866215]","[9.048476, 5.000278]","[13.514304, 13.002518, 0.29682866]","[10.298637, 6.4834967, 8.959191]"
5,Otso Björklund,SIATEC-C: Computationally efficient repeated pattern discovery in polyphonic music,2022,https://doi.org/10.5281/zenodo.7316594,Otso Björklund+University of Helsinki>FIN>education,"The use of point-set representations of music enable repeated pattern discoveryto be performed on polyphonic music. The discovery of patterns containing polyphony is also enabled by the use of point-set representations. The SIA and SIATEC algorithms discover repeated patterns in point-sets bycomputing maximal translatable patterns and their translational equivalence classes.While the algorithms are relatively efficient, their application to larger piecesof music is not viable due to quadratic space complexity.This paper introcudes a novel algorithm, SIATEC-C, for repeated pattern discovery in point-set representations of music. The algorithm discovers repeated patterns and finds all of their occurrences, whilerunning with subquadratic space complexity. The algorithm can also provide significant runningtime improvements over the comparable SIATEC algorithm.The computational performance of the algorithm is compared with SIATEC. The accuracy of the algorithmis also evaluated on the JKU-PDD data set.",FIN,education,Developed economies,"[15.784808, 21.57126]","[5.3571057, 17.188213]","[-2.4495482, -12.541651, 7.814072]","[1.7882315, -11.087213, 4.066309]","[11.564417, 7.6430454]","[8.891579, 1.1531018]","[12.608405, 13.225551, -0.55669326]","[10.533044, 6.721858, 12.932123]"
4,Anup Singh;Kris Demuynck;Vipul Arora,Attention-based audio embeddings for query-by-example,2022,https://doi.org/10.5281/zenodo.7316592,Anup Singh+imec - Ghent University>BEL>education|Indian Institute of Technology Kanpur>IND>education;Kris Demuynck+imec - Ghent University>BEL>education;Vipul Arora+Indian Institute of Technology Kanpur>IND>education,"An ideal audio retrieval system efficiently and robustly recognizes a short query snippet from an extensive database. However, the performance of well-known audio fingerprinting systems falls short at high signal distortion levels. This paper presents an audio retrieval system that generates noise and reverberation robust audio fingerprints using the contrastive learning framework. Using these fingerprints, the method performs a comprehensive search to identify the query audio and precisely estimate its timestamp in the reference audio. Our framework involves training a CNN to maximize the similarity between pairs of embeddings extracted from clean audio and its corresponding distorted and time-shifted version. We employ a channel-wise spectral-temporal attention mechanism to capture salient time indices and spectral bands in the CNN features. The attention mechanism enables the CNN to better discriminate the audio by giving more weight to the salient spectral-temporal patches in the signal. Experimental results indicate that our system is efficient in computation and memory usage while being more accurate, particularly at higher distortion levels, than competing state-of-the-art systems and scalable to a larger database.",BEL,education,Developed economies,"[-22.433485, 3.7876542]","[-23.903135, -32.027264]","[-3.3967524, 16.423117, -11.581012]","[-11.364041, -14.439875, -17.625593]","[13.120334, 8.447395]","[8.451401, 5.2703466]","[13.417843, 14.547157, -1.4681312]","[9.806194, 6.6119585, 8.82524]"
55,Igor Vatolkin;Cory Mckay,Stability of Symbolic Feature Group Importance in the Context of Multi-Modal Music Classification,2022,https://doi.org/10.5281/zenodo.7316702,Igor Vatolkin+TU Dortmund University>DEU>education;Cory McKay+Marianopolis College>CAN>education,"Multi-modal music classification creates supervised models trained on features from different sources (modalities): the audio signal, the score, lyrics, album covers, expert tags, etc. A concept of ""multi-group feature importance"" not only helps to measure the individual relevance of features of a feature type under investigation (such as the instruments present in a piece), but also serves to quantify the potential for further improving classification by adding features from other feature types or extracted from different kinds of sources, based on a multi-objective analysis of feature sets after evolutionary feature selection. In this study, we investigate the stability of feature group importance when different classification methods and different measures of classification quality are applied. Since musical scores are particularly helpful in deriving semantically meaningful, robust genre characteristics, we focus on the feature groups analyzed by the jSymbolic feature extraction software, which describe properties associated with instrumentation, basic pitch statistics, melody, chords, tempo, and other rhythmic aspects. These symbolic features are analyzed in the context of musical information drawn from five other modalities, and experiments are conducted involving two datasets, one small and one large. The results show that, although some feature groups can remain similarly important compared to others, differences can also be evident in various applications, and can depend on the particular classifier and evaluation measure being used. Insights drawn from this type of analysis can potentially be helpful in effectively matching specific features or feature groups to particular classifiers and evaluation measures in future feature-based MIR research.",DEU,education,Developed economies,"[-26.194534, -9.046331]","[14.814098, -4.663154]","[-7.2342634, -3.603426, 12.924097]","[9.211513, 7.122014, -2.9957051]","[12.680761, 10.496766]","[9.321328, 3.012825]","[13.65538, 13.88156, 0.95503265]","[11.085128, 6.944834, 10.951587]"
2,Matan Gover;Oded Zewi,Music Translation: Generating Piano Arrangements in Different Playing Levels,2022,https://doi.org/10.5281/zenodo.7316588,Matan Gover+Simply>USA>company;Oded Zewi+Simply>USA>company,"We present a novel task of playing level conversion: generating a music arrangement in a target difficulty level, given another arrangement of the same musical piece in a different level. For this task, we create a parallel dataset of piano arrangements in two strictly well-defined playing levels, annotated at individual phrase resolution, taken from the song catalog of a piano learning app.In a series of experiments, we train models that successfully modify the playing level while preserving the musical 'essence'. We further show, via an ablation study, the contributions of specific data representation and augmentation techniques to the model's performance.In order to evaluate the performance of our models, we conduct a human evaluation study with expert musicians. The evaluation shows that our best model creates arrangements that are almost as good as ground truth examples. Additionally, we propose MuTE, an automated evaluation metric for music translation tasks, and show that it correlates with human ratings.",USA,company,Developed economies,"[32.235966, 2.4137728]","[-37.24544, 4.979085]","[11.247788, -2.034782, 21.3159]","[-11.083751, -10.546518, -6.288069]","[10.171951, 7.5008435]","[7.743731, 4.0625005]","[12.710426, 11.662562, -0.21819721]","[9.277858, 6.0234656, 10.123978]"
1,Tsung-Ping Chen;Li Su,Toward postprocessing-free neural networks for joint beat and downbeat estimation,2022,https://doi.org/10.5281/zenodo.7342730,"Tsung-Ping Chen+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown;Li Su+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown","Recent deep learning-based models for estimating beats and downbeats are mainly composed of three successive stages---feature extraction, sequence modeling, and post processing. While such a framework is prevalent in the scenario of sequence labeling tasks and yields promising results in beat and downbeat estimations, it also indicates a shortage of the employed neural networks, given that the post-processing usually provides a notable performance gain over the previous stage. Moreover, the assumption often made for the post-processing is not suitable for many musical pieces. In this work, we attempt to improve the performance of joint beat and downbeat estimation without incorporating the post-processing stage. By inspecting a state-of-the-art approach, we propose reformulations regarding the network architecture and the loss function. We evaluate our model on various music data and show that the proposed methods are capable of improving the baseline approach without the aid of a post-processing stage.",TWN,education,Developing economies,"[36.670834, -34.778778]","[-31.434, -14.100263]","[8.93701, -28.13379, -3.112537]","[-7.1920304, 13.431048, -18.316055]","[10.50156, 4.1515203]","[4.9875283, 2.6008172]","[10.124805, 12.867158, -2.2634966]","[7.9194555, 6.9140034, 9.934041]"
0,Yixiao Zhang;Junyan Jiang;Gus Xia;Simon Dixon,Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model,2022,https://doi.org/10.5281/zenodo.7316584,"Yixiao Zhang+Centre for Digital Music, Queen Mary University of London>GBR>education;Junyan Jiang+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>ARE>education;Gus Xia+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>ARE>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","Lyric interpretations can help people understand songs and their lyrics quickly, and can also make it easier to manage, retrieve and discover songs efficiently from the growing mass of music archives. In this paper we propose BART-fusion, a novel model for generating lyrics interpretations from lyrics and music audio that combines a large-scale pre-trained language model with an audio encoder. We employ a cross-modal attention module to incorporate the audio representation into the lyrics representation to help the pre-trained language model understand the song from an audio perspective, while preserving the language model's original generative performance. We also release the Song Interpretation Dataset, a new large-scale dataset for training and evaluating our model. Experimental results show that the additional audio information helps our model to understand words and music better, and to generate precise and fluent interpretations. An additional experiment on cross-modal music retrieval shows that interpretations generated by BART-fusion can also help people retrieve music more accurately than with the original BART.",GBR,education,Developed economies,"[-26.06371, -32.35965]","[-10.187722, -27.503035]","[13.599405, 23.099869, -1.7867451]","[2.1297314, 0.6375499, -22.679855]","[11.208055, 11.81031]","[9.493027, 4.939181]","[12.2567005, 15.804909, 1.13478]","[10.837064, 6.4698954, 9.279023]"
25,Antonio Ríos-Vila;Jose M. Inesta;Jorge Calvo-Zaragoza,End-to-End Full-Page Optical Music Recognition for Mensural Notation,2022,https://doi.org/10.5281/zenodo.7342678,Antonio Ríos-Vila+University of Alicante>ESP>education;José M. Iñesta+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"Optical Music Recognition (OMR) systems typically consider workflows that include several steps, such as staff detection, symbol recognition, and semantic reconstruction. However, fine-tuning these systems is costly due to the specific data labeling process that has to be performed to train models for each of these steps. In this paper, we present the first segmentation-free full-page OMR system that receives a page image and directly outputs the transcription in a single step. This model requires only the annotations of full score pages, which greatly alleviates the task of manual labeling. The model has been tested with early music written in mensural notation, for which the presented approach is especially beneficial. Results show that this methodology provides a solution with promising results and establishes a new line of research for holistic transcription of music score pages.",ESP,education,Developed economies,"[41.7228, 21.329971]","[-21.915604, 39.199287]","[21.435919, 13.556988, 13.799599]","[-13.27778, -22.439331, -1.074496]","[8.641377, 6.0960393]","[6.623637, -0.8260824]","[10.613305, 11.021438, -0.22241594]","[7.8588753, 4.068852, 10.531803]"
26,Bruno Di Giorgi;Mark Levy;Richard Sharp,Mel Spectrogram Inversion with Stable Pitch,2022,https://doi.org/10.5281/zenodo.7316638,Bruno Di Giorgi+Apple>USA>company;Mark Levy+Apple>USA>company;Richard Sharp+Apple>USA>company,"Vocoders are models capable of transforming a low-dimensional spectral representation of an audio signal, typically the mel spectrogram, to a waveform. Modern speech generation pipelines use a vocoder as their final component. Recent vocoder models developed for speech achieve high degree of realism, such that it is natural to wonder how they would perform on music signals. Compared to speech, the heterogeneity and structure of the musical sound texture offers new challenges. In this work we focus on one specific artifact that some vocoder models designed for speech tend to exhibit when applied to music: the perceived instability of pitch when synthesizing sustained notes. We argue that the characteristic sound of this artifact is due to the lack of horizontal phase coherence, which is often the result of using a time-domain target space with a model that is invariant to time-shifts, such as a convolutional neural network. We propose a new vocoder model that is specifically designed for music. Key to improving the pitch stability is the choice of a shift-invariant target space that consists of the magnitude spectrum and the phase gradient. We discuss the reasons that inspired us to re-formulate the vocoder task, outline a working example, and evaluate it on musical signals. Our method results in 60% and 10% improved reconstruction of sustained notes and chords with respect to existing models, using a novel harmonic error metric.",USA,company,Developed economies,"[14.396702, -29.49826]","[-21.272203, -46.66781]","[24.52874, -2.6407871, 5.946754]","[-20.041697, -6.245984, -19.63374]","[9.173607, 8.62938]","[7.8665767, 6.479436]","[11.549455, 13.121939, 0.16482459]","[9.590368, 6.2462306, 8.055817]"
3,Ian Simon;Joshua Gardner;Curtis Hawthorne;Ethan Manilow;Jesse Engel,Scaling Polyphonic Transcription with Mixtures of Monophonic Transcriptions,2022,https://doi.org/10.5281/zenodo.7316590,Ian Simon+Google Research>USA>company;Josh Gardner+Google Research>USA>company;Curtis Hawthorne+Google Research>USA>company;Ethan Manilow+Google Research>USA>company;Jesse Engel+Google Research>USA>company,"Automatic Music Transcription (AMT), in particular the problem of automatically extracting notes from audio, has seen much recent progress via the training of neural network models on musical audio recordings paired with aligned ground-truth note labels.  However, progress is currently limited by the difficulty of obtaining such note labels for natural audio recordings at scale.  In this paper, we take advantage of the fact that for monophonic music, the transcription problem is much easier and largely solved via modern pitch-tracking methods.  Specifically, we show that we are able to combine recordings of real monophonic music (and their transcriptions) into artificial and musically-incoherent mixtures, greatly increasing the scale of labeled training data.  By pretraining on these mixtures, we can use a larger neural network model and significantly improve upon the state of the art in multi-instrument polyphonic transcription.  We demonstrate this improvement across a variety of datasets and in a ``zero-shot'' setting where the model has not been trained on any data from the evaluation domain.",USA,company,Developed economies,"[30.88179, -9.636175]","[-26.760939, -28.085207]","[10.395406, -3.642963, 9.082533]","[-8.666288, -7.6794143, -13.969863]","[9.47069, 7.907687]","[7.7447767, 5.100733]","[11.8713045, 12.283283, 0.18845537]","[9.228945, 6.9527297, 9.11156]"
28,Li Yi;Haochen Hu;Jingwei Zhao;Gus Xia,AccoMontage2: A Complete Harmonization and Accompaniment Arrangement System,2022,https://doi.org/10.5281/zenodo.7316642,"Li Yi+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>ARE>education;Haochen Hu+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>ARE>education;Jingwei Zhao+Institute of Data Science, NUS>SGP>education;Gus Xia+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>ARE>education","We propose AccoMontage2, a system capable of doing full-length song harmonization and accompaniment arrangement based on a lead melody. Following AccoMontage, this study focuses on generating piano arrangements for popular/folk songs and it carries on the generalized template-based retrieval method. The novelties of this study are twofold. First, we invent a harmonization module (which AccoMontage does not have). This module generates structured and coherent full-length chord progression by optimizing and balancing three loss terms: a micro-level loss for note-wise dissonance, a meso-level loss for phrase-template matching, and a macro-level loss for full piece coherency. Second, we develop a graphical user interface which allows users to select different styles of chord progression and piano texture. Currently, chord progression styles include Pop, R&amp;B, and Dark, while piano texture styles include several levels of voicing density and rhythmic complexity. Experimental results show that both our harmonization and arrangement results significantly outperform the baselines. Lastly, we release AccoMontage2 as an online application as well as the organized chord progression templates as a public dataset.",CHN,education,Developing economies,"[15.529926, 0.20856245]","[-2.6769857, -40.0288]","[7.465051, 11.798848, 25.810535]","[-24.620796, 4.740485, -3.7535565]","[10.553311, 8.784725]","[9.467352, 6.2151947]","[13.512742, 12.139437, -0.1545122]","[9.869727, 5.2863693, 9.5419235]"
27,Xingjian Du;Huidong Liang;Yuan Wan;Yuheng Lin;Ke Chen;Bilei Zhu;Zejun Ma,Latent feature augmentation for chorus detection,2022,https://doi.org/10.5281/zenodo.7316640,Xingjian Du+ByteDance AI Lab>CHN>company|University of California San Diego>USA>education;Huidong Liang+ByteDance AI Lab>CHN>company|University of California San Diego>USA>education;Yuan Wan+ByteDance AI Lab>CHN>company|University of California San Diego>USA>education;Yuheng Lin+ByteDance AI Lab>CHN>company|University of California San Diego>USA>education;Ke Chen+University of California San Diego>USA>education;Bilei Zhu+ByteDance AI Lab>CHN>company;Zejun Ma+ByteDance AI Lab>CHN>company,"In this paper, we introduce LA-Chorus, a chorus detection model based on latent feature augmentation and ResNet FPN architecture. Our contributions in LA-Chorus are three-fold. Firstly, we propose a method for implicitly augmenting chorus data in the latent space during the train7 ing stage. Compared to augmentations on audio surfaces such as time stretching and pitch shifting, latent augmentations indicate changes at a higher level in original audio, thereby increasing the diversity and sufficiency in training. Second, we apply Feature Pyramid Network (FPN) to generate additional embeddings from low dimension to high dimension, consequently achieving a multi-scale training paradigm. Lastly, we release Di-Chorus, a new open-source dataset of diverse genres and languages for the community of music structure analysis. In conjunction with other public datasets, we conduct comprehensive ex18 periments to evaluate the performance of LA-Chorus compared to other state-of-the-art models, which demonstrate the out-performance of LA-Chorus and the effectiveness of proposed latent feature augmentation.",CHN,company,Developing economies,"[-28.519014, -27.729918]","[-24.48577, -34.8872]","[11.774965, 22.50644, -9.962236]","[-4.2302256, -3.6569846, -17.12262]","[11.365406, 11.388944]","[8.755813, 5.00669]","[12.128663, 15.691586, 0.8364855]","[10.122471, 6.809746, 9.199299]"
53,Chen Zhang;Jiaxing Yu;Luchin Chang;Xu Tan;Jiawei Chen;Tao Qin;Kejun Zhang,PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription,2022,https://doi.org/10.5281/zenodo.7316698,Chen Zhang+Zhejiang University>CHN>education;Jiaxing Yu+Zhejiang University>CHN>education;Lu Chin Chang+Zhejiang University>CHN>education;Xu Tan+Microsoft Research Asia>CHN>company;Jiawei Chen+South China University of Technology>CHN>education;Tao Qin+Microsoft Research Asia>CHN>company;Kejun Zhang+Zhejiang University>CHN>education,"Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of the paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.",CHN,education,Developing economies,"[-25.31891, -33.15754]","[-28.871386, -40.04597]","[14.147807, 20.40359, -0.26684916]","[2.1424046, -6.0943575, -21.572504]","[11.130044, 11.732134]","[7.962109, 4.625465]","[12.131282, 15.722257, 1.1693894]","[10.416817, 7.311866, 9.07358]"
52,Huan Zhang;Jingjing Tang;Syed Rm Rafee;Simon Dixon;George Fazekas;Geraint A. Wiggins,ATEPP: A Dataset of Automatically Transcribed Expressive Piano Performance,2022,https://doi.org/10.5281/zenodo.7342764,Huan Zhang+Queen Mary University of London>GBR>education;Jingjing Tang+Queen Mary University of London>GBR>education;Syed Rifat Mahmud Rafee+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education;György Fazekas+Queen Mary University of London>GBR>education,"Computational models of expressive piano performance rely on attributes like tempo, timing, dynamics and pedalling. Despite some promising models for performance assessment and performance rendering, results are limited by the scale, breadth and uniformity of existing datasets. In this paper, we present ATEPP, a dataset that contains 1000 hours of performances of standard piano repertoire by 49 world-renowned pianists, organized and aligned by compositions and movements for comparative studies. Scores in MusicXML format are also available for around half of the tracks. We first evaluate and verify the use of transcribed MIDI for representing expressive performance with a listening evaluation that involves recent transcription models. Then, the process of sourcing and curating the dataset is outlined, including composition entity resolution and a pipeline for audio matching and solo filtering. Finally, we conduct baseline experiments for performer identification and performance rendering on our datasets, demonstrating its potential in generalizing expressive features of individual performing style.",GBR,education,Developed economies,"[31.801584, -2.9303784]","[-33.270027, 5.2139516]","[15.431801, -2.8325582, 19.364922]","[-12.162688, -0.16244936, -3.259053]","[9.953647, 7.2145762]","[7.5611405, 3.7815406]","[12.259207, 11.532982, -0.33020762]","[9.0594225, 6.153142, 10.632286]"
50,Darius Afchar;Romain Hennequin;Vincent Guigue,Learning Unsupervised Hierarchies of Audio Concepts,2022,https://doi.org/10.5281/zenodo.7316692,"Darius Afchar+Deezer Research>FRA>company|MLIA, ISIR - Sorbonne Université - CNRS>FRA>education;Romain Hennequin+Deezer Research>FRA>company|MLIA, ISIR - Sorbonne Université - CNRS>FRA>education;Vincent Guigue+MLIA, ISIR - Sorbonne Université - CNRS>FRA>education","Music signals are difficult to interpret from their low-level features, perhaps even more than images: e.g. highlighting part of a spectrogram or an image is often insufficient to convey high-level ideas that are genuinely relevant to humans. In computer vision, concept learning was therein proposed to adjust explanations to the right abstraction level (e.g. detect clinical concepts from radiographs). These methods have yet to be used for MIR.In this paper, we adapt concept learning to the realm of music, with its particularities. For instance, music concepts are typically non-independent and of mixed nature (e.g. genre, instruments, mood), unlike previous work that assumed disentangled concepts.We propose a method to learn numerous music concepts from audio and then automatically hierarchise them to expose their mutual relationships. We conduct experiments on datasets of playlists from a music streaming service, serving as a few annotated examples for diverse concepts. Evaluations show that the mined hierarchies are aligned with both ground-truth hierarchies of concepts -- when available -- and with proxy sources of concept similarity in the general case.",FRA,company,Developed economies,"[-16.975052, -15.108267]","[-18.687012, -35.203224]","[1.8087282, 16.100313, 9.855415]","[-9.99541, 2.0632913, -14.953744]","[11.55718, 9.150283]","[9.334972, 4.794075]","[13.35968, 13.328607, 0.522966]","[10.537448, 6.283756, 9.294507]"
49,Mahshid Alinoori;Vassilios Tzerpos,Music-STAR: a Style Translation system for Audio-based Re-instrumentation,2022,https://doi.org/10.5281/zenodo.7316690,Mahshid Alinoori+York University>CAN>education|Unknown>Unknown>Unknown;Vassilios Tzerpos+York University>CAN>education|Unknown>Unknown>Unknown,"Music style translation aims to generate variations of existing pieces of music by altering the style-related characteristics of the original piece while content, such as the melody, remains unchanged. These alterations could involve timbre translation, re-harmonization, or music rearrangement. Previous studies have achieved promising results utilizing time-frequency and symbolic music representations. Music style translation on raw audio has also been investigated and applied to single-instrument pieces. Although processing raw audio is more challenging, it provides richer information about timbres, dynamics, and articulations.In this paper, we introduce Music-STAR, the first audio-based translation system that translates the existing instruments in a piece into a set of target instruments without using source separation. To conduct our experiments, we also present an audio dataset that contains two-track pieces performed by two instrument sets alongside their stems. We carry out subjective and objective evaluations to compare Music-STAR with a variety of baseline methods and demonstrate its superiority.",CAN,education,Developed economies,"[19.969456, 13.562693]","[-23.487373, -19.533478]","[-2.257101, -8.039283, 22.977983]","[-10.748755, 0.12647218, -6.1500506]","[11.234466, 7.4298663]","[7.2698054, 5.585426]","[13.398189, 12.239975, -0.5894724]","[9.5961685, 7.6292806, 9.780205]"
48,Marco A Martinez Ramirez;Weihsiang Liao;Chihiro Nagashima;Giorgio Fabbro;Stefan Uhlich;Yuki Mitsufuji,Automatic music mixing with deep learning and out-of-domain data,2022,https://doi.org/10.5281/zenodo.7316688,Marco A. Martínez-Ramírez+Sony Group Corporation>JPN>company;Wei-Hsiang Liao+Sony Group Corporation>JPN>company;Giorgio Fabbro+Sony Group Corporation>JPN>company;Stefan Uhlich+Sony Group Corporation>JPN>company;Chihiro Nagashima+Sony Group Corporation>JPN>company;Yuki Mitsufuji+Sony Group Corporation>JPN>company,"Music mixing traditionally involves recording instruments in the form of clean, individual tracks and blending them into a final mixture using audio effects and expert knowledge (e.g., a mixing engineer). The automation of music production tasks has become an emerging field in recent years, where rule-based methods and machine learning approaches have been explored. Nevertheless, the lack of dry or clean instrument recordings limits the performance of such models, which is still far from professional human-made mixes. We explore whether we can use out-of-domain data such as wet or processed multitrack music recordings and repurpose it to train supervised deep learning models that can bridge the current gap in automatic mixing quality. To achieve this we propose a novel data preprocessing method that allows the models to perform automatic music mixing. We also redesigned a listening test method for evaluating music mixing systems. We validate our results through such subjective tests using highly experienced mixing engineers as participants.",JPN,company,Developed economies,"[16.762081, -37.183895]","[-34.713776, -22.12553]","[29.242376, 10.841411, -12.787133]","[-19.040571, 0.7048491, -23.708574]","[9.8895035, 8.629348]","[7.5176425, 6.1961927]","[12.913774, 12.252966, 0.46445504]","[9.4517, 7.359556, 9.136658]"
47,Sangjun Han;Hyeongrae Ihm;Moontae Lee;Woohyung Lim,Symbolic Music Loop Generation with Neural Discrete Representations,2022,https://doi.org/10.5281/zenodo.7342755,Sangjun Han+LG AI Research>KOR>company;Hyeongrae Ihm+LG AI Research>KOR>company;Moontae Lee+LG AI Research>KOR>company|University of Illinois at Chicago>USA>education;Woohyung Lim+LG AI Research>KOR>company,"Since most of music has repetitive structures from motifs to phrases, repeating musical ideas can be a basic operation for music composition. The basic block that we focus on is conceptualized as loops which are essential ingredients of music. Furthermore, meaningful note patterns can be formed in a finite space, so it is sufficient to represent them with combinations of discrete symbols as done in other domains. In this work, we propose symbolic music loop generation via learning discrete representations. We first extract loops from MIDI datasets using a loop detector and then learn an autoregressive model trained by discrete latent codes of the extracted loops. We show that our model outperforms well-known music generative models in terms of both fidelity and diversity, evaluating on random space. Our code and supplementary materials are available at https://github.com/sjhan91/Loop_VQVAE_Official.",KOR,company,Developing economies,"[20.914852, 4.6549144]","[-9.081033, -42.951385]","[3.9040873, -1.3677652, 20.777802]","[-18.937288, 3.8022234, -13.130506]","[10.169634, 8.47295]","[8.96597, 6.469074]","[13.242667, 11.8340435, 0.06676133]","[9.525188, 5.4748316, 8.665088]"
46,Lele Liu;Qiuqiang Kong;Veronica Morfi;Emmanouil Benetos,Performance MIDI-to-score conversion by neural beat tracking,2022,https://doi.org/10.5281/zenodo.7316682,"Lele Liu+Centre for Digital Music, Queen Mary University of London>GBR>education|The Alan Turing Institute>GBR>Unknown;Qiuqiang Kong+ByteDance Shanghai>CHN>company;Veronica Morfi+Centre for Digital Music, Queen Mary University of London>GBR>education|The Alan Turing Institute>GBR>Unknown;Emmanouil Benetos+Centre for Digital Music, Queen Mary University of London>GBR>education|The Alan Turing Institute>GBR>Unknown","Rhythm quantisation is an essential part of converting performance MIDI recordings into musical scores. Previous works on rhythm quantisation are limited to the use of probabilistic or statistical methods. In this paper, we propose a MIDI-to-score quantisation method using a convolutional-recurrent neural network (CRNN) trained on MIDI note sequences to predict whether notes are on beats. Then, we expand the CRNN model to predict the quantised times for all beat and non-beat notes. Furthermore, we enable the model to predict the key signatures, time signatures, and hand parts of all notes. Our proposed performance MIDI-to-score system achieves significantly better performance compared to commercial software evaluated on the MV2H metric. We release the toolbox for converting performance MIDI into MIDI scores at: https://github.com/cheriell/PM2S .",GBR,education,Developed economies,"[36.86338, -2.593997]","[-15.8357935, -24.802736]","[12.6058655, -9.599602, 22.329943]","[-11.600266, 4.1071243, -8.283313]","[10.15878, 6.6078215]","[8.308222, 5.370844]","[12.200709, 11.433397, -0.69043607]","[8.983176, 6.362092, 9.506604]"
45,Yuya Yamamoto;Juhan Nam;Hiroko Terasawa,Analysis and detection of singing techniques in repertoires of J-POP solo singers,2022,https://doi.org/10.5281/zenodo.7316680,Yuya Yamamoto+University of Tsukuba>JPN>education;Juhan Nam+KAIST>KOR>education;Hiroko Terasawa+University of Tsukuba>JPN>education,"In this paper, we focus on singing techniques within the scope of music information retrieval research. We investigate how singers use singing techniques using real-world recordings of famous solo singers in Japanese popular music songs (J-POP). First, we built a new dataset of singing techniques. The dataset consists of 168 commercial J-POP songs, and each song is annotated using various singing techniques with timestamps and vocal pitch contours. We also present descriptive statistics of singing techniques on the dataset to clarify what and how often singing techniques appear. We further explored the difficulty of the automatic detection of singing techniques using previously proposed machine learning techniques. In the detection, we also investigate the effectiveness of auxiliary information (i.e., pitch and distribution of label duration), not only providing the baseline. The best result achieves 40.4% at macro-average F-measure on nine-way multi-class detection. We provide the annotation of the dataset and its detail on the appendix website.",JPN,education,Developed economies,"[-9.764332, -35.33197]","[8.551111, -23.675404]","[17.30717, 12.142619, -17.707098]","[9.310376, -2.102339, -19.544651]","[10.107747, 11.246411]","[8.656181, 3.550207]","[11.301261, 15.356604, 0.62239647]","[10.784017, 7.5194764, 9.954569]"
44,Charilaos Papaioannou;Ioannis Valiantzas;Theodore Giannakopoulos;Maximos Kaliakatsos-Papakostas;Alexandros Potamianos,A Dataset for Greek Traditional and Folk Music: Lyra,2022,https://doi.org/10.5281/zenodo.7316678,Charilaos Papaioannou+National Technical University of Athens>GRC>education|National and Kapodistrian University Of Athens>GRC>education|National Center for Scientific Research - Demokritos>GRC>facility|Athena RC>GRC>facility;Ioannis Valiantzas+National Technical University of Athens>GRC>education|National and Kapodistrian University Of Athens>GRC>education|National Center for Scientific Research - Demokritos>GRC>facility|Athena RC>GRC>facility;Theodoros Giannakopoulos+National Technical University of Athens>GRC>education|National and Kapodistrian University Of Athens>GRC>education|National Center for Scientific Research - Demokritos>GRC>facility|Athena RC>GRC>facility;Maximos Kaliakatsos-Papakostas+National Technical University of Athens>GRC>education|National and Kapodistrian University Of Athens>GRC>education|National Center for Scientific Research - Demokritos>GRC>facility|Athena RC>GRC>facility;Alexandros Potamianos+National Technical University of Athens>GRC>education|National and Kapodistrian University Of Athens>GRC>education|National Center for Scientific Research - Demokritos>GRC>facility|Athena RC>GRC>facility,"Studying under-represented music traditions under the MIR scope is crucial, not only for developing novel analysis tools, but also for unveiling musical functions that might prove useful in studying world musics. This paper presents a dataset for Greek Traditional and Folk music that includes 1570 pieces, summing in around 80 hours of data. The dataset incorporates YouTube timestamped links for retrieving audio and video, along with rich metadata information with regards to instrumentation, geography and genre, among others. The content has been collected from a Greek documentary series that is available online, where academics present music traditions of Greece with live music and dance performance during the show, along with discussions about social, cultural and musicological aspects of the presented music. Therefore, this procedure has resulted in a significant wealth of descriptions regarding a variety of aspects, such as musical genre, places of origin and musical instruments. In addition, the audio recordings were performed under strict production-level specifications, in terms of recording equipment, leading to very clean and homogeneous audio content. In this work, apart from presenting the dataset in detail, we propose a baseline deep-learning classification approach to recognize the involved musicological attributes. The dataset, the baseline classification methods and the models are provided in public repositories. Future directions for further refining the dataset are also discussed.",GRC,education,Developed economies,"[-25.520363, -3.8876185]","[-9.5987, 13.31845]","[-17.580568, -9.2714, -6.9349475]","[-13.176222, 5.9511356, 4.681876]","[12.534757, 7.68084]","[9.4488325, 4.315817]","[13.475709, 13.829027, -1.4758282]","[10.439384, 6.331984, 10.109918]"
43,Kaustuv Kanti Ganguli;Sertan Şentürk;Carlos Guedes,Critiquing Task- versus Goal-oriented Approaches: A Case for Makam Recognition,2022,https://doi.org/10.5281/zenodo.7316676,Kaustuv Kanti Ganguli+Zayed University>ARE>education;Sertan Şentürk+Independent Researcher>GBR>Unknown;Carlos Guedes+New York University Abu Dhabi>ARE>education,"Computational Musicology and Music Information Retrieval (MIR) address the core musical question under study from a different perspective, often a combination of top-down vs. bottom-up approaches. However, the evaluation metrics for MIR tend to capture the model accuracy in terms of the goal. For instance, mode (melodic framework) recognition is implemented with a goal to evaluate and compare melodic analysis approaches, but it is worth investigating if at all it lends itself as one befitting proxy task. In this work, we aim to review whether the model actually learns the task it is intended for. This is particularly relevant in non-Eurogenetic music repertoires where the grammatical rules are rather prescriptive. We employ methodologies that combine domain-knowledge and data-driven optimizations as a possible way for a comprehensive understanding of these relationships. This is tested on Makam which is one of the understudied corpora in MIR. We evaluate an array of feature-engineering methods on the largest mode recognition dataset curated for Ottoman-Turkish makam music, composed of 1000 recordings in 50 makams. We adapted the time-delayed melody surfaces (TDMS) feature, which in combination with support vector machine (SVM) classifier yields 77.2% recognition accuracy, comparable to the current state-of-the-art. We also address (ethno)musicology-driven tasks with a view to gathering deeper insights into this music, such as tuning, intonation, and melodic similarity. We aim to propose avenues to extend the study to makam characterization over the mere goal of recognizing the mode, to better understand the (dis)similarity space and other plausible musically interesting facets.",ARE,education,Developing economies,"[20.250694, 40.83088]","[7.992101, -4.0110483]","[-5.680133, -33.953094, -0.74906313]","[-0.64512384, 7.5149527, 2.0266335]","[11.777723, 4.561046]","[8.749357, 2.327181]","[11.114349, 13.163825, -2.803088]","[10.13179, 6.857418, 11.721966]"
41,David Gillman;Atalay Kutlay;Uday Goyat,Teach Yourself Georgian Folk Songs Dataset: A Annotated Corpus Of Traditional Vocal Polyphony,2022,https://doi.org/10.5281/zenodo.7316672,David Gillman+New College of Florida>USA>education|Georgia Institute of Technology>USA>education;Uday Goyat+Georgia Institute of Technology>USA>education;Atalay Kutlay+New College of Florida>USA>education,"New datasets of non-Western traditional music contribute to the development of knowledge in MIR and allow computational techniques to inform ethnomusicology. We present an annotated dataset of traditional vocal polyphony from two regions of the Republic of Georgia with disparate musical characteristics. The audio for each song consists of four polyphonic recordings of one performance from different microphones. We present a process and workflow that we use to annotate the dataset, which takes advantage of the salience of individual voices in each recording. The process results in an $f_0$ estimate for each vocal part.",USA,education,Developed economies,"[-25.580126, -3.958838]","[-11.826556, 28.960352]","[19.289467, 12.677543, -4.432856]","[-4.7520633, 12.236452, 9.586178]","[10.3370075, 10.765892]","[7.9412847, 2.427486]","[11.700138, 14.842731, 0.40861446]","[9.931836, 6.4018536, 10.829017]"
51,Massimo Quadrana;Antoine Larreche-Mouly;Matthias Mauch,Multi-objective Hyper-parameter Optimization of Behavioral Song Embeddings,2022,https://doi.org/10.5281/zenodo.7316694,Massimo Quadrana+Apple>USA>company;Antoine Larreche-Mouly+Apple>USA>company;Matthias Mauch+Apple>USA>company,"Song embeddings are a key component of most music recommendation engines. In this work, we study the hyper-parameter optimization of behavioral song embeddings based on Word2Vec on a selection of downstream tasks, namely next-song recommendation, false neighbor rejection, and artist and genre clustering. We present new optimization objectives and metrics to monitor the effects of hyper-parameter optimization. We show that single-objective optimization can cause side effects on the non optimized metrics and propose a simple multi-objective optimization to mitigate these effects.We find that next-song recommendation quality of Word2Vec is anti-correlated with song popularity, and we show how song embedding optimization can balance performance across different popularity levels.We then show potential positive downstream effects on the task of play prediction.Finally, we provide useful insights on the effects of training dataset scale by testing hyper-parameter optimization on an industry-scale dataset.",USA,company,Developed economies,"[-8.2618475, -0.7836822]","[36.20681, 12.219615]","[3.0692956, 15.6549835, 4.07212]","[22.454796, 4.3111157, 16.630688]","[11.983292, 8.942024]","[12.192438, 2.237171]","[13.2822895, 14.37314, -0.16507535]","[13.342001, 5.659844, 12.484078]"
39,Thomas Nuttall;Genís Plaja-Roglans;Lara Pearson;Xavier Serra,In Search of Sañcāras: Tradition-informed Repeated Melodic Pattern Recognition in Carnatic Music,2022,https://doi.org/10.5281/zenodo.7316666,"Thomas Nuttall+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Max Planck Institute for Empirical Aesthetics>DEU>facility;Genís Plaja-Roglans+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Max Planck Institute for Empirical Aesthetics>DEU>facility;Lara Pearson+Max Planck Institute for Empirical Aesthetics>DEU>facility;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Carnatic Music is a South Indian art and devotional music practice in which melodic patterns (motifs and phrases), known as sañcāras, play a crucial structural and expressive role. We demonstrate how the combination of transposition invariant features learnt by a Complex Autoencoder (CAE) and predominant pitch tracks extracted using a Frequency-Temporal Attention Network (FTA-Net) can be used to annotate and group regions of variable-length, repeated, melodic patterns in audio recordings of multiple Carnatic Music performances. These models are trained on novel/expert-curated datasets of hundreds of Carnatic audio recordings and the extraction process tailored to account for the unique characteristics of sañcāras in Carnatic Music. Experimental results show that the proposed method is able to identify 54% of all sañcaras annotated by a professional Carnatic vocalist. Code to reproduce and interact with these results is available online.",ESP,education,Developed economies,"[6.8757906, -0.12449418]","[5.0032973, -18.280003]","[7.45691, 3.1863024, -13.597173]","[15.643026, -11.401741, -6.420862]","[11.245822, 10.302098]","[7.5268335, 1.2115953]","[11.765922, 15.220024, -1.3022423]","[9.011481, 7.1006217, 12.584687]"
40,Zhaowen Wang;Mingjin Che;Yue Yang;Wen Wu Meng;Qinyu Li;Fan Xia;Wei Li,Automatic Chinese National Pentatonic Modes Recognition Using Convolutional Neural Network,2022,https://doi.org/10.5281/zenodo.7316670,Zhaowen Wang+Central Conservatory of Music>CHN>education;Mingjin Che+Sichuan Conservatory of Music>CHN>education;Yue Yang+Central Conservatory of Music>CHN>education;Wenwu Meng+Sichuan Conservatory of Music>CHN>education;Qinyu Li+Sichuan Conservatory of Music>CHN>education;Fan Xia+Sichuan Conservatory of Music>CHN>education;Wei Li+Fudan University>CHN>education|Shanghai Key Laboratory of Intelligent Information Processing>CHN>facility,"Chinese national pentatonic modes, with five tones of Gong, Shang, Jue, Zhi and Yu as the core, play an essential role in traditional Chinese music culture. After the early twentieth century, with the development of new Chinese music, the ancient Chinese theory of scales gradually developed into a new pentatonic modes theory under the influence of western music. In this paper, we briefly introduce our self-built CNPM (Chinese National Pentatonic Modes) Dataset, then design residual convolutional neural network models to identify which TongGong system the mode belongs, the pitch of tonic, the mode pattern and the mode type from audio signals, in combination with musical domain knowledge. We use both single-task and multi-task models with three strategies for identification, and compare them with a simple template-based baseline method. In experiments, we use seven accuracy metrics to evaluate the models. The results on identifying both the tonic pitch and the pattern of mode correctly achieve an average accuracy of 69.65%. As an initial research on automatic Chinese national pentatonic modes recognition, this work will contribute to the development of multicultural music information retrieval, computational ethnomusicology and five-tone music therapy.",CHN,education,Developing economies,"[51.06331, -23.514746]","[-3.2522721, -22.99983]","[13.399772, -28.557556, 4.8864675]","[-6.737138, 6.6094418, -20.974148]","[9.966613, 5.5263605]","[8.460952, 2.3752978]","[10.740491, 12.773296, -0.94291353]","[9.82876, 6.7887087, 11.427772]"
30,Rishabh A Dahale;Vaibhav Vinayak Talwadker;Preeti Rao;Prateek Verma,Generating Coherent Drum Accompaniment with Fills and Improvisations,2022,https://doi.org/10.5281/zenodo.7316646,Rishabh Dahale+Indian Institute of Technology Bombay>IND>education;Vaibhav Talwadker+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education;Prateek Verma+Stanford University>USA>education,"Creating a complex work of art like music necessitates profound creativity. With recent advancements in Deep Learning and powerful models such as Transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite fills and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with fills/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments – Piano, Guitar, Bass and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to find that improvisation is largely absent, attributed possibly to its relatively low representation in the training data. We propose a novelty function that represents the extent of improvisation in a specific bar relative to its neighbors. We train a model to detect improvisation positions from the melodic accompaniment tracks. Finally, we use a novel BERT inspired in-filling architecture, to learn the structure of both the drums and melody to in-fill elements of improvised music.",IND,education,Developing economies,"[23.922148, -41.551643]","[-6.4964123, -43.49553]","[17.894245, -14.450979, 1.8468847]","[-21.758213, 5.986906, -9.324875]","[8.052597, 7.3961053]","[9.133746, 6.4108205]","[10.5296755, 11.694001, 0.92748123]","[9.416888, 5.4516497, 8.88677]"
31,Alia Morsi;Xavier Serra,Bottlenecks and solutions for audio to score alignment research,2022,https://doi.org/10.5281/zenodo.7343047,Alia Morsi+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education|Music Technology Group>ESP>education,"<p>Although audio to score alignment is a classic Music Information Retrieval problem, it has not been defined uniquely with the scope of musical scenarios representing its core. The absence of a unified vision makes it difficult to pinpoint its state-of-the-art and determine directions for improvement. To get past this bottleneck, it is necessary to consolidate datasets and evaluation methodologies to allow comprehensive benchmarking. In our review of prior work, we demonstrate the extent of variation in problem scope, datasets, and evaluation practices across audio to score alignment research. To circumvent the high cost of creating large-scale datasets with various instruments, styles, performance conditions, and musician proficiency from scratch, the research community could generate ground truth approximations from non-audio to score alignment datasets which include a temporal mapping between a music score and its corresponding audio. We show a methodology for adapting the Aligned Scores and Performances dataset, created originally for beat tracking and music transcription. We filter the dataset semi- automatically by applying a set of Dynamic Time Warping based Audio to Score Alignment methods using out-of-the-box Chroma and Constant-Q Transform extraction algorithms, suitable for the characteristics of the piano performances of the dataset. We use the results to discuss the limitations of the generated ground truths and data adaptation method. While the adapted dataset does not provide the necessary diversity for solving the initial problem, we conclude with ideas for expansion, and identify future directions for curating more comprehensive datasets through data adaptation, or synthesis.</p>",ESP,education,Developed economies,"[19.577724, -16.145441]","[-19.770346, -16.32758]","[2.4022534, -16.36618, -13.517802]","[-2.6871083, -22.82535, -5.7812514]","[10.88513, 6.048207]","[6.1115685, 0.6646191]","[11.692437, 12.551641, -1.7523367]","[8.255275, 5.8775454, 10.69827]"
32,Martin Clayton;Preeti Rao;Nithya Shikarpur;Sujoy Roychowdhury;Jin Li,Raga Classification From Vocal Performances Using Multimodal Analysis,2022,https://doi.org/10.5281/zenodo.7316650,Martin Clayton+Durham University>GBR>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education;Nithya Shikarpur+Indian Institute of Technology Bombay>IND>education;Sujoy Roychowdhury+Indian Institute of Technology Bombay>IND>education;Jin Li+Durham University>GBR>education,"<p>Work on musical gesture and embodied cognition suggests a rich complementarity between audio and movement information in musical performance. Pose estimation algorithms now make it possible (in contrast to Motion Capture) to collect rich movement information from unconstrained performances of indefinite length. Vocal performances of Indian art music offer the opportunity to carry out multimodal analysis using this information, combing musician&#39;s body movements (i.e. pose and gesture data) with audio features. In this work we investigate raga identification from 12 s excerpts from a dataset of 3 singers and 9 ragas using the combination of audio and visual representations that are each semantically salient on their own. While gesture based classification is relatively weak by itself, we show that combining latent representations from the pre-trained unimodal networks can surpass the already high performance obtained by audio features.</p>",GBR,education,Developed economies,"[3.4662547, -2.4284832]","[6.398641, -19.516006]","[8.318389, 1.7569417, -20.99126]","[17.021074, -10.553227, -4.1824985]","[11.111478, 10.683209]","[7.4166245, 1.2129686]","[11.489615, 15.13338, -1.6231974]","[9.095291, 7.030355, 12.505205]"
33,Oleg Lesota;Emilia Parada-Cabaleiro;Stefan Brandl;Elisabeth Lex;Navid Rekabsaz;Markus Schedl,Traces of Globalization in Online Music Consumption Patterns and Results of Recommendation Algorithms,2022,https://doi.org/10.5281/zenodo.7316652,"Oleg Lesota+Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Emilia Parada-Cabaleiro+Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Stefan Brandl+Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Elisabeth Lex+Graz University of Technology>AUT>education;Navid Rekabsaz+Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education;Markus Schedl+Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz>AUT>education|Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT)>AUT>education","<p>Music streaming platforms allow users to enjoy music from all over the globe.Such opportunity speeds up cultural exchange between different countries, a process often associated with globalization. While such an exchange could lead to more diverse music consumption, empirical evidence on its influence on online music consumption is limited. Besides, the extent to which music recommender systems foster exchange or amplify globalization in music remains an understudied problem.In this paper, we present findings from an empirical study to detect traces of globalization in domestic vs. foreign online music consumption. Besides, we investigate if popular recommendation algorithms, specifically ItemKNN and NeuMF, are prone to amplifying globalization processes. Our experiments on Last.fm listening data show nuanced patterns of globalization in music consumption. We observe a strong position of US music in all considered countries. In countries such as Sweden, Great Britain, or Brazil, US music shows various levels of coexistence with domestic music. We find that Finland is least influenced by US music, while greatly consuming and &#39;exporting&#39; domestic music. With respect to recommendation algorithms, ItemKNN tends to recommend domestic music to users of many countries, while NeuMF contributes to accelerating globalization and shifting balance towards dominance of US music on the market.</p>",AUT,education,Developed economies,"[-44.186386, 23.807085]","[45.178055, 14.65987]","[-11.501611, 21.188965, -11.777728]","[15.865308, 12.540901, 16.108997]","[15.77552, 9.142336]","[12.796802, 1.9577581]","[15.684026, 15.546107, -1.4062238]","[13.5784235, 5.0791907, 12.219244]"
29,Matthew C Mccallum;Filip Korzeniowski;Sergio Oramas;Fabien Gouyon;Andreas Ehmann,Supervised and Unsupervised Learning of Audio Representations for Music Understanding,2022,https://doi.org/10.5281/zenodo.7316644,Matthew C. McCallum+SiriusXM>USA>company;Filip Korzeniowski+SiriusXM>USA>company;Sergio Oramas+SiriusXM>USA>company;Fabien Gouyon+SiriusXM>USA>company;Andreas F. Ehmann+SiriusXM>USA>company,"In this work, we provide a broad comparative analysis of strategies for pre-training audio understanding models for several tasks in the music domain, including labelling of genre, era, origin, mood, instrumentation, key, pitch, vocal characteristics, tempo and sonority. Specifically, we explore how the domain of pre-training datasets (music or generic audio) and the pre-training methodology (supervised or unsupervised) affects the adequacy of the resulting audio embeddings for downstream tasks.We show that models trained via supervised learning on large-scale expert-annotated music datasets achieve state-of-the-art performance in a wide range of music labelling tasks, each with novel content and vocabularies. This can be done in an efficient manner with models containing less than 100 million parameters that require no fine-tuning or reparameterization for downstream tasks, making this approach practical for industry-scale audio catalogs.Within the class of unsupervised learning strategies, we show that the domain of the training dataset can significantly impact the performance of representations learned by the model. We find that restricting the domain of the pre-training dataset to music allows for training with smaller batch sizes while achieving state-of-the-art in unsupervised learning---and in some cases, supervised learning---for music understanding.We also corroborate that, while achieving state-of-the-art performance on many tasks, supervised learning can cause models to specialize to the supervised information provided, somewhat compromising a model's generality.",USA,company,Developed economies,"[-13.849899, -6.476425]","[-20.810799, -37.32486]","[3.109747, 12.080337, 12.074857]","[-9.226955, -0.8229311, -19.513714]","[11.468889, 9.118327]","[9.48332, 4.964313]","[13.485325, 13.142222, 0.37052667]","[10.635549, 6.4092164, 8.932114]"
35,Polykarpos Polykarpidis;Dionysios Kalofonos;Dimitrios Balageorgos;Christina Anagnostopoulou,Three related corpora in Middle Byzantine music notation and a preliminary comparative analysis,2022,https://doi.org/10.5281/zenodo.7316656,Polykarpos Polykarpidis+National and Kapodistrian University of Athens>GRC>education;Dionysios Kalofonos+Unknown>Unknown>Unknown;Dimitrios Balageorgos+National and Kapodistrian University of Athens>GRC>education;Christina Anagnostopoulou+National and Kapodistrian University of Athens>GRC>education,"The Middle Byzantine notation (MBn) is used to capture the plainchant melodies of eastern Orthodox Christian music from the middle of the 12th century until 1814. In the context of this research, we study the evolution of a subgenre of Byzantine music known as Heirmologic. We present three Heirmologic corpora spanning the periods before, during and after the 16th century. We discuss the challenges we faced during the digitisation process, and the steps we took to overcome them. For the analysis of the three corpora, we apply the three methods, namely notational texture, melodic arch similarity, and Jensen-Shannon distances of Markovian models, the second of which is novel and inspired by the idea of melodic arches. Through these methods, we aim at highlighting the differences of the corpora in order to obtain an outline of the evolution of the subgenre. We observe that the post 16th century Heirmologic pieces are more similar to the 16th century ones, while there is a greater difference with the pre 16th century pieces. This indicates that the 16th century constitutes a turning point in the melodic features of the Heirmologic subgenre.",GRC,education,Developed economies,"[21.469797, 18.382149]","[7.3283544, -1.3863317]","[-8.329519, -14.600112, 21.747843]","[-4.0569444, 9.587968, 2.7840874]","[11.653962, 7.078429]","[8.19741, 1.5553174]","[13.251532, 12.2628145, -1.4626223]","[9.772867, 6.7399836, 12.348459]"
36,Dichucheng Li;Yulun Wu;Qinyu Li;Jiahao Zhao;Yi Yu;Fan Xia;Wei Li,Playing Technique Detection by Fusing Note Onset Information in Guzheng Performance,2022,https://doi.org/10.5281/zenodo.7316658,Dichucheng Li+Fudan University>CHN>education;Yulun Wu+Fudan University>CHN>education;Qinyu Li+Sichuan Conservatory of Music>CHN>education;Jiahao Zhao+Fudan University>CHN>education;Yi Yu+National Institute of Informatics (NII)>JPN>education;Fan Xia+Sichuan Conservatory of Music>CHN>education;Wei Li+Fudan University>CHN>education|Shanghai Key Laboratory of Intelligent Information Processing>CHN>facility,"The Guzheng is a kind of traditional Chinese instruments with diverse playing techniques. Instrument playing techniques (IPT) play an important role in musical performance. However, most of the existing works for IPT detection show low efficiency for variable-length audio and provide no assurance in the generalization as they rely on a single sound bank for training and testing. In this study, we propose an end-to-end Guzheng playing technique detection system using Fully Convolutional Networks that can be applied to variable-length audio. Because each Guzheng playing technique is applied to a note, a dedicated onset detector is trained to divide an audio into several notes and its predictions are fused with frame-wise IPT predictions. During fusion, we add the IPT predictions frame by frame inside each note and get the IPT with the highest probability within each note as the final output of that note. We create a new dataset named GZ_IsoTech from multiple sound banks and real-world recordings for Guzheng performance analysis. Our approach achieves 87.97% in frame-level accuracy and 80.76% in note-level F1-score, outperforming existing works by a large margin, which indicates the effectiveness of our proposed method in IPT detection.",CHN,education,Developing economies,"[48.98035, -26.49679]","[-33.614758, -16.632185]","[10.992119, -21.054712, -0.8913575]","[-5.387415, 8.02625, -18.78371]","[10.186524, 5.2412763]","[7.8309717, 4.763619]","[10.565519, 12.928866, -1.3851299]","[8.900501, 6.989919, 9.502996]"
37,Babak Nikzat;Rafael Caro Repetto,KDC: an open corpus for computational research of dastgāhi music,2022,https://doi.org/10.5281/zenodo.7316660,Babak Nikzat+Kunstuniversität Graz>AUT>education;Rafael Caro Repetto+Kunstuniversität Graz>AUT>education,"Iranian dastgāhi music is considered as the classical repertory of contemporary Iran. In the 19th century, the melodic modes that developed during its long history were grouped in categories, each of them known as dastgāh. The dastgāhi system presents unique features, that have been object of musicological study since its inception. However, computational methods for its research are still scarce, due in good part to the lack of open, well curated corpora. The aim of the KUG Dastgāhi Corpus (KDC) is to contribute to the development of computational corpus driven research for this tradition. KDC is created following the FAIR principles, and in close collaboration with performers and scholars, who contribute to it with annotations and qualitative evaluations. Besides presenting the first version of KDC, in this paper we explore the possibilities that Iranian dastgāhi music offers to computational research. In order to test the performance of state-of-the-art technologies applied to this music tradition, we present preliminary results for several analytical tasks, and discuss thei opportunities and limitations learnt in the process.",AUT,education,Developed economies,"[-19.54994, 3.2624002]","[-1.4543345, 13.106051]","[8.556978, 15.685003, -14.666188]","[-2.7514763, 10.629123, 5.363369]","[11.069181, 10.506921]","[8.010758, 1.4124496]","[12.247163, 14.801997, -0.8256327]","[9.615183, 6.5648537, 12.167721]"
38,Ke Nie,Inaccurate Prediction or Genre Evolution? Rethinking Genre Classification,2022,https://doi.org/10.5281/zenodo.7342846,"Ke Nie+University of California, San Diego>USA>education","The existing MIR research on genre classification primarily focuses on how to classify a song into the ""correct"" genre while downplaying the fact that genres mutate over time and in response to social change in terms of their musical properties. Songs claiming the same genre can sound very different if they are released years apart, and genres may revive musical traditions from the past. In this paper, I show that the performance of genre classifiers fluctuates as genres evolve. Unsatisfactory performance of the classifiers may not indicate algorithmic flaws but rather the change of genre characteristics. I demonstrate this by studying the case of Chinese Hip-Hop music. Specifically, I collected and analyzed 69,427 songs from four genres (Hip-Hop, Pop, Rock, and Folk) released on a Chinese music platform between 2009 and 2019. Using classifiers trained from the songs in different year cohorts to predict the genre of all the songs, I show how genre classifiers can be used to detect the stylistic shift in Hip-Hop that happened during this period. The paper thus offers a novel, sociological perspective on contending with the much-challenged idea of improving genre classification accuracy for its own sake. However, instead of questioning the effort, I argue that MIR research on genre classification can be helpful for studying genre as a social construct and cultural phenomenon if the pursuit of prediction performance and the cultural meaning of inaccurate prediction are carefully balanced.",USA,education,Developed economies,"[-31.970022, -13.480564]","[28.077593, -1.0582546]","[-20.952444, 3.1631296, 14.984545]","[12.2346115, 11.359369, 3.2039468]","[13.113629, 10.898853]","[10.938993, 3.231716]","[13.999312, 14.366816, 1.4298244]","[12.534373, 6.0290895, 11.174442]"
34,Kongmeng Liew;Vipul Mishra;Yangyang Zhou;Elena V. Epure;Romain Hennequin;Shoko Wakamiya;Eiji Aramaki,Network Analyses for Cross-Cultural Music Popularity,2022,https://doi.org/10.5281/zenodo.7316654,Kongmeng Liew+Nara Institute of Science and Technology>JPN>education|Deezer Research>FRA>company;Vipul Mishra+Nara Institute of Science and Technology>JPN>education|Deezer Research>FRA>company;Yangyang Zhou+Nara Institute of Science and Technology>JPN>education|Deezer Research>FRA>company;Elena V. Epure+Deezer Research>FRA>company;Romain Hennequin+Deezer Research>FRA>company;Shoko Wakamiya+Nara Institute of Science and Technology>JPN>education;Eiji Aramaki+Nara Institute of Science and Technology>JPN>education,"Anglo-American popular culture has been said to be intricately connected to global popular culture, both shaping and being shaped by popular trends worldwide, yet few research has examined this issue empirically. Our research quantitatively maps the extent of these cultural influences in popular music consumption, by using network analyses to explore cross-cultural popularity in music from 30 countries corresponding to 6 cultural regions (N = 4863 unique songs over six timepoints from 2019-2021). Using Top100 charts from these countries, we constructed a network based on the co-occurrence of songs in charts, and used eigencentrality as an indicator of cross-cultural song popularity. We then compared the country-of-origin of the artists, arousal music features, and socioeconomic indicators. Songs from artists with Anglo-American backgrounds tended to have higher eigencentrality overall, and mixed effects regressions showed that eigencentrality was negatively associated with danceability, and positively associated with spectral energy, and the migrant population of the country (of the charts). Next, using community detection, we observed 11 separate 'communities' in the network. Most communities appeared to be limited by region/culture, but Anglo-American music seemed disproportionally able to transcend cultural boundaries far beyond their geographical borders. We also discuss implications pertaining to cultural hegemony, and the effectiveness of our method in estimating cross-cultural popularity.",JPN,education,Developed economies,"[-40.02698, 13.453329]","[45.86691, 13.309632]","[-28.626532, 7.458049, 3.0442266]","[15.090565, 12.713204, 13.582921]","[14.716457, 9.476012]","[12.779632, 2.2574224]","[15.088997, 14.804432, -0.7157838]","[13.52964, 5.182779, 11.972723]"
38,Ethan Lustig;David Temperley,"The FAV Corpus: An Audio Dataset of Favorite Pieces and Excerpts, With Formal Analyses and Music Theory Descriptors",2023,https://doi.org/10.5281/zenodo.10265293,"Ethan Lustig+Eastman School of Music, University of Rochester>USA>education;David Temperley+Eastman School of Music, University of Rochester>USA>education","We introduce a novel audio corpus, the FAV Corpus, of over 400 favorite musical excerpts and pieces, formal analyses, and free-response comments. In a survey, 140 American university students (mostly music majors) were asked to provide three of their favorite 15-second musical excerpts, from any genre or time period. For each selection, respondents were asked: ""Why do you love the excerpt? Try to be as specific and detailed as possible (music theory terms are encouraged but not required)."" Classical selections were dominated by a very small number of composers, while the pop and jazz artists were diverse. A thematic coding of the respondents' comments found that the most common themes were melody (34.2% of comments), harmony (27.2%), and sonic factors: texture (27.6%), instrumentation (24.3%), and timbre (12.5%). (Rhythm (19.5%) and meter (4.6%) were less present in the comments.) The comments cite simplicity three times more than complexity, and energy gain 14 times more than energy decrease, suggesting that people's favorite excerpts involve simple moments of energy gain or ""build-up"". The complete FAV Corpus is publicly available online at EthanLustig.com/FavCorpus. We will discuss future possibilities for the corpus, including potential directions in the spaces of machine learning and music recommendation.",USA,education,Developed economies,"[-13.525939, 11.568657]","[44.636906, 27.044407]","[1.6186332, 13.669836, -8.047848]","[7.4511433, 15.31812, 11.734958]","[13.481938, 8.436613]","[12.534392, 1.4997206]","[13.558033, 14.605756, -1.4986196]","[13.14329, 4.6874685, 11.3038435]"
47,SeungHeon Doh;Keunwoo Choi;Jongpil Lee;Juhan Nam,LP-MusicCaps: LLM-Based Pseudo Music Captioning,2023,https://doi.org/10.5281/zenodo.10265311,"Seung Heon Doh+Graduate School of Culture Technology, KAIST>KOR>education;Keunwoo Choi+Gaudio Lab, Inc.>KOR>company;Jongpil Lee+Neutune>KOR>company;Juhan Nam+Graduate School of Culture Technology, KAIST>KOR>education","Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.",KOR,education,Developing economies,"[-23.004396, -2.2059321]","[-11.280292, -28.532183]","[4.430634, 24.332993, 11.563695]","[0.18096568, 0.55083966, -21.18068]","[10.886903, 8.005056]","[9.647394, 5.031812]","[13.450385, 12.562549, -0.1941288]","[10.831949, 6.203766, 9.259931]"
40,Alia Morsi;Kana Tatsumi;Akira Maezawa;Takuya Fujishima;Xavier Serra,Sounds Out of Pläce? Score-Independent Detection of Conspicuous Mistakes in Piano Performances,2023,https://doi.org/10.5281/zenodo.10265297,"Alia Morsi+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Yamaha Corporation>JPN>company|Unknown>Unknown>Unknown;Kana Tatsumi+Nagoya Institute of Technology>JPN>education;Akira Maezawa+Yamaha Corporation>JPN>company;Takuya Fujishima+Yamaha Corporation>JPN>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","In piano performance, some mistakes stand out to listeners, whereas others may go unnoticed. Former research concluded that the salience of mistakes depended on factors including their contextual appropriateness and a listener's degree of familiarity to what is being performed. A conspicuous error is considered to be an area where there is something obviously wrong with the performance, which a listener can detect regardless of their degree of knowledge of what is being performed. Analogously, this paper attempts to build a score-independent conspicuous error detector for standard piano repertoire of beginner to intermediate students. We gather three qualitatively different piano playing MIDI data: (1) 103 sight-reading sessions for beginning and intermediate adult pianists with formal music training, (2) 245 performances by presumably late-beginner to early-advanced pianists on a digital piano, and (3) 50 etude performances by an advanced pianist. The data was annotated at the regions considered to contain conspicuous mistakes. Then, we use a Temporal Convolutional Network to detect the sites of such mistakes from the piano roll. We investigate the use of two pre-training methods to overcome data scarcity: (1) synthetic data with procedurally-generated mistakes, and (2) training a part of the model as a piano roll auto-encoder. Experimental evaluation shows that the TCN performs at an F-measure of 0.78 without pretraining for sight-reading data, but the proposed pretraining steps improve the F-measure on performance and etude data, approaching the agreement between human raters on conspicuous error labels. Importantly, we report on the lessons learned from this pilot study, and what should be addressed to continue this research direction.",ESP,education,Developed economies,"[29.845232, 1.2190828]","[-40.62778, 4.093351]","[8.627958, -17.320068, 2.4272068]","[-7.2743516, -13.716445, -6.4165726]","[9.907983, 6.9569235]","[7.696507, 3.9446537]","[11.915859, 11.842193, -0.6249975]","[9.237643, 6.383599, 10.272685]"
41,Hugo Flores García;Prem Seetharaman;Rithesh Kumar;Bryan Pardo,VampNet: Music Generation via Masked Acoustic Token Modeling,2023,https://doi.org/10.5281/zenodo.10265299,Hugo Flores García+Descript Inc.>USA>company;Prem Seetharaman+Descript Inc.>USA>company;Rithesh Kumar+Descript Inc.>USA>company;Bryan Pardo+Northwestern University>USA>education,"We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation.  We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",USA,company,Developed economies,"[22.405785, 9.210093]","[-16.80907, -44.12886]","[-1.2181876, -0.4467337, 21.560482]","[-21.754858, -4.7171226, -15.012305]","[10.211392, 8.303322]","[8.309744, 6.364974]","[13.284083, 11.706238, 0.12277461]","[9.633717, 5.889337, 8.554141]"
42,Yucong Jiang,Expert and Novice Evaluations of Piano Performances: Criteria for Computer-Aided Feedback,2023,https://doi.org/10.5281/zenodo.10265301,Yucong Jiang+University of Richmond>USA>education,"Learning an instrument can be rewarding, but is unavoidably a huge undertaking. Receiving constructive feedback on one's playing is crucial for improvement. However, personal feedback from an expert instructor is seldom available on demand. The goal motivating this project is to build software that will provide comparably useful feedback to beginners, in order to supplement feedback from human instructors. To lay the groundwork for that, in this paper we investigate performance assessment criteria from both quantitative and qualitative perspectives. We gathered 83 piano performances from 21 players. Each recording was evaluated by both expert piano instructors and novice players. This dataset is unique in that the novice evaluators are also players, and that both quantitative and qualitative evaluations are collected. Our analysis of the evaluations indicates that the kind of specific, concrete piano techniques that are most elusive to novice evaluators are precisely the kind of characteristics that can be detected, measured, and visualized for learners by a well-designed software tool.",USA,education,Developed economies,"[30.69029, -0.60315156]","[-38.632576, 6.008698]","[18.555002, -1.0424438, 21.350008]","[-9.441163, -11.110069, -3.9964926]","[9.952571, 7.0163436]","[7.7290907, 3.940251]","[12.179844, 11.62854, -0.5754054]","[9.22462, 6.116493, 10.287011]"
43,Andres Ferraro;Jaehun Kim;Sergio Oramas;Andreas Ehmann;Fabien Gouyon,Contrastive Learning for Cross-Modal Artist Retrieval,2023,https://doi.org/10.5281/zenodo.10265303,Andres Ferraro+Pandora-SiriusXM>USA>company;Jaehun Kim+Pandora-SiriusXM>USA>company;Sergio Oramas+Pandora-SiriusXM>USA>company;Andreas Ehmann+Pandora-SiriusXM>USA>company;Fabien Gouyon+Pandora-SiriusXM>USA>company,"Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally represented in several modalities, e.g., audio signals, user interaction data, or editorial data. However, data of any given modality might not be available for all items in any music dataset. In this work, we propose a method based on contrastive learning to combine embeddings from multiple modalities and explore the impact of the presence or absence of embeddings from diverse modalities in an artist similarity task. Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities, both in terms of artist retrieval accuracy and coverage. Improvements with respect to other methods are particularly significant for less popular query artists. We demonstrate our method successfully combines complementary information from diverse modalities, and is more robust to missing modality data (i.e., it better handles the retrieval of artists with different modality embeddings than the query artist's).",USA,company,Developed economies,"[-7.602277, 25.19519]","[41.82625, -8.310012]","[-28.721617, 12.554465, 12.299682]","[21.143332, 18.976604, 1.9239652]","[13.286214, 8.299619]","[11.594601, 2.8684258]","[14.005607, 13.978808, -1.0425769]","[13.040036, 6.0289097, 12.017404]"
44,Christoph Finkensiep;Matthieu Haeberle;Friedrich Eisenbrand;Markus Neuwirth;Martin Rohrmeier,Repetition-Structure Inference With Formal Prototypes,2023,https://doi.org/10.5281/zenodo.10265305,Christoph Finkensiep+École Polytechnique Fédérale de Lausanne>CHE>education;Matthieu Haeberle+École Polytechnique Fédérale de Lausanne>CHE>education;Friedrich Eisenbrand+École Polytechnique Fédérale de Lausanne>CHE>education;Markus Neuwirth+Anton Bruckner Privatuniversität Linz>AUT>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"The concept of form in music encompasses a wide range of musical aspects, such as phrases and (hierarchical) segmentation, formal functions, cadences and voice-leading schemata, form templates, and repetition structure. In an effort towards a unified model of form, this paper proposes an integration of repetition structure   (i.e., which segments of a piece occur several times) and formal templates (such as AABA). While repetition structure can be modeled using context-free grammars,   most prior approaches allow for arbitrary grammar rules. Constraining the structure of the inferred rules to conform to a small set of templates (meta-rules) not only reduces the space of possible rules that need to be considered but also ensures that the resulting repetition grammar remains interpretable in the context of musical form.   The resulting formalism can be extended to cases of varied repetition and thus constitutes a building block for a larger model of form.",CHE,education,Developed economies,"[16.803896, 31.076382]","[-18.256891, 24.150572]","[-15.898222, -17.250757, 5.0696607]","[-15.998542, 2.0804267, 3.6777534]","[11.615529, 7.751372]","[7.9349318, 1.9896787]","[12.419193, 13.744245, -0.6635095]","[9.467678, 7.277469, 12.259673]"
45,Peter van Kranenburg;Eoin J. Kearns,Algorithmic Harmonization of Tonal Melodies Using Weighted Pitch Context Vectors,2023,https://doi.org/10.5281/zenodo.10265307,Peter van Kranenburg+Meertens Institute>NLD>facility;Eoin Kearns+Meertens Institute>NLD>facility,"Most melodies from the Western common practice period have a harmonic background, i.e., a succession of chords that fit the melody. In this paper we provide a novel approach to infer this harmonic background from the score notation of a melody. We first construct a pitch context vector for each note in the melody. This vector summarises the pitches that are in the preceding and following contexts of the note. Next, we use these pitch context vectors to generate a list of candidate chords for each note. The candidate chords fit the pitch context of a given note each with a computed strength. Finally, we find an optimal path through the chord candidates, employing a score function for the fitness of a given candidate chord. The algorithm chooses one chord for each note, optimizing the total score. A set of heuristics is incorporated in the score function. The system is heavily parameterised, extremely flexible, and does not need training. This creates a framework to experiment with harmonization of melodies. The output is evaluated by an expert survey, which yields convincing and positive results.",NLD,facility,Developed economies,"[9.526806, -7.5174437]","[-23.86096, 23.362595]","[10.979829, 11.89418, 1.3110085]","[-22.997915, 3.2543666, 3.0647578]","[10.323975, 9.593138]","[7.1075406, 3.2365685]","[11.468578, 14.87494, -0.83415604]","[9.702034, 7.9983983, 12.251845]"
46,Kento Watanabe;Masataka Goto,Text-to-Lyrics Generation With Image-Based Semantics and Reduced Risk of Plagiarism,2023,https://doi.org/10.5281/zenodo.10265309,Kento Watanabe+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper proposes a text-to-lyrics generation method, aiming to provide lyric writing support by suggesting the generated lyrics to users who struggle to find the right words to convey their message. Previous studies on lyrics generation have focused on generating lyrics based on semantic constraints such as specific keywords, lyric style, and topics. However, these methods had limitations because users could not freely input their intentions as text. Even if such intentions can be given as input text, the lyrics generated from the input tend to contain similar wording, making it difficult to inspire the user. Our method is therefore developed to generate lyrics that (1) convey a message similar to the input text and (2) contain wording different from the input text. A straightforward approach of training a text-to-lyrics encoder-decoder is not feasible since there is no text-lyric paired data for this purpose. To overcome this issue, we divide the text-to-lyrics generation process into a two-step pipeline, eliminating the need for text-lyric paired data. (a) First, we use an existing text-to-image generation technique as a text analyzer to obtain an image that captures the meaning of the input text, ignoring the wording. (b) Next, we use our proposed image-to-lyrics encoder-decoder (I2L) to generate lyrics from the obtained image while preserving its meaning. The training of this I2L model only requires pairs of ""lyrics"" and ""images generated from lyrics"", which are readily prepared. In addition, we propose for the first time a lyrics generation method that reduces the risk of plagiarism by prohibiting the generation of uncommon phrases in the training data. Experimental results show that the proposed method can generate lyrics with different phrasing while conveying a message similar to the input text.",JPN,facility,Developed economies,"[-31.575651, -33.70651]","[-9.744388, -26.287188]","[9.2159, 24.45186, -1.6744094]","[-19.842794, 18.513031, 0.5476254]","[11.456741, 11.834659]","[9.159884, 4.611115]","[12.451737, 15.973419, 1.1661162]","[10.799563, 6.7846184, 9.408588]"
39,Le Zhuo;Ruibin Yuan;Jiahao Pan;Yinghao Ma;Yizhi Li;Ge Zhang;Si Liu;Roger B. Dannenberg;Jie Fu;Chenghua Lin;Emmanouil Benetos;Wenhu Chen;Wei Xue;Yike Guo,LyricWhiz: Robust Multilingual Zero-Shot Lyrics Transcription by Whispering to ChatGPT,2023,https://doi.org/10.5281/zenodo.10265295,Le Zhuo+Beihang University>CHN>education;Ruibin Yuan+Beijing Academy of Artificial Intelligence>CHN>company|Carnegie Mellon University>USA>education;Jiahao Pan+Hong Kong University of Science and Technology>HKG>education;Yinghao Ma+Queen Mary University of London>GBR>education;Yizhi Li+University of Sheffield>GBR>education;Ge Zhang+Beijing Academy of Artificial Intelligence>CHN>company|University of Waterloo>CAN>education;Si Liu+Beihang University>CHN>education;Roger Dannenberg+Carnegie Mellon University>USA>education;Jie Fu+Beijing Academy of Artificial Intelligence>CHN>company;Chenghua Lin+University of Sheffield>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Wenhu Chen+University of Waterloo>CAN>education;Wei Xue+Hong Kong University of Science and Technology>HKG>education;Yike Guo+Hong Kong University of Science and Technology>HKG>education,"We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the ""ear"" by transcribing the audio, while GPT-4 serves as the ""brain,"" acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copy-right license, based on MTG-Jamendo, and offer a human- annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.",CHN,education,Developing economies,"[-25.857553, -34.96509]","[-26.493475, -43.087006]","[13.431212, 18.368067, -3.003652]","[2.3975956, -2.830651, -22.477518]","[11.160042, 11.793359]","[8.286951, 4.670466]","[12.1521, 15.791684, 1.1678051]","[10.580738, 7.126212, 9.200177]"
53,Yinghao Ma;Ruibin Yuan;Yizhi Li;Ge Zhang;Chenghua Lin;Xingran Chen;Anton Ragni;Hanzhi Yin;Emmanouil Benetos;Norbert Gyenge;Ruibo Liu;Gus Xia;Roger B. Dannenberg;Yike Guo;Jie Fu,On the Effectiveness of Speech Self-Supervised Learning for Music,2023,https://doi.org/10.5281/zenodo.10265321,Yinghao Ma+Queen Mary University of London>GBR>education;Ruibin Yuan+Queen Mary University of London>GBR>education;Yizhi Li+Queen Mary University of London>GBR>education;Ge Zhang+Queen Mary University of London>GBR>education;Xingran Chen+Queen Mary University of London>GBR>education;Chenghua Lin+University of Sheffield>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Anton Ragni+University of Sheffield>GBR>education;Norbert Gyenge+University of Sheffield>GBR>education;Ruibo Liu+Dartmouth College>USA>education;Gus Xia+New York University Shanghai>CHN>education;Roger Dannenberg+Carnegie Mellon University>USA>education;Yike Guo+Hong Kong University of Science and Technology>HKG>education;Jie Fu+Beijing Academy of Artificial Intelligence>CHN>company,"Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent models such as wav2vec2.0 have shown promise. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train 12 SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.",GBR,education,Developed economies,"[-19.632011, -5.725943]","[-25.901375, -40.981056]","[-0.7363338, 15.387593, 0.3329363]","[4.98557, -6.678239, -25.528]","[12.259522, 9.326767]","[8.9880495, 4.699577]","[13.169658, 14.669276, -0.11470629]","[10.485948, 6.827541, 9.014763]"
49,Francesco Foscarin;Daniel Harasim;Gerhard Widmer,Predicting Music Hierarchies With a Graph-Based Neural Decoder,2023,https://doi.org/10.5281/zenodo.10265315,"Francesco Foscarin+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Daniel Harasim+Unknown>Unknown>Unknown;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education","This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis. The parsing involves two steps. First, the input sequence is passed through a transformer encoder to enrich it with contextual information. Then, a classifier filters the graph of all possible dependency arcs to produce the dependency tree. One major benefit of this system is that it can be easily integrated into modern deep-learning pipelines. Moreover, since it does not rely on any particular symbolic grammar, it can consider multiple musical features simultaneously, make use of sequential context information, and produce partial results for noisy inputs. We test our approach on two datasets of musical trees -- time-span trees of monophonic note sequences and harmonic trees of jazz chord sequences -- and show that our approach outperforms previous methods.",AUT,education,Developed economies,"[-15.057121, -3.6588457]","[-12.92099, -33.861866]","[-0.5123231, 8.010784, 11.138775]","[-13.231227, -1.2112756, -10.668208]","[11.721686, 8.844972]","[9.074932, 5.223725]","[13.446759, 13.214365, 0.088869415]","[9.817561, 6.027378, 9.586336]"
50,Johannes Zeitler;Simon Deniffel;Michael Krause;Meinard Müller,Stabilizing Training With Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation With Weakly Aligned Targets,2023,https://doi.org/10.5281/zenodo.10265317,Johannes Zeitler+International Audio Laboratories Erlangen>DEU>facility;Simon Deniffel+International Audio Laboratories Erlangen>DEU>facility;Michael Krause+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Soft dynamic time warping (SDTW) is a differentiable loss function that allows for training neural networks from weakly aligned data. Typically, SDTW is used to iteratively compute and refine soft alignments that compensate for temporal deviations between the training data and its weakly annotated targets. One major problem is that a mismatch between the estimated soft alignments and the reference alignments in the early training stage leads to incorrect parameter updates, making the overall training procedure unstable. In this paper, we investigate such stability issues by considering the task of pitch class estimation from music recordings as an illustrative case study. In particular, we introduce and discuss three conceptually different strategies (a hyperparameter scheduling, a diagonal prior, and a sequence unfolding strategy) with the objective of stabilizing intermediate soft alignment results. Finally, we report on experiments that demonstrate the effectiveness of the strategies and discuss efficiency and implementation issues.",DEU,facility,Developed economies,"[24.31945, -22.111837]","[-18.80674, -17.167624]","[12.0826025, -16.38613, -15.658579]","[0.58303076, -24.295996, -7.454256]","[10.156633, 5.694983]","[5.992212, 0.75163555]","[10.917968, 13.105565, -1.0132887]","[8.032856, 5.916251, 10.830082]"
51,Danbinaerin Han;Rafael Caro Repetto;Dasaem Jeong,Finding Tori: Self-Supervised Learning for Analyzing Korean Folk Song,2023,https://doi.org/10.5281/zenodo.10265319,Danbinaerin Han+Sogang University>KOR>education;Rafael Caro Repetto+Kunstuniversität Graz>AUT>education;Dasaem Jeong+Sogang University>KOR>education,"In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.",KOR,education,Developing economies,"[-28.118155, -5.9268713]","[-0.38786644, -23.456205]","[1.705829, 13.945785, -0.20235953]","[0.48724842, 12.071364, 1.3310562]","[12.615996, 9.842692]","[8.829459, 2.9585104]","[13.0783825, 14.96815, -0.058987334]","[9.975264, 6.700965, 11.048346]"
52,Bernardo Torres;Stefan Lattner;Gaël Richard,Singer Identity Representation Learning Using Self-Supervised Techniques,2023,https://doi.org/10.5281/zenodo.10265323,"Bernardo Torres+Telecom Paris, Institut Polytechnique de Paris>FRA>education|Sony Computer Science Laboratories Paris>FRA>company;Stefan Lattner+Sony Computer Science Laboratories Paris>FRA>company;Gaël Richard+Telecom Paris, Institut Polytechnique de Paris>FRA>education","Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and trained models to facilitate further research on singing voice and related areas.",FRA,education,Developed economies,"[-14.300499, -37.22515]","[-29.943092, -43.462814]","[14.75912, 13.937248, -22.329508]","[2.948601, -9.595117, -22.617638]","[10.2362385, 11.6122875]","[7.799043, 4.7753415]","[11.492765, 15.651345, 0.70228875]","[10.376457, 7.3235536, 8.885746]"
54,Tian Cheng;Masataka Goto,Transformer-Based Beat Tracking With Low-Resolution Encoder and High-Resolution Decoder,2023,https://doi.org/10.5281/zenodo.10265325,Tian Cheng+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"In this paper, we address the beat tracking task which is to predict beat times corresponding to the input audio. Due to the long sequential inputs, it is still challenging to model the global structure efficiently and to deal with the data imbalance between beats and no beats. In order to meet the above challenges, we propose a novel Transformer-based model consisting of a low-resolution encoder and a high-resolution decoder. The encoder with low temporal resolution is suited to capture global features with more balanced data. The decoder with high temporal resolution is designed to predict beat times at a desired resolution. In the decoder, the global structure is considered by the cross attention between the global features and high-dimensional features. There are two key modifications in the proposed model: (1) adding 1D convolutional layers in the encoder and (2) replacing positional embedding by the upsampled encoder features in the decoder. In the experiment, we achieved the state-of-the-art performance and showed that the decoder produced more precise and stable results.",JPN,facility,Developed economies,"[34.04851, -33.700886]","[-31.006603, -17.508793]","[7.8745832, -31.606682, -7.745114]","[-7.4365883, 12.531325, -21.843327]","[10.451516, 4.267858]","[4.982657, 2.711122]","[10.146423, 12.893129, -2.223185]","[8.043864, 6.863173, 9.876854]"
9,Max Johnson;Mark R. H. Gotham,Musical Micro-Timing for Live Coding,2023,https://doi.org/10.5281/zenodo.10265231,Max Johnson+University of Cambridge>GBR>education|Durham University>GBR>education;Mark Gotham+University of Cambridge>GBR>education|Durham University>GBR>education,"Micro-timing is an essential part of human music-making, yet it is absent from most computer music systems. Partly to address this gap, we present a novel system for generating music with style-specific micro-timing within the Sonic Pi live coding language. We use a probabilistic approach to control the exact timing according to patterns discovered in new analyses of existing micro-timing data (jembe drumming and Viennese waltz). This implementation also required the introduction of musical metre into Sonic Pi. The new metre and micro-timing systems are inherently flexible, and thus open to a wide range of creative possibilities including (but not limited to): creating new micro-timing profiles for additional styles; expanded definitions of metre; and the free mixing of one micro-timing style with the musical content of another. The code is freely available as a Sonic Pi plug-in and released open source at https://github.com/MaxTheComputerer/sonicpi-metre.",GBR,education,Developed economies,"[20.445751, -25.173122]","[-29.25909, 2.1692057]","[0.7199064, -18.274439, -1.2666483]","[-11.621078, 8.060103, -5.9226646]","[11.385498, 5.680867]","[5.8666797, 1.5328444]","[11.66601, 13.109636, -2.0340955]","[7.9990296, 6.489328, 11.401769]"
56,Karlijn Dinnissen;Christine Bauer,How Control and Transparency for Users Could Improve Artist Fairness in Music Recommender Systems,2023,https://doi.org/10.5281/zenodo.10265331,Karlijn Dinnissen+Utrecht University>NLD>education;Christine Bauer+Paris Lodron University Salzburg>AUT>education,"As streaming services have become a main channel for music consumption, they significantly impact various stakeholders: users, artists who provide music, and other professionals working in the music industry. Therefore, it is essential to consider all stakeholders' goals and values when developing and evaluating the music recommender systems integrated into these services. One vital goal is treating artists fairly, thereby giving them a fair chance to have their music recommended and listened to, and subsequently building a fan base. Such artist fairness is often assumed to have a trade-off with user goals such as satisfaction. Using insights from two studies, this work shows the opposite: some goals from different stakeholders are complementary. Our first study, in which we interview music artists, demonstrates that they often see increased transparency and control for users as a means to also improve artist fairness. We expand with a second study asking other music industry professionals about these topics using a questionnaire. Its results indicate that transparency towards users is highly valued and should be increased.",NLD,education,Developed economies,"[-46.95784, 23.24927]","[41.74327, 30.640842]","[-11.594222, 28.202148, -8.171233]","[11.395078, 15.486899, 17.627813]","[15.917714, 9.116037]","[12.976118, 1.1929399]","[15.797714, 15.563554, -1.4150833]","[13.451935, 4.3732905, 12.124727]"
57,Ahyeon Choi;Eunsik Shin;Haesun Joung;Joongseek Lee;Kyogu Lee,Towards a New Interface for Music Listening: A User Experience Study on YouTube,2023,https://doi.org/10.5281/zenodo.10265333,Ahyeon Choi+Seoul National University>KOR>education;Eunsik Shin+Seoul National University>KOR>education;Haesun Joung+Seoul National University>KOR>education;Joongseek Lee+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from traditional music streaming services in that they provide a diverse range of music-related videos as well as soundtracks. However, notwithstanding the surge in the platform's utilization as a music consumption tool, there is a lack of thorough research on the phenomenon. To investigate its usability and interface satisfaction as a music listening tool, we conducted semi-structured interviews with 27 users who listen to music through YouTube more than three times a week. Our qualitative analysis found that YouTube has five main meanings for users as a music streaming service: 1) exploring musical diversity, 2) sharing unique playlists, 3) providing visual satisfaction, 4) facilitating user interaction, and 5) allowing free and easy access. We also propose wireframes of a video streaming service for better audio-visual music listening in two stages: search and listening. By these wireframes, we offer practical solutions to enhance user satisfaction with YouTube for music listening. It has implications not only for YouTube but also for other streaming services for music.",KOR,education,Developing economies,"[-23.868149, 29.569233]","[39.375595, 28.640636]","[-17.575264, 9.843069, -19.99628]","[8.08094, 11.841954, 15.275373]","[14.377318, 7.3820295]","[12.860748, 1.1943252]","[14.458798, 14.094153, -2.3062682]","[13.339812, 4.31645, 12.179714]"
59,Louis Couturier;Louis Bigo;Florence Levé,Comparing Texture in Piano Scores,2023,https://doi.org/10.5281/zenodo.10265337,Louis Couturier+Université de Picardie Jules Verne>FRA>education|Univ. Lille>FRA>education;Louis Bigo+Univ. Lille>FRA>education;Florence Levé+Université de Picardie Jules Verne>FRA>education|Univ. Lille>FRA>education,"In this paper, we propose four different approaches to quantify similarities of compositional texture in symbolically encoded piano music. A melodic contour or harmonic progression can be shaped into a wide variety of different rhythms, densities, or combinations of layers. Instead of describing these textural organizations only locally, using existing formalisms, we question how these parameters may evolve throughout a musical piece, and more specifically how much they change. Hence, we define several distance functions to compare texture between two musical bars, based either on textural labels annotated with a dedicated syntax, or on symbolic scores. We propose an evaluation methodology based on textural heterogeneity and contrasts in classical Thema and Variations using the TAVERN dataset. Finally, we illustrate use cases of these tools to analyze long-term structure, and discuss the impact of these results on the understanding of musical texture.",FRA,education,Developed economies,"[29.372894, -0.409736]","[-15.114021, 17.88972]","[17.80577, 2.257538, 21.011269]","[-13.436604, -6.1162415, 5.2912626]","[10.133462, 6.840237]","[8.295737, 2.11816]","[12.203992, 11.806971, -0.8951077]","[10.218871, 7.0025434, 12.174354]"
37,Martin E. Malandro,Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling,2023,https://doi.org/10.5281/zenodo.10265291,Martin E. Malandro+Sam Houston State University>USA>education,"We introduce Composer's Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of measures from a MIDI file, and we train a T5-like model to accomplish this task. Composer's Assistant consists of this model together with scripts that enable interaction with the model in REAPER. We conduct objective and subjective tests of our model. We release our complete system, consisting of source code, pretrained models, and REAPER scripts. Our models were trained only on permissively-licensed MIDI files.",USA,education,Developed economies,"[37.16354, -0.2076546]","[-9.063591, 33.93744]","[10.020298, -7.3788295, 23.976759]","[-16.232267, -11.745105, 2.760353]","[10.24088, 7.2769628]","[9.167133, 5.820499]","[12.568411, 11.468227, -0.49361342]","[9.936592, 5.608794, 9.938769]"
58,Xavier Riley;Simon Dixon,FiloBass: A Dataset and Corpus Based Study of Jazz Basslines,2023,https://doi.org/10.5281/zenodo.10265335,Xavier Riley+Queen Mary University of London>GBR>education|Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education|Queen Mary University of London>GBR>education,"We present FiloBass: a novel corpus of music scores and annotations which focuses on the important but often overlooked role of the double bass in jazz accompaniment. Inspired by recent works that shed light on the role of the soloist, we offer a collection of 48 manually verified transcriptions of professional jazz bassists, comprising over 50,000 note events, which are based on the backing tracks used in the FiloSax dataset. For each recording we provide audio stems, scores, performance-aligned MIDI and associated metadata for beats, downbeats, chord symbols and markers for musical form.  We then use FiloBass to enrich our understanding of jazz bass lines, by conducting a corpus-based musical analysis with a contrastive study of existing instructional methods. Together with the original FiloSax dataset, our work represents a significant step toward a fully annotated performance dataset for a jazz quartet setting. By illuminating the critical role of the bass in jazz, this work contributes to a more nuanced and comprehensive understanding of the genre.",GBR,education,Developed economies,"[9.071385, 13.515972]","[-34.900116, 2.5367763]","[-9.359016, -5.2676806, 28.293327]","[-13.60544, -3.2505505, -1.6958194]","[10.773055, 9.4826]","[7.355942, 3.457138]","[12.112354, 14.368972, -0.6420591]","[9.3712225, 6.366239, 11.060309]"
48,Morgan Buisson;Brian McFee;Slim Essid;Helene C. Crayencour,A Repetition-Based Triplet Mining Approach for Music Segmentation,2023,https://doi.org/10.5281/zenodo.10265313,"Morgan Buisson+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Brian McFee+New York University>USA>education;Slim Essid+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Hélène C. Crayencour+L2S, CNRS-Univ. Paris-Sud-CentraleSupélec>FRA>facility","Contrastive learning has recently appeared as a well-suited method to find representations of music audio signals that are suitable for structural segmentation. However, most existing unsupervised training strategies omit the notion of repetition and therefore fail at encompassing this essential aspect of music structure. This work introduces a triplet mining method which explicitly considers repeating sequences occurring inside a music track by leveraging common audio descriptors. We study its impact on the learned representations through downstream music segmentation. Because musical repetitions can be of different natures, we give further insight on the role of the audio descriptors employed at the triplet mining stage as well as the trade-off existing between the quality of the triplets mined and the quantity of unlabelled data used for training. We observe that our method requires less non-annotated data while remaining competitive against other unsupervised methods trained on a larger corpus.",FRA,education,Developed economies,"[-2.101007, -1.8109288]","[-4.9180217, -25.762335]","[3.3203857, -3.188473, -1.5837417]","[-3.259167, -0.3442025, -14.571138]","[11.700317, 8.252398]","[8.964086, 4.395157]","[12.479955, 14.093829, 0.016551789]","[10.394607, 6.941438, 10.430868]"
36,Juan C. Martinez-Sevilla;Adrián Roselló;David Rizo;Jorge Calvo-Zaragoza,On the Performance of Optical Music Recognition in the Absence of Specific Training Data,2023,https://doi.org/10.5281/zenodo.10265289,Juan C. Martinez-Sevilla+University of Alicante>ESP>education;Adrian Rosello+University of Alicante>ESP>education;David Rizo+University of Alicante>ESP>education|Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"Optical Music Recognition (OMR) has become a popular technology to retrieve information present in musical scores in conjunction with the increasing improvement of Deep Learning techniques, which represent the state-of-the-art in the field. However, its effectiveness is limited to cases where the target collection is similar in musical context and graphical appearance to the available training examples. To address this limitation, researchers have resorted to labeling examples for specific neural models, which is time-consuming and raises questions about usability. In this study, we propose a holistic and comprehensive study for dealing with new music collections in OMR, including extensive experiments to identify key aspects to have in mind that lead to better performance ratios. We resort to collections written in Mensural notation as specific use case, comprising 5 different corpora of training domains and up to 15 test collections. Our experiments report many interesting insights that will be important to create a manual of best practices when dealing with new collections in OMR systems.",ESP,education,Developed economies,"[38.721897, 20.608027]","[-20.384607, -24.582224]","[18.453623, 11.804773, 9.458711]","[-16.063883, -19.676992, -4.0546947]","[8.6828575, 6.195918]","[6.4685683, -1.079358]","[10.682051, 11.134746, -0.11927289]","[8.146438, 4.2654424, 9.9025]"
23,Keisuke Toyama;Taketo Akama;Yukara Ikemiya;Yuhta Takida;Wei-Hsiang Liao;Yuki Mitsufuji,Automatic Piano Transcription With Hierarchical Frequency-Time Transformer,2023,https://doi.org/10.5281/zenodo.10265261,Keisuke Toyama+Sony Group Corporation>JPN>company;Taketo Akama+Sony Computer Science Laboratories>JPN>company;Yukara Ikemiya+Sony Group Corporation>JPN>company;Yuhta Takida+Sony Group Corporation>JPN>company;Wei-Hsiang Liao+Sony Group Corporation>JPN>company;Yuki Mitsufuji+Sony Group Corporation>JPN>company,"Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.",JPN,company,Developed economies,"[33.262173, -6.24599]","[-25.509157, -21.736517]","[16.344854, -7.485045, 14.863048]","[-7.2782264, -5.132893, -9.889865]","[9.689055, 7.263121]","[7.8792896, 5.3187814]","[12.040454, 11.398709, -0.17935994]","[8.873429, 6.6189127, 9.348653]"
34,Joan Serrà;Davide Scaini;Santiago Pascual;Daniel Arteaga;Jordi Pons;Jeroen Breebaart;Giulio Cengarle,Mono-to-Stereo Through Parametric Stereo Generation,2023,https://doi.org/10.5281/zenodo.10265285,Joan Serrà+Dolby Laboratories>USA>company;Davide Scaini+Dolby Laboratories>USA>company;Santiago Pascual+Dolby Laboratories>USA>company;Daniel Arteaga+Dolby Laboratories>USA>company;Jordi Pons+Dolby Laboratories>USA>company;Jeroen Breebaart+Dolby Laboratories>USA>company;Giulio Cengarle+Dolby Laboratories>USA>company,"Generating a stereophonic presentation from a monophonic audio signal is a challenging open task, especially if the goal is to obtain a realistic spatial imaging with a specific panning of sound elements. In this work, we propose to convert mono to stereo by means of predicting parametric stereo (PS) parameters using both nearest neighbor and deep network approaches. In combination with PS, we also propose to model the task with generative approaches, allowing to synthesize multiple and equally-plausible stereo renditions from the same mono signal. To achieve this, we consider both autoregressive and masked token modelling approaches. We provide evidence that the proposed PS-based models outperform a competitive classical decorrelation baseline and that, within a PS prediction framework, modern generative models outshine equivalent non-generative counterparts. Overall, our work positions both PS and generative modelling as strong and appealing methodologies for mono-to-stereo upmixing. A discussion of the limitations of these approaches is also provided.",USA,company,Developed economies,"[32.639282, -15.931585]","[-20.405045, -45.285797]","[28.038017, -11.944814, -9.836343]","[-17.778244, -3.5050652, -19.71012]","[9.429014, 8.775698]","[8.184643, 6.39169]","[12.323139, 12.285529, 0.62813103]","[9.612767, 6.074722, 8.188988]"
60,Johannes Hentschel;Andrew McLeod;Yannis Rammos;Martin Rohrmeier,Introducing DiMCAT for Processing and Analyzing Notated Music on a Very Large Scale,2023,https://doi.org/10.5281/zenodo.10265339,Johannes Hentschel+École Polytechnique Fédérale de Lausanne>CHE>education;Andrew McLeod+Fraunhofer IDMT>DEU>company;Yannis Rammos+École Polytechnique Fédérale de Lausanne>CHE>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"As corpora of digital musical scores continue to grow, the need for research tools capable of manipulating such data efficiently, with an intuitive interface, and support for a diversity of file formats, becomes increasingly pressing. In response, this paper introduces the Digital Musicology Corpus Analysis Toolkit (DiMCAT), a Python library for processing large corpora of digitally encoded musical scores. Equally aimed at music-analytical corpus studies, MIR, and machine-learning research, DiMCAT performs common data transformations and analyses using dataframes. Dataframes reduce the inherent complexity of atomic score contents (e.g., notes), larger score entities (e.g., measures), and abstractions (e.g., chord symbols) into easily manipulable computational structures, whose vectorized operations scale to large quantities of musical material. The design of DiMCAT's API prioritizes computational speed and ease of use, thus aiming to cater to machine-learning practitioners and musicologists alike.",CHE,education,Developed economies,"[-17.469353, 15.158923]","[-2.5662575, 28.867865]","[-9.96077, -1.0212121, -12.583393]","[-10.685062, 0.09105803, 13.63857]","[13.748607, 7.7304754]","[9.563029, 1.001447]","[14.014388, 14.272946, -1.9043148]","[10.355081, 5.7702904, 11.82855]"
12,Andrea Martelloni;Andrew P. McPherson;Mathieu Barthet,Real-Time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar,2023,https://doi.org/10.5281/zenodo.10265236,Andrea Martelloni+Queen Mary University of London>GBR>education|Imperial College>GBR>education;Andrew P McPherson+Imperial College>GBR>education;Mathieu Barthet+Queen Mary University of London>GBR>education,"Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distributions corresponding to different taxonomic classes. Results indicate that the networks are strong classifiers especially in a simplified 2-class recognition task, and the VAEs yield improved class separation compared to CNNs as evidenced by increased KL-Divergence across distributions. We argue that the VAE embedding quality could support control intimacy and rich interaction when the latent space's parameters are used to control an external synthesis engine. Further design challenges around generalisation to different datasets have been identified.",GBR,education,Developed economies,"[49.42398, -11.959222]","[-40.679806, -10.23932]","[26.671904, -7.4584246, 7.933967]","[-18.379484, -9.609572, -7.362719]","[7.6179705, 8.164404]","[7.7279453, 4.7011905]","[11.798879, 11.379731, 1.5177004]","[9.093378, 6.7877984, 9.440173]"
13,Hiromu Yakura;Masataka Goto,IteraTTA: An Interface for Exploring Both Text Prompts and Audio Priors in Generating Music With Text-to-Audio Models,2023,https://doi.org/10.5281/zenodo.10265239,Hiromu Yakura+University of Tsukuba>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.",JPN,education,Developed economies,"[24.296408, 10.4415455]","[3.647011, 25.14009]","[1.2267642, -2.1521058, 26.565699]","[-17.565336, 11.592459, 11.225563]","[10.583896, 8.228548]","[8.642659, 6.1763196]","[13.513907, 11.880339, -0.088626586]","[10.063453, 5.6696086, 8.916856]"
14,Mirco Pezzoli;Raffaele Malvermi;Fabio Antonacci;Augusto Sarti,Similarity Evaluation of Violin Directivity Patterns for Musical Instrument Retrieval,2023,https://doi.org/10.5281/zenodo.10265243,Mirco Pezzoli+Politecnico di Milano>ITA>education;Raffaele Malvermi+Politecnico di Milano>ITA>education;Fabio Antonacci+Politecnico di Milano>ITA>education;Augusto Sarti+Politecnico di Milano>ITA>education,"The directivity of a musical instrument is a function that describes the spatial characteristics of its sound radiation. The majority of the available literature focuses on measuring directivity patterns, with analysis mainly limited to visual inspections. Recently, some similarity metrics for directivity patterns have been introduced, yet their application has not being fully addressed. In this work, we introduce the problem of musical instrument retrieval based on the directivity pattern features. We aim to exploit the available similarity metrics for directivity patterns in order to determine distances between instruments. We apply the methodology to a data set of violin directivities, including historical and modern high-quality instruments. Results show that the methodology facilitates the comparison of musical instruments and the navigation of databases of directivity patterns.",ITA,education,Developed economies,"[12.944455, -20.730267]","[9.803045, 6.6326246]","[13.483792, -9.5640545, -2.000096]","[9.146117, -3.9587715, -10.862035]","[9.317517, 6.866976]","[9.416268, 2.0907557]","[11.2476225, 12.508937, -0.18685606]","[10.811577, 6.6022897, 12.234437]"
15,George Sioros,Polyrhythmic Modelling of Non-Isochronous and Microtiming Patterns,2023,https://doi.org/10.5281/zenodo.10265245,George Sioros+University of Plymouth>GBR>education,"Computational models and analyses of musical rhythms are predominantly based on the subdivision of durations down to a common isochronous pulse, which plays a fundamental structural role in the organization of their durational patterns. Meter, the most widespread example of such a temporal scheme, consists of several hierarchically organized pulses. Deviations from isochrony found in musical patterns are considered to form an expressive, micro level of organization that is distinct from the structural macro-organization of the basic pulse. However, polyrhythmic structures, such as those found in music from West Africa or the African diaspora, challenge both the hierarchical subdivision of durations and the structural isochrony of the above models. Here we present a model that integrates the macro- and micro-organization of rhythms by generating non-isochronous girds from isochronous pulses within a polyrhythmic structure. Observed micro-timing patterns may then be generated from structural non-isochronous grids, rather than being understood as expressive deviations from isochrony. We examine the basic mathematical properties of the model and show that meter can be generated as a special case. Finally, we demonstrate the model in the analysis of micro-timing patterns observed in Brazilian samba performances.",GBR,education,Developed economies,"[45.003704, -32.29645]","[-22.173126, 5.8297668]","[7.3090086, -29.763834, 2.405573]","[-0.3949095, 18.346376, -7.8206444]","[11.326787, 4.8761964]","[6.2201242, 1.731006]","[10.873963, 13.597614, -2.2502062]","[8.124528, 6.8273683, 11.833107]"
16,Shangda Wu;Dingyao Yu;Xu Tan;Maosong Sun,CLaMP: Contrastive Language-Music Pre-Training for Cross-Modal Symbolic Music Information Retrieval,2023,https://doi.org/10.5281/zenodo.10265247,Shangda Wu+Central Conservatory of Music>CHN>education|Microsoft Research Asia>CHN>company|Tsinghua University>CHN>education;Dingyao Yu+Microsoft Research Asia>CHN>company;Xu Tan+Microsoft Research Asia>CHN>company;Maosong Sun+Central Conservatory of Music>CHN>education|Microsoft Research Asia>CHN>company|Tsinghua University>CHN>education,"We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description. In comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP demonstrated comparable or superior performance on score-oriented datasets. Our models and code are available at https://github.com/microsoft/muzic/tree/main/clamp.",CHN,education,Developing economies,"[-8.121748, 24.505383]","[-10.563575, -28.104265]","[-7.6102242, 15.32475, -15.079842]","[0.6996744, 0.85383993, -22.785742]","[12.648684, 7.475497]","[9.762619, 4.974246]","[13.735751, 12.860343, -1.1161171]","[10.929529, 6.2834134, 9.284837]"
17,Luca Marinelli;György Fazekas;Charalampos Saitis,Gender-Coded Sound: Analysing the Gendering of Music in Toy Commercials via Multi-Task Learning,2023,https://doi.org/10.5281/zenodo.10265249,"Luca Marinelli+C4DM, Queen Mary University of London>GBR>education;György Fazekas+C4DM, Queen Mary University of London>GBR>education;Charalampos Saitis+C4DM, Queen Mary University of London>GBR>education","Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from examining music in isolation, while studies of the gendering of music within multimodal communicative events are sparse. In this paper, we outline a method to automatically analyse how music in TV advertising aimed at children may be deliberately used to reinforce traditional gender roles. Our dataset of 606 commercials included music-focused mid-level perceptual features, multimodal aesthetic emotions, and content analytical items. Despite its limited size, and because of the extreme gender polarisation inherent in toy advertisements, we obtained noteworthy results by leveraging multi-task transfer learning on our densely annotated dataset. The models were trained to categorise commercials based on their intended target audience, specifically distinguishing between masculine, feminine, and mixed audiences. Additionally, to provide explainability for the classification in gender targets, the models were jointly trained to perform regressions on emotion ratings across six scales, and on mid-level musical perceptual attributes across twelve scales. Standing in the context of MIR, computational social studies and critical analysis, this study may benefit not only music scholars but also advertisers, policymakers, and broadcasters.",GBR,education,Developed economies,"[-46.83995, 15.618043]","[47.407402, -8.45242]","[-9.302172, 28.036364, 5.7030325]","[11.575798, 23.322346, 0.34133804]","[15.190185, 8.795228]","[12.985724, 4.1129518]","[15.320425, 14.840752, -1.1496624]","[14.063524, 5.098684, 10.300563]"
18,Li-Yang Tseng;Tzu-Ling Lin;Hong-Han Shuai;Jen-Wei Huang;Wen-Whei Chang,A Dataset and Baselines for Measuring and Predicting the Music Piece Memorability,2023,https://doi.org/10.5281/zenodo.10265251,Li-Yang Tseng+National Yang Ming Chiao Tung University>TWN>education|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown;Tzu-Ling Lin+National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown;Hong-Han Shuai+National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown;Jen-Wei Huang+National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown;Wen-Whei Chang+National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown|National Yang Ming Chiao Tung University>Unknown>Unknown,"Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspired by this phenomenon, we focus on measuring and predicting music memorability. To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure. We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs. To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods. Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible. Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music. As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research.",TWN,education,Developing economies,"[-16.808, 6.6109586]","[-16.665987, -33.738556]","[-9.60485, -7.560308, -6.429051]","[-11.89555, -1.6973715, -14.091044]","[12.686929, 7.692139]","[9.326595, 5.0268993]","[13.726023, 13.404746, -1.0922803]","[10.377887, 6.168045, 9.404366]"
19,Carlos Peñarrubia;Carlos Garrido-Munoz;Jose J. Valero-Mas;Jorge Calvo-Zaragoza,Efficient Notation Assembly in Optical Music Recognition,2023,https://doi.org/10.5281/zenodo.10265253,Carlos Penarrubia+University of Alicante>ESP>education;Carlos Garrido-Munoz+University of Alicante>ESP>education;Jose J. Valero-Mas+Universitat Pompeu Fabra>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"Optical Music Recognition (OMR) is the field of research that studies how to computationally read music notation from written documents. Thanks to recent advances in computer vision and deep learning, there are successful approaches that can locate the music-notation elements from a given music score image. Once detected, these elements must be related to each other to reconstruct the musical notation itself, in the so-called notation assembly stage. However, despite its relevance in the eventual success of the OMR, this stage has been barely addressed in the literature. This work presents a set of neural approaches to perform this assembly stage. Taking into account the number of possible syntactic relationships in a music score, we give special importance to the efficiency of the process in order to obtain useful models in practice. Our experiments, using the MUSCIMA++ handwritten sheet music dataset, show that the considered approaches are capable of outperforming the existing state of the art in terms of efficiency with limited (or no) performance degradation. We believe that the conclusions of this work provide novel insights into the notation assembly step, while indicating clues on how to approach the previous stages of the OMR and improve the overall performance.",ESP,education,Developed economies,"[41.533474, 20.158655]","[-19.628336, -23.52996]","[21.252148, 16.212847, 11.926901]","[-14.341019, -20.16414, -4.7814803]","[8.627622, 6.0193224]","[6.526374, -1.0734153]","[10.637052, 11.050415, -0.20214674]","[8.069434, 4.197566, 9.977059]"
20,Yuting Yang;Zeyu Jin;Connelly Barnes;Adam Finkelstein,White Box Search Over Audio Synthesizer Parameters,2023,https://doi.org/10.5281/zenodo.10265255,Yuting Yang+Princeton University>USA>education|Adobe Research>USA>company;Zeyu Jin+Adobe Research>USA>company;Connelly Barnes+Adobe Research>USA>company;Adam Finkelstein+Princeton University>USA>education,"Synthesizer parameter inference searches for a set of patch connections and parameters to generate audio that best matches a given target sound. Such optimization tasks benefit from access to accurate gradients. However, typical audio synths incorporate components with discontinuities – such as sawtooth or square waveforms, or a categorical search over discrete parameters like a choice among such waveforms – that thwart conventional automatic differentiation (AD). AD libraries in frameworks like TensorFlow and PyTorch typically ignore discontinuities, providing incorrect gradients at such locations. Thus, SOTA parameter inference methods avoid differentiating the synth directly, and resort to workarounds such as genetic search or neural proxies. Instead, we adapt and extend recent computer graphics methods for differentiable rendering to directly differentiate the synth as a white box program, and thereby optimize its parameters using gradient descent. We evaluate our framework using a generic FM synth with ADSR, noise, and IIR filters, adapting its parameters to match a variety of target audio clips. Our method outperforms baselines in both quantitative and qualitative evaluations.",USA,education,Developed economies,"[14.748336, -35.185944]","[-20.510412, -50.423096]","[30.605633, 1.3965808, 9.032944]","[-24.477093, -6.5916133, -19.583818]","[9.567093, 8.546219]","[8.066317, 6.7419453]","[12.263252, 12.548724, 0.6880361]","[9.543817, 6.0112925, 7.940172]"
35,Charilaos Papaioannou;Emmanouil Benetos;Alexandros Potamianos,From West to East: Who Can Understand the Music of the Others Better?,2023,https://doi.org/10.5281/zenodo.10265287,Charilaos Papaioannou+National Technical University of Athens>GRC>education|Queen Mary University of London>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Alexandros Potamianos+National Technical University of Athens>GRC>education,"Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.",GRC,education,Developed economies,"[-28.066265, 6.047733]","[-21.339579, -36.761845]","[-21.894531, -5.900858, 4.756247]","[-7.7184553, 1.6149966, -17.883106]","[13.923082, 8.487264]","[9.678183, 4.733414]","[14.446982, 14.592518, -1.2024696]","[10.869278, 6.4847445, 9.102729]"
21,Vincent K. M. Cheung;Lana Okuma;Kazuhisa Shibata;Kosetsu Tsukuda;Masataka Goto;Shinichi Furuya,"Decoding Drums, Instrumentals, Vocals, and Mixed Sources in Music Using Human Brain Activity With fMRI",2023,https://doi.org/10.5281/zenodo.10265257,Vincent K. M. Cheung+Sony Computer Science Laboratories>JPN>facility;Lana Okuma+RIKEN Center for Brain Science>JPN>facility;Kazuhisa Shibata+RIKEN Center for Brain Science>JPN>facility;Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Shinichi Furuya+Sony Computer Science Laboratories>JPN>facility,"Brain decoding allows the read-out of stimulus and mental content from neural activity, and has been utilised in various neural-driven classification tasks related to the music information retrieval community. However, even the relatively simple task of instrument classification has only been demonstrated for single- or few-note stimuli when decoding from neural data recorded using functional magnetic resonance imaging (fMRI). Here, we show that drums, instrumentals, vocals, and mixed sources of naturalistic musical stimuli can be decoded from single-trial spatial patterns of auditory cortex activation as recorded using fMRI. Comparing classification based on convolutional neural networks (CNN), random forests (RF), and support vector machines (SVM) further revealed similar neural encoding of vocals and mixed sources, despite vocals being most easily identifiable. These results highlight the prominence of vocal information during music perception, and illustrate the potential of using neural representations towards evaluating music source separation performance and informing future algorithm design.",JPN,facility,Developed economies,"[-24.285873, 8.781108]","[-1.5854148, 24.439253]","[-11.477728, 13.283585, -7.4670167]","[-2.2635632, 17.859282, 9.343771]","[12.902374, 8.209555]","[10.613635, 4.452393]","[13.91926, 13.7174425, -0.76924014]","[11.65722, 4.841003, 10.505979]"
24,Nazif Can Tamer;Yigitcan Özer;Meinard Müller;Xavier Serra,High-Resolution Violin Transcription Using Weak Labels,2023,https://doi.org/10.5281/zenodo.10265263,"Nazif Can Tamer+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Yigitcan Özer+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","A descriptive transcription of a violin performance requires detecting not only the notes but also the fine-grained pitch variations, such as vibrato. Most existing deep learning methods for music transcription do not capture these variations and often need frame-level annotations, which are scarce for the violin. In this paper, we propose a novel method for high-resolution violin transcription that can leverage piece-level weak labels for training. Our conformer-based model works on the raw audio waveform and transcribes violin notes and their corresponding pitch deviations with 5.8 ms frame resolution and 10-cent frequency resolution. We demonstrate that our method (1) outperforms generic systems in the proxy tasks of violin transcription and pitch estimation, and (2) can automatically generate new training labels by aligning its feature representations with unseen scores. We share our model along with 34 hours of score-aligned solo violin performance dataset, notably including the 24 Paganini Caprices.",ESP,education,Developed economies,"[26.16173, -13.751642]","[-38.34468, -6.3093705]","[15.827117, -10.593307, 9.392801]","[-10.20281, -6.324504, -5.977965]","[9.52489, 7.1045184]","[7.8502307, 4.8221726]","[11.597144, 12.200277, -0.26829323]","[9.292434, 6.8176665, 9.348682]"
25,Lejun Min;Junyan Jiang;Gus Xia;Jingwei Zhao,Polyffusion: A Diffusion Model for Polyphonic Score Generation With Internal and External Controls,2023,https://doi.org/10.5281/zenodo.10265265,"Lejun Min+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>CHN>education|Zhiyuan College, Shanghai Jiao Tong University>CHN>education;Junyan Jiang+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>CHN>education;Gus Xia+Music X Lab, NYU Shanghai>CHN>education|MBZUAI>CHN>education;Jingwei Zhao+Institute of Data Science, NUS>SGP>education","We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.",CHN,education,Developing economies,"[20.816975, 0.8079679]","[-12.474179, -44.07958]","[5.130795, 1.7016821, 15.716629]","[-21.102816, -0.85288143, -14.638953]","[10.426984, 8.4106455]","[8.897892, 6.4628444]","[13.276158, 12.038742, 0.13252223]","[9.689406, 5.5354457, 8.595115]"
26,Claire Arthur;Nathaniel Condit-Schultz,The Coordinated Corpus of Popular Musics (CoCoPops): A Meta-Corpus of Melodic and Harmonic Transcriptions,2023,https://doi.org/10.5281/zenodo.10265267,Claire Arthur+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education;Nathaniel Condit-Schultz+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education,"This paper introduces a new corpus, CoCoPops: The Coordinated Corpus of Popular Musics. The corpus can be considered a ""meta corpus"" in that it both extends and combines two existing corpora—the widely-used McGill Bill- board corpus the and RS200 corpus. Both the McGill Billboard corpus and the RS200 contain expert harmonic annotations using different encoding schemes and each represent harmony in fundamentally different ways: Billboard using a root-quality representation and the RS200 using Roman numerals. By combining these corpora into a unified format, using the well-known **kern and**harm representations, we aim to facilitate research in computational musicology, which is frequently burdened by corpora spread across multiple encoding formats. The format will also facilitate cross-corpus comparison with the large body of existing works in **kern format. For a 100-song subset of the CoCoPops-Billboard collection, we also provide participant ratings of continuous valence and arousal ratings, along with the RMS (Root Mean Square) signal level and associated timestamps. In this paper we describe the corpus and the procedures used to create it.",USA,education,Developed economies,"[-19.21423, 2.6649213]","[-7.4270353, 17.526663]","[10.650821, 16.853752, -14.394801]","[-8.3703985, 8.904061, 7.8294625]","[10.785096, 10.867132]","[8.470653, 1.6688238]","[11.982889, 15.072369, -0.13818464]","[10.032746, 6.3114433, 12.00683]"
27,Anja Volk;Tinka Veldhuis;Katrien Foubert;Jos De Backer,Towards Computational Music Analysis for Music Therapy,2023,https://doi.org/10.5281/zenodo.10265269,Anja Volk+Utrecht University>NLD>education;Tinka Veldhuis+Utrecht University>NLD>education;Katrien Foubert+KU Leuven>BEL>education|LUCA School of Arts>BEL>education;Jos de Backer+KU Leuven>BEL>education|LUCA School of Arts>BEL>education,"The research field of music therapy has witnessed a rising interest in recent years to develop and employ computational methods to support therapists in their daily practice. While Music Information Retrieval (MIR) research has identified the area of health and well-being as a promising application field for MIR methods to support health professionals, collaborations with experts in this field are as of today sparse. This paper provides an overview of potential applications of computational music analysis as developed in MIR for the field of active music therapy. We elaborate on the music therapy method of improvisation, with a particular focus on introducing therapeutic concepts that relate to musical structures. We identify application scenarios for analysing musical structures in improvisations, introduce existing analysis methods of therapists, and discuss the potential of MIR methods to support these analyses. Upon identifying a current gap between high-level concepts of therapists and low-level features from existing computational methods, the paper concludes further steps towards developing computational approaches to music analysis for music therapy in an interdisciplinary collaboration.",NLD,education,Developed economies,"[1.8318746, 5.5850863]","[13.75593, 34.51049]","[-5.890738, -4.511674, 7.042313]","[-1.2627724, 12.246671, 14.155953]","[12.36514, 8.01956]","[11.538938, 0.7577275]","[13.235721, 13.683263, -0.8868821]","[11.81348, 4.5906925, 11.488801]"
28,Luca Comanducci;Fabio Antonacci;Augusto Sarti,Timbre Transfer Using Image-to-Image Denoising Diffusion Implicit Models,2023,https://doi.org/10.5281/zenodo.10265271,Luca Comanducci+Politecnico di Milano>ITA>education;Fabio Antonacci+Politecnico di Milano>ITA>education;Augusto Sarti+Politecnico di Milano>ITA>education,"Timbre transfer techniques aim at converting the sound of a musical piece generated by one instrument into the same one as if it was played by another instrument, while maintaining as much as possible the content in terms of musical characteristics such as melody and dynamics. Following their recent breakthroughs in deep learning-based generation, we apply Denoising Diffusion Models (DDMs) to perform timbre transfer. Specifically, we apply the recently proposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate the sampling procedure.  Inspired by the recent application of DDMs to image translation problems we formulate the timbre transfer task similarly, by first converting the audio tracks into log mel spectrograms and by conditioning the generation of the desired timbre spectrogram through the input timbre spectrogram.   We perform both one-to-one and many-to-many timbre transfer, by converting audio waveforms containing only single instruments and multiple instruments, respectively. We compare the proposed technique with existing state-of-the-art methods both through listening tests and objective measures in order to demonstrate the effectiveness of the proposed model.",ITA,education,Developed economies,"[11.1407, -36.14722]","[-22.435867, -47.38329]","[27.234087, 8.270708, 6.496954]","[-20.358625, -3.3863778, -21.408249]","[9.569243, 8.662884]","[8.093393, 6.6789684]","[12.615608, 12.283558, 0.6884251]","[9.593148, 6.0223327, 8.040355]"
29,Neha Rajagopalan;Blair Kaneshiro,Correlation of EEG Responses Reflects Structural Similarity of Choruses in Popular Music,2023,https://doi.org/10.5281/zenodo.10265273,Neha Rajagopalan+Stanford University>USA>education;Blair Kaneshiro+Stanford University>USA>education,"Music structure analysis is a core topic in Music Information Retrieval and could be advanced through the inclusion of new data modalities. In this study we consider neural correlates of music structure processing using popular music - specifically choruses of Bollywood songs - and the {NMED-H} electroencephalographic (EEG) dataset. Motivated by recent findings that listeners' EEG responses correlate when hearing a shared music stimulus, we investigate whether responses correlate not only within single choruses but across pairs of chorus instances as well. We find statistically significant correlations within and across several chorus instances, suggesting that brain responses synchronize across structurally matched music segments even if they are not contextually or acoustically identical. Correlations were only occasionally higher within than across choruses. Our findings advance the state of the art of naturalistic music neuroscience, while also highlighting a novel approach for further studies of music structure analysis and audio understanding more broadly.",USA,education,Developed economies,"[-49.041344, -8.050866]","[-0.73252666, 23.864962]","[-11.702687, 22.342724, 18.947554]","[-0.5469329, 16.325705, 9.236843]","[13.433827, 9.17856]","[10.930809, 4.4090476]","[14.075747, 15.23954, -0.90484446]","[11.786914, 4.7398143, 10.61054]"
30,Mark R. H. Gotham,Chromatic Chords in Theory and Practice,2023,https://doi.org/10.5281/zenodo.10265275,Mark R. H. Gotham+Durham University>GBR>education,"""Chromatic harmony"" is seen as a fundamental part of (extended) tonal music in the Western classical tradition (c.1700–1900). It routinely features in core curricula. Yet even in this globalised and data-driven age, 1) there are significant gaps between how different national ""schools"" identify important chords and progressions, label them, and shape the corresponding curricula; 2) even many common terms lack robust definition; and 3) empirical evidence rarely features, even in in discussions about ""typical"", ""representative"" practice. This paper addresses those three considerations by: 1) comparing English- and German-speaking traditions as an example of this divergence; 2) proposing a framework for defining common terms where that is lacking; and 3) surveying the actual usage of these chromatic chord categories using a computational corpus study of human harmonic analyses.",GBR,education,Developed economies,"[55.161022, -1.0257392]","[-21.12275, 18.747278]","[24.07781, -15.183841, 21.589458]","[-18.755627, -3.405678, 8.7664385]","[6.981697, 8.677144]","[7.8884034, 2.1791835]","[12.012445, 10.508799, 1.8925315]","[10.023994, 7.901281, 12.5161915]"
31,Yo-Wei Hsiao;Tzu-Yun Hung;Tsung-Ping Chen;Li Su,BPS-Motif: A Dataset for Repeated Pattern Discovery of Polyphonic Symbolic Music,2023,https://doi.org/10.5281/zenodo.10265277,Yo-Wei Hsiao+Academia Sinica>TWN>education;Tzu-Yun Hung+Academia Sinica>TWN>education|National Taiwan Normal University>TWN>education;Tsung-Ping Chen+Academia Sinica>TWN>education;Li Su+Academia Sinica>TWN>education,"Intra-opus repeated pattern discovery in polyphonic symbolic music data has  challenges in both algorithm design and data annotation. To solve these challenges, we propose BPS-motif, a new symbolic music dataset containing the note-level annotation of motives and occurrences in Beethoven's piano sonatas. The size of the proposed dataset is larger than previous symbolic datasets for repeated pattern discovery. We report the process of dataset annotation, specifically a peer review process and discussion phase to improve the annotation quality. Finally, we propose a motif discovery method which is shown outperforming baseline methods on repeated pattern discovery.",TWN,education,Developing economies,"[15.74144, 21.4662]","[3.7469788, 17.325333]","[-2.994023, -12.674778, 8.949185]","[-0.4157911, -11.711855, 5.8099685]","[11.595893, 7.44742]","[8.898542, 1.2959771]","[12.64477, 13.054038, -0.70999485]","[10.520032, 6.7551727, 12.739589]"
32,Michael Krause;Sebastian Strahl;Meinard Müller,Weakly Supervised Multi-Pitch Estimation Using Cross-Version Alignment,2023,https://doi.org/10.5281/zenodo.10265279,Michael Krause+International Audio Laboratories Erlangen>DEU>facility;Sebastian Strahl+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Multi-pitch estimation (MPE), the task of detecting active pitches within a polyphonic music recording, has garnered significant research interest in recent years. Most state-of-the-art approaches for MPE are based on deep networks trained using pitch annotations as targets. The success of current methods is therefore limited by the difficulty of obtaining large amounts of accurate annotations. In this paper, we propose a novel technique for learning MPE without any pitch annotations at all. Our approach exploits multiple recorded versions of a musical piece as surrogate targets. Given one version of a piece as input, we train a network to minimize the distance between its output and time-frequency representations of other versions of that piece.  Since all versions are based on the same musical score, we hypothesize that the learned output corresponds to pitch estimates. To further ensure that this hypothesis holds, we incorporate domain knowledge about overtones and noise levels into the network. Overall, our method replaces strong pitch annotations with weaker and easier-to-obtain cross-version targets. In our experiments, we show that our proposed approach yields viable multi-pitch estimates and outperforms two baselines.",DEU,facility,Developed economies,"[24.496834, -22.341408]","[-26.834229, -30.378677]","[13.218459, -15.874611, -14.898133]","[-7.79272, -6.7321577, -15.524719]","[9.987333, 5.6586337]","[7.9574685, 5.1342006]","[10.874886, 13.151579, -0.8761919]","[9.480002, 6.929057, 8.947986]"
33,Patricia Hu;Gerhard Widmer,The Batik-Plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations,2023,https://doi.org/10.5281/zenodo.10265283,"Patricia Hu+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","We present the Batik plays Mozart Corpus, a piano performance dataset combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored Bösendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can further be connected to the musicological annotations (harmony, cadences, phrases) on these scores that were recently published by [1].  The result is a high-quality, high-precision corpus mapping scores and musical structure annotations to precise note-level professional performance information. As the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects.  In the paper, we outline the curation process of the alignment and conduct two exploratory experiments to demonstrate its usefulness in analyzing expressive performance.  [1] Hentschel, J., Neuwirth, M., &amp; Rohrmeier, M. (2021). The Annotated Mozart Sonatas: Score, Harmony, and Cadence. Transactions of the International Society for Music Information Retrieval (TISMIR), Vol. 4, No. 1, pp. 67-80.",AUT,education,Developed economies,"[6.600608, 16.467733]","[-34.086353, 5.4599414]","[-14.16719, -5.6353846, -0.083257794]","[-11.28485, -0.54081124, -0.78454363]","[12.5237465, 7.199701]","[7.6969857, 3.6662898]","[13.326851, 13.428124, -1.5735829]","[9.22707, 6.1537237, 10.756887]"
22,Liyue Zhang;Xinyu Yang;Yichi Zhang;Jing Luo,Dual Attention-Based Multi-Scale Feature Fusion Approach for Dynamic Music Emotion Recognition,2023,https://doi.org/10.5281/zenodo.10265259,Liyue Zhang+Xi'an Jiaotong University>CHN>education;Xinyu Yang+Xi'an Jiaotong University>CHN>education;Yichi Zhang+Xi'an Jiaotong University>CHN>education;Jing Luo+Xi'an Jiaotong University>CHN>education,"Music Emotion Recognition (MER) refers to automatically extracting emotional information from music and predicting its perceived emotions, and it has social and psychological applications. This paper proposes a Dual Attention-based Multi-scale Feature Fusion (DAMFF) method and a newly developed dataset named MER1101 for Dynamic Music Emotion Recognition (DMER). Specifically, multi-scale features are first extracted from the log Mel-spectrogram by multiple parallel convolutional blocks. Then, a Dual Attention Feature Fusion (DAFF) module is utilized to achieve multi-scale context fusion and capture emotion-critical features in both spatial and channel dimensions. Finally, a BiLSTM-based sequence learning model is employed for dynamic music emotion prediction. To enrich existing music emotion datasets, we developed a high-quality dataset, MER1101, which has a balanced emotional distribution, covering over 10 genres, at least four languages, and more than a thousand song snippets. We demonstrate the effectiveness of our proposed DAMFF approach on both the developed MER1101 dataset, as well as on the established DEAM2015 dataset. Compared with other models, our model achieves a higher Consistency Correlation Coefficient (CCC), and has strong predictive power in arousal with comparable results in valence.",CHN,education,Developing economies,"[-59.39935, -2.213472]","[49.32173, -10.685614]","[-27.44891, 21.562763, 8.372092]","[8.863458, 26.86008, 3.3540757]","[14.107511, 12.993667]","[13.007929, 4.389552]","[15.986093, 14.340063, 1.8374914]","[14.19403, 5.030143, 10.1306925]"
61,Sehun Kim;Kazuya Takeda;Tomoki Toda,Sequence-to-Sequence Network Training Methods for Automatic Guitar Transcription With Tokenized Outputs,2023,https://doi.org/10.5281/zenodo.10265341,Sehun Kim+Nagoya University>JPN>education;Kazuya Takeda+Nagoya University>JPN>education;Tomoki Toda+Nagoya University>JPN>education,"We propose multiple methods for effectively training a sequence-to-sequence automatic guitar transcription model which uses tokenized music representation as an output. Our proposed method mainly consists of 1) a hybrid CTC-Attention model for sequence-to-sequence automatic guitar transcription that uses tokenized music representation, and 2) two data augmentation methods for training the model. Our proposed model is a generic encoder-decoder Transformer model but adopts multi-task learning with CTC from the encoder to speed up learning alignments between the output tokens and acoustic features. Our proposed data augmentation methods scale up the amount of training data by 1) creating bar overlap when splitting an excerpt to be used for network input, and 2) by utilizing MIDI-only data to synthetically create audio-MIDI pair data. We confirmed that 1) the proposed data augmentation methods were highly effective for training generic Transformer models that generate tokenized outputs, 2) our proposed hybrid CTC-Attention model outperforms conventional methods that transcribe guitar performance with tokens, and 3) the addition of multi-task learning with CTC in our proposed model is especially effective when there is an insufficient amount of training data.",JPN,education,Developed economies,"[42.947723, -7.518081]","[-18.494234, -32.99397]","[20.62989, -8.479561, 10.349313]","[-10.632671, -4.268875, -9.374368]","[8.053854, 8.374861]","[8.400428, 5.4795113]","[11.834657, 11.366088, 0.99417853]","[9.227312, 6.275971, 9.221201]"
55,Vanessa Nina Borsan;Mathieu Giraud;Richard Groult;Thierry Lecroq,Adding Descriptors to Melodies Improves Pattern Matching: A Study on Slovenian Folk Songs,2023,https://doi.org/10.5281/zenodo.10265329,Vanessa Nina Borsan+University of Lille>FRA>education|CNRS>FRA>facility|Centrale Lille>FRA>education;Mathieu Giraud+University of Lille>FRA>education|CNRS>FRA>facility|Centrale Lille>FRA>education;Richard Groult+University of Rouen Normandie>FRA>education|INSA Rouen Normandie>FRA>education|Université Le Havre Normandie>FRA>education|Normandie Univ>FRA>education;Thierry Lecroq+University of Rouen Normandie>FRA>education|INSA Rouen Normandie>FRA>education|Université Le Havre Normandie>FRA>education|Normandie Univ>FRA>education,"The objective of pattern-matching topics is to gain insights into repetitive patterns within or across various music genres and cultures. This approach aims to shed light on the recurring instances present in diverse musical traditions. The paper presents a study analyzing folk songs using symbolic music representation, including melodic sequences and musical information. By examining a corpus of 400 monophonic Slovenian tunes, we are releasing annotations of structure, contour, and implied harmony. We propose an efficient algorithm based on suffix arrays and bit-vectors to match both music content (melodic sequence) and context (descriptors). Our study reveals that certain descriptors, such as contour types and harmonic ""stability"" exhibit variations based on phrase position within a tune. Additionally, combining melody and descriptors in pattern-matching queries enhances precision for classification tasks. We emphasize the importance of the interplay between melodic sequences and music descriptors, highlighting that different pattern queries may have varying levels of detail requirements. As a result, our approach promotes flexibility in computational music analysis. Lastly, our objective is to foster the knowledge of Slovenian folk songs.",FRA,education,Developed economies,"[8.593686, -3.6324139]","[10.445928, 3.9671605]","[5.779384, 9.1742, -0.043248754]","[2.9890425, -1.3555288, 4.4092593]","[11.3798485, 9.928236]","[8.670616, 1.7055265]","[12.001405, 15.298546, -0.86800677]","[10.245776, 6.834059, 12.392099]"
63,Vanessa Nina Borsan;Mathieu Giraud;Richard Groult,The Games We Play: Exploring the Impact of ISMIR on Musicology,2023,https://doi.org/10.5281/zenodo.10265345,Vanessa Nina Borsan+University of Lille>FRA>education|CNRS>FRA>facility|Centrale Lille>FRA>education;Mathieu Giraud+University of Lille>FRA>education|CNRS>FRA>facility|Centrale Lille>FRA>education;Richard Groult+University of Rouen Normandie>FRA>education|INSA Rouen Normandie>FRA>education|Université Le Havre Normandie>FRA>education|Normandie Univ>FRA>education,"Throughout history, a consistent temporal and spatial gap has persisted between the inception of novel knowledge and technology and their subsequent adoption for extensive practical utilization. The article explores the dynamic interaction and exchange of methodologies between musicology and computational music research. It focuses on an analysis of ten years' worth of papers from the International Society for Music Information Retrieval (ISMIR) from 2012 to 2021. Over 1000 citations of ISMIR papers were reviewed, and out of these, 51 later works published in musicological venues drew from the findings of 28 ISMIR papers. Final results reveal that most contributions from ISMIR rarely make their way to musicology or humanities. Nevertheless, the paper highlights four examples of successful knowledge transfers between the fields and discusses best practices for collaborations while addressing potential causes for such disparities. In the epilogue, we address the interlaced origins of the problem as stemming from the language of new media, institutional restrictions, and the inability to engage in multidisciplinary communication.",FRA,education,Developed economies,"[-14.499848, 48.17881]","[27.397022, 38.14585]","[-29.333654, -5.689464, 1.1199784]","[0.28918132, 7.2075505, 21.820324]","[13.678038, 5.876962]","[11.812825, 0.22451743]","[14.961524, 12.287114, -1.5992535]","[12.088854, 4.2463026, 12.103978]"
91,Haven Kim;Kento Watanabe;Masataka Goto;Juhan Nam,A Computational Evaluation Framework for Singable Lyric Translation,2023,https://doi.org/10.5281/zenodo.10265405,"Haven Kim+Graduate School of Culture Technology, KAIST>KOR>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kento Watanabe+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Juhan Nam+Graduate School of Culture Technology, KAIST>KOR>education","Lyric translation plays a pivotal role in amplifying the global resonance of music, bridging cultural divides, and fostering universal connections. Translating lyrics, unlike conventional translation tasks, requires a delicate balance between singability and semantics. In this paper, we present a computational framework for the quantitative evaluation of singable lyric translation, which seamlessly integrates musical, linguistic, and cultural dimensions of lyrics. Our comprehensive framework consists of four metrics that measure syllable count distance, phoneme repetition similarity, musical structure distance, and semantic similarity. To substantiate the efficacy of our framework, we collected a singable lyrics dataset, which precisely aligns English, Japanese, and Korean lyrics on a line-by-line and section-by-section basis, and conducted a comparative analysis between singable and non-singable lyrics. Our multidisciplinary approach provides insights into the key components that underlie the art of lyric translation and establishes a solid groundwork for the future of computational lyric translation assessment.",KOR,education,Developing economies,"[-27.663567, -32.69958]","[-8.427334, -23.935854]","[10.8899765, 22.493208, 0.98328745]","[0.76733726, 9.7731695, 6.3286924]","[11.30177, 11.781196]","[8.781778, 4.2412095]","[12.269193, 15.832044, 1.1395701]","[10.710133, 7.010668, 9.564882]"
92,Kosetsu Tsukuda;Masahiro Hamasaki;Masataka Goto,Chorus-Playlist: Exploring the Impact of Listening to Only Choruses in a Playlist,2023,https://doi.org/10.5281/zenodo.10265403,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masahiro Hamasaki+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"When people listen to playlists on a music streaming service, they typically listen to each song from start to end in order. However, what if it were possible to use a function to listen to only the choruses of each song in a playlist one after another? In this paper, we call this music listening concept ""chorus-playlist,"" and we investigate its potential impact from various perspectives such as the demand and the objectives for listening to music with chorus-playlist. To this end, we conducted a questionnaire-based online user survey involving 214 participants. Our analysis results suggest reusable insights, including the following: (1) We show a high demand for listening to existing playlists with the chorus-playlist approach. We also reveal preferred options for chorus playback, such as adding crossfade transitions between choruses. (2) People listen to playlists with chorus-playlist for various objectives. For example, when they listen to their own self-made playlists, they want to boost a mood or listen to music in a specific context such as work or driving. (3) There is also a high demand for playlist creation on the premise of continuous listening to only the choruses of the songs in a playlist. The diversities of artists, genres, and moods are more important when creating such a playlist than when creating a usual playlist.",JPN,facility,Developed economies,"[-40.19128, 18.116373]","[40.61909, 26.587816]","[-12.217706, 25.738415, -2.7220194]","[5.9788594, 14.738143, 15.595926]","[15.204926, 9.082678]","[12.733982, 1.2782387]","[14.946197, 15.377716, -1.0647461]","[13.371718, 4.49821, 12.079595]"
93,David Lewis;Elisabete Shibata;Andrew Hankinson;Johannes Kepper;Kevin R. Page;Lisa Rosendahl;Mark Saccomano;Christine Siegert,Supporting Musicological Investigations With Information Retrieval Tools: An Iterative Approach to Data Collection,2023,https://doi.org/10.5281/zenodo.10265407,David Lewis+University of Oxford>GBR>education;Elisabete Shibata+BeethovenHaus Bonn>DEU>education;Andrew Hankinson+RISM Digital Centre>CHE>education;Johannes Kepper+Paderborn University>DEU>education;Kevin R. Page+University of Oxford>GBR>education;Lisa Rosendahl+BeethovenHaus Bonn>DEU>education;Mark Saccomano+Paderborn University>DEU>education;Christine Siegert+BeethovenHaus Bonn>DEU>education,"Digital musicology research often proceeds by extending and enriching its evidence base as it progresses, rather than starting with a complete corpus of data and metadata, as a consequence of an emergent research need.  In this paper, we consider a research workflow which assumes an incremental approach to data gathering and annotation. We describe tooling which implements parts of this workflow, developed to support the study of nineteenth-century music arrangements, and evaluate the applicability of our approach through interviews with musicologists and music editors who have used the tools. We conclude by considering extensions of this approach and the wider implications for digital musicology and music information retrieval.",GBR,education,Developed economies,"[-23.486465, 21.059366]","[26.613194, 37.378662]","[-14.3684, 5.155621, -7.4918857]","[-1.5980905, 7.2638893, 22.139986]","[14.2388315, 8.189578]","[11.475904, 0.16416997]","[14.270252, 14.6733265, -2.0336885]","[11.976998, 4.4134417, 11.992841]"
94,Federico Simonetta;Ana Llorens;Martín Serrano;Eduardo García-Portugués;Álvaro Torrente,Optimizing Feature Extraction for Symbolic Music,2023,https://doi.org/10.5281/zenodo.10265409,Federico Simonetta+ICCMU - Instituto Complutense de Ciencias Musicales>ESP>education;Ana Llorens+Universidad Complutense de Madrid>ESP>education;Martín Serrano+ICCMU - Instituto Complutense de Ciencias Musicales>ESP>education;Eduardo García-Portugués+Universidad Carlos III>ESP>education;Álvaro Torrente+ICCMU - Instituto Complutense de Ciencias Musicales>ESP>education|Universidad Complutense de Madrid>ESP>education,"This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development. The proposed tool also enhances classification accuracy when combined with other sets of features. We demonstrate the contribution of each set of features and the computational resources they require. Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one. To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks.",ESP,education,Developed economies,"[15.142268, 14.704437]","[-0.49614483, 28.65237]","[-2.1398833, -7.176594, 14.7818165]","[-7.7490225, -1.2524366, 13.5490885]","[11.774764, 7.074291]","[9.717587, 2.0281205]","[13.480893, 12.329002, -0.96407735]","[10.806623, 6.094639, 11.382921]"
95,Mathias Rose Bjare;Stefan Lattner;Gerhard Widmer,Exploring Sampling Techniques for Generating Melodies With a Transformer Language Model,2023,https://doi.org/10.5281/zenodo.10265411,"Mathias Bjare+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Stefan Lattner+Sony Computer Science Laboratories (CSL)>FRA>company;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed ""typical sampling"", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.",AUT,education,Developed economies,"[10.751317, -4.5803533]","[-2.9622915, -42.767097]","[9.039468, 9.53381, 2.8275201]","[-25.33713, -3.6149185, -8.283645]","[10.423384, 9.472296]","[9.063476, 6.1819024]","[11.538135, 14.982869, -0.9292573]","[9.458194, 5.4646354, 9.251838]"
96,John Ashley Burgoyne;Janne Spijkervet;David John Baker,Measuring the Eurovision Song Contest: A Living Dataset for Real-World MIR,2023,https://doi.org/10.5281/zenodo.10265415,John Ashley Burgoyne+University of Amsterdam>NLD>education;Janne Spijkervet+ByteDance>CHN>company;David John Baker+University of Amsterdam>NLD>education,"Every year, several dozen, primarily European, countries, send performers to compete on live television at the Eurovision Song Contest, with the goal of entertaining an international audience of more than 150 million viewers. Each participating country is able to evaluate every other country's performance via a combination of rankings from professional jurors and telephone votes from viewers. Between fan sites and the official Song Contest organisation, a complete historical record of musical performances and country-to-country contest scores is available, back to the very first edition in 1956, and for the most recent contests, there is also information about each individual juror's rankings. In this paper, we introduce MIRoVision, a set of scripts which collates the data from these sources into a single, easy-to-use dataset, and a discrete-choice model to convert the raw contest scores into a stable, interval-scale measure of the quality of Eurovision Song Contest entries across the years. We use this model to simulate contest outcomes from previous editions and compare the results to the implied win probabilities from bookmakers at various online betting markets. We also assess how successful content-based MIR could be at predicting Eurovision outcomes, using state-of-the-art music foundation models. Given its annual recurrence, emphasis on new music and lesser-known artists, and sophisticated voting structure, the Eurovision Song Contest is an outstanding testing ground for MIR algorithms, and we hope that this paper will inspire the community to use the contest as a regular assessment of the strength of modern MIR.",NLD,education,Developed economies,"[-21.81247, 7.373133]","[28.139523, 29.036535]","[-24.43561, 0.83185667, 2.6886501]","[6.3917317, 7.665971, 12.503333]","[12.852524, 7.770167]","[11.839264, 1.5609735]","[14.256411, 13.65184, -0.9043147]","[12.889381, 5.0434294, 12.3178835]"
97,Pablo Alonso-Jiménez;Xavier Serra;Dmitry Bogdanov,Efficient Supervised Training of Audio Transformers for Music Representation Learning,2023,https://doi.org/10.5281/zenodo.10265414,Pablo Alonso-Jiménez+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education;Dmitry Bogdanov+Universitat Pompeu Fabra>ESP>education,"In this work, we address music representation learning using convolution-free transformers. We build on top of existing spectrogram-based audio transformers such as AST and train our models on a supervised task using patchout training similar to PaSST. In contrast to previous works, we study how specific design decisions affect downstream music tagging tasks instead of focusing on the training task. We assess the impact of initializing the training with different existing weights, using various input audio segment lengths, using learned representations from different blocks and tokens of the transformer for downstream tasks, and applying patchout at inference to speed up feature extraction. We find that 1) initializing the audio training from ImageNet or AudioSet weights and longer input segments are beneficial both for the training and downstream tasks, 2) the best representations for the downstream tasks are located in the middle blocks of the transformer, and 3) using patchout at inference allows faster processing than our convolutional baselines while maintaining superior performance. The resulting models, MAEST, are publicly available and obtain the best performance among open models in music tagging tasks.",ESP,education,Developed economies,"[-14.34894, -6.953527]","[-21.643755, -38.364544]","[3.2259958, 12.416041, 10.488506]","[-8.040242, -0.7038769, -21.0342]","[11.43458, 9.0483465]","[9.620401, 4.93927]","[13.488611, 13.063784, 0.44323945]","[10.764215, 6.4248557, 8.868053]"
98,Michael Krause;Christof Weiß;Meinard Müller,A Cross-Version Approach to Audio Representation Learning for Orchestral Music,2023,https://doi.org/10.5281/zenodo.10265419,Michael Krause+International Audio Laboratories Erlangen>DEU>facility;Christof Weiß+University of Würzburg>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Deep learning systems have become popular for tackling a variety of music information retrieval tasks. However, these systems often require large amounts of labeled data for supervised training, which can be very costly to obtain. To alleviate this problem, recent papers on learning music audio representations employ alternative training strategies that utilize unannotated data. In this paper, we introduce a novel cross-version approach to audio representation learning that can be used with music datasets containing several versions (performances) of a musical work. Our method exploits the correspondences that exist between two versions of the same musical section. We evaluate our proposed cross-version approach qualitatively and quantitatively on complex orchestral music recordings and show that it can better capture aspects of instrumentation compared to techniques that do not use cross-version information.",DEU,facility,Developed economies,"[-14.802441, -6.951894]","[-22.182293, -32.577217]","[5.0197425, 11.861814, 10.587355]","[-9.710495, -4.61185, -17.72408]","[11.413162, 8.917667]","[9.288626, 4.7231255]","[13.414674, 13.045124, 0.39574668]","[10.310037, 6.472494, 9.166152]"
99,Tomoyasu Nakano;Masataka Goto,"Music Source Separation With MLP Mixing of Time, Frequency, and Channel",2023,https://doi.org/10.5281/zenodo.10265417,Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper proposes a new music source separation (MSS) model based on an architecture with MLP-Mixer that leverages multilayer perceptrons (MLPs). Most of the recent MSS techniques are based on architectures with CNNs, RNNs, and attention-based transformers that take waveforms or complex spectrograms or both as inputs. For the growth of the research field, we believe it is important to study not only the current established methodologies but also diverse perspectives. Therefore, since the MLP-Mixer-based architecture has been reported to perform as well as or better than architectures with CNNs and transformers in the computer vision field despite the MLP's simple computation, we report a way to effectively apply such an architecture to MSS as a reusable insight. In this paper we propose a model called TFC-MLP, which is a variant of the MLP-Mixer architecture that preserves time-frequency positional relationships and mixes time, frequency, and channel dimensions separately, using complex spectrograms as input. The TFC-MLP was evaluated with source-to-distortion ratio (SDR) using the MUSDB18-HQ dataset. Experimental results showed that the proposed model can achieve competitive SDRs when compared with state-of-the-art MSS models.",JPN,facility,Developed economies,"[8.17226, -44.94707]","[-40.211624, -35.47871]","[28.77881, -2.4979963, -5.5285363]","[-16.439695, -5.8042417, -25.214233]","[8.411794, 9.926166]","[6.837787, 5.9672966]","[11.089126, 13.547745, 1.5870388]","[9.615343, 8.263357, 8.744114]"
100,Huan Zhang;Emmanouil Karystinaios;Simon Dixon;Gerhard Widmer;Carlos Eduardo Cancino-Chacón,Symbolic Music Representations for Classification Tasks: A Systematic Evaluation,2023,https://doi.org/10.5281/zenodo.10265421,Huan Zhang+Queen Mary University of London>GBR>education;Emmanouil Karystinaios+Johannes Kepler University>AUT>education;Simon Dixon+Queen Mary University of London>GBR>education;Gerhard Widmer+Johannes Kepler University>AUT>education;Carlos Eduardo Cancino-Chacón+Johannes Kepler University>AUT>education,"Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language-like fashion. However, symbolic music is neither an image nor a sentence intrinsically, and research in the symbolic domain is lacking a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation.",GBR,education,Developed economies,"[13.96707, 15.511695]","[-15.079164, -28.803753]","[-4.5024333, -6.4264226, 13.715011]","[-18.512371, -11.833111, -12.644912]","[11.808137, 7.2383738]","[8.738288, 5.241312]","[13.432287, 12.425426, -0.8501895]","[9.796708, 6.104617, 9.472418]"
90,Shuqi Dai;Yuxuan Wu;Siqi Chen;Roy Huang;Roger B. Dannenberg,SingStyle111: A Multilingual Singing Dataset With Style Transfer,2023,https://doi.org/10.5281/zenodo.10265401,Shuqi Dai+Carnegie Mellon University>USA>education;Yuxuan Wu+Carnegie Mellon University>USA>education;Siqi Chen+University of Southern California>USA>education;Roy Huang+Carnegie Mellon University>USA>education;Roger B. Dannenberg+Carnegie Mellon University>USA>education,"There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.",USA,education,Developed economies,"[-4.8866396, -37.98882]","[-30.933027, -42.250717]","[20.762102, 12.355762, -7.563816]","[4.7976747, -9.746684, -19.728115]","[9.9674835, 10.9224]","[7.747592, 4.589439]","[11.44494, 14.961174, 0.81797725]","[10.315175, 7.4535823, 9.09884]"
101,Jacopo de Berardinis;Valentina Anita Carriero;Albert Meroño-Peñuela;Andrea Poltronieri;Valentina Presutti,The Music Meta Ontology: A Flexible Semantic Model for the Interoperability of Music Metadata,2023,https://doi.org/10.5281/zenodo.10265423,Jacopo de Berardinis+King's College London>GBR>education|University of Bologna>ITA>education;Valentina Anita Carriero+University of Bologna>ITA>education|King's College London>GBR>education;Albert Meroño-Penuela+King's College London>GBR>education|University of Bologna>ITA>education;Andrea Poltronieri+University of Bologna>ITA>education|King's College London>GBR>education;Valentina Presutti+University of Bologna>ITA>education,"The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity of musical concepts arising from different genres, styles, and periods – standing to benefit from a lingua franca to accommodate various stakeholders (musicologists, librarians, data engineers, etc.). To initiate this transition, we introduce the Music Meta ontology, a rich and flexible semantic model to describe music metadata related to artists, compositions, performances, recordings, and links. We follow eXtreme Design methodologies and best practices for data engineering, to reflect the perspectives and the requirements of various stakeholders into the design of the model, while leveraging ontology design patterns and accounting for provenance at different levels (claims, links). After presenting the main features of Music Meta, we provide a first evaluation of the model, alignments to other schema (Music Ontology, DOREMUS, Wikidata), and support for data transformation.",GBR,education,Developed economies,"[-25.835434, 36.818436]","[17.241955, 38.494877]","[-22.449427, 0.40851164, -5.2724643]","[-2.863428, -2.7414327, 25.069548]","[14.240392, 9.040149]","[10.919952, -0.08484495]","[15.129735, 13.750091, -1.2211487]","[12.080643, 5.07889, 11.670848]"
8,Nathan Fradet;Nicolas Gutowski;Fabien Chhel;Jean-Pierre Briot,Impact of Time and Note Duration Tokenizations on Deep Learning Symbolic Music Modeling,2023,https://doi.org/10.5281/zenodo.10265229,Nathan Fradet+Sorbonne University>FRA>education|Aubay>FRA>company;Nicolas Gutowski+University of Angers>FRA>education;Fabien Chhel+University of Angers>FRA>education|ESEO-TECH / ERIS>FRA>facility;Jean-Pierre Briot+Sorbonne University>FRA>education,"Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways, and recent research has focused on developing more efficient methods. However, the key differences between these methods are often unclear, and few studies have compared them. In this work, we analyze the current common tokenization methods and experiment with time and note duration representations. We compare the performance of these two impactful criteria on several tasks, including composer classification, emotion classification, music generation, and sequence representation. We demonstrate that explicit information leads to better results depending on the task.",FRA,education,Developed economies,"[15.117135, 13.473921]","[-13.931753, -35.42298]","[-2.1993723, -4.224286, 14.440372]","[-15.82334, -3.6226444, -9.304459]","[11.714079, 7.1018147]","[8.787288, 5.6658125]","[13.376126, 12.273761, -0.8843854]","[9.533394, 5.893121, 9.257979]"
7,Michele Newman;Lidia Morris;Jin Ha Lee,Human-AI Music Creation: Understanding the Perceptions and Experiences of Music Creators for Ethical and Productive Collaboration,2023,https://doi.org/10.5281/zenodo.10265227,Michele Newman+University of Washington>USA>education;Lidia Morris+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"Recently, there has been a surge in Artificial Intelligence (AI) tools that allow creators to develop melodies, harmonies, lyrics, and mixes with the touch of a button. The reception of and discussion on the use of these tools - and more broadly, any AI-based art creation tools - tend to be polarizing, with opinions ranging from enthusiasm about their potential to fear about how these tools will impact the livelihood and creativity of human creators. However, a more desirable future path is most likely somewhere in between these two polar opposites where productive and ethical human-AI collaboration could happen through the use of these tools. To explore this possibility, we first need to improve our understanding of how music creators perceive and utilize these types of tools in their creative process. We conducted case studies of a range of music creators to better understand their perception and usage of AI-based music creation tools. Through a thematic analysis of these cases, we identify the opportunities and challenges related to the use of AI for music creation from the perspective of the musicians and discuss the design implications for AI music tools.",USA,education,Developed economies,"[-26.37109, 5.168572]","[-5.773972, 44.129524]","[-25.106897, -4.728517, 6.209308]","[-19.495066, 10.032284, 3.895556]","[12.834149, 7.5497885]","[9.925621, 5.7337437]","[14.434851, 13.070684, -1.2318166]","[10.393427, 5.0942435, 9.622538]"
6,Michèle Duguay;Kate Mancey;Johanna Devaney,Collaborative Song Dataset (CoSoD): An Annotated Dataset of Multi-Artist Collaborations in Popular Music,2023,https://doi.org/10.5281/zenodo.10265225,"Michèle Duguay+Harvard University>USA>education;Kate Mancey+Harvard University>USA>education;Johanna Devaney+Brooklyn College and Graduate Center, City University of New York>USA>education","The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010–2019 Billboard ""Hot 100"" year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, panning, and gender of the performers), and relevant metadata. CoSoD complements other popular music datasets by focusing exclusively on musical collaborations between independent acts. In addition to facilitating the study of song form and vocal production, CoSoD allows for the in-depth study of gender as it relates to various timbral, pitch, and formal parameters in musical collaborations. In this paper, we detail the contents of the dataset and outline the annotation process. We also present an experiment using CoSoD that examines how the use of reverberation, layering, and panning are related to the gender of the artist. In this experiment, we find that men's voices are on average treated with less reverberation and occupy a more narrow position in the stereo mix than women's voices.",USA,education,Developed economies,"[-36.500435, 10.699858]","[8.505445, -20.306438]","[-24.143627, 4.5207524, 4.509046]","[11.799169, -8.249147, -19.223654]","[14.3638, 9.335044]","[7.670105, 3.0735643]","[14.742182, 14.223459, -0.738082]","[9.668128, 7.437997, 10.781222]"
5,Changhong Wang;Gaël Richard;Brian McFee,Transfer Learning and Bias Correction With Pre-Trained Audio Embeddings,2023,https://doi.org/10.5281/zenodo.10265223,"Changhong Wang+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Gaël Richard+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Brian McFee+New York University>USA>education","Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applications in MIR have sufficient quantities of training data, it is becoming increasingly common to transfer models across domains. This approach allows representations derived for one task to be applied to another, and can result in high accuracy with less stringent training data requirements for the downstream task. However, the properties of pre-trained audio embeddings are not fully understood. Specifically, and unlike traditionally engineered features, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition. We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets.",FRA,education,Developed economies,"[-16.970861, -12.026254]","[-19.665407, -37.415726]","[4.1680274, 4.1151347, 8.7137165]","[-10.828933, -0.33060327, -19.234518]","[11.271601, 9.417605]","[9.426147, 4.949777]","[13.127806, 13.069622, 0.9288424]","[10.522661, 6.3551, 8.904681]"
4,Gowriprasad R;Srikrishnan Sridharan;R Aravind;Hema A. Murthy,Segmentation and Analysis of Taniavartanam in Carnatic Music Concerts,2023,https://doi.org/10.5281/zenodo.10265221,Gowriprasad R+Indian Institute of Technology Madras>IND>education;Srikrishnan Sridharan+Unknown>Unknown>Unknown;R Aravind+Indian Institute of Technology Madras>IND>education;Hema A Murthy+Indian Institute of Technology Madras>IND>education,"In Carnatic music concerts, taniavartanam is a solo percussion segment that showcases intricate and elaborate extempore rhythmic evolution through a series of homogeneous sections with shared rhythmic characteristics. While taniavartanam segments have been segmented from concerts earlier, no effort has been made to analyze these percussion segments. This paper attempts to further segment the taniavartanam portion into musically meaningful segments. A taniavartanam segment consists of an abhipraya, where artists show their prowess at extempore enunciation of percussion stroke segments, followed by an optional korapu, where each artist challenges the other, and concluding with mohra and korvai, each with its own nuances. This work helps obtain a comprehensive musical description of the taniavartanam in Carnatic concerts. However, analysis is complicated owing to a plethora of tala and nade. The segmentation of a taniavartanam section can be used for further analysis, such as stroke sequence recognition, and help find relations between different learning schools. The study uses 12 hours of taniavartanam segments consisting of four tala-s and five nade-s for analysis and achieves 0.85 F1-score in the segmentation task.",IND,education,Developing economies,"[-0.12190941, -6.2159567]","[-23.844122, 8.243773]","[8.532921, -9.077719, -3.5636692]","[1.2426542, 9.834062, -14.9733305]","[11.614498, 8.152162]","[6.7701473, 1.5301547]","[12.264593, 14.268033, -0.06997418]","[8.597485, 7.0128646, 11.927165]"
3,Bob L. T. Sturm;Arthur Flexer,A Review of Validity and Its Relationship to Music Information Research,2023,https://doi.org/10.5281/zenodo.10265219,Bob L. T. Sturm+KTH Stockholm>SWE>education;Arthur Flexer+Johannes Kepler University Linz>AUT>education,"Validity is the truth of an inference made from evidence and is a central concern in scientific work. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has been so far. Puzzling MIR phenomena like adversarial attacks, horses, and performance glass ceilings become less mysterious through the lens of validity. In this paper, we review the subject of validity as presented in a key reference of causal inference: Shadish et al., Experimental and Quasi-experimental Designs for Generalised Causal Inference [1]. We discuss the four types of validity and threats to each one. We consider them in relationship to MIR experiments grounded with a practical demonstration using a typical MIR experiment.",SWE,education,Developed economies,"[-33.95851, 21.959555]","[26.178307, 44.540905]","[-20.888844, 13.613739, -8.064091]","[1.7635524, 7.138369, 11.512224]","[14.836082, 8.33297]","[11.955261, 0.7147187]","[14.8868065, 15.064918, -1.7336942]","[12.295409, 4.418505, 12.201639]"
2,Fabio Morreale;Megha Sharma;I-Chieh Wei,Data Collection in Music Generation Training Sets: A Critical Analysis,2023,https://doi.org/10.5281/zenodo.10265217,Fabio Morreale+University of Auckland>NZL>education;Megha Sharma+University of Tokyo>JPN>education;I-Chieh Wei+University of Auckland>NZL>education,"The practices of data collection in training sets for Automatic Music Generation (AMG) tasks are opaque and overlooked. In this paper, we aimed to identify these practices and surface the values they embed. We systematically identified all datasets used to train AMG models presented at the last ten editions of ISMIR. For each dataset, we checked how it was populated and the extent to which musicians wittingly contributed to its creation.\ Almost half of the datasets (42.6%) were indiscriminately populated by accumulating music data available online without seeking any sort of permission. We discuss the ideologies that underlie this practice and propose a number of suggestions AMG dataset creators might follow. Overall, this paper contributes to the emerging  self-critical corpus of work of the ISMIR community, reflecting on the ethical considerations and the social responsibility of our work.",NZL,education,Developed economies,"[-19.311497, 6.5288963]","[-4.305951, 44.41473]","[-21.036335, -0.05139367, 5.103224]","[-16.991552, 10.11838, 4.897968]","[12.801347, 7.726429]","[9.982986, 5.437402]","[14.118127, 13.419446, -0.8009742]","[10.513957, 5.1920366, 9.800717]"
1,Miguel Perez;Holger Kirchhoff;Xavier Serra,TriAD: Capturing Harmonics With 3D Convolutions,2023,https://doi.org/10.5281/zenodo.10265215,"Miguel Perez+Huawei, Munich Research Center>DEU>company;Holger Kirchhoff+Unknown>Unknown>Unknown;Xavier Serra+MTG, Universitat Pompeu Fabra>ESP>education","Thanks to advancements in deep learning (DL), automatic music transcription (AMT) systems recently outperformed previous ones fully based on manual feature design. Many of these highly capable DL models, however, are computationally expensive. Researchers are moving towards smaller models capable of maintaining state-of-the-art (SOTA) results by embedding musical knowledge in the network architecture. Existing approaches employ convolutional blocks specifically designed to capture the harmonic structure. These approaches, however, require either large kernels or multiple kernels, with each kernel aiming to capture a different harmonic. We present TriAD, a convolutional block that achieves an unequally distanced dilation over the frequency axis. This allows our method to capture multiple harmonics with a single yet small kernel. We compare TriAD with other methods of capturing harmonics, and we observe that our approach maintains SOTA results while reducing the number of parameters required. We also conduct an ablation study showing that our proposed method effectively relies on harmonic information.",DEU,company,Developed economies,"[32.786194, -16.025314]","[-29.91796, -31.585657]","[27.284666, -10.8628845, -9.841822]","[-6.9453316, -10.489806, -15.505712]","[9.1443, 9.139348]","[7.7898593, 5.372079]","[11.41272, 13.439257, 0.453977]","[9.472121, 7.0085816, 8.77014]"
0,Shreyas Nadkarni;Sujoy Roychowdhury;Preeti Rao;Martin Clayton,Exploring the Correspondence of Melodic Contour With Gesture in Raga Alap Singing,2023,https://doi.org/10.5281/zenodo.10265213,Shreyas Nadkarni+Indian Institute of Technology Bombay>IND>education;Sujoy Roychowdhury+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education;Martin Clayton+Durham University>GBR>education,"Musicology research suggests a correspondence between manual gesture and melodic contour in raga performance. Computational tools such as pose estimation from video and time series pattern matching potentially facilitate larger-scale studies of gesture and audio correspondence. We present a dataset of audiovisual recordings of Hindustani vocal music comprising 9 ragas sung by 11 expert performers. With the automatic segmentation of the audiovisual time series based on analyses of the extracted F0 contour, we study whether melodic similarity implies gesture similarity. Our results indicate that specific representations of gesture kinematics can predict high-level melodic features such as held notes and raga-characteristic motifs significantly better than chance.",IND,education,Developing economies,"[4.624209, -3.0550222]","[6.0756783, -18.886908]","[9.255609, 4.535598, -19.688126]","[15.889527, -10.821943, -3.9676278]","[11.03797, 10.522037]","[7.462191, 1.1598952]","[11.536643, 15.2523365, -1.402159]","[9.102331, 6.993031, 12.557185]"
11,Behzad Haki;Błażej Kotowski;Cheuk Lun Isaac Lee;Sergi Jordà,TapTamDrum: A Dataset for Dualized Drum Patterns,2023,https://doi.org/10.5281/zenodo.10265237,"Behzad Haki+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Błażej Kotowski+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Cheuk Lun Isaac Lee+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sergi Jordà+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Drummers spend extensive time practicing rudiments to develop technique, speed, coordination, and phrasing. These rudiments are often practiced on ""silent"" practice pads using only the hands. Additionally, many percussive instruments across cultures are played exclusively with the hands. Building on these concepts and inspired by Einstein's probably apocryphal quote, ""Make everything as simple as possible, but not simpler,"" we hypothesize that a dual-voice reduction could serve as a natural and meaningful compressed representation of multi-voiced drum patterns. This representation would retain more information than its corresponding monotonic representation while maintaining relative simplicity for tasks such as rhythm analysis and generation. To validate this potential representation, we investigate whether experienced drummers can consistently represent and reproduce the rhythmic essence of a given drum pattern using only their two hands. We present TapTamDrum: a novel dataset of repeated dualizations from four experienced drummers, along with preliminary analysis and tools for further exploration of the data.",ESP,education,Developed economies,"[25.385439, -47.231117]","[-19.392197, 5.070572]","[20.736874, -20.44095, 0.013561157]","[3.298159, 16.05452, -10.195126]","[7.7729115, 7.0175133]","[6.3644137, 1.7908096]","[10.171572, 11.685303, 0.9598017]","[8.231719, 6.5674663, 11.707917]"
102,Jeff Miller;Johan Pauwels;Mark Sandler,Polar Manhattan Displacement: Measuring Tonal Distances Between Chords Based on Intervallic Content,2023,https://doi.org/10.5281/zenodo.10265427,Jeff Miller+Queen Mary University of London>GBR>education;Johan Pauwels+Queen Mary University of London>GBR>education;Mark Sandler+Queen Mary University of London>GBR>education,"Large-scale studies of musical harmony are often hampered by lack of suitably labelled data. It would be highly advantageous if an algorithm were able to autonomously describe chords, scales, etc. in a consistent and musically informative way. In this paper, we revisit tonal interval vectors (TIVs), which reveal certain insights as to the interval and tonal nature of pitch class sets. We then describe the qualities and criteria required to comprehensively and consistently measure displacements between TIVs. Next, we present the Polar Manhattan Displacement (PMD), a compound magnitude and phase measure for describing the displacements between pitch class sets in a tonally-informed manner. We end by providing examples of how PMD can be used in automated harmonic sequence analysis over a complex chord vocabulary.",GBR,education,Developed economies,"[58.380417, -0.017007567]","[-20.481117, 17.143827]","[28.73082, -18.887844, 15.91534]","[-19.396568, -6.6141343, 6.174053]","[6.957007, 8.675566]","[7.766451, 2.2778702]","[11.882488, 10.599414, 1.9504646]","[10.000408, 7.9654016, 12.4379]"
89,Carey Bunks;Tillman Weyde;Simon Dixon;Bruno Di Giorgi,Modeling Harmonic Similarity for Jazz Using Co-occurrence Vectors and the Membrane Area,2023,https://doi.org/10.5281/zenodo.10265400,"Carey Bunks+Queen Mary University of London>GBR>education;Tillman Weyde+City, University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education;Bruno Di Giorgi+Apple>GBR>company","In jazz, measuring harmonic similarity is complicated by the common practice of reharmonization -- the altering or substitution of chords without fundamentally changing the piece's harmonic identity. This is analogous to natural language processing tasks where synonymous terms can be used interchangeably without significantly modifying the meaning of a text.  Our approach to modeling harmonic similarity borrows from NLP techniques, such as distributional semantics, by embedding chords into a vector space using a co-occurrence matrix.  We show that the method can robustly detect harmonic similarity between songs, even when reharmonized.  The co-occurrence matrix is computed from a corpus of symbolic jazz-chord progressions, and the result is a map from chords into vectors. A song's harmony can then be represented as a piecewise-linear path constructed from the cumulative sum of its chord vectors.  For any two songs, their harmonic similarity can be measured as the minimal surface membrane area between their vector paths.  Using a dataset of jazz contrafacts, we show that our approach reduces the median rank of matches from 318 to 18 compared to a baseline approach using pitch class vectors.",GBR,education,Developed economies,"[5.358935, 13.945752]","[-22.866627, 18.237291]","[-12.314383, -2.3699012, 29.808481]","[-22.414629, -3.7856874, 9.783884]","[11.315622, 9.432867]","[7.842746, 2.2960784]","[12.623789, 14.655105, -0.78223467]","[10.215276, 7.936509, 12.625205]"
88,Geoffroy Peeters,Self-Similarity-Based and Novelty-Based Loss for Music Structure Analysis,2023,https://doi.org/10.5281/zenodo.10265397,"Geoffroy Peeters+Télécom-Paris, Institut Polytechnique de Paris>FRA>education","Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity.  In this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels.  For this we jointly optimize  - a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and  - a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss.  We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA.  Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.",FRA,education,Developed economies,"[-4.2176127, 5.4144244]","[1.4105873, 2.76746]","[-3.3217268, -1.8778797, -0.60186076]","[-0.17253627, 3.4093275, -2.5080144]","[12.43366, 8.842206]","[8.352111, 3.2175984]","[13.189648, 14.311678, -0.38640153]","[10.669269, 7.535609, 11.337452]"
87,Alexandre D'Hooge;Louis Bigo;Ken Déguernel,Modeling Bends in Popular Music Guitar Tablatures,2023,https://doi.org/10.5281/zenodo.10265396,Alexandre D’Hooge+University of Lille>FRA>education;Louis Bigo+University of Lille>FRA>education;Ken Déguernel+University of Lille>FRA>education,"Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends. This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard.  In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 and a limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.",FRA,education,Developed economies,"[46.03347, -10.045412]","[-43.29243, -8.00378]","[24.772377, -13.862112, 7.4286118]","[-17.95103, -8.435527, -3.9501722]","[7.626227, 8.232883]","[7.2205076, 4.2602253]","[11.580994, 11.206745, 1.3710494]","[9.075818, 6.621464, 9.937527]"
64,Genís Plaja-Roglans;Marius Miron;Adithi Shankar;Xavier Serra,Carnatic Singing Voice Separation Using Cold Diffusion on Training Data With Bleeding,2023,https://doi.org/10.5281/zenodo.10265347,"Genís Plaja-Roglans+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Earth Species Project>Unknown>Unknown;Marius Miron+Earth Species Project>Unknown>Unknown;Adithi Shankar+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Supervised music source separation systems using deep learning are trained by minimizing a loss function between pairs of predicted separations and ground-truth isolated sources. However, open datasets comprising isolated sources are few, small, and restricted to a few music styles. At the same time, multi-track datasets with source bleeding are usually found larger in size, and are easier to compile. In this work, we address the task of singing voice separation when the ground-truth signals have bleeding and only the target vocals and the corresponding mixture are available. We train a cold diffusion model on the frequency domain to iteratively transform a mixture into the corresponding vocals with bleeding. Next, we build the final separation masks by clustering spectrogram bins according to their evolution along the transformation steps. We test our approach on a Carnatic music scenario for which solely datasets with bleeding exist, while current research on this repertoire commonly uses source separation models trained solely with Western commercial music. Our evaluation on a Carnatic test set shows that our system improves Spleeter on interference removal and it is competitive in terms of signal distortion. Code is open sourced",ESP,education,Developed economies,"[-1.73197, -40.98585]","[-39.578842, -32.822594]","[23.706612, 7.88618, -8.33901]","[-13.159441, -5.881483, -25.3649]","[9.154158, 10.787092]","[6.77742, 5.815222]","[10.87635, 14.623984, 1.3306248]","[9.684386, 8.231974, 8.900736]"
65,Kosetsu Tsukuda;Tomoyasu Nakano;Masahiro Hamasaki;Masataka Goto,Unveiling the Impact of Musical Factors in Judging a Song on First Listen: Insights From a User Survey,2023,https://doi.org/10.5281/zenodo.10265351,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masahiro Hamasaki+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"When a user listens to a song for the first time, what musical factors (e.g., melody, tempo, and lyrics) influence the user's decision to like or dislike the song? An answer to this question would enable researchers to more deeply understand how people interact with music. Thus, in this paper, we report the results of an online survey involving 302 participants to investigate the influence of 10 musical factors. We also evaluate how a user's personal characteristics (i.e., personality traits and musical sophistication) relate to the importance of each factor for the user. Moreover, we propose and evaluate three factor-based functions that would enable more effectively browsing songs on a music streaming service. The user survey results provide several reusable insights, including the following: (1) for most participants, the melody and singing voice are important factors in judging whether they like a song on first listen; (2) personal characteristics do influence the important factors (e.g., participants who have high openness and are sensitive to beat deviations emphasize melody); and (3) the proposed functions each have a certain level of demand because they enable users to easily find music that fits their tastes. We have released part of the survey results as publicly available data so that other researchers can reproduce the results and analyze the data from their own viewpoints.",JPN,facility,Developed economies,"[-40.697083, 21.049839]","[41.54629, 27.972828]","[-14.894795, 21.39932, -4.2900786]","[8.239694, 14.034164, 15.281356]","[15.149008, 9.022624]","[12.914887, 1.1963884]","[15.099623, 15.316905, -1.1291283]","[13.389946, 4.479497, 12.037399]"
66,Jan Hajič jr.;Gustavo A. Ballen;Klára Hedvika Mühlová;Hana Vlhová-Wörner,Towards Building a Phylogeny of Gregorian Chant Melodies,2023,https://doi.org/10.5281/zenodo.10340442,"Jan Hajič jr.+Masaryk Institute and Archive, Czech Academy of Sciences>CZE>facility;Gustavo A. Ballen+Queen Mary University of London>GBR>education;Klára Hedvika Mühlová+Masaryk University>CZE>education;Hana Vlhová-Wörner+Masaryk Institute and Archive, Czech Academy of Sciences>CZE>facility","The historical development of medieval plainchant melodies is an intriguing musicological topic that invites computational approaches to study it at scale. Plainchant melodies can be represented as strings from a limited alphabet, hence making it technically possible to apply bioinformatic tools that are used to study the relationships of biological sequences. We show that using phylogenetic trees to study relationships of plainchant sources is not merely possible, but that it can indeed produce meaningful results. We develop a simple plainchant substitution model for Multiple Sequence Alignment, adapt a Bayesian phylogenetic tree building method, and demonstrate the promise of this approach by validating the resultant phylogenetic tree built from a set of Divine Office sources for the Christmas Vespers against musicological knowledge.",CZE,facility,Developed economies,"[11.373288, -0.27615798]","[4.8481407, 6.825899]","[16.177076, -1.4000345, -22.440672]","[-5.4496236, 20.49597, 2.1563592]","[11.026015, 9.989423]","[8.231233, 0.9000605]","[11.800204, 15.266021, -0.9878966]","[10.14711, 6.502269, 13.15355]"
67,Yiwei Ding;Alexander Lerch,Audio Embeddings as Teachers for Music Classification,2023,https://doi.org/10.5281/zenodo.10265353,Yiwei Ding+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education,"Music classification has been one of the most popular tasks in the field of music information retrieval. With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks. However, the increasing model complexity makes both training and inference computationally expensive. In this paper, we integrate the ideas of transfer learning and feature-based knowledge distillation and systematically investigate using pre-trained audio embeddings as teachers to guide the training of low-complexity student networks. By regularizing the feature space of the student networks with the pre-trained embeddings, the knowledge in the teacher embeddings can be transferred to the students. We use various pre-trained audio embeddings and test the effectiveness of the method on the tasks of musical instrument classification and music auto-tagging. Results show that our method significantly improves the results in comparison to the identical model trained without the teacher's knowledge. This technique can also be combined with classical knowledge distillation approaches to further improve the model's performance.",USA,education,Developed economies,"[-18.57131, -3.6802526]","[-23.345722, -37.2107]","[0.48715895, 13.820972, 12.680015]","[-5.175274, 1.2329693, -18.812822]","[11.76467, 9.191059]","[9.759018, 4.6656847]","[13.72271, 13.282144, 0.40582627]","[10.91268, 6.5231223, 9.056143]"
68,Ilya Borovik;Vladimir Viro,ScorePerformer: Expressive Piano Performance Rendering With Fine-Grained Control,2023,https://doi.org/10.5281/zenodo.10265355,Ilya Borovik+Skoltech>RUS>education;Vladimir Viro+Peachnote GmbH>DEU>company,"We present ScorePerformer, an encoder-decoder transformer with hierarchical style encoding heads for controllable rendering of expressive piano music performances. We design a tokenized representation of symbolic score and performance music, the Score Performance Music tuple (SPMuple), and validate a novel way to encode the local performance tempo in a local note time window. Along with the encoding, we extend a transformer encoder with multi-level maximum mean discrepancy variational autoencoder style modeling heads that learn performance style at the global, bar, beat, and onset levels for fine-grained performance control. To offer an interpretation of the learned latent spaces, we introduce performance direction marking classifiers that associate vectors in the latent space with direction markings to guide performance rendering through the model. Evaluation results show the importance of the architectural design choices and demonstrate that ScorePerformer produces diverse and coherent piano performances that follow the control input.",RUS,education,Economies in transition,"[33.153152, -0.86192375]","[-12.876495, -40.41333]","[15.340119, -1.4895205, 23.732449]","[-15.907707, 1.1797534, -10.984494]","[10.062149, 7.154577]","[8.651412, 6.249061]","[12.43804, 11.55762, -0.42856237]","[9.281206, 5.7811985, 8.7963]"
69,Emmanouil Karystinaios;Gerhard Widmer,Roman Numeral Analysis With Graph Neural Networks: Onset-Wise Predictions From Note-Wise Features,2023,https://doi.org/10.5281/zenodo.10265357,"Emmanouil Karystinaios+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music.  This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score.  The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm.  Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets.  In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/manoskary/chordgnn",AUT,education,Developed economies,"[24.070642, 17.056702]","[-14.73256, -27.635666]","[-4.116052, -16.764183, 18.416948]","[-19.834532, -13.992387, -11.14774]","[11.477237, 6.786171]","[8.749972, 5.268603]","[13.283162, 12.068194, -1.2852049]","[9.86532, 6.1521344, 9.580943]"
70,Brian Regan;Desislava Hristova;Mariano Beguerisse-Díaz,Semi-Automated Music Catalog Curation Using Audio and Metadata,2023,https://doi.org/10.5281/zenodo.10265359,Brian Regan+Spotify>USA>company;Desislava Hristova+Spotify>USA>company;Mariano Beguerisse-Díaz+Spotify>USA>company,"We present a system to assist Subject Matter Experts (SMEs) to curate large online music catalogs. The system detects releases that are incorrectly attributed to an artist discography (misattribution), when the discography of a single artist is incorrectly separated (duplication), and predicts suitable relocations of misattributed releases. We use historical discography corrections to train and evaluate our system's component models. These models combine vector representations of audio with metadata-based features, which outperform models based on audio or metadata alone. We conduct three experiments with SMEs in which our system detects misattribution in artist discographies with precision greater than 77%, duplication with precision greater than 71%, and by combining the approaches, predicts a correct relocation for misattributed releases with precision up to 45%. These results demonstrate the potential of such proactive curation systems in saving valuable human time and effort by directing attention where it is most needed.",USA,company,Developed economies,"[-24.133268, 35.5064]","[33.96708, 0.41115928]","[-22.54797, 1.3941607, -9.466236]","[18.627184, 12.052156, -2.5341315]","[14.278419, 9.037487]","[10.903634, 3.508688]","[15.125606, 13.677866, -1.098645]","[12.308809, 6.0965357, 10.785574]"
71,Ioannis Petros Samiotis;Christoph Lofi;Alessandro Bozzon,Crowd's Performance on Temporal Activity Detection of Musical Instruments in Polyphonic Music,2023,https://doi.org/10.5281/zenodo.10265361,Ioannis Petros Samiotis+Delft University of Technology>NLD>education;Christoph Lofi+Delft University of Technology>NLD>education;Alessandro Bozzon+Delft University of Technology>NLD>education,"Musical instrument recognition enables applications such as instrument-based music search and audio manipulation, which are highly sought-after processes in everyday music consumption and production. Despite continuous progresses, advances in automatic musical instrument recognition is hindered by the lack of large, diverse and publicly available annotated datasets. As studies have shown, there is potential to scale up music data annotation processes through crowdsourcing. However, it is still unclear the extent to which untrained crowdworkers can effectively detect when a musical instrument is active in an audio excerpt. In this study, we explore the performance of non-experts on online crowdsourcing platforms, to detect temporal activity of instruments on audio extracts of selected genres. We study the factors that can affect their performance, while we also analyse user characteristics that could predict their performance. Our results bring further insights into the general crowd's capabilities to detect instruments.",NLD,education,Developed economies,"[10.5864725, -20.79253]","[-11.303646, 41.001465]","[15.520859, -2.3396847, -0.966706]","[6.1118984, 23.059929, -8.671942]","[8.653942, 7.277888]","[9.114428, 4.326295]","[11.024132, 12.53028, 0.47710282]","[10.376492, 6.1418285, 10.200113]"
72,Igor Pereira;Felipe Araújo;Filip Korzeniowski;Richard Vogl,MoisesDB: A Dataset for Source Separation Beyond 4-Stems,2023,https://doi.org/10.5281/zenodo.10265363,Igor Pereira+Moises Systems Inc.>USA>company;Felipe Araújo+Moises Systems Inc.>USA>company;Filip Korzeniowski+Moises Systems Inc.>USA>company;Richard Vogl+Moises Systems Inc.>USA>company,"In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres.  For each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems.  This will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data.  To facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB. Alongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results.",USA,company,Developed economies,"[7.698066, -48.406685]","[-40.12715, -29.45888]","[33.769844, -4.0435996, -4.602177]","[-10.596888, -5.2820106, -27.338736]","[8.359524, 10.043939]","[6.689031, 5.6541934]","[11.053804, 13.661681, 1.6551229]","[9.723929, 8.453817, 9.172382]"
73,Zeng Ren;Wulfram Gerstner;Martin Rohrmeier,Music as Flow: A Formal Representation of Hierarchical Processes in Music,2023,https://doi.org/10.5281/zenodo.10265365,Zeng Ren+EPFL>CHE>education|Unknown>Unknown>Unknown;Wulfram Gerstner+EPFL>CHE>education|Unknown>Unknown>Unknown;Martin Rohrmeier+EPFL>CHE>education|Unknown>Unknown>Unknown,"Modeling the temporal unfolding of musical events and its interpretation in terms of hierarchical relations is a common theme in music theory, cognition, and composition. To faithfully encode such relations, we need an elegant way to represent both the semantics of prolongation, where a single event is elaborated into multiple events, and process, where the connection from one event to another is elaborated into multiple connections. In existing works, trees are used to capture the former and graphs for the latter. Each such model has the potential to either encode relations between events (e.g., an event being a repetition of another), or relations between processes (e.g., two consecutive steps making up a larger skip), but not both together explicitly. To model meaningful relations between musical events and processes and combine the semantic expressiveness of trees and graphs, we propose a structured representation using  algebraic datatype (ADT) with dependent type. We demonstrate its applications towards encoding functional interpretations of harmonic progressions, and large scale organizations of key regions. This paper offers two contributions. First, we provide a novel unifying hierarchical framework for musical processes and events. Second, we provide a structured data type encoding such interpretations, which could facilitate computational approaches in music theory and generation.",CHE,education,Developed economies,"[6.760306, 9.487456]","[-17.130222, 23.330685]","[-5.222013, -16.19389, 2.831491]","[-15.523663, 0.2907297, 5.745995]","[12.283844, 8.20673]","[8.087807, 2.1734161]","[12.966582, 13.85165, -0.88307285]","[9.484907, 7.2188935, 12.134447]"
74,Silvan David Peter,Online Symbolic Music Alignment With Offline Reinforcement Learning,2023,https://doi.org/10.5281/zenodo.10265367,Silvan David Peter+Johannes Kepler University Linz>AUT>education,"Symbolic Music Alignment is the process of matching performed MIDI notes to corresponding score notes. In this paper, we introduce a reinforcement learning (RL)- based online symbolic music alignment technique. The RL agent — an attention-based neural network — itera- tively estimates the current score position from local score and performance contexts. For this symbolic alignment task, environment states can be sampled exhaustively and the reward is dense, rendering a formulation as a simpli- fied offline RL problem straightforward. We evaluate the trained agent in three ways. First, in its capacity to identify correct score positions for sampled test contexts; second, as the core technique of a complete algorithm for symbolic online note-wise alignment; and finally, as a real-time sym- bolic score follower. We further investigate the pitch-based score and performance representations used as the agent's inputs. To this end, we develop a second model, a two- step Dynamic Time Warping (DTW)-based offline align- ment algorithm leveraging the same input representation. The proposed model outperforms a state-of-the-art refer- ence model of offline symbolic music alignment.",AUT,education,Developed economies,"[17.652996, -9.6576]","[1.7296468, -34.024124]","[-3.5998354, -16.09523, -7.1622505]","[2.8427775, 7.9529433, -28.050316]","[11.320207, 6.635643]","[6.4092984, 0.9573756]","[12.327857, 12.515691, -1.4290806]","[8.543277, 6.062005, 10.489598]"
75,Oren Barkan;Shlomi Shvartzman;Noy Uzrad;Moshe Laufer;Almog Elharar;Noam Koenigstein,Inversynth II: Sound Matching via Self-Supervised Synthesizer-Proxy and Inference-Time Finetuning,2023,https://doi.org/10.5281/zenodo.10265371,Oren Barkan+The Open University of Israel>ISR>education;Shlomi Shvartzman+Tel Aviv University>ISR>education;Noy Uzrad+Tel Aviv University>ISR>education;Moshe Laufer+Tel Aviv University>ISR>education;Almog Elharar+Tel Aviv University>ISR>education;Noam Koenigstein+Tel Aviv University>ISR>education,"Synthesizers are widely used electronic musical instruments. Given an input sound, inferring the underlying synthesizer's parameters to reproduce it is a difficult task known as sound-matching. In this work, we tackle the problem of automatic sound matching, which is otherwise performed manually by professional audio experts. The novelty of our work stems from the introduction of a novel differentiable synthesizer-proxy that enables gradient-based optimization by comparing the input and reproduced audio signals. Additionally, we introduce a novel self-supervised finetuning mechanism that further refines the prediction at inference time. Both contributions lead to state-of-the-art results, outperforming previous methods across various metrics. Our code is available at: https://github.com/inversynth/InverSynth2.",ISR,education,Developing economies,"[14.303023, -34.481876]","[-20.753237, -50.79866]","[28.583393, 2.161237, 7.2225547]","[-25.212067, -5.949187, -18.334415]","[9.553423, 8.580748]","[8.082718, 6.7103395]","[12.411408, 12.300331, 0.6922531]","[9.555293, 5.998551, 7.9850855]"
76,Amantur Amatov;Dmitry Lamanov;Maksim Titov;Ivan Vovk;Ilya Makarov;Mikhail Kudinov,A Semi-Supervised Deep Learning Approach to Dataset Collection for Query-by-Humming Task,2023,https://doi.org/10.5281/zenodo.10265375,"Amantur Amatov+Higher School of Economics>RUS>education|Huawei Noah’s Ark Lab>RUS>company|AI Center, NUST MISiS>RUS>education;Dmitry Lamanov+Huawei Noah’s Ark Lab>RUS>company;Maksim Titov+Huawei Noah’s Ark Lab>RUS>company;Ivan Vovk+Huawei Noah’s Ark Lab>RUS>company;Ilya Makarov+AI Center, NUST MISiS>RUS>education;Mikhail Kudinov+Huawei Noah’s Ark Lab>RUS>company","Query-by-Humming (QbH) is a task that involves finding the most relevant song based on a hummed or sung fragment. Despite recent successful commercial solutions, implementing QbH systems remains challenging due to the lack of high-quality datasets for training machine learning models. In this paper, we propose a deep learning data collection technique and introduce Covers and Hummings Aligned Dataset (CHAD), a novel dataset that contains 18 hours of short music fragments, paired with time-aligned hummed versions. To expand our dataset, we employ a semi-supervised model training pipeline that leverages the QbH task as a specialized case of cover song identification (CSI) task. Starting with a model trained on the initial dataset, we iteratively collect groups of fragments of cover versions of the same song and retrain the model on the extended data. Using this pipeline, we collect over 308 hours of additional music fragments, paired with time-aligned cover versions. The final model is successfully applied to the QbH task and achieves competitive results on benchmark datasets. Our study shows that the proposed dataset and training pipeline can effectively facilitate the implementation of QbH systems.",RUS,education,Economies in transition,"[-3.2363727, 38.63298]","[3.072869, -28.925184]","[-12.499006, -5.369814, -26.168968]","[21.589695, -6.710661, -0.46621913]","[14.841512, 6.1389084]","[9.940444, 4.042765]","[13.179204, 15.371209, -2.965199]","[11.06612, 6.7110167, 9.6989355]"
77,Keren Shao;Ke Chen;Taylor Berg-Kirkpatrick;Shlomo Dubnov,Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction,2023,https://doi.org/10.5281/zenodo.10265373,Keren Shao+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Ke Chen+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Taylor Berg-Kirkpatrick+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Shlomo Dubnov+University of California San Diego>USA>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.",USA,education,Developed economies,"[-0.9092615, -31.230675]","[-29.800194, -32.293823]","[17.59225, 6.060025, -7.6334476]","[-5.2518287, -9.928665, -15.8119335]","[9.7066965, 10.29118]","[7.76806, 5.3083196]","[11.029961, 14.757311, 0.21253069]","[9.605163, 7.073147, 8.772395]"
78,Chin-Yun Yu;György Fazekas,Singing Voice Synthesis Using Differentiable LPC and Glottal-Flow-Inspired Wavetables,2023,https://doi.org/10.5281/zenodo.13916489,"Chin-Yun Yu+Centre for Digital Music, Queen Mary University of London>GBR>education;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education","This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for singing voice synthesis (SVS) that exploits the physical characteristics of the human voice using differentiable digital signal processing. GOLF employs a glottal model as the harmonic source and IIR filters to simulate the vocal tract, resulting in an interpretable and efficient approach. We show it is competitive with state-of-the-art singing voice vocoders, requiring fewer synthesis parameters and less memory to train, and runs an order of magnitude faster for inference. Additionally, we demonstrate that GOLF can model the phase components of the human voice, which has immense potential for rendering and analysing singing voices in a differentiable manner. Our results highlight the effectiveness of incorporating the physical properties of the human voice mechanism into SVS and underscore the advantages of signal-processing-based approaches, which offer greater interpretability and efficiency in synthesis.",GBR,education,Developed economies,"[-0.1566083, -36.292393]","[-32.136707, -45.539097]","[23.999443, 5.4264936, -15.699909]","[6.3130255, -13.3802, -19.665516]","[9.406781, 10.761627]","[7.4756393, 4.6077604]","[10.983754, 14.722988, 0.99495023]","[9.982939, 7.4712873, 8.730416]"
79,Qiaoyu Yang;Frank Cwitkowitz;Zhiyao Duan,Harmonic Analysis With Neural Semi-CRF,2023,https://doi.org/10.5281/zenodo.10265379,Qiaoyu Yang+University of Rochester>USA>education;Frank Cwitkowitz+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"Automatic harmonic analysis of symbolic music is an important and useful task for both composers and listeners. The task consists of two components: recognizing harmony labels and finding their time boundaries. Most of the previous attempts focused on the first component, while time boundaries were rarely modeled explicitly. Lack of boundary modeling in the objective function could lead to segmentation errors. In this paper, we introduce a novel approach named Harana, to jointly detect the labels and boundaries of harmonic regions using neural semi-CRF (conditional random field). In contrast to rule-based scores used in traditional semi-CRF, a neural score function is proposed to incorporate features with more representational power. To improve the robustness of the model to imperfect harmony profiles, we design an additional score component to penalize the match between the candidate harmony label and the absent notes in the music. Quantitative results from our experiments demonstrate that the proposed approach improves segmentation quality as well as frame-level accuracy compared to previous methods.",USA,education,Developed economies,"[35.25233, -14.910063]","[-31.330471, 16.311285]","[10.207394, -22.26351, 14.557302]","[-20.100069, -5.972766, 1.3371091]","[9.17653, 9.095963]","[7.866214, 2.7607164]","[11.41544, 13.490483, 0.13697687]","[9.9926605, 8.146987, 11.875848]"
80,Alberto Acquilino;Ninad Puranik;Ichiro Fujinaga;Gary Scavone,A Dataset and Baseline for Automated Assessment of Timbre Quality in Trumpet Sound,2023,https://doi.org/10.5281/zenodo.10265381,Alberto Acquilino+McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Ninad Puranik+McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Ichiro Fujinaga+McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility;Gary Scavone+McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)>CAN>facility,"Music Performance Analysis is based on the evaluation of performance parameters such as pitch, dynamics, timbre, tempo and timing. While timbre is the least specific parameter among these and is often only implicitly understood, prominent brass pedagogues have reported that the presence of excessive muscle tension and inefficiency in playing by a musician is reflected in the timbre quality of the sound produced. In this work, we explore the application of machine learning to automatically assess timbre quality in trumpet playing, given both its educational value and connection to performance quality. An extensive dataset consisting of more than 19,000 tones played by 110 trumpet players of different expertise has been collected. A subset of 1,481 tones from this dataset was labeled by eight professional graders on a scale of 1 to 4 based on the perceived efficiency of sound production. Statistical analysis is performed to identify the correlation among the assigned ratings by the expert graders. A Random Forest classifier is trained using the mode of the ratings and its accuracy and variability is assessed with respect to the variability in human graders as a reference. An analysis of the important discriminatory features identifies stability of spectral peaks as a critical factor in trumpet timbre quality.",CAN,education,Developed economies,"[7.1202264, -29.089712]","[-43.350307, 1.8418887]","[15.40426, -12.739037, -4.800663]","[-9.595002, 22.838696, -4.7975683]","[8.965331, 7.3851748]","[8.823446, 3.8148313]","[11.046958, 12.760398, 0.45819125]","[10.4935665, 7.2453814, 10.245627]"
81,Frank Heyen;Quynh Quang Ngo;Michael Sedlmair,Visual Overviews for Sheet Music Structure,2023,https://doi.org/10.5281/zenodo.10265383,"Frank Heyen+VISUS, University of Stuttgart>DEU>education;Quynh Quang Ngo+VISUS, University of Stuttgart>DEU>education;Michael Sedlmair+VISUS, University of Stuttgart>DEU>education","We propose different methods for alternative representation and visual augmentation of sheet music that help users gain an overview of general structure, repeating patterns, and the similarity of segments. To this end, we explored mapping the overall similarity between sections or bars to colors. For these mappings, we use dimensionality reduction or clustering to assign similar segments to similar colors and vice versa. To provide a better overview, we further designed simplified music notation representations, including hierarchical and compressed encodings. These overviews allow users to display whole pieces more compactly on a single screen without clutter and to find and navigate to distant segments more quickly. Our preliminary evaluation with guitarists and tablature shows that our design supports users in tasks such as analyzing structure, finding repetitions, and determining the similarity of specific segments to others.",DEU,education,Developed economies,"[31.613777, 6.2730646]","[5.26831, 4.192248]","[25.265396, 6.147555, 15.451816]","[9.805414, -5.748099, 2.1017172]","[10.742175, 6.845744]","[10.0830345, 1.9137939]","[12.544023, 12.080508, -1.3968962]","[11.383871, 6.5608478, 12.570192]"
82,Luís Carvalho;Gerhard Widmer,Passage Summarization With Recurrent Models for Audio – Sheet Music Retrieval,2023,https://doi.org/10.5281/zenodo.10265385,Luís Carvalho+Johannes Kepler University Linz>AUT>education|Institute of Computational Perception>AUT>education|LIT Artificial Intelligence Lab>AUT>education;Gerhard Widmer+Johannes Kepler University Linz>AUT>education|Institute of Computational Perception>AUT>education|LIT Artificial Intelligence Lab>AUT>education,"Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo deviations. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio - sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music. We conduct a number of experiments on synthetic and real piano data and scores, showing that our proposed recurrent method leads to more accurate retrieval in all possible configurations.",AUT,education,Developed economies,"[-7.167, 21.0592]","[-22.819191, -31.090443]","[-0.3984554, 13.715328, -12.946903]","[-11.495431, -14.620537, -15.003004]","[13.091213, 8.3189335]","[8.829736, 5.172839]","[13.409734, 14.504486, -1.377594]","[9.91716, 6.4328523, 8.994]"
83,Pedro Ramoneda;Jose J. Valero-Mas;Dasaem Jeong;Xavier Serra,Predicting Performance Difficulty From Piano Sheet Music Images,2023,https://doi.org/10.5281/zenodo.10265387,"Pedro Ramoneda+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Jose J. Valero-Mas+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Dasaem Jeong+MALer Lab, Sogang University>KOR>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the music information retrieval community has recently shown interest in this task, existing approaches mainly use machine-readable scores, leaving the broader case of sheet music images unaddressed. Based on previous works involving sheet music images, we use a mid-level representation, bootleg score, describing notehead positions relative to staff lines coupled with a transformer model. This architecture is adapted to our task by introducing a different encoding scheme that reduces the encoded sequence length to one-eighth of the original size. In terms of evaluation, we consider five datasets---more than 7500 scores with up to 9 difficulty levels---, two being mainly compiled for this work. The results obtained when pretraining the scheme on the IMSLP corpus and fine-tuning it on the considered datasets prove the proposal's validity, achieving the best-performing model with a balanced accuracy of 40.3\% and a mean square error of 1.3. Finally, we provide access to our code, data, and models for transparency and reproducibility.",ESP,education,Developed economies,"[29.999311, -0.71353465]","[-37.400913, 4.0311875]","[17.748201, 0.7079559, 20.574707]","[-12.511931, -12.010576, -6.201433]","[10.144517, 6.950562]","[7.9036045, 4.1208262]","[12.25626, 11.745283, -0.7252202]","[9.326997, 5.9613414, 10.141106]"
84,Junghyun Koo;Yunkee Chae;Chang-Bin Jeon;Kyogu Lee,Self-Refining of Pseudo Labels for Music Source Separation With Noisy Labeled Data,2023,https://doi.org/10.5281/zenodo.10265389,Junghyun Koo+Seoul National University>KOR>education|Seoul National University>KOR>education|Seoul National University>KOR>education;Yunkee Chae+Seoul National University>KOR>education|Seoul National University>KOR>education|Seoul National University>KOR>education;Chang-Bin Jeon+Seoul National University>KOR>education|Seoul National University>KOR>education|Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education|Seoul National University>KOR>education|Seoul National University>KOR>education,"Music source separation (MSS) faces challenges due to limited availability and potential noise in correctly labeled individual instrument tracks. In this paper, we propose an automated approach for refining mislabeled instrument tracks in a partially noisy-labeled dataset. The proposed self-refining technique with noisy-labeled dataset results in only a 1% accuracy degradation for multi-label instrument recognition compared to a classifier trained with a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data for training MSS models and shows that utilizing the refined dataset for MSS leads to comparable results to a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on self-refined datasets even outperformed those trained on datasets refined with a classifier trained on clean labels.",KOR,education,Developing economies,"[7.8562274, -46.469975]","[-37.219532, -27.804045]","[30.06544, -4.385508, -4.6326137]","[-10.791069, -2.0611503, -28.607641]","[8.351136, 10.062456]","[6.81367, 5.6539097]","[11.041721, 13.601912, 1.6514435]","[9.694468, 8.283157, 9.231422]"
85,Marcel A. Vélez Vásquez;Mariëlle Baelemans;Jonathan Driedger;Willem Zuidema;John Ashley Burgoyne,Quantifying the Ease of Playing Song Chords on the Guitar,2023,https://doi.org/10.5281/zenodo.10265391,Marcel A. Vélez Vásquez+University of Amsterdam>NLD>education;Mariëlle Baelemans+University of Amsterdam>NLD>education;Jonathan Driedger+Chordify>Unknown>company;Willem Zuidema+University of Amsterdam>NLD>education;John Ashley Burgoyne+University of Amsterdam>NLD>education,"Quantifying the difficulty of playing songs has recently gained traction in the MIR community. While previous work has mostly focused on piano, this paper concentrates on rhythm guitar, which is especially popular with amateur musicians and has a broad skill spectrum. This paper proposes a rubric-based 'playability' metric to formalise this spectrum. The rubric comprises seven criteria that contribute to a single playability score, representing the overall difficulty of a song. The rubric was created through interviewing and incorporating feedback from guitar teachers and experts. Additionally, we introduce the playability prediction task by adding annotations to a subset of 200 songs from the McGill Billboard dataset, labelled by a guitar expert using the proposed rubric. We use this dataset to weight each rubric criterion for maximal reliability. Finally, we create a rule-based baseline to score each rubric criterion automatically from chord annotations and timings, and compare this baseline against simple deep learning models trained on chord symbols and textual representations of guitar tablature. The rubric, dataset, and baselines lay a foundation for understanding what makes songs easy or difficult for guitar players and how we can use MIR tools to match amateurs with songs closer to their skill level.",NLD,education,Developed economies,"[52.010994, -7.907314]","[-38.580395, 3.9811234]","[27.820763, -13.457564, 11.397434]","[-12.597453, -11.080271, -3.7193303]","[6.973139, 8.5022335]","[7.607201, 4.0929008]","[11.9248, 10.584702, 1.9293057]","[9.238219, 6.2348366, 10.156702]"
86,Irmak Bükey;Jason Zhang;TJ Tsai,FlexDTW: Dynamic Time Warping With Flexible Boundary Conditions,2023,https://doi.org/10.5281/zenodo.10265393,Irmak Bükey+Pomona College>USA>education;Jason Zhang+University of Michigan>USA>education;TJ Tsai+Harvey Mudd College>USA>education,"Alignment algorithms like DTW and subsequence DTW assume specific boundary conditions on where an alignment path can begin and end in the cost matrix.  In practice, the boundary conditions may not be known a priori or may not satisfy such strict assumptions.  This paper introduces an alignment algorithm called FlexDTW that is designed to handle a wide range of boundary conditions.  FlexDTW allows alignment paths to start anywhere on the bottom or left edge of the cost matrix (adjacent to the origin) and to end anywhere on the top or right edge.  In order to properly compare paths of very different lengths, we use a goodness measure that normalizes the cumulative path cost by the path length.  The key insight of FlexDTW is that the Manhattan length of a path can be computed by simply knowing the starting point of the path, which can be computed recursively during dynamic programming.  We artificially generate a suite of 16 benchmarks based on the Chopin Mazurka dataset in order to characterize audio alignment performance under a variety of boundary conditions.  We show that FlexDTW has consistently strong performance that is comparable or better than commonly used alignment algorithms, and it is the only system with strong performance in some boundary conditions.",USA,education,Developed economies,"[52.54889, 10.292752]","[-17.476763, -18.331186]","[10.592687, -18.526052, -18.901194]","[-0.22885601, -26.39492, -4.824491]","[10.59302, 5.5905943]","[6.107053, 0.612045]","[11.183847, 12.918946, -1.6471709]","[8.11762, 5.8201942, 10.789445]"
62,Alain Riou;Stefan Lattner;Gaëtan Hadjeres;Geoffroy Peeters,PESTO: Pitch Estimation With Self-Supervised Transposition-Equivariant Objective,2023,https://doi.org/10.5281/zenodo.10265343,"Alain Riou+Télécom-Paris, Institut Polytechnique de Paris>FRA>education|Sony Computer Science Laboratories - Paris>FRA>company;Stefan Lattner+Sony Computer Science Laboratories - Paris>FRA>company;Gaëtan Hadjeres+Sony AI>Unknown>Unknown;Geoffroy Peeters+Télécom-Paris, Institut Polytechnique de Paris>FRA>education","In this paper, we address the problem of pitch estimation using self-supervised learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset.  We use a lightweight (&lt; 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its constant-Q transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.  We evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with low-resource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation.",FRA,education,Developed economies,"[25.102297, -22.350534]","[-27.33468, -37.38044]","[13.615295, -14.97809, -13.370996]","[-5.936128, -6.0908704, -14.9992285]","[9.99101, 5.655805]","[8.0375185, 5.129309]","[10.887111, 13.169855, -0.8307464]","[9.689276, 6.9354076, 8.797986]"
10,Francisco J. Castellanos;Antonio Javier Gallego;Ichiro Fujinaga,A Few-Shot Neural Approach for Layout Analysis of Music Score Images,2023,https://doi.org/10.5281/zenodo.10265233,"Francisco J. Castellanos+University Institute for Computing Research, University of Alicante>ESP>education;Antonio Javier Gallego+University Institute for Computing Research, University of Alicante>ESP>education;Ichiro Fujinaga+Schulich School of Music, McGill University>CAN>education","Optical Music Recognition (OMR) is a well-established research field focused on the task of reading musical notation from images of music scores. In the standard OMR workflow, layout analysis is a critical component for identifying relevant parts of the image, such as staff lines, text, or notes. State-of-the-art approaches to this task are based on machine learning, which entails having to label a training corpus, an error-prone, laborious, and expensive task that must be performed by experts. In this paper, we propose a novel few-shot strategy for building robust models by utilizing only partial annotations, therefore requiring minimal human effort. Specifically, we introduce a masking layer and an oversampling technique to train models using a small set of annotated patches from the training images. Our proposal enables achieving high performance even with scarce training data, as demonstrated by experiments on four benchmark datasets. The results indicate that this approach achieves performance values comparable to models trained with a fully annotated corpus, but, in this case, requiring the annotation of only between 20% and 39% of this data.",ESP,education,Developed economies,"[19.824644, -9.159189]","[-20.721144, -25.765663]","[24.09166, 22.17011, -0.5435178]","[-17.132528, -19.285776, -5.5819025]","[10.658318, 6.5432568]","[6.44667, -1.137571]","[12.304165, 12.280901, -1.3979292]","[8.169369, 4.2847023, 9.874337]"
4,Hitoshi Suda;Shunsuke Yoshida;Tomohiko Nakamura;Satoru Fukayama;Jun Ogata,FruitsMusic: A Real-World Corpus of Japanese Idol-Group Songs,2024,https://doi.org/10.5281/zenodo.14877287,Hitoshi Suda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Shunsuke Yoshida+The University of Tokyo>JPN>education;Tomohiko Nakamura+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Jun Ogata+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This study presents FruitsMusic, a metadata corpus of Japanese idol-group songs in the real world, precisely annotated with who sings what and when. Japanese idol-group songs, vital to Japanese pop culture, feature a unique vocal arrangement style, where songs are divided into several segments, and a specific individual or multiple singers are assigned to each segment. To enhance singer diarization methods for recognizing such structures, we constructed FruitsMusic as a resource using 40 music videos of Japanese idol groups from YouTube. The corpus includes detailed annotations, covering songs across various genres, division and assignment styles, and groups ranging from 4 to 9 members. FruitsMusic also facilitates the development of various music information retrieval techniques, such as lyrics transcription and singer identification, benefiting not only Japanese idol-group songs but also a wide range of songs featuring single or multiple singers from various cultures. This paper offers a comprehensive overview of FruitsMusic, including its creation methodology and unique characteristics compared to conversational speech. Additionally, this paper evaluates the efficacy of current methods for singer embedding extraction and diarization in challenging real-world conditions using FruitsMusic. Furthermore, this paper examines potential improvements in automatic diarization performance through evaluating human performance.",JPN,facility,Developed economies,"[-8.505444, -27.38818]","[7.6146207, -23.558802]","[9.167973, 12.3265505, -16.827528]","[8.943378, -2.0241642, -20.90336]","[10.799758, 10.863895]","[8.534686, 3.4928162]","[11.721663, 15.268995, 0.14702328]","[10.78457, 7.4655657, 9.891159]"
0,Zeng Ren;Yannis Rammos;Martin A. Rohrmeier,Formal Modeling of Structural Repetition Using Tree Compression,2024,https://doi.org/10.5281/zenodo.14877282,Zeng Ren+EPFL>CHE>education;Yannis Rammos+EPFL>CHE>education;Martin Rohrmeier+EPFL>CHE>education,"Repetition is central to musical structure as it gives rise both to piece-wise and stylistic coherence. Identifying repetitions in music is computationally not trivial, especially when they are varied or deeply hidden within tree-like structures. Rather than focusing on repetitions of musical events, we propose to pursue repeated structural relations between events. More specifically, given a context-free grammar that describes a tonal structure, we aim to computationally identify such relational repetitions within the derivation tree of the grammar. To this end, we first introduce the Template, a grammar-generic structure for generating trees that contain structural repetitions. We then approach the discovery of structural repetitions as a search for optimally compressible Templates that describe a corpus of pieces in the form of production-rule-labeled trees. To make it tractable, we develop a heuristic, inspired by tree compression algorithms, to approximate the optimally compressible Templates of the corpus. After implementing the algorithm in Haskell, we apply it to a corpus of jazz harmony trees, where we assess its performance based on the compressibility of the resulting Templates and the music-theoretical relevance of the identified repetitions.",CHE,education,Developed economies,"[16.667192, 31.180656]","[-18.65086, 23.911041]","[-16.854185, -16.99143, 4.707895]","[-16.68852, 1.0997822, 4.354548]","[11.657063, 7.758823]","[8.042855, 2.166781]","[12.503831, 13.685948, -0.6917963]","[9.507368, 7.4301577, 12.252968]"
36,Eun Ji Oh;Hyunjae Kim;Kyung Myun Lee,Which Audio Features Can Predict the Dynamic Musical Emotions of Both Composers and Listeners?,2024,https://doi.org/10.5281/zenodo.14877347,Eun Ji Oh+KAIST>KOR>education;Hyunjae Kim+KAIST>KOR>education;Kyung Myun Lee+KAIST>KOR>education,"Are composers' emotional intentions conveyed to listeners through audio features? In the field of Music Emotion Recognition (MER), recent efforts have been made to predict listeners' time-varying perceived emotions using machine-learning models. However, interpreting these models has been challenging due to their black-box nature. To increase the explainability of models for subjective emotional experiences, we focus on composers' emotional intentions. Our study aims to determine which audio features effectively predict both composers' time-varying emotions and listeners' perceived emotions. Seven composers performed 18 piano improvisations expressing three types of emotions (joy/happiness, sadness, and anger), which were then listened to by 36 participants in a laboratory setting. Both composers and listeners continuously assessed the emotional valence of the music clips on a 9-point scale (1: 'very negative' to 9: 'very positive'). Linear mixed-effect models analysis revealed that listeners significantly perceived the composers' intended emotions. Regarding audio features, the RMS was found to modulate the degree to which the listener's perceived emotion resembled the composer's emotion across all emotions. Moreover, the significant audio features that influenced this relationship varied depending on the emotion type. We propose that audio features related to the emotional responses of composers-listeners can be considered key factors in predicting listeners' emotional responses.",KOR,education,Developing economies,"[-58.940685, 3.0228286]","[50.99507, -12.40439]","[-24.752016, 25.185528, 4.446495]","[6.5550447, 21.703764, 5.26295]","[13.967066, 12.830443]","[12.804011, 4.2757044]","[16.182291, 14.501149, 1.7719437]","[13.939683, 4.8071856, 10.2803955]"
37,Julia Barnett;Hugo Flores García;Bryan Pardo,Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model,2024,https://doi.org/10.5281/zenodo.14877350,Julia Barnett+Northwestern University>USA>education;Hugo Flores García+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"Every artist has a creative process that draws inspiration from previous artists and their works. Today, ""inspiration"" has been automated by generative music models. The black box nature of these models obscures the identity of the works that influence their creative output. As a result, users may inadvertently appropriate or copy existing artists' works. We establish a replicable methodology to systematically identify similar pieces of music audio in a manner that is useful for understanding training data attribution. We compare the effect of applying CLMR [1] and CLAP [2] embeddings to similarity measurement in a set of 5 million audio clips used to train VampNet [3], a recent open source generative music model. We validate this approach with a human listening study. We also explore the effect that modifications of an audio example (e.g., pitch shifting) have on similarity measurements. This work is foundational to incorporating automated influence attribution into generative modeling, which promises to let model creators and users move from ignorant appropriation to informed creation. Audio samples accompanying this paper are available at tinyurl.com/exploring-musical-roots.",USA,education,Developed economies,"[-32.99275, 3.2532697]","[-2.5334556, 45.79274]","[-1.3832649, 13.443874, 20.787663]","[-16.181952, 11.784029, 1.9419725]","[12.874913, 8.790646]","[9.811204, 5.537333]","[14.027861, 13.667749, -0.27470762]","[10.430212, 5.4039645, 9.409769]"
38,Andre Holzapfel;Anna-Kaisa Kaila;Petra Jääskeläinen,Green MIR? Investigating Computational Cost of Recent Music-Ai Research in ISMIR,2024,https://doi.org/10.5281/zenodo.14877351,Andre Holzapfel+KTH Royal Institute of Technology>SWE>education;Anna-Kaisa Kaila+KTH Royal Institute of Technology>SWE>education;Petra Jääskeläinen+KTH Royal Institute of Technology>SWE>education,"The environmental footprint of Generative AI and other Deep Learning (DL) technologies is increasing. To understand the scale of the problem and to identify solutions for avoiding excessive energy use in DL research at communities such as ISMIR, more knowledge is needed of the current energy cost of the undertaken research. In this paper, we provide a scoping inquiry of how the ISMIR research concerning automatic music generation (AMG) and computing-heavy music analysis currently discloses information related to environmental impact. We present a study based on two corpora that document 1) ISMIR papers published in the years 2017–2023 that introduce an AMG model, and 2) ISMIR papers from the years 2022–2023 that propose music analysis models and include heavy computations with GPUs. Our study demonstrates a lack of transparency in model training documentation. It provides the first estimates of energy consumption related to model training at ISMIR, as a baseline for making more systematic estimates about the energy footprint of the ISMIR conference in relation to other machine learning events. Furthermore, we map the geographical distribution of generative model contributions and discuss the corporate role in the funding and model choices in this body of work.",SWE,education,Developed economies,"[-14.240379, 49.840023]","[-20.477633, -40.92684]","[-29.404657, -3.9986029, 3.098939]","[-9.464256, 2.4695773, -21.962692]","[13.420371, 5.761016]","[9.661907, 5.437441]","[14.699124, 12.212025, -1.3947102]","[10.56083, 6.0012608, 8.83961]"
39,Seikoh Fukuda;Yuko Fukuda;Masamichi Hosoda;Ami Motomura;Eri Sasao;Masaki Matsubara;Masahiro Niitsuma,Field Study on Children's Home Piano Practice: Developing a Comprehensive System for Enhanced Student-Teacher Engagement,2024,https://doi.org/10.5281/zenodo.14877353,Seikoh Fukuda+PTNA Research Institute of Music>JPN>facility|Kyoritsu Women’s University>JPN>education;Yuko Fukuda+PTNA Research Institute of Music>JPN>facility|Kyoritsu Women’s University>JPN>education;Masamichi Hosoda+NTT East Corporation>JPN>company;Ami Motomura+To-on Kikaku Company>JPN>company;Eri Sasao+To-on Kikaku Company>JPN>company;Masaki Matsubara+University of Tsukuba>JPN>education;Masahiro Niitsuma+Keio University>JPN>education,"Regular weekly lessons and daily home practice are key for skill development. This paper focuses on identifying the challenges within such practice routines and developing a system to address these issues, thereby enhancing teacher support and elevating student performance in piano. Observations from real-world lessons and an analysis of practice videos spanning 177 days from 30 students reveal successful tactics, including the assignment of suitably challenging pieces and motivational rewards like stickers or stamps. Furthermore, the study underscores issues such as tension in parent-led practice and ineffective repetition. Insights from the field study suggest the potential of third-party feedback, practice segmentation, reporting practice records to teachers, and rewarding practice sessions. We developed a system incorporating these solutions and tested it with 80 children over 4 months. Results showed increased teacher engagement with students' home practice, improved student motivation and practice duration, and enhanced sight-reading skills, demonstrating the system's effectiveness in supporting piano education.",JPN,facility,Developed economies,"[-47.836586, -14.985127]","[-39.025894, 6.312733]","[-4.8560853, 23.115028, 3.1906617]","[-8.719738, -11.600142, -3.249799]","[10.05205, 7.0895905]","[7.8659043, 3.819621]","[12.399665, 11.606085, -0.50945395]","[9.236774, 6.107106, 10.312002]"
40,Brian Bemman;Justin Christensen,Inner Metric Analysis as a Measure of Rhythmic Syncopation,2024,https://doi.org/10.5281/zenodo.14877355,Brian Bemman+Durham University>GBR>education;Justin Christensen+University of Sheffield>GBR>education,"Inner Metric Analysis (IMA) is a method for symbolic music analysis that identifies strong and weak metrical positions according to coinciding periodicities within note onsets. These periodicities are visualized with bar graphs known as metric weight and spectral weight profiles. Analyzing these profiles for the presence of syncopation has thus far required manual inspection. In this paper, we propose a simple measure using chi-squared distance for quantifying the level of syncopation found in IMA weight profiles by considering each as a distribution to be compared against (1) a uniform distribution 'nominal' weight profile, and (2) a non-uniform distribution based on beat strength. We apply this measure to the task of predicting perceptual ratings of syncopation using the Song (2014) dataset of 111 single-bar rhythmic patterns and compare its performance to seven existing models of syncopation/complexity. Our results indicate that the proposed measure based on (1) achieves a moderately high Spearman rank correlation (rs = 0.80) to all ratings and is the only single measure that reportedly works across all categories. For so-called polyrhythms in 4/4, the measure based on (2) surpasses all other models and further outperforms five models for monorhythms in 6/8 and three models for monorhythms in 4/4.",GBR,education,Developed economies,"[45.419697, 8.666888]","[-23.783533, 3.1971743]","[-10.42397, -22.835596, 2.4538522]","[-1.9229411, 16.023296, -5.102989]","[12.0507965, 5.4345465]","[5.9945006, 1.6975075]","[11.377456, 14.233621, -2.1293285]","[7.9804115, 6.782429, 11.755346]"
41,Lidia J. Morris;Rebecca Leger;Michele Newman;John Ashley Burgoyne;Ryan Groves;Natasha Mangal;Jin Ha Lee,HAISP: A Dataset of Human-AI Songwriting Processes From the AI Song Contest,2024,https://doi.org/10.5281/zenodo.14877357,Lidia Morris+University of Washington>USA>education;Rebecca Leger+Fraunhofer IIS>DEU>company;Michele Newman+University of Washington>USA>education;John Ashley Burgoyne+University of Amsterdam>NLD>education;Ryan Groves+Infinite Album>CHE>company;Natasha Mangal+CISAC>FRA>company;Jin Ha Lee+University of Washington>USA>education,"The advent of accessible artificial intelligence (AI) tools and systems has begun a new era for creative expression, challenging us to gain a better understanding of human-AI collaboration and creativity. In this paper, we introduce Human–AI Songwriting Processes Dataset (HAISP), consisting of 34 coded submissions from the 2023 AI Song Contest. This dataset offers a resource for exploring the complex dynamics of AI-supported songwriting processes, facilitating investigations into the possibilities and challenges posed by AI in creative endeavors. Overall, HAISP is anticipated to contribute to advancing understanding of human-AI co-creation from the users' perspective. We suggest potential use cases for the dataset, including examining AI tools used in songwriting and exploring users' ethical considerations and creative approaches. This could help inform academic research and practical applications in music composition and related fields.",USA,education,Developed economies,"[-25.335358, 5.1346817]","[-5.4045963, 44.24796]","[-26.43408, -2.7107854, 6.9430566]","[-18.64386, 10.1738615, 4.1269536]","[12.817041, 7.612393]","[9.962563, 5.670005]","[14.313035, 13.136674, -1.1377362]","[10.408448, 5.115524, 9.640309]"
42,Giulia Argüello;Luca A. Lanzendörfer;Roger Wattenhofer,Cue Point Estimation Using Object Detection,2024,https://doi.org/10.5281/zenodo.14877359,Giulia Argüello+ETH Zurich>CHE>education;Luca A. Lanzendörfer+ETH Zurich>CHE>education;Roger Wattenhofer+ETH Zurich>CHE>education,"Cue points indicate possible temporal boundaries in a transition between two pieces of music in DJ mixing and constitute a crucial element in autonomous DJ systems as well as for live mixing. In this work, we present a novel method for automatic cue point estimation, interpreted as a computer vision object detection task. Our proposed system is based on a pre-trained object detection transformer which we fine-tune on our novel cue point dataset. Our provided dataset contains 21k manually annotated cue points from human experts as well as metronome information for nearly 5k individual tracks, making this dataset 35x larger than the previously available cue point dataset. Unlike previous methods, our approach does not require low-level musical information analysis, while demonstrating increased precision in retrieving cue point positions. Moreover, our proposed method demonstrates high adherence to phrasing, a type of high-level music structure commonly emphasized in electronic dance music. The code, model checkpoints, and dataset are made publicly available.",CHE,education,Developed economies,"[-5.2055917, -11.623045]","[-24.645424, -11.050172]","[7.9489307, -9.253373, -7.6455865]","[-0.85292774, 12.931211, -14.410456]","[11.154176, 8.713771]","[5.4165764, 1.8144404]","[12.272726, 13.723014, 0.38563138]","[7.8473606, 6.680856, 10.634443]"
43,Kartik Ohri;Robert Kaye,The ListenBrainz Listens Dataset,2024,https://doi.org/10.5281/zenodo.14877361,Kartik Ohri+MetaBrainz Foundation Inc.>USA>company;Robert Kaye+MetaBrainz Foundation Inc.>USA>company,"The ListenBrainz listens dataset is a continually evolving repository of music listening history events submitted by all ListenBrainz users. Currently totalling over 800 million entries, each datum within the dataset encapsulates a timestamp, a pseudonymous user identifier, track metadata, and optionally MusicBrainz identifiers facilitating seamless linkage to external resources and datasets. This paper discusses the process of raw data acquisition, the subsequent steps of data synthesis and cleaning, the comprehensive contents of the refined dataset, and the diverse potential applications of this invaluable resource. Although not the largest dataset in terms of music listening events (yet), its distinctiveness lies in its perpetual evolution, with users contributing data daily. This paper underscores the significance of the ListenBrainz listens dataset as a significant asset for researchers and practitioners alike, offering insights into music consumption patterns, user preferences, and avenues for further exploration in the fields of music information retrieval and recommendation systems.",USA,company,Developed economies,"[-22.410452, 10.282226]","[42.205692, 21.47511]","[-18.653494, -7.8299775, -15.244478]","[10.724087, 9.600045, 13.397645]","[13.143868, 7.6292105]","[12.332196, 1.4642488]","[14.319042, 13.777614, -1.303833]","[13.028008, 5.0443354, 12.408659]"
44,Marco Comunità;Zhi Zhong;Akira Takahashi;Shiqi Yang;Mengjie Zhao;Koichi Saito;Yukara Ikemiya;Takashi Shibuya;Shusuke Takahashi;Yuki Mitsufuji,SpecMaskGIT: Masked Generative Modeling of Audio Spectrogram for Efficient Audio Synthesis and Beyond,2024,https://doi.org/10.5281/zenodo.14877363,Marco Comunità+Queen Mary University of London>GBR>education|Sony Group Corporation>JPN>company|Sony AI>USA>company;Zhi Zhong+Sony Group Corporation>JPN>company|Sony AI>USA>company;Akira Takahashi+Sony Group Corporation>JPN>company|Sony AI>USA>company;Shiqi Yang+Sony Group Corporation>JPN>company|Sony AI>USA>company;Mengjie Zhao+Sony Group Corporation>JPN>company|Sony AI>USA>company;Koichi Saito+Sony AI>USA>company;Yukara Ikemiya+Sony AI>JPN>company;Takashi Shibuya+Sony AI>JPN>company;Shusuke Takahashi+Sony Group Corporation>JPN>company|Sony AI>USA>company;Yuki Mitsufuji+Sony Group Corporation>JPN>company|Sony AI>USA>company,"Recent advances in generative models that iteratively synthesize audio clips sparked great success in text-to-audio synthesis (TTA), but at the cost of slow synthesis speed and heavy computation. Although there have been attempts to accelerate the iterative procedure, high-quality TTA systems remain inefficient due to the hundreds of iterations required in the inference phase and large amount of model parameters. To address these challenges, we propose SpecMaskGIT, a light-weight, efficient yet effective TTA model based on the masked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a realistic 10 s audio clip in less than 16 iterations, an order of magnitude less than previous iterative TTA methods. As a discrete model, SpecMaskGIT outperforms larger VQ-Diffusion and auto-regressive models in a TTA benchmark, while being real-time with only 4 CPU cores or even 30× faster with a GPU. Next, built upon a latent space of Mel-spectrograms, SpecMaskGIT has a wider range of applications (e.g., zero-shot bandwidth extension) than similar methods built on latent wave domains. Moreover, we interpret SpecMaskGIT as a generative extension to previous discriminative audio masked Transformers, and shed light on its audio representation learning potential. We hope that our work will inspire the exploration of masked audio modeling toward further diverse scenarios.",GBR,education,Developed economies,"[13.714384, -31.13021]","[-19.897644, -46.834522]","[25.64292, 0.12801544, 5.4588995]","[-21.565636, -5.480794, -17.459785]","[9.43403, 8.606515]","[8.092, 6.6469574]","[12.425405, 12.27994, 0.6834353]","[9.616568, 6.020555, 8.066998]"
45,Zach Evans;Julian D. Parker;CJ Carr;Zachary Zukowski;Josiah Taylor;Jordi Pons,Long-Form Music Generation With Latent Diffusion,2024,https://doi.org/10.5281/zenodo.14877365,Zach Evans+Stability AI>USA>company;Julian D. Parker+Stability AI>USA>company;CJ Carr+Stability AI>USA>company;Zack Zukowski+Stability AI>USA>company;Josiah Taylor+Stability AI>USA>company;Jordi Pons+Stability AI>USA>company,"Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure from text prompts. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m 45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.",USA,company,Developed economies,"[20.781017, 2.7773993]","[-14.895375, -45.127106]","[3.3560524, 1.0075631, 18.395638]","[-19.094065, -2.6152487, -16.20259]","[10.250251, 8.589169]","[8.626652, 6.37586]","[13.30091, 11.978354, 0.17491616]","[9.740445, 5.662381, 8.53505]"
46,Martin E. Malandro,Composer's Assistant 2: Interactive Multi-Track MIDI Infilling With Fine-Grained User Control,2024,https://doi.org/10.5281/zenodo.14877367,Martin E. Malandro+Sam Houston State University>USA>education,"We introduce Composer's Assistant 2, a system for interactive human-computer composition in the REAPER digital audio workstation. Our work upgrades the Composer's Assistant system (which performs multi-track infilling of symbolic music at the track-measure level) with a wide range of new controls to give users fine-grained control over the system's outputs. Controls introduced in this work include two types of rhythmic conditioning controls, horizontal and vertical note onset density controls, several types of pitch controls, and a rhythmic interest control. We train a T5-like transformer model to implement these controls and to serve as the backbone of our system. With these controls, we achieve a dramatic improvement in objective metrics over the original system. We also study how well our model understands the meaning of our controls, and we conduct a listening study that does not find a significant difference between real music and music composed in a co-creative fashion with our system. We release our complete system, consisting of source code, pretrained models, and REAPER scripts.",USA,education,Developed economies,"[37.10009, -0.15517846]","[-8.944332, 33.913628]","[9.904535, -6.539615, 24.403328]","[-16.22994, -11.498012, 2.7796261]","[10.307991, 7.266981]","[9.140706, 5.86334]","[12.678579, 11.533087, -0.5467137]","[10.04899, 5.486896, 9.922648]"
47,Yu-Hua Chen;Yen-Tung Yeh;Yuan-Chiao Cheng;Jui-Te Wu;Yu-Hsiang Ho;Jyh-Shing Roger Jang;Yi-Hsuan Yang,Towards Zero-Shot Amplifier Modeling: One-to-Many Amplifier Modeling via Tone Embedding Control,2024,https://doi.org/10.5281/zenodo.14877373,Yu-Hua Chen+National Taiwan University>TWN>education|Positive Grid>TWN>company;Yen-Tung Yeh+National Taiwan University>TWN>education|Positive Grid>TWN>company;Yuan-Chiao Cheng+Positive Grid>TWN>company;Jui-Te Wu+Positive Grid>TWN>company;Yu-Hsiang Ho+Positive Grid>TWN>company;Jyh-Shing Roger Jang+National Taiwan University>TWN>education|Positive Grid>TWN>company;Yi-Hsuan Yang+National Taiwan University>TWN>education,"Replicating analog device circuits through neural audio effect modeling has garnered increasing interest in recent years. Existing work has predominantly focused on a one-to-one emulation strategy, modeling specific devices individually. In this paper, we tackle the less-explored scenario of one-to-many emulation, utilizing conditioning mechanisms to emulate multiple guitar amplifiers through a single neural model. For condition representation, we use contrastive learning to build a tone embedding encoder that extracts style-related features of various amplifiers, leveraging a dataset of comprehensive amplifier settings. Targeting zero-shot application scenarios, we also examine various strategies for tone embedding representation, evaluating referenced tone embedding against two retrievalbased embedding methods for amplifiers unseen in the training time. Our findings showcase the efficacy and potential of the proposed methods in achieving versatile one-to-many amplifier modeling, contributing a foundational step towards zero-shot audio modeling applications.",TWN,education,Developing economies,"[1.0378695, -37.56631]","[-19.226751, -43.695347]","[24.60221, 2.3404934, -14.274137]","[-23.570381, -1.4772108, -22.964088]","[9.24586, 10.303181]","[8.403397, 6.4282713]","[11.039952, 13.97985, 1.0055956]","[9.941713, 5.95498, 8.357155]"
35,Jiajun Deng;Yaolong Ju;Jing Yang;Simon Lui;Xunying Liu,Efficient Adapter Tuning for Joint Singing Voice Beat and Downbeat Tracking With Self-Supervised Learning Features,2024,https://doi.org/10.5281/zenodo.14877345,"Jiajun Deng+Huawei Technologies Co., Ltd.>CHN>company|The Chinese University of Hong Kong>HKG>education;Yaolong Ju+Huawei Technologies Co., Ltd.>CHN>company|The Chinese University of Hong Kong>HKG>education;Jing Yang+Huawei Technologies Co., Ltd.>CHN>company|The Chinese University of Hong Kong>HKG>education;Simon Lui+Huawei Technologies Co., Ltd.>CHN>company|The Chinese University of Hong Kong>HKG>education;Xunying Liu+Huawei Technologies Co., Ltd.>CHN>company|The Chinese University of Hong Kong>HKG>education","Singing voice beat tracking is a challenging task, due to the lack of musical accompaniment that often contains robust rhythmic and harmonic patterns, something most existing beat tracking systems utilize and can be essential for estimating beats. In this paper, a novel temporal convolutional network-based beat-tracking approach featuring self-supervised learning (SSL) representations and adapter tuning is proposed to track the beat and downbeat of singing voices jointly. The SSL DistilHuBERT representations are utilized to capture the semantic information of singing voices and are further fused with the generic spectral features to facilitate beat estimation. Sources of variabilities that are particularly prominent with the non-homogeneous singing voice data are reduced by the efficient adapter tuning. Extensive experiments show that feature fusion and adapter tuning improve the performance individually, and the combination of both leads to significantly better performances than the un-adapted baseline system, with up to 31.6% and 42.4% absolute F1-score improvements on beat and downbeat tracking, respectively.",CHN,company,Developing economies,"[30.412485, -35.753628]","[-29.227163, -15.819838]","[11.069493, -35.837765, -8.365127]","[-10.719889, 11.322226, -15.38983]","[10.344233, 4.295365]","[4.9686923, 2.4263842]","[10.07654, 12.939403, -2.0734332]","[7.6285334, 7.0141544, 10.304456]"
48,Ju-Chiang Wang;Wei-Tsung Lu;Jitong Chen,Mel-RoFormer for Vocal Separation and Vocal Melody Transcription,2024,https://doi.org/10.5281/zenodo.14877371,Ju-Chiang Wang+ByteDance>USA>company;Wei-Tsung Lu+ByteDance>USA>company;Jitong Chen+ByteDance>USA>company,"Developing a versatile deep neural network to model music audio is crucial in MIR. This task is challenging due to the intricate spectral variations inherent in music signals, which convey melody, harmonics, and timbres of diverse instruments. In this paper, we introduce Mel-RoFormer, a spectrogram-based model featuring two key designs: a novel Mel-band Projection module at the front-end to enhance the model's capability to capture informative features across multiple frequency bands, and interleaved RoPE Transformers to explicitly model the frequency and time dimensions as two separate sequences. We apply Mel-RoFormer to tackle two essential MIR tasks: vocal separation and vocal melody transcription, aimed at isolating singing voices from audio mixtures and transcribing their lead melodies, respectively. Despite their shared focus on singing signals, these tasks possess distinct optimization objectives. Instead of training a unified model, we adopt a two-step approach. Initially, we train a vocal separation model, which subsequently serves as a foundation model for fine-tuning for vocal melody transcription. Through extensive experiments conducted on benchmark datasets, we showcase that our models achieve state-of-the-art performance in both vocal separation and melody transcription tasks, underscoring the efficacy and versatility of Mel-RoFormer in modeling complex music audio signals.",USA,company,Developed economies,"[0.9547007, -43.602413]","[-37.868275, -35.8869]","[29.378986, 4.8568573, -6.597427]","[-15.585738, -7.8764257, -23.112999]","[8.969509, 10.621395]","[7.38697, 5.8143525]","[10.819799, 14.486146, 1.3658013]","[9.731568, 8.000362, 8.742096]"
50,Jinlong Zhu;Keigo Sakurai;Ren Togo;Takahiro Ogawa;Miki Haseyama,MMT-BERT: Chord-Aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT,2024,https://doi.org/10.5281/zenodo.14877377,Jinlong Zhu+Hokkaido University>JPN>education;Keigo Sakurai+Hokkaido University>JPN>education;Ren Togo+Hokkaido University>JPN>education;Takahiro Ogawa+Hokkaido University>JPN>education;Miki Haseyama+Hokkaido University>JPN>education,"We propose a novel symbolic music representation and Generative Adversarial Network (GAN) framework specially designed for symbolic multitrack music generation. The main theme of symbolic music generation primarily encompasses the preprocessing of music data and the implementation of a deep learning framework. Current techniques dedicated to symbolic music generation generally encounter two significant challenges: training data's lack of information about chords and scales and the requirement of specially designed model architecture adapted to the unique format of symbolic music representation. In this paper, we solve the above problems by introducing new symbolic music representation with MusicLang chord analysis model. We propose our MMT-BERT architecture adapting to the representation. To build a robust multitrack music generator, we fine-tune a pre-trained MusicBERT model to serve as the discriminator, and incorporate relativistic standard loss. This approach, supported by the in-depth understanding of symbolic music encoded within MusicBERT, fortifies the consonance and humanity of music generated by our method. Experimental results demonstrate the effectiveness of our approach which strictly follows the state-of-the-art methods.",JPN,education,Developed economies,"[21.275267, 9.565423]","[-9.8851385, -40.265793]","[1.0157795, -2.6161609, 22.354973]","[-21.986147, 2.8350978, -12.526964]","[10.313332, 8.18983]","[9.000483, 6.4653044]","[13.317791, 11.583385, -0.007773206]","[9.633764, 5.5197206, 8.668449]"
51,Recep Oguz Araz;Xavier Serra;Dmitry Bogdanov,Discogs-VI: A Musical Version Identification Dataset Based on Public Editorial Metadata,2024,https://doi.org/10.5281/zenodo.14877379,"R. Oguz Araz+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Current version identification (VI) datasets often lack sufficient size and musical diversity to train robust neural networks (NNs). Additionally, their non-representative clique size distributions prevent realistic system evaluations. To address these challenges, we explore the untapped potential of the rich editorial metadata in the Discogs music database and create a large dataset of musical versions containing about 1,900,000 versions across 348,000 cliques. Utilizing a high-precision search algorithm, we map this dataset to official music uploads on YouTube, resulting in a dataset of approximately 493,000 versions across 98,000 cliques. This dataset offers over nine times the number of cliques and over four times the number of versions than existing datasets. We demonstrate the utility of our dataset by training a baseline NN without extensive model complexities or data augmentations, which achieves competitive results on the SHS100K and Da-TACOS datasets. Our dataset, along with the tools used for its creation, the extracted audio features, and a trained model, are all publicly available online.",ESP,education,Developed economies,"[1.8148689, 43.383003]","[9.191452, 23.1548]","[-1.6372564, 18.052225, -19.796337]","[15.533137, 17.35137, -6.6147513]","[13.942063, 9.273163]","[10.171444, 4.0494785]","[13.201618, 16.525972, -0.6294908]","[11.646605, 6.103141, 10.532267]"
52,Nicholas Cornia;Bruno Forment,Who's Afraid of the `Artyfyshall Byrd'? Historical Notions and Current Challenges of Musical Artificiality,2024,https://doi.org/10.5281/zenodo.14877381,Nicholas Cornia+Orpheus Instituut>BEL>Unknown;Bruno Forment+Orpheus Instituut>BEL>Unknown,"The meteoric surge of AI-generated music has prompted significant concerns among artists and publishers alike. Some fear that the adoption of AI is poised to result in massive job destruction; others sense it will jeopardize and eventually upend all legal frameworks of intellectual property. AI, however, is not the first instance where humanity has confronted the prospect of machines emulating musical creativity. Already in the Baroque, various modes of musical artificiality were explored, ranging from automata and organ stops mimicking human performance and natural sounds, up to devices for mechanized composition (e.g., Athanasius Kircher, Johann Philip Kirnberger, C.P.E. Bach, Antonio Calegari and Diederich Nickolaus Winkel). Valuable insights emerge from the reconsideration—and digital implementation—of these curiosities through the lens of present-day generative models. It can be argued that the very notion of 'artificiality' has presented humanity with long-standing philosophical dilemmas, in addressing the debate on the role of art as a substitute of (divine) nature. By digitally implementing and formalizing some pioneering instances of algorithmically-generated music we wish to illustrate how mechanical devices have played a role in human art and entertainment prior to our digital era.",BEL,Unknown,Developed economies,"[2.7938147, 6.8692985]","[-6.5658007, 43.877693]","[-8.603628, -2.6401286, 7.3164134]","[-20.00263, 8.629191, 4.525202]","[12.641832, 7.9195747]","[9.769964, 5.812507]","[13.381663, 13.530859, -1.0216366]","[10.280223, 5.134943, 9.611878]"
53,Yaolong Ju;Chun Yat Wu;Betty Cortiñas Lorenzo;Jing Yang;Jiajun Deng;Fan Fan;Simon Lui,End-to-End Automatic Singing Skill Evaluation Using Cross-Attention and Data Augmentation for Solo Singing and Singing With Accompaniment,2024,https://doi.org/10.5281/zenodo.14877383,"Yaolong Ju+Huawei Technologies Co., Ltd.>CHN>company;Chun Yat Wu+Huawei Technologies Co., Ltd.>CHN>company;Betty Cortiñas Lorenzo+Huawei Technologies Co., Ltd.>CHN>company;Jing Yang+Huawei Technologies Co., Ltd.>CHN>company;Jiajun Deng+Huawei Technologies Co., Ltd.>CHN>company;Fan Fan+Huawei Technologies Co., Ltd.>CHN>company;Simon Lui+Huawei Technologies Co., Ltd.>CHN>company","Automatic singing skill evaluation (ASSE) systems are predominantly designed for solo singing, and the scenario of singing with accompaniment is largely unaddressed. In this paper, we propose an end-to-end ASSE system that effectively processes both solo singing and singing with accompaniment using data augmentation, where a comparative study is conducted on four different data augmentation approaches. Additionally, we incorporate bi-directional cross-attention (BiCA) for feature fusion which, compared to simple concatenation, can better exploit the inter-relationships between different features. Results on the 10KSinging dataset show that data augmentation and BiCA boost performance individually. When combined, they contribute to further improvements, with a Pearson correlation coefficient of 0.769 for solo singing and 0.709 for singing with accompaniment. This represents relative improvements of 36.8% and 26.2% compared to the baseline model score of 0.562, respectively.",CHN,company,Developing economies,"[-5.159943, -33.835884]","[-29.50362, -39.11811]","[17.824781, 11.243506, -9.163452]","[0.21149549, -6.5590687, -21.072891]","[9.9549465, 11.059915]","[8.07326, 4.611946]","[11.349309, 15.138181, 0.8503428]","[10.44881, 7.2965326, 9.084664]"
54,Francesco Foscarin;Emmanouil Karystinaios;Eita Nakamura;Gerhard Widmer,Cluster and Separate: A GNN Approach to Voice and Staff Prediction for Score Engraving,2024,https://doi.org/10.5281/zenodo.14877385,"Francesco Foscarin+Johannes Kepler University>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Emmanouil Karystinaios+Johannes Kepler University>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Eita Nakamura+Kyushu University>JPN>education;Gerhard Widmer+Johannes Kepler University>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","This paper approaches the problem of separating the notes from a quantized symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score type-setting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualization of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.",AUT,education,Developed economies,"[3.6479702, -39.58789]","[-14.783385, -27.467402]","[30.033493, 6.473031, 0.36647764]","[-18.795662, -13.786151, -11.310512]","[8.971869, 10.384592]","[8.683453, 5.126415]","[11.04832, 14.231938, 1.2705523]","[9.825189, 6.150824, 9.636891]"
55,Huan Zhang;Jinhua Liang;Simon Dixon,From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano,2024,https://doi.org/10.5281/zenodo.14877387,"Huan Zhang+Centre for Digital Music, Queen Mary University of London>GBR>education;Jinhua Liang+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","Our study investigates an approach for understanding musical performances through the lens of audio encoding models, focusing on the domain of solo Western classical piano music. Compared to composition-level attribute understanding such as key or genre, we identify a knowledge gap in performance-level music understanding, and address three critical tasks: expertise ranking, difficulty estimation, and piano technique detection, introducing a comprehensive Pianism-Labelling Dataset (PLD) for this purpose. We leverage pre-trained audio encoders, specifically Jukebox, Audio-MAE, MERT, and DAC, demonstrating varied capabilities in tackling downstream tasks, to explore whether domain-specific fine-tuning enhances capability in capturing performance nuances. Our best approach achieved 93.6% accuracy in expertise ranking, 33.7% in difficulty estimation, and 46.7% in technique detection, with Audio-MAE as the overall most effective encoder. Finally, we conducted a case study on Chopin Piano Competition data using trained models for expertise ranking, which highlights the challenge of accurately assessing top-tier performances.",GBR,education,Developed economies,"[30.854464, -1.6729004]","[-37.737446, 5.2154164]","[17.090616, -0.9682409, 19.526468]","[-10.339318, -10.647622, -5.285671]","[9.968658, 7.109579]","[7.723622, 4.0483956]","[12.208558, 11.55767, -0.47074652]","[9.289727, 6.0868974, 10.169358]"
56,Pedro Ramoneda;Vsevolod E. Eremenko;Alexandre D'Hooge;Emilia Parada-Cabaleiro;Xavier Serra,Towards Explainable and Interpretable Musical Difficulty Estimation: A Parameter-Efficient Approach,2024,https://doi.org/10.5281/zenodo.14877389,"Pedro Ramoneda+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Univ. Lille>FRA>education|Nuremberg University of Music>DEU>education;Vsevolod Eremenko+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Univ. Lille>FRA>education|Nuremberg University of Music>DEU>education;Alexandre D’Hooge+Univ. Lille>FRA>education;Emilia Parada-Cabaleiro+Nuremberg University of Music>DEU>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Estimating music piece difficulty is important for organizing educational music collections. This process could be partially automatized to facilitate the educator's role. Nevertheless, the decisions performed by prevalent deep-learning models are hardly understandable, which may impair the acceptance of such a technology in music education curricula. Our work employs explainable descriptors for difficulty estimation in symbolic music representations. Furthermore, through a novel parameter-efficient white-box model, we outperform previous efforts while delivering interpretable results. These comprehensible outcomes emulate the functionality of a rubric, a tool widely used in music education. Our approach, evaluated in piano repertoire categorized in 9 classes, achieved 41.4% accuracy independently, with a mean squared error (MSE) of 1.7, showing precise difficulty estimation. Through our baseline, we illustrate how building on top of past research can offer alternatives for music difficulty assessment which are explainable and interpretable. With this, we aim to promote a more effective communication between the Music Information Retrieval (MIR) community and the music education one.",ESP,education,Developed economies,"[-10.624893, 2.5770643]","[-38.251854, 4.086418]","[-6.7350807, -0.22460926, 24.122438]","[-12.27719, -11.627, -4.5925226]","[12.3873415, 8.483778]","[7.747898, 4.064584]","[13.281134, 13.208104, -0.92305267]","[9.340009, 6.004839, 10.15311]"
57,Michele Newman;Lidia J. Morris;Jun Kato;Masataka Goto;Jason Yip;Jin Ha Lee,Purposeful Play: Evaluation and Co-Design of Casual Music Creation Applications With Children,2024,https://doi.org/10.5281/zenodo.14877391,Michele Newman+University of Washington>USA>education;Lidia Morris+University of Washington>USA>education;Jun Kato+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Jason Yip+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"The rise of digital technologies has increased interest in democratizing music creation, but current creativity support tools often prioritize literacy and education over meeting children's needs for casual creation. To address this, we conducted Participatory Design sessions with children aged 6-13 to explore their perceptions of casual music creation activities and identify elements of creative applications that support different expressions. Our study aimed to answer two key questions: (1) How do children perceive casual music creation activities and which elements of creative applications facilitate expression? and (2) What insights can inform the design of future casual music creation tools? Our findings indicate that children view casual music creation as involving diverse activities, with visuals aiding in understanding sounds, and engaging in various playful interactions leading to creative experiences. We present design implications based on our findings and introduce casual creation as ""purposeful play"". Furthermore, we discuss its implications for creative MIR.",USA,education,Developed economies,"[-28.2735, 29.391235]","[37.73167, 37.659004]","[-6.502181, 23.311085, 1.4420494]","[1.2869444, 13.08352, 22.20208]","[15.207498, 8.046765]","[12.026779, 0.543007]","[15.333881, 14.449042, -1.8543067]","[12.180494, 4.180538, 11.694004]"
58,Nicholas Evans;Behzad Haki;Daniel Gómez-Marín;Sergi Jordà,El Bongosero: A Crowd-Sourced Symbolic Dataset of Improvised Hand Percussion Rhythms Paired With Drum Patterns,2024,https://doi.org/10.5281/zenodo.14877393,"Nicholas Evans+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Behzad Haki+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Daniel Gómez-Marín+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Sergi Jordà+Music Technology Group, Universitat Pompeu Fabra>ESP>education","We present El Bongosero, a large-scale, open-source symbolic dataset comprising expressive, improvised drum performances crowd-sourced from a pool of individuals with varying levels of musical expertise. Originating from an interactive installation hosted at Centre de Cultura Contemporània de Barcelona, our dataset consists of 6,035 unique tapped sequences performed by 3,184 participants. To our knowledge, this is the only symbolic dataset of its size and type that includes expressive timing and dynamics information as well as each participant's level of expertise. These unique characteristics could prove to be valuable to future research, particularly in the areas of music generation and music education. Preliminary analysis, including a step-wise Jaccard similarity analysis on a subset of the data, demonstrate that this dataset is a diverse, non-random, and musically meaningful collection. To facilitate prompt exploration and understanding of the data, we have also prepared a dedicated website and an open-source API in order to interact with the data.",ESP,education,Developed economies,"[24.31573, -49.80878]","[-4.573364, 26.228785]","[20.550495, -22.446365, -4.043608]","[-10.017723, 4.183441, 12.912573]","[7.846042, 7.035836]","[9.179644, 3.9883952]","[10.263145, 11.67732, 0.93297064]","[10.303537, 6.050586, 11.012333]"
59,Joanne Affolter;Martin A. Rohrmeier,Utilizing Listener-Provided Tags for Music Emotion Recognition: A Data-Driven Approach,2024,https://doi.org/10.5281/zenodo.14877395,J. Affolter+Ecole Polytechnique Fédérale de Lausanne>CHE>education|EPFL>CHE>education;M. Rohrmeier+Ecole Polytechnique Fédérale de Lausanne>CHE>education|EPFL>CHE>education,"This work introduces a data-driven approach for assigning emotions to music tracks. Consisting of two distinct phases, our framework enables the creation of synthetic emotion-labeled datasets that can serve both Music Emotion Recognition and Auto-Tagging tasks. The first phase presents a versatile method for collecting listener-generated verbal data, such as tags and playlist names, from multiple online sources on a large scale. We compiled a dataset of 5, 892 tracks, each associated with textual data from four distinct sources. The second phase leverages Natural Language Processing for representing music-evoked emotions, relying solely on the data acquired during the first phase. By semantically matching user-generated text to a well-known corpus of emotion-labelled English words, we are ultimately able to represent each music track as an 8-dimensional vector that captures the emotions perceived by listeners. Our method departs from conventional labeling techniques: instead of defining emotions as generic ""mood tags"" found on social platforms, we leverage a refined psychological model drawn from Plutchik's theory [1], which appears more intuitive than the extensively used Valence-Arousal model.",CHE,education,Developed economies,"[-57.443523, -1.8180366]","[48.66528, -4.928371]","[-23.86831, 19.389473, 4.8791056]","[14.909453, 21.157528, 4.3198237]","[13.90217, 12.823537]","[12.811861, 3.9059687]","[16.01352, 14.520573, 1.6281452]","[13.940001, 5.2852206, 10.51272]"
60,Chih-Pin Tan;Hsin Ai;Yi-Hsin Chang;Shuen-Huei Guan;Yi-Hsuan Yang,PiCoGen2: Piano Cover Generation With Transfer Learning Approach and Weakly Aligned Data,2024,https://doi.org/10.5281/zenodo.14877397,Chih-Pin Tan+National Taiwan University>TWN>education|KKCompany Technologies>Unknown>company;Hsin Ai+National Taiwan University>TWN>education|KKCompany Technologies>Unknown>company;Yi-Hsin Chang+National Taiwan University>TWN>education|KKCompany Technologies>Unknown>company;Shuen-Huei Guan+KKCompany Technologies>Unknown>company|National Taiwan University>Unknown>education;Yi-Hsuan Yang+National Taiwan University>TWN>education|KKCompany Technologies>Unknown>company,"Piano cover generation aims to create a piano cover from a pop song. Existing approaches mainly employ supervised learning and the training demands strongly-aligned and paired song-to-piano data, which is built by remapping piano notes to song audio. This would, however, result in the loss of piano information and accordingly cause inconsistencies between the original and remapped piano versions. To overcome this limitation, we propose a transfer learning approach that pre-trains our model on piano-only data and fine-tunes it on weakly-aligned paired data constructed without note remapping. During pre-training, to guide the model to learn piano composition concepts instead of merely transcribing audio, we use an existing lead sheet transcription model as the encoder to extract high-level features from the piano recordings. The pre-trained model is then fine-tuned on the paired song-piano data to transfer the learned composition knowledge to the pop song domain. Our evaluation shows that this training strategy enables our model, named PiCoGen2, to attain high-quality results, outperforming baselines on both objective and subjective metrics across five pop genres.",TWN,education,Developing economies,"[31.807533, 3.1361742]","[-21.114473, -28.330791]","[10.713029, 0.0827038, 20.73612]","[-12.731081, -9.366078, -8.842436]","[10.19086, 7.5102224]","[8.352421, 4.959023]","[12.636997, 11.643225, -0.14769]","[9.379724, 6.314736, 9.471835]"
2,Xingjian Du;Mingyu Liu;Pei Zou;Xia Liang;Zijie Wang;Huidong Liang;Bilei Zhu,X-Cover: Better Music Version Identification System by Integrating Pretrained ASR Model,2024,https://doi.org/10.5281/zenodo.14877280,Xingjian Du+ByteDance Inc.>CHN>company|University of Oxford>GBR>education;Mingyu Liu+ByteDance Inc.>CHN>company;Pei Zou+ByteDance Inc.>CHN>company;Xia Liang+ByteDance Inc.>CHN>company;Zijie Wang+ByteDance Inc.>CHN>company;Huidong Liang+University of Oxford>GBR>education;Bilei Zhu+ByteDance Inc.>CHN>company,"Methods based on deep learning have emerged as a dominant approach for cover song identification (CSI) literature over the past years, among which ByteCover systems have consistently delivered state-of-the-art performance across major CSI datasets in the field. Despite its steady improvements along previous generations from audio feature dimensionality reduction to short query identification, the system is found to be vulnerable to audios with noise and ambiguous melody when extracting musical information from constant-Q transformation (CQT) spectrograms. Although some recent studies suggest that incorporating lyric-related features can enhance the overall performance of CSI systems, this approach typically requires training a separate automatic lyric recognition (ALR) model to extract lyric-related features from music recordings. In this work, we introduce X-Cover, the latest CSI system that incorporates a pre-trained automatic speech recognition (ASR) module, Whisper, to extract and integrate lyrics-related features into modelling. Specifically, we jointly fine-tune the ASR block and the previous ByteCover3 system in a parameter-efficient fashion, which largely reduces the cost of using lyric information compared to training a new ALR model from scratch. In addition, a bag of tricks is further applied to the training of this new generation, assisting X-Cover to achieve strong performance across various datasets.",CHN,company,Developing economies,"[2.2516212, 43.615047]","[3.037402, -28.94352]","[-0.3084771, 17.656641, -20.412413]","[20.730158, -6.259204, 0.35133693]","[15.906687, 10.948672]","[9.967702, 3.917722]","[13.057747, 16.93163, -0.48998997]","[11.103863, 6.720272, 9.818783]"
49,Noelia N. Luna-Barahona;Adrián Roselló;María Alfaro-Contreras;David Rizo;Jorge Calvo-Zaragoza,Unsupervised Synthetic-to-Real Adaptation for Optical Music Recognition,2024,https://doi.org/10.5281/zenodo.14877375,Noelia Luna-Barahona+University of Alicante>ESP>education;Adrián Roselló+University of Alicante>ESP>education;María Alfaro-Contreras+University of Alicante>ESP>education;David Rizo+University of Alicante>ESP>education|Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"The field of Optical Music Recognition (OMR) focuses on models capable of reading music scores from document images. Despite its growing popularity, OMR is still confined to settings where the target scores are similar in both musical context and visual presentation to the data used for training the model. The common scenario, therefore, involves manually annotating data for each specific case, a process that is not only labor-intensive but also raises concerns regarding practicality. We present a methodology based on training a neural model with synthetic images, thus reducing the difficulty of obtaining labeled data. As sheet music renderings depict regular visual characteristics compared to scores from real collections, we propose an unsupervised neural adaptation approach consisting of loss functions that promote alignment between the features learned by the model and those of the target collection while preventing the model from converging to undesirable solutions. This unsupervised adaptation bypasses the need for extensive retraining, requiring only the unlabeled target images. Our experiments, focused on music written in Mensural notation, demonstrate that the methodology is successful and that synthetic-to-real adaptation is indeed a promising way to create practical OMR systems with little human effort.",ESP,education,Developed economies,"[38.001987, 18.527555]","[-20.23823, -25.693703]","[15.19609, 12.0095825, 8.920573]","[-17.6936, -19.2567, -4.274383]","[8.724669, 6.2786803]","[6.432425, -1.1526154]","[10.758105, 11.128559, -0.10047835]","[8.169266, 4.283265, 9.865643]"
34,Jingyue Huang;Ke Chen;Yi-Hsuan Yang,Emotion-Driven Piano Music Generation via Two-Stage Disentanglement and Functional Representation,2024,https://doi.org/10.5281/zenodo.14877343,Jingyue Huang+UC San Diego>USA>education;Ke Chen+UC San Diego>USA>education;Yi-Hsuan Yang+National Taiwan University>TWN>education,"Managing the emotional aspect remains a challenge in automatic music generation. Prior works aim to learn various emotions at once, leading to inadequate modeling. This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework. The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes. To further capture features that shape valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music. This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures. Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling. We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation.",USA,education,Developed economies,"[22.826872, 5.4165497]","[51.87116, -11.359688]","[1.3717271, 6.8441057, 23.099905]","[8.089816, 21.267717, 2.7348983]","[10.395408, 8.227911]","[12.906464, 4.36201]","[13.337781, 11.875256, 0.19764978]","[13.985717, 4.9341755, 10.126905]"
33,Dorian Desblancs;Gabriel Meseguer-Brocal;Romain Hennequin;Manuel Moussallam,From Real to Cloned Singer Identification,2024,https://doi.org/10.5281/zenodo.14877342,Dorian Desblancs+Deezer Research>FRA>company;Gabriel Meseguer-Brocal+Deezer Research>FRA>company;Romain Hennequin+Deezer Research>FRA>company;Manuel Moussallam+Deezer Research>FRA>company,"Cloned voices of popular singers sound increasingly realistic and have gained popularity over the past few years. They however pose a threat to the industry due to personality rights concerns. As such, methods to identify the original singer in synthetic voices are needed. In this paper, we investigate how singer identification methods could be used for such a task. We present three embedding models that are trained using a singer-level contrastive learning scheme, where positive pairs consist of segments with vocals from the same singers. These segments can be mixtures for the first model, vocals for the second, and both for the third. We demonstrate that all three models are highly capable of identifying real singers. However, their performance deteriorates when classifying cloned versions of singers in our evaluation set. This is especially true for models that use mixtures as an input. These findings highlight the need to understand the biases that exist within singer identification systems, and how they can influence the identification of voice deepfakes in music.",FRA,company,Developed economies,"[-14.281226, -38.12755]","[-30.311325, -43.511936]","[16.24977, 12.287929, -22.717585]","[3.76008, -10.61281, -22.732618]","[10.206876, 11.58365]","[7.8101087, 4.764696]","[11.503178, 15.719731, 0.69453436]","[10.434657, 7.479807, 8.962692]"
32,Tim Beyer;Angela Dai,End-to-End Piano Performance-MIDI to Score Conversion With Transformers,2024,https://doi.org/10.5281/zenodo.14877339,Tim Beyer+Technical University of Munich>DEU>education|Technical University of Munich>DEU>education;Angela Dai+Technical University of Munich>DEU>education|Technical University of Munich>DEU>education,"The automated creation of accurate musical notation from an expressive human performance is a fundamental task in computational musicology. To this end, we present an end-to-end deep learning approach that constructs detailed musical scores directly from real-world piano performance-MIDI files. We introduce a modern transformer-based architecture with a novel tokenized representation for symbolic music data. Framing the task as sequence-to-sequence translation rather than note-wise classification reduces alignment requirements and annotation costs, while allowing the prediction of more concise and accurate notation. To serialize symbolic music data, we design a custom tokenization stage based on compound tokens that carefully quantizes continuous values. This technique preserves more score information while reducing sequence lengths by 3.5× compared to prior approaches. Using the transformer backbone, our method demonstrates better understanding of note values, rhythmic structure, and details such as staff assignment. When evaluated end-to-end using transcription metrics such as MUSTER, we achieve significant improvements over previous deep learning approaches and complex HMM-based state-of-the-art pipelines. Our method is also the first to directly predict notational details like trill marks or stem direction from performance data. Code and models are available on GitHub.",DEU,education,Developed economies,"[36.60301, -2.1352851]","[-14.980212, -33.307693]","[12.317119, -8.139125, 22.738121]","[-12.957091, -4.168494, -9.385432]","[10.026745, 7.0019393]","[8.498826, 5.5661044]","[12.231196, 11.417785, -0.51960063]","[9.22101, 6.0453243, 9.301341]"
5,Marios Glytsos;Christos Garoufis;Athanasia Zlatintsi;Petros Maragos,Classical Guitar Duet Separation Using GuitarDuets - A Dataset of Real and Synthesized Guitar Recordings,2024,https://doi.org/10.5281/zenodo.14877285,"Marios Glytsos+Robotics Institute, Athena Research Center>GRC>facility;Christos Garoufis+Robotics Institute, Athena Research Center>GRC>facility;Athanasia Zlatintsi+Robotics Institute, Athena Research Center>GRC>facility;Petros Maragos+Robotics Institute, Athena Research Center>GRC>facility","Recent advancements in music source separation (MSS) have focused in the multi-timbral case, with existing architectures tailored for the separation of distinct instruments, overlooking thus the challenge of separating instruments with similar timbral characteristics. Addressing this gap, our work focuses on monotimbral MSS, specifically within the context of classical guitar duets. To this end, we introduce the GuitarDuets dataset, featuring a combined total of approximately three hours of real and synthesized classical guitar duet recordings, as well as note-level annotations of the synthesized duets. We perform an extensive cross-dataset evaluation by adapting Demucs, a state-of-the-art MSS architecture, to monotimbral source separation. Furthermore, we develop a joint permutation-invariant transcription and separation framework, to exploit note event predictions as auxiliary information. Our results indicate that utilizing both the real and synthesized subsets of GuitarDuets leads to improved separation performance in an independently recorded test set compared to utilizing solely one subset. We also find that while the availability of ground-truth note labels greatly helps the performance of the separation network, the predicted note estimates result only in marginal improvement. Finally, we discuss the behavior of commonly utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.",GRC,facility,Developed economies,"[5.397386, -42.257336]","[-39.03628, -29.015175]","[25.449951, -0.9876952, 0.008372261]","[-12.335568, -6.0977807, -28.081217]","[8.26712, 9.7765665]","[6.831447, 5.7430067]","[11.011733, 13.333255, 1.575978]","[9.665904, 8.348627, 9.224307]"
6,Ziya Zhou;Yuhang Wu;Zhiyue Wu;Xinyue Zhang;Ruibin Yuan;Yinghao Ma;Lu Wang;Emmanouil Benetos;Wei Xue;Yike Guo,"Can LLMs ""Reason"" in Music? an Evaluation of LLMs' Capability of Music Understanding and Generation",2024,https://doi.org/10.5281/zenodo.14877281,"Ziya Zhou+The Hong Kong University of Science and Technology>HKG>education|Multimodal Art Projection>Unknown>Unknown|Shenzhen University>CHN>education|C4DM, Queen Mary University of London>GBR>education;Yuhang Wu+Multimodal Art Projection>Unknown>Unknown|Shenzhen University>CHN>education|C4DM, Queen Mary University of London>GBR>education;Zhiyue Wu+Shenzhen University>CHN>education|C4DM, Queen Mary University of London>GBR>education;Xinyue Zhang+Multimodal Art Projection>Unknown>Unknown;Ruibin Yuan+The Hong Kong University of Science and Technology>HKG>education|Multimodal Art Projection>Unknown>Unknown;Yinghao Ma+Multimodal Art Projection>Unknown>Unknown|C4DM, Queen Mary University of London>GBR>education;Lu Wang+Shenzhen University>CHN>education;Emmanouil Benetos+C4DM, Queen Mary University of London>GBR>education;Wei Xue+The Hong Kong University of Science and Technology>HKG>education;Yike Guo+The Hong Kong University of Science and Technology>HKG>education","Symbolic Music, akin to language, can be encoded in discrete symbols. Recent research has extended the application of large language models (LLMs) such as GPT-4 and Llama2 to the symbolic music domain including understanding and generation. Yet scant research explores the details of how these LLMs perform on advanced music understanding and conditioned generation, especially from the multi-step reasoning perspective, which is a critical aspect in the conditioned, editable, and interactive human-computer co-creation process. This study conducts a thorough investigation of LLMs' capability and limitations in symbolic music processing. We identify that current LLMs exhibit poor performance in song-level multi-step music reasoning, and typically fail to leverage learned music knowledge when addressing complex musical tasks. An analysis of LLMs' responses highlights distinctly their pros and cons. Our findings suggest achieving advanced musical capability is not intrinsically obtained by LLMs, and future research should focus more on bridging the gap between music knowledge and reasoning, to improve the co-creation experience for musicians.",HKG,education,Developing economies,"[-31.789158, 2.2430205]","[-8.387515, 37.303047]","[-2.6924927, 16.115423, 19.347063]","[-19.187387, 3.0161874, 13.831727]","[12.773582, 8.712953]","[9.403971, 5.8446283]","[13.903906, 13.663864, -0.407459]","[10.043747, 5.397111, 9.509683]"
7,Marco Pasini;Stefan Lattner;George Fazekas,Music2Latent: Consistency Autoencoders for Latent Audio Compression,2024,https://doi.org/10.5281/zenodo.14877289,Marco Pasini+Queen Mary University of London>GBR>education;Stefan Lattner+Sony Computer Science Laboratories>FRA>company;György Fazekas+Queen Mary University of London>GBR>education,"Efficient audio representations in a compressed continuous latent space are critical for generative audio modeling and Music Information Retrieval (MIR) tasks. However, some existing audio autoencoders have limitations, such as multi-stage training procedures, slow iterative sampling, or low reconstruction quality. We introduce Music2Latent, an audio autoencoder that overcomes these limitations by leveraging consistency models. Music2Latent encodes samples into a compressed continuous latent space in a single end-to-end training process while enabling high-fidelity single-step reconstruction. Key innovations include conditioning the consistency model on upsampled encoder outputs at all levels through cross connections, using frequency-wise self-attention to capture long-range frequency dependencies, and employing frequency-wise learned scaling to handle varying value distributions across frequencies at different noise levels. We demonstrate that Music2Latent outperforms existing continuous audio autoencoders in sound quality and reconstruction accuracy while achieving competitive performance on downstream MIR tasks using its latent representations. To our knowledge, this represents the first successful attempt at training an end-to-end consistency autoencoder model.",GBR,education,Developed economies,"[-12.893434, -22.98209]","[-18.874315, -38.52016]","[9.335652, -8.372497, -18.769218]","[-12.2257, 0.19364518, -17.585987]","[10.84949, 8.624083]","[9.203205, 5.211443]","[13.047808, 12.716111, 0.37075526]","[10.259412, 6.2917733, 8.7067175]"
8,Johannes Zeitler;Ben Maman;Meinard Müller,Robust and Accurate Audio Synchronization Using Raw Features From Transcription Models,2024,https://doi.org/10.5281/zenodo.14877291,Johannes Zeitler+International Audio Laboratories Erlangen>DEU>facility;Ben Maman+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"In music information retrieval (MIR), precise synchronization of musical events is crucial for tasks like aligning symbolic information with music recordings or transferring annotations between audio versions. To achieve high temporal accuracy, synchronization approaches integrate onset-related information extracted from music recordings using either traditional signal processing techniques or exploiting symbolic representations obtained by data-driven automated music transcription (AMT) approaches. In line with this research direction, our paper introduces a high-resolution synchronization approach that combines recent AMT techniques with traditional synchronization methods. Rather than relying on the final symbolic AMT results, we show how to exploit raw onset and frame predictions obtained as intermediate outcomes from a state-of-the-art AMT approach. Through extensive evaluations conducted on piano recordings under varied acoustic conditions across different transcription models, audio features, and dynamic time warping variants, we illustrate the advantages of our proposed method in both audio–audio and audio–score synchronization tasks. Specifically, we emphasize the effectiveness of our approach in aligning historical piano recordings with poor audio quality. We underscore how additional fine-tuning steps of the transcription model on the target dataset enhance alignment robustness, even in challenging acoustic environments.",DEU,facility,Developed economies,"[22.666992, -30.710436]","[-20.63858, -14.783049]","[0.034842357, -17.674417, -17.44187]","[-1.7112286, -23.379938, -7.3544645]","[10.953543, 5.8431044]","[5.9135365, 0.99690574]","[11.820306, 12.350231, -1.9889231]","[8.2548065, 6.062647, 10.649959]"
9,Takayuki Nakatsuka;Masahiro Hamasaki;Masataka Goto,Harnessing the Power of Distributions: Probabilistic Representation Learning on Hypersphere for Multimodal Music Information Retrieval,2024,https://doi.org/10.5281/zenodo.14877293,Takayuki Nakatsuka+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masahiro Hamasaki+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Probabilistic representation learning provides intricate and diverse representations of music content by characterizing the latent features of each content item as a probability distribution within a certain space. However, typical Music Information Retrieval (MIR) methods based on representation learning utilize a feature vector of each content item, thereby missing some details of their distributional properties. In this study, we propose a probabilistic representation learning method for multimodal MIR based on contrastive learning and optimal transport. Our method trains encoders that map each content item to a hypersphere so that the probability distributions of a positive pair of content items become close to each other, while those of an irrelevant pair are far apart. To achieve such training, we design novel loss functions that utilize both probabilistic contrastive learning and spherical sliced-Wasserstein distances. We demonstrate our method's effectiveness on benchmark datasets as well as its suitability for multimodal MIR through both a quantitative evaluation and a qualitative analysis.",JPN,facility,Developed economies,"[-14.031246, 17.191105]","[29.479965, -8.246289]","[-7.0751996, 12.277185, -11.793214]","[22.706205, -0.3339612, -1.1570652]","[13.567212, 8.470929]","[9.967047, 4.182837]","[13.721886, 14.669438, -1.5424868]","[11.433385, 6.5336385, 10.107047]"
10,Andrew M. Demetriou;Jaehun Kim;Sandy Manolios;Cynthia Liem,Towards Automated Personal Value Estimation in Song Lyrics,2024,https://doi.org/10.5281/zenodo.14877295,Andrew M. Demetriou+Delft University of Technology>NLD>education;Jaehun Kim+SiriusXM/Pandora>USA>company;Sandy Manolios+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education,"Most music widely consumed in Western Countries contains song lyrics, with U.S. samples reporting almost all of their song libraries contain lyrics. In parallel, social science theory suggests that personal values - the abstract goals that guide our decisions and behaviors - play an important role in communication: we share what is important to us to coordinate efforts, solve problems and meet challenges. Thus, the values communicated in song lyrics may be similar or different to those of the listener, and by extension affect the listener's reaction to the song. This suggests that working towards automated estimation of values in lyrics may assist in downstream MIR tasks, in particular, personalization. However, as highly subjective text, song lyrics present a challenge in terms of sampling songs to be annotated, annotation methods, and in choosing a method for aggregation. In this project, we take a perspectivist approach, guided by social science theory, to gathering annotations, estimating their quality, and aggregating them. We then compare aggregated ratings to estimates based on pre-trained sentence/word embedding models by employing a validated value dictionary. We discuss conceptually 'fuzzy' solutions to sampling and annotation challenges, promising initial results in annotation quality and in automated estimations, and future directions.",NLD,education,Developed economies,"[-33.89301, -29.688421]","[46.560104, -2.8156354]","[3.9111187, 25.622904, 1.1222857]","[17.395359, 23.655613, 3.1279685]","[11.708195, 11.775001]","[12.612042, 3.5452125]","[12.634835, 15.979918, 1.0582839]","[13.647236, 5.4064813, 10.846004]"
11,Simon Rouard;Yossi Adi;Jade Copet;Axel Roebel;Alexandre Defossez,Audio Conditioning for Music Generation via Discrete Bottleneck Features,2024,https://doi.org/10.5281/zenodo.14877297,Simon Rouard+FAIR Meta>USA>company|IRCAM - Sorbonne Université>FRA>education;Yossi Adi+FAIR Meta>USA>company|Hebrew University of Jerusalem>ISR>education;Jade Copet+FAIR Meta>USA>company;Axel Roebel+IRCAM - Sorbonne Université>FRA>education;Alexandre Défossez+Kyutai>Unknown>Unknown,"While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding ""pseudowords"" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies that validates our approach. We will release the code and we provide music samples on musicgenstyle.github.io in order to show the quality of our model.",USA,company,Developed economies,"[22.927618, 4.077653]","[-16.329517, -42.373756]","[5.4344096, -0.02838435, 23.098991]","[-20.801, -5.703618, -12.50086]","[10.17283, 8.527764]","[8.839443, 6.06644]","[13.207532, 11.897268, 0.30848882]","[10.045459, 5.7110033, 8.888991]"
12,Chenyu Gao;Federico Reuben;Tom Collins,"Variation Transformer: New Datasets, Models, and Comparative Evaluation for Symbolic Music Variation Generation",2024,https://doi.org/10.5281/zenodo.14877299,"Chenyu Gao+University of York>GBR>education;Federico Reuben+University of York>GBR>education;Tom Collins+University of Miami>USA>education|MAIA, Inc.>USA>company","Variation in music is defined as repetition of a theme, but with various modifications, playing an important role in many musical genres in developing core music ideas into longer passages. Existing research on variation in music is mostly confined to datasets consisting of classical theme-and-variation pieces, and generative models limited to melody-only representations. In this paper, to address the problem of the lack of datasets, we propose an algorithm to extract theme-and-variation pairs automatically, and use it to annotate two datasets called POP909-TVar (2,871 theme-and-variation pairs) and VGMIDI-TVar (7,830 theme-and-variation pairs). We propose both non-deep learning and deep learning based symbolic music variation generation models, and report the results of a listening study and feature-based evaluation for these models. One of our two newly proposed models, called Variation Transformer, outperforms all other models that listeners evaluated for ""variation success"", including non-deep learning and deep learning based approaches. An implication of this work for the wider field of music making is that we now have a model that can generate material with stronger and perceivably more successful relationships to some given prompt or theme.",GBR,education,Developed economies,"[20.095535, 9.668506]","[-9.457451, -41.291294]","[1.0053177, -1.978624, 19.297861]","[-20.461325, 2.1583955, -10.949387]","[10.313286, 8.250639]","[8.989708, 6.442166]","[13.25431, 11.837761, -0.08228288]","[9.591397, 5.4840355, 8.698672]"
13,Vjosa Preniqi;Iacopo Ghinassi;Julia Ive;Kyriaki Kalimeri;Charalampos Saitis,Automatic Detection of Moral Values in Music Lyrics,2024,https://doi.org/10.5281/zenodo.14877301,"Vjosa Preniqi+Centre for Digital Music, Queen Mary University of London>GBR>education;Iacopo Ghinassi+Centre for Digital Music, Queen Mary University of London>GBR>education;Julia Ive+Centre for Digital Music, Queen Mary University of London>GBR>education;Kyriaki Kalimeri+ISI Foundation>ITA>Unknown;Charalampos Saitis+Centre for Digital Music, Queen Mary University of London>GBR>education","Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts. We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines. Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.",GBR,education,Developed economies,"[-34.391014, -29.37239]","[50.570793, 1.1021398]","[3.84686, 26.24263, 2.5780013]","[17.60096, 26.197248, 5.363351]","[11.802493, 11.810235]","[12.675216, 3.7141178]","[12.664722, 15.951922, 1.0435566]","[13.6858225, 5.3623157, 10.647557]"
14,Sebastian Strahl;Meinard Müller,Semi-Supervised Piano Transcription Using Pseudo-Labeling Techniques,2024,https://doi.org/10.5281/zenodo.14877303,Sebastian Strahl+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Automatic piano transcription (APT) transforms piano recordings into symbolic note events. In recent years, APT has relied on supervised deep learning, which demands a large amount of labeled data that is often limited. This paper introduces a semi-supervised approach to APT, leveraging unlabeled data with techniques originally introduced in computer vision (CV): pseudo-labeling, consistency regularization, and distribution matching. The idea of pseudo-labeling is to use the current model for producing artificial labels for unlabeled data, and consistency regularization makes the model's predictions for unlabeled data robust to augmentations. Finally, distribution matching ensures that the pseudo-labels follow the same marginal distribution as the reference labels, adding an extra layer of robustness. Our method, tested on three piano datasets, shows improvements over purely supervised methods and performs comparably to existing semi-supervised approaches. Conceptually, this work illustrates that semi-supervised learning techniques from CV can be effectively transferred to the music domain, considerably reducing the dependence on large annotated datasets.",DEU,facility,Developed economies,"[30.387915, -5.2675934]","[-23.113987, -27.057335]","[16.641485, -4.5104327, 14.975344]","[-13.74422, -9.673755, -12.264618]","[9.685218, 7.281218]","[8.364803, 4.842578]","[11.953897, 11.527878, -0.20893505]","[9.450524, 6.791159, 9.361321]"
15,Huiran Yu;Zhiyao Duan,Note-Level Transcription of Choral Music,2024,https://doi.org/10.5281/zenodo.14877305,Huiran Yu+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"Choral music is a musical activity with one of the largest participant bases, yet it has drawn little attention from automatic music transcription research. The main reasons we argue are due to the lack of data and technical difficulties arise from diverse acoustic conditions and unique properties of choral singing. To address these challenges, in this paper we propose a Transformer-based framework for note-level transcription of choral music. This framework bypasses the frame-level processing and directly produces a sequence of notes with associated timestamps. We also introduce YouChorale, a novel choral music dataset in a cappella setting curated from the Internet. YouChorale contains 452 real-world recordings in diverse acoustic configurations of choral music from over 100 composers as well as their MIDI scores. Trained on YouChorale, our proposed model achieves state-of-the-art performance in choral music transcription, marking a significant advancement in the field.",USA,education,Developed economies,"[24.802097, -10.665544]","[-9.97347, -14.776559]","[16.92249, -0.70827717, 12.832064]","[-0.61219525, -13.384278, -10.509842]","[9.68436, 7.627057]","[6.9395056, 2.7690654]","[11.773822, 12.547772, -0.26225796]","[9.10667, 7.2136755, 10.488649]"
16,Tsung-Ping Chen;Kazuyoshi Yoshii,Learning Multifaceted Self-Similarity Over Time and Frequency for Music Structure Analysis,2024,https://doi.org/10.5281/zenodo.14877309,Tsung-Ping Chen+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"This paper describes a deep learning method for music structure analysis (MSA) that aims to split a music signal into temporal segments and assign a function label (e.g., intro, verse, or chorus) to each segment. The computational base for MSA is a spectro-temporal representation of input audio such as the spectrogram, where the compositional relationships of the spectral components provide valuable clues (e.g., chords) to the identification of structural units. However, such implicit features might be vulnerable to local operations such as convolution and pooling operations. In this paper, we hypothesize that the self-attention over the spectral domain as well as the temporal domain plays a key role in tackling MSA. Based on this hypothesis, we propose a novel MSA model built on the Transformer-in-Transformer architecture that alternately stacks spectral and temporal self-attention layers. Experiments with the Beatles, RWC, and SALAMI datasets showed the superiority of the dual-aspect self-attention. In particular, the differentiation between spectral and temporal self-attentions can provide extra performance gain. By analyzing the attention maps, we also demonstrate that self-attention can unfold tonal relationships and the internal structure of music.",JPN,education,Developed economies,"[-4.2385445, 5.5991964]","[-19.235573, -30.569942]","[-3.5563452, -2.5341954, -1.548122]","[-6.057907, -2.093317, -12.35924]","[12.352525, 8.852283]","[8.796295, 4.779323]","[13.118341, 14.334448, -0.3947437]","[10.181256, 6.9099917, 9.42445]"
17,Antonin Gagneré;Slim Essid;Geoffroy Peeters,A Contrastive Self-Supervised Learning Scheme for Beat Tracking Amenable to Few-Shot Learning,2024,https://doi.org/10.5281/zenodo.14877308,"Antonin Gagneré+LTCI - Télécom Paris, Institut Polytechnique de Paris>FRA>education;Slim Essid+LTCI - Télécom Paris, Institut Polytechnique de Paris>FRA>education;Geoffroy Peeters+LTCI - Télécom Paris, Institut Polytechnique de Paris>FRA>education","In this paper, we propose a novel Self-Supervised-Learning scheme to train rhythm analysis systems and instantiate it for few-shot beat tracking. Taking inspiration from the Contrastive Predictive Coding paradigm, we propose to train a Log-Mel-Spectrogram-Transformer-encoder to contrast observations at times separated by hypothesized beat intervals from those that are not. We do this without the knowledge of ground-truth tempo or beat positions, as we rely on the local maxima of a Predominant Local Pulse function, considered as a proxy for Tatum positions, to define candidate anchors, candidate positives (located at a distance of a power of two from the anchor) and negatives (remaining time positions). We show that a model pre-trained using this approach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be fine-tuned in the few-shot regime, i.e. with just a few annotated examples to get a competitive beat-tracking performance.",FRA,education,Developed economies,"[35.408253, -34.470722]","[-30.696577, -15.605746]","[8.872793, -33.484123, -4.9619784]","[-4.6351686, 14.514143, -16.324018]","[10.423766, 4.207225]","[4.986001, 2.518353]","[10.126486, 12.85785, -2.2385366]","[7.7156835, 6.9616303, 10.268491]"
18,Morgan Buisson;Brian McFee;Slim Essid,Using Pairwise Link Prediction and Graph Attention Networks for Music Structure Analysis,2024,https://doi.org/10.5281/zenodo.14877311,"Morgan Buisson+Télécom Paris, Institut Polytechnique de Paris>FRA>education;Brian McFee+New York University>USA>education|New York University>USA>education;Slim Essid+Télécom Paris, Institut Polytechnique de Paris>FRA>education","The task of music structure analysis has been mostly addressed as a sequential problem, by relying on the internal homogeneity of musical sections or their repetitions. In this work, we instead regard it as a pairwise link prediction task. If for any pair of time instants in a track, one can successfully predict whether they belong to the same structural entity or not, then the underlying structure can be easily recovered. Building upon this assumption, we propose a method that first learns to classify pairwise links between time frames as belonging to the same section (or segment) or not. The resulting link features, along with node-specific information, are combined through a graph attention network. The latter is regularized with a graph partitioning training objective and outputs boundary locations between musical segments and section labels. The overall system is lightweight and performs competitively with previous methods. The evaluation is done on two standard datasets for music structure analysis and an ablation study is conducted in order to gain insight on the role played by its different components.",FRA,education,Developed economies,"[-3.88723, 1.9191004]","[-0.19805084, 1.7469898]","[-1.816653, 7.3325477, 9.468079]","[2.5599523, 1.6338203, -2.1676183]","[11.848737, 8.633746]","[8.240988, 2.8511262]","[13.013495, 13.6158285, -0.19659849]","[10.5721445, 7.4747577, 11.458764]"
19,Danbinaerin Han;Mark R. H. Gotham;DongMin Kim;Hannah Park;Sihun Lee;Dasaem Jeong,Six Dragons Fly Again: Reviving 15th-Century Korean Court Music With Transformers and Novel Encoding,2024,https://doi.org/10.5281/zenodo.14877313,Danbinaerin Han+KAIST>KOR>education;Mark Gotham+King's College London>GBR>education;Dongmin Kim+Sogang University>KOR>education;Hannah Park+Sogang University>KOR>education;Sihun Lee+Sogang University>KOR>education;Dasaem Jeong+Sogang University>KOR>education,"We introduce a project that revives a piece of 15th-century Korean court music, Chihwapyeong and Chwipunghyeong, composed upon the poem Songs of the Dragon Flying to Heaven. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chihwapyeong and Chwipunghyeong were evaluated by experts and performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.",KOR,education,Developing economies,"[7.6774035, 24.262259]","[-0.67125344, -24.476103]","[-22.170887, -7.242519, 8.498662]","[-21.55733, -14.198706, -3.548155]","[12.945781, 7.2016916]","[8.826749, 4.287511]","[14.15333, 12.9867115, -1.5138962]","[9.902495, 6.2632256, 10.528516]"
20,David Rizo;Jorge Calvo-Zaragoza;Patricia García-Iasci;Teresa Delgado-Sánchez,Lessons Learned From a Project to Encode Mensural Music on a Large Scale With Optical Music Recognition,2024,https://doi.org/10.5281/zenodo.14877315,David Rizo+University of Alicante>ESP>education|Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education;Patricia García-Iasci+University of Alicante>ESP>education;Teresa Delgado-Sánchez+Biblioteca Nacional de España>ESP>facility,"This paper discusses the transcription of a collection of musical works using Optical Music Recognition (OMR) technologies during the implementation of the Spanish PolifonIA project. The project employs a research-oriented OMR application that leverages modern Artificial Intelligence (AI) technology to encode musical works from images into structured formats. The paper outlines the transcription workflow in several phases: selection, preparation, action, and resolution, emphasizing the efficiency of using AI to reduce manual transcription efforts. The tool facilitated various tasks such as document analysis, management of parts, and automatic content recognition, although manual corrections were still indispensable for ensuring accuracy, especially for complex musical notations and layouts. Our study also highlights the iterative process of model training and corrections that gradually improved transcription speed and accuracy. Furthermore, the paper delves into challenges like managing non-musical elements and the limitations of current OMR technologies with early musical notations. Our findings suggest that while automated tools significantly accelerate the transcription process, they require continuous refinement and human oversight to handle diverse and complex musical documents effectively.",ESP,education,Developed economies,"[40.984127, 21.596169]","[-22.293144, 37.707123]","[19.519793, 13.129605, 13.356976]","[-12.409988, -20.355574, 0.24957931]","[8.62236, 6.106434]","[6.8063836, -0.6599198]","[10.598132, 11.014049, -0.20715243]","[7.919308, 4.1476874, 10.573866]"
21,Elena Georgieva;Pablo Ripollés;Brian McFee,The Changing Sound of Music: An Exploratory Corpus Study of Vocal Trends Over Time,2024,https://doi.org/10.5281/zenodo.14877317,Elena Georgieva+New York University>USA>education;Pablo Ripollés+New York University>USA>education;Brian McFee+New York University>USA>education,"Recent advancements in audio processing provide a new opportunity to study musical trends using quantitative methods. While past work has investigated trends in music over time, there has been no large-scale study on the evolution of vocal lines. In this work, we conduct an exploratory study of 145,912 vocal tracks of popular songs spanning 55 years, from 1955 to 2010. We use source separation to extract the vocal stem and fundamental frequency (f0) estimation to analyze pitch tracks. Additionally, we extract pitch characteristics including mean pitch, total variation, and pitch class entropy of each song. We conduct statistical analysis of vocal pitch across years and genres, and report significant trends in our metrics over time, as well as significant differences in trends between genres. Our study demonstrates the utility of this method for studying vocals, contributes to the understanding of vocal trends, and showcases the potential of quantitative approaches in musicology.",USA,education,Developed economies,"[-11.532296, -31.75274]","[9.572426, -20.06619]","[13.094112, 16.799244, -14.426088]","[12.209438, -6.6503105, -18.563763]","[10.394348, 11.1279]","[7.747062, 3.2109194]","[11.674498, 15.256473, 0.4529244]","[9.688438, 7.7138367, 10.679516]"
22,Pedro Ramoneda;Martín Rocamora;Taketo Akama,Music Proofreading With RefinPaint: Where and How to Modify Compositions Given Context,2024,https://doi.org/10.5281/zenodo.14877319,Pedro Ramoneda+Universitat Pompeu Fabra>ESP>education;Martin Rocamora+Universitat Pompeu Fabra>ESP>education;Taketo Akama+Sony Computer Science Laboratories>JPN>company,"Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dualfocus methodology not only facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading. Experimental results suggest RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.",ESP,education,Developed economies,"[3.4692636, -51.839]","[-13.765414, -43.58696]","[33.154392, 0.14023992, 1.2636743]","[-23.018415, -2.7253485, -13.284044]","[10.996066, 8.663838]","[9.048265, 6.365335]","[13.268161, 12.909092, -0.12161122]","[9.754217, 5.522693, 8.78779]"
23,Yigitcan Özer;Hans-Ulrich Berendes;Vlora Arifi-Müller;Fabian-Robert Stöter;Meinard Müller,Notewise Evaluation for Music Source Separation: A Case Study for Separated Piano Tracks,2024,https://doi.org/10.5281/zenodo.14877321,"Yigitcan Özer+International Audio Laboratories Erlangen>DEU>facility;Hans-Ulrich Berendes+International Audio Laboratories Erlangen>DEU>facility;Vlora Ariﬁ-Müller+International Audio Laboratories Erlangen>DEU>facility;Fabian-Robert Stöter+AudioShake, Inc.>USA>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility","Deep learning has significantly advanced music source separation (MSS), aiming to decompose music recordings into individual tracks corresponding to singing or specific instruments. Typically, results are evaluated using quantitative measures like signal-to-distortion ratio (SDR) computed for entire excerpts or songs. As the main contribution of this article, we introduce a novel evaluation approach that decomposes an audio track into musically meaningful sound events and applies the evaluation metric based on these units. In a case study, we apply this strategy to the challenging task of separating piano concerto recordings into piano and orchestra tracks. To assess piano separation quality, we use a score-informed nonnegative matrix factorization approach to decompose the reference and separate piano tracks into notewise sound events. In our experiments assessing various MSS systems, we demonstrate that our notewise evaluation, which takes into account factors such as pitch range and musical complexity, enhances the comprehension of both the results of source separation and the intricacies within the underlying music.",DEU,facility,Developed economies,"[4.7786617, -45.496906]","[-37.390827, -31.793297]","[29.895561, 0.45443994, -0.057244953]","[-8.850521, -10.873306, -25.718445]","[8.367914, 10.110563]","[6.5728965, 5.470455]","[10.940854, 13.572272, 1.676187]","[9.591452, 8.548505, 9.342465]"
24,Jyoti Narang;Nazif Can Tamer;Viviana De La Vega;Xavier Serra,Automatic Estimation of Singing Voice Musical Dynamics,2024,https://doi.org/10.5281/zenodo.14877323,"Jyoti Narang+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Escuela Superior de Música de Cataluña (ESMUC)>ESP>education;Nazif Can Tamer+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Escuela Superior de Música de Cataluña (ESMUC)>ESP>education;Viviana de la Vega+Escuela Superior de Música de Cataluña (ESMUC)>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Musical dynamics form a core part of expressive singing voice performances. However, automatic analysis of musical dynamics for singing voice has received limited attention partly due to the scarcity of suitable datasets and a lack of clear evaluation frameworks. To address this challenge, we propose a methodology for dataset curation. Employing the proposed methodology, we compile a dataset comprising 509 musical dynamics annotated singing voice performances, aligned with 163 score files, leveraging state-of-the-art source separation and alignment techniques. The scores are sourced from the OpenScore Lieder corpus of romantic-era compositions, widely known for its wealth of expressive annotations. Utilizing the curated dataset, we train a multi-head attention based CNN model with varying window sizes to evaluate the effectiveness of estimating musical dynamics. We explored two distinct perceptually motivated input representations for the model training: log-Mel spectrum and bark-scale based features. For testing, we manually curate another dataset of 25 musical dynamics annotated performances in collaboration with a professional vocalist. We conclude through our experiments that bark-scale based features outperform log-Mel-features for the task of singing voice dynamics prediction. The dataset along with the code is shared publicly for further research on the topic.",ESP,education,Developed economies,"[-6.7274647, -32.072903]","[-28.416714, -35.423985]","[16.09601, 9.41228, -12.900401]","[-6.8065066, -5.8893576, -20.688423]","[10.074747, 10.872609]","[8.076679, 5.351999]","[11.271641, 14.974125, 0.5177549]","[10.006081, 6.9927297, 8.818324]"
25,Or Tal;Alon Ziv;Itai Gat;Felix Kreuk;Yossi Adi,Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation,2024,https://doi.org/10.5281/zenodo.14877325,Or Tal+The Hebrew University of Jerusalem>ISR>education|Meta>USA>company;Alon Ziv+The Hebrew University of Jerusalem>ISR>education|Meta>USA>company;Itai Gat+The Hebrew University of Jerusalem>ISR>education;Felix Kreuk+Meta>USA>company;Yossi Adi+The Hebrew University of Jerusalem>ISR>education|Meta>USA>company,"We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method that allows for both locally (e.g., chords) and globally (text description) controlled music generation. Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This allows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence using objective metrics and human studies. Results suggest that JA S C O is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/JASCO",ISR,education,Developing economies,"[21.631624, 7.7838445]","[-15.90209, -43.070545]","[2.5856571, -0.22649309, 23.523499]","[-21.314497, -6.7295647, -13.884075]","[10.299036, 8.400322]","[8.853154, 5.9987106]","[13.426675, 11.78535, 0.16076688]","[9.996398, 5.652123, 8.976894]"
26,Javier Nistal;Marco Pasini;Cyran Aouameur;Maarten Grachten;Stefan Lattner,Diff-a-Riff: Musical Accompaniment Co-Creation via Latent Diffusion Models,2024,https://doi.org/10.5281/zenodo.14877327,Javier Nistal+Sony Computer Science Laboratories>FRA>facility|Queen Mary University of London>GBR>education;Marco Pasini+Queen Mary University of London>GBR>education;Cyran Aouameur+Sony Computer Science Laboratories>FRA>facility;Maarten Grachten+Sony Computer Science Laboratories>FRA>facility;Stefan Lattner+Sony Computer Science Laboratories>FRA>facility,"Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce Diff-A-Riff, a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website.",FRA,facility,Developed economies,"[17.154572, 0.80358243]","[-15.024875, -45.072193]","[6.1355577, 10.46726, 22.285645]","[-19.48368, -2.875012, -15.935178]","[10.430656, 8.763675]","[8.596801, 6.4296274]","[13.505307, 12.143424, -0.03960739]","[9.849776, 5.6777306, 8.537352]"
27,Ngan V.T. Nguyen;Elizabeth Acosta;Tommy Dang;David Sears,Exploring Internet Radio Across the Globe With the MIRAGE Online Dashboard,2024,https://doi.org/10.5281/zenodo.14877329,"Ngan V.T. Nguyen+University of Science, VNU-HCMUS>VNM>education;Elizabeth A.M. Acosta+Texas Tech University>USA>education;Tommy Dang+Texas Tech University>USA>education;David R.W. Sears+Texas Tech University>USA>education","This study presents the Music Informatics for Radio Across the GlobE (MIRAGE) online dashboard, which allows users to access, interact with, and export metadata (e.g., artist name, track title) and musicological features (e.g., instrument list, voice type, key/mode) for 1 million events streaming on 10,000 internet radio stations across the globe. Users can search for stations or events according to several criteria, display, analyze, and listen to the selected station/event lists using interactive visualizations that include embedded links to streaming services, and finally export relevant metadata and visualizations for further study.",VNM,education,Developing economies,"[-34.33018, 33.75002]","[26.558, 24.996635]","[-13.977345, 20.398811, -17.506723]","[10.50647, -7.835618, 24.51003]","[14.8991, 7.5775876]","[11.198079, 1.4307456]","[14.983549, 14.395751, -2.0405989]","[12.230232, 5.391983, 12.93832]"
28,Andrew C. Edwards;Xavier Riley;Pedro Pereira Sarmento;Simon Dixon,MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling,2024,https://doi.org/10.5281/zenodo.14877333,"Drew Edwards+Centre for Digital Music, Queen Mary University of London>GBR>education;Xavier Riley+Centre for Digital Music, Queen Mary University of London>GBR>education;Pedro Sarmento+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","Guitar tablatures enrich the structure of traditional music notation by assigning each note to a string and fret of a guitar in a particular tuning, indicating precisely where to play the note on the instrument. The problem of generating tablature from a symbolic music representation involves inferring this string and fret assignment per note across an entire composition or performance. On the guitar, multiple string-fret assignments are possible for most pitches, which leads to a large combinatorial space that prevents exhaustive search approaches. Most modern methods use constraint-based dynamic programming to minimize some cost function (e.g. hand position movement). In this work, we introduce a novel deep learning solution to symbolic guitar tablature estimation. We train an encoder-decoder Transformer model in a masked language modeling paradigm to assign notes to strings. The model is first pre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on a curated set of professionally transcribed guitar performances. Given the subjective nature of assessing tablature quality, we conduct a user study amongst guitarists, wherein we ask participants to rate the playability of multiple versions of tablature for the same four-bar excerpt. The results indicate our system significantly outperforms competing algorithms.",GBR,education,Developed economies,"[45.528603, -9.052098]","[-42.804985, -8.791426]","[23.54841, -11.329353, 6.976173]","[-18.01559, -7.629743, -5.24137]","[7.7145123, 8.244566]","[7.4154425, 4.459219]","[11.606347, 11.21399, 1.3400126]","[9.05874, 6.441097, 9.812252]"
29,Jaehun Kim;Florian Henkel;Camilo Landau;Samuel E. Sandberg;Andreas F. Ehmann,Transcription-Based Lyrics Embeddings: Simple Extraction of Effective Lyrics Embeddings From Audio,2024,https://doi.org/10.5281/zenodo.14877331,Jaehun Kim+SiriusXM Pandora>USA>company;Florian Henkel+SiriusXM Pandora>USA>company;Camilo Landau+SiriusXM Pandora>USA>company;Samuel E. Sandberg+SiriusXM Pandora>USA>company;Andreas F. Ehmann+SiriusXM Pandora>USA>company,"The majority of Western popular music contains lyrics. Previous studies have shown that lyrics are a rich source of information and are complementary to other information sources, such as audio. One factor that hinders the research and application of lyrics on a large scale is their availability. To mitigate this, we propose the use of transcriptionbased lyrics embeddings (TLE). These estimate 'groundtruth' lyrics embeddings given only audio as input. Central to this approach is the use of transcripts derived from an automatic lyrics transcription (ALT) system instead of human-transcribed, 'ground-truth' lyrics, making them substantially more accessible. We conduct an experiment to assess the effectiveness of TLEs across various music information retrieval (MIR) tasks. Our results indicate that TLEs can improve the performance of audio embeddings alone, especially when combined, closing the gap with cases where ground-truth lyrics information is available.",USA,company,Developed economies,"[-26.639383, -32.35527]","[33.481987, -15.595507]","[12.293478, 22.101215, -2.8378968]","[4.3590674, 11.933753, 4.892822]","[11.271233, 11.805992]","[10.157067, 3.0687928]","[12.198943, 15.812606, 1.1735688]","[11.757729, 6.3320575, 11.189621]"
30,Hyon Kim;Xavier Serra,A Method for MIDI Velocity Estimation for Piano Performance by a U-Net With Attention and FiLM,2024,https://doi.org/10.5281/zenodo.14877335,Hyon Kim+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown;Xavier Serra+Universitat Pompeu Fabra>ESP>education|Unknown>Unknown>Unknown,"It is a well known fact that the dynamics in piano performance gives significant effect in expressiveness. Taking the polyphonic nature of the instrument into account, analysing information to form dynamics for each performed note has significant meaning to understand piano performance in a quantitative way. It is also a key element in an education context for piano learners. In this study, we developed a model for estimating MIDI velocity for each note, as one of indicators to represent loudness, with a condition of score assuming educational use case, by a Deep Neural Network (DNN) utilizing a U-Net with Scaled Dot-Product Attention (Attention) and Feature-wise Linear Modulation (FiLM) conditioning. As a result, we prove that effectiveness of Attention and FiLM conditioning, improved estimation accuracy and achieved the best result among previous researches using DNNs and showed its robustness across the various domain of test data.",ESP,education,Developed economies,"[36.262535, -3.0521128]","[-29.09604, -23.04228]","[12.2072315, -9.580476, 20.543478]","[-9.1529045, -13.288333, -11.545197]","[10.17245, 7.0322685]","[7.9454255, 5.652926]","[12.213491, 11.548813, -0.5644469]","[8.994668, 6.4378357, 8.903166]"
31,Yun-Han Lan;Wen-Yi Hsiao;Hao-Chung Cheng;Yi-Hsuan Yang,MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation,2024,https://doi.org/10.5281/zenodo.14877337,Yun-Han Lan+Taiwan AI Labs>TWN>company|National Taiwan University>TWN>education;Wen-Yi Hsiao+Taiwan AI Labs>TWN>company|National Taiwan University>TWN>education;Hao-Chung Cheng+National Taiwan University>TWN>education;Yi-Hsuan Yang+Taiwan AI Labs>TWN>company|National Taiwan University>TWN>education,"Existing text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporallyconditioned Transformer-based text-to-music model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automaticallyextracted rhythm and chords as the condition signal. During inference, the condition can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets—one derived from extracted features and the other from user-created inputs—demonstrates that MusiConGen can generate realistic backing track music that aligns well with the specified conditions. We open-source the code and model checkpoints, and provide audio examples online, https://musicongen.github.io/musicongen_demo/.",TWN,company,Developing economies,"[21.690397, 8.316964]","[-16.412134, -43.62959]","[1.3011334, -1.2281609, 23.652565]","[-21.141851, -5.0957355, -14.3170805]","[10.359614, 8.317808]","[8.652455, 6.129036]","[13.439385, 11.684548, 0.100269765]","[9.933826, 5.727712, 8.832668]"
62,Julien PM Guinot;Elio Quinton;George Fazekas,Semi-Supervised Contrastive Learning of Musical Representations,2024,https://doi.org/10.5281/zenodo.14877401,"Julien Guinot+Centre for Digital Music, Queen Mary University of London>GBR>education;Elio Quinton+Music & Audio Machine Learning Lab, Universal Music Group>GBR>company;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education","Despite the success of contrastive learning in Music Information Retrieval, the inherent ambiguity of contrastive self-supervision presents a challenge. Relying solely on augmentation chains and self-supervised positive sampling strategies can lead to a pretraining objective that does not capture key musical information for downstream tasks. We introduce semi-supervised contrastive learning (SemiSupCon), a simple method for leveraging musically informed labeled data (supervision signals) in the contrastive learning of musical representations. Our approach introduces musically relevant supervision signals into self-supervised contrastive learning by combining supervised and self-supervised contrastive objectives in a simpler framework than previous approaches. This framework improves downstream performance and robustness to audio corruptions on a range of downstream MIR tasks with moderate amounts of labeled data. Our approach enables shaping the learned similarity metric through the choice of labeled data that (1) infuses the representations with musical domain knowledge and (2) improves out-of-domain performance with minimal general downstream performance loss. We show strong transfer learning performance on musically related yet not trivially similar tasks - such as pitch and key estimation. Additionally, our approach shows performance improvement on automatic tagging over self-supervised approaches with only 5% of available labels included in pretraining.",GBR,education,Developed economies,"[-12.556917, -5.668874]","[-19.942286, -35.467667]","[4.3477407, 12.992407, 13.501164]","[-9.084817, 0.094458364, -16.3117]","[11.368961, 8.844469]","[9.376059, 4.878497]","[13.551585, 12.979258, 0.24896821]","[10.461814, 6.382216, 8.970422]"
63,Léo Géré;Nicolas Audebert;Philippe Rigaux,Improved Symbolic Drum Style Classification With Grammar-Based Hierarchical Representations,2024,https://doi.org/10.5281/zenodo.14877403,Léo Géré+Conservatoire national des arts et métiers>FRA>education;Nicolas Audebert+Conservatoire national des arts et métiers>FRA>education|Univ. Gustave Eiffel>FRA>education;Philippe Rigaux+Conservatoire national des arts et métiers>FRA>education,"Deep learning models have become a critical tool for analysis and classification of musical data. These models operate either on the audio signal, e.g. waveform or spectrogram, or on a symbolic representation, such as MIDI. In the latter, musical information is often reduced to basic features, i.e. durations, pitches and velocities. Most existing works then rely on generic tokenization strategies from classical natural language processing, or matrix representations, e.g. piano roll. In this work, we evaluate how enriched representations of symbolic data can impact deep models, i.e. Transformers and RNN, for music style classification. In particular, we examine representations that explicitly incorporate musical information implicitly present in MIDI-like encodings, such as rhythmic organization, and show that they outperform generic tokenization strategies. We introduce a new tree-based representation of MIDI data built upon a context-free musical grammar. We show that this grammar representation accurately encodes high-level rhythmic information and outperforms existing encodings on the GrooveMIDI Dataset for drumming style classification, while being more compact and parameter-efficient.",FRA,education,Developed economies,"[25.03128, -48.482628]","[-14.587048, -33.755154]","[19.970482, -23.108192, -1.1852331]","[-13.600628, -3.257701, -9.61272]","[7.7648187, 7.0156994]","[8.64758, 5.758538]","[10.2224245, 11.652607, 0.9639231]","[9.349869, 6.012407, 9.1975]"
64,Jiwoo Ryu;Hao-Wen Dong;Jongmin Jung;Dasaem Jeong,Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation,2024,https://doi.org/10.5281/zenodo.14877405,Jiwoo Ryu+Sogang University>KOR>education;Hao-Wen Dong+University of California San Diego>USA>education;Jongmin Jung+Sogang University>KOR>education;Dasaem Jeong+Sogang University>KOR>education,"Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",KOR,education,Developing economies,"[20.790352, 9.834335]","[-14.028039, -35.7609]","[-0.48949122, -2.2345536, 20.741224]","[-15.618975, -2.6017752, -8.501319]","[10.366469, 8.152353]","[8.595495, 5.810408]","[13.294414, 11.666126, -0.119352125]","[9.343724, 5.967407, 9.220026]"
65,Pedro González-Barrachina;María Alfaro-Contreras;Jorge Calvo-Zaragoza,Continual Learning for Music Classification,2024,https://doi.org/10.5281/zenodo.14877407,Pedro González-Barrachina+University of Alicante>ESP>education|Alice Biometrics>ESP>company;María Alfaro-Contreras+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"Music classification is a prominent research area within Music Information Retrieval. While Deep Learning methods can adequately perform this task, their classification space remains fixed once trained, which conflicts with the dynamic nature of the ever-evolving music landscape. This work explores, for the first time, the application of Continual Learning (CL) in the context of music classification. Specifically, we thoroughly evaluate five state-of-the-art CL approaches across four different music classification tasks. Additionally, we showcase that a foundation model might be the key to CL in music classification. To that end, we study a new approach called Pre-trained Class Centers, which leverages pre-trained features to create fixed class-center spaces. Our results reveal that existing CL methods struggle when applied to music classification tasks, whereas this simple method consistently out-performs them. This highlights the need for CL methods tailored specifically for music classification.",ESP,education,Developed economies,"[-22.393843, -10.465718]","[6.8208165, -36.068966]","[-13.27228, -0.9287619, 10.4741335]","[19.291943, 12.954577, -6.936025]","[12.4873905, 10.345228]","[9.804225, 4.5708423]","[13.644892, 13.797524, 0.8075284]","[10.99071, 6.5286837, 9.15515]"
97,Muhammad Taimoor Haseeb;Ahmad Hammoudeh;Gus Xia,Deep Recombinant Transformer: Enhancing Loop Compatibility in Digital Music Production,2024,https://doi.org/10.5281/zenodo.14877473,Muhammad Taimoor Haseeb+MBZUAI>ARE>education|MBZUAI>ARE>education|MBZUAI>ARE>education;Ahmad Hammoudeh+MBZUAI>ARE>education|MBZUAI>ARE>education|MBZUAI>ARE>education;Gus Xia+MBZUAI>ARE>education|MBZUAI>ARE>education|MBZUAI>ARE>education,"The widespread availability of music loops has revolutionized music production. However, combining loops requires a nuanced understanding of musical compatibility that can be difficult to learn and time-consuming. This study concentrates on the 'vertical problem' of music loop compatibility, which pertains to layering different loops to create a harmonious blend. The main limitation to applying deep learning in this domain is the absence of a large, high-quality, labeled dataset containing both positive and negative pairs. To address this, we synthesize high-quality audio from multi-track MIDI datasets containing independent instrument stems, and then extract loops to serve as positive pairs. This provides models with instrument-level information when learning compatibility. Moreover, we improve the generation of negative examples by matching the key and tempo of candidate loops, and then employing AutoMashUpper [1] to identify incompatible loops. Creating a large dataset allows us to introduce and examine the application of Transformer architectures for addressing vertical loop compatibility. Experimental results show that our method outperforms the previous state-of-the-art, achieving an 18.6% higher accuracy across multiple genres. Subjective assessments rate our model higher in seamlessly and creatively combining loops, underscoring our method's effectiveness. We name our approach the Deep Recombinant Transformer and provide audio samples.",ARE,education,Developing economies,"[40.244152, 2.8447301]","[-33.249596, -22.453447]","[8.38489, -7.0990667, 28.512144]","[-17.0708, 1.2057568, -22.81569]","[10.153185, 7.568552]","[7.907824, 6.07841]","[12.722667, 11.356637, -0.19077262]","[9.573324, 6.847448, 8.971766]"
98,Yannis Vasilakis;Rachel Bittner;Johan Pauwels,I Can Listen but Cannot Read: An Evaluation of Two-Tower Multimodal Systems for Instrument Recognition,2024,https://doi.org/10.5281/zenodo.14877475,Yannis Vasilakis+Queen Mary University of London>GBR>education|Spotify>USA>company;Rachel Bittner+Spotify>USA>company;Johan Pauwels+Queen Mary University of London>GBR>education,"Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition and a detailed analysis of the properties of the pre-joint and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an instrument ontology is proposed. This method reveals deficiencies in the systems' understanding of instruments and provides evidence of the need for fine-tuning text encoders on musical data.",GBR,education,Developed economies,"[11.370573, -25.611803]","[38.377552, -8.4634285]","[17.39159, -8.932334, 1.8075955]","[21.944729, 15.905601, -1.9850415]","[8.841576, 7.0703797]","[9.933825, 4.775877]","[11.028866, 12.491079, 0.30766872]","[11.173961, 6.229231, 9.465939]"
99,Weixing Wei;Jiahao Zhao;Yulun Wu;Kazuyoshi Yoshii,Streaming Piano Transcription Based on Consistent Onset and Offset Decoding With Sustain Pedal Detection,2024,https://doi.org/10.5281/zenodo.14877477,Weixing Wei+Kyoto University>JPN>education;Jiahao Zhao+Fudan University>CHN>education;Yulun Wu+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"This paper describes a streaming audio-to-MIDI transcription method that can sequentially translate a piano recording into a sequence of note-on and note-off events. The sequence-to-sequence learning nature of this task may call for using a Transformer model, which has been used for offline transcription and could be extended for streaming transcription with a causal restriction of the attention mechanism. We assume that the decoder of this model suffers from the performance limitation. Although time-frequency features useful for onset detection are considerably different from those for offset detection, the single decoder is trained to output a mixed sequence of onset and offset events without guarantee of the correspondence between the onset and offset events of the same note. To overcome this limitation, we propose a streaming encoder-decoder model that uses a convolutional encoder aggregating local acoustic features, followed by an autoregressive transformer decoder detecting a variable number of onset events and another decoder detecting the offset events of the active pitches with validation of the sustain pedal at each time frame. Experiments using the MAESTRO dataset showed that the proposed streaming method performed comparably with or even better than the state-of-the-art offline methods while significantly reducing the computational cost.",JPN,education,Developed economies,"[32.375156, -5.0222917]","[-25.53041, -21.821997]","[15.549848, -7.1746883, 18.070728]","[-8.051478, -5.7002077, -10.11788]","[9.72717, 7.239716]","[7.9073534, 5.415072]","[12.019908, 11.37818, -0.23141277]","[8.895818, 6.585261, 9.2613735]"
100,Juan Carlos Martinez-Sevilla;David Rizo;Jorge Calvo-Zaragoza,Towards Universal Optical Music Recognition: A Case Study on Notation Types,2024,https://doi.org/10.5281/zenodo.14877479,Juan C. Martinez-Sevilla+University of Alicante>ESP>education;David Rizo+University of Alicante>ESP>education|Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana>ESP>education;Jorge Calvo-Zaragoza+University of Alicante>ESP>education,"Recent advances in Deep Learning have propelled the development of fields such as Optical Music Recognition (OMR), which is responsible for extracting the content from music score images. Despite progress in the field, existing literature scarcely addresses core issues like performance in real-world scenarios, user experience, maintainability of multiple pipelines, reusability of architectures and data, among others. These factors result in high costs for both users and developers of such systems. Furthermore, research has often been conducted under certain constraints, such as using a single musical texture or type of notation, which may not align with the end-user requirements of OMR systems. For the first time, our study involves a comprehensive and extensive experimental setup to explore new ideas towards the development of a universal OMR system—capable of transcribing all textures and notation types. Our investigation provides valuable insights into several aspects, such as the ability of a model to leverage knowledge from different domains despite significant differences in music notation types.",ESP,education,Developed economies,"[41.499325, 19.942287]","[-20.715178, -24.51244]","[22.243761, 14.832234, 12.227098]","[-15.39415, -18.761084, -3.383407]","[8.66617, 6.035459]","[6.5109153, -1.0637373]","[10.628547, 11.040997, -0.21772465]","[8.111735, 4.235613, 9.95957]"
101,Mathias Rose Bjare;Stefan Lattner;Gerhard Widmer,Controlling Surprisal in Music Generation via Information Content Curve Matching,2024,https://doi.org/10.5281/zenodo.14877481,"Mathias Bjare+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Stefan Lattner+Sony Computer Science Laboratories (CSL)>FRA>company;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","In recent years, the quality and public interest in music generation systems have grown, encouraging research into various ways to control these systems. We propose a novel method for controlling surprisal in music generation using sequence models. To achieve this goal, we define a metric called Instantaneous Information Content (IIC). The IIC serves as a proxy function for the perceived musical surprisal (as estimated from a probabilistic model) and can be calculated at any point within a music piece. This enables the comparison of surprisal across different musical content even if the musical events occur in irregular time intervals. We use beam search to generate musical material whose IIC curve closely approximates a given target IIC. We experimentally show that the IIC correlates with harmonic and rhythmic complexity and note density. The correlation decreases with the length of the musical context used for estimating the IIC. Finally, we conduct a qualitative user study to test if human listeners can identify the IIC curves that have been used as targets when generating the respective musical material. We provide code for creating IIC interpolations and IIC visualizations on https://github.com/muthissar/iic.",AUT,education,Developed economies,"[23.943628, 7.708015]","[-31.773952, 3.5249064]","[-0.6896395, 1.0307289, 26.214104]","[-14.722314, 6.9375544, -6.09129]","[10.549744, 8.430822]","[7.5299387, 3.9617925]","[13.566358, 11.846641, -0.011517542]","[9.146259, 5.967269, 10.269449]"
102,Guang Yang;Muru Zhang;Lin Qiu;Yanming Wan;Noah A. Smith,Toward a More Complete OMR Solution,2024,https://doi.org/10.5281/zenodo.14877483,Guang Yang+University of Washington>USA>education;Muru Zhang+University of Washington>USA>education;Lin Qiu+University of Washington>USA>education;Yanming Wan+University of Washington>USA>education;Noah A. Smith+University of Washington>USA>education|Allen Institute for Artificial Intelligence>USA>facility,"Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the system first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we focus on the MUSCIMA++ v2.0 dataset, which represents musical notation as a graph with pairwise relationships among detected music objects, and we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stages in a more holistic way. These findings, together with our novel evaluation metric, are important steps toward a more complete OMR solution.",USA,education,Developed economies,"[32.971706, 32.0471]","[-18.911158, -24.107038]","[4.8484015, -22.452991, 23.753212]","[-15.2407055, -21.863653, -4.9827147]","[9.710931, 6.1699247]","[6.521967, -1.068576]","[12.687452, 11.248312, -1.5809231]","[8.066688, 4.1775494, 9.964743]"
103,Ilaria Manco;Justin Salamon;Oriol Nieto,"Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning",2024,https://doi.org/10.5281/zenodo.14877485,Ilaria Manco+Queen Mary University of London>GBR>education|Adobe Research>USA>company;Justin Salamon+Adobe Research>USA>company;Oriol Nieto+Adobe Research>USA>company,"Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.",GBR,education,Developed economies,"[-22.991404, -2.2117515]","[-20.331823, -39.283398]","[4.098527, 23.530119, 11.740382]","[-9.972925, -1.032518, -21.765778]","[11.0303135, 8.097445]","[9.51861, 5.1295013]","[13.577995, 12.767176, 0.009474039]","[10.595898, 6.3295307, 8.791333]"
104,Seungheon Doh;Keunwoo Choi;Daeyong Kwon;Taesoo Kim;Juhan Nam,Music Discovery Dialogue Generation Using Human Intent Analysis and Large Language Models,2024,https://doi.org/10.5281/zenodo.14877487,"Seung Heon Doh+Graduate School of Culture Technology, KAIST>KOR>education;Keunwoo Choi+Prescient Design, Genentech>USA>company;Daeyong Kwon+Graduate School of Culture Technology, KAIST>KOR>education;Taesu Kim+KAIST>KOR>education;Juhan Nam+Graduate School of Culture Technology, KAIST>KOR>education","A conversational music retrieval system can help users discover music that matches their preferences through dialogue. To achieve this, a conversational music retrieval system should seamlessly engage in multi-turn conversation by 1) understanding user queries and 2) responding with natural language and retrieved music. A straightforward solution would be a data-driven approach utilizing such conversation logs. However, few datasets are available for the research and are limited in terms of volume and quality. In this paper, we present a data generation framework for rich music discovery dialogue using a large language model (LLM) and user intents, system actions, and musical attributes. This is done by i) dialogue intent analysis using grounded theory, ii) generating attribute sequences via cascading database filtering, and iii) generating utterances using large language models. By applying this framework to the Million Song dataset, we create – LP-MusicDialog, a Large Language Model based Pseudo Music Dialogue dataset, containing over 288k music conversations using more than 319k music items. Our evaluation shows that the synthetic dataset is competitive with an existing, small human dialogue dataset in terms of dialogue consistency, item relevance, and naturalness. Furthermore, using the dataset, we train a conversational music retrieval model and show promising results.",KOR,education,Developing economies,"[-15.197535, 15.214431]","[5.886076, 23.70355]","[0.72271645, 17.469662, -12.496283]","[-14.531593, 15.551532, 6.2265983]","[13.143703, 8.417622]","[9.681768, 5.0828347]","[13.695816, 14.513303, -1.3972638]","[10.730448, 6.011315, 9.311824]"
105,Yuexuan Kong;Vincent Lostanlen;Gabriel Meseguer-Brocal;Stella Wong;Mathieu Lagrange;Romain Hennequin,STONE: Self-Supervised Tonality Estimator,2024,https://doi.org/10.5281/zenodo.14877489,Yuexuan Kong+Deezer Research>FRA>company|Nantes Université>FRA>education|École Centrale Nantes>FRA>education|CNRS>FRA>facility|LS2N>FRA>facility;Vincent Lostanlen+Deezer Research>FRA>company|Nantes Université>FRA>education|École Centrale Nantes>FRA>education|CNRS>FRA>facility|LS2N>FRA>facility;Gabriel Meseguer-Brocal+Deezer Research>FRA>company|Nantes Université>FRA>education|École Centrale Nantes>FRA>education|CNRS>FRA>facility|LS2N>FRA>facility;Stella Wong+Unknown>Unknown>Unknown;Mathieu Lagrange+Deezer Research>FRA>company|Nantes Université>FRA>education|École Centrale Nantes>FRA>education|CNRS>FRA>facility|LS2N>FRA>facility;Romain Hennequin+Deezer Research>FRA>company|Nantes Université>FRA>education|École Centrale Nantes>FRA>education|CNRS>FRA>facility|LS2N>FRA>facility,"Although deep neural networks can estimate the key of a musical piece, their supervision incurs a massive annotation effort. Against this shortcoming, we present STONE, the first self-supervised tonality estimator. The architecture behind STONE, named ChromaNet, is a convnet with octave equivalence which outputs a ""key signature profile"" (KSP) of 12 structured logits. First, we train ChromaNet to regress artificial pitch transpositions between any two unlabeled musical excerpts from the same audio track, as measured as cross-power spectral density (CPSD) within the circle of fifths (CoF). We observe that this self-supervised pretext task leads KSP to correlate with tonal key signature. Based on this observation, we extend STONE to output a structured KSP of 24 logits, and introduce supervision so as to disambiguate major versus minor keys sharing the same key signature. Applying different amounts of supervision yields semi-supervised and fully supervised tonality estimators: i.e., Semi-TONEs and Sup-TONEs. We evaluate these estimators on FMAK, a new dataset of 5489 real-world musical recordings with expert annotation of 24 major and minor keys. We find that Semi-TONE matches the classification accuracy of Sup-TONE with reduced supervision and outperforms it with equal supervision.",FRA,company,Developed economies,"[-1.2050222, -18.974386]","[-27.26576, -30.622248]","[1.2485552, -10.677895, 25.916231]","[-7.1054325, -5.6266756, -16.72244]","[11.084521, 9.291504]","[8.017856, 5.1930513]","[12.13119, 13.707458, -0.25075182]","[9.538011, 6.8720517, 8.854175]"
106,Francesco Foscarin;Jan Schlüter;Gerhard Widmer,Beat This! Accurate Beat Tracking Without DBN Postprocessing,2024,https://doi.org/10.5281/zenodo.14877491,"Francesco Foscarin+Johannes Kepler University>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Jan Schlüter+Johannes Kepler University>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","We propose a system for tracking beats and downbeats with two objectives: generality across a diverse music range, and high accuracy. We achieve generality by training on multiple datasets – including solo instrument recordings, pieces with time signature changes, and classical music with high tempo variations – and by removing the commonly used Dynamic Bayesian Network (DBN) postprocessing, which introduces constraints on the meter and tempo. For high accuracy, among other improvements, we develop a loss function tolerant to small time shifts of annotations, and an architecture alternating convolutions with transformers either over frequency or time. Our system surpasses the current state of the art in F1 score despite using no DBN. However, it can still fail, especially for difficult and underrepresented genres, and performs worse on continuity metrics, so we publish our model, code, and preprocessed datasets, and invite others to beat this.",AUT,education,Developed economies,"[36.94407, -33.943985]","[-28.595692, -11.253182]","[7.0925374, -30.110092, -4.5124044]","[-7.6693525, 16.093601, -12.93315]","[10.5042305, 4.1511154]","[5.159091, 2.3527193]","[10.1006565, 12.834084, -2.2928421]","[7.7649503, 6.883286, 10.441076]"
107,Yujia Yan;Zhiyao Duan,Scoring Time Intervals Using Non-Hierarchical Transformer for Automatic Piano Transcription,2024,https://doi.org/10.5281/zenodo.14877493,Yujia Yan+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"The neural semi-Markov Conditional Random Field (semi-CRF) framework has demonstrated promise for event-based piano transcription. In this framework, all events (notes or pedals) are represented as closed time intervals tied to specific event types. The neural semi-CRF approach requires an interval scoring matrix that assigns a score for every candidate interval. However, designing an efficient and expressive architecture for scoring intervals is not trivial. This paper introduces a simple method for scoring intervals using scaled inner product operations that resemble how attention scoring is done in transformers. We show theoretically that, due to the special structure from encoding the non-overlapping intervals, under a mild condition, the inner product operations are expressive enough to represent an ideal scoring matrix that can yield the correct transcription result. We then demonstrate that an encoder-only non-hierarchical transformer backbone, operating only on a low-time-resolution feature map, is capable of transcribing piano notes and pedals with high accuracy and time precision. The experiment shows that our approach achieves the new state-of-the-art performance across all subtasks in terms of the F1 measure on the Maestro dataset. See appendix for post-camera-ready updates.",USA,education,Developed economies,"[33.430573, -6.170747]","[-26.461899, -22.555666]","[17.411036, -7.3105745, 15.122185]","[-7.430487, -6.9229574, -8.530612]","[9.679755, 7.27511]","[7.861329, 5.481614]","[12.060272, 11.38371, -0.24936657]","[8.865369, 6.451244, 9.231926]"
108,Julian Lenz;Anirudh Mani,PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and Variations,2024,https://doi.org/10.5281/zenodo.14877495,Julian Lenz+Lemonaide Research>ESP>company;Anirudh Mani+Lemonaide Research>USA>company,"We introduce Cadenza, a new multi-stage generative framework for predicting expressive variations of symbolic musical ideas as well as unconditional generations. To accomplish this we propose a novel MIDI encoding method, PerTok (Performance Tokenizer) that captures minute expressive details whilst reducing sequence length up to 59% and vocabulary size up to 95% for polyphonic, monophonic and rhythmic tasks. The proposed framework comprises of two sequential stages: 1) Composer and 2) Performer. The Composer model is a transformer-based Variational Autoencoder (VAE), with Rotary Positional Embeddings (RoPE) [1] and an autoregressive decoder modified to more effectively integrate the latent codes of the input musical idea. The Performer model is a bidirectional transformer encoder that is separately trained to predict velocities and microtimings on MIDI sequences. Objective and human evaluations demonstrate Cadenza's versatile capability in 1) matching other unconditional state-of-the-art symbolic models in musical quality whilst sounding more expressive, and 2) composing new, expressive ideas that are both stylistically related to the input whilst providing novel ideas to the user. Our framework is designed, researched and implemented with the objective of ethically providing inspiration for musicians.",ESP,company,Developed economies,"[18.200565, 15.7527895]","[-12.904887, -38.853344]","[-5.5897784, -10.220733, 17.831114]","[-16.627487, -0.21711336, -9.914602]","[11.649119, 7.203037]","[8.690736, 6.2525]","[13.520566, 12.189759, -0.8919604]","[9.298053, 5.7753744, 8.843938]"
109,Nathaniel Condit-Schultz,Looking for Tactus in All the Wrong Places: Statistical Inference of Metric Alignment in Rap Flow,2024,https://doi.org/10.5281/zenodo.14877497,Nathaniel Condit-Schultz+Georgia Institute of Technology>USA>education,"Musical rhythm and meter are characterized by simple proportional relationships between event durations within pieces, making comparison of rhythms between different musical pieces a nebulous practice, especially at different tempos. Though the ""main tempo,"" or tactus, of a piece serves as an important cognitive reference point, it is difficult to identify objectively. In this paper, I investigate how statistical regularities in rhythmic patterns can be used to determine how to compare pieces at different tempos, speculating that these regularities could relate to the perception of tactus. Using a Bayesian statistical approach, I model first-order (two-gram) rhythmic event transitions in a symbolic dataset of rap transcriptions (MCFlow), allowing the model to renotate the rhythmic values of each transcription as needed to optimize fit. The resulting model predicts makes ""renotations"" which match a priori predictions from the original dataset's transcriber. I then demonstrate that the model can be used to rhythmically align new data, giving an objective basis for rhythmic annotation decisions.",USA,education,Developed economies,"[23.31644, -14.747725]","[-27.39239, -8.133685]","[-6.8825865, -17.132769, -4.3697205]","[-4.9393187, 17.677599, -8.800735]","[11.222054, 6.089563]","[5.7129126, 2.0467997]","[11.72341, 12.916134, -1.6946563]","[7.9695344, 6.864092, 11.2946615]"
110,Kun Fang;Ziyu Wang;Gus Xia;Ichiro Fujinaga,Exploring GPT's Ability as a Judge in Music Understanding,2024,https://doi.org/10.5281/zenodo.14877499,Kun Fang+McGill University>CAN>education|MBZUAI>ARE>education|NYU Shanghai>USA>education;Ziyu Wang+NYU Shanghai>USA>education|MBZUAI>ARE>education;Gus Xia+MBZUAI>ARE>education|NYU Shanghai>USA>education;Ichiro Fujinaga+McGill University>CAN>education,"Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs' music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT's error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.",CAN,education,Developed economies,"[-30.070665, 2.6405594]","[-16.242193, -36.410442]","[-5.9761925, 17.425465, 19.807777]","[-16.158148, -7.2862196, -10.553759]","[12.586453, 8.522831]","[9.119197, 5.448838]","[13.587409, 13.82109, -0.66572535]","[10.087103, 5.932615, 9.438921]"
111,Roser Batlle-Roca;Wei-Hsiang Liao;Xavier Serra;Yuki Mitsufuji;Emilia Gómez,Towards Assessing Data Replication in Music Generation With Music Similarity Metrics on Raw Audio,2024,https://doi.org/10.5281/zenodo.14877501,"Roser Batlle-Roca+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Wei-Hsiang Liao+Sony AI>JPN>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Yuki Mitsufuji+Sony AI>USA>company;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Joint Research Centre, European Commission>ESP>facility","Recent advancements in music generation are raising multiple concerns about the implications of AI in creative music processes, current business models and impacts related to intellectual property management. A relevant discussion and related technical challenge is the potential replication and plagiarism of the training set in AI-generated music, which could lead to misuse of data and intellectual property rights violations. To tackle this issue, we present the Music Replication Assessment (MiRA) tool: a model-independent open evaluation method based on diverse audio music similarity metrics to assess data replication. We evaluate the ability of five metrics to identify exact replication by conducting a controlled replication experiment in different music genres using synthetic samples. Our results show that the proposed methodology can estimate exact data replication with a proportion higher than 10%. By introducing the MiRA tool, we intend to encourage the open evaluation of music-generative models by researchers, developers, and users concerning data replication, highlighting the importance of the ethical, social, legal, and economic consequences. Code and examples are available for reproducibility purposes.",ESP,education,Developed economies,"[-5.676815, 11.844373]","[-2.5905223, 45.76563]","[-8.16032, 13.144961, 0.13493213]","[-16.309021, 11.721559, 2.4651673]","[13.240999, 9.315135]","[9.993761, 5.516674]","[13.728013, 15.164777, -0.6902511]","[10.473754, 5.1868057, 9.631349]"
112,Shahan Nercessian;Johannes Imort;Ninon Devis;Frederik Blang,Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models,2024,https://doi.org/10.5281/zenodo.14877503,Shahan Nercessian+Native Instruments>USA>company;Johannes Imort+Native Instruments>USA>company;Ninon Devis+Native Instruments>USA>company;Frederik Blang+Native Instruments>USA>company,"In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",USA,company,Developed economies,"[24.024803, -1.5516603]","[-17.13134, -42.793316]","[7.6532335, -2.1017342, 16.360147]","[-22.160631, -5.587534, -11.735409]","[9.730136, 8.298976]","[8.597493, 6.3368325]","[12.577924, 11.958487, 0.38451073]","[9.872472, 5.8035126, 8.499736]"
113,Nithya Nadig Shikarpur;Krishna Maneesha Dendukuri;Yusong Wu;Antoine Caillon;Cheng-Zhi Anna Huang,Hierarchical Generative Modeling of Melodic Vocal Contours in Hindustani Classical Music,2024,https://doi.org/10.5281/zenodo.14877505,"Nithya Shikarpur+Mila, Quebec Artificial Intelligence Institute>CAN>facility|Université de Montréal>CAN>education|Canada CIFAR AI Chair>CAN>facility|Google DeepMind>GBR>company;Krishna Maneesha Dendukuri+Mila, Quebec Artificial Intelligence Institute>CAN>facility|Université de Montréal>CAN>education|Canada CIFAR AI Chair>CAN>facility|Google DeepMind>GBR>company;Yusong Wu+Mila, Quebec Artificial Intelligence Institute>CAN>facility|Université de Montréal>CAN>education|Canada CIFAR AI Chair>CAN>facility|Google DeepMind>GBR>company;Antoine Caillon+Google DeepMind>GBR>company;Cheng-Zhi Anna Huang+Mila, Quebec Artificial Intelligence Institute>CAN>facility|Université de Montréal>CAN>education|Canada CIFAR AI Chair>CAN>facility|Google DeepMind>GBR>company","Hindustani music is a performance-driven oral tradition that exhibits the rendition of rich melodic patterns. In this paper, we focus on generative modeling of singers' vocal melodies extracted from audio recordings, as the voice is musically prominent within the tradition. Prior generative work in Hindustani music models melodies as coarse discrete symbols which fails to capture the rich expressive melodic intricacies of singing. Thus, we propose to use a finely quantized pitch contour, as an intermediate representation for hierarchical audio modeling. We propose GaMaDHaNi, a modular two-level hierarchy, consisting of a generative model on pitch contours, and a pitch contour to audio synthesis model. We compare our approach to non-hierarchical audio models and hierarchical models that use a self-supervised intermediate representation, through a listening test and qualitative analysis. We also evaluate audio model's ability to faithfully represent the pitch contour input using Pearson correlation coefficient. By using pitch contours as an intermediate representation, we show that our model may be better equipped to listen and respond to musicians in a human-AI collaborative setting by highlighting two potential interaction use cases (1) primed generation, and (2) coarse pitch conditioning.",CAN,facility,Developed economies,"[7.909546, 1.1797239]","[1.7511523, -21.264788]","[6.9602113, 6.337507, -10.014223]","[15.083695, -16.420258, -6.472845]","[11.094195, 10.249462]","[7.6212115, 1.8522992]","[11.819094, 15.54545, -1.0856031]","[9.183457, 7.051746, 12.218675]"
114,Haonan Chen;Jordan B. L. Smith;Janne Spijkervet;Ju-Chiang Wang;Pei Zou;Bochen Li;Qiuqiang Kong;Xingjian Du,SymPAC: Scalable Symbolic Music Generation With Prompts and Constraints,2024,https://doi.org/10.5281/zenodo.14877507,Haonan Chen+ByteDance Inc.>USA>company;Jordan B. L. Smith+Queen Mary University of London>GBR>education;Janne Spijkervet+ByteDance Inc.>USA>company;Ju-Chiang Wang+ByteDance Inc.>USA>company;Pei Zou+ByteDance Inc.>USA>company;Bochen Li+ByteDance Inc.>USA>company;Qiuqiang Kong+The Chinese University of Hong Kong>HKG>education;Xingjian Du+ByteDance Inc.>USA>company,"Progress in the task of symbolic music generation may be lagging behind other tasks like audio and text generation, in part because of the scarcity of symbolic training data. In this paper, we leverage the greater scale of audio music data by applying pre-trained MIR models (for transcription, beat tracking, structure analysis, etc.) to extract symbolic events and encode them into token sequences. To the best of our knowledge, this work is the first to demonstrate the feasibility of training symbolic generation models solely from auto-transcribed audio data. Furthermore, to enhance the controllability of the trained model, we introduce SymPAC (Symbolic Music Language Model with Prompting and Constrained Generation), which is distinguished by using (a) prompt bars in encoding and (b) a technique called Constrained Generation via Finite State Machines (FSMs) during inference time. We show the flexibility and controllability of this approach, which may be critical in making music AI useful to creators and users.",USA,company,Developed economies,"[21.108393, 11.058748]","[-15.441703, -36.487137]","[1.8070744, -4.839019, 21.225218]","[-16.960274, -6.307866, -10.212604]","[10.464925, 8.136058]","[9.118037, 5.659409]","[13.418896, 11.708274, -0.16650681]","[9.880766, 5.741015, 9.371865]"
115,Giovanni Bindi;Philippe Esling,Unsupervised Composable Representations for Audio,2024,https://doi.org/10.5281/zenodo.14877509,Giovanni Bindi+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility;Philippe Esling+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility,"Current generative models are able to generate high-quality artefacts but have been shown to struggle with compositional reasoning, which can be defined as the ability to generate complex structures from simpler elements. In this paper, we focus on the problem of compositional representation learning for music data, specifically targeting the fully-unsupervised setting. We propose a simple and extensible framework that leverages an explicit compositional inductive bias, defined by a flexible auto-encoding objective that can leverage any of the current state-of-art generative models. We demonstrate that our framework, used with diffusion models, naturally addresses the task of unsupervised audio source separation, showing that our model is able to perform high-quality separation. Our findings reveal that our proposal achieves comparable or superior performance with respect to other blind source separation methods and, furthermore, it even surpasses current state-of-art supervised baselines on signal-to-interference ratio metrics. Additionally, by learning an a-posteriori masking diffusion model in the space of composable representations, we achieve a system capable of seamlessly performing unsupervised source separation, unconditional generation, and variation generation. Finally, as our proposal works in the latent space of pre-trained neural audio codecs, it also provides a lower computational cost with respect to other neural baselines.",FRA,facility,Developed economies,"[-16.772041, -15.13912]","[-38.624172, -32.598797]","[2.5811076, 17.169195, 9.327328]","[-13.936267, -4.664709, -23.487322]","[11.397264, 9.126427]","[7.2763205, 6.1092978]","[13.239388, 13.266432, 0.5847691]","[9.628208, 8.108956, 8.7230835]"
116,Pavani B. Chowdary;Bhavyajeet Singh;Rajat Agarwal;Vinoo Alluri,"Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes and Depression Risk",2024,https://doi.org/10.5281/zenodo.14877511,"Pavani Chowdary+International Institute of Information Technology, Hyderabad>IND>education;Bhavyajeet Singh+International Institute of Information Technology, Hyderabad>IND>education;Rajat Agarwal+International Institute of Information Technology, Hyderabad>IND>education;Vinoo Alluri+International Institute of Information Technology, Hyderabad>IND>education","Lyrics play a crucial role in affecting and reinforcing emotional states by providing meaning and emotional connotations that interact with the acoustic properties of the music. Specific lyrical themes and emotions may intensify existing negative states in listeners and may lead to undesirable outcomes, especially in listeners with mood disorders such as depression. Hence, it is important for such individuals to be mindful of their listening strategies. In this study, we examine online music consumption of individuals at risk of depression in light of lyrical themes and emotions. Lyrics obtained from the listening histories of 541 Last.fm users, divided into At-Risk and No-Risk based on their mental well-being scores, were analyzed using natural language processing techniques. Statistical analyses of the results revealed that individuals at risk for depression prefer songs with lyrics associated with low valence and low arousal. Additionally, lyrics associated with themes of denial, self-reference and blame were preferred. This study opens up the possibility of an approach to assessing depression risk from the digital footprint of individuals and potentially developing personalized recommendation systems.",IND,education,Developing economies,"[-51.033295, 0.13926192]","[53.941914, 0.63984174]","[-14.159977, 23.594362, 8.825349]","[9.955461, 19.94197, 15.886458]","[13.157775, 12.220337]","[13.090332, 3.4617746]","[15.761419, 14.977041, 1.343388]","[14.004091, 4.9733653, 10.995825]"
117,Karn N. Watcharasupat;Alexander Lerch,A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems,2024,https://doi.org/10.5281/zenodo.14877513,Karn N. Watcharasupat+Georgia Institute of Technology>USA>education|Music Informatics Group>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education|Music Informatics Group>USA>education,"Despite significant recent progress across multiple sub-tasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. We propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument recognition PaSST model. On the MoisesDB dataset, Banquet — at only 24.9 M trainable parameters — performed on par with or better than the significantly more complex 6-stem Hybrid Transformer Demucs. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs.",USA,education,Developed economies,"[7.5600624, -47.56423]","[-39.985527, -29.193577]","[32.615963, -4.086257, -3.0412548]","[-10.567554, -5.5754056, -28.751696]","[8.372024, 10.015437]","[6.724185, 5.6767836]","[11.07008, 13.624227, 1.6654452]","[9.69439, 8.368118, 9.118566]"
118,Mickael Zehren;Marco Alunno;Paolo Bientinesi,In-Depth Performance Analysis of the ADTOF-Based Algorithm for Automatic Drum Transcription,2024,https://doi.org/10.5281/zenodo.14877515,Mickaël Zehren+Umeå Universitet>SWE>education;Marco Alunno+Universidad EAFIT Medellín>COL>education;Paolo Bientinesi+Umeå Universitet>SWE>education,"The importance of automatic drum transcription lies in the potential to extract useful information from a musical track; however, the low reliability of the models for this task represents a limiting factor. Indeed, even though in the recent literature the quality of the generated transcription has improved thanks to the curation of large training datasets via crowdsourcing, there is still a large margin of improvement for this task to be considered solved. Aiming to steer the development of future models, we identify the most common errors from training and testing on the aforementioned crowdsourced datasets. We perform this study in three steps: First, we detail the quality of the transcription for each class of interest; second, we employ a new metric and a pseudo confusion matrix to quantify different mistakes in the estimations; last, we compute the agreement between different annotators of the same track to estimate the accuracy of the ground-truth. Our findings are twofold: On the one hand, we observe that the previously reported issue that less represented instruments (e.g., toms) are less reliably transcribed is mostly solved now. On the other hand, cymbal instruments have unprecedented relative low performance. We provide intuitive explanations as to why cymbal instruments are difficult to transcribe and we identify that they represent the main source of disagreement among annotators.",SWE,education,Developed economies,"[28.413641, -43.493443]","[-37.739216, -13.579671]","[21.395054, -18.28774, 4.969368]","[3.5402527, 12.357368, -18.276052]","[7.592775, 7.203528]","[8.376686, 4.292464]","[10.353809, 11.480593, 1.0268744]","[9.231855, 7.1107783, 9.825607]"
119,Patricia Hu;Lukáš Samuel Marták;Carlos Eduardo Cancino-Chacón;Gerhard Widmer,Towards Musically Informed Evaluation of Piano Transcription Models,2024,https://doi.org/10.5281/zenodo.14877517,"Patricia Hu+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Lukáš Samuel Marták+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Carlos Cancino-Chacón+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Automatic piano transcription models are typically evaluated using simple frame- or note-wise information retrieval (IR) metrics. Such benchmark metrics do not provide insights into the transcription quality of specific musical aspects such as articulation, dynamics, or rhythmic precision of the output, which are essential in the context of expressive performance analysis. Furthermore, in recent years, MAESTRO has become the de-facto training and evaluation dataset for such models. However, inference performance has been observed to deteriorate substantially when applied on out-of-distribution data, thereby questioning the suitability and reliability of transcribed outputs from such models for specific MIR tasks. In this work, we investigate the performance of three state-of-the-art piano transcription models in two experiments. In the first one, we propose a variety of musically informed evaluation metrics which, in contrast to the IR metrics, offer more detailed insight into the musical quality of the transcriptions. In the second experiment, we compare inference performance on real-world and perturbed audio recordings, and highlight musical dimensions which our metrics can help explain. Our experimental results highlight the weaknesses of existing piano transcription metrics and contribute to a more musically sound error analysis of transcription outputs.",AUT,education,Developed economies,"[30.728226, -3.7638278]","[-33.641197, 4.407796]","[15.3314705, -1.8045878, 16.833818]","[-8.248662, -9.758883, -7.5732756]","[9.866915, 7.2605386]","[7.5341234, 4.0227127]","[12.114679, 11.599449, -0.3736391]","[9.207099, 6.1560802, 10.272951]"
120,Tomoyasu Nakano;Masataka Goto,Using Item Response Theory to Aggregate Music Annotation Results of Multiple Annotators,2024,https://doi.org/10.5281/zenodo.14877519,Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Human music annotation is one of the most important tasks in music information retrieval (MIR) research. Results of labeling, tagging, assessment, and evaluation can be used as training data for machine learning models that estimate them automatically. For such machine learning purposes, a single target (e.g., song) is usually annotated by multiple human annotators, and the results are aggregated by majority voting or averaging. Majority voting, however, requires the number of annotators to be an odd number, which is not always possible. And averaging is sensitive to differences in the judgmental characteristics of each annotator and cannot be used for ordinal scales. This paper therefore proposes that the item response theory (IRT) be used to aggregate the music annotation results of multiple annotators. IRT-based models can jointly estimate annotators' characteristics and latent scores (i.e., aggregations of annotation results) of the targets, and they are also applicable to ordinal scales. We evaluated the IRT-based models in two actual cases of music annotation — semantic tagging of music and Likert scale-based evaluation of singing skill — and compared those models with their simplified models that do not consider the characteristics of each annotator.",JPN,facility,Developed economies,"[-31.625965, 8.744586]","[46.30768, -2.5559165]","[-14.849328, 5.297462, 3.9798467]","[17.960728, 24.14992, 2.3189538]","[14.023283, 9.613074]","[12.500069, 3.3533576]","[14.852173, 13.914941, -0.5487833]","[13.533898, 5.4285426, 10.99702]"
121,Irmak Bukey;Michael Feffer;Chris Donahue,Just Label the Repeats for In-the-Wild Audio-to-Score Alignment,2024,https://doi.org/10.5281/zenodo.14877521,Irmak Bukey+Carnegie Mellon University>USA>education;Michael Feffer+Carnegie Mellon University>USA>education;Chris Donahue+Carnegie Mellon University>USA>education,"We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images).1 Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs—this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150% relative to prior work (33% → 82%).",USA,education,Developed economies,"[20.639385, -14.723862]","[-19.54623, -16.380695]","[-1.5387579, -15.231118, -12.10921]","[-2.9169352, -23.056664, -4.8343215]","[10.842757, 5.967798]","[6.1469016, 0.72056246]","[11.755367, 12.44986, -1.816051]","[8.120273, 5.8666863, 10.746304]"
122,Lucas S. Maia;Richa Namballa;Martín Rocamora;Magdalena Fuentes;Carlos Guedes,Investigating Time-Line-Based Music Traditions With Field Recordings: A Case Study of Candomblé Bell Patterns,2024,https://doi.org/10.5281/zenodo.14877523,Lucas S. Maia+New York University Abu Dhabi>ARE>education|Universidade Federal do Rio de Janeiro>BRA>education;Richa Namballa+New York University>USA>education;Martín Rocamora+Universitat Pompeu Fabra>ESP>education|Universidad de la República>URY>education;Magdalena Fuentes+New York University>USA>education;Carlos Guedes+New York University Abu Dhabi>ARE>education|Universidade Federal do Rio de Janeiro>BRA>education,"We introduce a series of transdisciplinary corpus studies aimed at investigating cross-cultural trends in time-line-based music traditions. Our analyses concentrate on a compilation of field recordings from the Centre de Recherche en Ethnomusicologie (CREM) sound archive. To demonstrate the value of an interdisciplinary approach combining ethnomusicology and music information research to rhythmic analysis, we propose a case study on the bell patterns used in the musical practices of Candomblé, an Afro-Brazilian religion. After removing vocals from the recordings with a deep learning source separation technique, we further process the instrumental segments using non-negative matrix factorization and select the bell components. Then, we compute a tempo-agnostic rhythmic feature from the bell track and use it to cluster the data. Finally, we use synthesized patterns from the musicological literature about Candomblé as references to propagate labels to the rhythmic clusters in our data. This semi-supervised approach to pattern analysis precludes the need for downbeat and cycle annotations, making it particularly suited for extensive archive investigations. Lastly, by comparing bell patterns in Candomblé and a West African music tradition, we lay the foundation for our future crosscultural research and observe the potential application of this methodology to other time-line-based music.",ARE,education,Developing economies,"[20.545286, -27.984146]","[-20.326687, 9.524775]","[-8.997737, -30.916685, -7.027825]","[-3.0828948, 14.23474, 0.36468548]","[11.74884, 5.651374]","[6.0663824, 1.8266037]","[11.453692, 13.644207, -2.0825021]","[8.454482, 6.7074676, 11.59706]"
1,Adithi Shankar;Genís Plaja-Roglans;Thomas Nuttall;Martín Rocamora;Xavier Serra,Saraga Audiovisual: A Large Multimodal Open Data Collection for the Analysis of Carnatic Music,2024,https://doi.org/10.5281/zenodo.14877279,"Adithi Shankar+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Genís Plaja-Roglans+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Thomas Nuttall+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Martín Rocamora+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","Carnatic music is a style of South Indian art music whose analysis using computational methods is an active area of research in Music Information Research (MIR). A core, open dataset for such analysis is the Saraga dataset, which includes multi-stem audio, expert annotations, and accompanying metadata. However, it has been noted that there are several limitations to the Saraga collections, and that additional relevant aspects of the tradition still need to be covered to facilitate musicologically important research lines. In this work, we present Saraga Audiovisual, a dataset that includes new and more diverse renditions of Carnatic vocal performances, totalling 42 concerts and more than 60 hours of music. A major contribution of this dataset is the inclusion of video recordings for all concerts, allowing for a wide range of multimodal analyses. We also provide high-quality human pose estimation data of the musicians extracted from the video footage, and perform benchmarking experiments for the different modalities to validate the utility of the novel collection. Saraga Audiovisual, along with access tools and results of our experiments, is made available for research purposes.",ESP,education,Developed economies,"[-18.763954, 8.473882]","[6.796424, -20.003962]","[-14.846709, -7.5259933, -13.651115]","[17.5585, -10.43198, -5.636839]","[12.872537, 7.975024]","[7.4073906, 1.257962]","[12.387257, 14.628218, -1.190029]","[9.063674, 7.06747, 12.477541]"
96,Christopher J. Tralie;Ben Cantil,The Concatenator: A Bayesian Approach to Real Time Concatenative Musaicing,2024,https://doi.org/10.5281/zenodo.14877471,Christopher J. Tralie+Ursinus College>USA>education|DataMind Audio>USA>company;Ben Cantil+DataMind Audio>USA>company,"We present ""The Concatenator,"" a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.'s ""musaicing"" (or ""audio mosaicing"") technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger's NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.",USA,education,Developed economies,"[43.734303, 4.405279]","[-12.185052, -22.348505]","[-8.704806, -24.415615, 9.302255]","[0.6988701, -21.177801, -14.117966]","[11.430545, 6.3137903]","[6.5823307, 1.317321]","[11.952819, 12.910204, -0.9161042]","[8.658403, 6.287943, 10.419358]"
3,Emmanuel Deruty,Harmonic and Transposition Constraints Arising From the Use of the Roland TR-808 Bass Drum,2024,https://doi.org/10.5281/zenodo.14877286,Emmanuel Deruty+Sony Computer Science Laboratories>FRA>company|Aalborg University>DNK>education,"The study investigates hip-hop music producer Scott Storch's approach to tonality, where the song's key is transposed to fit the Roland TR-808 bass drum instead of tuning the drums to the song's key. This process, involving the adjustment of all tracks except the bass drum, suggests significant production motives. The primary constraint stems from the limited usable pitch range of the TR-808 bass drum if its characteristic sound is to be preserved. The research examines drum tuning practices, the role of the Roland TR-808 in music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples reveals their characteristics and their integration into modern genres like trap and hip-hop. The study also considers the impact of loudspeaker frequency response and human ear sensitivity on bass drum perception. The findings suggest that Storch's method prioritizes the spectral properties of the bass drum over traditional pitch values to enhance the bass response. The need to maintain the unique sound of the TR-808 bass drum underscores the importance of spectral formants and register in contemporary popular music production.",FRA,company,Developed economies,"[25.459213, -42.094193]","[-17.784246, 0.83666694]","[21.255335, -14.357203, 2.650777]","[7.2036133, 14.261849, -11.513467]","[7.897064, 7.3016663]","[8.426832, 4.0004096]","[10.461497, 11.748468, 0.93976665]","[9.573447, 7.428254, 10.299387]"
95,Zachary Novack;Julian McAuley;Taylor Berg-Kirkpatrick;Nicholas J. Bryan,DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation,2024,https://doi.org/10.5281/zenodo.14877469,Zachary Novack+University of California – San Diego>USA>education;Julian McAuley+University of California – San Diego>USA>education;Taylor Berg-Kirkpatrick+University of California – San Diego>USA>education;Nicholas J. Bryan+Adobe Research>USA>company,"Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion inference-time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T -Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.",USA,education,Developed economies,"[21.294655, 2.2723832]","[-14.832686, -46.3765]","[3.3035843, 0.60044813, 16.86768]","[-21.971128, -1.8153814, -15.9989805]","[10.085343, 8.603791]","[8.666045, 6.5557904]","[13.233319, 11.952562, 0.28700623]","[9.645623, 5.6274695, 8.378179]"
93,Jan Melechovsky;Abhinaba Roy;Dorien Herremans,MidiCaps: A Large-Scale MIDI Dataset With Text Captions,2024,https://doi.org/10.5281/zenodo.14877465,Jan Melechovsky+Singapore University of Technology and Design>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Abhinaba Roy+Singapore University of Technology and Design>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Dorien Herremans+Singapore University of Technology and Design>SGP>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting MidiCaps, the first openly available large-scale MIDI dataset with text captions. MIDI (Musical Instrument Digital Interface) files are widely used for encoding musical information and can capture the nuances of musical composition. They are widely used by music producers, composers, musicologists, and performers alike. Inspired by recent advancements in captioning techniques, we present a curated dataset of over 168k MIDI files with textual descriptions. Each MIDI caption describes the musical content, including tempo, chord progression, time signature, instruments, genre, and mood, thus facilitating multi-modal exploration and analysis. The dataset encompasses various genres, styles, and complexities, offering a rich data source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research at the intersection of music and natural language processing, fostering advancements in both fields.",SGP,education,Developing economies,"[38.50196, -0.41709095]","[-11.418498, -28.723825]","[8.422695, -8.349147, 21.097013]","[0.11328148, 0.89348465, -20.48255]","[10.451738, 7.2494807]","[9.514795, 5.214282]","[12.650273, 11.615436, -0.53149796]","[10.518169, 6.0177436, 9.413697]"
66,Silvan Peter;Gerhard Widmer,TheGlueNote: Learned Representations for Robust and Flexible Note Alignment,2024,https://doi.org/10.5281/zenodo.14877409,"Silvan David Peter+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network — TheGlueNote 1 — which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.",AUT,education,Developed economies,"[16.03697, -14.978823]","[-14.537277, -21.926685]","[4.872805, -13.301163, -9.259234]","[-1.7901886, -20.056602, -10.070945]","[10.775218, 6.5859766]","[6.4918513, 1.2343875]","[11.865524, 12.6692095, -1.298201]","[8.410633, 6.0635433, 10.456155]"
67,Xavier Riley;Zixun Guo;Andrew C. Edwards;Simon Dixon,GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model,2024,https://doi.org/10.5281/zenodo.14877413,Xavier Riley+Queen Mary University of London>GBR>education;Zixun Guo+Queen Mary University of London>GBR>education;Drew Edwards+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"We introduce GAPS (Guitar-Aligned Performance Scores), a new dataset of classical guitar performances, and a benchmark guitar transcription model that achieves state-of-the-art performance on GuitarSet in both supervised and zero-shot settings. GAPS is the largest dataset of real guitar audio, containing 14 hours of freely available audio-score aligned pairs, recorded in diverse conditions by over 200 performers, together with high-resolution note-level MIDI alignments and performance videos. These enable us to train a state-of-the-art model for automatic transcription of solo guitar recordings which can generalise well to real world audio that is unseen during training. For each track in the dataset, we provide metadata of the composer and performer, giving dates, nationality, gender and links to IMSLP or Wikipedia. We also analyse guitar-specific features of the dataset, such as the distribution of fret-string combinations and alternate tunings. This dataset has applications to various MIR tasks, including automatic music transcription, score following, performance analysis, generative music modelling and the study of expressive performance timing.",GBR,education,Developed economies,"[42.55648, -7.82495]","[-40.457325, -5.1703186]","[20.931398, -7.0188565, 8.640955]","[-13.71021, -7.940729, -4.564058]","[8.025473, 8.406571]","[7.425234, 4.0714254]","[11.814839, 11.372238, 0.9743866]","[9.087507, 6.3551564, 10.211397]"
68,Hugo T. Carvalho;Min Susan Li;Massimiliano Di Luca;Alan M. Wing,A Kalman Filter Model for Synchronization in Musical Ensembles,2024,https://doi.org/10.5281/zenodo.14877411,Hugo T. Carvalho+Federal University of Rio de Janeiro>BRA>education;Min S. Li+University of Bristol>GBR>education;Massimiliano di Luca+University of Birmingham>GBR>education|Virtual Reality Lab>Unknown>Unknown;Alan M. Wing+University of Birmingham>GBR>education|Virtual Reality Lab>Unknown>Unknown,"The synchronization of motor responses to rhythmic auditory cues is a fundamental biological phenomenon observed across various species. While the importance of temporal alignment varies across different contexts, achieving precise temporal synchronization is a prominent goal in musical performances. Musicians often incorporate expressive timing variations, which require precise control over timing and synchronization, particularly in ensemble performance. This is crucial because both deliberate expressive nuances and accidental timing deviations can affect the overall timing of a performance. This discussion prompts the question of how musicians adjust their temporal dynamics to achieve synchronization within an ensemble. This paper introduces a novel feedback correction model based on the Kalman Filter, aimed at improving the understanding of interpersonal timing in ensemble music performances. The proposed model performs similarly to other linear correction models in the literature, with the advantage of low computational cost and good performance even in scenarios where the underlying tempo varies.",BRA,education,Developing economies,"[26.770157, -31.839989]","[-26.568975, 2.6533124]","[-0.6732376, -22.61612, -13.23262]","[-9.83713, 11.110051, -1.9850583]","[11.259467, 5.780169]","[5.820739, 1.3430386]","[11.9937105, 12.575635, -2.2194078]","[7.899057, 6.436516, 11.356378]"
69,Alain Riou;Stefan Lattner;Gaëtan Hadjeres;Michael Anslow;Geoffroy Peeters,Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation,2024,https://doi.org/10.5281/zenodo.14877415,"Alain Riou+Télécom-Paris, Institut Polytechnique de Paris>FRA>education;Stefan Lattner+Sony Computer Science Laboratories - Paris>FRA>company;Gaëtan Hadjeres+Sony AI>Unknown>Unknown;Michael Anslow+Sony Computer Science Laboratories - Paris>FRA>company;Geoffroy Peeters+Télécom-Paris, Institut Polytechnique de Paris>FRA>education","This paper explores the automated process of determining stem compatibility by identifying audio recordings of single instruments that blend well with a given musical context. To tackle this challenge, we present Stem-JEPA, a novel Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset using a self-supervised learning approach. Our model comprises two networks: an encoder and a predictor, which are jointly trained to predict the embeddings of compatible stems from the embeddings of a given context, typically a mix of several instruments. Training a model in this manner allows its use in estimating stem compatibility—retrieving, aligning, or generating a stem to match a given mix—or for downstream tasks such as genre or key estimation, as the training paradigm requires the model to learn information related to timbre, harmony, and rhythm. We evaluate our model's performance on a retrieval task on the MUSDB18 dataset, testing its ability to find the missing stem from a mix and through a subjective user study. We also show that the learned embeddings capture temporal alignment information and, finally, evaluate the representations learned by our model on several downstream tasks, highlighting that they effectively capture meaningful musical features.",FRA,education,Developed economies,"[-16.684813, 1.2815938]","[-22.033464, -34.08499]","[-7.0142317, 7.360882, 15.55109]","[-9.73537, -2.4603727, -18.934837]","[12.179956, 8.635058]","[9.339337, 4.964413]","[13.70897, 13.56957, -0.3478796]","[10.352567, 6.49041, 9.029889]"
70,Fang Duo Tsai;Shih-Lun Wu;Haven Kim;Bo-Yu Chen;Hao-Chung Cheng;Yi-Hsuan Yang,Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music With Lightweight Finetuning,2024,https://doi.org/10.5281/zenodo.14877417,Fang-Duo Tsai+National Taiwan University>TWN>education|Carnegie Mellon University>USA>education|University of California San Diego>USA>education;Shih-Lun Wu+Carnegie Mellon University>USA>education;Haven Kim+University of California San Diego>USA>education;Bo-Yu Chen+National Taiwan University>TWN>education;Hao-Chung Cheng+National Taiwan University>TWN>education;Yi-Hsuan Yang+National Taiwan University>TWN>education,"Text-to-music models allow users to generate nearly realistic musical audio with textual commands. However, editing music audios remains challenging due to the conflicting desiderata of performing fine-grained alterations on the audio while maintaining a simple user interface. To address this challenge, we propose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion-based text-to-music model. With 22M trainable parameters, AP-Adapter empowers users to harness both global (e.g., genre and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP-Adapter on three tasks: timbre transfer, genre transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.",TWN,education,Developing economies,"[24.380014, 10.526503]","[-15.980051, -44.3318]","[0.9406473, -3.0000536, 27.103907]","[-19.725782, -4.52086, -14.970322]","[10.499315, 8.08218]","[8.609944, 6.2552743]","[13.518272, 11.875686, -0.23535521]","[10.018583, 5.729182, 8.748072]"
71,Shangda Wu;Yashan Wang;Xiaobing Li;Feng Yu;Maosong Sun,MelodyT5: A Unified Score-to-Score Transformer for Symbolic Music Processing,2024,https://doi.org/10.5281/zenodo.14877419,Shangda Wu+Central Conservatory of Music>CHN>education;Yashan Wang+Central Conservatory of Music>CHN>education;Xiaobing Li+Central Conservatory of Music>CHN>education;Feng Yu+Central Conservatory of Music>CHN>education;Maosong Sun+Central Conservatory of Music>CHN>education|Tsinghua University>CHN>education,"In the domain of symbolic music research, the progress of developing scalable systems has been notably hindered by the scarcity of available training data and the demand for models tailored to specific tasks. To address these issues, we propose MelodyT5, a novel unified framework that leverages an encoder-decoder architecture tailored for symbolic music processing in ABC notation. This framework challenges the conventional task-specific approach, considering various symbolic music tasks as score-to-score transformations. Consequently, it integrates seven melody-centric tasks, from generation to harmonization and segmentation, within a single model. Pre-trained on Melody-Hub, a newly curated collection featuring over 261K unique melodies encoded in ABC notation and encompassing more than one million task instances, MelodyT5 demonstrates superior performance in symbolic music processing via multi-task transfer learning. Our findings highlight the efficacy of multi-task transfer learning in symbolic music processing, particularly for data-scarce tasks, challenging the prevailing task-specific paradigms and offering a comprehensive dataset and framework for future explorations in this domain.",CHN,education,Developing economies,"[19.471392, 10.2838545]","[-13.686463, -31.59969]","[-0.9572532, -3.3073437, 18.693264]","[-13.116739, -3.0532148, -6.8083386]","[11.2712555, 7.0871263]","[8.8055, 5.661652]","[13.21583, 12.023252, -0.7555075]","[9.663776, 5.975689, 9.37411]"
72,Emmanouil Karystinaios;Gerhard Widmer,GraphMuse: A Library for Symbolic Music Graph Processing,2024,https://doi.org/10.5281/zenodo.14877421,"Emmanouil Karystinaios+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|LIT AI Lab, Linz Institute of Technology>AUT>education","Graph Neural Networks (GNNs) have recently gained traction in symbolic music tasks, yet a lack of a unified framework impedes progress. Addressing this gap, we present GraphMuse, a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks. Experiments with two specific musical prediction tasks – pitch spelling and cadence detection – demonstrate significant performance improvement over previous methods. Our hope is that GraphMuse will lead to a boost in, and standardization of, symbolic music processing based on graph representations. The library is available at https: //github.com/manoskary/graphmuse",AUT,education,Developed economies,"[16.631765, 14.807877]","[-14.843665, -28.077503]","[-1.907528, -5.568231, 17.933647]","[-19.529125, -12.953673, -11.977301]","[11.878549, 7.121077]","[8.756302, 5.1951575]","[13.509169, 12.335666, -1.0090611]","[9.886231, 6.124965, 9.584429]"
73,Christian J. Steinmetz;Shubhr Singh;Marco Comunità;Ilias Ibnyahya;Shanxin Yuan;Emmanouil Benetos;Joshua D. Reiss,ST-ITO: Controlling Audio Effects for Style Transfer With Inference-Time Optimization,2024,https://doi.org/10.5281/zenodo.14877423,"Christian J. Steinmetz+Centre for Digital Music, Queen Mary University of London>GBR>education;Shubhr Singh+Centre for Digital Music, Queen Mary University of London>GBR>education;Marco Comunità+Centre for Digital Music, Queen Mary University of London>GBR>education;Ilias Ibnyahya+Centre for Digital Music, Queen Mary University of London>GBR>education;Shanxin Yuan+Centre for Digital Music, Queen Mary University of London>GBR>education;Emmanouil Benetos+Centre for Digital Music, Queen Mary University of London>GBR>education;Joshua D. Reiss+Centre for Digital Music, Queen Mary University of London>GBR>education","Audio production style transfer is the task of processing an input to impart stylistic elements from a reference recording. Existing approaches often train a neural network to estimate control parameters for a set of audio effects. However, these approaches are limited in that they can only control a fixed set of effects, where the effects must be differentiable or otherwise employ specialized training techniques. In this work, we introduce ST-ITO, Style Transfer with Inference-Time Optimization, an approach that instead searches the parameter space of an audio effect chain at inference. This method enables control of arbitrary audio effect chains, including unseen and non-differentiable effects. Our approach employs a learned metric of audio production style, which we train through a simple and scalable self-supervised pretraining strategy, along with a gradient-free optimizer. Due to the limited existing evaluation methods for audio production style transfer, we introduce a multi-part benchmark to evaluate audio production style metrics and style transfer systems. This evaluation demonstrates that our audio representation better captures attributes related to audio production and enables expressive style transfer via control of arbitrary audio effects.",GBR,education,Developed economies,"[11.470976, -35.567814]","[-18.195982, -45.81071]","[25.872114, 6.710399, 7.832292]","[-21.450602, -0.60338926, -21.440855]","[9.580872, 8.618576]","[8.21246, 6.632701]","[12.645133, 12.242039, 0.68506724]","[9.745192, 5.9600005, 8.266792]"
74,Qixin Deng;Qikai Yang;Ruibin Yuan;Yipeng Huang;Yi Wang;Xubo Liu;Zeyue Tian;Jiahao Pan;Ge Zhang;Hanfeng Lin;Yizhi Li;Yinghao Ma;Jie Fu;Chenghua Lin;Emmanouil Benetos;Wenwu Wang;Guangyu Xia;Wei Xue;Yike Guo,ComposerX: Multi-Agent Symbolic Music Composition With LLMs,2024,https://doi.org/10.5281/zenodo.14877425,Qixin Deng+University of Rochester>USA>education;Qikai Yang+University of Illinois at Urbana-Champaign>USA>education;Ruibin Yuan+Hong Kong University of Science and Technology>HKG>education;Yipeng Huang+Multimodal Art Projection Research Community>Unknown>Unknown;Yi Wang+Multimodal Art Projection Research Community>Unknown>Unknown;Xubo Liu+University of Surrey>GBR>education;Zeyue Tian+Hong Kong University of Science and Technology>HKG>education;Jiahao Pan+Hong Kong University of Science and Technology>HKG>education;Ge Zhang+01.AI>Unknown>Unknown;Hanfeng Lin+Multimodal Art Projection Research Community>Unknown>Unknown;Yizhi Li+The University of Manchester>GBR>education;Yinghao Ma+Queen Mary University of London>GBR>education;Jie Fu+Hong Kong University of Science and Technology>HKG>education;Chenghua Lin+The University of Manchester>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Wenwu Wang+University of Surrey>GBR>education;Guangyu Xia+Mohamed bin Zayed University of Artificial Intelligence>ARE>education;Wei Xue+Hong Kong University of Science and Technology>HKG>education;Yike Guo+Hong Kong University of Science and Technology>HKG>education,"Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. Current LLMs often struggle with this task, sometimes generating poorly written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs' potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",USA,education,Developed economies,"[21.42937, 11.929613]","[-8.27901, 37.09]","[1.5431573, -6.139964, 19.65869]","[-18.460386, 3.211016, 14.399955]","[10.477462, 8.136398]","[9.451706, 5.8841386]","[13.478381, 11.782149, -0.154083]","[10.103343, 5.313421, 9.757598]"
75,Megan Wei;Michael Freeman;Chris Donahue;Chen Sun,Do Music Generation Models Encode Music Theory?,2024,https://doi.org/10.5281/zenodo.14877427,Megan Wei+Brown University>USA>education|Carnegie Mellon University>USA>education;Michael Freeman+Brown University>USA>education|Carnegie Mellon University>USA>education;Chris Donahue+Carnegie Mellon University>USA>education;Chen Sun+Brown University>USA>education,"Music foundation models possess impressive music generation capabilities. When people compose music, they may infuse their understanding of music into their work, by using notes and intervals to craft melodies, chords to build progressions, and tempo to create a rhythmic feel. To what extent is this true of music generation models? More specifically, are fundamental Western music theory concepts observable within the ""inner workings"" of these models? Recent work proposed leveraging latent audio representations from music generation models towards music information retrieval tasks (e.g. genre classification, emotion recognition), which suggests that high-level musical characteristics are encoded within these models. However, probing individual music theory concepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus, we introduce SynTheory, a synthetic MIDI and audio music theory dataset, consisting of tempos, time signatures, notes, intervals, scales, chords, and chord progressions concepts. We then propose a framework to probe for these music theory concepts in music foundation models (Jukebox and MusicGen) and assess how strongly they encode these concepts within their internal representations. Our findings suggest that music theory concepts are discernible within foundation models and that the degree to which they are detectable varies by model size and layer.",USA,education,Developed economies,"[19.298512, 1.9973288]","[-2.5040185, -35.563656]","[0.6133765, 3.4823139, 18.67645]","[-17.025185, 9.788612, -1.6416895]","[10.29978, 8.645602]","[9.493675, 5.363023]","[13.3861475, 12.220859, 0.2968804]","[10.15088, 5.740644, 9.599078]"
76,Silas Antonisen;Iván López-Espejo,PolySinger: Singing-Voice to Singing-Voice Translation From English to Japanese,2024,https://doi.org/10.5281/zenodo.14877429,Silas Antonisen+University of Granada>ESP>education;Iván López-Espejo+University of Granada>ESP>education,"The speech domain prevails in the spotlight for several natural language processing (NLP) tasks while the singing domain remains less explored. The culmination of NLP is the speech-to-speech translation (S2ST) task, referring to translation and synthesis of human speech. A disparity between S2ST and the possible adaptation to the singing domain, which we describe as singing-voice to singing-voice translation (SV2SVT), is becoming prominent as the former is progressing ever faster, while the latter is at a standstill. Singing-voice synthesis systems are overcoming the barrier of multi-lingual synthesis, despite limited attention has been paid to multi-lingual songwriting and song translation. This paper endeavors to determine what is required for successful SV2SVT and proposes PolySinger (Polyglot Singer): the first system for SV2SVT, performing lyrics translation from English to Japanese. A cascaded approach is proposed to establish a framework with a high degree of control which can potentially diminish the disparity between SV2SVT and S2ST. The performance of PolySinger is evaluated by a mean opinion score test with native Japanese speakers. Results and in-depth discussions with test subjects suggest a solid foundation for SV2SVT, but several shortcomings must be overcome, which are discussed for the future of SV2SVT.",ESP,education,Developed economies,"[0.42235816, -33.69599]","[-27.26868, -44.86577]","[20.813122, 2.1302269, -10.175317]","[4.6738467, -2.2648091, -23.680029]","[9.825346, 10.828342]","[8.239354, 4.5664525]","[11.144995, 14.822895, 0.759273]","[10.587866, 7.141408, 9.229629]"
77,Arthur Flexer,On the Validity of Employing ChatGPT for Distant Reading of Music Similarity,2024,https://doi.org/10.5281/zenodo.14877431,Arthur Flexer+Johannes Kepler University Linz>AUT>education,"In this work we explore whether large language models (LLM) can be a useful and valid tool for music knowledge discovery. LLMs offer an interface to enormous quantities of text and hence can be seen as a new tool for 'distant reading', i.e. the computational analysis of text including sources about music. More specifically we investigated whether ratings of music similarity, as measured via human listening tests, can be recovered from textual data by using ChatGPT. We examined the inferences that can be drawn from these experiments through the formal lens of validity. We showed that correlation of ChatGPT with human raters is of moderate positive size but also lower than the average human inter-rater agreement. By evaluating a number of threats to validity and conducting additional experiments with ChatGPT, we were able to show that especially construct validity of such an approach is seriously compromised. The opaque black box nature of ChatGPT makes it close to impossible to judge the experiment's construct validity, i.e. the relationship between what is meant to be inferred from the experiment, which are estimates of music similarity, and what is actually being measured. As a consequence the use of LLMs for music knowledge discovery cannot be recommended.",AUT,education,Developed economies,"[-2.4846122, 15.103489]","[31.226898, 7.5909624]","[-9.070539, 6.6629376, 2.401266]","[10.359377, 7.302328, 9.210625]","[12.901094, 9.512106]","[11.540027, 2.1217194]","[13.415115, 15.386867, -0.7787838]","[12.852637, 6.1819196, 12.482669]"
78,Venkatakrishnan Vaidyanathapuram Krishnan;Noel Alben;Anish A. Nair;Nathaniel Condit-Schultz,Sanidha: A Studio Quality Multi-Modal Dataset for Carnatic Music,2024,https://doi.org/10.5281/zenodo.14877433,Venkatakrishnan Vaidyanathapuram Krishnan+Georgia Institute of Technology>USA>education;Noel Alben+Georgia Institute of Technology>USA>education;Anish Nair+Georgia Institute of Technology>USA>education;Nathaniel Condit-Schultz+Georgia Institute of Technology>USA>education,"Music source separation demixes a piece of music into its individual sound sources (vocals, percussion, melodic instruments, etc.), a task with no simple mathematical solution. It requires deep learning methods involving training on large datasets of isolated music stems. The most commonly available datasets are made from commercial Western music, limiting the models' applications to non-Western genres like Carnatic music. Carnatic music is a live tradition, with the available multi-track recordings containing overlapping sounds and bleeds between the sources. This poses a challenge to commercially available source separation models like Spleeter and Hybrid Demucs. In this work, we introduce Sanidha, the first open-source novel dataset1 for Carnatic music, offering studio-quality, multi-track recordings with minimal to no overlap or bleed. Along with the audio files, we provide high-definition videos of the artists' performances. Additionally, we fine-tuned Spleeter, one of the most commonly used source separation models, on our dataset and observed improved SDR performance compared to finetuning on a pre-existing Carnatic multi-track dataset. The outputs of the fine-tuned model with Sanidha are evaluated through a listening study.",USA,education,Developed economies,"[-18.61859, 8.442504]","[-38.70799, -30.03008]","[-13.955657, -8.221412, -13.27693]","[-12.077504, -4.966892, -26.470024]","[12.926478, 7.914829]","[6.787916, 5.758928]","[13.762692, 13.613907, -1.0530409]","[9.634642, 8.283926, 9.035431]"
79,Pedro Pereira Sarmento;Jackson J. Loth;Mathieu Barthet,Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music,2024,https://doi.org/10.5281/zenodo.14877435,Pedro Sarmento+Queen Mary University of London>GBR>education;Jackson Loth+Queen Mary University of London>GBR>education;Mathieu Barthet+Queen Mary University of London>GBR>education,"Generative AI models have recently blossomed, significantly impacting artistic and musical traditions. Research investigating how humans interact with and deem these models is therefore crucial. Through a listening and reflection study, we explore participants' perspectives on AI-vs human-generated progressive metal, in symbolic format, using rock music as a control group. AI-generated examples were produced by ProgGP [1], a Transformer-based model. We propose a mixed methods approach to assess the effects of generation type (human vs. AI), genre (progressive metal vs. rock), and curation process (random vs. cherry-picked). This combines quantitative feedback on genre congruence, preference, creativity, consistency, playability, humanness, and repeatability, and qualitative feedback to provide insights into listeners' experiences. A total of 32 progressive metal fans completed the study. Our findings validate the use of fine-tuning to achieve genre-specific specialization in AI music generation, as listeners could distinguish between AI-generated rock and progressive metal. Despite some AI-generated excerpts receiving similar ratings to human music, listeners exhibited a preference for human compositions. Thematic analysis identified key features for genre and AI vs. human distinctions. Finally, we consider the ethical implications of our work in promoting musical data diversity within MIR research by focusing on an under-explored genre.",GBR,education,Developed economies,"[-24.885525, 6.1783123]","[-5.6362047, 43.433712]","[-28.905142, -3.0909708, 6.250824]","[-20.871328, 9.872727, 3.056319]","[12.753616, 7.5175934]","[9.843795, 5.7909303]","[14.304806, 13.062455, -1.1823444]","[10.350076, 5.122064, 9.565109]"
80,Nils Demerlé;Philippe Esling;Guillaume Doras;David Genova,Combining Audio Control and Style Transfer Using Latent Diffusion,2024,https://doi.org/10.5281/zenodo.14877437,"Nils Demerlé+Ircam, STMS Lab, Sorbonne Université, CNRS>FRA>education;Philippe Esling+Ircam, STMS Lab, Sorbonne Université, CNRS>FRA>education;Guillaume Doras+Ircam, STMS Lab, Sorbonne Université, CNRS>FRA>education;David Genova+Ircam, STMS Lab, Sorbonne Université, CNRS>FRA>education","Deep generative models are now able to synthesize high-quality audio signals, shifting the critical aspect in their development from audio quality to control capabilities. Although text-to-music generation is getting largely adopted by the general public, explicit control and example-based style transfer are more adequate modalities to capture the intents of artists and musicians. In this paper, we aim to unify explicit control and style transfer within a single model by separating local and global information to capture musical structure and timbre respectively. To do so, we leverage the capabilities of diffusion autoencoders to extract semantic features, in order to build two representation spaces. We enforce disentanglement between those spaces using an adversarial criterion and a two-stage training strategy. Our resulting model can generate audio matching a timbre target, while specifying structure either with explicit controls or through another audio example. We evaluate our model on one-shot timbre transfer and MIDI-to-audio tasks on instrumental recordings and show that we outperform existing baselines in terms of audio quality and target fidelity. Furthermore, we show that our method can generate cover versions of complete musical pieces by transferring rhythmic and melodic content to the style of a target audio in a different genre.",FRA,education,Developed economies,"[11.510466, -35.70772]","[-17.831814, -46.41314]","[26.697126, 6.94855, 7.529829]","[-21.411446, -1.3242054, -20.085932]","[9.535486, 8.636232]","[8.281984, 6.6959224]","[12.613308, 12.245061, 0.7013334]","[9.60129, 5.8749495, 8.14821]"
81,Mequanent Argaw Muluneh;Yan-Tsung Peng;Li Su,Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox Tewahedo Church Chants,2024,https://doi.org/10.5281/zenodo.14877441,"Mequanent Argaw Muluneh+Institute of Information Science, Academia Sinica>TWN>education|Department of Computer Science, National Chengchi University>TWN>education;Yan-Tsung Peng+Department of Computer Science, National Chengchi University>TWN>education;Li Su+Institute of Information Science, Academia Sinica>TWN>education","Despite its musicological, cultural, and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented in music research. Historical records, including manuscripts, research papers, and oral traditions, confirm Saint Yared's establishment of three canonical EOTC chanting modes during the 6th century. This paper attempts to investigate the EOTC chants using music information retrieval (MIR) techniques. Among the research questions regarding the analysis and understanding of EOTC chants, Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's standards, is of primary importance. Therefore, we consider the task of Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset and showcasing a series of classification experiments for this task. Results show that using the distribution of stabilized pitch contours as the feature representation on a simple neural-network-based classifier becomes an effective solution. The musicological implications and insights of such results are further discussed through a comparative study with the previous ethnomusicology literature on EOTC chants. By making this dataset publicly accessible, our aim is to promote future exploration and analysis of EOTC chants and highlight potential directions for further research, thereby fostering a deeper understanding and preservation of this unique spiritual and cultural heritage.",TWN,education,Developing economies,"[10.840707, 1.0266964]","[-0.25202248, -22.900707]","[12.954355, -0.65397143, -21.020075]","[-0.67936015, 11.635344, 1.6601187]","[11.18107, 10.211066]","[8.880884, 2.4120991]","[11.93772, 15.273225, -1.0361135]","[9.663085, 6.898004, 11.498466]"
82,Ondřej Cífka;Hendrik Schreiber;Luke Miner;Fabian-Robert Stöter,Lyrics Transcription for Humans: A Readability-Aware Benchmark,2024,https://doi.org/10.5281/zenodo.14877443,Ondřej Cífka+AudioShake>USA>company;Hendrik Schreiber+AudioShake>USA>company;Luke Miner+AudioShake>USA>company;Fabian-Robert Stöter+AudioShake>USA>company,"Writing down lyrics for human consumption involves not only accurately capturing word sequences, but also incorporating punctuation and formatting for clarity and to convey contextual information. This includes song structure, emotional emphasis, and contrast between lead and background vocals. While automatic lyrics transcription (ALT) systems have advanced beyond producing unstructured strings of words and are able to draw on wider context, ALT benchmarks have not kept pace and continue to focus exclusively on words. To address this gap, we introduce Jam-ALT, a comprehensive lyrics transcription benchmark. The benchmark features a complete revision of the JamendoLyrics dataset, in adherence to industry standards for lyrics transcription and formatting, along with evaluation metrics designed to capture and assess the lyric-specific nuances, laying the foundation for improving the readability of lyrics. We apply the benchmark to recent transcription systems and present additional error analysis, as well as an experimental comparison with a classical music dataset.",USA,company,Developed economies,"[-27.152187, -32.66426]","[33.182804, -15.651251]","[11.489943, 22.13917, -0.48092738]","[3.5533733, 11.208841, 4.559048]","[11.243565, 11.819782]","[9.91832, 3.3681328]","[12.240715, 15.80487, 1.1352153]","[10.666408, 7.1168656, 9.466446]"
83,Owen Green;Bob L. T. Sturm;Georgina Born;Melanie Wald-Fuhrmann,A Critical Survey of Research in Music Genre Recognition,2024,https://doi.org/10.5281/zenodo.14877445,Owen Green+Max Planck Institute for Empirical Aesthetics>DEU>facility;Bob L. T. Sturm+KTH>SWE>education;Georgina Born+University College London>GBR>education;Melanie Wald-Fuhrmann+Max Planck Institute for Empirical Aesthetics>DEU>facility,"This paper surveys 560 publications about music genre recognition (MGR) published between 2013–2022, complementing the comprehensive survey of [474], which covered the time frame 1995–2012 (467 publications). For each publication we determine its main functions: a review of research, a contribution to evaluation methodology, or an experimental work. For each experimental work we note the data, experimental approach, and figure of merit it applies. We also note the extents to which any publication engages with work critical of MGR as a research problem, as well as genre theory. Our bibliographic analysis shows for MGR research: 1) it typically does not meaningfully engage with any critique of itself; and 2) it typically does not meaningfully engage with work in genre theory.",DEU,facility,Developed economies,"[-30.135607, -11.776305]","[27.811827, -1.3812834]","[-17.704823, 6.813074, 16.504862]","[12.737582, 12.594504, 1.9612905]","[13.031674, 10.845254]","[10.868868, 3.3153884]","[14.007823, 14.272836, 1.3324454]","[12.518828, 6.0921054, 11.153646]"
84,Liwei Lin;Gus Xia;Junyan Jiang;Yixiao Zhang,Content-Based Controls for Music Large Language Modeling,2024,https://doi.org/10.5281/zenodo.14877447,"Liwei Lin+Music X Lab, New York University Shanghai>USA>education|Mohamed bin Zayed University of Artificial Intelligence>ARE>education;Gus Xia+Music X Lab, New York University Shanghai>USA>education|Mohamed bin Zayed University of Artificial Intelligence>ARE>education;Junyan Jiang+Music X Lab, New York University Shanghai>USA>education|Mohamed bin Zayed University of Artificial Intelligence>ARE>education;Yixiao Zhang+C4DM, Queen Mary University of London>GBR>education","Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with low-resource semi-supervised learning. We fine-tune the model with less than 4% of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online.",USA,education,Developed economies,"[19.005589, 7.745105]","[-16.583168, -41.355064]","[-0.64852864, 4.4512095, 25.42989]","[-19.275534, -6.501226, -12.686567]","[10.594709, 8.3388405]","[9.201648, 5.6909313]","[13.683285, 12.084641, 0.008225003]","[10.314883, 5.855192, 8.895403]"
85,Marcel A. Vélez Vásquez;Charlotte Pouw;John Ashley Burgoyne;Willem Zuidema,Exploring the Inner Mechanisms of Large Generative Music Models,2024,https://doi.org/10.5281/zenodo.14877449,"Marcel A. Vélez Vásquez+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education;Charlotte Pouw+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education;John Ashley Burgoyne+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education;Willem Zuidema+Institute for Logic, Language and Computation, University of Amsterdam>NLD>education","GGenerative models are starting to become very good at generating realistic text, images, and even music. Identifying how exactly these models conceptualize data has become crucial. To date, however, interpretability research has mainly focused on the text and image domain, leaving a gap in the music domain. In this paper, we investigate the transferability of straightforward text-oriented interpretability techniques to the music domain. Specifically, we examine the usability of these techniques for analyzing how the generative music model MusicGen constructs representations of human-interpretable musicological concepts. Using the DecoderLens, we gain insight into how the model gradually composes these concepts, and using interchange interventions, we observe the contributions of individual model components in generating the sound of specific instruments and genres. We also encounter several shortcomings of the interpretability techniques for the music domain, which underscore the complexity of music and need for proper audio-oriented adaptation. Our research marks an initial step toward understanding generative music models, fundamentally, paving the way for future advancements in controlling music generation.",NLD,education,Developed economies,"[19.8182, 2.2999196]","[-2.5779023, -35.737156]","[2.029961, 2.5967758, 18.61261]","[-17.402445, 10.310518, -0.67266065]","[10.177413, 8.713711]","[9.422597, 5.755721]","[13.251106, 12.09164, 0.2679038]","[10.147886, 5.50279, 9.403231]"
86,Saebyul Park;Halla Kim;Jiye Jung;Juyong Park;Jeounghoon Kim;Juhan Nam,Quantitative Analysis of Melodic Similarity in Music Copyright Infringement Cases,2024,https://doi.org/10.5281/zenodo.14877451,Saebyul Park+KAIST>KOR>education|Heinrich Heine University Düsseldorf>DEU>education;Halla Kim+KAIST>KOR>education;Jiye Jung+Heinrich Heine University Düsseldorf>DEU>education;Juyong Park+KAIST>KOR>education;Jeounghoon Kim+KAIST>KOR>education;Juhan Nam+KAIST>KOR>education,"This study aims to measure the similarity of melodies objectively using natural language processing (NLP) techniques. We utilize Mel2word which is a melody tokenization method based on byte-pair encoding to facilitate the semantic analysis of melodies. In addition, we apply two word weighting methods: the modified Tversky measure for word salience and the TF-IDF method for word importance and uniqueness, to better understand the characteristics of each melodic element. We validate our approach by comparing song vectors calculated from an average of Mel2Word vectors to the ground truth in 108 cases of music copyright infringement, sourced from an extensive review of legal documents from law archives. The results demonstrate that the proposed approach is more in accordance with court rulings and perceptual similarity.",KOR,education,Developing economies,"[-2.5890422, 19.117373]","[18.900566, 4.1205664]","[-0.9566922, 3.2928815, 3.5657384]","[9.035391, 2.416426, 5.1746783]","[12.465561, 9.695492]","[9.6028805, 1.9173173]","[12.98895, 15.558313, -0.8890482]","[11.43116, 6.9847417, 13.0311165]"
87,Hendrik Vincent Koops;Gianluca Micchi;Elio Quinton,Robust Lossy Audio Compression Identification,2024,https://doi.org/10.5281/zenodo.14877453,Hendrik Vincent Koops+Universal Music Group>GBR>company;Gianluca Micchi+Universal Music Group>GBR>company;Elio Quinton+Universal Music Group>GBR>company,"Previous research contributions on blind lossy compression identification report near perfect performance metrics on their test set, across a variety of codecs and bit rates. However, we show that such results can be deceptive and may not accurately represent true ability of the system to tackle the task at hand. In this article, we present an investigation into the robustness and generalisation capability of a lossy audio identification model. Our contributions are as follows. (1) We show the lack of robustness to codec parameter variations of a model equivalent to prior art. In particular, when naively training a lossy compression detection model on a dataset of music recordings processed with a range of codecs and their lossless counterparts, we obtain near perfect performance metrics on the held-out test set, but severely degraded performance on lossy tracks produced with codec parameters not seen in training. (2) We propose and show the effectiveness of an improved training strategy to significantly increase the robustness and generalisation capability of the model beyond codec configurations seen during training. Namely we apply a random mask to the input spectrogram to encourage the model not to rely solely on the training set's codec cutoff frequency.",GBR,company,Developed economies,"[-12.820666, -23.56633]","[20.600508, -18.678837]","[8.070595, -8.316289, -20.131004]","[19.636118, -1.2833213, -7.0262623]","[9.205653, 4.8342075]","[9.710723, 2.9831774]","[10.882477, 11.707954, -1.6369362]","[11.195337, 6.7006583, 11.056365]"
88,Malcolm Sailor,RNBert: Fine-Tuning a Masked Language Model for Roman Numeral Analysis,2024,https://doi.org/10.5281/zenodo.14877455,Malcolm Sailor+Yale University>USA>education,"Music is plentiful, but labeled data for music theory tasks like roman numeral analysis is scarce. Self-supervised pre-training is therefore a promising avenue for improving performance on these tasks, especially because, in learning a task like predicting masked notes, a model may acquire latent representations of music theory concepts like keys and chords. However, existing models for roman numeral analysis have not used pretraining, instead training from scratch on labeled data, while conversely, pretrained models for music understanding have generally been applied to sequence-level tasks requiring little explicit music theory, such as composer classification. In contrast, this paper applies pretraining methods to a music theory task by fine-tuning a masked language model, MusicBERT, for roman numeral analysis. We apply token classification to get a chord label for each note and then aggregate the predictions of simultaneous notes to achieve a single label at each time step. The resulting model substantially outperforms previous roman numeral analysis models. Our approach can readily be extended to other note- and/or chord- level music theory tasks (e.g., nonharmonic tone analysis, melody harmonization).",USA,education,Developed economies,"[23.967573, 17.456673]","[-9.289453, -33.6052]","[-4.348265, -17.585615, 20.222048]","[-16.768702, -3.2772515, -12.536105]","[11.338759, 6.7796936]","[8.841812, 5.624245]","[13.235341, 12.007915, -1.3269963]","[9.778719, 5.9194746, 9.326107]"
89,Benno Weck;Ilaria Manco;Emmanouil Benetos;Elio Quinton;George Fazekas;Dmitry Bogdanov,MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models,2024,https://doi.org/10.5281/zenodo.14877459,Benno Weck+Universitat Pompeu Fabra>ESP>education|Queen Mary University of London>GBR>education|Universal Music Group>USA>company;Ilaria Manco+Queen Mary University of London>GBR>education;Emmanouil Benetos+Queen Mary University of London>GBR>education;Elio Quinton+Universal Music Group>USA>company;George Fazekas+Queen Mary University of London>GBR>education;Dmitry Bogdanov+Universitat Pompeu Fabra>ESP>education,"Multimodal models that jointly process audio and language hold great promise in audio understanding and are increasingly being adopted in the music domain. By allowing users to query via text and obtain information about a given audio input, these models have the potential to enable a variety of music understanding tasks via language-based interfaces. However, their evaluation poses considerable challenges, and it remains unclear how to effectively assess their ability to correctly interpret music-related inputs with current methods. Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music understanding in multimodal language models focused on audio. MuChoMusic comprises 1,187 multiple-choice questions, all validated by human annotators, on 644 music tracks sourced from two publicly available music datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to assess knowledge and reasoning abilities across several dimensions that cover fundamental musical concepts and their relation to cultural and functional contexts. Through the holistic analysis afforded by the benchmark, we evaluate five open-source models and identify several pitfalls, including an over-reliance on the language modality, pointing to a need for better multimodal integration. Data and code are open-sourced.",ESP,education,Developed economies,"[-19.017878, -1.8420258]","[39.502262, -8.524908]","[-1.002481, 16.255533, 15.102034]","[20.604063, 17.644512, -1.1171051]","[12.823897, 8.537453]","[9.803443, 4.8811316]","[13.749924, 13.7074795, -0.7427391]","[11.025335, 6.2418265, 9.386857]"
90,Sujoy Roychowdhury;Preeti Rao;Sharat Chandran,Human Pose Estimation for Expressive Movement Descriptors in Vocal Musical Performances,2024,https://doi.org/10.5281/zenodo.14877458,Sujoy Roychowdhury+Indian Institute of Technology Bombay>IND>education;Preeti Rao+Indian Institute of Technology Bombay>IND>education;Sharat Chandran+Indian Institute of Technology Bombay>IND>education,"Vocal concerts in Indian music are invariably associated with the performers' hand gesticulations that are believed to convey emotion, music semantics as well as the individual style of the performers. Video recordings, with one or more cameras, along with markerless human pose estimation algorithms can be employed to capture such movements, and thus potentially solve music information retrieval (MIR) queries. Nevertheless, off-the-shelf algorithms are built for the most part for upright human configurations contrasting with seated positions in Indian vocal concerts and the upper body movements in the context of performing music. Current state-of-the-art algorithms are black box neural network based and this calls for an investigation of the components of such algorithms. Key decisions involve the choice of one or more cameras, the choice of 2D or 3D features, and relevant parameters such as confidence thresholds in common machine learning methods. In this paper, we quantify the increase in the performance with three cameras on two music information retrieval tasks. We offer insights for single and multi-view processing of videos.",IND,education,Developing economies,"[-6.8524914, -32.26496]","[6.7350225, -20.195992]","[16.78779, 8.357268, -14.273808]","[18.360731, -10.232776, -5.1930027]","[10.190673, 11.031388]","[7.4286947, 1.2757019]","[11.345579, 15.057144, 0.4960994]","[9.113077, 7.034473, 12.4558325]"
91,Seokbeom Park;Hyunjae Kim;Kyung Myun Lee,Enhancing Predictive Models of Music Familiarity With EEG: Insights From Fans and Non-Fans of K-Pop Group NCT127,2024,https://doi.org/10.5281/zenodo.14877461,Seokbeom Park+KAIST>KOR>education;Hyunjae Kim+KAIST>KOR>education;Kyung Myun Lee+KAIST>KOR>education,"Predicting a listener's experience of music based solely on audio features has its limitations due to the individual variability in responses to the same music. This study examines the effectiveness of electroencephalogram (EEG) in predicting the subjective experiences while listening to music, including arousal, valence, familiarity, and preference. We collected EEG data alongside subjective ratings of arousal, valence, familiarity, and preference from both fans (N=20) and non-fans (N=34) of the K-pop idol group, NCT127 to investigate response variability to the same NCT127 music. Our analysis focused on determining whether the inclusion of EEG alongside audio features could enhance the predictive power of linear mixed-effect models for these subjective ratings. Specifically, we employed stimulus-response correlation (SRC), a recent approach in neuroscience correlating stimulus features with EEG responses to the ecologically valid stimuli. The results showed that familiarity and preference was significantly higher in the fan group. Furthermore, the inclusion of SRC significantly enhanced the prediction of familiarity compared to models based solely on audio features. However, the impact of SRC on predictions of arousal and valence exhibited variation depending on the correlated audio features, with certain SRCs improving predictions while others diminished them. For preference, only a few SRCs negatively affected model performance. These results suggest that correlations of EEG responses and audio features can provide information of individual listeners' subjective responses, particularly in predicting familiarity.",KOR,education,Developing economies,"[-49.27627, -7.936073]","[48.34676, -14.601606]","[-11.976694, 23.54501, 18.370872]","[1.3159631, 18.293312, 9.735337]","[13.618647, 9.25102]","[11.158632, 4.3649535]","[14.318889, 15.252311, -0.9439437]","[11.981444, 4.728459, 10.558741]"
92,Robert Sowula;Peter Knees,Mosaikbox: Improving Fully Automatic DJ Mixing Through Rule-Based Stem Modification and Precise Beat-Grid Estimation,2024,https://doi.org/10.5281/zenodo.14877463,Robert Sowula+TU Wien>AUT>education;Peter Knees+TU Wien>AUT>education,"We present a novel system for automatic music mixing combining diverse music information retrieval (MIR) techniques and sources for song selection and transitioning. Specifically, we explore how music source separation and stem analysis can contribute to the task of music similarity calculation by modifying incompatible stems using a rule-based approach and investigate how audio-based similarity measures can be supplemented by lyrics as contextual information to capture more aspects of music. Additionally, we propose a novel approach for tempo detection, outperforming state-of-the-art techniques in low error-tolerance windows. We evaluate our approaches using a listening experiment and compare them to a state-of-the-art model as a baseline. The results show that our approach to automatic song selection and automated music mixing significantly outperforms the baseline and that our rule-based stem removal approach significantly enhances the perceived quality of a mix. No improvement can be observed for the inclusion of contextual information, i.e., mood information derived from lyrics, into the music similarity measure.",AUT,education,Developed economies,"[18.03608, -39.687416]","[-5.478318, 8.029643]","[9.466949, -26.233408, -14.314382]","[-0.320182, 3.3768375, 7.396546]","[9.940987, 4.6137013]","[9.572475, 2.627636]","[10.639465, 12.251792, -1.8918958]","[10.818048, 6.3319283, 11.097903]"
94,Stephen Ni-Hahn;Weihan Xu;Zirui Yin;Rico Zhu;Simon Mak;Yue Jiang;Cynthia Rudin,"A New Dataset, Notation Software, and Representation for Computational Schenkerian Analysis",2024,https://doi.org/10.5281/zenodo.14877467,Stephen Ni-Hahn+Duke University>USA>education;Weihan Xu+Duke University>USA>education;Jerry Yin+Duke University>USA>education;Rico Zhu+Duke University>USA>education;Simon Mak+Duke University>USA>education;Yue Jiang+Duke University>USA>education;Cynthia Rudin+Duke University>USA>education,"Schenkerian Analysis (SchA) is a uniquely expressive method of music analysis, combining elements of melody, harmony, counterpoint, and form to describe the hierarchical structure supporting a work of music. However, despite its powerful analytical utility and potential to improve music understanding and generation, SchA has rarely been utilized by the computer music community. This is in large part due to the paucity of available high-quality data in a computer-readable format. With a larger corpus of Schenkerian data, it may be possible to infuse machine learning models with a deeper understanding of musical structure, thus leading to more ""human"" results. To encourage further research in Schenkerian analysis and its potential benefits for music informatics and generation, this paper presents three main contributions: 1) a new and growing dataset of SchAs, the largest in human- and computer-readable formats to date (&gt;140 excerpts), 2) a novel software for visualization and collection of SchA data, and 3) a novel, flexible representation of SchA as a heterogeneous-edge graph data structure.",USA,education,Developed economies,"[21.7877, 30.42079]","[-10.075119, 20.057554]","[-13.11786, -15.915621, 9.955297]","[-10.94465, 0.5374262, 8.584034]","[11.873483, 6.7390556]","[8.6166315, 1.9842365]","[13.121288, 12.418569, -1.4911188]","[10.013714, 6.323502, 11.740453]"
61,Soumya Sai Vanka;Christian J. Steinmetz;Jean-Baptiste Rolland;Joshua D. Reiss;George Fazekas,Diff-MST: Differentiable Mixing Style Transfer,2024,https://doi.org/10.5281/zenodo.14877399,"Soumya Sai Vanka+Centre for Digital Music, Queen Mary University of London>GBR>education|Steinberg Media Technologies GmbH>DEU>company;Christian Steinmetz+Centre for Digital Music, Queen Mary University of London>GBR>education|Steinberg Media Technologies GmbH>DEU>company;Jean-Baptiste Rolland+Steinberg Media Technologies GmbH>DEU>company;Joshua Reiss+Centre for Digital Music, Queen Mary University of London>GBR>education;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education","Mixing style transfer automates the generation of a multi-track mix for a given set of tracks by inferring production attributes from a reference song. However, existing systems for mixing style transfer are limited in that they often operate only on a fixed number of tracks, introduce artifacts, and produce mixes in an end-to-end fashion, without grounding in traditional audio effects, prohibiting interpretability and controllability. To overcome these challenges, we introduce Diff-MST, a framework comprising a differentiable mixing console, a transformer controller, and an audio production style loss function. By inputting raw tracks and a reference song, our model estimates control parameters for audio effects within a differentiable mixing console, producing high-quality mixes and enabling post-hoc adjustments. Moreover, our architecture supports an arbitrary number of input tracks without source labelling, enabling real-world applications. We evaluate our model's performance against robust baselines and showcase the effectiveness of our approach, architectural design, tailored audio production style loss, and innovative training methodology for the given task. We provide code and listening examples online.",GBR,education,Developed economies,"[12.402683, -35.97727]","[-18.173374, -45.474415]","[28.286516, 5.826224, 7.91467]","[-20.673613, -0.05677075, -22.445997]","[9.528724, 8.625834]","[7.887706, 6.446172]","[12.601571, 12.243918, 0.7048779]","[9.690441, 6.1103644, 8.352822]"
