Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Abhishek Singhi;Daniel G. Brown 0001,"On Cultural, Textual and Experiential Aspects of Music Mood.",2014,https://doi.org/10.5281/zenodo.1417391,Abhishek Singhi+University of Waterloo>CAN>education;Daniel G. Brown+University of Waterloo>CAN>education,"We study the impact of the presence of lyrics on music mood perception for both Canadian and Chinese listeners by conducting a user study of Canadians not of Chinese origin, Chinese-Canadians, and Chinese people who have lived in Canada for fewer than three years. While our original hypotheses were largely connected to cultural components of mood perception, we also analyzed how stable mood assignments were when listeners could read the lyrics of recent popular English songs they were hearing versus when they only heard the songs. We also showed the lyrics of some songs to participants without playing the recorded music. We conclude that people assign different moods to the same song in these three scenarios. People tend to assign a song to the mood cluster that includes “melancholy” more often when they read the lyrics without listening to it, and having access to the lyrics does not help reduce the difference in music mood perception between Canadian and Chinese listeners significantly. Our results cause us to question the idea that songs have “inherent mood”. Rather, we suggest that the mood depends on both cultural and experiential context."
1,Li Su;Li-Fan Yu;Yi-Hsuan Yang,"Sparse Cepstral, Phase Codes for Guitar Playing Technique Classification.",2014,https://doi.org/10.5281/zenodo.1417215,Li Su+Academia Sinica>TWN>education;Li-Fan Yu+Academia Sinica>TWN>education;Yi-Hsuan Yang+Academia Sinica>TWN>education,"Automatic recognition of guitar playing techniques is challenging as it is concerned with subtle nuances of guitar timbres. In this paper, we investigate this research problem by a comparative study on the performance of features extracted from the magnitude spectrum, cepstrum and phase derivatives such as group-delay function (GDF) and instantaneous frequency deviation (IFD) for classifying the playing techniques of electric guitar recordings. We consider up to 7 distinct playing techniques of electric guitar and create a new individual-note dataset comprising of 7 types of guitar tones for each playing technique. The dataset contains 6,580 clips and 11,928 notes. Our evaluation shows that sparse coding is an effective means of mining useful patterns from the primitive time-frequency representations and that combining the sparse representations of logarithm cepstrum, GDF and IFD leads to the highest average F-score of 71.7%. Moreover, from analyzing the confusion matrices we find that cepstral and phase features are particularly important in discriminating highly similar techniques such as pull-off, hammer-on and bending. We also report a preliminary study that demonstrates the potential of the proposed methods in automatic transcription of real-world electric guitar solos."
2,Münevver Köküer;Peter Jancovic;Islah Ali-MacLachlan;Cham Athwal,Automated Detection of Single- and Multi-Note Ornaments in Irish Traditional Flute Playing.,2014,https://doi.org/10.5281/zenodo.1415266,Münevver Köküer+Birmingham City University>GBR>education|University of Birmingham>GBR>education;Peter Jančovič+Birmingham City University>GBR>education|University of Birmingham>GBR>education;Islah Ali-MacLachlan+Birmingham City University>GBR>education|University of Birmingham>GBR>education;Cham Athwal+Birmingham City University>GBR>education|University of Birmingham>GBR>education,"This paper presents an automatic system for the detection of single- and multi-note ornaments in Irish traditional flute playing. This is a challenging problem because ornaments are notes of a very short duration. The presented ornament detection system is based on first detecting onsets and then exploiting the knowledge of musical ornamentation. We employed onset detection methods based on signal envelope and fundamental frequency and customised their parameters to the detection of soft onsets of possibly short duration. Single-note ornaments are detected based on the duration and pitch of segments, determined by adjacent onsets. Multi-note ornaments are detected based on analysing the sequence of segments. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD, which was manually annotated by an experienced flute player. The onset and single- and multi-note ornament detection performance is presented in terms of the precision, recall and F-measure."
3,Bob L. Sturm;Nick Collins,The Kiki-Bouba Challenge: Algorithmic Composition for Content-based MIR Research and Development.,2014,https://doi.org/10.5281/zenodo.1416364,Bob L. Sturm+Aalborg University>DNK>education;Nick Collins+Durham University>GBR>education,"We propose the “Kiki-Bouba Challenge” (KBC) for the research and development of content-based music information retrieval (MIR) systems. This challenge is unencumbered by several problems typically encountered in MIR research: insufficient data, restrictive copyrights, imperfect ground truth, a lack of specific criteria for classes (e.g., genre), a lack of explicit problem definition, and irreproducibility. KBC provides a limitless amount of free data, a perfect ground truth, and well-specifiable and meaningful characteristics defining each class. These ideal conditions are made possible by open source algorithmic composition — a hitherto under-exploited resource for MIR."
4,Aäron van den Oord;Sander Dieleman;Benjamin Schrauwen,Transfer Learning by Supervised Pre-training for Audio-based Music Classification.,2014,https://doi.org/10.5281/zenodo.1415890,A¨aron van den Oord+Ghent University>BEL>education;Sander Dieleman+Ghent University>BEL>education;Benjamin Schrauwen+Ghent University>BEL>education,"Very few large-scale music research datasets are publicly available. There is an increasing need for such datasets, because the shift from physical to digital distribution in the music industry has given the listener access to a large body of music, which needs to be cataloged efficiently and be easily browsable. Additionally, deep learning and feature learning techniques are becoming increasingly popular for music information retrieval applications, and they typically require large amounts of training data to work well. In this paper, we propose to exploit an available large-scale music dataset, the Million Song Dataset (MSD), for classification tasks on other datasets, by reusing models trained on the MSD for feature extraction. This transfer learning approach, which we refer to as supervised pre-training, was previously shown to be very effective for computer vision problems. We show that features learned from MSD audio fragments in a supervised manner, using tag labels and user listening data, consistently outperform features learned in an unsupervised manner in this setting, provided that the learned feature extractor is of limited complexity. We evaluate our approach on the GTZAN, 1517-Artists, Unique and Magnatagatune datasets."
5,Harald Grohganz;Michael Clausen;Meinard Müller,Estimating Musical Time Information from Performed MIDI Files.,2014,https://doi.org/10.5281/zenodo.1417925,Harald Grohganz+Bonn University>DEU>education;Michael Clausen+Bonn University>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Even though originally developed for exchanging control commands between electronic instruments, MIDI has been used as quasi standard for encoding and storing score-related parameters. MIDI allows for representing musical time information as specified by sheet music as well as physical time information that reflects performance aspects. However, in many of the available MIDI files the musical beat and tempo information is set to a preset value with no relation to the actual music content. In this paper, we introduce a procedure to determine the musical beat grid from a given performed MIDI file. As one main contribution, we show how the global estimate of the time signature can be used to correct local errors in the pulse grid estimation. Different to MIDI quantization, where one tries to map MIDI note onsets onto a given musical pulse grid, our goal is to actually estimate such a grid. In this sense, our procedure can be used in combination with existing MIDI quantization procedures to convert performed MIDI files into semantically enriched score-like MIDI files."
6,Isabel Barbancho;George Tzanetakis;Lorenzo J. Tardón;Peter F. Driessen;Ana M. Barbancho,Estimation of the Direction of Strokes and Arpeggios.,2014,https://doi.org/10.5281/zenodo.1418053,Isabel Barbancho+Universidad de Málaga>ESP>education|University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Peter F. Driessen+University of Victoria>CAN>education;Ana M. Barbancho+Universidad de Málaga>ESP>education,"Whenever a chord is played in a musical instrument, the notes are not commonly played at the same time. Actually, in some instruments, it is impossible to trigger multiple notes simultaneously. In others, the player can consciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke). In this paper, we describe a system to automatically estimate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analysis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direction, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ."
7,Sam van Herwaarden;Maarten Grachten;W. Bas de Haas,Predicting Expressive Dynamics in Piano Performances using Neural Networks.,2014,https://doi.org/10.5281/zenodo.1416678,Sam van Herwaarden+Austrian Research Institute for AI>AUT>facility;Maarten Grachten+Austrian Research Institute for AI>AUT>facility;W. Bas de Haas+Utrecht University>NLD>education,"This paper presents a model for predicting expressive accentuation in piano performances with neural networks. Using Restricted Boltzmann Machines (RBMs), features are learned from performance data, after which these features are used to predict performed loudness. During feature learning, data describing more than 6000 musical pieces is used; when training for prediction, two datasets are used, both recorded on a Bösendorfer piano (accurately measuring note on- and offset times and velocity values), but describing different compositions performed by different pianists. The resulting model is tested by predicting note velocity for unseen performances. Our approach differs from earlier work in a number of ways: (1) an additional input representation based on a local history of velocity values is used, (2) the RBMs are trained to result in a network with sparse activations, (3) network connectivity is increased by adding skip-connections, and (4) more data is used for training. These modifications result in a network performing better than the state-of-the-art on the same data and more descriptive features, which can be used for rendering performances, or for gaining insight into which aspects of a musical piece influence its performance."
8,Siddharth Sigtia;Emmanouil Benetos;Srikanth Cherla;Tillman Weyde;Artur S. d'Avila Garcez;Simon Dixon,An RNN-based Music Language Model for Improving Automatic Music Transcription.,2014,https://doi.org/10.5281/zenodo.1416792,"Siddharth Sigtia+Centre for Digital Music, Queen Mary University of London>GBR>education;Emmanouil Benetos+City University London>GBR>education;Srikanth Cherla+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Artur S. d’Avila Garcez+City University London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a significant 3% improvement in terms of F-measure, when compared to using an acoustic-only model."
9,Mathieu Giraud;Florence Levé;Florent Mercier;Marc Rigaudière;Donatien Thorez,Towards Modeling Texture in Symbolic Data.,2014,https://doi.org/10.5281/zenodo.1415030,"Mathieu Giraud+LIFL, CNRS>FRA>facility|University of Lille 1>FRA>education;Florence Léve+MIS, UPJV>FRA>education|LIFL, University of Lille 1>FRA>facility;Florent Mercier+University of Lille 1>FRA>education;Marc Rigaudière+University of Lorraine>FRA>education;Donatien Thorez+University of Lille 1>FRA>education","Studying texture is a part of many musicological analyses. The change of texture plays an important role in the cognition of musical structures. Texture is a feature commonly used to analyze musical audio data, but it is rarely taken into account in symbolic studies. We propose to formalize the texture in classical Western instrumental music as melody and accompaniment layers, and provide an algorithm able to detect homorhythmic layers in polyphonic data where voices are not separated. We present an evaluation of these methods for parallel motions against a ground truth analysis of ten instrumental pieces, including the first movements of the six quatuors op. 33 by Haydn."
10,Nadine Kroher;Emilia Gómez;Catherine Guastavino;Francisco Gómez 0001;Jordi Bonada,Computational Models for Perceived Melodic Similarity in A Cappella Flamenco Singing.,2014,https://doi.org/10.5281/zenodo.1416306,N. Kroher+Universitat Pompeu Fabra>ESP>education|McGill University>CAN>education|Technical University of Madrid>ESP>education;E. Gómez+Universitat Pompeu Fabra>ESP>education|McGill University>CAN>education|Technical University of Madrid>ESP>education;C. Guastavino+McGill University>CAN>education;F. Gómez+Technical University of Madrid>ESP>education;J. Bonada+Universitat Pompeu Fabra>ESP>education,"The present study investigates the mechanisms involved in the perception of melodic similarity in the context of a cappella flamenco singing performances. Flamenco songs belonging to the same style are characterized by a common melodic skeleton, which is subject to spontaneous improvisation containing strong prolongations and ornamentations. For our research we collected human similarity judgements from naïve and expert listeners who listened to audio recordings of a cappella flamenco performances as well as synthesized versions of the same songs. We furthermore calculated distances from manually extracted high-level descriptors defined by flamenco experts. The suitability of a set of computational melodic similarity measures was evaluated by analyzing the correlation between computed similarity and human ratings. We observed significant differences between listener groups and stimuli types. Furthermore, we observed a high correlation between human ratings and similarities computed from features from flamenco experts. We also observed that computational models based on temporal deviation, dynamics and ornamentation are better suited to model perceived similarity for this material than models based on chroma distance."
11,Christopher Antila;Julie Cumming,The VIS Framework: Analyzing Counterpoint in Large Datasets.,2014,https://doi.org/10.5281/zenodo.1417767,Christopher Antila+McGill University>CAN>education;Julie Cumming+McGill University>CAN>education,"The VIS Framework for Music Analysis is a modular Python library designed for “big data” queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strategy in VIS, and the flexibility of this approach for future researchers."
12,Yoonchang Han;Kyogu Lee,Hierarchical Approach to Detect Common Mistakes of Beginner Flute Players.,2014,https://doi.org/10.5281/zenodo.1415878,Yoonchang Han+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education,"Music lessons are a repetitive process of giving feedback on a student’s performance techniques. The manner in which performance skills are improved depends on the particular instrument, and therefore, it is important to consider the unique characteristics of the target instrument. In this paper, we investigate the common mistakes of beginner flute players and propose a hierarchical approach to detect such mistakes. We first examine the structure and mechanism of the flute, and define several types of common mistakes that can be caused by incorrect assembly, poor blowing skills, or mis-fingering. We propose tailored algorithms for detecting each case by combining deterministic signal processing and deep learning, to quantify the quality of a flute sound. The system is structured hierarchically, as mis-fingering detection requires the input sound to be correctly assembled and blown to discriminate minor sound difference. Experimental results show that it is possible to identify different mistakes in flute performance using our proposed algorithms."
13,Siying Wang;Sebastian Ewert;Simon Dixon,Robust Joint Alignment of Multiple Versions of a Piece of Music.,2014,https://doi.org/10.5281/zenodo.1416382,Siying Wang+Queen Mary University of London>GBR>education;Sebastian Ewert+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Large music content libraries often comprise multiple versions of a piece of music. To establish a link between different versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Due to the leeway in interpreting a piece, any two versions can differ significantly, for example, in terms of local tempo, articulation, or playing style. For a given pair of versions, these differences can be significant such that even state-of-the-art methods fail to identify a correct alignment. In this paper, we present a novel method that increases the robustness for difficult to align cases. Instead of aligning only pairs of versions as done in previous methods, our method aligns multiple versions in a joint manner. This way, the alignment can be computed by comparing each version not only with one but with several versions, which stabilizes the comparison and leads to an increase in alignment robustness. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state-of-the-art method, with significantly less outliers (standard deviation 53% lower)."
14,Bob L. Sturm;Rolf Bardeli;Thibault Langlois;Valentin Emiya,Formalizing the Problem of Music Description.,2014,https://doi.org/10.5281/zenodo.1414914,Bob L. Sturm+Aalborg University>DNK>education;Rolf Bardeli+Fraunhofer IAIS>DEU>facility;Thibault Langlois+Lisbon University>PRT>education;Valentin Emiya+Aix-Marseille Université>FRA>education,"The lack of a formalism for “the problem of music description” results in, among other things: ambiguity in what problem a music description system must address, how it should be evaluated, what criteria define its success, and the paradox that a music description system can reproduce the “ground truth” of a music dataset without attending to the music it contains. To address these issues, we formalize the problem of music description such that all elements of an instance of it are made explicit. This can thus inform the building of a system, and how it should be evaluated in a meaningful way. We provide illustrations of this formalism applied to three examples drawn from the literature."
15,Tom Arjannikov;John Z. Zhang,An Association-based Approach to Genre Classification in Music.,2014,https://doi.org/10.5281/zenodo.1415786,Tom Arjannikov+University of Lethbridge>CAN>education;John Z. Zhang+University of Lethbridge>CAN>education,"Music Information Retrieval (MIR) is a multi-disciplinary research area that aims to automate the access to large-volume music data, including browsing, retrieval, storage, etc. The work that we present in this paper tackles a non-trivial problem in the field, namely music genre classification, which is one of the core tasks in MIR. In our proposed approach, we make use of association analysis to study and predict music genres based on the acoustic features extracted directly from music. In essence, we build an associative classifier, which finds inherent associations between content-based features and individual genres and then uses them to predict the genre(s) of a new music piece. We demonstrate the feasibility of our approach through a series of experiments using two publicly available music datasets. One of them is the largest available in MIR and contains real world data, while the other has been widely used and provides a good benchmarking basis. We show the effectiveness of our approach and discuss various related issues. In addition, due to its associative nature, our classifier can assign multiple genres to a single music piece; hopefully this would offer insights into the prevalent multi-label situation in genre classification."
16,Srikanth Cherla;Tillman Weyde;Artur S. d'Avila Garcez,Multiple Viewpiont Melodic Prediction with Fixed-Context Neural Networks.,2014,https://doi.org/10.5281/zenodo.1416944,Srikanth Cherla+City University London>GBR>education;Tillman Weyde+City University London>GBR>education;Artur d’Avila Garcez+City University London>GBR>education,"The multiple viewpoints representation is an event-based representation of symbolic music data which offers a means for the analysis and generation of notated music. Previous work using this representation has predominantly relied on n-gram and variable order Markov models for music sequence modelling. Recently the efficacy of a class of distributed models, namely restricted Boltzmann machines, was demonstrated for this purpose. In this paper, we demonstrate the use of two neural network models which use fixed-length sequences of various viewpoint types as input to predict the pitch of the next note in the sequence. The predictive performance of each of these models is comparable to that of models previously evaluated on the same task. We then combine the predictions of individual models using an entropy-weighted combination scheme to improve the overall prediction performance, and compare this with the predictions of a single equivalent model which takes as input all the viewpoint types of each of the individual models in the combination."
17,Laurent Pugin;Rodolfo Zitellini;Perry Roland,Verovio: A library for Engraving MEI Music Notation into SVG.,2014,https://doi.org/10.5281/zenodo.1417589,Laurent Pugin+Swiss RISM Office>CHE>facility;Rodolfo Zitellini+Swiss RISM Office>CHE>facility;Perry Roland+University of Virginia>USA>education,"Rendering symbolic music notation is a common component of many MIR applications, and many tools are available for this task. There is, however, a need for a tool that can natively render the Music Encoding Initiative (MEI) notation encodings that are increasingly used in music research projects. In this paper, we present Verovio, a library and toolkit for rendering MEI. A significant advantage of Verovio is that it implements MEI’s structure internally, making it the best suited solution for rendering features that make MEI unique. Verovio is designed as a fast, portable, lightweight tool written in pure standard C++ with no dependencies on third-party frameworks or libraries. It can be used as a command-line rendering tool, as a library, or it can be compiled to JavaScript using the Emscripten LLVM-to-JavaScript compiler. This last option is particularly interesting because it provides a complete in-browser music MEI typesetter. The SVG output from Verovio is organized in such a way that the MEI structure is preserved as much as possible. Since every graphic in SVG is an XML element that is easily addressable, Verovio is particularly well-suited for interactive applications, especially in web browsers. Verovio is available under the GPL open-source license."
18,Diego Furtado Silva;Rafael Geraldeli Rossi;Solange Oliveira Rezende;Gustavo Enrique De Almeida Prado Alves Batista,Music Classification by Transductive Learning Using Bipartite Heterogeneous Networks.,2014,https://doi.org/10.5281/zenodo.1418265,"Diego F. Silva+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Rafael G. Rossi+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Solange O. Rezende+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education;Gustavo E. A. P. A. Batista+Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo>BRA>education","The popularization of music distribution in electronic format has increased the amount of music with incomplete metadata. The incompleteness of data can hamper some important tasks, such as music and artist recommendation. In this scenario, transductive classification can be used to classify the whole dataset considering just few labeled instances. Usually transductive classification is performed through label propagation, in which data are represented as networks and the examples propagate their labels through their connections. Similarity-based networks are usually applied to model data as network. However, this kind of representation requires the definition of parameters, which significantly affect the classification accuracy, and presents a high cost due to the computation of similarities among all dataset instances. In contrast, bipartite heterogeneous networks have appeared as an alternative to similarity-based networks in text mining applications. In these networks, the words are connected to the documents which they occur. Thus, there is no parameter or additional costs to generate such networks. In this paper, we propose the use of the bipartite network representation to perform transductive classification of music, using a bag-of-frames approach to describe music signals. We demonstrate that the proposed approach outperforms other music classification approaches when few labeled instances are available."
19,Antti Laaksonen,Automatic Melody Transcription based on Chord Transcription.,2014,https://doi.org/10.5281/zenodo.1415218,Antti Laaksonen+University of Helsinki>FIN>education,"This paper focuses on automatic melody transcription in a situation where a chord transcription is already available. Given an excerpt of music in audio form and a chord transcription in symbolic form, the task is to create a symbolic melody transcription that consists of note onset times and pitches. We present an algorithm that divides the audio into segments based on the chord transcription, and then matches potential melody patterns to each segment. The algorithm uses chord information to favor melody patterns that are probable in the given harmony context. To evaluate the algorithm, we present a new ground truth dataset that consists of 1.5 hours of audio excerpts together with hand-made melody and chord transcriptions."
20,Marius Miron;Julio José Carabias-Orti;Jordi Janer,Audio-to-score Alignment at the Note Level for Orchestral Recordings.,2014,https://doi.org/10.5281/zenodo.1416150,Marius Miron+Universitat Pompeu Fabra>ESP>education;Julio José Carabias-Orti+Universitat Pompeu Fabra>ESP>education;Jordi Janer+Universitat Pompeu Fabra>ESP>education,"In this paper we propose an offline method for refining audio-to-score alignment at the note level in the context of orchestral recordings. State-of-the-art score alignment systems estimate note onsets with a low time resolution, and without detecting note offsets. For applications such as score-informed source separation we need a precise alignment at note level. Thus, we propose a novel method that refines alignment by determining the note onsets and offsets in complex orchestral mixtures by combining audio and image processing techniques. First, we introduce a note-wise pitch salience function that weighs the harmonic contribution according to the notes present in the score. Second, we perform image binarization and blob detection based on connectivity rules. Then, we pick the best combination of blobs, using dynamic programming. We finally obtain onset and offset times from the boundaries of the most salient blob. We evaluate our method on a dataset of Bach chorales, showing that the proposed approach can accurately estimate note onsets and offsets."
21,Matevz Pesek;Ales Leonardis;Matija Marolt,A Compositional Hierarchical Model for Music Information Retrieval.,2014,https://doi.org/10.5281/zenodo.1416084,Matevž Pesek+University of Ljubljana>SVN>education;Aleš Leonardis+University of Birmingham>GBR>education;Matija Marolt+University of Ljubljana>SVN>education,"This paper presents a biologically-inspired compositional hierarchical model for MIR. The model can be treated as a deep learning model, and poses an alternative to deep architectures based on neural networks. Its main features are generativeness and transparency that allow clear insight into concepts learned from the input music signals. The model consists of multiple layers, each is composed of a number of parts. The hierarchical nature of the model corresponds well with the hierarchical structures in music. Parts in lower layers correspond to low-level concepts (e.g. tone partials), while parts in higher layers combine lower-level representations into more complex concepts (tones, chords). The layers are unsupervisedly learned one-by-one from music signals. Parts in each layer are compositions of parts from previous layers based on statistical co-occurrences as the driving force of the learning process. We present the model’s structure and compare it to other deep architectures. A preliminary evaluation of the model’s usefulness for automated chord estimation and multiple fundamental frequency estimation tasks is provided. Additionally, we show how the model can be extended to event-based music processing, which is our final goal."
22,Brecht De Man;Brett Leonard;Richard L. King;Joshua D. Reiss,An Analysis and Evaluation of Audio Features for Multitrack Music Mixtures.,2014,https://doi.org/10.5281/zenodo.1416832,"Brecht De Man+Centre for Digital Music, Queen Mary University of London>GBR>education;Brett Leonard+The Graduate Program in Sound Recording, Schulich School of Music, McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology>CAN>facility;Richard King+The Graduate Program in Sound Recording, Schulich School of Music, McGill University>CAN>education|Centre for Interdisciplinary Research in Music Media and Technology>CAN>facility;Joshua D. Reiss+Centre for Digital Music, Queen Mary University of London>GBR>education","Mixing multitrack music is an expert task where characteristics of the individual elements and their sum are manipulated in terms of balance, timbre and positioning, to resolve technical issues and to meet the creative vision of the artist or engineer. In this paper we conduct a mixing experiment where eight songs are each mixed by eight different engineers. We consider a range of features describing the dynamic, spatial and spectral characteristics of each track, and perform a multidimensional analysis of variance to assess whether the instrument, song and/or engineer is the determining factor that explains the resulting variance, trend, or consistency in mixing methodology. A number of assumed mixing rules from literature are discussed in the light of this data, and implications regarding the automation of various mixing processes are explored. Part of the data used in this work is published in a new online multitrack dataset through which public domain recordings, mixes, and mix settings (DAW projects) can be shared."
23,Karthik Yadati;Martha Larson;Cynthia C. S. Liem;Alan Hanjalic,Detecting Drops in Electronic Dance Music: Content based approaches to a socially significant music event.,2014,https://doi.org/10.5281/zenodo.1417081,Karthik Yadati+Delft University of Technology>NLD>education;Martha Larson+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education;Alan Hanjalic+Delft University of Technology>NLD>education,"Electronic dance music (EDM) is a popular genre of music. In this paper, we propose a method to automatically detect the characteristic event in an EDM recording that is referred to as a drop. Its importance is reflected in the number of users who leave comments in the general neighborhood of drop events in music on online audio distribution platforms like SoundCloud. The variability that characterizes realizations of drop events in EDM makes automatic drop detection challenging. We propose a two-stage approach to drop detection that first models the sound characteristics during drop events and then incorporates temporal structure by zeroing in on a watershed moment. We also explore the possibility of using the drop-related social comments on the SoundCloud platform as weak reference labels to improve drop detection. The method is evaluated using data from SoundCloud. Performance is measured as the overlap between tolerance windows centered around the hypothesized and the actual drop. Initial experimental results are promising, revealing the potential of the proposed method for combining content analysis and social activity to detect events in music recordings."
24,Nikolay Glazyrin,Towards Automatic Content-Based Separation of DJ Mixes into Single Tracks.,2014,https://doi.org/10.5281/zenodo.1415566,Nikolay Glazyrin+Ural Federal University>RUS>education,"DJ mixes and radio show recordings constitute an important and underexploited music and data source. In this paper we try to approach the problem of separation of a continuous DJ mix into single tracks or timestamping a mix. Sharing some aspects with the task of structural segmentation, this problem has a number of distinctive features that make difficulties for structural segmentation algorithms designed to work with a single track. We use the information derived from spectrum data to separate tracks from each other. We show that the metadata that usually comes with DJ mixes can be exploited to improve the separation. An iterative algorithm that can consider both content-based data and user provided metadata is proposed and evaluated on a collection of freely available timestamped DJ mix recordings of various styles."
25,Rachel M. Bittner;Justin Salamon;Mike Tierney;Matthias Mauch;Chris Cannam;Juan Pablo Bello,MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research.,2014,https://doi.org/10.5281/zenodo.1417889,"Rachel Bittner+Music and Audio Research Lab, New York University>USA>education;Justin Salamon+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Mike Tierney+Music and Audio Research Lab, New York University>USA>education;Matthias Mauch+Centre for Digital Music, Queen Mary University of London>GBR>education;Chris Cannam+Centre for Digital Music, Queen Mary University of London>GBR>education;Juan Bello+Music and Audio Research Lab, New York University>USA>education","We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research."
26,Zheng Tang;Dawn A. A. Black,Melody Extraction from Polyphonic Audio of Western Opera: A Method based on Detection of the Singer's Formant.,2014,https://doi.org/10.5281/zenodo.1416714,Zheng Tang+University of Washington>USA>education;Dawn A. A. Black+Queen Mary University of London>GBR>education,"Current melody extraction approaches perform poorly on the genre of opera [1, 2]. The singer’s formant is defined as a prominent spectral-envelope peak around 3 kHz found in the singing of professional Western opera singers [3]. In this paper we introduce a novel melody extraction algorithm based on this feature for opera signals. At the front end, it automatically detects the singer’s formant according to the Long-Term Average Spectrum (LTAS). This detection function is also applied to the short-term spectrum in each frame to determine the melody. The Fan Chirp Transform (FChT) [4] is used to compute pitch salience as its high time-frequency resolution overcomes the difficulties introduced by vibrato. Subharmonic attenuation is adopted to handle octave errors which are common in opera vocals. We improve the FChT algorithm so that it is capable of correcting outliers in pitch detection. The performance of our method is compared to 5 state-of-the-art melody extraction algorithms on a newly created dataset and parts of the ADC2004 dataset. Our algorithm achieves an accuracy of 87.5% in singer’s formant detection. In the evaluation of melody extraction, it has the best performance in voicing detection (91.6%), voicing false alarm (5.3%) and overall accuracy (82.3%)."
27,Dawen Liang;John William Paisley;Dan Ellis,Codebook-based Scalable Music Tagging with Poisson Matrix Factorization.,2014,https://doi.org/10.5281/zenodo.1416120,Dawen Liang+Columbia University>USA>education;John Paisley+Columbia University>USA>education;Daniel P. W. Ellis+Columbia University>USA>education,"Automatic music tagging is an important but challenging problem within MIR. In this paper, we treat music tagging as a matrix completion problem. We apply the Poisson matrix factorization model jointly on the vector-quantized audio features and a “bag-of-tags” representation. This approach exploits the shared latent structure between semantic tags and acoustic codewords. Leveraging the recently-developed technique of stochastic variational inference, the model can tractably analyze massive music collections. We present experimental results on the CAL500 dataset and the Million Song Dataset for both annotation and retrieval tasks, illustrating the steady improvement in performance as more data is used."
28,Emmanouil Benetos;Roland Badeau;Tillman Weyde;Gaël Richard,Template Adaptation for Improving Automatic Music Transcription.,2014,https://doi.org/10.5281/zenodo.1418059,Emmanouil Benetos+City University London>GBR>education|Institut Mines-Télécom>FRA>education;Roland Badeau+Institut Mines-Télécom>FRA>education;Tillman Weyde+City University London>GBR>education;Gaël Richard+Institut Mines-Télécom>FRA>education,"In this work, we propose a system for automatic music transcription which adapts dictionary templates so that they closely match the spectral shape of the instrument sources present in each recording. Current dictionary-based automatic transcription systems keep the input dictionary fixed, thus the spectral shape of the dictionary components might not match the shape of the test instrument sources. By performing a conservative transcription pre-processing step, the spectral shape of detected notes can be extracted and utilized in order to adapt the template dictionary. We propose two variants for adaptive transcription, namely for single-instrument transcription and for multiple-instrument transcription. Experiments are carried out using the MAPS and Bach10 databases. Results in terms of multi-pitch detection and instrument assignment show that there is a clear and consistent improvement when adapting the dictionary in contrast with keeping the dictionary fixed."
29,Zhiyao Duan;David Temperley,Note-level Music Transcription by Maximum Likelihood Sampling.,2014,https://doi.org/10.5281/zenodo.1416534,Zhiyao Duan+University of Rochester>USA>education;David Temperley+University of Rochester>USA>education,"Note-level music transcription, which aims to transcribe note events (often represented by pitch, onset and offset times) from music audio, is an important intermediate step towards complete music transcription. In this paper, we present a note-level music transcription system, which is built on a state-of-the-art frame-level multi-pitch estimation (MPE) system. Preliminary note-level transcription achieved by connecting pitch estimates into notes often lead to many spurious notes due to MPE errors. In this paper, we propose to address this problem by randomly sampling notes in the preliminary note-level transcription. Each sample is a subset of all notes and is viewed as a note-level transcription candidate. We evaluate the likelihood of each candidate using the MPE model, and select the one with the highest likelihood as the final transcription. The likelihood treats notes in a transcription as a whole and favors transcriptions with less spurious notes. Experiments conducted on 110 pieces of J.S. Bach chorales with polyphony from 2 to 4 show that the proposed sampling scheme significantly improves the transcription performance from the preliminary approach. The proposed system also significantly outperforms two other state-of-the-art systems in both frame-level and note-level transcriptions."
30,Lucas Thompson;Simon Dixon;Matthias Mauch,Drum Transcription via Classification of Bar-Level Rhythmic Patterns.,2014,https://doi.org/10.5281/zenodo.1417341,"Lucas Thompson+Centre for Digital Music, Queen Mary University of London>GBR>education;Matthias Mauch+Centre for Digital Music, Queen Mary University of London>GBR>education;Simon Dixon+Centre for Digital Music, Queen Mary University of London>GBR>education","We propose a novel method for automatic drum transcription from audio that achieves the recognition of individual drums by classifying bar-level drum patterns. Automatic drum transcription has to date been tackled by recognising individual drums or drum combinations. In high-level tasks such as audio similarity, statistics of longer rhythmic patterns have been used, reflecting that musical rhythm emerges over time. We combine these two approaches by classifying bar-level drum patterns on sub-beat quantised timbre features using support vector machines. We train the classifier using synthesised audio and carry out a series of experiments to evaluate our approach. Using six different drum kits, we show that the classifier generalises to previously unseen drum kits when trained on the other five (80% accuracy). Measures of precision and recall show that even for incorrectly classified patterns many individual drum events are correctly transcribed. Tests on 14 acoustic performances from the ENST-Drums dataset indicate that the system generalises to real-world recordings. Limited by the set of learned patterns, performance is slightly below that of a comparable method. However, we show that for rock music, the proposed method performs as well as the other method and is substantially more robust to added polyphonic accompaniment."
31,Carlos Eduardo Cancino Chacón;Stefan Lattner;Maarten Grachten,Developing Tonal Perception through Unsupervised Learning.,2014,https://doi.org/10.5281/zenodo.1416058,Carlos Eduardo Cancino Chacón+Austrian Research Institute for Artificial Intelligence>AUT>facility;Stefan Lattner+Austrian Research Institute for Artificial Intelligence>AUT>facility;Maarten Grachten+Austrian Research Institute for Artificial Intelligence>AUT>facility,"The perception of tonal structure in music seems to be rooted both in low-level perceptual mechanisms and in enculturation, the latter accounting for cross-cultural differences in perceived tonal structure. Unsupervised machine learning methods are a powerful tool for studying how musical concepts may emerge from exposure to music. In this paper, we investigate to what degree tonal structure can be learned from musical data by unsupervised training of a Restricted Boltzmann Machine, a generative stochastic neural network. We show that even based on a limited set of musical data, the model learns several aspects of tonal structure. Firstly, the model learns an organization of musical material from different keys that conveys the topology of the circle of fifths (CoF). Although such a topology can be learned using principal component analysis (PCA) when using pitch-only representations, we found that using a pitch-duration representation impedes the extraction of the CoF topology much more for PCA than for the RBM. Furthermore, we replicate probe-tone experiments by Krumhansl and Shepard, measuring the organization of tones within a key in human perception. We find that the responses of the RBM share qualitative characteristics with those of both trained and untrained listeners."
32,Alessio Bazzica;Cynthia C. S. Liem;Alan Hanjalic,Exploiting Instrument-wise Playing/Non-Playing Labels for Score Synchronization of Symphonic Music.,2014,https://doi.org/10.5281/zenodo.1415772,Alessio Bazzica+Delft University of Technology>NLD>education;Cynthia C. S. Liem+Delft University of Technology>NLD>education;Alan Hanjalic+Delft University of Technology>NLD>education,"Synchronization of a score to an audio-visual music performance recording is usually done by solving an audio-to-MIDI alignment problem. In this paper, we focus on the possibility to represent both the score and the performance using information about which instrument is active at a given time stamp. More specifically, we investigate to what extent instrument-wise “playing” (P) and “non-playing” (NP) labels are informative in the synchronization process and what role the visual channel can have for the extraction of P/NP labels. After introducing the P/NP-based representation of the music piece, both at the score and performance level, we define an efficient way of computing the distance between the two representations, which serves as input for the synchronization step based on dynamic time warping. In parallel with assessing the effectiveness of the proposed representation, we also study its robustness when missing and/or erroneous labels occur. Our experimental results show that P/NP-based music piece representation is informative for performance-to-score synchronization and may benefit the existing audio-only approaches."
33,Marcelo Enrique Rodríguez-López;Anja Volk;Dimitrios Bountouridis,Multi-Strategy Segmentation of Melodies.,2014,https://doi.org/10.5281/zenodo.1418013,Marcelo Rodríguez-López+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education;Dimitrios Bountouridis+Utrecht University>NLD>education,"Melodic segmentation is a fundamental yet unsolved problem in automatic music processing. At present most melody segmentation models rely on a ‘single strategy’ (i.e. they model a single perceptual segmentation cue). However, cognitive studies suggest that multiple cues need to be considered. In this paper we thus propose and evaluate a ‘multi-strategy’ system to automatically segment symbolically encoded melodies. Our system combines the contribution of different single strategy boundary detection models. First, it assesses the perceptual relevance of a given boundary detection model for a given input melody; then it uses the boundaries predicted by relevant detection models to search for the most plausible segmentation of the melody. We use our system to automatically segment a corpus of instrumental and vocal folk melodies. We compare the predictions to human annotated segments, and to state of the art segmentation methods. Our results show that our system outperforms the state-of-the-art in the instrumental set."
34,Phillip B. Kirlin,A Data Set for Computational Studies of Schenkerian Analysis.,2014,https://doi.org/10.5281/zenodo.1417833,Phillip B. Kirlin+Rhodes College>USA>education,"Schenkerian analysis, a kind of hierarchical music analysis, is widely used by music theorists. Though it is part of the standard repertoire of analytical techniques, computational studies of Schenkerian analysis have been hindered by the lack of available data sets containing both musical compositions and ground-truth analyses of those compositions. Without such data sets, it is difficult to empirically study the patterns that arise in analyses or rigorously evaluate the performance of intelligent systems for this kind of analysis. To combat this, we introduce the first publicly available large-scale data set of computer-processable Schenkerian analyses. We discuss the choice of musical selections in the data set, the encoding of the music and the corresponding ground-truth analyses, and the possible uses of these data. As an example of the utility of the data set, we present an algorithm that transforms the Schenkerian analyses into hierarchically-organized data structures that are easily manipulated in software."
35,Agustín Martorell;Emilia Gómez,Systematic Multi-scale Set-class Analysis.,2014,https://doi.org/10.5281/zenodo.1417595,Agustín Martorell+Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Universitat Pompeu Fabra>ESP>education,"This work reviews and elaborates a methodology for hierarchical multi-scale set-class analysis of music pieces. The method extends the systematic segmentation and representation of Sapp’s ‘keyscapes’ to the description stage, by introducing a set-class level of description. This provides a systematic, mid-level, and standard analytical lexicon, which allows the description of any notated music based on fixed temperaments. The method benefits from the representation completeness, the compromise between generalisation and discrimination of the set-class spaces, and the access to hierarchical inclusion relations over time. The proposed class-matrices are multidimensional time series encoding the pitch content of every possible music segment over time, regardless the involved time-scales, in terms of a given set-class space. They provide the simplest information mining methods with the ability of capturing sophisticated tonal relations. The proposed class-vectors, quantifying the presence of every possible set-class in a piece, are discussed for advanced explorations of corpora. The compromise between dimensionality and informativeness provided by the class-matrices and class-vectors, is discussed in relation with standard content-based tonal descriptors, and music information retrieval applications."
36,Taro Masuda;Kazuyoshi Yoshii;Masataka Goto;Shigeo Morishima,Spotting a Query Phrase from Polyphonic Music Audio Signals Based on Semi-supervised Nonnegative Matrix Factorization.,2014,https://doi.org/10.5281/zenodo.1415780,Taro Masuda+Waseda University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Shigeo Morishima+Waseda University>JPN>education,"This paper proposes a query-by-audio system that aims to detect temporal locations where a musical phrase given as a query is played in musical pieces. The “phrase” in this paper means a short audio excerpt that is not limited to a main melody (singing part) and is usually played by a single musical instrument. A main problem of this task is that the query is often buried in mixture signals consisting of various instruments. To solve this problem, we propose a method that can appropriately calculate the distance between a query and partial components of a musical piece. More specifically, gamma process nonnegative matrix factorization (GaP-NMF) is used for decomposing the spectrogram of the query into an appropriate number of basis spectra and their activation patterns. Semi-supervised GaP-NMF is then used for estimating activation patterns of the learned basis spectra in the musical piece by presuming the piece to partially consist of those spectra. This enables distance calculation based on activation patterns. The experimental results showed that our method outperformed conventional matching methods."
37,Akira Maezawa;Katsutoshi Itoyama;Kazuyoshi Yoshii;Hiroshi G. Okuno,Bayesian Audio Alignment based on a Unified Model of Music Composition and Performance.,2014,https://doi.org/10.5281/zenodo.1415594,Akira Maezawa+Yamaha Corporation>JPN>company;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Hiroshi G. Okuno+Waseda University>JPN>education,"This paper presents a new probabilistic model that can align multiple performances of a particular piece of music. Conventionally, dynamic time warping (DTW) and left-to-right hidden Markov models (HMMs) have often been used for audio-to-audio alignment based on a shallow acoustic similarity between performances. Those methods, however, cannot distinguish latent musical structures common to all performances and temporal dynamics unique to each performance. To solve this problem, our model explicitly represents two state sequences: a top-level sequence that determines the common structure inherent in the music itself and a bottom-level sequence that determines the actual temporal fluctuation of each performance. These two sequences are fused into a hierarchical Bayesian HMM and can be learned at the same time from the given performances. Since the top-level sequence assigns the same state for note combinations that repeatedly appear within a piece of music, we can unveil the latent structure of the piece. Moreover, we can easily compare different performances of the same piece by analyzing the bottom-level sequences. Experimental evaluation showed that our method outperformed the conventional methods."
38,Ju-Chiang Wang;Ming-Chi Yen;Yi-Hsuan Yang;Hsin-Min Wang,Automatic Set List Identification and Song Segmentation for Full-Length Concert Videos.,2014,https://doi.org/10.5281/zenodo.1417361,"Ju-Chiang Wang+Academia Sinica>TWN>education|University of California, San Diego>USA>education;Ming-Chi Yen+Academia Sinica>TWN>education;Yi-Hsuan Yang+Academia Sinica>TWN>education;Hsin-Min Wang+Academia Sinica>TWN>education","Recently, plenty of full-length concert videos have become available on video-sharing websites such as YouTube. As each video generally contains multiple songs, natural questions that arise include “what is the set list?” and “when does each song begin and end?” Indeed, many full concert videos on YouTube contain song lists and timecodes contributed by uploaders and viewers. However, newly uploaded content and videos of lesser-known artists typically lack this metadata. Manually labeling such metadata would be labor-intensive, and thus an automated solution is desirable. In this paper, we define a novel research problem, automatic set list segmentation of full concert videos, which calls for techniques in music information retrieval (MIR) such as audio fingerprinting, cover song identification, musical event detection, music alignment, and structural segmentation. Moreover, we propose a greedy approach that sequentially identifies a song from a database of studio versions and simultaneously estimates its probable boundaries in the concert. We conduct preliminary evaluations on a collection of 20 full concerts and 1,152 studio tracks. Our result demonstrates the effectiveness of the proposed greedy algorithm."
39,Arthur Flexer,On Inter-rater Agreement in Audio Music Similarity.,2014,https://doi.org/10.5281/zenodo.1416970,Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,One of the central tasks in the annual MIREX evaluation campaign is the ”Audio Music Similarity and Retrieval (AMS)” task. Songs which are ranked as being highly similar by algorithms are evaluated by human graders as to how similar they are according to their subjective judgment. By analyzing results from the AMS tasks of the years 2006 to 2013 we demonstrate that: (i) due to low inter-rater agreement there exists an upper bound of performance in terms of subjective gradings; (ii) this upper bound has already been achieved by participating algorithms in 2009 and not been surpassed since then. Based on this sobering result we discuss ways to improve future evaluations of audio music similarity.
40,Bin Wu;Andrew Horner;Chung Lee,Emotional Predisposition of Musical Instrument Timbres with Static Spectra.,2014,https://doi.org/10.5281/zenodo.1417153,Bin Wu+Hong Kong University of Science and Technology>HKG>education;Andrew Horner+Hong Kong University of Science and Technology>HKG>education;Chung Lee+Singapore University of Technology and Design>SGP>education,"Music is one of the strongest triggers of emotions. Recent studies have shown strong emotional predispositions for musical instrument timbres. They have also shown significant correlations between spectral centroid and many emotions. Our recent study on spectral centroid-equalized tones further suggested that the even/odd harmonic ratio is a salient timbral feature after attack time and brightness. The emergence of the even/odd harmonic ratio motivated us to go a step further: to see whether the spectral shape of musical instruments alone can have a strong emotional predisposition. To address this issue, we conducted follow-up listening tests of static tones. The results showed that the even/odd harmonic ratio again significantly correlated with most emotions, consistent with the theory that static spectral shapes have a strong emotional predisposition."
41,Joren Six;Marc Leman,Panako - A Scalable Acoustic Fingerprinting System Handling Time-Scale and Pitch Modification.,2014,https://doi.org/10.5281/zenodo.1416190,Six Joren+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>facility;Marc Leman+Ghent University>BEL>education|Institute for Psychoacoustics and Electronic Music (IPEM)>BEL>facility,"This paper presents a scalable granular acoustic fingerprinting system. An acoustic fingerprinting system uses condensed representation of audio signals, acoustic fingerprints, to identify short audio fragments in large audio databases. A robust fingerprinting system generates similar fingerprints for perceptually similar audio signals. The system presented here is designed to handle time-scale and pitch modifications. The open source implementation of the system is called Panako and is evaluated on commodity hardware using a freely available reference database with fingerprints of over 30,000 songs. The results show that the system responds quickly and reliably on queries, while handling time-scale and pitch modifications of up to ten percent. The system is also shown to handle GSM-compression, several audio effects and band-pass filtering. After a query, the system returns the start time in the reference audio and how much the query has been pitch-shifted or time-stretched with respect to the reference audio. The design of the system that offers this combination of features is the main contribution of this paper."
42,Oriol Nieto;Morwaread M. Farbood;Tristan Jehan;Juan Pablo Bello,Perceptual Analysis of the F-Measure to Evaluate Section Boundaries in Music.,2014,https://doi.org/10.5281/zenodo.1414958,"Oriol Nieto+Music and Audio Research Lab, New York University>USA>education;Morwaread M. Farbood+Music and Audio Research Lab, New York University>USA>education;Tristan Jehan+The Echo Nest>USA>company;Juan Pablo Bello+Music and Audio Research Lab, New York University>USA>education","In this paper, we aim to raise awareness of the limitations of the F-measure when evaluating the quality of the boundaries found in the automatic segmentation of music. We present and discuss the results of various experiments where subjects listened to different musical excerpts containing boundary indications and had to rate the quality of the boundaries. These boundaries were carefully generated from state-of-the-art segmentation algorithms as well as human-annotated data. The results show that humans tend to give more relevance to the precision component of the F-measure rather than the recall component, therefore making the classical F-measure not as perceptually informative as currently assumed. Based on the results of the experiments, we discuss the potential of an alternative evaluation based on the F-measure that emphasizes precision over recall, making the section boundary evaluation more expressive and reliable."
43,Anna M. Kruspe,Keyword Spotting in A-capella Singing.,2014,https://doi.org/10.5281/zenodo.1416870,Anna M. Kruspe+Fraunhofer IDMT>DEU>facility|Johns Hopkins University>USA>education,"Keyword spotting (or spoken term detection) is an interesting task in Music Information Retrieval that can be applied to a number of problems. Its purposes include topical search and improvements for genre classification. Keyword spotting is a well-researched task on pure speech, but state-of-the-art approaches cannot be easily transferred to singing because phoneme durations have much higher variations in singing. To our knowledge, no keyword spotting system for singing has been presented yet. We present a keyword spotting approach based on keyword-filler Hidden Markov Models (HMMs) and test it on a-capella singing and spoken lyrics. We test Mel-Frequency Cepstral Coefficients (MFCCs), Perceptual Linear Predictive Features (PLPs), and Temporal Patterns (TRAPs) as front ends. These features are then used to generate phoneme posteriors using Multilayer Perceptrons (MLPs) trained on speech data. The phoneme posteriors are then used as the system input. Our approach produces useful results on a-capella singing, but depend heavily on the chosen keyword. We show that results can be further improved by training the MLP on a-capella data. We also test two post-processing methods on our phoneme posteriors before the keyword spotting step. First, we average the posteriors of all three feature sets. Second, we run the three concatenated posteriors through a fusion classifier."
44,Emilio Molina;Lorenzo J. Tardón;Isabel Barbancho;Ana M. Barbancho,The Importance of F0 Tracking in Query-by-singing-humming.,2014,https://doi.org/10.5281/zenodo.1416818,Emilio Molina+Universidad de Málaga>ESP>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Isabel Barbancho+Universidad de Málaga>ESP>education;Ana M. Barbancho+Universidad de Málaga>ESP>education,"In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of query-by-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baseline method. For the evaluation, we measured the QBSH performance for 189 different combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In addition, we also found clear differences in robustness to F0 transcription errors between different matchers."
45,Shrikant Venkataramani;Nagesh Nayak;Preeti Rao;Rajbabu Velmurugan,Vocal Separation using Singer-Vowel Priors Obtained from Polyphonic Audio.,2014,https://doi.org/10.5281/zenodo.1414974,Shrikant Venkataramani+IIT Bombay>IND>education|Sensibol Audio Technologies Pvt. Ltd.>IND>company;Nagesh Nayak+Sensibol Audio Technologies Pvt. Ltd.>IND>company;Preeti Rao+IIT Bombay>IND>education;Rajbabu Velmurugan+IIT Bombay>IND>education,"Single-channel methods for the separation of the lead vocal from mixed audio have traditionally included harmonic-sinusoidal modeling and matrix decomposition methods, each with its own strengths and shortcomings. In this work we use a hybrid framework to incorporate prior knowledge about singer and phone identity to achieve the superior separation of the lead vocal from the instrumental background. Singer specific dictionaries learned from available polyphonic recordings provide the soft mask that effectively attenuates the bleeding-through of accompanying melodic instruments typical of purely harmonic-sinusoidal model based separation. The dictionary learning uses NMF optimization across a training set of mixed signal utterances while keeping the vocal signal bases constant across the utterances. A soft mask is determined for each test mixed utterance frame by imposing sparseness constraints in the NMF partial co-factorization. We demonstrate significant improvements in reconstructed signal quality arising from the more accurate estimation of singer-vowel spectral envelope."
46,Chun-Ta Chen;Jyh-Shing Roger Jang;Chun-Hung Lu,Improving Query by Tapping via Tempo Alignment.,2014,https://doi.org/10.5281/zenodo.1416910,Chun-Ta Chen+National Tsing Hua University>TWN>education;Jyh-Shing Roger Jang+National Taiwan University>TWN>education;Chun-Hung Lu+Institute for Information Industry>TWN>company,"Query by tapping (QBT) is a content retrieval method that can retrieve a song by taking the user’s tapping or clapping at the note onsets of the intended song in the database for comparison. This paper proposes a new query-by-tapping algorithm that aligns the IOI (inter-onset interval) vector of the query sequence with songs in the dataset by building an IOI ratio matrix, and then applies a dynamic programming (DP) method to compute the optimum path with minimum cost. Experiments on different datasets indicate that our algorithm outperforms other previous approaches in accuracy (top-10 and MRR), with a speedup factor of 3 in computation. With the advent of personal handheld devices, QBT provides an interesting and innovative way for music retrieval by shaking or tapping the devices, which is also discussed in the paper."
47,Dominique Fourer;Jean-Luc Rouas;Pierre Hanna;Matthias Robine,Automatic Instrument Classification of Ethnomusicological Audio Recordings.,2014,https://doi.org/10.5281/zenodo.1417655,Dominique Fourer+University of Bordeaux>FRA>education;Jean-Luc Rouas+University of Bordeaux>FRA>education;Pierre Hanna+University of Bordeaux>FRA>education;Matthias Robine+University of Bordeaux>FRA>education,"Automatic timbre characterization of audio signals can help to measure similarities between sounds and is of interest for automatic or semi-automatic databases indexing. The most effective methods use machine learning approaches which require qualitative and diversified training databases to obtain accurate results. In this paper, we introduce a diversified database composed of worldwide non-western instruments audio recordings on which is evaluated an effective timbre classification method. A comparative evaluation based on the well studied Iowa musical instruments database shows results comparable with those of state-of-the-art methods. Thus, the proposed method offers a practical solution for automatic ethnomusicological indexing of a database composed of diversified sounds with various quality. The relevance of audio features for the timbre characterization is also discussed in the context of non-western instruments analysis."
48,Kirill A. Sidorov;Andrew Jones;A. David Marshall,Music Analysis as a Smallest Grammar Problem.,2014,https://doi.org/10.5281/zenodo.1417653,Kirill Sidorov+Cardiff University>GBR>education;Andrew Jones+Cardiff University>GBR>education;David Marshall+Cardiff University>GBR>education,"In this paper we present a novel approach to music analysis, in which a grammar is automatically generated explaining a musical work’s structure. The proposed method is predicated on the hypothesis that the shortest possible grammar provides a model of the musical structure which is a good representation of the composer’s intent. The effectiveness of our approach is demonstrated by comparison of the results with previously-published expert analysis; our automated approach produces results comparable to human annotation. We also illustrate the power of our approach by showing that it is able to locate errors in scores, such as introduced by OMR or human transcription. Further, our approach provides a novel mechanism for intuitive high-level editing and creative transformation of music. A wide range of other possible applications exists, including automatic summarization and simplification; estimation of musical complexity and similarity, and plagiarism detection."
49,Thomas Prätzlich;Meinard Müller,Frame-Level Audio Segmentation for Abridged Musical Works.,2014,https://doi.org/10.5281/zenodo.1418039,Thomas Prätzlich+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility,"Large-scale musical works such as operas may last several hours and typically involve a huge number of musicians. For such compositions, one often finds different arrangements and abridged versions (often lasting less than an hour), which can also be performed by smaller ensembles. Abridged versions still convey the flavor of the musical work containing the most important excerpts and melodies. In this paper, we consider the task of automatically segmenting an audio recording of a given version into semantically meaningful parts. Following previous work, the general strategy is to transfer a reference segmentation of the original complete work to the given version. Our main contribution is to show how this can be accomplished when dealing with strongly abridged versions. To this end, opposed to previously suggested segment-level matching procedures, we adapt a frame-level matching approach for transferring the reference segment information to the unknown version. Considering the opera “Der Freischütz” as an example scenario, we discuss how to balance out flexibility and robustness properties of our proposed frame-level segmentation procedure."
50,Rafael Caro Repetto;Xavier Serra,Creating a Corpus of Jingju (Beijing Opera) Music and Possibilities for Melodic Analysis.,2014,https://doi.org/10.5281/zenodo.1416030,Rafael Caro Repetto+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"Jingju (Beijing opera) is a Chinese traditional performing art form in which theatrical and musical elements are intimately combined. As an oral tradition, its musical dimension is the result of the application of a series of predefined conventions and it offers unique concepts for musicological research. Computational analyses of jingju music are still scarce, and only a few studies have dealt with it from an MIR perspective. In this paper we present the creation of a corpus of jingju music in the framework of the CompMusic project that is formed by audio, editorial metadata, lyrics and scores. We discuss the criteria followed for the acquisition of the data, describe the content of the corpus, and evaluate its suitability for computational and musicological research. We also identify several research problems that can take advantage of this corpus in the context of computational musicology, especially for melodic analysis, and suggest approaches for future work."
51,Jens Madsen;Bjørn Sand Jensen;Jan Larsen,Modeling Temporal Structure in Music for Emotion Prediction using Pairwise Comparisons.,2014,https://doi.org/10.5281/zenodo.1416384,Jens Madsen+Technical University of Denmark>DNK>education;Bjørn Sand Jensen+Technical University of Denmark>DNK>education;Jan Larsen+Technical University of Denmark>DNK>education,"The temporal structure of music is essential for the cognitive processes related to the emotions expressed in music. However, such temporal information is often disregarded in typical Music Information Retrieval modeling tasks of predicting higher-level cognitive or semantic aspects of music such as emotions, genre, and similarity. This paper addresses the specific hypothesis whether temporal information is essential for predicting expressed emotions in music, as a prototypical example of a cognitive aspect of music. We propose to test this hypothesis using a novel processing pipeline: 1) Extracting audio features for each track resulting in a multivariate ""feature time series"". 2) Using generative models to represent these time series (acquiring a complete track representation). Specifically, we explore the Gaussian Mixture model, Vector Quantization, Autoregressive model, Markov and Hidden Markov models. 3) Utilizing the generative models in a discriminative setting by selecting the Probability Product Kernel as the natural kernel for all considered track representations. We evaluate the representations using a kernel based model specifically extended to support the robust two-alternative forced choice self-report paradigm, used for eliciting expressed emotions in music. The methods are evaluated using two data sets and show increased predictive performance using temporal information, thus supporting the overall hypothesis."
52,Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo,Musical Structural Analysis Database Based on GTTM.,2014,https://doi.org/10.5281/zenodo.1415658,Masatoshi Hamanaka+Kyoto University>JPN>education;Keiji Hirata+Future University Hakodate>JPN>education;Satoshi Tojo+JAIST>JPN>education,"This paper, we present the publication of our analysis data and analyzing tool based on the generative theory of tonal music (GTTM). Musical databases such as score databases, instrument sound databases, and musical pieces with standard MIDI files and annotated data are key to advancements in the field of music information technology. We started implementing the GTTM on a computer in 2004 and ever since have collected and publicized test data by musicologists in a step-by-step manner. In our efforts to further advance the research on musical structure analysis, we are now publicizing 300 pieces of analysis data as well as the analyzer. Experiments showed that for 267 of 300 pieces the analysis results obtained by a new musicologist were almost the same as the original results in the GTTM database and that the other 33 pieces had different interpretations."
53,Marcelo F. Caetano;Frans Wiering,Theoretical Framework of A Computational Model of Auditory Memory for Music Emotion Recognition.,2014,https://doi.org/10.5281/zenodo.1417211,Marcelo Caetano+INESC TEC>PRT>facility;Frans Wiering+Utrecht University>NLD>education,"The bag of frames (BOF) approach commonly used in music emotion recognition (MER) has several limitations. The semantic gap is believed to be responsible for the glass ceiling on the performance of BOF MER systems. However, there are hardly any alternative proposals to address it. In this article, we introduce the theoretical framework of a computational model of auditory memory that incorporates temporal information into MER systems. We advocate that the organization of auditory memory places time at the core of the link between musical meaning and musical emotions. The main goal is to motivate MER researchers to develop an improved class of systems capable of overcoming the limitations of the BOF approach and coping with the inherent complexity of musical emotions."
54,Geoffroy Peeters;Victor Bisot,Improving Music Structure Segmentation using lag-priors.,2014,https://doi.org/10.5281/zenodo.1414764,Geoffroy Peeters+STMS IRCAM-CNRS-UPMC>FRA>Unknown;Victor Bisot+STMS IRCAM-CNRS-UPMC>FRA>Unknown,"Methods for music structure discovery usually process a music track by first detecting segments and then labeling them. Depending on the assumptions made on the signal content (repetition, homogeneity or novelty), different methods are used for these two steps. In this paper, we deal with the segmentation in the case of repetitive content. In this field, segments are usually identified by looking for sub-diagonals in a Self-Similarity-Matrix (SSM). In order to make this identification more robust, Goto proposed in 2003 to cumulate the values of the SSM over constant-lag and search only for segments in the SSM when this sum is large. Since the various repetitions of a segment start simultaneously in a self-similarity-matrix, Serra et al. proposed in 2012 to cumulate these simultaneous values (using a so-called structure feature) to enhance the novelty of the starting and ending time of a segment. In this work, we propose to combine both approaches by using Goto method locally as a prior to the lag-dimensions of Serra et al. structure features used to compute the novelty curve. Through a large experiment on RWC and Isophonics test-sets and using MIREX segmentation evaluation measure, we show that this simple combination allows a large improvement of the segmentation results."
55,Shuo Zhang;Rafael Caro Repetto;Xavier Serra,Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing.,2014,https://doi.org/10.5281/zenodo.1416250,Shuo Zhang+Universitat Pompeu Fabra>ESP>education;Rafael Caro Repetto+Universitat Pompeu Fabra>ESP>education;Xavier Serra+Universitat Pompeu Fabra>ESP>education,"Features of linguistic tone contours are important factors that shape the distinct melodic characteristics of different genres of Chinese opera. In Beijing opera, the presence of a two-dialectal tone system makes the tone-melody relationship more complex. In this paper, we propose a novel data-driven approach to analyze syllable-sized tone-pitch contour similarity in a corpus of Beijing Opera (381 arias) with statistical modeling and machine learning methods. A total number of 1,993 pitch contour units and attributes were extracted from a selection of 20 arias. We then build Smoothing Spline ANOVA models to compute matrixes of average melodic contour curves by tone category and other attributes. A set of machine learning and statistical analysis methods are applied to 30-point pitch contour vectors as well as dimensionality-reduced representations using Symbolic Aggregate approXimation(SAX). The results indicate an even mixture of shapes within all tone categories, with the absence of evidence for a predominant dialectal tone system in Beijing opera. We discuss the key methodological issues in melody-tone analysis and future work on pair-wise contour unit analysis."
56,Christian Frisson;Stéphane Dupont;Willy Yvart;Nicolas Riche;Xavier Siebert;Thierry Dutoit,A Proximity Grid Optimization Method to Improve Audio Search for Sound Design.,2014,https://doi.org/10.5281/zenodo.1417245,"Christian Frisson+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Stéphane Dupont+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Willy Yvart+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Nicolas Riche+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Xavier Siebert+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education;Thierry Dutoit+numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education|numediart Institute, University of Mons>BEL>education","Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques."
57,Matevz Pesek;Primoz Godec;Mojca Poredos;Gregor Strle;Joze Guna;Emilija Stojmenova;Matevz Pogacnik;Matija Marolt,Introducing a Dataset of Emotional and Color Responses to Music.,2014,https://doi.org/10.5281/zenodo.1415926,Matevž Pesek+University of Ljubljana>SVN>education;Primož Godec+University of Ljubljana>SVN>education;Mojca Poredoš+University of Ljubljana>SVN>education;Gregor Strle+Scientific Research Centre of the Slovenian Academy of Sciences and Arts>SVN>facility;Jože Guna+University of Ljubljana>SVN>education;Emilija Stojmenova+University of Ljubljana>SVN>education;Matevž Pogačnik+University of Ljubljana>SVN>education;Matija Marolt+University of Ljubljana>SVN>education,"The paper presents a new dataset of mood-dependent and color responses to music. The methodology of gathering user responses is described along with two new interfaces for capturing emotional states: the MoodGraph and MoodStripe. An evaluation study showed both interfaces have significant advantage over more traditional methods in terms of intuitiveness, usability and time complexity. The preliminary analysis of current data (over 6.000 responses) gives an interesting insight into participants’ emotional states and color associations, as well as relationships between musically perceived and induced emotions. We believe the size of the dataset, interfaces and multi-modal approach (connecting emotional, visual and auditory aspects of human perception) give a valuable contribution to current research."
58,Olivier Lartillot,In-depth Motivic Analysis based on Multiparametric Closed Pattern and Cyclic Sequence Mining.,2014,https://doi.org/10.5281/zenodo.1418345,Olivier Lartillot+Aalborg University>DNK>education,"The paper describes a computational system for exhaustive but compact description of repeated motivic patterns in symbolic representations of music. The approach follows a method based on closed heterogeneous pattern mining in multiparametrical space with control of pattern cyclicity. This paper presents a much simpler description and justification of this general strategy, as well as significant simplifications of the model, in particular concerning the management of pattern cyclicity. A new method for automated bundling of patterns belonging to same motivic or thematic classes is also presented. The good performance of the method is shown through the analysis of a piece from the JKUPDD database. Ground-truth motives are detected, while additional relevant information completes the ground-truth musicological analysis. The system, implemented in Matlab, is made publicly available as part of MiningSuite, a new open-source framework for audio and music analysis."
59,Colin Raffel;Brian McFee;Eric J. Humphrey;Justin Salamon;Oriol Nieto;Dawen Liang;Daniel P. W. Ellis,MIR_EVAL: A Transparent Implementation of Common MIR Metrics.,2014,https://doi.org/10.5281/zenodo.1416528,"Colin Raffel+LabROSA, Columbia University>USA>education;Brian McFee+LabROSA, Columbia University>USA>education|Music and Audio Research Lab, New York University>USA>education;Eric J. Humphrey+Music and Audio Research Lab, New York University>USA>education;Justin Salamon+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Oriol Nieto+Music and Audio Research Lab, New York University>USA>education;Dawen Liang+LabROSA, Columbia University>USA>education;Daniel P. W. Ellis+LabROSA, Columbia University>USA>education","Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir_eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir_eval and quantitatively compare each to existing implementations. When the scores reported by mir_eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir_eval’s architecture, design, and intended use."
60,Anna Aljanaki;Frans Wiering;Remco C. Veltkamp,Computational Modeling of Induced Emotion Using GEMS.,2014,https://doi.org/10.5281/zenodo.1415192,Anna Aljanaki+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"Most researchers in the automatic music emotion recognition field focus on the two-dimensional valence and arousal model. This model though does not account for the whole diversity of emotions expressible through music. Moreover, in many cases it might be important to model induced (felt) emotion, rather than perceived emotion. In this paper we explore a multidimensional emotional space, the Geneva Emotional Music Scales (GEMS), which addresses these two issues. We collected the data for our study using a game with a purpose. We exploit a comprehensive set of features from several state-of-the-art toolboxes and propose a new set of harmonically motivated features. The performance of these feature sets is compared. Additionally, we use expert human annotations to explore the dependency between musicologically meaningful characteristics of music and emotional categories of GEMS, demonstrating the need for algorithms that can better approximate human perception."
61,Jan Van Balen;Dimitrios Bountouridis;Frans Wiering;Remco C. Veltkamp,Cognition-inspired Descriptors for Scalable Cover Song Retrieval.,2014,https://doi.org/10.5281/zenodo.1417795,Jan Van Balen+Utrecht University>NLD>education;Dimitrios Bountouridis+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco Veltkamp+Utrecht University>NLD>education,"Inspired by representations used in music cognition studies and computational musicology, we propose three simple and interpretable descriptors for use in mid- to high-level computational analysis of musical audio and applications in content-based retrieval. We also argue that the task of scalable cover song retrieval is very suitable for the development of descriptors that effectively capture musical structures at the song level. The performance of the proposed descriptions in a cover song problem is presented. We further demonstrate that, due to the musically-informed nature of the descriptors, an independently established model of stability and variation in covers songs can be integrated to improve performance."
63,Peter van Kranenburg;Folgert Karsdorp,Cadence Detection in Western Traditional Stanzaic Songs using Melodic and Textual Features.,2014,https://doi.org/10.5281/zenodo.1416172,Peter van Kranenburg+Meertens Institute>NLD>education;Folgert Karsdorp+Meertens Institute>NLD>education,"Many Western songs are hierarchically structured in stanzas and phrases. The melody of the song is repeated for each stanza, while the lyrics vary. Each stanza is subdivided into phrases. It is to be expected that melodic and textual formulas at the end of the phrases offer intrinsic clues of closure to a listener or singer. In the current paper we aim at a method to detect such cadences in symbolically encoded folk songs. We take a trigram approach in which we classify trigrams of notes and pitches as cadential or as non-cadential. We use pitch, contour, rhythmic, textual, and contextual features, and a group of features based on the conditions of closure as stated by Narmour. We employ a random forest classification algorithm. The precision of the classifier is considerably improved by taking the class labels of adjacent trigrams into account. An ablation study shows that none of the kinds of features is sufficient to account for good classification, while some of the groups perform moderately well on their own."
64,Shrey Dutta;Hema A. Murthy,Discovering Typical Motifs of a Raga from One-Liners of Songs in Carnatic Music.,2014,https://doi.org/10.5281/zenodo.1418157,Shrey Dutta+Indian Institute of Technology Madras>IND>education;Hema A. Murthy+Indian Institute of Technology Madras>IND>education,"Typical motifs of a raga can be found in the various songs that are composed in the same raga by different composers. The compositions in Carnatic music have a definite structure, the one commonly seen being pallavi, anupallavi and charanam. The tala is also fixed for every song. Taking lines corresponding to one or more cycles of the pallavi, anupallavi and charanam as one-liners, one-liners across different songs are compared using a dynamic programming based algorithm. The density of match between the one-liners and normalized cost along-with a new measure, which uses the stationary points in the pitch contour to reduce the false alarms, are used to determine and locate the matched pattern. The typical motifs of a raga are then filtered using compositions of various ragas. Motifs are considered typical if they are present in the compositions of the given raga and are not found in compositions of other ragas."
65,Brian McFee;Dan Ellis,Analyzing Song Structure with Spectral Clustering.,2014,https://doi.org/10.5281/zenodo.1415778,Brian McFee+Columbia University>USA>education;Daniel P.W. Ellis+Columbia University>USA>education,"Many approaches to analyzing the structure of a musical recording involve detecting sequential patterns within a self-similarity matrix derived from time-series features. Such patterns ideally capture repeated sequences, which then form the building blocks of large-scale structure. In this work, techniques from spectral graph theory are applied to analyze repeated patterns in musical recordings. The proposed method produces a low-dimensional encoding of repetition structure, and exposes the hierarchical relationships among structural components at differing levels of granularity. Finally, we demonstrate how to apply the proposed method to the task of music segmentation."
66,Oriol Nieto;Morwaread M. Farbood,Identifying Polyphonic Musical Patterns From Audio Recordings Using Music Segmentation Techniques.,2014,https://doi.org/10.5281/zenodo.1417259,Oriol Nieto+New York University>USA>education|Unknown>Unknown>Unknown;Morwaread M. Farbood+New York University>USA>education|Unknown>Unknown>Unknown,"This paper presents a method for discovering patterns of note collections that repeatedly occur in a piece of music. We assume occurrences of these patterns must appear at least twice across a musical work and that they may contain slight differences in harmony, timbre, or rhythm. We describe an algorithm that makes use of techniques from the music information retrieval task of music segmentation, which exploits repetitive features in order to automatically identify polyphonic musical patterns from audio recordings. The novel algorithm is assessed using the recently published JKU Patterns Development Dataset, and we show how it obtains state-of-the-art results employing the standard evaluation metrics."
67,Karen Ullrich;Jan Schlüter;Thomas Grill,Boundary Detection in Music Structure Analysis using Convolutional Neural Networks.,2014,https://doi.org/10.5281/zenodo.1415886,Karen Ullrich+Austrian Research Institute for Artificial Intelligence>AUT>facility;Jan Schlüter+Austrian Research Institute for Artificial Intelligence>AUT>facility;Thomas Grill+Austrian Research Institute for Artificial Intelligence>AUT>facility,"The recognition of boundaries, e.g., between chorus and verse, is an important task in music structure analysis. The goal is to automatically detect such boundaries in audio signals so that the results are close to human annotation. In this work, we apply Convolutional Neural Networks to the task, trained directly on mel-scaled magnitude spectrograms. On a representative subset of the SALAMI structural annotation dataset, our method outperforms current techniques in terms of boundary retrieval F-measure at different temporal tolerances: We advance the state-of-the-art from 0.33 to 0.46 for tolerances of ±0.5 seconds, and from 0.52 to 0.62 for tolerances of ±3 seconds. As the algorithm is trained on annotated audio data without the need of expert knowledge, we expect it to be easily adaptable to changed annotation guidelines and also to related tasks such as the detection of song transitions."
68,Andre Holzapfel;Florian Krebs;Ajay Srinivasamurthy,"Tracking the ""Odd"": Meter Inference in a Culturally Diverse Music Corpus.",2014,https://doi.org/10.5281/zenodo.1415000,Andre Holzapfel+New York University Abu Dhabi>ARE>education;Florian Krebs+Johannes Kepler University>AUT>education;Ajay Srinivasamurthy+Universitat Pompeu Fabra>ESP>education,"In this paper, we approach the tasks of beat tracking, downbeat recognition and rhythmic style classification in non-Western music. Our approach is based on a Bayesian model, which infers tempo, downbeats and rhythmic style, from an audio signal. The model can be automatically adapted to rhythmic styles and time signatures. For evaluation, we compiled and annotated a music corpus consisting of eight rhythmic styles from three cultures, containing a variety of meter types. We demonstrate that by adapting the model to specific styles, we can track beats and downbeats in odd meter types like 9/8 or 7/8 with an accuracy significantly improved over the state of the art. Even if the rhythmic style is not known in advance, a unified model is able to recognize the meter and track the beat with comparable results, providing a novel method for inferring the metrical structure in culturally diverse datasets."
69,Ajay Srinivasamurthy;Rafael Caro Repetto;Sundar Harshavardhan;Xavier Serra,Transcription and Recognition of Syllable based Percussion Patterns: The Case of Beijing Opera.,2014,https://doi.org/10.5281/zenodo.1417267,"Ajay Srinivasamurthy+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Rafael Caro Repetto+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Harshavardhan Sundar+Indian Institute of Science>IND>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","In many cultures of the world, traditional percussion music uses mnemonic syllables that are representative of the timbres of instruments. These syllables are orally transmitted and often provide a language for percussion in those music cultures. Percussion patterns in these cultures thus have a well defined representation in the form of these syllables, which can be utilized in several computational percussion pattern analysis tasks. We explore a connected word speech recognition based framework that can effectively utilize the syllabic representation for automatic transcription and recognition of audio percussion patterns. In particular, we consider the case of Beijing opera and present a syllable level hidden markov model (HMM) based system for transcription and classification of percussion patterns. The encouraging classification results on a representative dataset of Beijing opera percussion patterns supports our approach and provides further insights on the utility of these syllables for computational description of percussion patterns."
70,Joshua L. Moore;Thorsten Joachims;Douglas Turnbull,Taste Space Versus the World: an Embedding Analysis of Listening Habits and Geography.,2014,https://doi.org/10.5281/zenodo.1415178,Joshua L. Moore+Cornell University>USA>education;Thorsten Joachims+Cornell University>USA>education;Douglas Turnbull+Ithaca College>USA>education,"Probabilistic embedding methods provide a principled way of deriving new spatial representations of discrete objects from human interaction data. The resulting assignment of objects to positions in a continuous, low-dimensional space not only provides a compact and accurate predictive model, but also a compact and flexible representation for understanding the data. In this paper, we demonstrate how probabilistic embedding methods reveal the “taste space” in the recently released Million Musical Tweets Dataset (MMTD), and how it transcends geographic space. In particular, by embedding cities around the world along with preferred artists, we are able to distill information about cultural and geographical differences in listening patterns into spatial representations. These representations yield a similarity metric among city pairs, artist pairs, and city-artist pairs, which can then be used to draw conclusions about the similarities and contrasts between taste space and geographic location."
71,Zhe Xing;Xinxi Wang;Ye Wang,Enhancing Collaborative Filtering Music Recommendation by Balancing Exploration and Exploitation.,2014,https://doi.org/10.5281/zenodo.1416776,Zhe Xing+National University of Singapore>SGP>education;Xinxi Wang+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"Collaborative filtering (CF) techniques have shown great success in music recommendation applications. However, traditional collaborative-filtering music recommendation algorithms work in a greedy way, invariably recommending songs with the highest predicted user ratings. Such a purely exploitative strategy may result in suboptimal performance over the long term. Using a novel reinforcement learning approach, we introduce exploration into CF and try to balance between exploration and exploitation. In order to learn users’ musical tastes, we use a Bayesian graphical model that takes account of both CF latent factors and recommendation novelty. Moreover, we designed a Bayesian inference algorithm to efficiently estimate the posterior rating distributions. In music recommendation, this is the first attempt to remedy the greedy nature of CF approaches. Results from both simulation experiments and user study show that our proposed approach significantly improves recommendation performance."
72,Audrey Laplante,Improving Music Recommender Systems: What Can We Learn from Research on Music Tastes?,2014,https://doi.org/10.5281/zenodo.1417797,Audrey Laplante+Université de Montréal>CAN>education,"The success of a music recommender system depends on its ability to predict how much a particular user will like or dislike each item in its catalogue. However, such predictions are difficult to make accurately due to the complex nature of music tastes. In this paper, we review the literature on music tastes from social psychology and sociology of music to identify the correlates of music tastes and to understand how music tastes are formed and evolve through time. Research shows associations between music preferences and a wide variety of sociodemographic and individual characteristics, including personality traits, values, ethnicity, gender, social class, and political orientation. It also reveals the importance of social influences on music tastes, more specifically from family and peers, as well as the central role of music tastes in the construction of personal and social identities. Suggestions for the design of music recommender systems are made based on this literature review."
73,Sally Jo Cunningham;David M. Nichols;David Bainbridge 0001;Hassan Ali,Social Music in Cars.,2014,https://doi.org/10.5281/zenodo.1415676,Sally Jo Cunningham+University of Waikato>NZL>education;David M. Nichols+University of Waikato>NZL>education;David Bainbridge+University of Waikato>NZL>education;Hasan Ali+University of Waikato>NZL>education,"This paper builds an understanding of how music is currently experienced by a social group travelling together in a car—how songs are chosen for playing, how music both reflects and influences the group’s mood and social interaction, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of ethnographic data (participant observations and interviews) focusing primarily on the experience of in-car music on moderate length and long trips. We suggest features and functionality for music software to enhance the social experience when travelling in cars, and prototype and test a user interface based on design suggestions drawn from the data."
74,Mohamed Morchid;Richard Dufour;Georges Linarès,A Combined Thematic and Acoustic Approach for a Music Recommendation Service in TV Commercials.,2014,https://doi.org/10.5281/zenodo.1415098,Abhishek Singhi+University of Waterloo>CAN>education;Daniel G. Brown+University of Waterloo>CAN>education,"We hypothesize that different genres of writing use different adjectives for the same concept. We test our hypothesis on lyrics, articles and poetry. We use the English Wikipedia and over 13,000 news articles from four leading newspapers for the article data set. Our lyrics data set consists of lyrics of more than 10,000 songs by 56 popular English singers, and our poetry dataset is made up of more than 20,000 poems from 60 famous poets. We find the probability distribution of synonymous adjectives in all the three different categories and use it to predict if a document is an article, lyrics or poetry given its set of adjectives. We achieve an accuracy level of 67% for lyrics, 80% for articles and 57% for poetry. Using these probability distribution we show that adjectives more likely to be used in lyrics are more rhymable than those more likely to be used in poetry, but they do not differ significantly in their semantic orientations. Furthermore we show that our algorithm is successfully able to detect poetic lyricists like Bob Dylan from non-poetic ones like Bryan Adams, as their lyrics are more often misclassified as poetry."
75,Abhishek Singhi;Daniel G. Brown 0001,Are Poetry and Lyrics All That Different?,2014,https://doi.org/10.5281/zenodo.1417004,Mohamed Morchid+LIA - University of Avignon>FRA>education;Richard Dufour+LIA - University of Avignon>FRA>education;Georges Linares+LIA - University of Avignon>FRA>education,"Most of modern advertisements contain a song to illustrate the commercial message. The success of a product, and its economic impact, can be directly linked to this choice. Finding the most appropriate song is usually made manually. Nonetheless, a single person is not able to listen and choose the best music among millions. The need for an automatic system for this particular task becomes increasingly critical. This paper describes the LIA music recommendation system for advertisements using both textual and acoustic features. This system aims at providing a song to a given commercial video and was evaluated in the context of the MediaEval 2013 Soundtrack task [14]. The goal of this task is to predict the most suitable soundtrack from a list of candidate songs, given a TV commercial. The organizers provide a development dataset including multimedia features. The initial assumption of the proposed system is that commercials which sell the same type of product, should also share the same music rhythm. A two-fold system is proposed: find commercials with close subjects in order to determine the mean rhythm of this subset, and then extract, from the candidate songs, the music which better corresponds to this mean rhythm."
76,Po-Sen Huang;Minje Kim;Mark Hasegawa-Johnson;Paris Smaragdis,Singing-Voice Separation from Monaural Recordings using Deep Recurrent Neural Networks.,2014,https://doi.org/10.5281/zenodo.1415678,Po-Sen Huang+University of Illinois at Urbana-Champaign>USA>education;Minje Kim+University of Illinois at Urbana-Champaign>USA>education;Mark Hasegawa-Johnson+University of Illinois at Urbana-Champaign>USA>education;Paris Smaragdis+University of Illinois at Urbana-Champaign>USA>education|Adobe Research>USA>company,"Monaural source separation is important for many real world applications. It is challenging since only single channel information is available. In this paper, we explore using deep recurrent neural networks for singing voice separation from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal connections are explored. We propose jointly optimizing the networks for multiple source signals by including the separation step as a nonlinear operation in the last layer. Different discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30∼2.48 dB GNSDR gain and 4.32∼5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset."
77,Katayoun Farrahi;Markus Schedl;Andreu Vall;David Hauger;Marko Tkalcic,Impact of Listening Behavior on Music Recommendation.,2014,https://doi.org/10.5281/zenodo.1417505,"Katayoun Farrahi+Goldsmiths, University of London>GBR>education;Markus Schedl+Johannes Kepler University>AUT>education;Andreu Vall+Johannes Kepler University>AUT>education;David Hauger+Johannes Kepler University>AUT>education;Marko Tkalčič+Johannes Kepler University>AUT>education","The next generation of music recommendation systems will be increasingly intelligent and likely take into account user behavior for more personalized recommendations. In this work we consider user behavior when making recommendations with features extracted from a user’s history of listening events. We investigate the impact of listener’s behavior by considering features such as play counts, “mainstreaminess”, and diversity in music taste on the performance of various music recommendation approaches. The underlying dataset has been collected by crawling social media (specifically Twitter) for listening events. Each user’s listening behavior is characterized into a three dimensional feature space consisting of play count, “mainstreaminess” (i.e. the degree to which the observed user listens to currently popular artists), and diversity (i.e. the diversity of genres the observed user listens to). Drawing subsets of the 28,000 users in our dataset, according to these three dimensions, we evaluate whether these dimensions influence figures of merit of various music recommendation approaches, in particular, collaborative filtering (CF) and CF enhanced by cultural information such as users located in the same city or country."
78,Bogdan Vera;Elaine Chew,Towards Seamless Network Music Performance: Predicting an Ensemble's Expressive Decisions for Distributed Performance.,2014,https://doi.org/10.5281/zenodo.1416186,Bogdan Vera+Queen Mary University of London>GBR>education;Elaine Chew+Queen Mary University of London>GBR>education,"Internet performance faces the challenge of network latency. One proposed solution is music prediction, wherein musical events are predicted in advance and transmitted to distributed musicians ahead of the network delay. We present a context-aware music prediction system focusing on expressive timing: a Bayesian network that incorporates stylistic model selection and linear conditional gaussian distributions on variables representing proportional tempo change. The system can be trained using rehearsals of distributed or co-located ensembles. We evaluate the model by comparing its prediction accuracy to two others: one employing only linear conditional dependencies between expressive timing nodes but no stylistic clustering, and one using only independent distributions for timing changes. The three models are tested on performances of a custom-composed piece that is played ten times, each in one of two styles. The results are promising, with the proposed system outperforming the other two. In predictable parts of the performance, the system with conditional dependencies and stylistic clustering achieves errors of 15ms; in more difficult sections, the errors rise to 100ms; and, in unpredictable sections, the error is too great for seamless timing emulation. Finally, we discuss avenues for further research and propose the use of predictive timing cues using our system."
79,Ling-Chi Hsu;Yu-Lin Wang;Yi-Ju Lin;Cheryl D. Metcalf;Alvin W. Y. Su,Detection of Motor Changes in Violin Playing by EMG Signals.,2014,https://doi.org/10.5281/zenodo.1416452,Ling-Chi Hsu+National Cheng-Kung University>TWN>education;Yu-Lin Wang+National Cheng-Kung University>TWN>education;Yi-Ju Lin+National Cheng-Kung University>TWN>education;Alvin W.Y. Su+National Cheng-Kung University>TWN>education;Cheryl D. Metcalf+University of Southampton>GBR>education,"Playing a music instrument relies on the harmonious body movements. Motor sequences are trained to achieve the perfect performances in musicians. Thus, the information from audio signal is not enough to understand the sensorimotor programming in players. Recently, the investigation of muscular activities of players during performance has attracted our interests. In this work, we propose a multi-channel system that records the audio sounds and electromyography (EMG) signal simultaneously and also develop algorithms to analyze the music performance and discover its relation to player’s motor sequences. The movement segment was first identified by the information of audio sounds, and the direction of violin bowing was detected by the EMG signal. Six features were introduced to reveal the variations of muscular activities during violin playing. With the additional information of the audio signal, the proposed work could efficiently extract the period and detect the direction of motor changes in violin bowing. Therefore, the proposed work could provide a better understanding of how players activate the muscles to organize the multi-joint movement during violin performance."
80,Wang-Kong Lam;Tan Lee,Automatic Key Partition Based on Tonal Organization Information of Classical Music.,2014,https://doi.org/10.5281/zenodo.1414748,Lam Wang Kong+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown;Tan Lee+The Chinese University of Hong Kong>HKG>education|Unknown>Unknown>Unknown,"Key information is a useful information for tonal music analysis. It is related to chord progressions, which follows some specific structures and rules. In this paper, we describe a generative account of chord progression consisting of phrase-structure grammar rules proposed by Martin Rohrmeier. With some modifications, these rules can be used to partition a chord symbol sequence into different key areas, if modulation occurs. Exploiting tonal grammar rules, the most musically sensible key partition of chord sequence is derived. Some examples of classical music excerpts are evaluated. This rule-based system is compared against another system which is based on dynamic programming of harmonic-hierarchy information. Using Kostka-Payne corpus as testing data, the experimental result shows that our system is better in terms of key detection accuracy."
81,Po-Kai Yang;Chung-Chien Hsu;Jen-Tzung Chien,Bayesian Singing-Voice Separation.,2014,https://doi.org/10.5281/zenodo.1417373,Po-Kai Yang+National Chiao Tung University>TWN>education;Chung-Chien Hsu+National Chiao Tung University>TWN>education;Jen-Tzung Chien+National Chiao Tung University>TWN>education,"This paper presents a Bayesian nonnegative matrix factorization (NMF) approach to extract singing voice from background music accompaniment. Using this approach, the likelihood function based on NMF is represented by a Poisson distribution and the NMF parameters, consisting of basis and weight matrices, are characterized by the exponential priors. A variational Bayesian expectation-maximization algorithm is developed to learn variational parameters and model parameters for monaural source separation. A clustering algorithm is performed to establish two groups of bases: one is for singing voice and the other is for background music. Model complexity is controlled by adaptively selecting the number of bases for different mixed signals according to the variational lower bound. Model regularization is tackled through the uncertainty modeling via variational inference based on marginal likelihood. The experimental results on MIR-1K database show that the proposed method performs better than various unsupervised separation algorithms in terms of the global normalized source to distortion ratio."
82,Filip Korzeniowski;Sebastian Böck;Gerhard Widmer,Probabilistic Extraction of Beat Positions from a Beat Activation Function.,2014,https://doi.org/10.5281/zenodo.1415118,Filip Korzeniowski+Johannes Kepler University>AUT>education;Sebastian Böck+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"We present a probabilistic way to extract beat positions from the output (activations) of the neural network that is at the heart of an existing beat tracker. The method can serve as a replacement for the greedy search the beat tracker currently uses for this purpose. Our experiments show improvement upon the current method for a variety of data sets and quality measures, as well as better results compared to other state-of-the-art algorithms."
83,Sanghoon Jun;Seungmin Rho;Eenjun Hwang,Geographical Region Mapping Scheme Based on Musical Preferences.,2014,https://doi.org/10.5281/zenodo.1418011,Sanghoon Jun+Korea University>KOR>education|Korea University>KOR>education;Seungmin Rho+Sungkyul University>KOR>education;Eenjun Hwang+Korea University>KOR>education,"Many countries and cities in the world tend to have different types of preferred or popular music, such as pop, K-pop, and reggae. Music-related applications utilize geographical proximity for evaluating the similarity of music preferences between two regions. Sometimes, this can lead to incorrect results due to other factors such as culture and religion. To solve this problem, in this paper, we propose a scheme for constructing a music map in which regions are positioned close to one another depending on the similarity of the musical preferences of their populations. That is, countries or cities in a traditional map are rearranged in the music map such that regions with similar musical preferences are close to one another. To do this, we collect users’ music play history and extract popular artists and tag information from the collected data. Similarities among regions are calculated using the tags and their frequencies. And then, an iterative algorithm for rearranging the regions into a music map is applied. We present a method for constructing the music map along with some experimental results."
84,John Ashley Burgoyne;W. Bas de Haas;Johan Pauwels,On Comparative Statistics for Labelling Tasks: What can We Learn from MIREX ACE 2013?,2014,https://doi.org/10.5281/zenodo.1417091,John Ashley Burgoyne+Universiteit van Amsterdam>NLD>education;W. Bas de Haas+Universiteit Utrecht>NLD>education;Johan Pauwels+Unknown>Unknown>Unknown,"For the evaluation of audio chord estimation, the evaluation of audio chord estimation followed a new scheme. Using chord vocabularies of differing complexity as well as segmentation measures, the new scheme provides more information than the evaluations from previous years. With this new information, however, comes new interpretive challenges. What are the correlations among different songs and, more importantly, different submissions across the new measures? Performance falls off for all submissions as the vocabularies increase in complexity, but does it do so directly in proportion to the number of more complex chords, or are certain algorithms indeed more robust? What are the outliers, song-algorithm pairs where the performance was substantially higher or lower than would be predicted, and how can they be explained? Answering these questions requires moving beyond the Friedman tests that have most often been used to compare algorithms to a richer underlying model. We propose a logistic-regression approach for generating comparative statistics for audio chord estimation, supported with generalised estimating equations to correct for repeated measures. We use the MIREX ACE 2013 results as a case study to illustrate our proposed method, including some of interesting aspects of the evaluation that might not apparent from the headline results alone."
85,Eita Nakamura;Nobutaka Ono;Shigeki Sagayama,Merged-Output HMM for Piano Fingering of Both Hands.,2014,https://doi.org/10.5281/zenodo.1415152,Eita Nakamura+National Institute of Informatics>JPN>facility;Nobutaka Ono+National Institute of Informatics>JPN>facility;Shigeki Sagayama+Meiji University>JPN>education,"This paper discusses a piano fingering model for both hands and its applications. One of our motivations behind the study is automating piano reduction from ensemble scores. For this, quantifying the difficulty of piano performance is important where a fingering model of both hands should be relevant. Such a fingering model is proposed that is based on merged-output hidden Markov model and can be applied to scores in which the voice part for each hand is not indicated. The model is applied for decision of fingering for both hands and voice-part separation, automation of which is itself of great use and were previously difficult. A measure of difficulty of performance based on the fingering model is also proposed and yields reasonable results."
86,Maria Panteli;Niels Bogaards;Aline K. Honingh,Modeling Rhythm Similarity for Electronic Dance Music.,2014,https://doi.org/10.5281/zenodo.1416664,Maria Panteli+University of Amsterdam>NLD>education|Elephantcandy>NLD>company;Niels Bogaards+Elephantcandy>NLD>company;Aline Honingh+University of Amsterdam>NLD>education,"A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a ‘loop’, a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between segments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most notably: attack phase of onsets, periodicity of rhythmic elements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, after which the similarity between segments can be calculated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the overall performance of the model with perceptual ratings of rhythm similarity."
87,Martin Przyjaciel-Zablocki;Thomas Hornung 0001;Alexander Schätzle;Sven Gauß;Io Taxidou;Georg Lausen,MuSe: A Music Recommendation Management System.,2014,https://doi.org/10.5281/zenodo.1418195,Martin Przyjaciel-Zablocki+University of Freiburg>DEU>education;Thomas Hornung+University of Freiburg>DEU>education;Alexander Schätzle+University of Freiburg>DEU>education;Sven Gauß+University of Freiburg>DEU>education;Io Taxidou+University of Freiburg>DEU>education;Georg Lausen+University of Freiburg>DEU>education,"Evaluating music recommender systems is a highly repetitive, yet non-trivial, task. But it has the advantage over other domains that recommended songs can be evaluated immediately by just listening to them. In this paper, we present MUSE – a music recommendation management system – for solving the typical tasks of an in vivo evaluation. MUSE provides the typical off-the-shelf evaluation algorithms, offers an online evaluation system with automatic reporting, and by integrating online streaming services also a legal possibility to evaluate the quality of recommended songs in real time. Finally, it has a built-in user management system that conforms with state-of-the-art privacy standards. New recommender algorithms can be plugged in comfortably and evaluations can be configured and managed online."
88,Andreas Arzt;Gerhard Widmer;Reinhard Sonnleitner,Tempo- and Transposition-invariant Identification of Piece and Score Position.,2014,https://doi.org/10.5281/zenodo.1415850,Andreas Arzt+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Reinhard Sonnleitner+Johannes Kepler University>AUT>education|Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility,"We present an algorithm that, given a very small snippet of an audio performance and a database of musical scores, quickly identifies the piece and the position in the score. The algorithm is both tempo- and transposition-invariant. We approach the problem by extending an existing tempo-invariant symbolic fingerprinting method, replacing the absolute pitch information in the fingerprints with a relative representation. Not surprisingly, this leads to a big decrease in the discriminative power of the fingerprints. To overcome this problem, we propose an additional verification step to filter out the introduced noise. Finally, we present a simple tracking algorithm that increases the retrieval precision for longer queries. Experiments show that both modifications improve the results, and make the new algorithm usable for a wide range of applications."
89,Ming-Ju Wu;Jyh-Shing Roger Jang;Chun-Hung Lu,Gender Identification and Age Estimation of Users Based on Music Metadata.,2014,https://doi.org/10.5281/zenodo.1417605,Ming-Ju Wu+National Tsing Hua University>TWN>education;Jyh-Shing Roger Jang+National Taiwan University>TWN>education;Chun-Hung Lu+Institute for Information Industry>TWN>facility,"Music recommendation is a crucial task in the field of music information retrieval. However, users frequently withhold their real-world identity, which creates a negative impact on music recommendation. Thus, the proposed method recognizes users’ real-world identities based on music metadata. The approach is based on using the tracks most frequently listened to by a user to predict their gender and age. Experimental results showed that the approach achieved an accuracy of 78.87% for gender identification and a mean absolute error of 3.69 years for the age estimation of 48403 users, demonstrating its effectiveness and feasibility, and paving the way for improving music recommendation based on such personal information."
90,Daniel Boland;Roderick Murray-Smith,Information-Theoretic Measures of Music Listening Behaviour.,2014,https://doi.org/10.5281/zenodo.1416192,Daniel Boland+University of Glasgow>GBR>education;Roderick Murray-Smith+University of Glasgow>GBR>education,"We present an information-theoretic approach to the measurement of users’ music listening behaviour and selection of music features. Existing ethnographic studies of music use have guided the design of music retrieval systems however are typically qualitative and exploratory in nature. We introduce the SPUD dataset, comprising 10,000 handmade playlists, with user and audio stream metadata. With this, we illustrate the use of entropy for analysing music listening behaviour, e.g. identifying when a user changed music retrieval system. We then develop an approach to identifying music features that reflect users’ criteria for playlist curation, rejecting features that are independent of user behaviour. The dataset and the code used to produce it are made available. The techniques described support a quantitative yet user-centred approach to the evaluation of music features and retrieval systems, without assuming objective ground truth labels."
91,Emilio Molina;Ana M. Barbancho;Lorenzo J. Tardón;Isabel Barbancho,Evaluation Framework for Automatic Singing Transcription.,2014,https://doi.org/10.5281/zenodo.1417729,Emilio Molina+Universidad de Málaga>ESP>education;Ana M. Barbancho+Universidad de Málaga>ESP>education;Lorenzo J. Tardón+Universidad de Málaga>ESP>education;Isabel Barbancho+Universidad de Málaga>ESP>education,"In this paper, we analyse the evaluation strategies used in previous works on automatic singing transcription, and we present a novel, comprehensive and freely available evaluation framework for automatic singing transcription. This framework consists of a cross-annotated dataset and a set of extended evaluation measures, which are integrated in a Matlab toolbox. The presented evaluation measures are based on standard MIREX note-tracking measures, but they provide extra information about the type of errors made by the singing transcriber. Finally, a practical case of use is presented, in which the evaluation framework has been used to perform a comparison in detail of several state-of-the-art singing transcribers."
92,Julián Urbano;Dmitry Bogdanov;Perfecto Herrera;Emilia Gómez;Xavier Serra,What is the Effect of Audio Quality on the Robustness of MFCCs and Chroma Features?,2014,https://doi.org/10.5281/zenodo.1416276,"Julián Urbano+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Dmitry Bogdanov+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Perfecto Herrera+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emilia Gómez+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education|Music Technology Group, Universitat Pompeu Fabra>ESP>education","Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical applications they are to be computed on music corpora containing audio files encoded in a variety of lossy formats. Such encodings distort the original signal and therefore may affect the computation of descriptors. This raises the question of the robustness of these descriptors across various audio encodings. We examine this assumption for the case of MFCCs and chroma features. In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre. Using two different audio analysis tools over a diverse collection of music tracks, we compute several statistics to quantify the robustness of the resulting descriptors, and then estimate the practical effects for a sample task like genre classification."
93,Xiao Hu 0001;Jin Ha Lee;Leanne Ka Yan Wong,Music Information Behaviors and System Preferences of University Students in Hong Kong.,2014,https://doi.org/10.5281/zenodo.1414778,Xiao Hu+University of Hong Kong>HKG>education|University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education;Leanne Ka Yan Wong+University of Hong Kong>HKG>education,"This paper presents a user study on music information needs and behaviors of university students in Hong Kong. A mix of quantitative and qualitative methods was used. A survey was completed by 101 participants and supplemental interviews were conducted in order to investigate users’ music information related activities. We found that university students in Hong Kong listened to music frequently and mainly for the purposes of entertainment, singing and playing instruments, and stress reduction. This user group often searches for music with multiple methods, but common access points like genre and time period were rarely used. Sharing music with people in their online social networks such as Facebook and Weibo was a common activity. Furthermore, the popularity of smartphones prompted the need for streaming music and mobile music applications. We also examined users’ preferences on music services available in Hong Kong such as YouTube and KKBox, as well as the characteristics liked and disliked by the users. The results not only offer insights into non-Western users’ music behaviors but also for designing online music services for young music listeners in Hong Kong."
94,Shoto Sasaki;Kazuyoshi Yoshii;Tomoyasu Nakano;Masataka Goto;Shigeo Morishima,LyricsRadar: A Lyrics Retrieval System Based on Latent Topics of Lyrics.,2014,https://doi.org/10.5281/zenodo.1418075,Shoto Sasaki+Waseda University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education;Tomoyasu Nakano+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Shigeo Morishima+Waseda University>JPN>education,"This paper presents a lyrics retrieval system called LyricsRadar that enables users to interactively browse song lyrics by visualizing their topics. Since conventional lyrics retrieval systems are based on simple word search, those systems often fail to reflect user’s intention behind a query when a word given as a query can be used in different contexts. For example, the word “tears” can appear not only in sad songs (e.g., feel heartrending), but also in happy songs (e.g., weep for joy). To overcome this limitation, we propose to automatically analyze and visualize topics of lyrics by using a well-known text analysis method called latent Dirichlet allocation (LDA). This enables LyricsRadar to offer two types of topic visualization. One is the topic radar chart that visualizes the relative weights of five latent topics of each song on a pentagon-shaped chart. The other is radar-like arrangement of all songs in a two-dimensional space in which song lyrics having similar topics are arranged close to each other. The subjective experiments using 6,902 Japanese popular songs showed that our system can appropriately navigate users to lyrics of interests."
95,Eric J. Humphrey;Justin Salamon;Oriol Nieto;Jon Forsyth;Rachel M. Bittner;Juan Pablo Bello,JAMS: A JSON Annotated Music Specification for Reproducible MIR Research.,2014,https://doi.org/10.5281/zenodo.1415924,"Eric J. Humphrey+Music and Audio Research Lab, New York University>USA>education;Justin Salamon+Music and Audio Research Lab, New York University>USA>education|Center for Urban Science and Progress, New York University>USA>education;Oriol Nieto+Music and Audio Research Lab, New York University>USA>education;Jon Forsyth+Music and Audio Research Lab, New York University>USA>education;Rachel M. Bittner+Music and Audio Research Lab, New York University>USA>education;Juan P. Bello+Music and Audio Research Lab, New York University>USA>education","The continued growth of MIR is motivating more complex annotation data, consisting of richer information, multiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of addressing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, comprehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we provide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and discuss how now is a crucial time to make a concerted effort toward sustainable annotation standards."
96,Pierre Saurel;Francis Rousseaux;Marc Danger,On The Changing Regulations of Privacy and Personal Information in MIR.,2014,https://doi.org/10.5281/zenodo.1416638,Pierre Saurel+Université Paris-Sorbonne>FRA>education;Francis Rousseaux+IRCAM>FRA>facility;Marc Danger+ADAMI>FRA>company,"In recent years, MIR research has continued to focus more and more on user feedback, human subjects data, and other forms of personal information. Concurrently, the European Union has adopted new, stringent regulations to take effect in the coming years regarding how such information can be collected, stored and manipulated, with equally strict penalties for being found in violation of the law. Here, we provide a summary of these changes, consider how they relate to our data sources and research practices, and identify promising methodologies that may serve researchers well, both in order to be in compliance with the law and conduct more subject-friendly research. We additionally provide a case study of how such changes might affect a recent human subjects project on the topic of style, and conclude with a few recommendations for the near future. This paper is not intended to be legal advice: our personal legal interpretations are strictly mentioned for illustration purpose, and reader should seek proper legal counsel."
97,Sebastian Böck;Florian Krebs;Gerhard Widmer,A Multi-model Approach to Beat Tracking Considering Heterogeneous Music Styles.,2014,https://doi.org/10.5281/zenodo.1415240,Sebastian Böck+Johannes Kepler University>AUT>education;Florian Krebs+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possible beat positions. It chooses the model with the most appropriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27% over existing state-of-the-art methods. Under certain conditions the system is able to match even human tapping performance."
98,Jonathan Driedger;Meinard Müller;Sascha Disch,Extending Harmonic-Percussive Separation of Audio Signals.,2014,https://doi.org/10.5281/zenodo.1415226,Jonathan Driedger+International Audio Laboratories Erlangen>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility;Sascha Disch+Fraunhofer Institute for Integrated Circuits IIS>DEU>facility,"In recent years, methods to decompose an audio signal into a harmonic and a percussive component have received a lot of interest and are frequently applied as a processing step in a variety of scenarios. One problem is that the computed components are often not of purely harmonic or percussive nature but also contain noise-like sounds that are neither clearly harmonic nor percussive. Furthermore, depending on the parameter settings, one often can observe a leakage of harmonic sounds into the percussive component and vice versa. In this paper we present two extensions to a state-of-the-art harmonic-percussive separation procedure to target these problems. First, we introduce a separation factor parameter into the decomposition process that allows for tightening separation results and for enforcing the components to be clearly harmonic or percussive. As second contribution, inspired by the classical sines+transients+noise (STN) audio model, this novel concept is exploited to add a third residual component to the decomposition which captures the sounds that lie in between the clearly harmonic and percussive sounds of the audio signal."
99,Frederick Z. Yen;Yin-Jyun Luo;Tai-Shih Chi,Singing Voice Separation Using Spectro-Temporal Modulation Features.,2014,https://doi.org/10.5281/zenodo.1417695,Frederick Yen+National Chiao-Tung University>TWN>education;Yin-Jyun Luo+National Chiao-Tung University>TWN>education;Tai-Shih Chi+National Chiao-Tung University>TWN>education,"An auditory-perception inspired singing voice separation algorithm for monaural music recordings is proposed in this paper. Under the framework of computational auditory scene analysis (CASA), the music recordings are first transformed into auditory spectrograms. After extracting the spectral-temporal modulation contents of the time-frequency (T-F) units through a two-stage auditory model, we define modulation features pertaining to three categories in music audio signals: vocal, harmonic, and percussive. The T-F units are then clustered into three categories and the singing voice is synthesized from T-F units in the vocal category via time-frequency masking. The algorithm was tested using the MIR-1K dataset and demonstrated comparable results to other unsupervised masking approaches. Meanwhile, the set of novel features gives a possible explanation on how the auditory cortex analyzes and identifies singing voice in music audio mixtures."
100,Tomohiko Nakamura;Kotaro Shikata;Norihiro Takamune;Hirokazu Kameoka,Harmonic-Temporal Factor Decomposition Incorporating Music Prior Information for Informed Monaural Source Separation.,2014,https://doi.org/10.5281/zenodo.1417463,Tomohiko Nakamura+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility;Kotaro Shikata+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility;Norihiro Takamune+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility;Hirokazu Kameoka+The University of Tokyo>JPN>education|NTT Communication Science Laboratories>JPN>facility,"For monaural source separation two main approaches have thus far been adopted. One approach involves applying non-negative matrix factorization (NMF) to an observed magnitude spectrogram, interpreted as a non-negative matrix. The other approach is based on the concept of computational auditory scene analysis (CASA). A CASA-based approach called the “harmonic-temporal clustering (HTC)” aims to cluster the time-frequency components of an observed signal based on a constraint designed according to the local time-frequency structure common in many sound sources (such as harmonicity and the continuity of frequency and amplitude modulations). This paper proposes a new approach for monaural source separation called the “Harmonic-Temporal Factor Decomposition (HTFD)” by introducing a spectrogram model that combines the features of the models employed in the NMF and HTC approaches. We further describe some ideas how to design the prior distributions for the present model to incorporate musically relevant information into the separation scheme."
101,Mi Tian;György Fazekas;Dawn A. A. Black;Mark B. Sandler,Design And Evaluation of Onset Detectors using Different Fusion Policies.,2014,https://doi.org/10.5281/zenodo.1415002,"Mi Tian+Centre for Digital Music, Queen Mary University of London>GBR>education;György Fazekas+Centre for Digital Music, Queen Mary University of London>GBR>education;Dawn A. A. Black+Centre for Digital Music, Queen Mary University of London>GBR>education;Mark Sandler+Centre for Digital Music, Queen Mary University of London>GBR>education","Note onset detection is one of the most investigated tasks in Music Information Retrieval (MIR) and various detection methods have been proposed in previous research. The primary aim of this paper is to investigate different fusion policies to combine existing onset detectors, thus achieving better results. Existing algorithms are fused using three strategies, first by combining different algorithms, second, by using the linear combination of detection functions, and third, by using a late decision fusion approach. Large scale evaluation was carried out on two published datasets and a new percussion database composed of Chinese traditional instrument samples. An exhaustive search through the parameter space was used enabling a systematic analysis of the impact of each parameter, as well as reporting the most generally applicable parameter settings for the onset detectors and the fusion. We demonstrate improved results attributed to both fusion and the optimised parameter settings."
102,Matthew E. P. Davies;Sebastian Böck,Evaluating the Evaluation Measures for Beat Tracking.,2014,https://doi.org/10.5281/zenodo.1415128,Mathew E. P. Davies+INESC TEC>PRT>facility;Sebastian Böck+Johannes Kepler University>AUT>education,"The evaluation of audio beat tracking systems is normally addressed in one of two ways. One approach is for human listeners to judge performance by listening to beat times mixed as clicks with music signals. The more common alternative is to compare beat times against ground truth annotations via one or more of the many objective evaluation measures. However, despite a large body of work in audio beat tracking, there is currently no consensus over which evaluation measure(s) to use, meaning multiple accuracy scores are typically reported. In this paper, we seek to evaluate the evaluation measures by examining the relationship between objective accuracy scores and human judgements of beat tracking performance. First, we present the raw correlation between objective scores and subjective ratings, and show that evaluation measures which allow alternative metrical levels appear more correlated than those which do not. Second, we explore the effect of parameterisation of objective evaluation measures, and demonstrate that correlation is maximised for smaller tolerance windows than those currently used. Our analysis suggests that true beat tracking performance is currently being overestimated via objective evaluation."
103,Maura Church;Michael Scott Cuthbert,Improving Rhythmic Transcriptions via Probability Models Applied Post-OMR.,2014,https://doi.org/10.5281/zenodo.1416752,Maura Church+Harvard University>USA>education|Google Inc.>USA>company;Michael Scott Cuthbert+M.I.T.>USA>education,"Despite many improvements in the recognition of graphical elements, even the best implementations of Optical Music Recognition (OMR) introduce inaccuracies in the resultant score. These errors, particularly rhythmic errors, are time consuming to fix. Most musical compositions repeat rhythms between parts and at various places throughout the score. Information about rhythmic self-similarity, however, has not previously been used in OMR systems. This paper describes and implements methods for using the prior probabilities for rhythmic similarities in scores produced by a commercial OMR system to correct rhythmic errors which cause a contradiction between the notes of a measure and the underlying time signature. Comparing the OMR output and post-correction results to hand-encoded scores of 37 polyphonic pieces and movements (mostly drawn from the classical repertory), the system reduces incorrect rhythms by an average of 19% (min: 2%, max: 36%). The paper includes a public release of an implementation of the model in music21 and also suggests future refinements and applications to pitch correction that could further improve the accuracy of OMR systems."
104,Sebastian Stober;Daniel J. Cameron;Jessica A. Grahn,Classifying EEG Recordings of Rhythm Perception.,2014,https://doi.org/10.5281/zenodo.1415734,Sebastian Stober+Western University>CAN>education;Daniel J. Cameron+Western University>CAN>education;Jessica A. Grahn+Western University>CAN>education,"Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. In this paper, we present first classification results using deep learning techniques on EEG data recorded within a rhythm perception study in Kigali, Rwanda. We tested 13 adults, mean age 21, who performed three behavioral tasks using rhythmic tone sequences derived from either East African or Western music. For the EEG testing, 24 rhythms – half East African and half Western with identical tempo and based on a 2-bar 12/8 scheme – were each repeated for 32 seconds. During presentation, the participants’ brain waves were recorded via 14 EEG channels. We applied stacked denoising autoencoders and convolutional neural networks on the collected data to distinguish African and Western rhythms on a group and individual participant level. Furthermore, we investigated how far these techniques can be used to recognize the individual rhythms."
