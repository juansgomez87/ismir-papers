Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Jin Ha Lee;J. Stephen Downie;Sally Jo Cunningham,Challenges in Cross-Cultural/Multilingual Music Information Seeking.,2005,https://doi.org/10.5281/zenodo.1416706,"Jin Ha Lee, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;Sally Jo Cunningham, Dept. of Computer Science, University of Waikato, New Zealand",Understanding and meeting the needs of a broad range of music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multilingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from a Korean knowledge search portal Naver 지식 (knowledge) iN and 150 queries from Google Answers website. We conclude that new sets of access points must be developed to accommodate music queries that cross cultural or language boundaries.
1,Cynthia M. Grund,"Music Information Retrieval, Memory and Culture: Some Philosohpical Remarks.",2005,https://doi.org/10.5281/zenodo.1415626,"Cynthia M. Grund, Institute of Philosophy, Education and the Study of Religions, University of Southern Denmark","The purpose of this paper is twofold: the first goal is to highlight and briefly discuss a selection of these issues, while the second is to make a case for increased mutual awareness of each other on the parts of MIR and of humanistic research. Many traditional debates within the latter receive infusions of new perspectives from MIR, while research within MIR could be fruitfully pointed in directions suggested by questions of interest within traditional research in the humanities, e.g. the relationship of individual memory to cultural memory, issues regarding cross-cultural understanding and the importance of authenticity in artistic contexts."
2,Noris Mohd. Norowi;Shyamala Doraisamy;Rahmita Wirza O. K. Rahmat,Factors Affecting Automatic Genre Classification: An Investigation Incorporating Non-Western Musical Forms.,2005,https://doi.org/10.5281/zenodo.1418067,"Noris Mohd Norowi, Faculty of Computer Science and Information Technology, University Putra Malaysia;Shyamala Doraisamy, Faculty of Computer Science and Information Technology, University Putra Malaysia;Rahmita Wirza, Faculty of Computer Science and Information Technology, University Putra Malaysia","The number of studies investigating automated genre classification is growing following the increasing amounts of digital audio data available. The underlying techniques to perform automated genre classification in general include feature extraction and classification. This study investigates the factors affecting automated genre classification. As for the dataset, most studies in this area work with western genres and traditional Malay music is incorporated in this study. Eight genres were introduced; Dikir Barat, Etnik Sabah, Inang, Joget, Keroncong, Tumbuk Kalang, Wayang Kulit, and Zapin. A total of 417 tracks from various Audio Compact Discs were collected and used as the dataset. Results show that various factors such as the musical features extracted, classifiers employed, the size of the dataset, excerpt length, excerpt location and test set parameters improve classification results."
3,Markus Schedl;Peter Knees;Gerhard Widmer,Discovering and Visualizing Prototypical Artists by Web-Based Co-Occurrence Analysis.,2005,https://doi.org/10.5281/zenodo.1418315,"Markus Schedl, Johannes Kepler University (JKU), Austrian Research Institute for Artiﬁcial Intelligence (ÖFAI);Peter Knees, Johannes Kepler University (JKU);Gerhard Widmer, Johannes Kepler University (JKU), Austrian Research Institute for Artiﬁcial Intelligence (ÖFAI)","Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository."
4,Ian Knopke,Geospatial Location of Music and Sound Files for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1417765,"Ian Knopke, McGill Music Technology","A relatively new avenue of Web-based information retrieval research is the use of geographical information to locate resources. This paper introduces a technique for locating sound and music files geographically by combining information extracted from the Web with geospatial location data. The results demonstrate the potential for Music Information Retrieval (MIR) to utilize the vast amount of audio materials on the Web within a physical and geographical context. The paper also discusses potential applications of these techniques, such as geospatial music web browsing, music marketing, and bandwidth optimization."
5,Thomas Lidy;Andreas Rauber,Evaluation of Feature Extractors and Psycho-Acoustic Transformations for Music Genre Classification.,2005,https://doi.org/10.5281/zenodo.1416856,"Thomas Lidy, Vienna University of Technology;Andreas Rauber, Vienna University of Technology","We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy."
6,Cory McKay;Rebecca Fiebrink;Daniel McEnnis;Beinan Li;Ichiro Fujinaga,ACE: A Framework for Optimizing Music Classification.,2005,https://doi.org/10.5281/zenodo.1415720,"Cory McKay, Music Technology, McGill University;Rebecca Fiebrink, Music Technology, McGill University;Daniel McEnnis, Music Technology, McGill University;Beinan Li, Music Technology, McGill University;Ichiro Fujinaga, Music Technology, McGill University","This paper presents ACE (Autonomous Classification Engine), a framework for using and optimizing classifiers. Given a set of feature vectors, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality reduction techniques in order to arrive at a good configuration for the problem at hand. In addition to evaluating classification methodologies in terms of success rates, functionality is also being incorporated into ACE allowing users to specify constraints on training and classification times as well as on the amount of time that ACE has to arrive at a solution. ACE is designed to facilitate classification for those new to pattern recognition as well as provide flexibility for those with more experience. ACE is packaged with audio and MIDI feature extraction software, although it can certainly be used with existing feature extractors. This paper includes a discussion of ways in which existing general-purpose classification software can be adapted to meet the needs of music researchers and shows how these ideas have been implemented in ACE. A standardized XML format for communicating features and other information to classifiers is proposed. A special emphasis is placed on the potential of classifier ensembles, which have remained largely untapped by the MIR community to date. A brief theoretical discussion of ensemble classification is presented in order to promote this powerful approach."
7,Koen Tanghe;Micheline Lesaffre;Sven Degroeve;Marc Leman;Bernard De Baets;Jean-Pierre Martens,Collecting Ground Truth Annotations for Drum Detection in Polyphonic Music.,2005,https://doi.org/10.5281/zenodo.1417715,"Koen Tanghe, IPEM, Department of Musicology, Ghent University;Micheline Lesaffre, IPEM, Department of Musicology, Ghent University;Sven Degroeve, Department of Applied Mathematics, Biometrics and Process Control, Ghent University;Marc Leman, IPEM, Department of Musicology, Ghent University;Bernard De Baets, Department of Applied Mathematics, Biometrics and Process Control, Ghent University;Jean-Pierre Martens, Department of Electronics and Information Systems, Ghent University","In order to train and test algorithms for drum detection in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for real-world music fragments containing different drum event types. The annotators were experienced drummers or percussionists. The purpose of this paper is to provide annotation data for algorithm training and evaluation, describe a practical way of setting up a drum annotation task, and report issues that came up during the annotation sessions."
8,Gavin Wood;Simon O'Keefe,On Techniques for Content-Based Visual Annotation to Aid Intra-Track Music Navigation.,2005,https://doi.org/10.5281/zenodo.1417401,"Gavin Wood, University of York;Simon O’Keefe, University of York","Despite the fact that people are increasingly listening to music electronically, the core interface of the common tools for playing the music have had very little improvement. In particular the tools for intra-track navigation have remained basically static, not taking advantage of recent studies into the field of audio jisting, summarising and segmentation. We introduce a novel mechanism for musical audio linear summarisation and modify a widely used open source media player to utilise several music information retrieval techniques directly in the graphical user interface. With a broad range of music, we provide a qualitative discussion on several techniques used for content-based music information retrieval and perform quantitative investigation to their usefulness."
9,Christopher Harte;Mark B. Sandler;Samer A. Abdallah;Emilia Gómez,Symbolic Representation of Musical Chords: A Proposed Syntax for Text Annotations.,2005,https://doi.org/10.5281/zenodo.1415114,"Christopher Harte, Mark Sandler and Samer Abdallah, Centre for Digital Music, Queen Mary, University of London;Emilia G´omez, Music Technology Group, IUA, Universitat Pompeu Fabra","In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs."
10,Mika Kuuskankare;Mikael Laurson,Annotating Musical Scores in ENP.,2005,https://doi.org/10.5281/zenodo.1417805,"Mika Kuuskankare, Sibelius Academy;Mikael Laurson, Sibelius Academy","The focus of this paper is on ENP-expressions that can be used for annotating ENP scores with user deﬁnable information. ENP is a music notation program written in Lisp and CLOS with a special focus on compositional and music analytical applications. We present a number of built-in expressions suitable for visualizing, for example, music analytical information as a part of music notation. A Lisp and CLOS based system for creating user-deﬁnable annotation information is also presented along with some sample algorithms. Finally, our system for automatically analyzing and annotating an ENP score is illustrated through several examples including some dealing with music information retrieval."
11,Perfecto Herrera;Òscar Celma;Jordi Massaguer;Pedro Cano;Emilia Gómez;Fabien Gouyon;Markus Koppenberger,MUCOSA: A Music Content Semantic Annotator.,2005,https://doi.org/10.5281/zenodo.1415980,"Perfecto Herrera, Universitat Pompeu Fabra;Òscar Celma, Universitat Pompeu Fabra;Jordi Massaguer, Universitat Pompeu Fabra;Pedro Cano, Universitat Pompeu Fabra;Emilia Gómez, Universitat Pompeu Fabra;Fabien Gouyon, Universitat Pompeu Fabra;Markus Koppenberger, Universitat Pompeu Fabra;David García, Universitat Pompeu Fabra;José-Pedro García, Universitat Pompeu Fabra;Nicolas Wack, Universitat Pompeu Fabra","MUCOSA (Music Content Semantic Annotator) is an environment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with micro-annotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. across-files annotations), and a collaborative annotation subsystem, which manages large-scale annotation tasks that can be shared among different research centres. The annotation client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes tools for automatic generation of unary descriptors, invention of new descriptors, and propagation of descriptors across sub-collections or playlists. Finally, the collaborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results between several research institutions. A collection of annotated songs is available, as a “starter pack” to all the individuals or institutions that are eager to join this initiative."
12,Shoichiro Saito;Hirokazu Kameoka;Takuya Nishimoto;Shigeki Sagayama,Specmurt Analysis of Multi-Pitch Music Signals with Adaptive Estimation of Common Harmonic Structure .,2005,https://doi.org/10.5281/zenodo.1417707,"Shoichiro Saito, Graduate School of Information Science and Technology, The University of Tokyo;Hirokazu Kameoka, Graduate School of Information Science and Technology, The University of Tokyo;Takuya Nishimoto, Graduate School of Information Science and Technology, The University of Tokyo;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo","This paper describes a multi-pitch analysis method using specmurt analysis with iterative estimation of the quasi-optimal common harmonic structure function. The iterative algorithm proposed in this paper automatically chooses a proper structure, which results in finding concurrent multiple fundamental frequencies and reduces the dependency on heuristically chosen initial common harmonic structure. The experimental evaluation showed promising results."
13,Olivier Gillet;Gaël Richard,Drum Track Transcription of Polyphonic Music Using Noise Subspace Projection.,2005,https://doi.org/10.5281/zenodo.1415606,"Olivier Gillet, GET / T´el´ecom Paris CNRS LTCI;Ga¨el Richard, GET / T´el´ecom Paris CNRS LTCI","This paper presents a novel drum transcription system for polyphonic music. The system uses a band-wise harmonic/noise decomposition to suppress the deterministic part of the signal contributed by non-rhythmic instruments. The transcription is then performed on the residual noise signal, which contains most of the rhythmic information. The events associated with each onset are classified using support vector machines with probabilistic outputs. The system achieves precision and recall rates of 84% for the bass drum and snare drum detection tasks."
14,Nick Collins,Using a Pitch Detector for Onset Detection.,2005,https://doi.org/10.5281/zenodo.1417309,"Nick Collins, University of Cambridge","A segmentation strategy is explored for monophonic instrumental pitched non-percussive material (PNP) which proceeds from the assertion that human-like event analysis can be founded on a notion of stable pitch percept. A constant-Q pitch detector following the work of Brown and Puckette provides pitch tracks which are post processed in such a way as to identify likely transitions between notes. A core part of this preparation of the pitch detector signal is an algorithm for vibrato suppression. An evaluation task is undertaken on slow attack and high vibrato PNP source files with human annotated onsets, exemplars of a difficult case in monophonic source segmentation. The pitch track onset detection algorithm shows an improvement over the previous best performing algorithm from a recent comparison study of onset detectors. Whilst further timbral cues must play a part in a general solution, the method shows promise as a component of a note event analysis system."
15,Parag Chordia,Segmentation and Recognition of Tabla Strokes.,2005,https://doi.org/10.5281/zenodo.1416000,"Parag Chordia, CCRMA, Stanford University","A system that segments and labels tabla strokes from real performances is described. Performance is evaluated on a large database taken from three performers under different recording conditions, containing a total of 16,834 strokes. The current work extends previous work by Gillet and Richard (2003) on categorizing tabla strokes, by using a larger, more diverse database that includes their data as a benchmark, and by testing neural networks and tree-based classification methods. First, the time-domain signal was segmented using complex-domain thresholding that looked for sudden changes in amplitude and phase discontinuities. At the optimal point on the ROC curve, false positives were less than 1% and false negatives were less than 2%. Then, classification was performed using a multivariate Gaussian model (mv gauss) as well as non-parametric techniques such as probabilistic neural networks (pnn), feed-forward neural networks (ffnn), and tree-based classifiers. Two evaluation protocols were used. The first used 10-fold cross validation. The recognition rate averaged over several experiments that contained 10-15 classes was 92% for the mv gauss, 94% for the ffnn and pnn, and 84% for the tree based classifier. To test generalization, a more difficult independent evaluation was undertaken in which no test strokes came from the same recording as the training strokes. The average recognition rate over a wide variety of test conditions was 76% for the mv gauss, 83% for the ffnn, 76% for the pnn, and 66% for the tree classifier."
16,Hirokazu Kameoka;Takuya Nishimoto;Shigeki Sagayama,Harmonic-Temporal Clustering via Deterministic Annealing EM Algorithm for Audio Feature Extraction.,2005,https://doi.org/10.5281/zenodo.1417629,"Hirokazu Kameoka, Graduate School of Information Science and Technology, The University of Tokyo;Takuya Nishimoto, Graduate School of Information Science and Technology, The University of Tokyo;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo","This paper proposes “harmonic-temporal structured clustering (HTC) method”, that allows simultaneous estimation of pitch, intensity, onset, duration, etc., of each underlying source in multi-stream audio signal, which we expect to be an effective feature extraction for MIR systems. STC decomposes the energy patterns diffused in time-frequency space, i.e., a time series of power spectrum, into distinct clusters such that each of them is originated from a single sound stream. It becomes clear that the problem is equivalent to geometrically approximating the observed time series of power spectrum by superimposed harmonic-temporal structured models (HTMs), whose parameters are directly associated with the specific acoustic characteristics. The update equations in DA(Deterministic Annealing)EM algorithm for the optimal parameter convergence are derived by formulating the model with Gaussian kernel representation. The experiment showed promising results, and verified the potential of the proposed method."
17,Jenn Riley,Exploiting Musical Connections: A Proposal for Support of Work Relationships in a Digital Music Library.,2005,https://doi.org/10.5281/zenodo.1415660,"Jenn Riley, Indiana University Digital Library Program","ABSTRACT
Musical works in the Western art music tradition exist in a complex, inter-related web. Works that are derivative or part of another work are common; however, most music information retrieval systems, including traditional library catalogs, don’t use these essential relationships to improve search results or provide information about them to end-users. As part of the NSF-funded Variations2 Digital Music Library project at Indiana University, we have developed a set of functional requirements defining how derivative and whole/part relationships between musical works should be acted upon in search results, and how these results should be displayed. This paper describes recent research into these relationships, provides examples why they are important in Western art music, outlines how Variations2 or any other music information retrieval system could use these relationships in matching user queries, and describes optimal displays of these relationships to end-users."
18,Ajay Kapur;Richard I. McWalter;George Tzanetakis,New Music Interfaces for Rhythm-Based Retrieval.,2005,https://doi.org/10.5281/zenodo.1418313,"Ajay Kapur, University of Victoria;Richard I. McWalter, University of Victoria;George Tzanetakis, University of Victoria","In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and Indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of performance practice."
19,Ioannis Karydis;Alexandros Nanopoulos;Apostolos N. Papadopoulos;Dimitrios Katsaros 0001;Yannis Manolopoulos,Content-Based Music Information Retrieval in Wireless Ad-Hoc Networks.,2005,https://doi.org/10.5281/zenodo.1417665,"Ioannis Karydis, Aristotle University, Thessaloniki 54124, Greece;Alexandros Nanopoulos, Aristotle University, Thessaloniki 54124, Greece;Dimitrios Katsaros, Aristotle University, Thessaloniki 54124, Greece;Yannis Manolopoulos, Aristotle University, Thessaloniki 54124, Greece;Apostolos Papadopoulos, ",This paper introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks. The authors investigate the challenges posed by the wireless medium and propose novel techniques that reduce response times and traffic compared to naive approaches. Experimental results demonstrate the effectiveness and efficiency of the proposed method in this bandwidth-starving and volatile environment.
20,Richard Lobb;Tim Bell;David Bainbridge 0001,Fast Capture of Sheet Music for an Agile Digital Music Library.,2005,https://doi.org/10.5281/zenodo.1417989,"Richard Lobb, Department of Computer Science and Software Engineering, University of Canterbury;Tim Bell, Department of Computer Science and Software Engineering, University of Canterbury;David Bainbridge, Department of Computer Science, University of Waikato","A personal digital music library needs to be “agile”, that is, it needs to make it easy to capture and index material on the ﬂy. A digital camera is a particularly effective way of achieving this, but there are several issues with the quality of the captured image, including distortions in the shape of the image due to the camera not being aligned properly with the page, non-planarity of the page, lens distortion from close-up shots, and inconsistent lighting across the page. In this paper we explore ways to improve the quality of music images captured by a digital camera or an inexpensive scanner, where the user is not expected to pay a lot of attention to the process. Such pre-processing will significantly aid Music Information Retrieval indexing through Optical Music Recognition, for example. The research presented here is primarily based around using a Fast Fourier Transform (FFT) to determine the orientation of the page. We ﬁnd that a windowed FFT is effective at correcting rotational errors, and we make significant progress towards removing perspective distortion introduced by the camera not being parallel with the music."
21,Rainer Typke;Frans Wiering;Remco C. Veltkamp,A Survey of Music Information Retrieval Systems.,2005,https://doi.org/10.5281/zenodo.1417383,"Rainer Typke, Universiteit Utrecht;Frans Wiering, Universiteit Utrecht;Remco C. Veltkamp, Universiteit Utrecht","This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks."
22,Graham E. Poliner;Daniel P. W. Ellis,A Classification Approach to Melody Transcription.,2005,https://doi.org/10.5281/zenodo.1414796,"Graham E. Poliner and Daniel P.W. Ellis, LabROSA, Dept. of Electrical Engineering, Columbia University","Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. In contrast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Support Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems."
23,Emilios Cambouropoulos;Maxime Crochemore;Costas S. Iliopoulos;Manal Mohamed;Marie-France Sagot,A Pattern Extraction Algorithm for Abstract Melodic Representations that Allow Partial Overlapping of Intervallic Categories.,2005,https://doi.org/10.5281/zenodo.1415008,"Emilios Cambouropoulos, Department of Music Studies, University of Thessaloniki, 540006, Thessaloniki, Greece;Maxime Crochemore, Institut Gaspard-Monge, University of Marne-la-Vallée, 77454 Marne-la-Vallée CEDEX 2, France;Costas Iliopoulos, Department of Computer Science, King’s College London, London WC2R 2LS, England;Manal Mohamed, Department of Computer Science, King’s College London, London WC2R 2LS, England;Marie-France Sagot, INRIA Rhône-Alpes, Université Claude Bernard, 43 Bd du 11 novembre 1918, 69622 Villeurbanne cedex, France","This paper proposes an efficient pattern extraction algorithm that can be applied on melodic sequences that are represented as strings of abstract intervallic symbols; the melodic representation introduces special “don’t care” symbols for intervals that may belong to two partially overlapping intervallic categories. As a special case the well established “step-leap” representation is examined. In the step-leap representation, each melodic diatonic interval is classified as a step (±s), a leap (±l) or a unison (u). Binary don’t care symbols are introduced to represent the possible overlapping between the various abstract categories e.g. ∗ = s, ∗ = l and # = −s, # = −l. For such a sequence, we are interested in finding maximal repeating pairs and repetitions with a hole (two matching subsequences separated with an intervening non-matching symbol). We propose an O(n + d(n − d) + z)-time algorithm for computing all such repetitions in a given sequence x = x[1..n] with d binary don’t care symbols, where z is the output size."
24,Rui Pedro Paiva,On the Detection of Melody Notes in Polyphonic Audio.,2005,https://doi.org/10.5281/zenodo.1417681,"Rui Pedro Paiva, CISUC – Centre for Informatics and Systems of the University of Coimbra;Teresa Mendes, CISUC – Centre for Informatics and Systems of the University of Coimbra;Amílcar Cardoso, CISUC – Centre for Informatics and Systems of the University of Coimbra","This paper describes a method for melody detection in polyphonic musical signals. Our approach starts by obtaining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, low-salience and harmonically related notes are then eliminated. Finally, the notes comprising the melody are extracted. This is the main topic of this paper. We select the melody notes by making use of note saliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour. Finally, false positives in the extracted melody should be eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or durations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives."
25,Wei-Ho Tsai;Hung-Ming Yu;Hsin-Min Wang,Query-By-Example Technique for Retrieving Cover Versions of Popular Songs with Similar Melodies.,2005,https://doi.org/10.5281/zenodo.1415200,"Wei-Ho Tsai, Institute of Information Science, Academia Sinica;Hung-Ming Yu, Institute of Information Science, Academia Sinica;Hsin-Min Wang, Institute of Information Science, Academia Sinica","Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of songs specified by users. The technique enables users to listen to songs with an identical tune, but performed by different singers, in different languages, genres, and so on. The proposed system is built on a query-by-example framework, which takes a fragment of the song submitted by the user as input, and returns songs similar to the query in terms of the main melody as output. To handle the likely discrepancies, e.g., tempos, transpositions, and accompaniments between cover versions and the original song, methods are presented to remove the non-vocal portions of the song, extract the sung notes from the accompanied vocals, and compare the similarities between the sung note sequences."
26,Olivier Lartillot,Efficient Extraction of Closed Motivic Patterns in Multi-Dimensional Symbolic Representations of Music.,2005,https://doi.org/10.5281/zenodo.1418129,"Olivier Lartillot, University of Jyv¨askyl¨a","An efficient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is usually filtered using global selective mechanisms, based on pattern frequency and length. The proposed approach is founded instead on the concept of closed pattern, and ensures lossless compression through an adaptive selection of most specific descriptions in the multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling the filtering of another form of combinatorial redundancy provoked by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalizing particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but requires instead a complex and detailed modeling of the cognitive system ruling the listening processes. The resulting algorithm is able to offer for the first time compact and relevant motivic analyses of monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex motivic transformations."
27,Norman H. Adams;Daniela Marquez;Gregory H. Wakefield,Iterative Deepening for Melody Alignment and Retrieval.,2005,https://doi.org/10.5281/zenodo.1415712,"Norman Adams, University of Michigan;Daniela Marquez, University of Michigan;Gregory Wakefield, University of Michigan","For melodic theme retrieval there is a fundamental trade-off between retrieval performance and retrieval speed. Melodic representations of large dimension yield the best retrieval performance, but at high computational cost, and vice versa. In the present work we explore the use of iterative deepening to achieve robust retrieval performance, but without the accompanying computational burden. In particular, we propose the use of a smooth pitch contour that facilitates query and target representations of variable length. We implement an iterative query-by-humming system that yields a dramatic increase in speed, without degrading performance compared to contemporary retrieval systems. Furthermore, we expand the conventional iterative framework to retain the alignment paths found in each iteration. These alignment paths are used to adapt the alignment window of subsequent iterations, further expediting retrieval without degrading performance."
28,Jeremy Pickens;Costas S. Iliopoulos,Markov Random Fields and Maximum Entropy Modeling for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1414716,"Jeremy Pickens, Department of Computer Science, King’s College London;Costas Iliopoulos, Department of Computer Science, King’s College London","Music information retrieval is a complex task that involves finding models or measures to determine the similarity between two pieces of music. However, the features extracted from music sources are often single-dimensional or assumed to be orthogonal. In this paper, the authors present a framework that allows for the combination of non-independent features without making the independence assumption. The effectiveness of the framework is demonstrated through evaluation on the polyphonic theme similarity task. The framework is general and can be applied to a range of music information retrieval tasks."
29,Bryan Pardo;Manan Sanghi,Polyphonic Musical Sequence Alignment for Database Search.,2005,https://doi.org/10.5281/zenodo.1417909,"Bryan Pardo, Computer Science Department, Northwestern University;Manan Sanghi, Computer Science Department, Northwestern University","Finding the best matching database target to a melodic query has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces. However, most tonal music is polyphonic, with multiple concurrent musical lines. Such pieces are not adequately represented as strings. Moreover, users often represent polyphonic pieces in their queries by skipping from one part (the soprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. These approaches are compared using synthetic queries on a database of Bach pieces. Results indicate that when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not."
30,Ning Hu;Roger B. Dannenberg,A Bootstrap Method for Training an Accurate Audio Segmenter.,2005,https://doi.org/10.5281/zenodo.1416200,"Ning Hu, Carnegie Mellon University;Roger B. Dannenberg, Carnegie Mellon University","Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and hand-labeling is quite difficult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are first used to find the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to refine the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data."
31,Pierre Roy;Jean-Julien Aucouturier;François Pachet;Anthony Beurivé,Exploiting the Tradeoff Between Precision and Cpu-Time to Speed Up Nearest Neighbor Search.,2005,https://doi.org/10.5281/zenodo.1417453,"Pierre Roy, SONY Computer Science Laboratory Paris;Jean-Julien Aucouturier, SONY Computer Science Laboratory Paris;Franc¸ois Pachet, SONY Computer Science Laboratory Paris;Anthony Beuriv´e, SONY Computer Science Laboratory Paris","We describe an incremental ﬁltering algorithm to quickly compute the N nearest neighbors according to a similarity measure in a metric space. The algorithm exploits an intrinsic property of a large class of similarity measures for which some parameter p has a positive influence both on the precision and the cpu cost (precision-cputime trade-off). The algorithm uses successive approximations of the measure to compute ﬁrst cheap distances on the whole set of possible items, then more and more expensive measures on smaller and smaller sets. We illustrate the algorithm on the case of a timbre similarity algorithm, which compares gaussian mixture models using a Monte Carlo approximation of the Kullback-Leibler distance, where p is the number of points drawn from the distributions. We describe several Monte Carlo algorithmic variants, which improve the convergence speed of the approximation. On this problem, the algorithm performs more than 30 times faster than the naive approach."
32,Nancy Bertin;Alain de Cheveigné,Scalable Metadata and Quick Retrieval of Audio Signals.,2005,https://doi.org/10.5281/zenodo.1417079,"Nancy Bertin, Equipe Audition CNRS UMR 8581 - ENS (DEC);Alain de Cheveign´e, Equipe Audition CNRS UMR 8581 - ENS (DEC)","Audio search algorithms have become faster and more accurate, allowing for efficient searching within large databases of audio. However, the size of the metadata used for these algorithms also increases exponentially, leading to increased storage costs and search times. This paper introduces the concept of scalable metadata, which adjusts to the increasing size of data and metadata. The authors argue that scalability is beneficial for hierarchical structures that enable fast search and demonstrate this by adapting a state-of-the-art search algorithm to a scalable indexing structure. Scalability allows search algorithms to adapt to database size increases without sacrificing performance."
33,Charles L. Parker,Applications of Binary Classification and Adaptive Boosting to the Query-By-Humming Problem.,2005,https://doi.org/10.5281/zenodo.1416482,"Charles Parker, Oregon State University","In the ""query-by-humming"" problem, the goal is to retrieve a specific song from a target set based on a sung query. Recent evaluations have shown that the state-of-the-art algorithm is a dynamic programming-based interval matching technique. In this paper, we propose an algorithm that combines techniques from artificial intelligence to outperform the current state-of-the-art with only a negligible increase in running time."
34,Ming Li;Ronan Sleep,Genre Classification via an LZ78-Based String Kernel.,2005,https://doi.org/10.5281/zenodo.1415162,"Ming Li, School of Computing Sciences, University of East Anglia;Ronan Sleep, School of Computing Sciences, University of East Anglia","We develop the notion of normalized information distance (NID) [7] into a kernel distance suitable for use with a Support Vector Machine classifier, and demonstrate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is efficient to compute, yet generates an accuracy which compares well with recent works."
35,Arthur Flexer;Elias Pampalk;Gerhard Widmer,Novelty Detection Based on Spectral Similarity of Songs.,2005,https://doi.org/10.5281/zenodo.1416504,"Arthur Flexer, Institute of Medical Cybernetics and Artiﬁcial Intelligence, Center for Brain Research, Medical University of Vienna;Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Gerhard Widmer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Department of Computational Perception, Johannes Kepler University","We are introducing novelty detection, i.e. the automatic identification of new or unknown data not covered by the training data, to the field of music information retrieval. Two methods for novelty detection - one based solely on the similarity information and one also utilizing genre label information - are evaluated within the context of genre classification based on spectral similarity. Both are shown to perform equally well."
36,Richard Stenzel;Thomas Kamps,Improving Content-Based Similarity Measures by Training a Collaborative Model.,2005,https://doi.org/10.5281/zenodo.1416090,"Richard Stenzel, Fraunhofer IPSI;Thomas Kamps, Fraunhofer IPSI","We observed that for multimedia data – especially music - collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with >250,000,000 collaborative data points crawled from the web and >190,000 songs annotated with content-based sound feature sets. A song mentioned in a playlist is regarded as one collaborative data point. In this paper we present a novel approach to bridging the performance gap between collaborative and content-based similarity measures. In the initial training phase a model vector for each song is computed, based on collaborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using explicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale e-commerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vectors. These predicted model vectors are finally used to compute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that offer a collaborative similarity recommender as service to their customers."
37,Fabio Vignoli;Steffen Pauws,A Music Retrieval System Based on User Driven Similarity and Its Evaluation.,2005,https://doi.org/10.5281/zenodo.1418359,"Fabio Vignoli, Philips Research Laboratories;Steffen Pauws, Philips Research Laboratories","Large music collections require new ways to let users interact with their music. The concept of finding ‘similar’ songs, albums, or artists provides handles to users for easy navigation and instant retrieval. This paper presents the realization and user evaluation of a music retrieval music that sorts songs on the basis of similarity to a given seed song. Similarity is based on a user-weighted combination of timbre, genre, tempo, year, and mood. A conclusive user evaluation assessed the usability of the system in comparison to two control systems in which the user control of defining the similarity measure was diminished."
38,David Meredith 0001;Geraint A. Wiggins,Comparing Pitch Spelling Algorithms.,2005,https://doi.org/10.5281/zenodo.1416366,"David Meredith, Centre for Cognition, Computation and Culture, Department of Computing, Goldsmiths’ College, University of London;Geraint A. Wiggins, Centre for Cognition, Computation and Culture, Department of Computing, Goldsmiths’ College, University of London","A pitch spelling algorithm predicts the pitch names of the notes in a musical passage when given the onset-time, MIDI note number and possibly the duration and voice of each note. Various versions of the algorithms of Longuet-Higgins, Cambouropoulos, Temperley and Sleator, Chew and Chen, and Meredith were run on a corpus containing 195972 notes, equally divided between eight classical and baroque composers. The standard deviation of the accuracies achieved by each algorithm over the eight composers was used as a measure of its style dependence (SD). Meredith’s ps1303 was the most accurate algorithm, spelling 99.43% of the notes correctly (SD = 0.54). The best version of Chew and Chen’s algorithm was the least dependent on style (SD = 0.35) and spelt 99.15% of the notes correctly. A new version of Cambouropoulos’s algorithm, combining features of all three versions described by Cambouropoulos himself, also spelt 99.15% of the notes correctly (SD = 0.47). The best version of Temperley and Sleator’s algorithm spelt 97.79% of the notes correctly, but nearly 70% of its errors were due to a single sudden enharmonic change. Longuet-Higgins’s algorithm spelt 98.21% of the notes correctly (SD = 1.79) but only when it processed the music a voice at a time."
39,Meinard Müller;Frank Kurth;Michael Clausen,Audio Matching via Chroma-Based Statistical Features.,2005,https://doi.org/10.5281/zenodo.1416800,"Meinard M¨uller, Universit¨at Bonn, Institut f¨ur Informatik III;Frank Kurth, Universit¨at Bonn, Institut f¨ur Informatik III;Michael Clausen, Universit¨at Bonn, Institut f¨ur Informatik III","In this paper, we describe an efficient method for audio matching which performs effectively for a wide range of classical music. The basic goal of audio matching can be described as follows: consider an audio database containing several CD recordings for one and the same piece of music interpreted by various musicians. Then, given a short query audio clip of one interpretation, the goal is to automatically retrieve the corresponding excerpts from the other interpretations. To solve this problem, we introduce a new type of chroma-based audio feature that strongly correlates to the harmonic progression of the audio signal. Our feature shows a high degree of robustness to variations in parameters such as dynamics, timbre, articulation, and local tempo deviations. As another contribution, we describe a robust matching procedure, which allows to handle global tempo variations. Finally, we give a detailed account on our experiments, which have been carried out on a database of more than 110 hours of audio comprising a wide range of classical music."
40,Ching-Hua Chuan;Elaine Chew,Fuzzy Analysis in Pitch-Class Determination for Polyphonic Audio Key Finding.,2005,https://doi.org/10.5281/zenodo.1417297,"Ching-Hua Chuan, Department of Computer Science, University of Southern California;Elaine Chew, Epstein Dep of Industrial & Systems Eng, University of Southern California","This paper presents a fuzzy analysis technique for pitch class determination that improves the accuracy of key finding from audio information. Errors in audio key finding, typically incorrect assignments of closely related keys, commonly result from imprecise pitch class determination and biases introduced by the quality of the sound. Our technique is motivated by hypotheses on the sources of audio key finding errors, and uses fuzzy analysis to reduce the errors caused by noisy detection of lower pitches, and to refine the biased raw frequency data, in order to extract more correct pitch classes. We compare the proposed system to two others, an earlier one employing only peak detection from FFT results, and another providing direct key finding from MIDI. All three used the same key finding algorithm (Chew’s Spiral Array CEG algorithm) and the same 410 classical music pieces (ranging from Baroque to Contemporary). Considering only the first 15 seconds of music in each piece, the proposed fuzzy analysis technique outperforms the peak detection method by 12.18% on average, matches the performance of direct key finding from MIDI 41.73% of the time, and achieves an overall maximum correct rate of 75.25% (compared to 80.34% for MIDI key finding)."
41,Juan Pablo Bello;Jeremy Pickens,A Robust Mid-Level Representation for Harmonic Content in Music Signals.,2005,https://doi.org/10.5281/zenodo.1417431,"Juan P. Bello and Jeremy Pickens, Centre for Digital Music, Queen Mary, University of London","When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research."
42,Jean-François Paiement;Douglas Eck;Samy Bengio,A Probabilistic Model for Chord Progressions.,2005,https://doi.org/10.5281/zenodo.1416922,"Jean-François Paiement, IDIAP Research Institute;Douglas Eck, University of Montreal;Samy Bengio, IDIAP Research Institute","Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progressions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies."
43,J. Stephen Downie;Kris West;Andreas F. Ehmann;Emmanuel Vincent,The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview.,2005,https://doi.org/10.5281/zenodo.1416044,"J. Stephen Downie, GSLIS, University of Illinois at Urbana-Champaign;Kris West, School of Computing Sciences, University of East Anglia;Andreas Ehmann, Electrical Engineering, University of Illinois at Urbana-Champaign;Emmanuel Vincent, Electronic Engineering, Queen Mary University of London",This paper is an extended abstract which provides a brief preliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper concludes with a listing of targets items to be undertaken before MIREX 2006 to ensure the ongoing success of the MIREX framework.
44,Slim Essid;Gaël Richard;Bertrand David,Inferring Efficient Hierarchical Taxonomies for MIR Tasks: Application to Musical Instruments.,2005,https://doi.org/10.5281/zenodo.1416268,"Slim ESSID, GET-T´el´ecom Paris, CNRS LTCI;Ga¨el RICHARD, GET-T´el´ecom Paris, CNRS LTCI;Bertrand DAVID, GET-T´el´ecom Paris, CNRS LTCI","A number of approaches for automatic audio classification are based on hierarchical taxonomies since it is acknowledged that improved performance can be thereby obtained. In this paper, we propose a new strategy to automatically acquire hierarchical taxonomies, using machine learning methods, which are expected to maximize the performance of subsequent classification. It is shown that the optimal hierarchical taxonomy of musical instruments (in the sense of inter-class distances) does not follow the traditional and more intuitive instrument classification into instrument families."
45,Hiromasa Fujihara;Tetsuro Kitahara;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Singer Identification Based on Accompaniment Sound Reduction and Reliable Frame Selection.,2005,https://doi.org/10.5281/zenodo.1418285,"Hiromasa Fujihara, Dept. of Intelligence Science and Technology;Tetsuro Kitahara, Dept. of Intelligence Science and Technology;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Kazunori Komatani, Dept. of Intelligence Science and Technology;Tetsuya Ogata, Dept. of Intelligence Science and Technology;Hiroshi G. Okuno, Dept. of Intelligence Science and Technology","This paper describes a method for automatic singer identification from polyphonic musical audio signals including sounds of various instruments. The main problem in automatically identifying singers is the negative influences caused by accompaniment sounds. To solve this problem, the authors developed two methods: accompaniment sound reduction and reliable frame selection. Experimental results showed that their method was able to reduce the influences of accompaniment sounds and achieved an accuracy of 95%."
46,Shankar Vembu;Stephan Baumann 0001,Separation of Vocals from Polyphonic Audio Recordings .,2005,https://doi.org/10.5281/zenodo.1414852,"Shankar Vembu, German Research Centre for AI;Stephan Baumann, German Research Centre for AI","Source separation techniques like independent component analysis and non-negative matrix factorization are commonly used for separating individual tracks in music samples. However, these techniques struggle to separate non-stationary sources like speech or vocals. In this paper, the authors propose solutions for extracting vocal tracks from polyphonic audio recordings. They also present techniques for identifying vocal sections in a music sample and designing a classifier for vocal-nonvocal segmentation. The authors conclude that while the quality of vocal source separation is satisfactory, it is not sufficient for further analysis to extract the melody line from the vocal track. Further investigation is needed to improve the quality of vocal source separation."
47,Norman Casagrande;Douglas Eck;Balázs Kégl,Frame-Level Audio Feature Extraction Using AdaBoost.,2005,https://doi.org/10.5281/zenodo.1414718,"Norman Casagrande, University of Montreal;Douglas Eck, University of Montreal;Bal´azs K´egl, University of Montreal","In this paper we adapt an AdaBoost-based image processing algorithm to the task of predicting whether an audio signal contains speech or music. We derive a frame-level discriminator that is both fast and accurate. Using a simple FFT and no built-in prior knowledge of signal structure we obtain an accuracy of 88% on frames sampled at 20ms intervals. When we smooth the output of the classifier with the output of the previous 40 frames our forecast rate rises to 93% on the Scheirer-Slaney (Scheirer and Slaney, 1997) database. To demonstrate the efficiency and effectiveness of the model, we have implemented it as a graphical real-time plugin to the popular Winamp audio player."
48,Petri Toiviainen;Tuomas Eerola,Classification of Musical Metre with Autocorrelation and Discriminant Functions.,2005,https://doi.org/10.5281/zenodo.1416040,"Petri Toiviainen, Department of Music, University of Jyväskylä, Finland;Tuomas Eerola, Department of Music, University of Jyväskylä, Finland","The performance of autocorrelation-based metre induction was tested with two large collections of folk melodies, consisting of approximately 13,000 melodies in MIDI file format, for which the correct metres were available. The analysis included a number of melodic accents assumed to contribute to metric structure. The performance was measured by the proportion of melodies whose metre was correctly classified by Multiple Discriminant Analysis. Overall, the method predicted notated metre with an accuracy of 75% for classification into nine categories of metre. The most frequent confusions were made within the groups of duple and triple/compound metres, whereas confusions across these groups where significantly less frequent. In addition to note onset locations and note durations, Thomassen's melodic accent was found to be an important predictor of notated metre."
49,Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo,ATTA: Automatic Time-Span Tree Analyzer Based on Extended GTTM.,2005,https://doi.org/10.5281/zenodo.1415572,"Masatoshi Hamanaka, PRESTO, Japan Science and Technology Agency;Keiji Hirata, NTT Communication Science Laboratories;Satoshi Tojo, Japan Advanced Institute of Science and Technology","This paper describes a music analyzing system called the automatic time-span tree analyzer (ATTA), which we have developed. The ATTA derives a time-span tree that assigns a hierarchy of 'structural importance' to the notes of a piece of music based on the generative theory of tonal music (GTTM). Although the time-span tree has been applied with music summarization and collaborative music creation systems, these systems use time-span trees manually analyzed by experts in musicology. Previous systems based on GTTM cannot acquire a time-span tree without manual application of most of the rules, because GTTM does not resolve much of the ambiguity that exists with the application of the rules. To solve this problem, we propose a novel computational model of the GTTM that re-formalizes the rules with computer implementation. The main advantage of our approach is that we can introduce adjustable parameters, which enables us to assign priority to the rules. Our analyzer automatically acquires time-span trees by configuring the parameters that cover 26 rules out of 36 GTTM rules for constructing a time-span tree. Experimental results showed that after these parameters were tuned, our method outperformed a baseline performance. We hope to distribute the time-span tree as the content for various musical tasks, such as searching and arranging music."
50,Roger B. Dannenberg,"Toward Automated Holistic Beat Tracking, Music Analysis and Understanding.",2005,https://doi.org/10.5281/zenodo.1415246,"Roger B. Dannenberg, School of Computer Science Carnegie Mellon University","Most music processing focuses on analyzing specific features or elements such as pitch, beat location, tempo, or genre. However, music is interconnected at many levels, and the interplay of melody, harmony, and rhythm are important in perception. This paper presents a step towards more holistic music analysis by using music structure to constrain a beat tracking program. By incorporating structural information, the beat tracker shows a significant improvement. The implications of this work for other music analysis problems are discussed."
51,Kristoffer Jensen;Jieping Xu;Martin Zachariasen,Rhythm-Based Segmentation of Popular Chinese Music.,2005,https://doi.org/10.5281/zenodo.1418117,"Kristoffer Jensen, Department of Medialogy, University of Aalborg Esbjerg;Jieping Xu, School of Information, Renmin University;Martin Zachariasen, Department of Computer Science, University of Copenhagen","We present a new method to segment popular music based on rhythm. By computing a shortest path based on the self-similarity matrix calculated from a model of rhythm, segmenting boundaries are found along the diagonal of the matrix. The cost of a new segment is optimized by matching manual and automatic segment boundaries. We compile a small song database of 21 randomly selected popular Chinese songs which come from Chinese Mainland, Taiwan and Hong Kong. The segmenting results on the small corpus show that 78% manual segmentation points are detected and 74% automatic segmentation points are correct. Automatic segmentation achieved 100% correct detection for 5 songs. The results are very encouraging."
52,Frank Kurth;Meinard Müller;David Damm;Christian Fremerey;Andreas Ribbrock;Michael Clausen,Syncplayer - An Advanced System for Multimodal Music Access.,2005,https://doi.org/10.5281/zenodo.1416496,"Frank Kurth, Universit¨at Bonn;Meinard M¨uller, Universit¨at Bonn;David Damm, Universit¨at Bonn;Christian Fremerey, Universit¨at Bonn;Andreas Ribbrock, Universit¨at Bonn;Michael Clausen, Universit¨at Bonn","In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The recording is then identified by the SyncPlayer server, a process which is performed entirely content-based. Subsequently, the server delivers music-related data like scores or lyrics to the client, which are then displayed synchronously with audio playback using a multimodal visualization plug-in. In addition to visualization, the system provides functionality for content-based music retrieval and semi-manual content annotation. To the best of our knowledge, our system is moreover the first to systematically exploit automatically generated synchronization data for content-based symbolic browsing in high quality audio recordings. SyncPlayer has already proved to be a valuable tool for evaluating algorithms in MIR research on a larger scale. In this paper, we describe the technical background of the SyncPlayer framework in detail. We also give an overview of the underlying MIR techniques of audio matching, music synchronization, and text-based retrieval that are incorporated in the current version of the system."
53,Eric J. Isaacson,What You See Is What You Get: on Visualizing Music.,2005,https://doi.org/10.5281/zenodo.1415992,"Eric Isaacson, Indiana University School of Music","Though music is fundamentally an aural phenomenon, we often communicate about music through visual means. The paper examines a number of visualization techniques developed for music, focusing especially on those developed for music analysis by specialists in the field, but also looking at some less successful approaches. It is hoped that, by presenting them in this way, those in the MIR community will develop a greater awareness of the kinds of musical problems music scholars are concerned with, and might lend a hand toward addressing them."
54,Fabian Mörchen;Alfred Ultsch;Mario Nöcker;Christian Stamm,Databionic Visualization of Music Collections According to Perceptual Distance.,2005,https://doi.org/10.5281/zenodo.1417967,"Fabian Mörchen, Data Bionics Research Group;Alfred Ultsch, Data Bionics Research Group;Mario Nöcker, Data Bionics Research Group;Christian Stamm, Data Bionics Research Group;Philipps-University Marburg, ;35032 Marburg, Germany, ","We describe the MusicMiner system for organizing large collections of music with databionic mining techniques. Low level audio features are extracted from the raw audio data on short time windows during which the sound is assumed to be stationary. Static and temporal statistics were consistently and systematically used for aggregation of low level features to form high level features. A supervised feature selection targeted to model perceptual distance between different sounding music lead to a small set of non-redundant sound features. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map, displaying local sound differences based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song."
55,Masataka Goto;Takayuki Goto,"Musicream: New Music Playback Interface for Streaming, Sticking, Sorting, and Recalling Musical Pieces.",2005,https://doi.org/10.5281/zenodo.1415842,"Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Takayuki Goto, National Institute of Advanced Industrial Science and Technology (AIST)","This paper describes a novel music playback interface, called Musicream, which lets a user unexpectedly come across various musical pieces similar to those liked by the user. With most previous “query-by-example” interfaces used for similarity-based searching, for the same query and music collection a user will always receive the same list of musical pieces ranked by their similarity and opportunities to encounter unfamiliar musical pieces in the collection are limited. Musicream facilitates active, flexible, and unexpected encounters with musical pieces by providing four functions: the music-disc streaming function which creates a flow of many musical-piece entities (discs) from a (huge) music collection, the similarity-based sticking function which allows a user to easily pick out and listen to similar pieces from the flow, the meta-playlist function which can generate a playlist of playlists (ordered lists of pieces) while editing them with a high degree of freedom, and the time-machine function which automatically records all Musicream activities and allows a user to visit and retrieve a past state as if using a time machine. In our experiments, these functions were used seamlessly to achieve active and creative querying and browsing of music collections, confirming the effectiveness of Musicream."
56,Jean-Julien Aucouturier;François Pachet,Ringomatic: A Real-Time Interactive Drummer Using Constraint-Satisfaction and Drum Sound Descriptors.,2005,https://doi.org/10.5281/zenodo.1416532,"Jean-Julien Aucouturier, SONY CSL Paris;Franc¸ois Pachet, SONY CSL Paris","We describe a real-time musical agent that generates an audio drum-track by concatenating audio segments automatically extracted from pre-existing musical files. The drum-track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata automatically extracted from the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We report on several drum track audio descriptors designed for the system. We also describe a basic mechanism for controlling the tradeoff between the agent’s autonomy and reactivity, which we illustrate with experiments made in the context of a virtual duet between the system and a human pianist."
57,Samer A. Abdallah;Katy C. Noland;Mark B. Sandler;Michael A. Casey;Christophe Rhodes,Theory and Evaluation of a Bayesian Music Structure Extractor.,2005,https://doi.org/10.5281/zenodo.1416018,"Samer Abdallah, Centre for Digital Music;Katy Noland, Centre for Digital Music;Mark Sandler, Centre for Digital Music;Michael Casey, Centre for Cognition, Computation and Culture;Christophe Rhodes, Centre for Cognition, Computation and Culture","We introduce a new model for extracting classiﬁed structural segments, such as intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify signal frames on the basis of their audio properties and then to agglomerate contiguous runs of similarly classiﬁed frames into texturally homogenous (or ‘self-similar’) segments which inherit the classiﬁcaton of their constituent frames. Our work extends previous work on automatic structure extraction by addressing the classiﬁcation problem using using an unsupervised Bayesian clustering model, the parameters of which are estimated using a variant of the expectation maximisation (EM) algorithm which includes deterministic annealing to help avoid local optima. The model identiﬁes and classiﬁes all the segments in a song, not just the chorus or longest segment. We discuss the theory, implementation, and evaluation of the model, and test its performance against a ground truth of human judgements. Using an analogue of a precision-recall graph for segment boundaries, our results indicate an optimal trade-off point at approximately 80% precision for 80% recall."
58,Xavier Amatriain;Jordi Massaguer;David García;Ismael Mosquera,The CLAM Annotator: A Cross-Platform Audio Descriptors Editing Tool.,2005,https://doi.org/10.5281/zenodo.1416908,"Xavier Amatriain, CREATE;Jordi Massaguer, Universitat Pompeu Fabra;David Garcia, Universitat Pompeu Fabra;Ismael Mosquera, Universitat Pompeu Fabra","This paper presents the CLAM Annotator tool. This application has been developed in the context of the CLAM framework and can be used to manually edit any previously computed audio descriptors. The application offers a convenient GUI that allows to edit low-level frame descriptors, global descriptors of any kind and segmentation marks. It is designed in such a way that the interface adapts itself to a user-defined schema, offering possibilities to a large range of applications."
59,Tim Bell;David Blizzard;Richard D. Green;David Bainbridge 0001,Design of a Digital Music Stand.,2005,https://doi.org/10.5281/zenodo.1416290,,
60,Stuart Bray;George Tzanetakis,Distributed Audio Feature Extraction for Music.,2005,https://doi.org/10.5281/zenodo.1417563,"Stuart Bray, Computer Science Department, University of Victoria;George Tzanetakis, Computer Science Department (also in Music), University of Victoria","One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area where computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented."
61,John Ashley Burgoyne;Lawrence K. Saul,Learning Harmonic Relationships in Digital Audio with Dirichlet-Based Hidden Markov Models.,2005,https://doi.org/10.5281/zenodo.1414870,"J. Ashley Burgoyne, University of Pennsylvania;Lawrence K. Saul, University of Pennsylvania","Harmonic analysis is a standard musicological tool for understanding many pieces of Western classical music and making comparisons among them. This paper presents an approach to teach machines to analyze music in the same way that human music students do, by listening to musical recordings and performances. The approach uses a corpus of recorded Mozart symphonies and transforms the audio files into a series of normalized pitch class profile (PCP) vectors. Simplified rules of tonal harmony are encoded in a transition matrix, and a hidden Markov model (HMM) is used to train Dirichlet distributions for major and minor keys on the PCP vectors. The system successfully tracks chords and keys and shows promise for real-time implementation."
62,Giordano Ribeiro de Eulalio Cabral;François Pachet;Jean-Pierre Briot,Automatic X Traditional Descriptor Extraction: the Case of Chord Recognition.,2005,https://doi.org/10.5281/zenodo.1415702,"Giordano Cabral, LIP6 – Paris 6;François Pachet, Sony CSL Paris;Jean-Pierre Briot, LIP6 – Paris 6","Audio descriptor extraction is the activity of finding mathematical models which describe properties of the sound, requiring signal processing skills. The scientific literature presents a vast collection of descriptors (e.g. energy, tempo, tonality) each one representing a significant effort of research in finding an appropriate descriptor for a particular application. The Extractor Discovery System (EDS) is a recent approach for the discovery of such descriptors, which aim is to extract them automatically. This system can be useful for both non experts – who can let the system work fully automatically – and experts – who can start the system with an initial solution expecting it to enhance their results. Nevertheless, EDS still needs to be massively tested. We consider that its comparison with the results of problems already studied would be very useful to validate it as an effective tool. This work intends to perform the first part of this validation, comparing the results from classic approaches with EDS results when operated by a completely naïve user building a guitar chord recognizer."
63,Margaret Cahill;Donncha Ó Maidín,Melodic Similarity Algorithms -- Using Similarity Ratings for Development and Early Evaluation.,2005,https://doi.org/10.5281/zenodo.1415642,"Margaret Cahill, Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland;Donncha Ó Maidín, Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","This paper focuses on gathering similarity ratings for use in the construction, optimization and evaluation of melodic similarity algorithms. The approach involves conducting listening experiments to gather these ratings for a piece in Theme and Variation form."
64,Domenico Cantone;Salvatore Cristofaro;Simone Faro,"On Tuning the (\delta, \alpha)-Sequential-Sampling Algorithm for \delta-Approximate Matching with Alpha-Bounded Gaps in Musical Sequences.",2005,https://doi.org/10.5281/zenodo.1417791,"Domenico Cantone, Universit`a di Catania, Dipartimento di Matematica e Informatica;Salvatore Cristofaro, Universit`a di Catania, Dipartimento di Matematica e Informatica;Simone Faro, Universit`a di Catania, Dipartimento di Matematica e Informatica","We present a very efficient variant of the (δ, α)-SEQUENTIAL-SAMPLING algorithm, recently introduced by the authors, for the δ-approximate string matching problem with α-bounded gaps, which often arises in many questions on musical information retrieval and musical analysis. Though it retains the same worst-case O(mn)-time and O(mα)-space complexity of its progenitor to compute the number of distinct δ-approximate α-gapped occurrences of a pattern of length m at each position in a text of length n, our new variant achieves an average O(n)-time complexity in practical cases. Extensive experimentations indicate that our algorithm is more efficient than existing solutions for the same problem, especially in the case of long patterns."
65,Domenico Cantone;Salvatore Cristofaro;Simone Faro,"Solving the (\delta, \alpha)-Approximate Matching Problem Under Transposition Invariance in Musical Sequences.",2005,https://doi.org/10.5281/zenodo.1416892,"Domenico Cantone, Universit`a di Catania, Dipartimento di Matematica e Informatica;Salvatore Cristofaro, Universit`a di Catania, Dipartimento di Matematica e Informatica;Simone Faro, Universit`a di Catania, Dipartimento di Matematica e Informatica","The δ-approximate matching problem arises in many questions concerning musical information retrieval and musical analysis. In the case in which gaps are not allowed between consecutive pitches of the melody, transposition invariance is automatically taken care of, provided that the musical melodies are encoded using the pitch interval encoding. However, in the case in which non-null gaps are allowed between consecutive pitches of the melodies, transposition invariance is not dealt with properly by the algorithms present in literature. In this paper, we propose two slightly different variants of the approximate matching problem under transposition invariance and for each of them provide an algorithm, obtained by adapting an efficient algorithm for the δ-approximate matching problem with α-bounded gaps."
66,Òscar Celma;Miquel Ramírez;Perfecto Herrera,Foafing the Music: A Music Recommendation System based on RSS Feeds and User Preferences.,2005,https://doi.org/10.5281/zenodo.1414800,"Oscar Celma, Music Technology Group, Universitat Pompeu Fabra, Barcelona, SPAIN;Miquel Ram´ırez, Music Technology Group, Universitat Pompeu Fabra, Barcelona, SPAIN;Perfecto Herrera, Music Technology Group, Universitat Pompeu Fabra, Barcelona, SPAIN","In this paper we give an overview of the Foaﬁng the Music system. The system uses the Friend of a Friend (FOAF) and Rich Site Summary (RSS) vocabularies for recommending music to a user, depending on her musical tastes. Music information (new album releases, related artists’ news and available audio) is gathered from thousands of RSS feeds —an XML format for syndicating Web content. On the other hand, FOAF documents are used to deﬁne user preferences. The presented system provides music discovery by means of: user proﬁling —deﬁned in the user’s FOAF description—, context-based information —extracted from music related RSS feeds— and content-based de- scriptions —extracted from the audio itself."
67,Wei Chai;Barry Vercoe,Detection of Key Change in Classical Piano Music.,2005,https://doi.org/10.5281/zenodo.1415538,"Wei Chai, MIT Media Laboratory;Barry Vercoe, MIT Media Laboratory","Tonality is an important aspect of musical structure. Detecting key of music is one of the major tasks in tonal analysis and will benefit semantic segmentation of music for indexing and searching. This paper presents an HMM-based approach for segmenting musical signals based on key change and identifying the key of each segment. Classical piano music was used in the experiment. The performance, evaluated by three proposed measures (recall, precision and label accuracy), demonstrates the promise of the method."
68,Sally Jo Cunningham;J. Stephen Downie;David Bainbridge 0001,"""The Pain, the Pain"": Modelling Music Information Behavior and the Songs We Hate.",2005,https://doi.org/10.5281/zenodo.1417209,"Sally Jo Cunningham, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand;J. Stephen Downie, GSLIS, University of Illinois at Urbana-Champaign;David Bainbridge, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","The paper presents a grounded theory analysis of 395 user responses to the survey question, “What is the worst song ever?” Important factors uncovered include: lyric quality, the “earworm” effect, voice quality, the influence of associated music videos, over-exposure, perceptions of pretentiousness, and associations with unpleasant personal experiences."
69,Christophe Dalitz;Thomas Karsten,Using the Gamera Framework for Building a Lute Tablature Recognition System.,2005,https://doi.org/10.5281/zenodo.1416378,"Christoph Dalitz, Niederrhein University of Applied Sciences;Thomas Karsten, Niederrhein University of Applied Sciences","In this article we describe an optical recognition system for historic lute tablature prints that we have built with the aid of the Gamera toolkit for document analysis and recognition. We give recognition rates for various historic sources and show that our system works quite well on printed tablature sources using movable types. For engraved and manuscript sources, we discuss some principal current limitations of our system and Gamera."
70,Sven Degroeve;Koen Tanghe;Bernard De Baets;Marc Leman;Jean-Pierre Martens,A Simulated Annealing Optimization of Audio Features for Drum Classification.,2005,https://doi.org/10.5281/zenodo.1417311,"Sven Degroeve, Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium;Koen Tanghe, Department of Musicology (IPEM), Ghent University, Belgium;Bernard De Baets, Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium;Marc Leman, Department of Musicology (IPEM), Ghent University, Belgium;Jean-Pierre Martens, Department of Electronics and Information Systems (ELIS), Ghent University, Belgium",Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments.
71,Ruth Dhanaraj;Beth Logan,Automatic Prediction of Hit Songs.,2005,https://doi.org/10.5281/zenodo.1417571,"Ruth Dhanaraj, Hewlett Packard Labs;Beth Logan, Hewlett Packard Labs","We explore the automatic analysis of music to identify likely hit songs. We extract both acoustic and lyric information from each song and separate hits from non-hits using standard classifiers, specifically Support Vector Machines and boosting classifiers. Our features are based on global sounds learnt in an unsupervised fashion from acoustic data or global topics learnt from a lyrics database. Experiments on a corpus of 1700 songs demonstrate performance that is much better than random. The lyric-based features are slightly more useful than the acoustic features in correctly identifying hit songs. Concatenating the two features does not produce significant improvements. Analysis of the lyric-based features shows that the absence of certain semantic information indicates that a song is more likely to be a hit."
72,Simon Dixon;Gerhard Widmer,MATCH: A Music Alignment Tool Chest.,2005,https://doi.org/10.5281/zenodo.1416952,"Simon Dixon, Austrian Research Institute for Artiﬁcial Intelligence;Gerhard Widmer, Department of Computational Perception, Johannes Kepler University Linz","We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasizes note onsets in the alignment process. In tests with Classical and Romantic piano music, the average alignment error was 41ms (median 20ms), with only 2 out of 683 test cases failing to align. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed."
73,Peter Jan O. Doets;Reginald L. Lagendijk,Extracting Quality Parameters for Compressed Audio from Fingerprints.,2005,https://doi.org/10.5281/zenodo.1416080,"P.J.O. Doets, Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology;R.L. Lagendijk, Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology","An audio fingerprint is a compact yet very robust representation of the perceptually relevant parts of audio content. It can be used to identify audio, even when severely distorted. Audio compression causes small changes in the fingerprint. We aim to exploit these small fingerprint differences due to compression to assess the perceptual quality of the compressed audio file. Analysis shows that for uncorrelated signals the Bit Error Rate (BER) is approximately inversely proportional to the square root of the Signal-to-Noise Ratio (SNR) of the signal. Experiments using real music confirm this relation. Further experiments show how the various local spectral characteristics cause a large variation in the behavior of the fingerprint difference as a function of SNR or the bitrate set for compression."
74,Douglas Eck;Norman Casagrande,Finding Meter in Music Using An Autocorrelation Phase Matrix and Shannon Entropy.,2005,https://doi.org/10.5281/zenodo.1415650,"Douglas Eck, University of Montreal;Norman Casagrande, University of Montreal","This paper introduces a novel way to detect metrical structure in music. We introduce a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting autocorrelation phase matrix is useful for several tasks involving metrical structure. First we can use the matrix to enhance standard autocorrelation by calculating the Shannon entropy at each lag. This approach yields improved results for autocorrelation-based tempo induction. Second, we can efficiently search the matrix for combinations of lags that suggest particular metrical hierarchies. This approach yields a good model for predicting the meter of a piece of music. Finally, we can use the phase information in the matrix to align a candidate meter with music, making it possible to perform beat induction with an autocorrelation-based model. We present results for several meter prediction and tempo induction datasets, demonstrating that the approach is competitive with models designed specifically for these tasks. We also present preliminary beat induction results on a small set of artificial patterns."
75,Rebecca Fiebrink;Cory McKay;Ichiro Fujinaga,Combining D2K and JGAP for Efficient Feature Weighting for Classification Tasks in Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1415754,"Rebecca Fiebrink, Music Technology, McGill University, Montreal, Canada;Cory McKay, Music Technology, McGill University, Montreal, Canada;Ichiro Fujinaga, Music Technology, McGill University, Montreal, Canada","Music classification is an important aspect of music information retrieval research. Feature weighting is a tool that can improve the performance of classifiers, but it is often underutilized due to the time it takes to achieve optimal results. Genetic algorithms offer a solution at a reduced calculation time, but they can still be costly. This paper investigates the advantages of implementing genetic algorithms in a parallel computing environment to make feature weighting more affordable for researchers in music information retrieval."
76,David Gerhard,Pitch Track Target Deviation in Natural Singing.,2005,https://doi.org/10.5281/zenodo.1418115,"David Gerhard, Department of Computer Science, Department of Music, University of Regina","Unlike ﬁxed-pitch instruments such as the piano, human singing can stray from a target pitch by as much as a semitone while still being perceived as a single ﬁxed note. This paper presents a study of the difference between tar- get pitch and actualized pitch in natural singing. A set of 50 subjects singing the same melody and lyric is used to compare utterance styles. An algorithm for alignment of idealized template pitch tracks to measured frequency tracks is presented. Speciﬁc examples are discussed, and generalizations are made with respect to the types of devi- ations typical in human singing. Demographics, including the skill of the singer, are presented and discussed in the context of the pitch track deviation from the ideal."
77,Rob van Gulik;Fabio Vignoli,Visual Playlist Generation on the Artist Map.,2005,https://doi.org/10.5281/zenodo.1415206,"Rob van Gulik, Institute of Information and Computing Sciences, Utrecht University;Fabio Vignoli, Philips Research Laboratories","This paper describes a visual playlist creation method based on a previously designed visualization technique for large music collections. The method gives users high-level control over the contents of a playlist as well as the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly portable. Future work includes an extensive user evaluation to compare the described method with alternative techniques and to measure its qualities, such as the perceived ease of use and perceived usefulness."
78,Peyman Heydarian;Joshua D. Reiss,The Persian Music and the Santur Instrument.,2005,https://doi.org/10.5281/zenodo.1415048,"Peyman Heydarian, Centre For Digital Music, Queen Mary, University of London;Joshua D. Reiss, Centre For Digital Music, Queen Mary, University of London","Persian music has had a profound effect on various Eastern musical cultures, and also influenced Southern European and Northern African music. The Santur, a hammered dulcimer, is one of the most important instruments in Persia. In this paper, Persian music and the Santur instrument are explained and analyzed. Techniques for fundamental frequency detection are applied to data acquired from the Santur and results are reported."
79,Helge Homburg;Ingo Mierswa;Bülent Möller;Katharina Morik;Michael Wurst,A Benchmark Dataset for Audio Classification and Clustering.,2005,https://doi.org/10.5281/zenodo.1417065,"Helge Homburg, University of Dortmund, AI Unit;Ingo Mierswa, University of Dortmund, AI Unit;B¨ulent M¨oller, University of Dortmund, AI Unit;Katharina Morik, University of Dortmund, AI Unit;Michael Wurst, University of Dortmund, AI Unit","We present a freely available benchmark dataset for audio classification and clustering. This dataset consists of 10 seconds samples of 1886 songs obtained from the Garageband site. Beside the audio clips themselves, textual metadata is provided for the individual songs. The songs are classified into 9 genres. In addition to the genre information, our dataset also consists of 24 hierarchical cluster models created manually by a group of users. This enables a user-centric evaluation of audio classification and clustering algorithms and gives researchers the opportunity to test the performance of their methods on heterogeneous data. We first give a motivation for assembling our benchmark dataset. Then we describe the dataset and its elements in more detail. Finally, we present some initial results using a set of audio features generated by a feature construction approach."
80,Toru Hosoya;Motoyuki Suzuki;Akinori Ito;Shozo Makino,Lyrics Recognition from a Singing Voice Based on Finite State Automaton for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1417855,"Toru Hosoya, Motoyuki Suzuki, Akinori Ito and Shozo Makino, Graduate School of Engineering, Tohoku University","Recently, several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user’s singing voice. All of these systems use only the melody information for retrieval. Although the lyrics information is useful for retrieval, there have been few attempts to exploit lyrics in the user’s input. In order to develop a MIR system that uses lyrics and melody information, lyrics recognition is needed. Lyrics recognition from a singing voice is achieved by similar technology to that of speech recognition. The difference between lyrics recognition and general speech recognition is that the input lyrics are a part of the lyrics of songs in a database. To exploit linguistic constraints maximally, we described the recognition grammar using a finite state automaton (FSA) that accepts only lyrics in the database. In addition, we carried out a “singing voice adaptation” using a speaker adaptation technique. In our experimental results, about 86% retrieval accuracy was obtained."
81,Xiao Hu 0001;J. Stephen Downie;Kris West;Andreas F. Ehmann,Mining Music Reviews: Promising Preliminary Results.,2005,https://doi.org/10.5281/zenodo.1417067,"Xiao Hu, GSLIS, University of Illinois at Urbana-Champaign;J. Stephen Downie, GSLIS, University of Illinois at Urbana-Champaign;Kris West, School of Computing Sciences, University of East Anglia;Andreas Ehmann, Electrical Engineering, University of Illinois at Urbana-Champaign","In this paper we present a system for the automatic mining of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genre of the music reviewed and to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically mine arbitrary bodies of text, such as weblogs (blogs) for musically relevant information."
82,Özgür Izmirli,Tonal Similarity from Audio Using a Template Based Attractor Model.,2005,https://doi.org/10.5281/zenodo.1416688,"Özgür İzmirli, Center for Arts and Technology Connecticut College","A model that calculates similarity of tonal evolution among pieces in an audio database is presented. The model employs a template based key finding algorithm. This algorithm is used in a sliding window fashion to obtain a sequence of tonal center estimates that delineate the trajectory of tonal evolution in tonal space. A chroma based representation is used to capture tonality information. Templates are formed from instrument sounds weighted according to pitch distribution profiles. For each window in the input audio, the chroma based representation is interpreted with respect to the precalculated templates that serve as attractor points in tonal space. This leads to a discretization in both time and tonal space making the output representation compact. Local and global variations in tempo are accounted for using dynamic time warping that employs a special type of music theoretical distance measure. Evaluation is given in two stages. The first is evaluation of the key finding model to assess its performance in key finding for raw audio input. The second is based on cross validation testing for pieces that have multiple performances in the database to determine the success of recall by distance."
83,Jyh-Shing Roger Jang;Chao-Ling Hsu;Hong-Ru Lee,Continuous HMM and Its Enhancement for Singing/Humming Query Retrieval.,2005,https://doi.org/10.5281/zenodo.1414842,"Jyh-Shing Roger Jang, Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University;Chao-Ling Hsu, Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University;Hong-Ru Lee, Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University","The use of HMM (Hidden Markov Models) for speech recognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by singing/humming, has seldom been reported, partly due to the difference in acoustic characteristics between speech and singing/humming inputs. This paper will derive the formula of CHMM training for frame-based MRAI. In particular, we shall propose enhancement to CHMM and demonstrate that with the enhancement scheme, CHMM can compare favourably with DTW in both efficiency and effectiveness."
84,Phillip B. Kirlin;Paul E. Utgoff,VOISE: Learning to Segregate Voices in Explicit and Implicit Polyphony.,2005,https://doi.org/10.5281/zenodo.1417225,"Phillip B. Kirlin and Paul E. Utgoff, Department of Computer Science, University of Massachusetts Amherst","Finding multiple occurrences of themes and patterns in music can be hampered due to polyphonic textures. This is caused by the complexity of music that weaves multiple independent lines of music together. We present and demonstrate a system, VoiSe, that is capable of isolating individual voices in both explicit and implicit polyphonic music. VoiSe is designed to work on a symbolic representation of a music score, and consists of two components: a same-voice predicate implemented as a learned decision tree, and a hard-coded voice numbering algorithm."
85,Tetsuro Kitahara;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,"Instrument Identification in Polyphonic Music: Feature Weighting with Mixed Sounds, Pitch-Dependent Timbre Modeling, and Use of Musical Context.",2005,https://doi.org/10.5281/zenodo.1415724,"Tetsuro Kitahara, Dept. of Intelligence Science and Technology;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Kazunori Komatani, Dept. of Intelligence Science and Technology;Tetsuya Ogata, Dept. of Intelligence Science and Technology;Hiroshi G. Okuno, Dept. of Intelligence Science and Technology","This paper addresses the problem of identifying musical instruments in polyphonic music. Only a few studies have dealt with this issue, and there are three main challenges: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. To address these challenges, the authors propose a method that involves extracting feature vectors from both isolated sounds and sound mixtures, weighting features based on their robustness, using an F0-dependent multivariate normal distribution to model pitch dependency, and calculating the a priori probability of each note based on the a posteriori probabilities of temporally neighboring notes. Experimental results show improved recognition rates for trio and duo music."
86,Peter Knees;Markus Schedl;Gerhard Widmer,Multiple Lyrics Alignment: Automatic Retrieval of Song Lyrics.,2005,https://doi.org/10.5281/zenodo.1415140,"Peter Knees, Department of Computational Perception, Johannes Kepler University Linz;Markus Schedl, Department of Computational Perception, Johannes Kepler University Linz;Gerhard Widmer, Department of Computational Perception, Johannes Kepler University Linz","We present an approach to automatically retrieve and extract lyrics of arbitrary songs from the Internet. It is intended to provide easy and convenient access to lyrics for users, as well as a basis for further research based on lyrics, e.g. semantic analysis. Due to the fact that many lyrics found on the web suffer from individual errors like typos, we make use of multiple versions from different sources to eliminate mistakes. This is accomplished by Multiple Sequence Alignment. The different sites are aligned and examined for matching sequences of words, finding those parts on the pages that are likely to contain the lyrics. This provides a means to find the most probable version of lyrics, i.e. a version with highest consensus among different sources."
87,Catherine Lai;Beinan Li;Ichiro Fujinaga,Preservation Digitization of David Edelberg's Handel LP Collection: A Pilot Project.,2005,https://doi.org/10.5281/zenodo.1416616,"Catherine Lai, Music Technology, Faculty of Music, McGill University;Beinan Li, Music Technology, Faculty of Music, McGill University;Ichiro Fujinaga, Music Technology, Faculty of Music, McGill University",This paper describes the digitization process for building an online collection of LPs and the procedure for creating the ground-truth data essential for developing an automated metadata and content capturing system.
88,Aristomenis S. Lampropoulos;Paraskevi S. Lampropoulou;George A. Tsihrintzis,Musical Genre Classification Enhanced by Improved Source Separation Technique.,2005,https://doi.org/10.5281/zenodo.1416292,"Aristomenis S. Lampropoulos, University of Piraeus;Paraskevi S. Lampropoulou, University of Piraeus;George A. Tsihrintzis, University of Piraeus","We present a system for musical genre classification based on audio features extracted from signals which correspond to distinct musical instrument sources. For the separation of the musical sources, we propose an innovative technique in which the convolutive sparse coding algorithm is applied to several portions of the audio signal. The system is evaluated and its performance is assessed."
89,Wei Liang;Shuwu Zhang;Bo Xu 0002,A Hierarchical Approach for Audio Stream Segmentation and Classification.,2005,https://doi.org/10.5281/zenodo.1415166,"Wei Liang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Shuwu Zhang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Bo Xu, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","This paper describes a hierarchical approach for fast audio stream segmentation and classification. With this approach, the audio stream is firstly segmented into audio clips by MBCR (Multiple sub-Bands spectrum Centroid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips into six pre-defined categories in terms of discriminative background sounds, which is pure speech, pure music, song, speech with music, speech with noise and silence. The experiments on real TV program recordings showed that this approach has higher accuracy and recall rate for audio classification with a fast speed under noise environments."
90,Wei Liang;Shuwu Zhang;Bo Xu 0002,A Histogram Algorithm for Fast Audio Retrieval.,2005,https://doi.org/10.5281/zenodo.1415812,"Wei Liang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Shuwu Zhang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Bo Xu, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China",This paper describes a fast audio detection method for specific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio features based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detection in TV program showed that the algorithm can achieve a very high overall precision and recall rate both about 97% with very fast search time about 1/40 on real time.
91,Dominik Lübbers,SoniXplorer: Combining Visualization and Auralization for Content-Based Exploration of Music Collections.,2005,https://doi.org/10.5281/zenodo.1418021,"Dominik L¨ubbers, RWTH Aachen University","Music can be described best by music. However, current research in the design of user interfaces for the exploration of music collections has mainly focused on visualization aspects ignoring possible benefits from spatialized music playback. We describe our first development steps towards two novel user-interface designs: The Sonic Radar arranges a fixed number of prototypes resulting from a content-based clustering process in a circle around the user's standpoint. To derive an auralization of the scene, we introduce the concept of an aural focus of perception that adapts well-known principles from the visual domain. The Sonic SOM is based on Kohonen's Self-Organizing Map. It helps the user in understanding the structure of his music collection by positioning titles on a two-dimensional grid according to their high-dimensional similarity. We show how our auralization concept can be adapted to extend this visualization technique and thereby support multimodal navigation."
92,Michael I. Mandel;Dan Ellis,Song-Level Features and Support Vector Machines for Music Classification.,2005,https://doi.org/10.5281/zenodo.1415024,"Michael I. Mandel and Daniel P.W. Ellis, LabROSA, Dept. of Elec. Eng., Columbia University, NY NY USA","Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplar-based classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums."
93,Daniel McEnnis;Cory McKay;Ichiro Fujinaga;Philippe Depalle,jAudio: An Feature Extraction Library.,2005,https://doi.org/10.5281/zenodo.1416648,"Daniel McEnnis, Faculty of Music, McGill University;Cory McKay, Faculty of Music, McGill University;Ichiro Fujinaga, Faculty of Music, McGill University;Philippe Depalle, Faculty of Music, McGill University","jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straightforward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio files as input. In the GUI, users select the features that they wish to have extracted—letting jAudio take care of all dependency problems—and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML file or an ARFF file depending on the user’s preference."
94,Anders Meng;John Shawe-Taylor,An Investigation of Feature Models for Music Genre Classification Using the Support Vector Classifier.,2005,https://doi.org/10.5281/zenodo.1416052,"Anders Meng, Informatics and Mathematical Modelling - B321, Technical University of Denmark;John Shawe-Taylor, University of Southampton","In music genre classification, most automatic systems focus on short time features derived from 10-50ms, even though the decision time for genre classification is typically several seconds. This study investigates two models, the multivariate Gaussian model and the multivariate autoregressive model, for modeling short time features. The study also explores how these models can be integrated over a segment of short time features into a kernel for application in a support vector machine. Two kernels with this property, the convolution kernel and product probability kernel, are considered. The accuracy of the best performing model on an 11 genre music setup was approximately 44%, compared to a human performance of approximately 52% on the same dataset."
95,Annamaria Mesaros;Jaakko Astola,The Mel-Frequency Cepstral Coefficients in the Context of Singer Identification.,2005,https://doi.org/10.5281/zenodo.1417539,"Annamaria Mesaros, Technical University of Cluj Napoca;Jaakko Astola, Institute of Signal Processing, Tampere University of Technology","The singing voice is the oldest and most complex musical instrument. A familiar singer’s voice is easily recognizable for humans, even when hearing a song for the first time. On the other hand, for automatic identification this is a difficult task among sound source identification applications. The signal processing techniques aim to extract features that are related to identity characteristics. The research presented in this paper considers 32 Mel-Frequency Cepstral Coefficients in two subsets: the low order MFCCs characterizing the vocal tract resonances and the high order MFCCs related to the glottal wave shape. We explore possibilities to identify and discriminate singers using the two sets. Based on the results we can affirm that both subsets have their contribution in defining the identity of the voice, but the high order subset is more robust to changes in singing style."
96,J. Enrique Muñoz Expósito;Sebastian García Galán;Nicolás Ruiz-Reyes;Pedro Vera-Candeas;F. Rivas-Peña,Speech/Music Discrimination Using a Single Warped LPC-Based Feature.,2005,https://doi.org/10.5281/zenodo.1417711,"J.E. Mu˜noz-Exp´osito, Electronics and Telecommunication Engineering Department, University of Ja´en;S. Garcia-Gal´an, Electronics and Telecommunication Engineering Department, University of Ja´en;N. Ruiz-Reyes, Electronics and Telecommunication Engineering Department, University of Ja´en;P. Vera-Candeas, Electronics and Telecommunication Engineering Department, University of Ja´en;F. Rivas-Pe˜na, Electronics and Telecommunication Engineering Department, University of Ja´en","Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in Tzanetakis and Cook (2002) is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for real-time multimedia applications."
97,Robert Neumayer;Michael Dittenbach;Andreas Rauber,"PlaySOM and PocketSOMPlayer, Alternative Interfaces to Large Music Collections.",2005,https://doi.org/10.5281/zenodo.1414818,"Robert Neumayer, Vienna University of Technology;Michael Dittenbach, eCommerce Competence Center;Andreas Rauber, Vienna University of Technology","With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. In this paper we present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information."
98,Giovanna Neve;Nicola Orio,Experiments on Segmentation Techniques for Music Documents Indexing.,2005,https://doi.org/10.5281/zenodo.1416996,"Nicola Orio, Department of Information Engineering;Giovanna Neve, Department of Information Engineering","This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units, which can be used as content descriptors of music documents. Four approaches have been implemented and compared on a test collection of real documents and queries, showing their impact on index term size and on retrieval effectiveness. From the results, simple but extensive approaches seem to give better performances than more sophisticated segmentation algorithms."
99,Elias Pampalk;Arthur Flexer;Gerhard Widmer,Improvements of Audio-Based Music Similarity and Genre Classificaton.,2005,https://doi.org/10.5281/zenodo.1418083,"Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Arthur Flexer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Institute of Medical Cybernetics and Artiﬁcial Intelligence, Center for Brain Research, Medical University of Vienna;Gerhard Widmer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Department of Computational Perception, Johannes Kepler University, Linz, Austria","Audio-based music similarity measures can be applied to automatically generate playlists or recommendations. In this paper, spectral similarity is combined with complementary information from fluctuation patterns, including two new descriptors derived thereof. The performance is evaluated in a series of experiments on four music collections, based on genre classification. The main findings are that improvements are substantial on two of the four collections, but simple audio statistics have limitations. Evaluating similarity through genre classification is biased by the music collection used, and in cross-validation, no pieces from the same artist should be in both the training and test set."
100,Elias Pampalk;Tim Pohle;Gerhard Widmer,Dynamic Playlist Generation Based on Skipping Behavior.,2005,https://doi.org/10.5281/zenodo.1414932,"Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Tim Pohle, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Gerhard Widmer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Department of Computational Perception, Johannes Kepler University","Common approaches to creating playlists are to randomly shuffle a collection (e.g. iPod shuffle) or manually select songs. In this paper, we present and evaluate heuristics to adapt playlists automatically given a song to start with (seed song) and immediate user feedback. Instead of rich metadata, we use audio-based similarity. The user gives feedback by pressing a skip button if the user dislikes the current song. Songs similar to skipped songs are removed, while songs similar to accepted ones are added to the playlist. We evaluate the heuristics with hypothetical use cases. For each use case, we assume a specific user behavior (e.g. the user always skips songs by a particular artist). Our results show that using audio similarity and simple heuristics, it is possible to drastically reduce the number of necessary skips."
101,Steffen Pauws;Sander van de Wijdeven,User Evaluation of a New Interactive Playlist Generation Concept.,2005,https://doi.org/10.5281/zenodo.1415180,"Steffen Pauws, Philips Research;Sander van de Wijdeven, Philips Research","Selecting the ‘right’ songs and putting them in the ‘right’ order are key to a great music listening or dance experience. ‘SatisFly’ is an interactive playlist generation system in which the user can tell what kind of songs should be contained in what order in the playlist, while she navigates through the music collection. The system uses constraint satisfaction to generate a playlist that meets all user wishes. In a user evaluation, it was found that users created high-quality playlists in a swift way and with little effort using the system, while still having complete control on their music choices. The novel interactive way of creating a playlist, while browsing through the music collection, was highly appreciated. Ease of navigation through a music collection is still an issue that needs further attention."
102,Geoffroy Peeters,Rhythm Classification Using Spectral Rhythm Patterns.,2005,https://doi.org/10.5281/zenodo.1417495,"Geoffroy Peeters, IRCAM - Sound Analysis/Synthesis Team","In this paper, we study the use of spectral patterns to represent the characteristics of the rhythm of an audio signal. Three feature sets derived from the amplitude of the Discrete Fourier Transform, the Auto-Correlation Function, and the product of the DFT and a Frequency-Mapped ACF are evaluated for their ability to represent the rhythm characteristics of an audio item through a classification task. We show that using such simple spectral representations allows obtaining results comparable to the state of the art."
103,Jeremy Pickens,Classifier Combination for Capturing Musical Variation.,2005,https://doi.org/10.5281/zenodo.1418219,"Jeremy Pickens, Department of Computer Science, King’s College London","At its heart, music information retrieval is characterized by the need to find the similarity between pieces of music. However, “similar” does not mean “the same”. Therefore, techniques for approximate matching are crucial to the development of good music information retrieval systems. Yet as one increases the level of approximation, one finds not only additional similar, relevant music, but also a larger number of not-as-similar, non-relevant music. The purpose of this work is to show that if two different retrieval systems do approximate matching in different manners, and both give decent results, they can be combined to give results better than either system individually. One need not sacrifice accuracy for the sake of flexibility."
104,Aggelos Pikrakis;Sergios Theodoridis,A Novel HMM Approach to Melody Spotting in Raw Audio Recordings.,2005,https://doi.org/10.5281/zenodo.1414886,"Aggelos Pikrakis, University of Athens;Sergios Theodoridis, University of Athens","This paper presents a melody spotting system based on Variable Duration Hidden Markov Models (VDHMM’s), capable of locating monophonic melodies in a database of raw audio recordings. The system treats the melody as a sequence of note durations and music intervals and constructs a VDHMM based on this pattern prototype. The probabilities of the VDHMM are determined according to rules that account for note duration flexibility and structural deviations from the prototype pattern. The system has been successfully tested with cello recordings in Western Classical music and Greek traditional multi-instrument recordings."
105,Christopher Raphael,A Graphical Model for Recognizing Sung Melodies.,2005,https://doi.org/10.5281/zenodo.1417052,"Christopher Raphael, School of Informatics, Indiana Univ.","A method is presented for automatic transcription of sung melodic fragments to score-like representation, including metric values and pitch. A joint model for pitch, rhythm, segmentation, and tempo is defined for a sung fragment. We then discuss the identification of the globally optimal musical transcription, given the observed audio data. A post process estimates the location of the tonic, so the transcription can be presented into the key of C. Experimental results are presented for a small test collection."
106,Craig Stuart Sapp,Online Database of Scores in the Humdrum File Format.,2005,https://doi.org/10.5281/zenodo.1417281,"Craig Stuart Sapp, Stanford University","KernScores, an online library of musical data currently consisting of over 5 million notes, has been created to assist projects dealing with the computational analysis of musical scores. The online scores are in a format suitable for processing with the Humdrum Toolkit for Music Research, but the website also provides automatic translations into several other popular data formats for digital musical scores."
107,Nicolas Scaringella;Giorgio Zoia,On the Modeling of Time Information for Automatic Genre Recognition Systems in Audio Signals.,2005,https://doi.org/10.5281/zenodo.1416064,"Nicolas Scaringella, Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne, EPFL, Lausanne, CH-1015 Switzerland;Giorgio Zoia, Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne, EPFL, Lausanne, CH-1015 Switzerland","The creation of huge databases coming from both restoration of existing analogue archives and new content is demanding fast and more and more reliable tools for content analysis and description, to be used for searches, content queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music catalogues, libraries and shops. Despite their use musical genres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the same pattern recognition architecture: extracting features from chunks of audio signal and classifying features independently. In this paper, we focus instead on the low-level temporal relationships between chunks when classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music segments to consolidate classification consistency by reducing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classification results are reported for a database of 1400 songs evenly distributed over 7 genres."
108,Elliot Sinyor;Cory McKay;Rebecca Fiebrink;Daniel McEnnis;Ichiro Fujinaga,Beatbox Classification Using ACE.,2005,https://doi.org/10.5281/zenodo.1414920,"Elliot Sinyor, McGill University;Cory McKay, McGill University;Rebecca Fiebrink, McGill University;Daniel McEnnis, McGill University;Ichiro Fujinaga, McGill University","This paper describes the use of the Autonomous Classification Engine (ACE) to classify beatboxing (vocal percussion) sounds. A set of unvoiced percussion sounds belonging to five classes (bass drum, open hihat, closed hihat and two types of snare drum) were recorded and manually segmented. ACE was used to compare various classification techniques, both with and without feature selection. The best result was 95.55% accuracy using AdaBoost with C4.5 decision trees."
109,Robert Young Walser,Herding Folksongs.,2005,https://doi.org/10.5281/zenodo.1415640,"Robert Young Walser, The James Madison Carpenter Project, Elphinstone Institute, University of Aberdeen","Cataloging a large, multi-media collection of traditional song and drama in preparation for online presentation highlights issues of song identity and access in the context of contemporary digitized archives. In the James Madison Carpenter collection a particular folksong sung by a particular individual may exist in multiple manifestations: typed song text, sound recording(s), and/or manuscript music notation. While controlled vocabulary references such as Child and Roud numbers provide a degree of identification, such narrative- and text-centric tools are only partly effective in differentiating folkloric materials. Additional means are needed for identifying and controlling folk materials which are distinguished by other aspects of the song such as melody or non-narrative text. The Carpenter project team’s experience with Encoded Archival Description (EAD) illustrates the value of this platform-independent, widely recognized standard and suggests opportunities for further developments particularly suited to locating and retrieving folk music materials."
110,Kris West;Stephen Cox,Finding An Optimal Segmentation for Audio Genre Classification.,2005,https://doi.org/10.5281/zenodo.1416746,"Kris West, School of Computing Sciences, University of East Anglia;Stephen Cox, School of Computing Sciences, University of East Anglia","In the automatic classification of music, different segmentations of the audio signal have been used to calculate features. This study evaluates these different segmentations and introduces the use of an onset detection based segmentation, which outperforms other segmentation schemes in terms of classification accuracy and model size."
111,Tillman Weyde;Christian Datzko,Efficient Melody Retrieval with Motif Contour Classes.,2005,https://doi.org/10.5281/zenodo.1414738,"Tillman Weyde, City University;Christian Datzko, University of Osnabrück","This paper describes the use of motif contour classes for efficient retrieval of melodies from music collections. Instead of extracting incipits or themes, complete monophonic pieces are indexed for their motifs, using classes of motif contours. Similarity relations between these classes can be used for a very efficient search. This can serve as a first level search, which can be refined by using more computationally intensive comparisons on its results. The model introduced has been implemented and tested using the MUSITECH framework. We present empirical and analytical results on the retrieval quality, the complexity, and quality/efficiency trade-off."
112,Wen Xue;Mark Sandler,A Partial Searching Algorithm and Its Application for Polyphonic Music Transcription.,2005,https://doi.org/10.5281/zenodo.1415740,"Xue Wen, Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London;Mark Sandler, Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London","This paper proposes an algorithm for studying spectral contents of pitched sounds in real-world recordings. We assume that the 2nd-order difference, w.r.t. partial index, of a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. Given a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be associated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the algorithm to make it work with reverberant background, such as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level."
113,Yi Yu 0001;Chiemi Watanabe;Kazuki Joe,Towards a Fast and Efficient Match Algorithm for Content-Based Music Retrieval on Acoustic Data.,2005,https://doi.org/10.5281/zenodo.1415874,"Yi YU, Graduate school of Humanity and Science, Nara Women’s University;Chiemi WATANABE, Graduate school of Humanity and Science, Nara Women’s University;Kazuki JOE, Graduate school of Humanity and Science, Nara Women’s University","In this paper we present a fast and efficient match algorithm, which consists of two key techniques: Spectral Correlation Based Feature Merge(SCBFM) and Two-Step Retrieval(TSR). SCBFM can remove the redundant information. In consequence, the resulting feature sequence has a smaller size, requiring less storage and computation. In addition, most of the tempo variation is removed; thus a much simpler sequence match method can be adopted. Also, TSR relies on the characteristics of Mel-Frequency Cepstral Coefficient(MFCC), where the precise match in the second step depends on the first step to filter out most of the dissimilar references with only the low order MFCC feature. As a result, the whole retrieval speed can be further improved. The experimental evaluation verifies that SCBFM-TSR yields more meaningful results in comparatively short time. The experiment results are analyzed with a theoretical approach that seeks to find the relation between Spectral Correlation(SC) threshold and storage, computation."
