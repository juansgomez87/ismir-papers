Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Roman B. Gebhardt;Michael Stein;Athanasios Lykartsis,A Confidence Measure For Key Labelling,2018,https://doi.org/10.5281/zenodo.1492333,Roman B. Gebhardt+TU Berlin>DEU>education|Native Instruments GmbH>DEU>company;Athanasios Lykartsis+TU Berlin>DEU>education;Michael Stein+Native Instruments GmbH>DEU>company,"We present a new measure for automatically estimating the confidence of musical key classification. Our approach leverages the degree of harmonic information held within a musical audio signal (its ""keyness"") as well as the steadiness of local key detections across the its duration (its ""stability""). Using this confidence measure, musical tracks which are likely to be misclassified, i.e. those with low confidence, can then be handled differently from those analysed by standard, fully automatic key detection methods. By means of a listening test, we demonstrate that our developed features significantly correlate with listeners' ratings of harmonic complexity, steadiness and the uniqueness of key. Furthermore, we demonstrate that tracks which are incorrectly labelled using an existing key detection system obtain low confidence values. Finally, we introduce a new method called ""root note heuristics"" for the special treatment of tracks with low confidence. We show that by applying these root note heuristics, key detection results can be improved for minimalistic music."
1,Filip Korzeniowski;Gerhard Widmer,Improved Chord Recognition by Combining Duration and Harmonic Language Models,2018,https://doi.org/10.5281/zenodo.1492335,Filip Korzeniowski+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility,"Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model—to be applied on chord sequences—and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results."
2,Tristan Carsault;Jerome Nika;Philippe Esling,Using Musical Relationships Between Chord Labels in Automatic Chord Extraction Tasks,2018,https://doi.org/10.5281/zenodo.1492329,"Tristan Carsault+Ircam>FRA>facility|CNRS>FRA>facility|Sorbonne Université>FRA>education;Jérôme Nika+Ircam>FRA>facility|CNRS>FRA>facility|Sorbonne Université>FRA>education|L3i Lab, University of La Rochelle>FRA>education;Philippe Esling+Ircam>FRA>facility|CNRS>FRA>facility|Sorbonne Université>FRA>education","Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors."
3,Stefan Lattner;Maarten Grachten;Gerhard Widmer,A Predictive Model for Music based on Learned Interval Representations,2018,https://doi.org/10.5281/zenodo.1492331,"Stefan Lattner+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Maarten Grachten+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Gerhard Widmer+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company","Connectionist sequence models (e.g., RNNs) applied to musical sequences suffer from two known problems: First, they have strictly ""absolute pitch perception"". Therefore, they fail to generalize over musical concepts which are commonly perceived in terms of relative distances between pitches (e.g., melodies, scale types, modes, cadences, or chord types). Second, they fall short of capturing the concepts of repetition and musical form. In this paper we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network which learns and operates on interval representations of musical sequences. The relative pitch modeling increases generalization and reduces sparsity in the input data. Furthermore, it can learn sequences of copy-and-shift operations (i.e. chromatically transposed copies of musical fragments)—a promising capability for learning musical repetition structure. We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably. Furthermore, we show that the relative pitch processing of the RGAE naturally facilitates the learning and the generation of sequences of copy-and-shift operations, wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural network on this task."
4,Miguel A. Román;Antonio Pertusa;Jorge Calvo-Zaragoza,An End-to-end Framework for Audio-to-Score Music Transcription on Monophonic Excerpts,2018,https://doi.org/10.5281/zenodo.1492337,Miguel A. Román+University of Alicante>ESP>education;Antonio Pertusa+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+Universitat Politècnica de València>ESP>education,"In this work, we present an end-to-end framework for audio-to-score transcription. To the best of our knowledge, this is the first automatic music transcription approach which obtains directly a symbolic score from audio, instead of performing separate stages for piano-roll estimation (pitch detection and note tracking), meter detection or key estimation. The proposed method is based on a Convolutional Recurrent Neural Network architecture directly trained with pairs of spectrograms and their corresponding symbolic scores in Western notation. Unlike standard pitch estimation methods, the proposed architecture does not need the music symbols to be aligned with their audio frames thanks to a Connectionist Temporal Classification loss function. Training and evaluation were performed using a large dataset of short monophonic scores (incipits) from the RISM collection, that were synthesized to get the ground-truth data. Although there is still room for improvement, most musical symbols were correctly detected and the evaluation results validate the proposed approach. We believe that this end-to-end framework opens new avenues for automatic music transcription."
5,Andrew McLeod;Mark Steedman,Evaluating Automatic Polyphonic Music Transcription,2018,https://doi.org/10.5281/zenodo.1492339,Andrew McLeod+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education;Mark Steedman+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education,"Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a timefrequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce M V 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation— for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H."
6,Curtis Hawthorne;Erich Elsen;Jialin Song;Adam Roberts;Ian Simon;Colin Raffel;Jesse Engel;Sageev Oore;Douglas Eck,Onsets and Frames: Dual-Objective Piano Transcription,2018,https://doi.org/10.5281/zenodo.1492341,Curtis Hawthorne+Google Brain>USA>company;Erich Elsen+Google Brain>USA>company;Jialin Song+Google Brain>USA>company;Adam Roberts+Google Brain>USA>company;Ian Simon+Google Brain>USA>company;Colin Raffel+Google Brain>USA>company;Jesse Engel+Google Brain>USA>company;Sageev Oore+Google Brain>USA>company;Douglas Eck+Google Brain>USA>company,"We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions."
7,Carl Southall;Ryan Stables;Jason Hockman,Player Vs Transcriber: A Game Approach To Data Manipulation For Automatic Drum Transcription,2018,https://doi.org/10.5281/zenodo.1492343,Carl Southall+Birmingham City University>GBR>education;Ryan Stables+Birmingham City University>GBR>education;Jason Hockman+Birmingham City University>GBR>education,"State-of-the-art automatic drum transcription (ADT) approaches utilise deep learning methods reliant on timeconsuming manual annotations and require congruence between training and testing data. When these conditions are not held, they often fail to generalise. We propose a game approach to ADT, termed player vs transcriber (PvT), in which a player model aims to reduce transcription accuracy of a transcriber model by manipulating training data in two ways. First, existing data may be augmented, allowing the transcriber to be trained using recordings with modified timbres. Second, additional individual recordings from sample libraries are included to generate rare combinations. We present three versions of the PvT model: AugExist, which augments pre-existing recordings; AugAddExist, which adds additional samples of drum hits to the AugExist system; and Generate, which generates training examples exclusively from individual drum hits from sample libraries. The three versions are evaluated alongside a state-of-the-art deep learning ADT system using two evaluation strategies. The results demonstrate that including the player network improves the ADT performance and suggests that this is due to improved generalisability. The results also indicate that although the Generate model achieves relatively low results, it is a viable choice when annotations are not accessible."
8,Nathaniel Condit-Schultz;Yaolong Ju;Ichiro Fujinaga,A Flexible Approach to Automated Harmonic Analysis: Multiple Annotations of Chorales by Bach and Prætorius,2018,https://doi.org/10.5281/zenodo.1492345,Nathaniel Condit-Schultz+McGill University>CAN>education;Yaolong Ju+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Despite being a core component of Western music theory, harmonic analysis remains a subjective endeavor, resistant automation. This subjectivity arises from disagreements regarding, among other things, the interpretation of contrapuntal figures, the set of ""legal"" harmonies, and how harmony relates to more abstract features like tonal function. In this paper, we provide a formal specification of harmonic analysis. We then present a novel approach to computational harmonic analysis: rather than computing harmonic analyses based on one specific set of rules, we compute all possible analyses which satisfy only basic, uncontroversial constraints. These myriad interpretations can later be filtered to extract preferred analyses; for instance, to forbid 7th chords or to prefer analyses with fewer non-chord tones. We apply this approach to two concrete musical datasets: existing encodings of 371 chorales by J.S. Bach and new encodings of 200 chorales by M. Prætorius. Through an online API users can filter and download numerous harmonic interpretations of these 571 chorales. This dataset will serve as a useful resource in the study of harmonic/functional progression, voice-leading, and the relationship between melody and harmony, and as a stepping stone towards automated harmonic analysis of more complex music."
9,Tejaswinee Kelkar;Udit Roy;Alexander Refsum Jensenius,Evaluating a Collection of Sound-Tracing Data of Melodic Phrases,2018,https://doi.org/10.5281/zenodo.1492347,Tejaswinee Kelkar+University of Oslo>NOR>education;Udit Roy+Unknown>Unknown>Unknown;Alexander Refsum Jensenius+University of Oslo>NOR>education,"Melodic contour, the 'shape' of a melody, is a common way to visualize and remember a musical piece. The purpose of this paper is to explore the building blocks of a future 'gesture-based' melody retrieval system. We present a dataset containing 16 melodic phrases from four musical styles and with a large range of contour variability. This is accompanied by full-body motion capture data of 26 participants performing sound-tracing to the melodies. The dataset is analyzed using canonical correlation analysis (CCA), and its neural network variant (Deep CCA), to understand how melodic contours and sound tracings relate to each other. The analyses reveal non-linear relationships between sound and motion. The link between pitch and verticality does not appear strong enough for complex melodies. We also find that descending melodic contours have the least correlation with tracings."
10,Dogac Basaran;Slim Essid;Geoffroy Peeters,Main Melody Estimation with Source-Filter NMF and CRNN,2018,https://doi.org/10.5281/zenodo.1492349,"Dogac Basaran+CNRS, Ircam Lab, Sorbonne Université>FRA>education;Slim Essid+Télécom ParisTech, Université Paris Saclay>FRA>education;Geoffroy Peeters+CNRS, Ircam Lab, Sorbonne Université>FRA>education","Estimating the main melody of a polyphonic audio recording remains a challenging task. We approach the task from a classification perspective and adopt a convolutional recurrent neural network (CRNN) architecture that relies on a particular form of pretraining by source-filter nonnegative matrix factorisation (NMF). The source-filter NMF decomposition is chosen for its ability to capture the pitch and timbre content of the leading voice/instrument, providing a better initial pitch salience than standard timefrequency representations. Starting from such a musically motivated representation, we propose to further enhance the NMF-based salience representations with CNN layers, then to model the temporal structure by an RNN network and to estimate the dominant melody with a final classification layer. The results show that such a system achieves state-of-the-art performance on the MedleyDB dataset without any augmentation methods or large training sets."
11,Tsung-Ping Chen;Li Su,Functional Harmony Recognition of Symbolic Music Data with Multi-task Recurrent Neural Networks,2018,https://doi.org/10.5281/zenodo.1492351,Tsung-Ping Chen+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown;Li Su+Academia Sinica>TWN>education|Institute of Information Science>Unknown>Unknown,"Previous works on chord recognition mainly focus on chord symbols but overlook other essential features that matter in musical harmony. To tackle the functional harmony recognition problem, we compile a new professionally annotated dataset of symbolic music encompassing not only chord symbols, but also various interrelated chord functions such as key modulation, chord inversion, secondary chords, and chord quality. We further present a novel holistic system in functional harmony recognition; a multi-task learning (MTL) architecture is implemented with the recurrent neural network (RNN) to jointly model chord functions in an end-to-end scenario. Experimental results highlight the capability of the proposed recognition system, and a promising improvement of the system by employing multi-task learning instead of single-task learning. This is one attempt to challenge the end-to-end chord recognition task from the perspective of functional harmony so as to uncover the grand structure ruling the flow of musical sound. The dataset and the source code of the proposed system is announced at https://github.com/ Tsung-Ping/functional-harmony."
12,Hendrik Schreiber;Meinard Müller,A Single-step Approach to Musical Tempo Estimation using a Convolutional Neural Network,2018,https://doi.org/10.5281/zenodo.1492353,Hendrik Schreiber+tagtraum industries incorporated>USA>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>education,"We present a single-step musical tempo estimation system based solely on a convolutional neural network (CNN). Contrary to existing systems, which typically first identify onsets or beats and then derive a tempo, our system estimates the tempo directly from a conventional melspectrogram in a single step. This is achieved by framing tempo estimation as a multi-class classification problem using a network architecture that is inspired by conventional approaches. The system's CNN has been trained with the union of three datasets covering a large variety of genres and tempi using problem-specific data augmentation techniques. Two of the three ground-truths are novel and will be released for research purposes. As input the system requires only 11.9 s of audio and is therefore suitable for local as well as global tempo estimation. When used as a global estimator, it performs as well as or better than other state-of-the-art algorithms. Especially the exact estimation of tempo without tempo octave confusion is significantly improved. As local estimator it can be used to identify and visualize tempo drift in musical performances."
13,Magdalena Fuentes;Brian McFee;Hélène C. Crayencour;Slim Essid;Juan Pablo Bello,Analysis of Common Design Choices in Deep Learning Systems for Downbeat Tracking,2018,https://doi.org/10.5281/zenodo.1492355,"Magdalena Fuentes+L2S, CNRS-Univ. Paris-Sud-CentraleSup´elec>FRA>education|LTCI, T´el´ecom ParisTech, Univ. Paris-Saclay>FRA>education;Brian McFee+Music and Audio Research Laboratory, New York University>USA>education|Center of Data Science, New York University>USA>education;Hélène C. Crayencour+L2S, CNRS-Univ. Paris-Sud-CentraleSup´elec>FRA>education;Slim Essid+LTCI, T´el´ecom ParisTech, Univ. Paris-Saclay>FRA>education;Juan P. Bello+Music and Audio Research Laboratory, New York University>USA>education|Center of Data Science, New York University>USA>education","Downbeat tracking consists of annotating a piece of musical audio with the estimated position of the first beat of each bar. In recent years, increasing attention has been paid to applying deep learning models to this task, and various architectures have been proposed, leading to a significant improvement in accuracy. However, there are few insights about the role of the various design choices and the delicate interactions between them. In this paper we offer a systematic investigation of the impact of largely adopted variants. We study the effects of the temporal granularity of the input representation (i.e. beat-level vs tatum-level) and the encoding of the networks outputs. We also investigate the potential of convolutional-recurrent networks, which have not been explored in previous downbeat tracking systems. To this end, we exploit a state-of-the-art recurrent neural network where we introduce those variants, while keeping the training data, network learning parameters and postprocessing stages fixed. We find that temporal granularity has a significant impact on performance, and we analyze its interaction with the encoding of the networks outputs."
14,Andrew McLeod;Mark Steedman,Meter Detection and Alignment of MIDI Performance,2018,https://doi.org/10.5281/zenodo.1492357,Andrew McLeod+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education;Mark Steedman+University of Edinburgh>GBR>education|University of Edinburgh>GBR>education,"Metrical alignment is an integral part of any complete automatic music transcription (AMT) system. In this paper, we present an HMM for both detecting the metrical structure of given live performance MIDI data, and aligning that structure with the underlying notes. The model takes as input only a list of the notes present in a performance, and labels bars, beats, and sub beats in time. We also present an incremental algorithm which can perform inference on the model efficiently using a modified Viterbi search. We propose a new metric designed for the task, and using it, we show that our model achieves state-of-the-art performance on a corpus of metronomically aligned MIDI data, as well as a second corpus of live performance MIDI data. The code for the model described in this paper is available at https://www.github.com/apmcleod/met-align."
15,Dasaem Jeong;Taegyun Kwon;Juhan Nam,A Timbre-based Approach to Estimate Key Velocity from Polyphonic Piano Recordings,2018,https://doi.org/10.5281/zenodo.1492359,Dasaem Jeong+KAIST>KOR>education;Taegyun Kwon+KAIST>KOR>education;Juhan Nam+KAIST>KOR>education,"Estimating the key velocity of each note from polyphonic piano music is a highly challenging task. Previous work addressed the problem by estimating note intensity using a polyphonic note model. However, they are limited because the note intensity is vulnerable to various factors in a recording environment. In this paper, we propose a novel method to estimate the key velocity focusing on timbre change which is another cue associated with the key velocity. To this end, we separate individual notes of polyphonic piano music using non-negative matrix factorization (NMF) and feed them into a neural network that is trained to discriminate the timbre change according to the key velocity. Combining the note intensity from the separated notes with the statistics of the neural network prediction, the proposed method estimates the key velocity in the dimension of MIDI note velocity. The evaluation on Saarland Music Data and the MAPS dataset shows promising results in terms of robustness to changes in the recording environment."
16,Francesco Bigoni;Sofia Dahl,Timbre Discrimination for Brief Instrument Sounds,2018,https://doi.org/10.5281/zenodo.1492361,Francesco Bigoni+Aalborg University>DNK>education;Soﬁa Dahl+Aalborg University>DNK>education,"Timbre discrimination, even for very brief sounds, allows identification and separation of different sound sources. The existing literature on the effect of duration on timbre recognition shows high performance for remarkably short time window lengths, but does not address the possible effect of musical training. In this study, we applied an adaptive procedure to investigate the effect of musical training on individual thresholds for instrument identification. A timbre discrimination task consisting of a 4-alternative forced choice (4AFC) of brief instrument sounds with varying duration was assigned to 16 test subjects using an adaptive staircase method. The effect of musical training has been investigated by dividing the participants into two groups: musicians and non-musicians. The experiment showed lowest thresholds for the guitar sound and highest for the violin sound, with a high overall performance level, but no significant difference between the two groups. It is suggested that the test subjects adjust the weightings of the perceptual dimensions of timbre according to different degrees of acoustic degradation of the stimuli, which are evaluated both by plotting extracted audio features in a feature space and by considering the timbral specificities of the four instruments."
17,Yun-Ning Hung;Yi-Hsuan Yang,Frame-level Instrument Recognition by Timbre and Pitch,2018,https://doi.org/10.5281/zenodo.1492363,"Yun-Ning Hung+Research Center for IT Innovation, Academia Sinica>TWN>facility;Yi-Hsuan Yang+Research Center for IT Innovation, Academia Sinica>TWN>facility","Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at https://biboamy. github.io/instrument-recognition/."
18,Hiroaki Tsushima;Eita Nakamura;Katsutoshi Itoyama;Kazuyoshi Yoshii,Interactive Arrangement of Chords and Melodies Based on a Tree-Structured Generative Model,2018,https://doi.org/10.5281/zenodo.1492365,Hiroaki Tsushima+Kyoto University>JPN>education;Eita Nakamura+Kyoto University>JPN>education;Katsutoshi Itoyama+Kyoto University>JPN>education;Kazuyoshi Yoshii+Kyoto University>JPN>education,"We describe an interactive music composition system that assists a user in refining chords and melodies by generating chords for melodies (harmonization) and vice versa (melodization). Since these two tasks have been dealt with independently, it is difficult to jointly estimate chords and melodies that are optimal in both tasks. Another problem is developing an interactive GUI that enables a user to partially update chords and melodies by considering the latent tree structure of music. To solve these problems, we propose a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chord symbols, (2) a metrical Markov model for chord boundaries, (3) a Markov model for melody pitches, and (4) a metrical Markov model for melody onsets. The harmonic functions (syntactic roles) and repetitive structure of chords are learned by the PCFG. Any variables specified by a user can be optimized or sampled in a principled manner according to a unified posterior distribution. For improved melodization, a long short-term memory (LSTM) network can also be used. The subjective experimental result showed the effectiveness of the proposed system."
19,Daniel Harasim;Martin Rohrmeier;Timothy J. O'Donnell,A Generalized Parsing Framework for Generative Models of Harmonic Syntax,2018,https://doi.org/10.5281/zenodo.1492367,"Daniel Harasim+École Polytechnique Fédérale de Lausanne>CHE>education|Institut für Kunst- und Musikwissenschaft, TU Dresden>DEU>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education|Institut für Kunst- und Musikwissenschaft, TU Dresden>DEU>education;Timothy J. O’Donnell+McGill University>CAN>education","Modeling the structure of musical pieces constitutes a central research problem for music information retrieval, music generation, and musicology. At the present, models of harmonic syntax face challenges on the tasks of detecting local and higher-level modulations (most previous models assume a priori knowledge of key), computing connected parse trees for long sequences, and parsing sequences that do not end with tonic chords, but in turnarounds. This paper addresses those problems by proposing a new generative formalism Probabilistic Abstract Context-Free Grammars (PACFGs) to address these issues, and presents variants of standard parsing algorithms that efficiently enumerate all possible parses of long chord sequences and to estimate their probabilities. PACFGs specifically allow for structured non-terminal symbols in rich and highly flexible feature spaces. The inference procedure moreover takes advantage of these abstractions by sharing probability mass between grammar rules over joint features. The paper presents a model of the harmonic syntax of Jazz using this formalism together with stochastic variational inference to learn the probabilistic parameters of a grammar from a corpus of Jazz-standards. The PACFG model outperforms the standard context-free approach while reducing the number of free parameters and performing key finding on the fly."
20,Peter M. C. Harrison;Marcus T. Pearce,An Energy-based Generative Sequence Model for Testing Sensory Theories of Western Harmony,2018,https://doi.org/10.5281/zenodo.1492369,Peter M. C. Harrison+Queen Mary University of London>GBR>education;Marcus T. Pearce+Queen Mary University of London>GBR>education,"The relationship between sensory consonance and Western harmony is an important topic in music theory and psychology. We introduce new methods for analysing this relationship, and apply them to large corpora representing three prominent genres of Western music: classical, popular, and jazz music. These methods centre on a generative sequence model with an exponential-family energy-based form that predicts chord sequences from continuous features. We use this model to investigate one aspect of instantaneous consonance (harmonicity) and two aspects of sequential consonance (spectral distance and voice-leading distance). Applied to our three musical genres, the results generally support the relationship between sensory consonance and harmony, but lead us to question the high importance attributed to spectral distance in the psychological literature. We anticipate that our methods will provide a useful platform for future work linking music psychology to music theory."
21,Shun-Yao Shih;Heng-Yu Chi,"Automatic, Personalized, and Flexible Playlist Generation using Reinforcement Learning",2018,https://doi.org/10.5281/zenodo.1492371,Shun-Yao Shih+National Taiwan University>TWN>education;Heng-Yu Chi+KKBOX Inc.>TWN>company,"Songs can be well arranged by professional music curators to form a riveting playlist that creates engaging listening experiences. However, it is time-consuming for curators to timely rearrange these playlists for fitting trends in future. By exploiting the techniques of deep learning and reinforcement learning, in this paper, we consider music playlist generation as a language modeling problem and solve it by the proposed attention language model with policy gradient. We develop a systematic and interactive approach so that the resulting playlists can be tuned flexibly according to user preferences. Considering a playlist as a sequence of words, we first train our attention RNN language model on baseline recommended playlists. By optimizing suitable imposed reward functions, the model is thus refined for corresponding preferences. The experimental results demonstrate that our approach not only generates coherent playlists automatically but is also able to flexibly recommend personalized playlists for diversity, novelty and freshness."
22,Philippe Esling;Axel Chemla--Romeu-Santos;Adrien Bitton,"Bridging Audio Analysis, Perception and Synthesis with Perceptually-regularized Variational Timbre Spaces",2018,https://doi.org/10.5281/zenodo.1492373,Philippe Esling+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility;Axel Chemla–Romeu-Santos+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility;Adrien Bitton+Institut de Recherche et Coordination Acoustique-Musique (IRCAM)>FRA>facility,"Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception."
23,Rachel Manzelli;Vijay Thakkar;Ali Siahkamari;Brian Kulis,Conditioning Deep Generative Raw Audio Models for Structured Automatic Music,2018,https://doi.org/10.5281/zenodo.1492375,Rachel Manzelli+Boston University>USA>education;Vijay Thakkar+Boston University>USA>education;Ali Siahkamari+Boston University>USA>education;Brian Kulis+Boston University>USA>education,"Existing automatic music generation approaches that feature deep learning can be broadly classified into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind's WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work."
24,Hao-Wen Dong;Yi-Hsuan Yang,Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation,2018,https://doi.org/10.5281/zenodo.1492377,Hao-Wen Dong+Academia Sinica>TWN>education|Research Center for IT Innovation>Unknown>Unknown;Yi-Hsuan Yang+Academia Sinica>TWN>education|Research Center for IT Innovation>Unknown>Unknown,"It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binaryvalued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binaryvalued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445. github.io/bmusegan/."
25,Christopher Tralie,Cover Song Synthesis by Analogy,2018,https://doi.org/10.5281/zenodo.1492381,Christopher J. Tralie+Duke University>USA>education,"In this work, we pose and address the following ""cover song analogies"" problem: given a song A by artist 1 and a cover song A' of this song by artist 2, and given a different song B by artist 1, synthesize a song B' which is a cover of B in the style of artist 2. Normally, such a polyphonic style transfer problem would be quite challenging, but we show how the cover songs example constrains the problem, making it easier to solve. First, we extract the longest common beat-synchronous subsequence between A and A', and we time stretch the corresponding beat intervals in A' so that they align with A. We then derive a version of joint 2D convolutional NMF, which we apply to the constant-Q spectrograms of the synchronized segments to learn a translation dictionary of sound templates from A to A'. Finally, we apply the learned templates as filters to the song B, and we mash up the translated filtered components into the synthesized song B' using audio mosaicing. We showcase our algorithm on several examples, including a synthesized cover version of Michael Jackson's ""Bad"" by Alien Ant Farm, learned from the latter's ""Smooth Criminal"" cover."
26,Yujia Yan;Ethan Lustig;Joseph VanderStel;Zhiyao Duan,Part-invariant Model for Music Generation and Harmonization,2018,https://doi.org/10.5281/zenodo.1492383,Yujia Yan+University of Rochester>USA>education;Ethan Lustig+University of Rochester>USA>education;Joseph Vander Stel+University of Rochester>USA>education;Zhiyao Duan+University of Rochester>USA>education,"Automatic music generation has been gaining more attention in recent years. Existing approaches, however, are mostly ad hoc to specific rhythmic structures or instrumentation layouts, and lack music-theoretic rigor in their evaluations. In this paper, we present a neural language (music) model that tries to model symbolic multi-part music. Our model is part-invariant, i.e., it can process/generate any part (voice) of a music score consisting of an arbitrary number of parts, using a single trained model. For better incorporating structural information of pitch spaces, we use a structured embedding matrix to encode multiple aspects of a pitch into a vector representation. The generation is performed by Gibbs Sampling. Meanwhile, our model directly generates note spellings to make outputs human-readable. We performed objective (grading) and subjective (listening) evaluations by recruiting music theorists to compare the outputs of our algorithm with those of music students on the task of bassline harmonization (a traditional pedagogical task). Our experiment shows that errors of our algorithm and students are differently distributed, and the range of ratings for generated pieces overlaps with students' to varying extents for our three provided basslines. This experiment suggests some future research directions."
27,David Sears;Filip Korzeniowski;Gerhard Widmer,Evaluating Language Models of Tonal Harmony,2018,https://doi.org/10.5281/zenodo.1492385,David R. W. Sears+Texas Tech University>USA>education;Filip Korzeniowski+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education,"This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most stateof-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types."
28,Bochen Li;Akira Maezawa;Zhiyao Duan,Skeleton Plays Piano: Online Generation of Pianist Body Movements from MIDI Performance,2018,https://doi.org/10.5281/zenodo.1492387,Bochen Li+University of Rochester>USA>education;Akira Maezawa+Yamaha Corporation>JPN>company;Zhiyao Duan+University of Rochester>USA>education,"Generating expressive body movements of a pianist for a given symbolic sequence of key depressions is important for music interaction, but most existing methods cannot incorporate musical context information and generate movements of body joints that are further away from the fingers such as head and shoulders. This paper addresses such limitations by directly training a deep neural network system to map a MIDI note stream and additional metric structures to a skeleton sequence of a pianist playing a keyboard instrument in an online fashion. Experiments show that (a) incorporation of metric information yields in 4% smaller error, (b) the model is capable of learning the motion behavior of a specific player, and (c) no significant difference between the generated and real human movements is observed by human subjects in 75% of the pieces."
29,Jan Hajič jr.;Matthias Dorfer;Gerhard Widmer;Pavel Pecina,Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets,2018,https://doi.org/10.5281/zenodo.1492389,Jan Hajič jr.+Charles University>CHE>education;Matthias Dorfer+Johannes Kepler University>AUT>education;Gerhard Widmer+Johannes Kepler University>AUT>education;Pavel Pecina+Charles University>CHE>education,"Detecting music notation symbols is the most immediate unsolved subproblem in Optical Music Recognition for musical manuscripts. We show that a U-Net architecture for semantic segmentation combined with a trivial detector already establishes a high baseline for this task, and we propose tricks that further improve detection performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81. Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach."
30,Tim Crawford;Golnaz Badkobeh;David Lewis,Searching Page-Images of Early Music Scanned with OMR: A Scalable Solution Using Minimal Absent Words,2018,https://doi.org/10.5281/zenodo.1492391,"Tim Crawford+Goldsmiths, University of London>GBR>education;Golnaz Badkobeh+Goldsmiths, University of London>GBR>education;David Lewis+Oxford eResearch Centre>GBR>facility","We define three retrieval tasks requiring efficient search of the musical content of a collection of ~32k pageimages of 16th-century music to find: duplicates; pages with the same musical content; pages of related music.  The images are subjected to Optical Music Recognition (OMR), introducing inevitable errors. We encode pages as strings of diatonic pitch intervals, ignoring rests, to reduce the effect of such errors. We extract indices comprising lists of two kinds of 'word'. Approximate matching is done by counting the number of common words between a query page and those in the collection.  The two word-types are (a) normal ngrams and (b) minimal absent words (MAWs). The latter have three important properties for our purpose: they can be built and searched in linear time, the number of MAWs generated tends to be smaller, and they preserve the structure and order of the text, obviating the need for expensive sorting operations.  We show that retrieval performance of MAWs is comparable with ngrams, but with a marked speed improvement. We also show the effect of word length on retrieval. Our results suggest that an index of MAWs of mixed length provides a good method for these tasks which is scalable to larger collections."
31,Alexander Pacha;Jorge Calvo-Zaragoza,Optical Music Recognition in Mensural Notation with Region-based Convolutional Neural Networks,2018,https://doi.org/10.5281/zenodo.1492393,"Alexander Pacha+Institute of Visual Computing and Human-Centered Technology, TU Wien>AUT>education;Jorge Calvo-Zaragoza+PRHLT Research Center, Universitat Politècnica de València>ESP>facility","In this work, we present an approach for the task of optical music recognition (OMR) using deep neural networks. Our intention is to simultaneously detect and categorize musical symbols in handwritten scores, written in mensural notation. We propose the use of region-based convolutional neural networks, which are trained in an end-toend fashion for that purpose. Additionally, we make use of a convolutional neural network that predicts the relative position of a detected symbol within the staff, so that we cover the entire image-processing part of the OMR pipeline. This strategy is evaluated over a set of 60 ancient scores in mensural notation, with more than 15000 annotated symbols belonging to 32 different classes. The results reflect the feasibility and capability of this approach, with a weighted mean average precision of around 76% for symbol detection, and over 98% accuracy for predicting the position."
32,Jorge Calvo-Zaragoza;David Rizo,Camera-PrIMuS: Neural End-to-End Optical Music Recognition on Realistic Monophonic Scores,2018,https://doi.org/10.5281/zenodo.1492395,Jorge Calvo-Zaragoza+PRHLT Research Center>ESP>facility;David Rizo+Universidad de Alicante>ESP>education,"The optical music recognition (OMR) field studies how to automate the process of reading the musical notation present in a given image. Among its many uses, an interesting scenario is that in which a score captured with a camera is to be automatically reproduced. Recent approaches to OMR have shown that the use of deep neural networks allows important advances in the field. However, these approaches have been evaluated on images with ideal conditions, which do not correspond to the previous scenario. In this work, we evaluate the performance of an end-to-end approach that uses a deep convolutional recurrent neural network (CRNN) over non-ideal image conditions of music scores. Consequently, our contribution also consists of Camera-PrIMuS, a corpus of printed monophonic scores of real music synthetically modified to resemble camera-based realistic scenarios, involving distortions such as irregular lighting, rotations, or blurring. Our results confirm that the CRNN is able to successfully solve the task under these conditions, obtaining an error around 2% at music-symbol level, thereby representing a groundbreaking piece of research towards useful OMR systems."
33,Francisco Castellanos;Jorge Calvo-Zaragoza;Gabriel Vigliensoni;Ichiro Fujinaga,Document Analysis of Music Score Images with Selectional Auto-Encoders,2018,https://doi.org/10.5281/zenodo.1492397,"Francisco J. Castellanos+University of Alicante>ESP>education;Jorge Calvo-Zaragoza+PRHLT Research Center, Universitat Politècnica de València>ESP>facility;Gabriel Vigliensoni+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education","The document analysis of music score images is a key step in the development of successful Optical Music Recognition systems. The current state of the art considers the use of deep neural networks trained to classify every pixel of the image according to the image layer it belongs to. This process, however, involves a high computational cost that prevents its use in interactive machine learning scenarios. In this paper, we propose the use of a set of deep selectional auto-encoders, implemented as fully-convolutional networks, to perform image-to-image categorizations. This strategy retains the advantages of using deep neural networks, which have demonstrated their ability to perform this task, while dramatically increasing the efficiency by processing a large number of pixels in a single step. The results of an experiment performed with a set of high-resolution images taken from Medieval manuscripts successfully validate this approach, with a similar accuracy to that of the state of the art but with a computational time orders of magnitude smaller, making this approach appropriate for being used in interactive applications."
34,Filip Korzeniowski;Gerhard Widmer,Genre-Agnostic Key Classification With Convolutional Neural Networks,2018,https://doi.org/10.5281/zenodo.1492399,Filip Korzeniowski+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education|Institute of Computational Perception>AUT>facility,"We propose modifications to the model structure and training procedure to a recently introduced Convolutional Neural Network for musical key classification. These modifications enable the network to learn a genre-independent model that performs better than models trained for specific music styles, which has not been the case in existing work. We analyse this generalisation capability on three datasets comprising distinct genres. We then evaluate the model on a number of unseen data sets, and show its superior performance compared to the state of the art. Finally, we investigate the model's performance on short excerpts of audio. From these experiments, we conclude that models need to consider the harmonic coherence of the whole piece when classifying the local key of short segments of audio."
35,Lukas Tuggener;Ismail Elezi;Jürgen Schmidhuber;Thilo Stadelmann,Deep Watershed Detector for Music Object Recognition,2018,https://doi.org/10.5281/zenodo.1492401,"Lukas Tuggener+ZHAW Datalab, Zurich University of Applied Sciences>CHE>education|Faculty of Informatics, Università della Svizzera italiana>CHE>education;Ismail Elezi+ZHAW Datalab, Zurich University of Applied Sciences>CHE>education|Ca’ Foscari University of Venice>ITA>education;Jürgen Schmidhuber+Faculty of Informatics, Università della Svizzera italiana>CHE>education;Thilo Stadelmann+ZHAW Datalab, Zurich University of Applied Sciences>CHE>education","Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD's ability to work with synthetic scores equally well as with handwritten music."
36,Reinier de Valk;Tillman Weyde,Deep Neural Networks with Voice Entry Estimation Heuristics for Voice Separation in Symbolic Music Representations,2018,https://doi.org/10.5281/zenodo.1492403,"Reinier de Valk+Jukedeck Ltd.>GBR>company;Tillman Weyde+City, University of London>GBR>education","In this study we explore the use of deep feedforward neural networks for voice separation in symbolic music representations. We experiment with different network architectures, varying the number and size of the hidden layers, and with dropout. We integrate two voice entry estimation heuristics that estimate the entry points of the individual voices in the polyphonic fabric into the models. These heuristics serve to reduce error propagation at the beginning of a piece, which, as we have shown in previous work, can seriously hamper model performance. The models are evaluated on the 48 fugues from Johann Sebastian Bach's The Well-Tempered Clavier and his 30 inventions—a dataset that we curated and make publicly available. We find that a model with two hidden layers yields the best results. Using more layers does not lead to a significant performance improvement. Furthermore, we find that our voice entry estimation heuristics are highly effective in the reduction of error propagation, improving performance significantly. Our best-performing model outperforms our previous models, where the difference is significant, and, depending on the evaluation metric, performs close to or better than the reported state of the art."
37,Sungheon Park;Taehoon Kim;Kyogu Lee;Nojun Kwak,Music Source Separation Using Stacked Hourglass Networks,2018,https://doi.org/10.5281/zenodo.1492405,Sungheon Park+Seoul National University>KOR>education;Taehoon Kim+Seoul National University>KOR>education;Kyogu Lee+Seoul National University>KOR>education;Nojun Kwak+Seoul National University>KOR>education,"In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks."
38,Ethan Manilow;Prem Seetharaman;Bryan Pardo,The Northwestern University Source Separation Library,2018,https://doi.org/10.5281/zenodo.1492407,Ethan Manilow+Northwestern University>USA>education;Prem Seetharaman+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"Audio source separation is the process of isolating individual sonic elements from a mixture or auditory scene. We present the Northwestern University Source Separation Library, or nussl for short. nussl (pronounced 'nuzzle') is an open-source, object-oriented audio source separation library implemented in Python. nussl provides implementations for many existing source separation algorithms and a platform for creating the next generation of source separation algorithms. By nature of its design, nussl easily allows new algorithms to be benchmarked against existing algorithms on established data sets and facilitates development of new variations on algorithms. Here, we present the design methodologies in nussl, two experiments using it, and use nussl to showcase benchmarks for some algorithms contained within."
39,Jakob Abeßer;Stefan Balke;Meinard Müller,Improving Bass Saliency Estimation using Transfer Learning and Label Propagation,2018,https://doi.org/10.5281/zenodo.1492411,Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Stefan Balke+International Audio Laboratories Erlangen>DEU>education;Meinard Müller+International Audio Laboratories Erlangen>DEU>education,"In this paper, we consider two methods to improve an algorithm for bass saliency estimation in jazz ensemble recordings which are based on deep neural networks. First, we apply label propagation to increase the amount of training data by transferring pitch labels from our labeled dataset to unlabeled audio recordings using a spectral similarity measure. Second, we study in several transfer learning experiments, whether isolated note recordings can be beneficial for pre-training a model which is later fine-tuned on ensemble recordings. Our results indicate that both strategies can improve the performance on bass saliency estimation by up to five percent in accuracy."
40,Carl Southall;Ryan Stables;Jason Hockman,Improving Peak-picking Using Multiple Time-step Loss Functions,2018,https://doi.org/10.5281/zenodo.1492409,Carl Southall+Birmingham City University>GBR>education;Ryan Stables+Birmingham City University>GBR>education;Jason Hockman+Birmingham City University>GBR>education,"The majority of state-of-the-art methods for music information retrieval (MIR) tasks now utilise deep learning methods reliant on minimisation of loss functions such as cross entropy. For tasks that include framewise binary classification (e.g., onset detection, music transcription) classes are derived from output activation functions by identifying points of local maxima, or peaks. However, the operating principles behind peak picking are different to that of the cross entropy loss function, which minimises the absolute difference between the output and target values for a single frame. To generate activation functions more suited to peak-picking, we propose two versions of a new loss function that incorporates information from multiple time-steps: 1) multi-individual, which uses multiple individual time-step cross entropies; and 2) multi-difference, which directly compares the difference between sequential time-step outputs. We evaluate the newly proposed loss functions alongside standard cross entropy in the popular MIR tasks of onset detection and automatic drum transcription. The results highlight the effectiveness of these loss functions in the improvement of overall system accuracies for both MIR tasks. Additionally, directly comparing the output from sequential time-steps in the multidifference approach achieves the highest performance."
41,Jan Schlüter;Bernhard Lehner,Zero-Mean Convolutions for Level-Invariant Singing Voice Detection,2018,https://doi.org/10.5281/zenodo.1492413,Jan Schlüter+Austrian Research Institute for Artificial Intelligence>AUT>facility;Bernhard Lehner+Johannes Kepler University Linz>AUT>education,"State-of-the-art singing voice detectors are based on classifiers trained on annotated examples. As recently shown, such detectors have an important weakness: Since singing voice is correlated with sound level in training data, classifiers learn to become sensitive to input magnitude, and give different predictions for the same signal at different sound levels. Starting from a Convolutional Neural Network (CNN) trained on logarithmic-magnitude mel spectrogram excerpts, we eliminate this dependency by forcing each first-layer convolutional filter to be zero-mean – that is, to have its coefficients sum to zero. In contrast to four other methods – data augmentation, instance normalization, spectral delta features, and per-channel energy normalization (PCEN) – that we evaluated on a largescale public dataset, zero-mean convolutions achieve perfect sound level invariance without any impact on prediction accuracy or computational requirements. We assume that zero-mean convolutions would be useful for other machine listening tasks requiring robustness to level changes."
42,Mathieu Andreux;Stéphane Mallat,Music Generation and Transformation with Moment Matching-Scattering Inverse Networks,2018,https://doi.org/10.5281/zenodo.1492415,Mathieu Andreux+Ecole normale supérieure>FRA>education|CNRS>FRA>facility|PSL Research University>FRA>education;Stéphane Mallat+Ecole normale supérieure>FRA>education|CNRS>FRA>facility|PSL Research University>FRA>education,"We introduce a Moment Matching-Scattering Inverse Network (MM-SIN) to generate and transform musical sounds. The MM-SIN generator is similar to a variational autoencoder or an adversarial network. However, the encoder or the discriminator are not learned, but computed with a scattering transform defined from prior information on sparse time-frequency audio properties. The generator is trained by jointly minimizing the reconstruction loss of an inverse problem, and a generation loss which computes a distance over scattering moments. It has a similar causal architecture as a WaveNet and provides a simpler mathematical model related to time-frequency decompositions. Numerical experiments demonstrate that this MMSIN generates new realistic musical signals. It can transform low-level musical attributes such as pitch with a linear transformation in the embedding space of scattering coefficients."
43,Daniel Stoller;Sebastian Ewert;Simon Dixon,Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation,2018,https://doi.org/10.5281/zenodo.1492417,Daniel Stoller+Queen Mary University of London>GBR>education;Sebastian Ewert+Spotify>USA>company;Simon Dixon+Queen Mary University of London>GBR>education,"Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyperparameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a stateof-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem."
44,Melissa R. McGuirl;Katherine M. Kinnaird;Claire Savard;Erin H. Bugbee,SE and SNL diagrams: Flexible data structures for MIR,2018,https://doi.org/10.5281/zenodo.1492419,Melissa R. McGuirl+Brown University>USA>education;Katherine M. Kinnaird+Brown University>USA>education;Claire Savard+University of Michigan>USA>education;Erin H. Bugbee+Brown University>USA>education,"to interpret. The matrix-based representations commonly used in MIR tasks are often difficult This work introduces start-end (SE) diagrams and start(normalized)length (SNL) diagrams, two novel structure-based representations for sequential music data. Inspired by methods from topological data analysis, both SE and SNL diagrams come equipped with efficiently computable and stable metrics. Utilizing SE or SNL diagrams as input, we address the cover song task for score-based data with high accuracy. While both representations are concisely defined and flexible, SNL diagrams in particular address issues introduced by commonly used resampling methods."
45,Cory McKay;Julie Cumming;Ichiro Fujinaga,JSYMBOLIC 2.2: Extracting Features from Symbolic Music for use in Musicological and MIR Research,2018,https://doi.org/10.5281/zenodo.1492421,Cory McKay+McGill University>CAN>education;Julie E. Cumming+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"jSymbolic is an open-source platform for extracting features from symbolic music. These features can serve as inputs to machine learning algorithms, or they can be analyzed statistically to derive musicological insights.  jSymbolic implements 246 unique features, comprising 1497 different values, making it by far the most extensive symbolic feature extractor to date. These features are designed to be applicable to a diverse range of musics, and may be extracted from both symbolic music files as a whole and from windowed subsets of them. Researchers can also use jSymbolic as a platform for developing and distributing their own bespoke features, as it has an easily extensible plug-in architecture.  In addition to implementing 135 new unique features, version 2.2 of jSymbolic places a special focus on functionality for avoiding biases associated with how symbolic music is encoded. In addition, new interface elements and documentation improve convenience, ease-of-use and accessibility to researchers with diverse ranges of technical expertise. jSymbolic now includes a GUI, command-line interface, API , flexible configuration file format, extensive manual and detailed tutorial.  The enhanced effectiveness of jSymbolic 2.2's features is demonstrated in two sets of experiments: 1) genre classification and 2) Renaissance composer attribution."
46,Louis Bigo;Laurent Feisthauer;Mathieu Giraud;Florence Levé,Relevance of Musical Features for Cadence Detection,2018,https://doi.org/10.5281/zenodo.1492423,"Louis Bigo+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Laurent Feisthauer+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Mathieu Giraud+CRIStAL, UMR 9189, CNRS, Université de Lille>FRA>education;Florence Levé+MIS, Université de Picardie Jules Verne>FRA>education","Cadences, as breaths in music, are felt by the listener or studied by the theorist by combining harmony, melody, texture and possibly other musical aspects. We formalize and discuss the significance of 44 cadential features, correlated with the occurrence of cadences in scores. These features describe properties at the arrival beat of a cadence and its surroundings, but also at other onsets heuristically identified to pinpoint chords preparing the cadence. The representation of each beat of the score as a vector of cadential features makes it possible to reformulate cadence detection as a classification task. An SVM classifier was run on two corpora from Bach and Haydn totaling 162 perfect authentic cadences and 70 half cadences. In these corpora, the classifier correctly identified more than 75% of perfect authentic cadences and 50% of half cadences, with low false positive rates. The experiment results are consistent with common knowledge that classification is more complex for half cadences than for authentic cadences."
47,Xiao Hu;Fanjie Li;Jeremy T. D. Ng,On the Relationships between Music-induced Emotion and Physiological Signals,2018,https://doi.org/10.5281/zenodo.1492425,Xiao Hu+The University of Hong Kong>HKG>education;Fanjie Li+Sichuan University>CHN>education;Jeremy T. D. Ng+The University of Hong Kong>HKG>education,"to optimize emotion-aware music retrieval.  Emotion-aware music information retrieval (MIR) has been difficult due to the subjectivity and temporality of emotion responses to music. Physiological signals are regarded as related to emotion and thus could potentially be exploited in emotion-aware music discovery. This study explored the possibility of using physiological signals to detect users' emotion responses to music, with consideration of individual characteristics (personality, music preferences, etc.). A user experiment was conducted with 23 participants who searched for music in a novel MIR system. Users' listening behaviors and self-reported emotion responses to a total of 628 music pieces were collected. During music listening, a series of peripheral physiological signals (e.g., heart rate, skin conductance) were recorded from participants unobtrusively using a researchgrade wearable wristband. A set of features in the time- and frequency- domains were extracted from the physiological signals and analyzed using statistical and machine learning methods. Results reveal 1) significant differences in some physiological features between positive and negative arousal and mood categories, and 2) effective classification of emotion responses based on physiological signals for some individuals. The findings can contribute to further improvement of emotion-aware intelligent MIR systems exploiting physiological signals as an objective and personalized input."
48,Rémi Delbouys;Romain Hennequin;Francesco Piccoli;Jimena Royo-Letelier;Manuel Moussallam,Music Mood Detection Based on Audio and Lyrics with Deep Neural Net,2018,https://doi.org/10.5281/zenodo.1492427,R´emi Delbouys+Deezer>FRA>company;Romain Hennequin+Deezer>FRA>company;Francesco Piccoli+Deezer>FRA>company;Jimena Royo-Letelier+Deezer>FRA>company;Manuel Moussallam+Deezer>FRA>company,"1.1 Related work We consider the task of multimodal music mood prediction based on the audio signal and the lyrics of a track. We reproduce the implementation of traditional feature engineering based approaches and propose a new model based on deep learning. We compare the performance of both approaches on a database containing 18,000 tracks with associated valence and arousal values and show that our approach outperforms classical models on the arousal detection task, and that both approaches perform equally on the valence prediction task. We also compare the a posteriori fusion with fusion of modalities optimized simultaneously with each unimodal model, and observe a significant improvement of valence prediction. We release part of our database for comparison purposes."
49,Emilia Parada-Cabaleiro;Maximilian Schmitt;Anton Batliner;Simone Hantke;Giovanni Costantini;Klaus Scherer;Bjoern Schuller,Identifying Emotions in Opera Singing: Implications of Adverse Acoustic Conditions,2018,https://doi.org/10.5281/zenodo.1492429,Emilia Parada-Cabaleiro+University of Augsburg>DEU>education;Maximilian Schmitt+University of Augsburg>DEU>education;Anton Batliner+University of Augsburg>DEU>education;Simone Hantke+Technische Universität München>DEU>education;Giovanni Costantini+University of Rome Tor Vergata>ITA>education;Klaus Scherer+University of Geneva>CHE>education;Björn W. Schuller+Imperial College London>GBR>education,"The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners' and machines' identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners' gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion."
50,Renato Panda;Ricardo Malheiro;Rui Pedro Paiva,Musical Texture and Expressivity Features for Music Emotion Recognition,2018,https://doi.org/10.5281/zenodo.1492431,"Renato Panda+Centre for Informatics and Systems, University of Coimbra>PRT>education;Ricardo Malheiro+Centre for Informatics and Systems, University of Coimbra>PRT>education;Rui Pedro Paiva+Centre for Informatics and Systems, University of Coimbra>PRT>education","We present a set of novel emotionally-relevant audio features to help improving the classification of emotions in audio music. First, a review of the state-of-the-art regarding emotion and music was conducted, to understand how the various music concepts may influence human emotions. Next, well known audio frameworks were analyzed, assessing how their extractors relate with the studied musical concepts. The intersection of this data showed an unbalanced representation of the eight musical concepts. Namely, most extractors are low-level and related with tone color, while musical form, musical texture and expressive techniques are lacking. Based on this, we developed a set of new algorithms to capture information related with musical texture and expressive techniques, the two most lacking concepts. To validate our work, a public dataset containing 900 30-second clips, annotated in terms of Russell's emotion quadrants was created. The inclusion of our features improved the F1-score obtained using the best 100 features by 8.6% (to 76.0%), using support vector machines and 20 repetitions of 10-fold cross-validation."
51,André Ofner;Sebastian Stober,Shared Generative Representation of Auditory Concepts and EEG to Reconstruct Perceived and Imagined Music,2018,https://doi.org/10.5281/zenodo.1492433,André Ofner+University of Potsdam>DEU>education;Sebastian Stober+University of Potsdam>DEU>education,"Retrieving music information from brain activity is a challenging and still largely unexplored research problem. In this paper we investigate the possibility to reconstruct perceived and imagined musical stimuli from electroencephalography (EEG) recordings based on two datasets. One dataset contains multichannel EEG of subjects listening to and imagining rhythmical patterns presented both as sine wave tones and short looped spoken utterances. These utterances leverage the well-known speech-to-song illusory transformation which results in very catchy and easy to reproduce motifs. A second dataset provides EEG recordings for the perception of 10 full length songs. Using a multi-view deep generative model we demonstrate the feasibility of learning a shared latent representation of brain activity and auditory concepts, such as rhythmical motifs appearing across different instrumentations. Introspection of the model trained on the rhythm dataset reveals disentangled rhythmical and timbral features within and across subjects. The model allows continuous interpolation between representations of different observed variants of the presented stimuli. By decoding the learned embeddings we were able to reconstruct both perceived and imagined music. Stimulus complexity and the choice of training data shows strong effect on the reconstruction quality."
52,Renan de Padua;Verônica Oliveira de Carvalho;Solange Rezende;Diego Furtado Silva,Exploring Musical Relations Using Association Rule Networks,2018,https://doi.org/10.5281/zenodo.1492435,"Renan de Padua+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education|Data Science Team, Itaú-Unibanco>BRA>company;Verônica Oliveira de Carvalho+Instituto de Geociências e Ciências Exatas – Universidade Estadual Paulista>BRA>education;Solange de Oliveira Rezende+Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo>BRA>education;Diego Furtado Silva+Departamento de Computação – Universidade Federal de São Carlos>BRA>education","Music information retrieval (MIR) has been gaining increasing attention in both industry and academia. While many algorithms for MIR rely on assessing feature subsequences, the user normally has no resources to interpret the significance of these patterns. Interpreting the relations between these temporal patterns and some aspects of the assessed songs can help understanding not only some algorithms' outcomes but the kind of patterns which better defines a set of similarly labeled recordings. In this work, we present a novel method to assess these relations, constructing an association rule network from temporal patterns obtained by a simple quantization process. With an empirical evaluation, we illustrate how we can use our method to explore these relations in a varied set of data and labels."
53,Hendrik Schreiber;Meinard Müller,A Crowdsourced Experiment for Tempo Estimation of Electronic Dance Music,2018,https://doi.org/10.5281/zenodo.1492437,Hendrik Schreiber+tagtraum industries incorporated>DEU>company;Meinard Müller+International Audio Laboratories Erlangen>DEU>education,"Relative to other datasets, state-of-the-art tempo estimation algorithms perform poorly on the GiantSteps Tempo dataset for electronic dance music (EDM). In order to investigate why, we conducted a large-scale, crowdsourced experiment involving 266 participants from two distinct groups. The quality of the collected data was evaluated with regard to the participants' input devices and background. In the data itself we observed significant tempo ambiguities, which we attribute to annotator subjectivity and tempo instability. As a further contribution, we then constructed new annotations consisting of tempo distributions for each track. Using these annotations, we reevaluated two recent state-of-the-art tempo estimation systems achieving significantly improved results. The main conclusions of this investigation are that current tempo estimation systems perform better than previously thought and that evaluation quality needs to be improved. The new crowdsourced annotations will be released for evaluation purposes."
54,Christof Weiss;Stefan Balke;Jakob Abeßer;Meinard Müller,Computational Corpus Analysis: A Case Study on Jazz Solos,2018,https://doi.org/10.5281/zenodo.1492439,"Christof Weiß+International Audio Laboratories Erlangen>DEU>facility;Stefan Balke+International Audio Laboratories Erlangen>DEU>facility;Jakob Abeßer+Semantic Music Technologies Group, Fraunhofer IDMT>DEU>facility;Meinard Müller+International Audio Laboratories Erlangen>DEU>facility","For musicological studies on large corpora, the compilation of suitable data constitutes a time-consuming step. In particular, this is true for high-quality symbolic representations that are generated manually in a tedious process. A recent study on Western classical music has shown that musical phenomena such as the evolution of tonal complexity over history can also be analyzed on the basis of audio recordings. As our first contribution, we transfer this corpus analysis method to jazz music using the Weimar Jazz Database, which contains high-level symbolic transcriptions of jazz solos along with the audio recordings. Second, we investigate the influence of the input representation type on the corpus-level observations. In our experiments, all representation types led to qualitatively similar results. We conclude that audio recordings can build a reasonable basis for conducting such type of corpus analysis."
55,Pasquale Lisena;Konstantin Todorov;Cécile Cecconi;Françoise Leresche;Isabelle Canno;Frédéric Puyrenier;Martine Voisin;Thierry Le Meur;Raphaël Troncy,Controlled Vocabularies for Music Metadata,2018,https://doi.org/10.5281/zenodo.1492441,Pasquale Lisena+EURECOM>FRA>education|LIRMM>FRA>education|Philharmonie de Paris>FRA>facility|Bibliotheque nationale de France>FRA>facility|Radio France>FRA>company;Konstantin Todorov+LIRMM>FRA>education;Cécile Cecconi+Philharmonie de Paris>FRA>facility;Françoise Leresche+Bibliotheque nationale de France>FRA>facility;Isabelle Canno+Radio France>FRA>company;Frédéric Puyrenier+Bibliotheque nationale de France>FRA>facility;Martine Voisin+Radio France>FRA>company;Thierry Le Meur+Philharmonie de Paris>FRA>facility;Raphaël Troncy+EURECOM>FRA>education,"We present a set of music-specific controlled vocabularies, formalized using Semantic Web languages, describing topics like musical genres, keys, or medium of performance. We have collected a number of existing vocabularies in various formats, converted them to SKOS and performed the interconnection of their equivalent terms. In addition, novel vocabularies, not available online before, have been designed by an editorial team. Next to multilingual labels and definitions, we provide hierarchical relations as well as links to external resources. We also show the application of those vocabularies for the production of vector embeddings, allowing for the calculation of distances between keys or between instruments."
56,Gabriel Meseguer-Brocal;Alice Cohen-Hadria;Geoffroy Peeters,"DALI: A Large Dataset of Synchronized Audio, Lyrics and notes, Automatically Created using Teacher-student Machine Learning Paradigm.",2018,https://doi.org/10.5281/zenodo.1492443,"Gabriel Meseguer-Brocal+Ircam Lab, CNRS, Sorbonne Université>FRA>facility;Alice Cohen-Hadria+Ircam Lab, CNRS, Sorbonne Université>FRA>facility;Geoffroy Peeters+Ircam Lab, CNRS, Sorbonne Université>FRA>facility","The goal of this paper is twofold. First, we introduce DALI, a large and rich multimodal dataset containing 5358 audio tracks with their time-aligned vocal melody notes and lyrics at four levels of granularity. The second goal is to explain our methodology where dataset creation and learning models interact using a teacher-student machine learning paradigm that benefits each other. We start with a set of manual annotations of draft time-aligned lyrics and notes made by non-expert users of Karaoke games. This set comes without audio. Therefore, we need to find the corresponding audio and adapt the annotations to it. To that end, we retrieve audio candidates from the Web. Each candidate is then turned into a singing-voice probability over time using a teacher, a deep convolutional neural network singing-voice detection system (SVD), trained on cleaned data. Comparing the time-aligned lyrics and the singing-voice probability, we detect matches and update the time-alignment lyrics accordingly. From this, we obtain new audio sets. They are then used to train new SVD students used to perform again the above comparison. The process could be repeated iteratively. We show that this allows to progressively improve the performances of our SVD and get better audiomatching and alignment."
57,Eric Humphrey;Simon Durand;Brian McFee,OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition,2018,https://doi.org/10.5281/zenodo.1492445,Eric J. Humphrey+Spotify>USA>company;Simon Durand+Spotify>USA>company;Brian McFee+New York University>USA>education,"Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music information retrieval. While there has been significant progress in developing predictive models for this and related classification tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset contains 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the dataset was sampled and annotated, and compare its characteristics to similar, previous data-sets. Finally, we present experimental results and baseline model performance to motivate future work."
58,Chih-Wei Wu;Alexander Lerch,From Labeled to Unlabeled Data – On the Data Challenge in Automatic Drum Transcription,2018,https://doi.org/10.5281/zenodo.1492447,Chih-Wei Wu+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"Automatic Drum Transcription (ADT), like many other music information retrieval tasks, has made progress in the past years through the integration of machine learning and audio signal processing techniques. However, with the increasing popularity of data-hungry approaches such as deep learning, the insufficient amount of data becomes more and more a challenge that concerns the generality of the resulting models and the validity of the evaluation. To address this challenge in ADT, this paper first examines the existing labeled datasets and how representative they are of the research problem. Next, possibilities of using unlabeled data to improve general ADT systems are explored. Specifically, two paradigms that harness information from unlabeled data, namely feature learning and student-teacher learning, are applied to two major types of ADT systems. All systems are evaluated on four different drum datasets. The results highlight the necessity of more and larger annotated datasets and indicate the feasibility of exploiting unlabeled data for improving ADT systems."
59,Qingyang Xi;Rachel Bittner;Johan Pauwels;Xuzhou Ye;Juan Pablo Bello,GuitarSet: A Dataset for Guitar Transcription,2018,https://doi.org/10.5281/zenodo.1492449,"Qingyang Xi+Music and Audio Research Lab, New York University>USA>education;Rachel M. Bittner+Music and Audio Research Lab, New York University>USA>education;Johan Pauwels+Center for Digital Music, Queen Mary University of London>GBR>education;Xuzhou Ye+Music and Audio Research Lab, New York University>USA>education;Juan P. Bello+Music and Audio Research Lab, New York University>USA>education","The guitar is a popular instrument for a variety of reasons, including its ability to produce polyphonic sound and its musical versatility. The resulting variability of sounds, however, poses significant challenges to automated methods for analyzing guitar recordings. As data driven methods become increasingly popular for difficult problems like guitar transcription, sets of labeled audio data are highly valuable resources. In this paper we present GuitarSet, a dataset that provides high quality guitar recordings alongside rich annotations and metadata. In particular, by recording guitars using a hexaphonic pickup, we are able to not only provide recordings of the individual strings but also to largely automate the expensive annotation process. The dataset contains recordings of a variety of musical excerpts played on an acoustic guitar, along with time-aligned annotations of string and fret positions, chords, beats, downbeats, and playing style. We conclude with an analysis of new challenges presented by this data, and see that it is interesting for a wide variety of tasks in addition to guitar transcription, including performance analysis, beat/downbeat tracking, and chord estimation."
60,Emilia Parada-Cabaleiro;Maximilian Schmitt;Anton Batliner;Bjoern Schuller,Musical-Linguistic Annotations of Il Lauro Secco,2018,https://doi.org/10.5281/zenodo.1492451,Emilia Parada-Cabaleiro+University of Augsburg>DEU>education;Maximilian Schmitt+University of Augsburg>DEU>education;Anton Batliner+University of Augsburg>DEU>education;Björn W. Schuller+University of Augsburg>DEU>education|Imperial College London>GBR>education,"In madrigals, The Italian madrigal, a polyphonic secular a cappella composition of the 16th century, is characterised by a strong musical-linguistic relationship, which has made it an icon of the 'Renaissance humanism'. lyrical meaning is mimicked by the music, through the utilisation of a composition technique known as madrigalism. The synergy between Renaissance music and poetry makes madrigals of great value to musicologists, linguists, and historians—thus, it is a promising repertoire for computational musicology. However, the application of computational techniques for automatic detection of madrigalisms within scores of such repertoire is limited by the lack of annotations to refer to. In this regard, we present 30 madrigals of the anthology Il Lauro Secco encoded in two symbolic formats, MEI and **kern, with hand-encoded annotations of madrigalisms. This work aims to encourage the development of algorithms for madrigalism detection, a composition procedure typical of early music, but still underrepresented in music information retrieval research."
61,Julia Wilkins;Prem Seetharaman;Alison Wahl;Bryan Pardo,VocalSet: A Singing Voice Dataset,2018,https://doi.org/10.5281/zenodo.1492453,Julia Wilkins+Northwestern University>USA>education|Ithaca College>USA>education;Prem Seetharaman+Northwestern University>USA>education;Alison Wahl+Ithaca College>USA>education;Bryan Pardo+Northwestern University>USA>education,"We present VocalSet, a singing voice dataset of a capella singing. Existing singing voice datasets either do not capture a large range of vocal techniques, have very few singers, or are single-pitch and devoid of musical context. VocalSet captures not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts. VocalSet has recordings of 10.1 hours of 20 professional singers (11 male, 9 female) performing 17 different different vocal techniques. This data will facilitate the development of new machine learning models for singer identification, vocal technique identification, singing generation and other related applications. To illustrate this, we establish baseline results on vocal technique classification and singer identification by training convolutional network classifiers on VocalSet to perform these tasks."
62,Chris Donahue;Huanru Henry Mao;Julian McAuley,The NES Music Database: A multi-instrumental dataset with expressive performance attributes,2018,https://doi.org/10.5281/zenodo.1492455,Chris Donahue+UC San Diego>USA>education|UC San Diego>USA>education|UC San Diego>USA>education;Huanru Henry Mao+UC San Diego>USA>education;Julian McAuley+UC San Diego>USA>education,"Existing research on music generation focuses on composition, but often ignores the expressive performance characteristics required for plausible renditions of resultant pieces. In this paper, we introduce the Nintendo Entertainment System Music Database (NES-MDB), a large corpus allowing for separate examination of the tasks of composition and performance. NES-MDB contains thousands of multi-instrumental songs composed for playback by the compositionally-constrained NES audio synthesizer. For each song, the dataset contains a musical score for four instrument voices as well as expressive attributes for the dynamics and timbre of each voice. Unlike datasets comprised of General MIDI files, NES-MDB includes all of the information needed to render exact acoustic performances of the original compositions. Alongside the dataset, we provide a tool that renders generated compositions as NESstyle audio by emulating the device's audio processor. Additionally, we establish baselines for the tasks of composition, which consists of learning the semantics of composing for the NES synthesizer, and performance, which involves finding a mapping between a composition and realistic expressive attributes."
63,Vsevolod Eremenko;Emir Demirel;Baris Bozkurt;Xavier Serra,Audio-Aligned Jazz Harmony Dataset for Automatic Chord Transcription and Corpus-based Research,2018,https://doi.org/10.5281/zenodo.1492457,"Vsevolod Eremenko+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Emir Demirel+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Baris Bozkurt+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education","In this paper we present a new dataset of time-aligned jazz harmony transcriptions. This dataset is a useful resource for content-based analysis, especially for training and evaluating chord transcription algorithms. Most of the available chord transcription datasets only contain annotations for rock and pop, and the characteristics of jazz, such as the extensive use of seventh chords, are not represented. Our dataset consists of annotations of 113 tracks selected from ""The Smithsonian Collection of Classic Jazz"" and ""Jazz: The Smithsonian Anthology,"" covering a range of performers, subgenres, and historical periods. Annotations were made by a jazz musician and contain information about the meter, structure, and chords for entire audio tracks. We also present evaluation results of this dataset using stateof-the-art chord estimation algorithms that support seventh chords. The dataset is valuable for jazz scholars interested in corpus-based research. To demonstrate this, we extract statistics for symbolic data and chroma features from the audio tracks."
64,Julie Cumming;Cory McKay;Jonathan Stuchbery;Ichiro Fujinaga,Methodologies for Creating Symbolic Corpora of Western Music Before 1600,2018,https://doi.org/10.5281/zenodo.1492459,Julie E. Cumming+McGill University>CAN>education;Cory McKay+Marianopolis College>CAN>education;Jonathan Stuchbery+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"The creation of a corpus of compositions in symbolic formats is an essential step for any project in systematic research. There are, however, many potential pitfalls, especially in early music, where scores are edited in different ways: variables include clefs, note values, types of barline, and editorial accidentals. Different score editors and optical music recognition software have their own ways of storing and exporting musical data. Choice of software and file formats, and their various parameters, can thus unintentionally bias data, as can decisions on how to interpret potentially ambiguous markings in original sources. This becomes especially problematic when data from different corpora are combined for computational processing, since observed regularities and irregularities may in fact be linked with inconsistent corpus collection methodologies, internal and external, rather than the underlying music.  This paper proposes guidelines, templates, and workflows for the creation of consistent early music corpora, and for detecting encoding biases in existing corpora. We have assembled a corpus of Renaissance duos as a sample implementation, and present machine learning experiments demonstrating how inconsistent or naïve encoding methodologies for corpus collection can distort results."
65,Venkata Viraraghavan;Rangarajan Aravind;Hema Murthy,Precision of Sung Notes in Carnatic Music,2018,https://doi.org/10.5281/zenodo.1492461,"Venkata Subramanian Viraraghavan+TCS Research and Innovation>IND>company|Indian Institute of Technology, Madras>IND>education;Hema A Murthy+Indian Institute of Technology, Madras>IND>education;R Aravind+Indian Institute of Technology, Madras>IND>education","Carnatic music is replete with continuous pitch movement called gamakas and can be viewed as consisting of constant-pitch notes (CPNs) and transients. The stationary points (STAs) of transients – points where the pitch curve changes direction – also carry melody information. In this paper, the precision of sung notes in Carnatic music is studied in detail by treating CPNs and STAs separately. There is variation among the nineteen musicians considered, but on average, the precision of CPNs increases exponentially with duration and settles at about 10 cents for CPNs longer than 0.5 seconds. For analyzing STAs, in contrast to Western music, r¯aga (melody) information is found to be necessary, and errors in STAs show a significantly larger standard deviation of about 60 cents. To corroborate these observations, the music was automatically transcribed and re-synthesized using CPN and STA information using two interpolation techniques. The results of perceptual tests clearly indicate that the grammar is highly flexible. We also show that the precision errors are not due to poor pitch tracking, singer deficiencies or delay in auditory feedback."
66,Kyungyun Lee;Keunwoo Choi;Juhan Nam,Revisiting Singing Voice Detection: A quantitative review and the future outlook,2018,https://doi.org/10.5281/zenodo.1492463,Kyungyun Lee+KAIST>KOR>education;Keunwoo Choi+Spotify Inc.>USA>company;Juhan Nam+KAIST>KOR>education,"Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval. Although several proposed algorithms have shown high performances, we argue that there is still room for improving the singing voice detection system. In order to identify the area of improvement, we first perform an error analysis on three recent singing voice detection systems. Based on the analysis, we design novel methods to test the systems on multiple sets of internally curated and generated data to further examine the pitfalls, which are not clearly revealed with the currently available datasets. From the experiment results, we also propose several directions towards building a more robust singing voice detector."
67,Andrew Demetriou;Andreas Jansson;Aparna Kumar;Rachel Bittner,Vocals in Music Matter: the Relevance of Vocals in the Minds of Listeners,2018,https://doi.org/10.5281/zenodo.1492465,Andrew Demetriou+Spotify Inc.>USA>company;Andreas Jansson+Spotify Inc.>USA>company;Aparna Kumar+Spotify Inc.>USA>company;Rachel M. Bittner+Spotify Inc.>USA>company,"In music information retrieval, we often make assertions about what features of music are important to study, one of which is vocals. While the importance of vocals in music preference is both intuitive and anticipated by psychological theory, we have not found any survey studies that confirm this commonly held assertion. We address two questions: (1) what components of music are most salient to people's musical taste, and (2) how do vocals rank relative to other components of music, in regards to whether people like or dislike a song. Lastly, we explore the aspects of the voice that listeners find important. Two surveys of Spotify users were conducted. The first gathered open-format responses that were then card-sorted into semantic categories by the team of researchers. The second asked respondents to rank the semantic categories derived from the first survey. Responses indicate that vocals were a salient component in the minds of listeners. Further, vocals ranked high as a self-reported factor for a listener liking or disliking a track, among a statistically significant ranking of musical attributes. In addition, we open several new interesting problem areas that have yet to be explored in MIR."
68,Wei Tsung Lu;Li Su,Vocal Melody Extraction with Semantic Segmentation and Audio-symbolic Domain Transfer Learning,2018,https://doi.org/10.5281/zenodo.1492467,"Wei-Tsung Lu+Institute of Information Science, Academia Sinica>TWN>facility|Unknown>Unknown>Unknown;Li Su+Institute of Information Science, Academia Sinica>TWN>facility|Unknown>Unknown>Unknown","The melody extraction problem is analogue to semantic segmentation on a time-frequency image, in which every pixel on the image is classified as a part of a melody object or not. Such an approach can benefit from a signal processing method that helps to enhance the true pitch contours on an image, and, a music language model with structural information on large-scale symbolic music data to be transfer into an audio-based model. In this paper, we propose a novel melody extraction system, using a deep convolutional neural network (DCNN) with dilated convolution as the semantic segmentation tool. The candidate pitch contours on the time-frequency image are enhanced by combining the spectrogram and cepstral-based features. Moreover, an adaptive progressive neural network is employed to transfer the semantic segmentation model in the symbolic domain to the one in the audio domain. This paper makes an attempt to bridge the semantic gaps between signal-level features and perceived melodies, and between symbolic data and audio data. Experiments show competitive accuracy of the proposed method on various datasets."
69,Michael Barone;Karim Ibrahim;Chitralekha Gupta;Ye Wang,Empirically Weighting the Importance of Decision Factors for Singing Preference,2018,https://doi.org/10.5281/zenodo.1492469,Michael Mustaine+National University of Singapore>SGP>education;Karim M. Ibrahim+National University of Singapore>SGP>education;Chitralekha Gupta+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"Although music cognition and music information retrieval have many common areas of research interest, relatively little work utilizes a combination of signal- and humancentric approaches when assessing complex cognitive phenomena. This work explores the importance of four cognitive decision-making factors (familiarity, genre preference, ease of vocal reproducibility, and overall preference) influence in the perception of ""singability"", how attractive a song is to sing. In Experiment One, we develop a model to validate and empirically determine to what degree these factors are important when evaluating its singability. Results indicate that evaluations of how these four factors impact singability strongly correlate with pairwise evaluations (ρ = 0.692, p &lt; 0.0001), supporting the notion that singability is a measurable cognitive process. Experiment Two examines the degree to which timbral and rhythmic features contribute to singability. Regression and random forest analysis find that some selected features are more significant than others. We discuss the method we use to empirically assess the complex decisions, and provide a preliminary exploration regarding what acoustic features may motivate these choices."
70,Iris Yuping Ren;Anja Volk;Wouter Swierstra;Remco Veltkamp,Analysis by Classification: A Comparative Study of Annotated and Algorithmically Extracted Patterns in Symbolic Music Data,2018,https://doi.org/10.5281/zenodo.1492471,Iris Yuping Ren+Utrecht University>NLD>education;Anja Volk+Utrecht University>NLD>education;Wouter Swierstra+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education,"Musical patterns are salient passages that repeatedly appear in music. Such passages are vital for compression, classification and prediction tasks in MIR, and algorithms employing different techniques have been proposed to find musical patterns automatically. Human-annotated patterns have been collected and used to evaluate pattern discovery algorithms, e.g., in the Discovery of Repeated Themes &amp; Sections MIREX task. However, state-of-the-art algorithms are not yet able to reproduce human-annotated patterns. To understand what gives rise to the discrepancy between algorithmically extracted patterns and human-annotated patterns, we use jSymbolic to extract features from patterns, visualise the feature space using PCA and perform a comparative analysis using classification techniques. We show that it is possible to classify algorithmically extracted patterns, human-annotated patterns and randomly sampled passages. This implies: (a) Algorithmically extracted patterns possess different properties than human-annotated patterns (b) Algorithmically extracted patterns have different structures than randomly sampled passages (c) Human-annotated patterns contain more information than randomly sampled passages despite subjectivity involved in the annotation process. We further discover that rhythmic features are of high importance in the classification process, which should influence future research on automatic pattern discovery."
71,Christoph Finkensiep;Markus Neuwirth;Martin Rohrmeier,Generalized Skipgrams for Pattern Discovery in Polyphonic Streams,2018,https://doi.org/10.5281/zenodo.1492473,Christoph Finkensiep+École Polytechnique Fédérale de Lausanne>CHE>education;Markus Neuwirth+École Polytechnique Fédérale de Lausanne>CHE>education;Martin Rohrmeier+École Polytechnique Fédérale de Lausanne>CHE>education,"The discovery of patterns using a minimal set of assumptions constitutes a central challenge in the modeling of polyphonic music and complex streams in general. Skipgrams have been found to be a powerful model for capturing semi-local dependencies in sequences of entities when dependencies may not be directly adjacent (see, for instance, the problems of modeling sequences of words or letters in computational linguistics). Since common skipgrams define locality based on indices, they can only be applied to a single stream of non-overlapping entities. This paper proposes a generalized skipgram model that allows arbitrary cost functions (defining locality), efficient filtering, recursive application (skipgrams over skipgrams), and memory efficient streaming. Further, a sampling mechanism is proposed that flexibly controls runtime and output size. These generalizations and optimizations make it possible to employ skipgrams for the discovery of repeated patterns of close, nonsimultaneous events or notes. The extensions to the skipgram model provided here do not only apply to musical notes but to any list of entities that is monotonic with respect to a given cost function."
72,Igor Vatolkin;Günter Rudolph,Comparison of Audio Features for Recognition of Western and Ethnic Instruments in Polyphonic Mixtures,2018,https://doi.org/10.5281/zenodo.1492475,Igor Vatolkin+TU Dortmund>DEU>education|TU Dortmund>DEU>education;Günter Rudolph+TU Dortmund>DEU>education|TU Dortmund>DEU>education,"Studies on instrument recognition are almost always restricted to either Western or ethnic music. Only little work has been done to compare both musical worlds. In this paper, we analyse the performance of various audio features for recognition of Western and ethnic instruments in chords. The feature selection is done with the help of a minimum redundancy - maximum relevance strategy and a multi-objective evolutionary algorithm. We compare the features found to be the best for individual categories and propose a novel strategy based on non-dominated sorting to evaluate and select trade-off features which may contribute as best as possible to the recognition of individual and all instruments."
73,Takumi Takahashi;Satoru Fukayama;Masataka Goto,Instrudive: A Music Visualization System Based on Automatically Recognized Instrumentation,2018,https://doi.org/10.5281/zenodo.1492477,Takumi Takahashi+University of Tsukuba>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"A music visualization system called Instrudive is presented that enables users to interactively browse and listen to musical pieces by focusing on instrumentation. Instrumentation is a key factor in determining musical sound characteristics. For example, a musical piece performed with vocals, electric guitar, electric bass, and drums can generally be associated with pop/rock music but not with classical or electronic. Therefore, visualizing instrumentation can help listeners browse music more efficiently. Instrudive visualizes musical pieces by illustrating instrumentation with multi-colored pie charts and displays them on a map in accordance with the similarity in instrumentation. Users can utilize three functions. First, they can browse musical pieces on a map by referring to the visualized instrumentation. Second, they can interactively edit a playlist that showing the items to be played later. Finally, they can discern the temporal changes in instrumentation and skip to a preferable part of a piece with a multi-colored graph. The instruments are identified using a deep convolutional neural network that has four convolutional layers with different filter shapes. Evaluation of the proposed model against conventional and state-of-the-art methods showed that it has the best performance."
74,Siddharth Gururani;Cameron Summers;Alexander Lerch,Instrument Activity Detection in Polyphonic Music using Deep Neural Networks,2018,https://doi.org/10.5281/zenodo.1492479,"Siddharth Gururani+Center for Music Technology, Georgia Institute of Technology>USA>education;Cameron Summers+Gracenote>USA>company;Alexander Lerch+Center for Music Technology, Georgia Institute of Technology>USA>education","Although instrument recognition has been thoroughly research, recognition in polyphonic music still faces challenges. While most research in polyphonic instrument recognition focuses on predicting the predominant instruments in a given audio recording, instrument activity detection represents a generalized problem of detecting the presence or activity of instruments in a track on a fine-grained temporal scale. We present an approach for instrument activity detection in polyphonic music with temporal resolution ranging from one second to the track level. This system allows, for instance, to retrieve specific areas of interest such as guitar solos. Three classes of deep neural networks are trained to detect up to 18 instruments. The architectures investigated in this paper are: multi-layer perceptrons, convolutional neural networks, and convolutional-recurrent neural networks. An in-depth evaluation on publicly available multi-track datasets using methods such as AUC-ROC and Label Ranking Average Precision highlights different aspects of the model performance and indicates the importance of using multiple evaluation metrics. Furthermore, we propose a new visualization to discuss instrument confusion in a multi-label scenario."
75,Juan S. Gómez;Jakob Abeßer;Estefanía Cano,"Jazz Solo Instrument Classification with Convolutional Neural Networks, Source Separation, and Transfer Learning",2018,https://doi.org/10.5281/zenodo.1492481,Juan S. Gómez+Fraunhofer IDMT>DEU>facility;Jakob Abeßer+Fraunhofer IDMT>DEU>facility;Estefanía Cano+Fraunhofer IDMT>DEU>facility,"Predominant instrument recognition in ensemble recordings remains a challenging task, particularly if closelyrelated instruments such as alto and tenor saxophone need to be distinguished. In this paper, we build upon a recentlyproposed instrument recognition algorithm based on a hybrid deep neural network: a combination of convolutional and fully connected layers for learning characteristic spectral-temporal patterns. We systematically evaluate harmonic/percussive and solo/accompaniment source separation algorithms as pre-processing steps to reduce the overlap among multiple instruments prior to the instrument recognition step. For the particular use-case of solo instrument recognition in jazz ensemble recordings, we further apply transfer learning techniques to fine-tune a previously trained instrument recognition model for classifying six jazz solo instruments. Our results indicate that both source separation as pre-processing step as well as transfer learning clearly improve recognition performance, especially for smaller subsets of highly similar instruments."
76,Katherine M. Kinnaird,Aligned Sub-Hierarchies: A Structure-based Approach to the Cover Song Task,2018,https://doi.org/10.5281/zenodo.1492483,Katherine M. Kinnaird+Brown University>USA>education,"Extending previous structure-based approaches to the song comparison tasks such as the fingerprint and cover song tasks, this paper introduces the aligned sub-hierarchies (AsH) representation. Built by applying a post-processing technique to the aligned hierarchies of a song, the AsH representation is the set of unique aligned hierarchies for repeats (called AHR) encoded in the original aligned hierarchies of the whole song. Effectively each AHR within AsH is a section of the aligned hierarchies for the original song. Like aligned hierarchies, the AsH representation can be embedded into a classification space with a natural metric that makes inter-song comparisons based on sections of the songs. Experiments addressing a version of the cover song task on score-based data using AsH as the basis of inter-song comparison demonstrate potential of AsH-based approaches for MIR tasks."
77,Andreas Arzt;Stefan Lattner,Audio-to-Score Alignment using Transposition-invariant Features,2018,https://doi.org/10.5281/zenodo.1492485,"Andreas Arzt+Institute of Computational Perception, Johannes Kepler University>AUT>education;Stefan Lattner+Sony Computer Science Laboratories (CSL)>FRA>company","Audio-to-score alignment is an important pre-processing step for in-depth analysis of classical music. In this paper, we apply novel transposition-invariant audio features to this task. These low-dimensional features represent local pitch intervals and are learned in an unsupervised fashion by a gated autoencoder. Our results show that the proposed features are indeed fully transposition-invariant and enable accurate alignments between transposed scores and performances. Furthermore, they can even outperform widely used features for audio-to-score alignment on 'untransposed data', and thus are a viable and more flexible alternative to well-established features for music alignment and matching."
78,Chitralekha Gupta;Rong Tong;Haizhou Li;Ye Wang,Semi-supervised Lyrics and Solo-singing Alignment,2018,https://doi.org/10.5281/zenodo.1492487,Chitralekha Gupta+National University of Singapore>SGP>education;Rong Tong+Alibaba Inc. Singapore R&D Center>SGP>company;Haizhou Li+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"We propose a semi-supervised algorithm to align lyrics to the corresponding singing vocals. The proposed method transcribes and aligns lyrics to solo-singing vocals using the imperfect transcripts from an automatic speech recognition (ASR) system and the published lyrics. The ASR provides time alignment between vocals and hypothesized lyrical content, while the non-aligned published lyrics correct the hypothesized lyrical content. The effectiveness of the proposed method is validated through three experiments. First, a human listening test shows that 73.32% of our automatically aligned sentence-level transcriptions are correct. Second, the automatically aligned sung segments are used for singing acoustic model adaptation, which reduces the word error rate (WER) of automatic transcription of sung lyrics from 72.08% to 37.15% in an open test. Third, another iteration of decoding and model adaptation increases the amount of reliably decoded segments from 44.40% to 91.96% and further reduces the WER to 36.32%. The proposed framework offers an automatic way to generate reliable alignments between lyrics and solosinging. A large-scale solo-singing and lyrics aligned corpus can be derived with the proposed method, which will be beneficial for music and singing voice related research."
79,Vinod Subramanian;Alexander Lerch,Concert Stitch: Organization and Synchronization of Crowd Sourced Recordings,2018,https://doi.org/10.5281/zenodo.1492489,Vinod Subramanian+Georgia Institute of Technology>USA>education;Alexander Lerch+Georgia Institute of Technology>USA>education,"The number of audience recordings of concerts on the internet has exploded with the advent of smartphones. This paper proposes a method to organize and align these recordings in order to create one or more complete renderings of the concert. The process comprises two steps: first, using audio fingerprints to represent the recordings, identify overlapping segments, and compute an approximate alignment using a modified Dynamic Time Warping (DTW) algorithm and second, applying a cross-correlation around the approximate alignment points in order to improve the accuracy of the alignment. The proposed method is compared to two baseline systems using approaches previously proposed for similar tasks. One baseline cross-correlates the audio fingerprints directly without DTW. The second baseline replaces the audio fingerprints with pitch chroma in the DTW algorithm. A new dataset annotating real-world data obtained from the Live Music Archive is presented and used for evaluation of the three systems."
80,Anna Aljanaki;Mohammad Soleymani,A Data-driven Approach to Mid-level Perceptual Musical Feature Modeling,2018,https://doi.org/10.5281/zenodo.1492491,Anna Aljanaki+Johannes Kepler University>AUT>education;Mohammad Soleymani+University of Geneva>CHE>education,"Musical features and descriptors could be coarsely divided into three levels of complexity. The bottom level contains the basic building blocks of music, e.g., chords, beats and timbre. The middle level contains concepts that emerge from combining the basic blocks: tonal and rhythmic stability, harmonic and rhythmic complexity, etc. High-level descriptors (genre, mood, expressive style) are usually modeled using the lower level ones. The features belonging to the middle level can both improve automatic recognition of high-level descriptors, and provide new music retrieval possibilities. Mid-level features are subjective and usually lack clear definitions. However, they are very important for human perception of music, and on some of them people can reach high agreement, even though defining them and therefore, designing a hand-crafted feature extractor for them can be difficult. In this paper, we derive the mid-level descriptors from data. We collect and release a dataset 1 of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation. We then compare several approaches to predicting these descriptors from spectrograms using deep-learning. We also demonstrate the usefulness of these mid-level features using music emotion recognition as an application."
81,Jimena Royo-Letelier;Romain Hennequin;Viet-Anh Tran;Manuel Moussallam,Disambiguating Music Artists at Scale with Audio Metric Learning,2018,https://doi.org/10.5281/zenodo.1492493,Jimena Royo-Letelier+Deezer>FRA>company;Romain Hennequin+Deezer>FRA>company;Viet-Anh Tran+Deezer>FRA>company;Manuel Moussallam+Deezer>FRA>company,"We address the problem of disambiguating large scale catalogs through the definition of an unknown artist clustering task. We explore the use of metric learning techniques to learn artist embeddings directly from audio, and using a dedicated homonym artists dataset, we compare our method with a recent approach that learn similar embeddings using artist classifiers. While both systems have the ability to disambiguate unknown artists relying exclusively on audio, we show that our system is more suitable in the case when enough audio data is available for each artist in the train dataset. We also propose a new negative sampling method for metric learning that takes advantage of side information such as music genre during the learning phase and shows promising results for the artist clustering task."
82,Simon Waloschek;Aristotelis Hadjakos,Driftin' Down the Scale: Dynamic Time Warping in the Presence of Pitch Drift and Transpositions,2018,https://doi.org/10.5281/zenodo.1492495,Simon Waloschek+Detmold University of Music>DEU>education;Aristotelis Hadjakos+Detmold University of Music>DEU>education,"Recordings of a cappella music often exhibit significant pitch drift. This drift may accumulate over time to a total transposition of several semitones, which renders the canonical 2-dimensional Dynamic Time Warping (DTW) useless. We propose Transposition-Aware Dynamic Time Warping (TA-DTW), an approach that introduces a 3rd dimension to DTW. Steps in this dimension represent changes in transposition. Paired with suitable input features, TA-DTW computes an optimal alignment path between a symbolic score and a corresponding audio recording in the presence of pitch drift or arbitrary transpositions."
83,Jordi Pons;Oriol Nieto;Matthew Prockup;Erik M. Schmidt;Andreas F. Ehmann;Xavier Serra,End-to-end Learning for Music Audio Tagging at Scale,2018,https://doi.org/10.5281/zenodo.1492497,"Jordi Pons+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Oriol Nieto+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Matthew Prockup+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Erik Schmidt+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Andreas Ehmann+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company;Xavier Serra+Music Technology Group, Universitat Pompeu Fabra>ESP>education|Pandora Media Inc.>USA>company","The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogrambased ones in large-scale data scenarios."
84,Romain Hennequin;Jimena Royo-Letelier;Manuel Moussallam,Audio Based Disambiguation of Music Genre Tags,2018,https://doi.org/10.5281/zenodo.1492499,Romain Hennequin+Deezer R&D>FRA>company;Jimena Royo-Letelier+Deezer R&D>FRA>company;Manuel Moussallam+Deezer R&D>FRA>company,"In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres. We show that such embeddings can be used for disambiguating genre tags (identification of different labels for the same genre, tag translation from a tag system to another, inference of hierarchical taxonomies on these genre tags). These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system. We show empirically that they makes it possible to retrieve the original taxonomy of a tag system, spot duplicates tags and translate tags from a tag system to another."
85,Yin-Jyun Luo;Li Su,Learning Domain-Adaptive Latent Representations of Music Signals Using Variational Autoencoders,2018,https://doi.org/10.5281/zenodo.1492501,"Yin-Jyun Luo+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown;Li Su+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown","In this paper, we tackle the problem of domain-adaptive representation learning for music processing. Domain adaptation is an approach aiming to eliminate the distributional discrepancy of the modeling data, so as to transfer learnable knowledge from one domain to another. With its great success in the fields of computer vision and natural language processing, domain adaptation also shows great potential in music processing, for music is essentially a highly-structured semantic system having domaindependent information. Our proposed model contains a Variational Autoencoder (VAE) that encodes the training data into a latent space, and the resulting latent representations along with its model parameters are then reused to regularize the representation learning of the downstream task where the data are in the other domain. The experiments on cross-domain music alignment, namely an audioto-MIDI alignment, and a monophonic-to-polyphonic music alignment of singing voice show that the learned representations lead to better higher alignment accuracy than that using conventional features. Furthermore, a preliminary experiment on singing voice source separation, by regarding the mixture and the voice as two distinct domains, also demonstrates the capability to solve music processing problems from the perspective of domain-adaptive representation learning."
86,Stefan Lattner;Maarten Grachten;Gerhard Widmer,Learning Interval Representations from Polyphonic Music Sequences,2018,https://doi.org/10.5281/zenodo.1492503,"Stefan Lattner+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Maarten Grachten+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company;Gerhard Widmer+Institute of Computational Perception, JKU Linz>AUT>education|Sony Computer Science Laboratories (CSL)>FRA>company","Many music theoretical constructs (such as scale types, modes, cadences, and chord types) are defined in terms of pitch intervals—relative distances between pitches. Therefore, when computer models are employed in music tasks, it can be useful to operate on interval representations rather than on the raw musical surface. Moreover, interval representations are transposition-invariant, valuable for tasks like audio alignment, cover song detection and music structure analysis. We employ a gated autoencoder to learn fixed-length, invertible and transposition-invariant interval representations from polyphonic music in the symbolic domain and in audio. An unsupervised training method is proposed yielding an organization of intervals in the representation space which is musically plausible. Based on the representations, a transposition-invariant self-similarity matrix is constructed and used to determine repeated sections in symbolic music and in audio, yielding competitive results in the MIREX task ""Discovery of Repeated Themes and Sections""."
87,Louis Spinelli;Josephine Lau;Liz Pritchard;Jin Ha Lee,Influences on the Social Practices Surrounding Commercial Music Services: A Model for Rich Interactions,2018,https://doi.org/10.5281/zenodo.1492505,Louis Spinelli+University of Washington>USA>education;Josephine Lau+University of Washington>USA>education;Liz Pritchard+University of Washington>USA>education;Jin Ha Lee+University of Washington>USA>education,"Music can play an important role in social experiences and interactions. Technologies in-use affect these experiences and interactions and as they continue to evolve, social behaviors and norms surrounding them also evolve. In this paper, we explore the social aspects of commercial music services through focus group observation and interview data. We seek to better understand how existing services are used for social music practices and can be improved. We identified 9 social practices and 24 influences surrounding commercial music services. Based on the user data, we created a model of these practices and influences that provides a lens through which social experiences surrounding commercial music services can be understood. An understanding of these social practices within their contextual ecosystem help inform what influences should be considered when designing new technologies. Our findings include the identification of: the underlying relationships between practices and their influences; practices and influences that inform the weight of relationships in social networks; social norms to be considered when designing social features; influences that add additional insight to previously observed behaviors; and a detailed explanation of how music selection and listening practices can be supported by commercial music services."
88,Christine Bauer;Markus Schedl,Investigating Cross-Country Relationship between Users' Social Ties and Music Mainstreaminess,2018,https://doi.org/10.5281/zenodo.1492507,Christine Bauer+Johannes Kepler University Linz>AUT>education;Markus Schedl+Johannes Kepler University Linz>AUT>education,"We investigate the complex relationship between the factors (i) preference for music mainstream, (ii) social ties in an online music platform, and (iii) demographics. We define (i) on a global and a country level, (ii) by several network centrality measures such as Jaccard index among users' connections, closeness centrality, and betweenness centrality, and (iii) by country and age information. Using the LFM-1b dataset of listening events of Last.fm users, we are able to uncover country-dependent differences in consumption of mainstream music as well as in user behavior with respect to social ties and users' centrality. We could identify that users inclined to mainstream music tend to have stronger connections than the group of less mainstreamy users. Furthermore, our analysis revealed that users typically have less connections within a country than cross-country ones, with the first being stronger social ties, though. Results will help building better user models of listeners and in turn improve personalized music retrieval and recommendation algorithms."
89,Kosetsu Tsukuda;Satoru Fukayama;Masataka Goto,Listener Anonymizer: Camouflaging Play Logs to Preserve User's Demographic Anonymity,2018,https://doi.org/10.5281/zenodo.1492509,Kosetsu Tsukuda+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"When a user signs up with an online music service, she is often requested to register her demographic attributes such as age, gender, and nationality. Even if she does not input such information, it has been reported that user attributes can be predicted with high accuracy by using her play log. How can users enjoy music when using an online music service while preserving their demographic anonymity? To solve this problem, we propose a system called Listener Anonymizer. Listener Anonymizer monitors the user's play log. When it detects that her confidential attributes can be predicted, it selects songs that can decrease the prediction accuracy and recommends them to her. The user can camouflage her play logs by playing these songs to preserve her demographic anonymity. Since such songs do not always match her music taste, selecting as few songs as possible that can effectively anonymize her attributes is required. Listener Anonymizer realizes this by selecting songs based on feature ablation analysis. Our experimental results using Last.fm play logs showed that Listener Anonymizer was able to preserve anonymity with fewer songs than a method that randomly selected songs."
90,Elad Liebman;Corey N. White;Peter Stone,On the Impact of Music on Decision Making in Cooperative Tasks,2018,https://doi.org/10.5281/zenodo.1492511,Elad Liebman+The University of Texas at Austin>USA>education|The University of Texas at Austin>USA>education;Corey N. White+Missouri Western State University>USA>education;Peter Stone+The University of Texas at Austin>USA>education,"Numerous studies have demonstrated that mood affects emotional and cognitive processing. Previous work has established that music-induced mood can measurably alter people's behavior in different contexts. However, the nature of how decision-making is affected by music in social settings hasn't been sufficiently explored. The goal of this study is to examine which aspects of people's decision making in inter-social tasks are affected when exposed to music. For this purpose, we devised an experiment in which people drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results indicate that music indeed alters people's behavior with respect to this social task. To further understand the correspondence between auditory features and decision making, we have also studied how individual aspects of music affected response patterns."
91,Emmanouil Krasanakis;Emmanouil Schinas;Symeon Papadopoulos;Yiannis Kompatsiaris;Pericles Mitkas,VenueRank: Identifying Venues that Contribute to Artist Popularity,2018,https://doi.org/10.5281/zenodo.1492513,Emmanouil Krasanakis+CERTH-ITI>GRC>facility;Emmanouil Schinas+CERTH-ITI>GRC>facility|Aristotle University of Thessaloniki>GRC>education;Symeon Papadopoulos+CERTH-ITI>GRC>facility;Yiannis Kompatsiaris+CERTH-ITI>GRC>facility;Pericles Mitkas+Aristotle University of Thessaloniki>GRC>education,"An important problem in the live music industry is finding venues that help expose artists to wider audiences. However, it is often difficult to obtain live music audience data to tackle this task. In this work, we investigate whether important venues can instead be inferred through social media data. Our approach consists of employing bipartite graph ranking algorithms to help discover important venues in artist-venue graphs mined from Facebook. We use both well-established algorithms, such as BiRank, and a modification of their common iterative scheme that avoids the impact of possibly erroneous heuristics to the ranking, which we call VenueRank. Resulting venue ranks are compared to those obtained from feature extraction for predicting the most listened artists and large listener increments in Spotify. This comparison yields high correlation between venue importance for listener prediction and bipartite graph ranking algorithms, with VenueRank found more robust against overfitting."
92,Eva Zangerle;Martin Pichl,The Many Faces of Users: Modeling Musical Preference,2018,https://doi.org/10.5281/zenodo.1492515,Eva Zangerle+Universität Innsbruck>AUT>education;Martin Pichl+Universität Innsbruck>AUT>education,"User models that capture the musical preferences of users are central for many tasks in music information retrieval and music recommendation, yet, it has not been fully explored and exploited. To this end, the musical preferences of users in the context of music recommender systems have mostly been captured in collaborative filtering-based approaches. Alternatively, users can be characterized by their average listening behavior and hence, by the mean values of a set of content descriptors of tracks the users listened to. However, a user may listen to highly different tracks and genres. Thus, computing the average of all tracks does not capture the user's listening behavior well. We argue that each user may have many different preferences that depend on contextual aspects (e.g., listening to classical music when working and hard rock when doing sports) and that user models should account for these different sets of preferences. In this paper, we provide a detailed analysis and evaluation of different user models that describe a user's musical preferences based on acoustic features of tracks the user has listened to."
93,Jiyoung Park;Jongpil Lee;Jangyeon Park;Jung-Woo Ha;Juhan Nam,Representation Learning of Music Using Artist Labels,2018,https://doi.org/10.5281/zenodo.1492517,Jiyoung Park+NAVER Corp.>KOR>company;Jongpil Lee+KAIST>KOR>education;Jangyeon Park+NAVER Corp.>KOR>company;Jung-Woo Ha+NAVER Corp.>KOR>company;Juhan Nam+KAIST>KOR>education,"In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models."
94,Gabriele Medeot;Srikanth Cherla;Katerina Kosta;Matt McVicar;Samer Abdallah;Marco Selvi;Ed Newton-Rex;Kevin Webster,StructureNet: Inducing Structure in Generated Melodies,2018,https://doi.org/10.5281/zenodo.1492519,Gabriele Medeot+Jukedeck Ltd.>GBR>company;Srikanth Cherla+Jukedeck Ltd.>GBR>company;Katerina Kosta+Jukedeck Ltd.>GBR>company;Matt McVicar+Jukedeck Ltd.>GBR>company;Samer Abdallah+Jukedeck Ltd.>GBR>company;Marco Selvi+Jukedeck Ltd.>GBR>company;Ed Newton-Rex+Jukedeck Ltd.>GBR>company;Kevin Webster+Imperial College London>GBR>education,"We present the StructureNet - a recurrent neural network for inducing structure in machine-generated compositions. This model resides in a musical structure space and works in tandem with a probabilistic music generation model as a modifying agent. It favourably biases the probabilities of those notes that result in the occurrence of structural elements it has learnt from a dataset. It is extremely flexible in that it is able to work with any such probabilistic model, it works well when training data is limited, and the types of structure it can be made to induce are highly customisable. We demonstrate through our experiments on a subset of the Nottingham dataset that melodies generated by a recurrent neural network based melody model are indeed more structured in the presence of the StructureNet."
95,Diego Furtado Silva;Felipe Falcão;Nazareno Andrade,Summarizing and Comparing Music Data and Its Application on Cover Song Identification,2018,https://doi.org/10.5281/zenodo.1492521,Diego Furtado Silva+Universidade Federal de São Carlos>BRA>education;Felipe Vieira Falcão+Universidade Federal de Campina Grande>BRA>education;Nazareno Andrade+Universidade Federal de Campina Grande>BRA>education,"While there is a multitude of music information retrieval algorithms that have distance functions as their core procedure, comparing the similarity between recordings is a costly procedure. At the same, the recent growth of digital music repositories makes necessary the development of novel time- and memory-efficient algorithms to deal with music data. One particularly interesting idea on the literature is transforming the music data into reduced representations, improving the memory usage and reducing the time necessary to assess the similarity. However, these techniques usually add other issues, such as an expensive preprocessing or a reduced retrieval performance. In this paper, we propose a novel method to summarize a recording in small snippets based on its self-similarity information. Besides, we present a simple way to compare other recordings to these summaries. We demonstrate, in the scenario of cover song identification, that our method is more than one order of magnitude faster than state-of-the-art adversaries, at the same time that the retrieval performance is not affected significantly. Additionally, our method is incremental, which allows the easy and fast update of the database when a new song needs to be inserted into the retrieval system."
96,Wei Tsung Lu;Li Su,Transferring the Style of Homophonic Music Using Recurrent Neural Networks and Autoregressive Model,2018,https://doi.org/10.5281/zenodo.1492523,"Wei-Tsung Lu+Institute of Information Science, Academia Sinica>TWN>education|Unknown>Unknown>Unknown;Li Su+Institute of Information Science, Academia Sinica>TWN>education","Utilizing deep learning techniques to generate musical contents has caught wide attention in recent years. Within this context, this paper investigates a specific problem related to music generation, music style transfer. This practical problem aims to alter the style of a given music piece from one to another while preserving the essence of that piece, such as melody and chord progression. In particular, we discuss the style transfer of homophonic music, composed of a predominant melody part and an accompaniment part, where the latter is modified through Gibbs sampling on a generative model combining recurrent neural networks and autoregressive models. Both objective and subjective test experiment are performed to assess the performance of transferring the style of an arbitrary music piece having a homophonic texture into two different distinct styles, Bachs chorales and Jazz."
97,Gino Brunner;Andres Konrad;Yuyi Wang;Roger Wattenhofer,MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer,2018,https://doi.org/10.5281/zenodo.1492525,Gino Brunner+ETH Zurich>CHE>education;Andres Konrad+ETH Zurich>CHE>education;Yuyi Wang+ETH Zurich>CHE>education;Roger Wattenhofer+ETH Zurich>CHE>education,"We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions."
98,Saumitra Mishra;Bob L. Sturm;Simon Dixon,Understanding a Deep Machine Listening Model Through Feature Inversion,2018,https://doi.org/10.5281/zenodo.1492527,Saumitra Mishra+Queen Mary University of London>GBR>education;Bob L. Sturm+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"Methods for interpreting machine learning models can help one understand their global and/or local behaviours, and thereby improve them. In this work, we apply a global analysis method to a machine listening model, which essentially inverts the features generated in a model back into an interpretable form like a sonogram. We demonstrate this method for a state-of-the-art singing voice detection model. We train up-convolutional neural networks to invert the feature generated at each layer of the model. The results suggest that the deepest fully connected layer of the model does not preserve temporal and harmonic structures, but that the inverted features from the deepest convolutional layer do. Moreover, a qualitative analysis of a large number of inputs suggests that the deepest layer in the model learns a decision function as the information it preserves depends on the class label associated with an input."
99,Tian Cheng;Satoru Fukayama;Masataka Goto,Comparing RNN Parameters for Melodic Similarity,2018,https://doi.org/10.5281/zenodo.1492529,Tian Cheng+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Satoru Fukayama+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"Melodic similarity is an important task in the Music Information Retrieval (MIR) domain, with promising applications including query by example, music recommendation and visualisation. Most current approaches compute the similarity between two melodic sequences by comparing their local features (distance between pitches, intervals, etc.) or by comparing the sequences after aligning them. In order to find a better feature representing global characteristics of a melody, we propose to represent the melodic sequence of each musical piece by the parameters of a generative Recurrent Neural Network (RNN) trained on its sequence. Because the trained RNN can generate the identical melodic sequence of each piece, we can expect that the RNN parameters contain the temporal information within the melody. In our experiment, we first train an RNN on all melodic sequences, and then use it as an initialisation to train an individual RNN on each melodic sequence. The similarity between two melodies is computed by using the distance between their individual RNN parameters. Experimental results showed that the proposed RNN-based similarity outperformed the baseline similarity obtained by directly comparing melodic sequences."
100,Mathieu Lagrange;Mathias Rossignol;Grégoire Lafay,Visualization of Audio Data Using Stacked Graphs,2018,https://doi.org/10.5281/zenodo.1492531,"Mathieu Lagrange+LS2N, CNRS, École Centrale de Nantes>FRA>education|LS2N, CNRS>FRA>facility;Mathias Rossignol+LS2N, CNRS, École Centrale de Nantes>FRA>education|LS2N, CNRS>FRA>facility;Grégoire Lafay+LS2N, CNRS, École Centrale de Nantes>FRA>education|LS2N, CNRS>FRA>facility","In this paper, we study the benefit of considering stacked graphs to display audio data. Thanks to a careful use of layering of the spectral information, the resulting display is both concise and intuitive. Compared to the spectrogram display, it allows the reader to focus more on the temporal aspect of the time/frequency decomposition while keeping an abstract view of the spectral information. The use of such a display is validated using two perceptual experiments that demonstrate the potential of the approach. The first considers the proposed display to perform an identification task of the musical instrument and the second considers the proposed display to evaluate the technical level of a musical performer. Both experiments show the potential of the display and potential applications scenarios in musical training are discussed."
101,Klaus Frieler;Frank Höger;Martin Pfleiderer;Simon Dixon,Two Web Applications for Exploring Melodic Patterns in Jazz Solos,2018,https://doi.org/10.5281/zenodo.1492533,Klaus Frieler+University of Music “Franz Liszt” Weimar>DEU>education;Frank Höger+University of Music “Franz Liszt” Weimar>DEU>education;Martin Pfliederer+University of Music “Franz Liszt” Weimar>DEU>education;Simon Dixon+Queen Mary University of London>GBR>education,"This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project ""Dig That Lick"" is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The first one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans."
102,Matthias Dorfer;Florian Henkel;Gerhard Widmer,"Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game",2018,https://doi.org/10.5281/zenodo.1492535,"Matthias Dorfer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Florian Henkel+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Institute of Computational Perception, Johannes Kepler University Linz>AUT>education|The Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility","Score following is the process of tracking a musical performance (audio) with respect to a known symbolic representation (a score). We start this paper by formulating score following as a multimodal Markov Decision Process, the mathematical foundation for sequential decision making. Given this formal definition, we address the score following task with state-of-the-art deep reinforcement learning (RL) algorithms such as synchronous advantage actor critic (A2C). In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. All this behavior is learned entirely from scratch, based on a weak and potentially delayed reward signal that indicates to the agent how close it is to the correct position in the score. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images."
103,Olivier Gouvert;Thomas Oberlin;Cédric Févotte,Matrix Co-Factorization for Cold-Start Recommendation,2018,https://doi.org/10.5281/zenodo.1492537,"Olivier Gouvert+IRIT, Université de Toulouse>FRA>education|CNRS>FRA>facility;Thomas Oberlin+IRIT, Université de Toulouse>FRA>education|CNRS>FRA>facility;Cédric Févotte+IRIT, Université de Toulouse>FRA>education|CNRS>FRA>facility","Song recommendation from listening counts is now a classical problem, addressed by different kinds of collaborative filtering (CF) techniques. Among them, Poisson matrix factorization (PMF) has raised a lot of interest, since it seems well-suited to the implicit data provided by listening counts. Additionally, it has proven to achieve state-ofthe-art performance while being scalable to big data. Yet, CF suffers from a critical issue, usually called cold-start problem: the system cannot recommend new songs, i.e., songs which have never been listened to. To alleviate this, one should complement the listening counts with another modality. This paper proposes a multi-modal extension of PMF applied to listening counts and tag labels extracted from the Million Song Dataset. In our model, every song is represented by the same activation pattern in each modality but with possibly different scales. As such, the method is not prone to the cold-start problem, i.e., it can learn from a single modality when the other one is not informative. Our model is symmetric (it equally uses both modalities) and we evaluate it on two tasks: new songs recommendation and tag labeling."
