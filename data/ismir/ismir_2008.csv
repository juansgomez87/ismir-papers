Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Ricardo Scholz;Geber Ramalho,COCHONUT: Recognizing Complex Chords from MIDI Guitar Sequences.,2008,https://doi.org/10.5281/zenodo.1416986,Ricardo Scholz+Federal University of Pernambuco>BRA>education;Geber Ramalho+Federal University of Pernambuco>BRA>education,"""Chord recognition from symbolic data is a complex task, due to its strong context dependency and the large number of possible combinations of the intervals which the chords are made of, specially when dealing with dissonances, such as 7ths, 9ths, 13ths and suspended chords. None of the current approaches deal with such complexity. Most of them consider only simple chord patterns, in the best cases, including sevenths. In addition, when considering symbolic data captured from a MIDI guitar, we need to deal with non quantized and noisy data, which increases the difficulty of the task. The current symbolic approaches deal only with quantized data, with no automatic technique to reduce noise. This paper proposes a new approach to recognize chords, from symbolic MIDI guitar data, called COCHONUT (Complex Chords Nutting). The system uses contextual harmonic information to solve ambiguous cases, integrated with other techniques, such as decision theory, optimization, pattern matching and rule-based recognition. The results are encouraging and provide strong indications that the use of harmonic contextual information, integrated with other techniques, can actually improve the results currently found in literature."""
1,Xinglin Zhang;David Gerhard,Chord Recognition using Instrument Voicing Constraints.,2008,https://doi.org/10.5281/zenodo.1414826,Xinglin Zhang+University of Regina>CAN>education;David Gerhard+University of Regina>CAN>education,"This paper presents a technique of disambiguation for chord recognition based on a-priori knowledge of probabilities of chord voicings in the specific musical medium. The main motivating example is guitar chord recognition, where the physical layout and structure of the instrument, along with human physical and temporal constraints, make certain chord voicings and chord sequences more likely than others. Pitch classes are first extracted using the Pitch Class Profile (PCP) technique, and chords are then recognized using Artificial Neural Networks. The chord information is then analyzed using an array of voicing vectors (VV) indicating likelihood for chord voicings based on constraints of the instrument. Chord sequence analysis is used to reinforce accuracy of individual chord estimations. The specific notes of the chord are then inferred by combining the chord information and the best estimated voicing of the chord."
2,Kouhei Sumi;Katsutoshi Itoyama;Kazuyoshi Yoshii;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Automatic Chord Recognition Based on Probabilistic Integration of Chord Transition and Bass Pitch Estimation.,2008,https://doi.org/10.5281/zenodo.1417221,Kouhei Sumi+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Katsutoshi Itoyama+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazunori Komatani+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Tetsuya Ogata+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Hiroshi G. Okuno+Kyoto University>JPN>education|National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a method that identifies musical chords in polyphonic musical signals. As musical chords mainly represent the harmony of music and are related to other musical elements such as melody and rhythm, the performance of chord recognition should improve if this interrelationship is taken into consideration. Nevertheless, this interrelationship has not been utilized in the literature as far as the authors are aware. In this paper, bass lines are utilized as clues for improving chord recognition because they can be regarded as an element of the melody. A probabilistic framework is devised to uniformly integrate bass lines extracted by using bass pitch estimation into a hypothesis-search-based chord recognition. To prune the hypothesis space of the search, the hypothesis reliability is defined as the weighted sum of three reliabilities: the likelihood of Gaussian Mixture Models for the observed features, the joint probability of chord and bass pitch, and the chord transition N-gram probability. Experimental results show that our method recognized the chord sequences of 150 songs in twelve Beatles albums; the average frame-rate accuracy of the results was 73.4%."
3,Matthias Mauch;Simon Dixon,A Discrete Mixture Model for Chord Labelling.,2008,https://doi.org/10.5281/zenodo.1416982,"Matthias Mauch+Queen Mary, University of London>GBR>education;Simon Dixon+Queen Mary, University of London>GBR>education","Chord labels for recorded audio are in high demand both as an end product used by musicologists and hobby musicians and as an input feature for music similarity applications. Many past algorithms for chord labelling are based on chromagrams, but distribution of energy in chroma frames is not well understood. Furthermore, non-chord notes complicate chord estimation. We present a new approach which uses as a basis a relatively simple chroma model to represent short-time sonorities derived from melody range and bass range chromagrams. A chord is then modelled as a mixture of these sonorities, or subchords. We prove the practicability of the model by implementing a hidden Markov model (HMM) for chord labelling, in which we use the discrete subchord features as observations. We model gamma-distributed chord durations by duplicate states in the HMM, a technique that had not been applied to chord labelling. We test the algorithm by five-fold cross-validation on a set of 175 hand-labelled songs performed by the Beatles. Accuracy figures compare very well with other state of the art approaches. We include accuracy specified by chord type as well as a measure of temporal coherence."
4,W. Bas de Haas;Remco C. Veltkamp;Frans Wiering,Tonal Pitch Step Distance: a Similarity Measure for Chord Progressions.,2008,https://doi.org/10.5281/zenodo.1418069,W. Bas de Haas+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education,"The computational analysis of musical harmony has received a lot of attention the last decades. Although it is widely recognized that extracting symbolic chord labels from music yields useful abstractions, and the number of chord labeling algorithms for symbolic and audio data is steadily growing, surprisingly little effort has been put into comparing sequences of chord labels. This study presents and tests a new distance function that measures the difference between chord progressions. The presented distance function is based on Lerdahl’s Tonal Pitch Space. It compares the harmonic changes of two sequences of chord labels over time. This distance, named the Tonal Pitch Step Distance (TPSD), is shown to be effective for retrieving similar jazz standards found in the Real Book. The TPSD matches the human intuitions about harmonic similarity which is demonstrated on a set of blues variations."
5,Ching-Hua Chuan;Elaine Chew,Evaluating and Visualizing Effectiveness of Style Emulation in Musical Accompaniment.,2008,https://doi.org/10.5281/zenodo.1417095,Ching-Hua Chuan+University of Southern California>USA>education|Radcliffe Institute for Advanced Study at Harvard University>USA>facility;Elaine Chew+University of Southern California>USA>education|Radcliffe Institute for Advanced Study at Harvard University>USA>facility,"We propose general quantitative methods for evaluating and visualizing the results of machine-generated style-specific accompaniment. The evaluation of automated accompaniment systems, and the degree to which they emulate a style, has been based primarily on subjective opinion. To quantify style similarity between machine-generated and original accompaniments, we propose two types of measures: one based on transformations in the neo-Riemannian chord space, and another based on the distribution of melody-chord intervals. The first set of experiments demonstrate the methods on an automatic style-specific accompaniment (ASSA) system. They test the effect of training data choice on style emulation effectiveness, and challenge the assumption that more data is better. The second set of experiments compare the output of the ASSA system with those of a rule-based system, and random chord generator. While the examples focus primarily on machine emulation of Pop/Rock accompaniment, the methods generalize to music of other genres."
6,Amelie Anglade;Simon Dixon,Characterisation of Harmony With Inductive Logic Programming.,2008,https://doi.org/10.5281/zenodo.1416550,Amélie Anglade+Queen Mary University of London>GBR>education;Simon Dixon+Queen Mary University of London>GBR>education,"We present an approach for the automatic characterisation of the harmony of song sets making use of relational induction of logical rules. We analyse manually annotated chord data available in RDF and interlinked with web identifiers for chords which themselves give access to the root, bass, component intervals of the chords. We pre-process these data to obtain high-level information such as chord category, degree and intervals between chords before passing them to an Inductive Logic Programming software which extracts the harmony rules underlying them. This framework is tested over the Beatles songs and the Real Book songs. It generates a total over several experiments of 12,450 harmony rules characterising and differentiating the Real Book (jazz) songs and the Beatles’ (pop) music. Encouragingly, a preliminary analysis of the most common rules reveals a list of well-known pop and jazz patterns that could be completed by a more in depth analysis of the other rules."
7,Mathieu Bergeron;Darrell Conklin,Structured Polyphonic Patterns.,2008,https://doi.org/10.5281/zenodo.1415638,Mathieu Bergeron+City University London>GBR>education;Darrell Conklin+City University London>GBR>education,"This paper presents a new approach to polyphonic music retrieval, based on a structured pattern representation. Polyphonic patterns are formed by joining and layering pattern components into sequences and simultaneities. Pattern components are conjunctions of features which encode event properties or relations with other events. Relations between events that overlap in time but are not simultaneous are supported, enabling patterns to express many of the temporal relations encountered in polyphonic music. The approach also provides a mechanism for defining new features. It is illustrated and evaluated by querying for three musicological patterns in a corpus of 185 chorale harmonizations by J.S.Bach."
8,James B. Maxwell;Arne Eigenfeldt,A Music Database and Query System for Recombinant Composition.,2008,https://doi.org/10.5281/zenodo.1415794,James B. Maxwell+Simon Fraser University>CAN>education;Arne Eigenfeldt+Simon Fraser University>CAN>education,"""We propose a design and implementation for a music information database and query system, the MusicDB, which can be used for Music Information Retrieval (MIR). The MusicDB is implemented as a Java package, and is loaded in MaxMSP using the mxj external. The MusicDB contains a music analysis module, capable of extracting musical information from standard MIDI files, and a search engine. The search engine accepts queries in the form of a simple six-part syntax, and can return a variety of different types of musical information, drawing on the encoded knowledge of musical form stored in the database."""
9,Dimitrios Rafailidis;Alexandros Nanopoulos;Yannis Manolopoulos;Emilios Cambouropoulos,Detection of Stream Segments in Symbolic Musical Data.,2008,https://doi.org/10.5281/zenodo.1416002,Dimitris Rafailidis+Aristotle University of Thessaloniki>GRC>education;Alexandros Nanopoulos+Aristotle University of Thessaloniki>GRC>education;Emilios Cambouropoulos+Aristotle University of Thessaloniki>GRC>education;Yannis Manolopoulos+Aristotle University of Thessaloniki>GRC>education,"A listener is thought to be able to organise musical notes into groups within musical streams/voices. A stream segment is a relatively short coherent sequence of tones that is separated horizontally from co-sounding streams and, vertically from neighbouring musical sequences. This paper presents a novel algorithm that discovers musical stream segments in symbolic musical data. The proposed algorithm makes use of a single set of fundamental auditory principles for the concurrent horizontal and vertical segregation of a given musical texture into stream segments. The algorithm is tested against a small manually-annotated dataset of musical excerpts, and results are analysed; it is shown that the technique is promising."
10,Marcus Pearce;Daniel Müllensiefen;Geraint A. Wiggins,A Comparison of Statistical and Rule-Based Models of Melodic Segmentation.,2008,https://doi.org/10.5281/zenodo.1417115,"M. T. Pearce+Goldsmiths, University of London>GBR>education;D. Müllensiefen+Goldsmiths, University of London>GBR>education;G. A. Wiggins+Goldsmiths, University of London>GBR>education","""We introduce a new model for melodic segmentation based on information-dynamic analysis of melodic structure. The performance of the model is compared to several existing algorithms in predicting the annotated phrase boundaries in a large corpus of folk music."""
11,Michael Skalak;Jinyu Han;Bryan Pardo,Speeding Melody Search With Vantage Point Trees.,2008,https://doi.org/10.5281/zenodo.1415714,Michael Skalak+Northwestern University>USA>education;Jinyu Han+Northwestern University>USA>education;Bryan Pardo+Northwestern University>USA>education,"Melodic search engines let people find music in online collections by specifying the desired melody. Comparing the query melody to every item in a large database is prohibitively slow. If melodies can be placed in a metric space, search can be sped by comparing the query to a limited number of vantage melodies, rather than the entire database. We describe a simple melody metric that is customizable using a small number of example queries. This metric allows use of a generalized vantage point tree to organize the database. We show on a standard melodic database that the general vantage tree approach achieves superior search results for query-by-humming compared to an existing vantage point tree method. We then show this method can be used as a preprocessor to speed search for non-metric melodic comparison."
12,Anja Volk;Peter van Kranenburg;Jörg Garbers;Frans Wiering;Remco C. Veltkamp;Louis P. Grijp,A Manual Annotation Method for Melodic Similarity and the Study of Melody Feature Sets.,2008,https://doi.org/10.5281/zenodo.1416428,Anja Volk+Utrecht University>NLD>education;Peter van Kranenburg+Utrecht University>NLD>education;Jörg Garbers+Utrecht University>NLD>education;Frans Wiering+Utrecht University>NLD>education;Remco C. Veltkamp+Utrecht University>NLD>education|Meertens Institute>NLD>facility;Louis P. Grijp+Meertens Institute>NLD>facility,This paper describes both a newly developed method for manual annotation for aspects of melodic similarity and its use for evaluating melody features concerning their contribution to perceived similarity. The second issue is also addressed with a computational evaluation method. These approaches are applied to a corpus of folk song melodies. We show that classification of melodies could not be based on single features and that the feature sets from the literature are not sufficient to classify melodies into groups of related melodies. The manual annotations enable us to evaluate various models for melodic similarity.
13,Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo,Melody Expectation Method Based on GTTM and TPS.,2008,https://doi.org/10.5281/zenodo.1416822,Masatoshi Hamanaka+University of Tsukuba>JPN>education;Keiji Hirata+NTT Communication Science Laboratories>JPN>company;Satoshi Tojo+Japan Advanced Institute of Science and Technology>JPN>education,"A method that predicts the next notes is described for assisting musical novices to play improvisations. Melody prediction is one of the most difficult problems in musical information retrieval because composers and players may or may not create melodies that conform to our expectation. The development of a melody expectation method is thus important for building a system that supports musical novices because melody expectation is one of the most basic skills for a musician. Unlike most previous prediction methods, which use statistical learning, our method evaluates the appropriateness of each candidate note from the view point of musical theory. In particular, it uses the concept of melody stability based on the generative theory of tonal music (GTTM) and the tonal pitch space (TPS) to evaluate the appropriateness of the melody. It can thus predict the candidate next notes not only from the surface structure of the melody but also from the deeper structure of the melody acquired by GTTM and TPS analysis. Experimental results showed that the method can evaluate the appropriateness of the melody sufficiently well."
14,Wei-Ho Tsai;Shih-Jie Liao;Catherine Lai,Automatic Identification of Simultaneous Singers in Duet Recordings.,2008,https://doi.org/10.5281/zenodo.1416358,Wei-Ho Tsai+National Taipei University of Technology>TWN>education;Shih-Jie Liao+National Taipei University of Technology>TWN>education;Catherine Lai+Open Text Corporation>CAN>company,"The problem of identifying singers in music recordings has received considerable attention with the explosive growth of the Internet and digital media. Although a number of studies on automatic singer identification from acoustic features have been reported, most systems to date, however, reliably establish the identity of singers in solo recordings only. The research presented in this paper attempts to automatically identify singers in music recordings that contain overlapping singing voices. Two approaches to overlapping singer identification are proposed and evaluated. Results obtained demonstrate the feasibility of the systems."
15,Ling Feng;Andreas Brinch Nielsen;Lars Kai Hansen,Vocal Segment Classification in Popular Music.,2008,https://doi.org/10.5281/zenodo.1416932,Ling Feng+Technical University of Denmark>DNK>education;Andreas Brinch Nielsen+Technical University of Denmark>DNK>education;Lars Kai Hansen+Technical University of Denmark>DNK>education,"This paper explores the vocal and non-vocal music classification problem within popular songs. A newly built labeled database covering 147 popular songs is announced. It is designed for classifying signals from 1sec time windows. Features are selected for this particular task, in order to capture both the temporal correlations and the dependencies among the feature dimensions. We systematically study the performance of a set of classifiers, including linear regression, generalized linear model, Gaussian mixture model, reduced kernel orthonormalized partial least squares and K-means on cross-validated training and test setup. The database is divided in two different ways: with/without artist overlap between training and test sets, so as to study the so called ‘artist effect’. The performance and results are analyzed in depth: from error rates to sample-to-sample error correlation. A voting scheme is proposed to enhance the performance under certain conditions."
16,David Little;Bryan Pardo,Learning Musical Instruments from Mixtures of Audio with Weak Labels.,2008,https://doi.org/10.5281/zenodo.1417815,David Little+Northwestern University>USA>education|Unknown>Unknown>Unknown;Bryan Pardo+Northwestern University>USA>education|Unknown>Unknown>Unknown,"""We are interested in developing a system that learns to recognize individual sound sources in an auditory scene where multiple sources may be occurring simultaneously. We focus here on sound source recognition in music audio mixtures. Many researchers have made progress by using isolated training examples or very strongly labeled training data. We consider an alternative approach: the learner is presented with a variety of weakly-labeled mixtures. Positive examples include the target instrument at some point in a mixture of sounds, and negative examples are mixtures that do not contain the target. We show that it not only possible to learn from weakly-labeled mixtures of instruments, but that it works significantly better (78% correct labeling compared to 55%) than learning from isolated examples when the task is identification of an instrument in novel mixtures."""
17,Katsutoshi Itoyama;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Instrument Equalizer for Query-by-Example Retrieval: Improving Sound Source Separation Based on Integrated Harmonic and Inharmonic Models.,2008,https://doi.org/10.5281/zenodo.1417407,"Katsutoshi Itoyama+Graduate School of Infomatics, Kyoto University>JPN>education;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Kazunori Komatani+Graduate School of Infomatics, Kyoto University>JPN>education;Tetsuya Ogata+Graduate School of Infomatics, Kyoto University>JPN>education;Hiroshi G. Okuno+Graduate School of Infomatics, Kyoto University>JPN>education","This paper describes a music remixing interface, called Instrument Equalizer, that allows users to control the volume of each instrument part within existing audio recordings in real time. Although query-by-example retrieval systems need a user to prepare favorite examples (songs) in general, our interface gives a user to generate examples from existing ones by cutting or boosting some instrument/vocal parts, resulting in a variety of retrieved results. To change the volume, all instrument parts are separated from the input sound mixture using the corresponding standard MIDI file. For the separation, we used an integrated tone (timbre) model consisting of harmonic and inharmonic models that are initialized with template sounds recorded from a MIDI sound generator. The remaining but critical problem here is to deal with various performance styles and instrument bodies that are not given in the template sounds. To solve this problem, we train probabilistic distributions of timbre features by using various sounds. By adding a new constraint of maximizing the likelihood of timbre features extracted from each tone model, we succeeded in estimating model parameters that better express actual timbre."
18,Nobutaka Ono;Kenichi Miyamoto;Hirokazu Kameoka;Shigeki Sagayama,A Real-time Equalizer of Harmonic and Percussive Components in Music Signals.,2008,https://doi.org/10.5281/zenodo.1415044,Nobutaka Ono+The University of Tokyo>JPN>education;Kenichi Miyamoto+The University of Tokyo>JPN>education;Hirokazu Kameoka+The University of Tokyo>JPN>education;Shigeki Sagayama+The University of Tokyo>JPN>education,"""In this paper, we present a real-time equalizer to control a volume balance of harmonic and percussive components in music signals without a priori knowledge of scores or included instruments. The harmonic and percussive components of music signals have much different structures in the power spectrogram domain, the former is horizontal, while the latter is vertical. Exploiting the anisotropy, our methods separate input music signals into them based on the MAP estimation framework. We derive two kind of algorithm based on a I-divergence-based mixing model and a hard mixing model. Although they include iterative update equations, we realized the real-time processing by a sliding analysis technique. The separated harmonic and percussive components are finally remixed in an arbitrary volume balance and played. We show the prototype system implemented on Windows environment."""
19,Ian Knopke,The PerlHumdrum and PerlLilypond Toolkits for Symbolic Music Information Retrieval.,2008,https://doi.org/10.5281/zenodo.1416894,Ian Knopke+Goldsmiths Digital Studios>GBR>facility,"PerlHumdrum is an alternative toolkit for working with large numbers of Humdrum scores. While based on the original Humdrum toolkit, it is a completely new, self-contained implementation that can serve as a replacement, and may be a better choice for some computing systems. PerlHumdrum is fully object-oriented, is designed to easily facilitate analysis and processing of multiple humdrum files, and to answer common musicological questions across entire sets, collections of music, or even the entire output of single or multiple composers. Several extended capabilities that are not available in the original toolkit are also provided, such as translation of MIDI scores to Humdrum, provisions for constructing graphs, a graphical user interface for non-programmers, and the ability to generate complete scores or partial musical examples as standard musical notation using PerlLilypond. These tools are intended primarily for use by music theorists, computational musicologists, and Music Information Retrieval (MIR) researchers."
20,Rebecca Fiebrink;Ge Wang 0002;Perry R. Cook,Support for MIR Prototyping and Real-Time Applications in the ChucK Programming Language.,2008,https://doi.org/10.5281/zenodo.1415268,Rebecca Fiebrink+Princeton University>USA>education;Ge Wang+Stanford University>USA>education;Perry Cook+Princeton University>USA>education,"In this paper, we discuss our recent additions of audio analysis and machine learning infrastructure to the ChucK music programming language, wherein we provide a complementary system prototyping framework for MIR researchers and lower the barriers to applying many MIR algorithms in live music performance. The new language capabilities preserve ChucK’s breadth of control—from high-level control using building block components to sample-level manipulation—and on-the-fly reprogrammability, allowing the programmer to experiment with new features, signal processing techniques, and learning algorithms with ease and flexibility. Furthermore, our additions integrate tightly with ChucK’s synthesis system, allowing the programmer to apply the results of analysis and learning to drive real-time music creation and interaction within a single framework. In this paper, we motivate and describe our recent additions to the language, outline a ChucK-based approach to rapid MIR prototyping, present three case studies in which we have applied ChucK to audio analysis and MIR tasks, and introduce our new toolkit to facilitate experimentation with analysis and learning in the language."
21,Terence Magno;Carl Sable,"A Comparison of Signal Based Music Recommendation to Genre Labels, Collaborative Filtering, Musicological Analysis, Human Recommendation and Random Baseline.",2008,https://doi.org/10.5281/zenodo.1418139,Terence Magno+Cooper Union>USA>education|Cooper Union>USA>education;Carl Sable+Cooper Union>USA>education|Cooper Union>USA>education,"The emergence of the Internet as today’s primary medium of music distribution has brought about demands for fast and reliable ways to organize, access, and discover music online. To date, many applications designed to perform such tasks have risen to popularity; each relies on a specific form of music metadata to help consumers discover songs and artists that appeal to their tastes. Very few of these applications, however, analyze the signal waveforms of songs directly. This low-level representation can provide dimensions of information that are inaccessible by metadata alone. To address this issue, we have implemented signal-based measures of musical similarity that have been optimized based on their correlations with human judgments. Furthermore, multiple recommendation engines relying on these measures have been implemented. These systems recommend songs to volunteers based on other songs they find appealing. Blind experiments have been conducted in which volunteers rate the systems’ recommendations along with recommendations of leading online music discovery tools (Allmusic which uses genre labels, Pandora which uses musicological analysis, and Last.fm which uses collaborative filtering), random baseline recommendations, and personal recommendations by the first author. This paper shows that the signal-based engines perform about as well as popular, commercial, state-of-the-art systems."
22,Yvonne Moh;Joachim M. Buhmann,Kernel Expansion for Online Preference Tracking.,2008,https://doi.org/10.5281/zenodo.1415962,Yvonne Moh+Swiss Federal Institute of Technology (ETH) Zurich>CHE>education|Institute of Computational Science>CHE>education;Joachim M. Buhmann+Swiss Federal Institute of Technology (ETH) Zurich>CHE>education|Institute of Computational Science>CHE>education,"User preferences of music genres can significantly change over time depending on fashions and the personal situation of music consumers. We propose a model to learn user preferences and their changes in an adaptive way. Our approach refines a model for user preferences by explicitly considering two plausible constraints of computational costs and limited storage space. The model is required to adapt itself to changing data distributions, and yet be able to compress “historical” data. We exploit the success of kernel SVM, and we consider an online expansion of the induced space as a preprocessing step to a simple linear online learner that updates with maximal agreement to previously seen data."
23,Arthur Flexer;Dominik Schnitzer;Martin Gasser;Gerhard Widmer,Playlist Generation using Start and End Songs.,2008,https://doi.org/10.5281/zenodo.1418273,Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Dominik Schnitzer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education;Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|Johannes Kepler University>AUT>education,A new algorithm for automatic generation of playlists with an inherent sequential order is presented. Based on a start and end song it creates a smooth transition allowing users to discover new songs in a music collection. The approach is based on audio similarity and does not require any kind of meta data. It is evaluated using both objective genre labels and subjective listening tests. Our approach allows users of the website of a public radio station to create their own digital “mixtapes” online.
24,Markus Dopler;Markus Schedl;Tim Pohle;Peter Knees,Accessing Music Collections Via Representative Cluster Prototypes in a Hierarchical Organization Scheme.,2008,https://doi.org/10.5281/zenodo.1417305,Markus Dopler+Johannes Kepler University>AUT>education;Markus Schedl+Johannes Kepler University>AUT>education;Tim Pohle+Johannes Kepler University>AUT>education;Peter Knees+Johannes Kepler University>AUT>education,"This paper addresses the issue of automatically organizing a possibly large music collection for intuitive access. We present an approach to cluster tracks in a hierarchical manner and to automatically find representative pieces of music for each cluster on each hierarchy level. To this end, audio signal-based features are complemented with features derived via Web content mining in a novel way. Automatic hierarchical clustering is performed using a variant of the Self-Organizing Map, which we further modified in order to create playlists containing similar tracks. The proposed approaches for playlist generation on a hierarchically structured music collection and finding prototypical tracks for each cluster are then integrated into the Traveller’s Sound Player, a mobile audio player application that organizes music in a playlist such that the distances between consecutive tracks are minimal. We extended this player to deal with the hierarchical nature of the playlists generated by the proposed structuring approach. As for evaluation, we first assess the quality of the clustering method using the measure of entropy on a genre-annotated test set. Second, the goodness of the method to find prototypical tracks for each cluster is investigated in a user study."
25,Sally Jo Cunningham;Edmond Zhang,Development of a Music Organizer for Children.,2008,https://doi.org/10.5281/zenodo.1418329,Sally Jo Cunningham+University of Waikato>NZL>education;Yiwen (Edmond) Zhang+University of Waikato>NZL>education,"Software development for children is challenging; children have their own needs, which often are not met by ‘grown up’ software. We focus on software for playing songs and managing a music collection—tasks that children take great interest in, but for which they have few or inappropriate tools. We address this situation with the design of a new music management system, created with children as design partners: the Kids Music Box."
26,Masahiro Niitsuma;Hiroshi Takaesu;Hazuki Demachi;Masaki Oono;Hiroaki Saito,Development of an Automatic Music Selection System Based on Runner's Step Frequency.,2008,https://doi.org/10.5281/zenodo.1415900,Niitsuma Masahiro+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Hiroshi Takaesu+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Hazuki Demachi+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Masaki Oono+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown;Hiroaki Saito+Keio University>JPN>education|Department of Computer Science>Unknown>Unknown,"This paper presents an automatic music selection system based on runner’s step frequency. Recent development of portable music players like iPod has increased the number of those who listen to music while exercising. However, few systems which connect exercises with music selection have been developed. We propose a system that automatically selects music suitable for user’s running exercises. Although many parameters can be taken into account, as a first step we focus on runner’s step frequency. This system selects music with tempo suitable for runner’s step frequency and when runner’s step frequency changes, it executes another music selection. The system consists of three modules: step frequency estimation, music selection, and music playing. In the first module, runner’s step frequency is estimated from data derived from an acceleration sensor. In the second module, appropriate music is selected based on the estimated step frequency. In the third module, the selected music is played until runner’s step frequency changes. In the experiment, subjects ran on a running machine at different paces listening to the music selected by the proposed system. Experimental results show that the system can estimate runner’s SPM accurately and on the basis of the estimated SPM it can select music appropriate for users’ exercises with more than 85.0% accuracy, and makes running exercises more pleasing."
27,Kazumasa Murata;Kazuhiro Nakadai;Kazuyoshi Yoshii;Ryu Takeda;Toyotaka Torii;Hiroshi G. Okuno;Yuji Hasegawa;Hiroshi Tsujino,A Robot Singer with Music Recognition Based on Real-Time Beat Tracking.,2008,https://doi.org/10.5281/zenodo.1415108,"Kazumasa Murata+Tokyo Institute of Technology>JPN>education;Kazuhiro Nakadai+Honda Research Institute Japan Co., Ltd.>JPN>company;Kazuyoshi Yoshii+Kyoto University>JPN>education;Ryu Takeda+Kyoto University>JPN>education;Toyotaka Torii+Honda Research Institute Japan Co., Ltd.>JPN>company;Hiroshi G. Okuno+Kyoto University>JPN>education;Yuji Hasegawa+Honda Research Institute Japan Co., Ltd.>JPN>company;Hiroshi Tsujino+Honda Research Institute Japan Co., Ltd.>JPN>company","A robot that can provide an active and enjoyable user interface is one of the most challenging applications for music information processing, because the robot should cope with high-power noises including self voices and motor noises. This paper proposes noise-robust musical beat tracking by using a robot-embedded microphone, and describes its application to a robot singer with music recognition. The proposed beat tracking introduces two key techniques, that is, spectro-temporal pattern matching and echo cancellation. The former realizes robust tempo estimation with a shorter window length, thus, it can quickly adapt to tempo changes. The latter is effective to cancel self periodic noises such as stepping, scatting, and singing. We constructed a robot singer based on the proposed beat tracking for Honda ASIMO. The robot detects a musical beat with its own microphone in a noisy environment. It tries to recognize music based on the detected musical beat. When it successfully recognizes music, it sings while stepping according to the beat. Otherwise, it performs scatting instead of singing because the lyrics are unavailable. Experimental results showed fast adaptation to tempo changes and high robustness in beat tracking even when stepping, scatting and singing."
28,Martin Gasser;Arthur Flexer;Gerhard Widmer,Streamcatcher: Integrated Visualization of Music Clips and Online Audio Streams.,2008,https://doi.org/10.5281/zenodo.1416362,Martin Gasser+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Arthur Flexer+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility;Gerhard Widmer+Johannes Kepler University>AUT>education,"We propose a content-based approach to explorative visualization of online audio streams (e.g., web radio streams). The visualization space is defined by prototypical instances of musical concepts taken from personal music collections. Our system shows the relation of prototypes to each other and generates an animated visualization that places representations of audio streams in the vicinity of their most similar prototypes. Both computation of music similarity and visualization are formulated for online real time performance. A software implementation of these ideas is presented and evaluated."
29,Kazuyoshi Yoshii;Masataka Goto,Music Thumbnailer: Visualizing Musical Pieces in Thumbnail Images Based on Acoustic Features.,2008,https://doi.org/10.5281/zenodo.1415936,Kazuyoshi Yoshii+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"This paper presents a principled method called MusicThumbnailer to transform musical pieces into visual thumbnail images based on acoustic features extracted from their audio signals. These thumbnails can help users immediately guess the musical contents of audio signals without trial listening. This method is consistent in ways that optimize thumbnails according to the characteristics of a target music collection. This means the appropriateness of transformation should be defined to eliminate ad hoc transformation rules. In this paper, we introduce three top-down criteria to improve memorability of thumbnails (generate gradations), deliver information more completely, and distinguish thumbnails more clearly. These criteria are mathematically implemented as minimization of brightness differences of adjacent pixels and maximization of brightness variances within and between thumbnails. The optimized parameters of a modified linear mapping model we assumed are obtained by minimizing a unified cost function based on the three criteria with a steepest descent method. Experimental results indicate that generated thumbnails can provide users with useful hints as to the musical contents of musical pieces."
30,Panagiotis Symeonidis;Maria M. Ruxanda;Alexandros Nanopoulos;Yannis Manolopoulos,Ternary Semantic Analysis of Social Tags for Personalized Music Recommendation.,2008,https://doi.org/10.5281/zenodo.1416672,Panagiotis Symeonidis+Aristotle University of Thessaloniki>GRC>education|Aalborg University>DNK>education;Maria Ruxanda+Aalborg University>DNK>education;Alexandros Nanopoulos+Aristotle University of Thessaloniki>GRC>education;Yannis Manolopoulos+Aristotle University of Thessaloniki>GRC>education,"""Social tagging is the process by which many users add metadata in the form of keywords, to annotate information items. In case of music, the annotated items can be songs, artists, albums. Current music recommenders which employ social tagging to improve the music recommendation, fail to always provide appropriate item recommendations, because: (i) users may have different interests for a musical item, and (ii) musical items may have multiple facets. In this paper, we propose an approach that tackles the problem of the multimodal use of music. We develop a unified framework, represented by a 3-order tensor, to model altogether users, tags, and items. Then, we recommend musical items according to users multimodal perception of music, by performing latent semantic analysis and dimensionality reduction using the Higher Order Singular Value Decomposition technique. We experimentally evaluate the proposed method against two state-of-the-art recommendations algorithms using real Last.fm data. Our results show significant improvements in terms of effectiveness measured through recall/precision."""
31,Douglas Turnbull;Luke Barrington;Gert R. G. Lanckriet,Five Approaches to Collecting Tags for Music.,2008,https://doi.org/10.5281/zenodo.1416804,Douglas Turnbull+UC San Diego>USA>education|UC San Diego>USA>education|UC San Diego>USA>education;Luke Barrington+UC San Diego>USA>education;Gert Lanckriet+UC San Diego>USA>education,"We compare five approaches to collecting tags for music: conducting a survey, harvesting social tags, deploying annotation games, mining web documents, and autotagging audio content. The comparison includes a discussion of both scalability (financial cost, human involvement, and computational resources) and quality (the cold start problem & popularity bias, strong vs. weak labeling, vocabulary structure & size, and annotation accuracy). We then describe one state-of-the-art system for each approach. The performance of each system is evaluated using a tag-based music information retrieval task. Using this task, we are able to quantify the effect of popularity bias on each approach by making use of a subset of more popular (short-head) songs and a set of less popular (long-tail) songs. Lastly, we propose a simple hybrid context-content system that combines our individual approaches and produces superior retrieval results."
32,Youngmoo E. Kim;Erik M. Schmidt;Lloyd Emelle,MoodSwings: A Collaborative Game for Music Mood Label Collection.,2008,https://doi.org/10.5281/zenodo.1416586,Youngmoo E. Kim+Drexel University>USA>education;Erik Schmidt+Drexel University>USA>education;Lloyd Emelle+Drexel University>USA>education,"There are many problems in the field of music information retrieval that are not only difficult for machines to solve, but that do not have well-defined answers. In labeling and detecting emotions within music, this lack of specificity makes it difficult to train systems that rely on quantified labels for supervised machine learning. The collection of such “ground truth” data for these subjectively perceived features necessarily requires human subjects. Traditional methods of data collection, such as the hiring of subjects, can be flawed, since labeling tasks are time-consuming, tedious, and expensive. Recently, there have been many initiatives to use customized online games to harness so-called “Human Computation” for the collection of label data, and several such games have been proposed to collect labels spanning an excerpt of music. We present a new game, MoodSwings (http://schubert.ece.drexel.edu/moodswings), which differs in that it records dynamic (per-second) labels of players’ mood ratings of music, in keeping with the unique time-varying quality of musical mood. As in prior collaborative game approaches, players are partnered to verify each others’ results, and the game is designed to maximize consensus-building between users. We present preliminary results from an initial set of game play data."
33,Zhiyao Duan;Lie Lu;Changshui Zhang,Collective Annotation of Music from Multiple Semantic Categories.,2008,https://doi.org/10.5281/zenodo.1416694,Zhiyao Duan+Microsoft Research Asia (MSRA)>CHN>company;Lie Lu+Microsoft Research Asia (MSRA)>CHN>company;Changshui Zhang+Tsinghua University>CHN>education,"Music semantic annotation aims to automatically annotate a music signal with a set of semantic labels (words or tags). Existing methods on music semantic annotation usually take it as a multi-label binary classification problem, and model each semantic label individually while ignoring their relationships. However, there are usually strong correlations between some labels. Intuitively, investigating this correlation can be helpful to improve the overall annotation performance. In this paper, we report our attempts to collective music semantic annotation, which not only builds a model for each semantic label, but also builds models for the pairs of labels that have significant correlations. Two methods are exploited in this paper, one based on a generative model (Gaussian Mixture Model), and another based on a discriminative model (Conditional Random Field). Experiments show slight but consistent improvement in terms of precision and recall, compared with the individual-label modeling methods."
34,Geoffroy Peeters;David Fenech;Xavier Rodet,MCIpa: A Music Content Information Player and Annotator for Discovering Music.,2008,https://doi.org/10.5281/zenodo.1415796,Geoffroy Peeters+Ircam - CNRS STMS>FRA>facility;David Fenech+Ircam>FRA>facility;Xavier Rodet+Ircam - CNRS STMS>FRA>facility,"In this paper, we present a new tool for intra-document browsing of musical pieces. This tool is a multimedia player which represents the content of a musical piece visually. Each type of musical content (structure, chords, downbeats/beats, notes, events) is associated with a distinct visual representation. The user sees what he/she is listening to. He can also browse inside the music according to the visual content. For this, each type of visual object has a dedicated feedback, either as an audio-feedback or as a playhead feedback. Content information can be extracted automatically from audio (using signal processing algorithms) or annotated by hand by the user. This multimedia player can also be used as an annotator tool guided by the content."
35,Nik Corthaut;Sten Govaerts;Katrien Verbert;Erik Duval,"Connecting the Dots: Music Metadata Generation, Schemas and Applications.",2008,https://doi.org/10.5281/zenodo.1415244,Nik Corthaut+Katholieke Universiteit Leuven>BEL>education;Sten Govaerts+Katholieke Universiteit Leuven>BEL>education;Katrien Verbert+Katholieke Universiteit Leuven>BEL>education;Erik Duval+Katholieke Universiteit Leuven>BEL>education,"With the ever-increasing amount of digitized music becoming available, metadata is a key driver for different music related application domains. A service that combines different metadata sources should be aware of the existence of different schemas to store and exchange music metadata. The user of a metadata provider could benefit from knowledge about the metadata needs for different music application domains. In this paper, we present how we can compare the expressiveness and richness of a metadata schema for an application. To cope with different levels of granularity in metadata fields we defined clusters of semantically related metadata fields. Similarly, application domains were defined to tackle the fine-grained functionality space in music applications. Next is shown to what extent music application domains and metadata schemas make use of the metadata field clusters. Finally, we link the metadata schemas with the application domains. A decision table is presented that assists the user of a metadata provider in choosing the right metadata schema for his application."
36,Mohamed Sordo;Òscar Celma;Martin Blech;Enric Guaus,The Quest for Musical Genres: Do the Experts and the Wisdom of Crowds Agree?,2008,https://doi.org/10.5281/zenodo.1415262,Mohamed Sordo+Universitat Pompeu Fabra>ESP>education;Oscar Celma+Universitat Pompeu Fabra>ESP>education;Martín Blech+Universitat Pompeu Fabra>ESP>education;Enric Guaus+Universitat Pompeu Fabra>ESP>education,"This paper presents some findings around musical genres. The main goal is to analyse whether there is any agreement between a group of experts and a community, when defining a set of genres and their relationships. For this purpose, three different experiments are conducted using two datasets: the MP3.com expert taxonomy, and last.fm tags at artist level. The experimental results show a clear agreement for some components of the taxonomy (Blues, Hip-Hop), whilst in other cases (e.g. Rock) there is no correlations. Interestingly enough, the same results are found in the MIREX2007 results for audio genre classification task. Therefore, a multi-faceted approach for musical genre using expert based classifications, dynamic associations derived from the wisdom of crowds, and content-based analysis can improve genre classification, as well as other relevant MIR tasks such as music similarity or music recommendation."
37,Yves Raimond;Mark B. Sandler,A Web of Musical Information.,2008,https://doi.org/10.5281/zenodo.1414840,"Yves Raimond+Queen Mary, University of London>GBR>education;Mark Sandler+Queen Mary, University of London>GBR>education","We describe our recent achievements in interlinking several music-related data sources on the Semantic Web. In particular, we describe interlinked datasets dealing with Creative Commons content, editorial, encyclopedic, geographic and statistical data, along with queries they can answer and tools using their data. We describe our web services, providing an on-demand access to content-based features linked with such data sources and information pertaining to their creation (including processing steps, applied algorithms, inputs, parameters or associated developers). We also provide a tool allowing such music analysis services to be set up and scripted in a simple way."
38,Kurt Jacobson;Mark B. Sandler;Benjamin Fields,Using Audio Analysis and Network Structure to Identify Communities in On-Line Social Networks of Artists.,2008,https://doi.org/10.5281/zenodo.1417867,"Kurt Jacobson+Queen Mary, University of London>GBR>education;Mark Sandler+Queen Mary, University of London>GBR>education;Ben Fields+Goldsmiths, University of London>GBR>education","Community detection methods from complex network theory are applied to a subset of the Myspace artist network to identify groups of similar artists. Methods based on the greedy optimization of modularity and random walks are used. In a second iteration, inter-artist audio-based similarity scores are used as input to enhance these community detection methods. The resulting community structures are evaluated using a collection of artist-assigned genre tags. Evidence suggesting the Myspace artist network structure is closely related to musical genre is presented and a Semantic Web service for accessing this structure is described."
39,Claudio Baccigalupo;Enric Plaza;Justin Donaldson,Uncovering Affinity of Artists to Multiple Genres from Social Behaviour Data.,2008,https://doi.org/10.5281/zenodo.1417823,"Claudio Baccigalupo+IIIA, Artificial Intelligence Research Institute>ESP>facility|Indiana University>USA>education;Enric Plaza+IIIA, Artificial Intelligence Research Institute>ESP>facility|Indiana University>USA>education;Justin Donaldson+Indiana University>USA>education","In organisation schemes, musical artists are commonly identified with a unique ‘genre’ label attached, even when they have affinity to multiple genres. To uncover this hidden cultural awareness about multi-genre affinity, we present a new model based on the analysis of the way in which a community of users organise artists and genres in playlists. Our work is based on a novel dataset that we have elaborated identifying the co-occurrences of artists in the playlists shared by the members of a popular Web-based community, and that is made publicly available. The analysis defines an automatic social-based method to uncover relationships between artists and genres, and introduces a series of novel concepts that characterises artists and genres in a richer way than a unique ‘genre’ label would do."
40,Hiromasa Fujihara;Masataka Goto;Jun Ogata,Hyperlinking Lyrics: A Method for Creating Hyperlinks Between Phrases in Song Lyrics.,2008,https://doi.org/10.5281/zenodo.1418251,Hiromasa Fujihara+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Masataka Goto+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility;Jun Ogata+National Institute of Advanced Industrial Science and Technology (AIST)>JPN>facility,"We describe a novel method for creating a hyperlink from a phrase in the lyrics of a song to the same phrase in the lyrics of another song. This method can be applied to various applications, such as song clustering based on the meaning of the lyrics and a music playback interface that will enable a user to browse and discover songs on the basis of lyrics. Given a song database consisting of songs with their text lyrics and songs without their text lyrics, our method first extracts appropriate keywords (phrases) from the text lyrics without using audio signals. It then finds these keywords in audio signals by estimating the keywords’ start and end times. Although the performance obtained in our experiments has room for improvement, the potential of this new approach is shown."
41,Florian Kleedorfer;Peter Knees;Tim Pohle,Oh Oh Oh Whoah! Towards Automatic Topic Detection In Song Lyrics.,2008,https://doi.org/10.5281/zenodo.1416154,Florian Kleedorfer+Studio Smart Agent Technologies>AUT>company|Research Studios Austria>AUT>facility;Peter Knees+Johannes Kepler University Linz>AUT>education;Tim Pohle+Johannes Kepler University Linz>AUT>education,We present an algorithm that allows for indexing music by topic. The application scenario is an information retrieval system into which any song with known lyrics can be inserted and indexed so as to make a music collection browsable by topic. We use text mining techniques for creating a vector space model of our lyrics collection and non-negative matrix factorization (NMF) to identify topic clusters which are then labeled manually. We include a discussion of the decisions regarding the parametrization of the applied methods. The suitability of our approach is assessed by measuring the agreement of test subjects who provide the labels for the topic clusters.
42,Matthew Riley;Eric Heinen;Joydeep Ghosh,A Text Retrieval Approach to Content-Based Audio Hashing.,2008,https://doi.org/10.5281/zenodo.1417985,Matthew Riley+University of Texas at Austin>USA>education;Eric Heinen+University of Texas at Austin>USA>education;Joydeep Ghosh+University of Texas at Austin>USA>education,"This paper presents a novel approach to robust, content-based retrieval of digital music. We formulate the hashing and retrieval problems analogously to that of text retrieval and leverage established results for this unique application. Accordingly, songs are represented as a ""Bag-of-Audio-Words"" and similarity calculations follow directly from the well-known Vector Space model [12]. We evaluate our system on a 4000 song data set to demonstrate its practical applicability, and evaluation shows our technique to be robust to a variety of signal distortions. Most interestingly, the system is capable of matching studio recordings to live recordings of the same song with high accuracy."
43,Riccardo Miotto;Nicola Orio,A Music Identification System Based on Chroma Indexing and Statistical Modeling.,2008,https://doi.org/10.5281/zenodo.1415254,Riccardo Miotto+University of Padova>ITA>education;Nicola Orio+University of Padova>ITA>education,"A methodology is described for the automatic identification of classical music works. It can be considered an extension of fingerprinting techniques because the identification is carried out also when the query is a different performance of the work stored in the database, possibly played by different instruments and with background noise. The proposed methodology integrates an already existing approach based on hidden Markov models with an additional component that aims at improving scalability. The general idea is to carry out a clustering of the collection to highlight a limited number of candidates to be used for the HMM-based identification. Clustering is computed using the chroma features of the music works, hashed in a single value and retrieved using a bag of terms approach. Evaluation results are provided to show the validity of the combined approaches."
44,Mark Godfrey;Parag Chordia,Hubs and Homogeneity: Improving Content-Based Music Modeling.,2008,https://doi.org/10.5281/zenodo.1415154,Mark T. Godfrey+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education;Parag Chordia+Georgia Institute of Technology>USA>education|Georgia Institute of Technology>USA>education,"We explore the origins of hubs in timbre-based song modeling in the context of content-based music recommendation and propose several remedies. Specifically, we find that a process of model homogenization, in which certain components of a mixture model are systematically removed, improves performance as measured against several ground-truth similarity metrics. Extending the work of Aucouturier, we introduce several new methods of homogenization. On a subset of the uspop data set, model homogenization improves artist R-precision by a maximum of 3.5% and agreement to user collection co-occurrence data by 7.4%. We also explore differences in the effectiveness of the various homogenization methods for hub reduction. Further, we extend the modeling of frame-based MFCC features by using a kernel density estimation approach to non-parametric modeling. We find that such an approach significantly reduces the number of hubs (by 2.6% of the dataset) while improving agreement to ground-truth by 5% and slightly improving artist R-precision as compared with the standard parametric model."
45,Malcolm Slaney;Kilian Q. Weinberger;William White,Learning a Metric for Music Similarity.,2008,https://doi.org/10.5281/zenodo.1415554,Malcolm Slaney+Yahoo! Research>USA>company;Kilian Weinberger+Yahoo! Research>USA>company;William White+Yahoo! Media Innovation>USA>company,"This paper describe five different principled ways to embed songs into a Euclidean metric space. In particular, we learn embeddings so that the pairwise Euclidean distance between two songs reflects semantic dissimilarity. This allows distance-based analysis, such as for example straightforward nearest-neighbor classification, to detect and potentially suggest similar songs within a collection. Each of the six approaches (baseline, whitening, LDA, NCA, LMNN and RCA) rotate and scale the raw feature space with a linear transform. We tune the parameters of these models using a song-classification task with content-based features."
46,Yuxiang Liu;Ye Wang;Arun Shenoy;Wei-Ho Tsai;Lianhong Cai,Clustering Music Recordings by Their Keys.,2008,https://doi.org/10.5281/zenodo.1417375,Yuxiang Liu+Tsinghua University>CHN>education|National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education;Arun Shenoy+Unknown>Unknown>Unknown;Wei-Ho Tsai+National Taipei University of Technology>TWN>education;Lianhong Cai+Tsinghua University>CHN>education,"Music key, a high level feature of musical audio, is an effective tool for structural analysis of musical works. This paper presents a novel unsupervised approach for clustering music recordings by their keys. Based on chroma-based features extracted from acoustic signals, an inter-recording distance metric which characterizes diversity of pitch distribution together with harmonic center of music pieces, is introduced to measure dissimilarities among musical features. Then, recordings are divided into categories via unsupervised clustering, where the best number of clusters can be determined automatically by minimizing estimated Rand Index. Any existing technique for key detection can then be employed to identify key assignment for each cluster. Empirical evaluation on a dataset of 91 pop songs illustrates an average cluster purity of 57.3% and a Rand Index of close to 50%, thus highlighting the possibility of integration with existing key identification techniques to improve accuracy, based on strong cross-correlation data available from this framework for input dataset."
47,Konstantinos Trohidis;Grigorios Tsoumakas;George Kalliris;Ioannis P. Vlahavas,Multi-Label Classification of Music into Emotions.,2008,https://doi.org/10.5281/zenodo.1414900,Konstantinos Trohidis+Aristotle University of Thessaloniki>GRC>education;Grigorios Tsoumakas+Aristotle University of Thessaloniki>GRC>education;George Kalliris+Aristotle University of Thessaloniki>GRC>education;Ioannis Vlahavas+Aristotle University of Thessaloniki>GRC>education,"In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features."
48,Shyamala Doraisamy;Shahram Golzari;Noris Mohd. Norowi;Md Nasir Sulaiman;Nur Izura Udzir,A Study on Feature Selection and Classification Techniques for Automatic Genre Classification of Traditional Malay Music.,2008,https://doi.org/10.5281/zenodo.1415124,Shyamala Doraisamy+University Putra Malaysia>MYS>education;Shahram Golzari+University Putra Malaysia>MYS>education;Noris Mohd. Norowi+University Putra Malaysia>MYS>education;Md. Nasir B Sulaiman+University Putra Malaysia>MYS>education;Nur Izura Udzir+University Putra Malaysia>MYS>education,"Machine learning techniques for automated musical genre classification is currently widely studied. With large collections of digital musical files, one approach to classification is to classify by musical genres such as pop, rock and classical in Western music. Beat, pitch and temporal related features are extracted from audio signals and various machine learning algorithms are applied for classification. Features that resulted in better classification accuracies for Traditional Malay Music (TMM), in comparison to western music, in a previous study were beat related features. However, only the J48 classifier was used and in this study we perform a more comprehensive investigation on improving the classification of TMM. In addition, feature selection was performed for dimensionality reduction. Classification accuracies using classifiers of varying paradigms on a dataset comprising ten TMM genres were obtained. Results identify potentially useful classifiers and show the impact of adding a feature selection phase for TMM genre classification."
49,Rudolf Mayer;Robert Neumayer;Andreas Rauber,Rhyme and Style Features for Musical Genre Classification by Song Lyrics.,2008,https://doi.org/10.5281/zenodo.1416758,Rudolf Mayer+Vienna University of Technology>AUT>education;Robert Neumayer+Vienna University of Technology>AUT>education|Norwegian University of Science and Technology>NOR>education;Andreas Rauber+Vienna University of Technology>AUT>education,"How individuals perceive music is influenced by many different factors. The audible part of a piece of music, its sound, does for sure contribute, but is only one aspect to be taken into account. Cultural information influences how we experience music, as does the songs’ text and its sound. Next to symbolic and audio based music information retrieval, which focus on the sound of music, song lyrics, may thus be used to improve classification or similarity ranking of music. Song lyrics exhibit specific properties different from traditional text documents – many lyrics are for example composed in rhyming verses, and may have different frequencies for certain parts-of-speech when compared to other text documents. Further, lyrics may use ‘slang’ language or differ greatly in the length and complexity of the language used, which can be measured by some statistical features such as word / verse length, and the amount of repetative text. In this paper, we present a novel set of features developed for textual analysis of song lyrics, and combine them with and compare them to classical bag-of-words indexing approaches. We present results for musical genre classification on a test collection in order to demonstrate our analysis."
50,Bill Z. Manaris;Dwight Krehbiel;Patrick Roos;Thomas Zalonis,Armonique: Experiments in Content-Based Similarity Retrieval using Power-Law Melodic and Timbre Metrics.,2008,https://doi.org/10.5281/zenodo.1416778,Bill Manaris+College of Charleston>USA>education;Dwight Krehbiel+Bethel College>USA>education;Patrick Roos+College of Charleston>USA>education;Thomas Zalonis+College of Charleston>USA>education,"This paper presents results from an on-going MIR study utilizing hundreds of melodic and timbre features based on power laws for content-based similarity retrieval. These metrics are incorporated into a music search engine prototype, called Armonique. This prototype is used with a corpus of 9153 songs encoded in both MIDI and MP3 to identify pieces similar to and dissimilar from selected songs. The MIDI format is used to extract various power-law features measuring proportions of music-theoretic and other attributes, such as pitch, duration, melodic intervals, and chords. The MP3 format is used to extract power-law features measuring proportions within FFT power spectra related to timbre. Several assessment experiments have been conducted to evaluate the effectiveness of the similarity model. The results suggest that power-law metrics are very promising for content-based music querying and retrieval, as they seem to correlate with aspects of human emotion and aesthetics."
51,Matthew D. Hoffman;David M. Blei;Perry R. Cook,Content-Based Musical Similarity Computation using the Hierarchical Dirichlet Process.,2008,https://doi.org/10.5281/zenodo.1417223,Matthew Hoffman+Princeton University>USA>education;David Blei+Princeton University>USA>education;Perry Cook+Princeton University>USA>education,"We develop a method for discovering the latent structure in MFCC feature data using the Hierarchical Dirichlet Process (HDP). Based on this structure, we compute timbral similarity between recorded songs. The HDP is a nonparametric Bayesian model. Like the Gaussian Mixture Model (GMM), it represents each song as a mixture of some number of multivariate Gaussian distributions However, the number of mixture components is not fixed in the HDP, but is determined as part of the posterior inference process. Moreover, in the HDP the same set of Gaussians is used to model all songs, with only the mixture weights varying from song to song. We compute the similarity of songs based on these weights, which is faster than previous approaches that compare single Gaussian distributions directly. Experimental results on a genre-based retrieval task illustrate that our HDP-based method is both faster and produces better retrieval quality than such previous approaches."
53,Phillip B. Kirlin;Paul E. Utgoff,A Framework for Automated Schenkerian Analysis.,2008,https://doi.org/10.5281/zenodo.1415892,Phillip B. Kirlin+University of Massachusetts Amherst>USA>education;Paul E. Utgoff+University of Massachusetts Amherst>USA>education,"In Schenkerian analysis, one seeks to find structural dependences among the notes of a composition and organize these dependences into a coherent hierarchy that illustrates the function of every note. This type of analysis reveals multiple levels of structure in a composition by constructing a series of simplifications of a piece showing various elaborations and prolongations. We present a framework for solving this problem, called IVI, that uses a state-space search formalism. IVI includes multiple interacting components, including modules for various preliminary analyses (harmonic, melodic, rhythmic, and cadential), identifying and performing reductions, and locating pieces of the Ursatz. We describe a number of the algorithms by which IVI forms, stores, and updates its hierarchy of notes, along with details of the Ursatz-finding algorithm. We illustrate IVI’s functionality on an excerpt from a Schubert piano composition, and also discuss the issues of subproblem interactions and the multiple parsings problem."
54,Jouni Paulus;Anssi Klapuri,Music Structure Analysis Using a Probabilistic Fitness Measure and an Integrated Musicological Model.,2008,https://doi.org/10.5281/zenodo.1415826,Jouni Paulus+Tampere University of Technology>FIN>education;Anssi Klapuri+Tampere University of Technology>FIN>education,"This paper presents a system for recovering the sectional form of a musical piece: segmentation and labelling of musical parts such as chorus or verse. The system uses three types of acoustic features: mel-frequency cepstral coefficients, chroma, and rhythmogram. An analysed piece is first subdivided into a large amount of potential segments. The distance between each two segments is then calculated and the value is transformed to a probability that the two segments are occurrences of a same musical part. Different features are combined in the probability space and are used to define a fitness measure for a candidate structure description. Musicological knowledge of the temporal dependencies between the parts is integrated into the fitness measure. A novel search algorithm is presented for finding the description that maximises the fitness measure. The system is evaluated with a data set of 557 manually annotated popular music pieces. The results suggest that integrating the musicological model to the fitness measure leads to a more reliable labelling of the parts than performing the labelling as a post-processing step."
55,Hanna M. Lukashevich,Towards Quantitative Measures of Evaluating Song Segmentation.,2008,https://doi.org/10.5281/zenodo.1417623,Hanna Lukashevich+Fraunhofer IDMT>DEU>facility,"Automatic music structure analysis or song segmentation has immediate applications in the field of music information retrieval. Among these applications is active music navigation, automatic generation of audio summaries, automatic music analysis, etc. One of the important aspects of a song segmentation task is its evaluation. Commonly, that implies comparing the automatically estimated segmentation with a ground-truth, annotated by human experts. The automatic evaluation of segmentation algorithms provides the quantitative measure that reflects how well the estimated segmentation matches the annotated ground-truth. In this paper we present a novel evaluation measure based on information-theoretic conditional entropy. The principal advantage of the proposed approach lies in the applied normalization, which enables the comparison of the automatic evaluation results, obtained for songs with a different amount of states. We discuss and compare the evaluation scores commonly used for evaluating song segmentation at present. We provide several examples illustrating the behavior of different evaluation measures and weigh the benefits of the presented metric against the others."
56,Jörg Garbers;Frans Wiering,Towards Structural Alignment of Folk Songs.,2008,https://doi.org/10.5281/zenodo.1415838,Jörg Garbers+Utrecht University>NLD>education|Unknown>Unknown>Unknown;Frans Wiering+Utrecht University>NLD>education|Unknown>Unknown>Unknown,"We describe an alignment-based similarity framework for folk song variation research. The framework makes use of phrase and meter information encoded in Humdrum scores. Local similarity measures are used to compute match scores, which are combined with gap scores to form increasingly larger alignments and higher-level similarity values. We discuss the effects of some similarity measures on the alignment of four groups of melodies that are variants of each other."
57,Meinard Müller;Sebastian Ewert,Joint Structure Analysis with Applications to Music Annotation and Synchronization.,2008,https://doi.org/10.5281/zenodo.1415236,Meinard Müller+Saarland University>DEU>education|MPI Informatik>DEU>facility;Sebastian Ewert+Bonn University>DEU>education,"The general goal of music synchronization is to automatically align different versions and interpretations related to a given musical work. In computing such alignments, recent approaches assume that the versions to be aligned correspond to each other with respect to their overall global structure. However, in real-world scenarios, this assumption is often violated. For example, for a popular song there often exist various structurally different album, radio, or extended versions. Or, in classical music, different recordings of the same piece may exhibit omissions of repetitions or significant differences in parts such as solo cadenzas. In this paper, we introduce a novel approach for automatically detecting structural similarities and differences between two given versions of the same piece. The key idea is to perform a single structural analysis for both versions simultaneously instead of performing two separate analyses for each of the two versions. Such a joint structure analysis reveals the repetitions within and across the two versions. As a further contribution, we show how this information can be used for deriving musically meaningful partial alignments and annotations in the presence of structural variations."
58,Kyogu Lee;Markus Cremer,Segmentation-Based Lyrics-Audio Alignment using Dynamic Programming.,2008,https://doi.org/10.5281/zenodo.1416934,Kyogu Lee+Gracenote>USA>company|Media Technology Lab>USA>company;Markus Cremer+Gracenote>USA>company|Media Technology Lab>USA>company,"In this paper, we present a system for automatic alignment of textual lyrics with musical audio. Given an input audio signal, structural segmentation is first performed and similar segments are assigned a label by computing the distance between the segment pairs. Using the results of segmentation and hand-labeled paragraphs in lyrics as a pair of input strings, we apply a dynamic programming (DP) algorithm to find the best alignment path between the two strings, achieving segment-to-paragraph synchronization. We demonstrate that the proposed algorithm performs well for various kinds of musical audio."
60,John Ashley Burgoyne;Johanna Devaney;Laurent Pugin;Ichiro Fujinaga,Enhanced Bleedthrough Correction for Early Music Documents with Recto-Verso Registration.,2008,https://doi.org/10.5281/zenodo.1417235,John Ashley Burgoyne+McGill University>CAN>education;Johanna Devaney+McGill University>CAN>education;Laurent Pugin+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"Ink bleedthrough is common problem in early music documents. Even when such bleedthrough does not pose problems for human perception, it can inhibit the performance of optical music recognition (OMR). One way to reduce the amount of bleedthrough is to take into account what is printed on the reverse of the page. In order to do so, the reverse of the page must be registered to match the front of the page on a pixel-by-pixel basis. This paper describes our approach to registering scanned early music scores as well as our modifications to two robust binarization approaches to take into account bleedthrough and the information available from the registration process. We determined that although the information from registration itself often makes little difference in recognition performance, other modifications to binarization algorithms for correcting bleedthrough can yield dramatic increases in OMR results."
61,Christian Fremerey;Meinard Müller;Frank Kurth;Michael Clausen,Automatic Mapping of Scanned Sheet Music to Audio Recordings.,2008,https://doi.org/10.5281/zenodo.1416034,Christian Fremerey+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility;Meinard Müller+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility;Frank Kurth+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility;Michael Clausen+University of Bonn>DEU>education|Max-Planck-Institut (MPI)>DEU>facility|Research Establishment for Applied Science (FGAN)>DEU>facility,"Signiﬁcant digitization efforts have resulted in large multi-modal music collections comprising visual (scanned sheet music) as well as acoustic material (audio recordings). In this paper, we present a novel procedure for mapping scanned pages of sheet music to a given collection of audio recordings by identifying musically corresponding audio clips. To this end, both the scanned images as well as the audio recordings are ﬁrst transformed into a common feature representation using optical music recognition (OMR) and methods from digital signal processing, respectively. Based on this common representation, a direct comparison of the two different types of data is facilitated. This allows for a search of scan-based queries in the audio collection. We report on systematic experiments conducted on the corpus of Beethoven’s piano sonatas showing that our mapping procedure works with high precision across the two types of music data in the case that there are no severe OMR errors. The proposed mapping procedure is relevant in a real-world application scenario at the Bavarian State Library for automatically identifying and annotating scanned sheet music by means of already available annotated audio material."
62,Laurent Pugin;Jason Hockman;John Ashley Burgoyne;Ichiro Fujinaga,Gamera Versus Aruspix: Two Optical Music Recognition Approaches.,2008,https://doi.org/10.5281/zenodo.1417683,Laurent Pugin+Schulich School of Music of McGill University>CAN>education;Jason Hockman+Schulich School of Music of McGill University>CAN>education;John Ashley Burgoyne+Schulich School of Music of McGill University>CAN>education;Ichiro Fujinaga+Schulich School of Music of McGill University>CAN>education,"Optical music recognition (OMR) applications are predominantly designed for common music notation and as such, are inherently incapable of adapting to specialized notation forms within early music. Two OMR systems, namely Gamut (a Gamera application) and Aruspix, have been proposed for early music. In this paper, we present a novel comparison of the two systems, which use markedly different approaches to solve the same problem, and pay close attention to the performance and learning rates of both applications. In order to obtain a complete comparison of Gamut and Aruspix, we evaluated the core recognition systems and the pitch determination processes separately. With our experiments, we were able to highlight the advantages of both approaches as well as causes of problems and possibilities for future improvements."
63,Juan José Burred;Carmine-Emanuele Cella;Geoffroy Peeters;Axel Röbel;Diemo Schwarz,Using the SDIF Sound Description Interchange Format for Audio Features.,2008,https://doi.org/10.5281/zenodo.1415762,Juan José Burred+IRCAM - CNRS STMS>FRA>facility;Carmine Emanuele Cella+IRCAM - CNRS STMS>FRA>facility;Geoffroy Peeters+IRCAM - CNRS STMS>FRA>facility;Axel Röbel+IRCAM - CNRS STMS>FRA>facility;Diemo Schwarz+IRCAM - CNRS STMS>FRA>facility,"We present a set of extensions to the Sound Description Interchange Format (SDIF) for the purpose of storage and/or transmission of general audio descriptors. The aim is to allow portability and interoperability between the feature extraction module of an audio information retrieval application and the remaining modules, such as training, classification or clustering. A set of techniques addressing the needs of short-time features and temporal modeling over longer windows are proposed, together with the mechanisms that allow further extensions or adaptations by the user. The paper is completed by an overview of the general aspects of SDIF and its practical use by means of a set of existing programming interfaces for, among others, C, C++ and Matlab."
64,Joachim Ganseman;Paul Scheunders;Wim D'haes,Using XQuery on MusicXML Databases for Musicological Analysis.,2008,https://doi.org/10.5281/zenodo.1414836,Joachim Ganseman+IBBT - Visionlab>BEL>facility|University of Antwerp>BEL>education;Paul Scheunders+IBBT - Visionlab>BEL>facility|University of Antwerp>BEL>education;Wim D’haes+Mu Technologies NV>BEL>company,"MusicXML is a fairly recent XML-based file format for music scores, now supported by many score and audio editing software applications. Several online score library projects exist or are emerging, some of them using MusicXML as main format. When storing a large set of XML-encoded scores in an XML database, XQuery can be used to retrieve information from this database. We present some small practical examples of such large scale analysis, using the Wikifonia lead sheet database and the eXist XQuery engine. This shows the feasibility of automated musicological analysis on digital score libraries using the latest software tools. Bottom line: it’s easy."
65,Jenn Riley,Application of the Functional Requirements for Bibliographic Records (FRBR) to Music.,2008,https://doi.org/10.5281/zenodo.1416446,Jenn Riley+Indiana University>USA>education,"This paper describes work applying the Functional Requirements for Bibliographic Records (FRBR) model to music, as the basis for implementing a fully FRBR-compliant music digital library system. A detailed analysis of the FRBR and Functional Requirements for Authority Data (FRAD) entities and attributes is presented. The paper closes with a discussion of the ways in which FRBR is gaining adoption outside of the library environment in which it was born. This work benefits the MIR community by demonstrating a model that can be used in MIR systems for the storage of descriptive information in support of metadata-based searching, and by positioning the Variations system to be a source of robust descriptive information for use by third-party MIR systems."
66,Travis M. Doll;Raymond Migneco;Youngmoo E. Kim,Online Activities for Music Information and Acoustics Education and Psychoacoustic Data Collection.,2008,https://doi.org/10.5281/zenodo.1418341,Travis M. Doll+Drexel University>USA>education;Ray V. Migneco+Drexel University>USA>education;Youngmoo E. Kim+Drexel University>USA>education,"Online collaborative activities provide a powerful platform for the collection of psychoacoustic data on the perception of audio and music from a very large numbers of subjects. Furthermore, these activities can be designed to simultaneously educate users about aspects of music information and acoustics, particularly for younger students in grades K-12. We have created prototype interactive activities illustrating aspects of two different sound and acoustics concepts: musical instrument timbre and the cocktail party problem (sound source isolation within mixtures) that also provide a method of collecting perceptual data related to these problems with a range of parameter variation that is difficult to achieve for large subject populations using traditional psychoacoustic evaluation. We present preliminary data from a pilot study where middle school students were engaged with the two activities to demonstrate the potential benefits as an education and data collection platform."
67,Carlos Nascimento Silla Jr.;Alessandro L. Koerich;Celso A. A. Kaestner,The Latin Music Database.,2008,https://doi.org/10.5281/zenodo.1416282,Carlos N. Silla Jr.+University of Kent>GBR>education;Alessandro L. Koerich+Pontifical Catholic University of Paraná>BRA>education;Celso A. A. Kaestner+Federal University of Technology of Paraná>BRA>education,"In this paper we present the Latin Music Database, a novel database of Latin musical recordings which has been developed for automatic music genre classification, but can also be used in other music information retrieval tasks. The method for assigning genres to the musical recordings is based on human expert perception and therefore capture their tacit knowledge in the genre labeling process. We also present the ethnomusicology of the genres available in the database as it might provide important information for the analysis of the results of any experiment that employs the database."
68,Megan A. Winget,"The Liner Notes Digitization Project: Providing Users with Cultural, Historical, and Critical Music Information.",2008,https://doi.org/10.5281/zenodo.1417933,Megan A. Winget+University of Texas at Austin>USA>education,"Digitizing cultural information is a complex endeavor. Not only do users expect to have access to primary information like digital music files; it is also becoming more important for digital systems to provide contextual information for the primary artifacts contained within. The Liner Notes Markup Language (LNML) was developed to provide an XML vocabulary for encoding complex contextual documents that include an album’s packaging, informational notes and inserts, liners, and album labels. This paper describes the development of the LNML framework, its major structural elements and functions, and some of the more pressing problems related to usability and purpose. The current LNML model is based on the examination and encoding of fifty albums from the 80s Rock genre. We are currently encoding fifty additional Jazz albums, which will provide data to augment and strengthen the model. Development of the LNML is ongoing, with plans to examine Classical and World Music examples to further augment the model."
69,Xiao Hu 0001;J. Stephen Downie;Cyril Laurier;Mert Bay;Andreas F. Ehmann,The 2007 MIREX Audio Mood Classification Task: Lessons Learned.,2008,https://doi.org/10.5281/zenodo.1416380,"Xiao Hu+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education;J. Stephen Downie+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education;Cyril Laurier+Music Technology Group, Universitat Pompeu Fabra>ESP>education;Mert Bay+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+International Music Information Retrieval System Evaluation Laboratory, University of Illinois at Urbana-Champaign>USA>education","Recent music information retrieval (MIR) research pays increasing attention to music classification based on moods expressed by music pieces. The first Audio Mood Classification (AMC) evaluation task was held in the 2007 running of the Music Information Retrieval Evaluation eXchange (MIREX). This paper describes important issues in setting up the task, including dataset construction and ground-truth labeling, and analyzes human assessments on the audio dataset, as well as system performances from various angles. Interesting findings include system performance differences with regard to mood clusters and the levels of agreement amongst human judgments regarding mood labeling. Based on these analyses, we summarize experiences learned from the first community scale evaluation of the AMC task and propose recommendations for future AMC and similar evaluation tasks."
70,J. Stephen Downie;Mert Bay;Andreas F. Ehmann;M. Cameron Jones,Audio Cover Song Identification: MIREX 2006-2007 Results and Analyses.,2008,https://doi.org/10.5281/zenodo.1417133,J. Stephen Downie+University of Illinois at Urbana-Champaign>USA>education;Mert Bay+University of Illinois at Urbana-Champaign>USA>education;Andreas F. Ehmann+University of Illinois at Urbana-Champaign>USA>education;M. Cameron Jones+University of Illinois at Urbana-Champaign>USA>education,"This paper presents analyses of the 2006 and 2007 results of the Music Information Retrieval Evaluation eXchange (MIREX) Audio Cover Song Identification (ACS) tasks. The Music Information Retrieval Evaluation eXchange (MIREX) is a community-based endeavor to scientifically evaluate music information retrieval (MIR) algorithms and techniques. The ACS task was created to motivate MIR researchers to expand their notions of similarity beyond acoustic similarity to include the important idea that musical works retain their identity notwithstanding variations in style, genre, orchestration, rhythm or melodic ornamentation, etc. A series of statistical analyses were performed that indicate significant improvements in this domain have been made over the course of 2006-2007. Post-hoc analyses reveal distinct differences between individual systems and the effects of certain classes of queries on performance. This paper discusses some of the techniques that show promise in this research domain."
71,Charlie Inskip;Andy MacFarlane;Pauline Rafferty,"Music, Movies and Meaning: Communication in Film-Makers' Search for Pre-Existing Music, and the Implications for Music Information Retrieval.",2008,https://doi.org/10.5281/zenodo.1418101,Charlie Inskip+City University London>GBR>education;Andy Macfarlane+City University London>GBR>education;Pauline Rafferty+University of Aberystwyth>GBR>education,"""While the use of music to accompany moving images is widespread, the information behaviour, communicative practice and decision making by creative professionals within this area of the music industry is an under-researched area. This investigation discusses the use of music in films and advertising focusing on communication and meaning of the music and introduces a reflexive communication model. The model is discussed in relation to interviews with a sample of music professionals who search for and use music for their work. Key factors in this process include stakeholders, briefs, product knowledge and relevance. Searching by both content and context is important, although the final decision when matching music to picture is partly intuitive and determined by a range of stakeholders."""
72,Rafael Ramírez 0001;Alfonso Pérez;Stefan Kersten,Performer Identification in Celtic Violin Recordings.,2008,https://doi.org/10.5281/zenodo.1416988,Rafael Ramirez+Universitat Pompeu Fabra>ESP>education;Alfonso Perez+Universitat Pompeu Fabra>ESP>education;Stefan Kersten+Universitat Pompeu Fabra>ESP>education,We present an approach to the task of identifying performers from their playing styles. We investigate how violinists express and communicate their view of the musical content of Celtic popular pieces and how to use this information in order to automatically identify performers. We study note-level deviations of parameters such as timing and amplitude. Our approach to performer identification consists of inducing an expressive performance model for each of the interpreters (essentially establishing a performer dependent mapping of inter-note features to a timing and amplitude expressive transformations). We present a successful performer identification case study.
73,Mitsuyo Hashida;Toshie Matsui;Haruhiro Katayose,A New Music Database Describing Deviation Information of Performance Expressions.,2008,https://doi.org/10.5281/zenodo.1416418,"Mitsuyo Hashida+Kwansei Gakuin University>JPN>education|CrestMuse Project, JST>JPN>facility;Toshie Matsui+Kwansei Gakuin University>JPN>education|CrestMuse Project, JST>JPN>facility;Haruhiro Katayose+Kwansei Gakuin University>JPN>education|CrestMuse Project, JST>JPN>facility","We introduce the CrestMuse Performance Expression Database (CrestMusePEDB), a music database that describes music performance expression and is available for academic research. While music databases are being provided as MIR technologies continue to progress, few databases deal with performance expression. We constructed a music expression database, CrestMusePEDB. It may be utilized in the research fields of music informatics, music perception and cognition, and musicology. It will contain music expression information on virtuosis’ expressive performances, including those of 3 to 10 players at a time, on about 100 pieces of classical Western music. The latest version of the database, CrestMusePEDB Ver. 2.0, is available. The paper gives an overview of CrestMusePEDB."
74,Miguel Molina-Solana;Josep Lluís Arcos;Emilia Gómez,Using Expressive Trends for Identifying Violin P erformers.,2008,https://doi.org/10.5281/zenodo.1417907,"Miguel Molina-Solana+University of Granada>ESP>education;Josep Lluís Arcos+CSIC, Spanish National Research Council>ESP>facility;Emilia Gomez+Universitat Pompeu Fabra>ESP>education","This paper presents a new approach for identifying professional performers in commercial recordings. We propose a Trend-based model that, analyzing the way Narmour’s Implication-Realization patterns are played, is able to characterize performers. Concretely, starting from automatically extracted descriptors provided by state-of-the-art extraction tools, the system performs a mapping to a set of qualitative behavior shapes and constructs a collection of frequency distributions for each descriptor. Experiments were conducted in a data-set of violin recordings from 23 different performers. Reported results show that our approach is able to achieve high identification rates."
75,Craig Sapp,Hybrid Numeric/Rank Similarity Metrics for Musical Performance Analysis.,2008,https://doi.org/10.5281/zenodo.1417561,"Craig Stuart Sapp+CHARM, Royal Holloway, University of London>GBR>education","This paper describes a numerical method for examining similarities among tempo and loudness features extracted from recordings of the same musical work and evaluates its effectiveness compared to Pearson correlation. Starting with correlation at multiple timescales, other concepts such as a performance “noise-floor” are used to generate measurements which are more refined than correlation alone. The measurements are evaluated and compared to plain correlation in their ability to identify performances of the same Chopin mazurka played by the same pianist out of a collection of recordings by various pianists."
77,Chee-Chuan Toh;Bingjun Zhang;Ye Wang,Multiple-Feature Fusion Based Onset Detection for Solo Singing Voice.,2008,https://doi.org/10.5281/zenodo.1414756,Chee Chuan Toh+National University of Singapore>SGP>education;Bingjun Zhang+National University of Singapore>SGP>education;Ye Wang+National University of Singapore>SGP>education,"Onset detection is a challenging problem in automatic singing transcription. In this paper, we address singing onset detection with three main contributions. First, we outline the nature of a singing voice and present a new singing onset detection approach based on supervised machine learning. In this approach, two Gaussian Mixture Models (GMMs) are used to classify audio features of onset frames and non-onset frames. Second, existing audio features are thoroughly evaluated for this approach to singing onset detection. Third, feature-level and decision-level fusion are employed to fuse different features for a higher level of performance. Evaluated on a recorded singing database, the proposed approach outperforms state-of-the-art onset detection algorithms significantly."
78,Olivier Lartillot;Tuomas Eerola;Petri Toiviainen;José Fornari,"Multi-Feature Modeling of Pulse Clarity: Design, Validation and Optimization.",2008,https://doi.org/10.5281/zenodo.1415514,"Olivier Lartillot+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Tuomas Eerola+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Petri Toiviainen+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education;Jose Fornari+Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvaskyla>FIN>education","Pulse clarity is considered as a high-level musical dimension that conveys how easily in a given musical piece, or a particular moment during that piece, listeners can perceive the underlying rhythmic or metrical pulsation. The objective of this study is to establish a composite model explaining pulse clarity judgments from the analysis of audio recordings. A dozen of descriptors have been designed, some of them dedicated to low-level characterizations of the onset detection curve, whereas the major part concentrates on descriptions of the periodicities developed throughout the temporal evolution of music. A high number of variants have been derived from the systematic exploration of alternative methods proposed in the literature on onset detection curve estimation. To evaluate the pulse clarity model and select the best predictors, 25 participants have rated the pulse clarity of one hundred excerpts from movie soundtracks. The mapping between the model predictions and the ratings was carried out via regressions. Nearly a half of listeners’ rating variance can be explained via a combination of periodicity-based factors."
79,Emmanuel Ravelli;Gaël Richard;Laurent Daudet,Fast MIR in a Sparse Transform Domain.,2008,https://doi.org/10.5281/zenodo.1417002,Emmanuel Ravelli+Université Paris 6>FRA>education|TELECOM ParisTech>FRA>education;Gaël Richard+TELECOM ParisTech>FRA>education;Laurent Daudet+Université Paris 6>FRA>education,"We consider in this paper sparse audio coding as an alternative to transform audio coding for efficient MIR in the transform domain. We use an existing audio coder based on a sparse representation in a union of MDCT bases, and propose a fast algorithm to compute mid-level representations for beat tracking and chord recognition, respectively an onset detection function and a chromagram. The resulting transform domain system is significantly faster than a comparable state-of-the-art system while obtaining close performance above 8 kbps."
80,Arturo Camacho,Detection of Pitched/Unpitched Sound using Pitch Strength Clustering.,2008,https://doi.org/10.5281/zenodo.1418169,Arturo Camacho+University of Florida>USA>education,"A method for detecting pitched/unpitched sound is presented. The method tracks the pitch strength trace of the signal, determining clusters of pitch and unpitched sound. The criterion used to determine the clusters is the local maximization of the distance between the centroids. The method makes no assumption about the data except that the pitched and unpitched clusters have different centroids. This allows the method to dispense with free parameters. The method is shown to be more reliable than using fixed thresholds when the SNR is unknown."
81,John Woodruff;Yipeng Li;DeLiang Wang,Resolving Overlapping Harmonics for Monaural Musical Sound Separation using Fundamental Frequency and Common Amplitude Modulation.,2008,https://doi.org/10.5281/zenodo.1417279,John Woodruff+The Ohio State University>USA>education;Yipeng Li+The Ohio State University>USA>education;DeLiang Wang+The Ohio State University>USA>education,"In mixtures of pitched sounds, the problem of overlapping harmonics poses a significant challenge to monaural musical sound separation systems. In this paper we present a new algorithm for sinusoidal parameter estimation of overlapping harmonics for pitched instruments. Our algorithm is based on the assumptions that harmonics of the same source have correlated amplitude envelopes and the phase change of harmonics can be accurately predicted from an instrument’s pitch. We exploit these two assumptions in a least-squares estimation framework to resolve overlapping harmonics. This new algorithm is incorporated into a separation system and quantitative evaluation shows that the resulting system performs significantly better than an existing monaural music separation system for mixtures of harmonic instruments."
82,Bernhard Niedermayer,Non-Negative Matrix Division for the Automatic Transcription of Polyphonic Music.,2008,https://doi.org/10.5281/zenodo.1415040,Bernhard Niedermayer+Johannes Kepler University Linz>AUT>education,"In this paper we present a new method in the style of non-negative matrix factorization for automatic transcription of polyphonic music played by a single instrument (e.g., a piano). We suggest using a fixed repository of base vectors corresponding to tone models of single pitches played on a certain instrument. This assumption turns the blind factorization into a kind of non-negative matrix division for which an algorithm is presented. The same algorithm can be applied for learning the model dictionary from sample tones as well. This method is biased towards the instrument used during the training phase. But this is admissible in applications like performance analysis of solo music. The proposed approach is tested on a Mozart sonata where a symbolic representation is available as well as the recording on a computer controlled grand piano."
83,Adrien Daniel;Valentin Emiya;Bertrand David,Perceptually-Based Evaluation of the Errors Usually Made When Automatically Transcribing Music.,2008,https://doi.org/10.5281/zenodo.1417155,Adrien Daniel+TELECOM ParisTech (ENST)>FRA>education|CNRS LTCI>FRA>facility;Valentin Emiya+TELECOM ParisTech (ENST)>FRA>education|CNRS LTCI>FRA>facility;Bertrand David+TELECOM ParisTech (ENST)>FRA>education|CNRS LTCI>FRA>facility,"This paper investigates the perceptual importance of typical errors occurring when transcribing polyphonic music excerpts into a symbolic form. The case of the automatic transcription of piano music is taken as the target application and two subjective tests are designed. The main test aims at understanding how human subjects rank typical transcription errors such as note insertion, deletion or replacement, note doubling, incorrect note onset or duration, and so forth. The Bradley-Terry-Luce (BTL) analysis framework is used and the results show that pitch errors are more clearly perceived than incorrect loudness estimations or temporal deviations from the original recording. A second test presents a first attempt to include this information in more perceptually motivated measures for evaluating transcription systems."
84,Benjamin Fields;Christophe Rhodes;Michael A. Casey;Kurt Jacobson,Social Playlists and Bottleneck Measurements: Exploiting Musician Social Graphs Using Content-Based Dissimilarity and Pairwise Maximum Flow Values.,2008,https://doi.org/10.5281/zenodo.1417939,"Ben Fields+Goldsmiths, University of London>GBR>education;Christophe Rhodes+Goldsmiths, University of London>GBR>education;Michael Casey+Goldsmiths, University of London>GBR>education;Kurt Jacobson+Queen Mary, University of London>GBR>education","We have sampled the artist social network of Myspace and to it applied the pairwise relational connectivity measure Minimum cut/Maximum flow. These values are then compared to a pairwise acoustic Earth Mover’s Distance measure and the relationship is discussed. Further, a means of constructing playlists using the maximum flow value to exploit both the social and acoustic distances is realized."
85,François Deliège;Bee Yong Chua;Torben Bach Pedersen,High-Level Audio Features: Distributed Extraction and Similarity Search.,2008,https://doi.org/10.5281/zenodo.1416514,François Deliège+Aalborg University>DNK>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Bee Yong Chua+Aalborg University>DNK>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown;Torben Bach Pedersen+Aalborg University>DNK>education|Unknown>Unknown>Unknown|Unknown>Unknown>Unknown,"Today, automatic extraction of high-level audio features suffers from two main scalability issues. First, the extraction algorithms are very demanding in terms of memory and computation resources. Second, copyright laws prevent the audio files to be shared among computers, limiting the use of existing distributed computation frameworks and reducing the transparency of the methods evaluation process. The iSound Music Warehouse (iSoundMW), presented in this paper, is a framework to collect and query high-level audio features. It performs the feature extraction in a two-step process that allows distributed computations while respecting copyright laws. Using public computers, the extraction can be performed on large scale music collections. However, to be truly valuable, data management tools to search among the extracted features are needed. The iSoundMW enables similarity search among the collected high-level features and demonstrates its flexibility and efficiency by using a weighted combination of high-level features and constraints while showing good search performance results."
86,Parag Chordia;Mark Godfrey;Alex Rae,Extending Content-Based Recommendation: The Case of Indian Classical Music.,2008,https://doi.org/10.5281/zenodo.1415132,Parag Chordia+Georgia Tech>USA>education;Mark Godfrey+Georgia Tech>USA>education;Alex Rae+Georgia Tech>USA>education,"We describe a series of experiments that attempt to create a content-based similarity model suitable for making recommendations about North Indian classical music (NICM). We introduce a dataset (nicm2008) consisting of 897 tracks of NICM along with substantial ground-truth annotations, including artist, predominant instrument, tonic pitch, raag, and parent scale (thaat). Using a timbre-based similarity model derived from short-time MFCCs we find that artist R-precision is 32.69% and that the predominant instrument is correctly classified 90.30% of the time. Consistent with previous work, we find that certain tracks (“hubs”) appear falsely similar to many other tracks. We find that this problem can be attenuated by model homogenization. We also introduce the use of pitch-class distribution (PCD) features to measure melodic similarity. Its effectiveness is evaluated by raag R-precision (16.97%), thaat classification accuracy (75.83%), and comparison to reference similarity metrics. We propose that a hybrid timbral-melodic similarity model may be effective for Indian classical music recommendation. Further, this work suggests that “hubs” are a general features of such similarity modeling that may be partially alleviated by model homogenization."
87,Michael I. Mandel;Daniel P. W. Ellis,Multiple-Instance Learning for Music Information Retrieval.,2008,https://doi.org/10.5281/zenodo.1418299,Michael I. Mandel+Columbia University>USA>education|LabROSA>USA>facility;Daniel P. W. Ellis+Columbia University>USA>education|LabROSA>USA>facility,"Multiple-instance learning algorithms train classifiers from lightly supervised data, i.e. labeled collections of items, rather than labeled items. We compare the multiple-instance learners mi-SVM and MILES on the task of classifying 10-second song clips. These classifiers are trained on tags at the track, album, and artist levels, or granularities, that have been derived from tags at the clip granularity, allowing us to test the effectiveness of the learners at recovering the clip labeling in the training set and predicting the clip labeling for a held-out test set. We find that mi-SVM is better than a control at the recovery task on training clips, with an average classification accuracy as high as 87% over 43 tags; on test clips, it is comparable to the control with an average classification accuracy of up to 68%. MILES performed adequately on the recovery task, but poorly on the test clips."
88,Ioannis Panagakis;Emmanouil Benetos;Constantine Kotropoulos,Music Genre Classification: A Multilinear Approach.,2008,https://doi.org/10.5281/zenodo.1414714,Ioannis Panagakis+Aristotle University of Thessaloniki>GRC>education;Emmanouil Benetos+Aristotle University of Thessaloniki>GRC>education;Constantine Kotropoulos+Aristotle University of Thessaloniki>GRC>education,"In this paper, music genre classification is addressed in a multilinear perspective. Inspired by a model of auditory cortical processing, multiscale spectro-temporal modulation features are extracted. Such spectro-temporal modulation features have been successfully used in various content-based audio classification tasks recently, but not yet in music genre classification. Each recording is represented by a third-order feature tensor generated by the auditory model. Thus, the ensemble of recordings is represented by a fourth-order data tensor created by stacking the third-order feature tensors associated to the recordings. To handle large data tensors and derive compact feature vectors suitable for classification, three multilinear subspace techniques are examined, namely the Non-Negative Tensor Factorization (NTF), the High-Order Singular Value Decomposition (HOSVD), and the Multilinear Principal Component Analysis (MPCA). Classification is performed by a Support Vector Machine. Stratified cross-validation tests on the GTZAN dataset and the ISMIR 2004 Genre one demonstrate the advantages of NTF and HOSVD versus MPCA. The best accuracies obtained by the proposed multilinear approach is comparable with those achieved by state-of-the-art music genre classification algorithms."
89,Patrick Rabbat;François Pachet,Direct and Inverse Inference in Music Databases: How to Make at Song Funk?,2008,https://doi.org/10.5281/zenodo.1416486,Patrick Rabbat+Sony CSL>FRA>company;François Pachet+Sony CSL>FRA>company,"We propose an algorithm for exploiting statistical properties of large-scale metadata databases about music titles to answer musicological queries. We introduce two inference schemes called “direct” and “inverse” inference, based on an efficient implementation of a kernel regression approach. We describe an evaluation experiment conducted on a large-scale database of fine-grained musical metadata. We use this database to train the direct inference algorithm, test it, and also to identify the optimal parameters of the algorithm. The inverse inference algorithm is based on the direct inference algorithm. We illustrate it with some examples."
90,Cory McKay;Ichiro Fujinaga,"Combining Features Extracted from Audio, Symbolic and Cultural Sources.",2008,https://doi.org/10.5281/zenodo.1415576,Cory McKay+McGill University>CAN>education;Ichiro Fujinaga+McGill University>CAN>education,"This paper experimentally investigates the classification utility of combining features extracted from separate audio, symbolic and cultural sources of musical information. This was done via a series of genre classification experiments performed using all seven possible combinations and subsets of the three corresponding types of features. These experiments were performed using jMIR, a software suite designed for use both as a toolset for performing MIR research and as a platform for developing and sharing new algorithms. The experimental results indicate that combining feature types can indeed substantively improve classification accuracy. Accuracies of 96.8% and 78.8% were attained respectively on 5 and 10-class genre taxonomies when all three feature types were combined, compared to average respective accuracies of 85.5% and 65.1% when features extracted from only one of the three sources of data were used. It was also found that combining feature types decreased the seriousness of those misclassifications that were made, on average, particularly when cultural features were included."
91,Pierre-Antoine Manzagol;Thierry Bertin-Mahieux;Douglas Eck,On the Use of Sparce Time Relative Auditory Codes for Music.,2008,https://doi.org/10.5281/zenodo.1415034,Pierre-Antoine Manzagol+Université de Montréal>CAN>education;Thierry Bertin-Mahieux+Université de Montréal>CAN>education;Douglas Eck+Université de Montréal>CAN>education,"Many if not most audio features used in MIR research are inspired by work done in speech recognition and are variations on the spectrogram. Recently, much attention has been given to new representations of audio that are sparse and time-relative. These representations are efficient and able to avoid the time-frequency trade-off of a spectrogram. Yet little work with music streams has been conducted and these features remain mostly unused in the MIR community. In this paper we further explore the use of these features for musical signals. In particular, we investigate their use on realistic music examples (i.e. released commercial music) and their use as input features for supervised learning. Furthermore, we identify three specific issues related to these features which will need to be further addressed in order to obtain the full benefit for MIR applications."
92,Tomonori Izumitani;Kunio Kashino,A Robust Musical Audio Search Method Based on Diagonal Dynamic Programming Matching of Self-Similarity Matrices.,2008,https://doi.org/10.5281/zenodo.1417399,Tomonori Izumitani+NTT Communication Science Laboratories>JPN>facility;Kunio Kashino+NTT Communication Science Laboratories>JPN>facility,"We propose a new musical audio search method based on audio signal matching that can cope with key and tempo variations. The method employs the self-similarity matrix of an audio signal to represent a key-invariant structure of musical audio. And, we use dynamic programming (DP) matching of self-similarity matrices to deal with time variations. However, conventional DP-based sequence matching methods cannot be directly applied for self-similarity matrices because they cannot treat gaps independently of other time frames. We resolve this problem by introducing “matched element indices,” which reflect the history of matching, to a DP-based sequence matching method. We performed experiments using musical audio signals. The results indicate that the proposed method improves the detection accuracy in comparison to that that obtained by two conventional methods, namely, DP matching with chroma-based vector rotations and a simple matching of self-similarity feature vectors."
93,Luke Barrington;Mehrdad Yazdani;Douglas Turnbull;Gert R. G. Lanckriet,Combining Feature Kernels for Semantic Music Retrieval.,2008,https://doi.org/10.5281/zenodo.1415070,"Luke Barrington+University of California, San Diego>USA>education;Mehrdad Yazdani+University of California, San Diego>USA>education;Douglas Turnbull+University of California, San Diego>USA>education;Gert Lanckriet+University of California, San Diego>USA>education","We apply a new machine learning tool, kernel combination, to the task of semantic music retrieval. We use 4 different types of acoustic content and social context feature sets to describe a large music corpus and derive 4 individual kernel matrices from these feature sets. Each kernel is used to train a support vector machine (SVM) classifier for each semantic tag (e.g., ‘aggressive’, ‘classic rock’, ‘distorted electric guitar’) in a large tag vocabulary. We examine the individual performance of each feature kernel and then show how to learn an optimal linear combination of these kernels using convex optimization. We find that the retrieval performance of the SVMs trained using the combined kernel is superior to SVMs trained using the best individual kernel for a large number of tags. In addition, the weights placed on individual kernels in the linear combination reflect the relative importance of each feature set when predicting a tag."
94,Yusuke Tsuchihashi;Tetsuro Kitahara;Haruhiro Katayose,Using Bass-line Features for Content-Based MIR.,2008,https://doi.org/10.5281/zenodo.1417895,"Yusuke Tsuchihashi+Kwansei Gakuin University>JPN>education;Tetsuro Kitahara+Kwansei Gakuin University>JPN>education|CrestMuse Project, CREST, JST>JPN>facility;Haruhiro Katayose+Kwansei Gakuin University>JPN>education|CrestMuse Project, CREST, JST>JPN>facility","We propose new audio features that can be extracted from bass lines. Most previous studies on content-based music information retrieval (MIR) used low-level features such as the mel-frequency cepstral coefficients and spectral centroid. Musical similarity based on these features works well to some extent but has a limit to capture fine musical characteristics. Because bass lines play important roles in both harmonic and rhythmic aspects and have a different style for each music genre, our bass-line features are expected to improve the similarity measure and classification accuracy. Furthermore, it is possible to achieve a similarity measure that enhances the bass-line characteristics by weighting the bass-line and other features. Results for applying our features to automatic genre classification and music collection visualization showed that our features improved genre classification accuracy and did achieve a similarity measure that enhances bass-line characteristics."
95,Nicolas Scaringella,Timbre and Rhythmic TRAP-TANDEM Features for Music Information Retrieval.,2008,https://doi.org/10.5281/zenodo.1418113,Nicolas Scaringella+Idiap Research Institute>CHE>facility|Ecole Polytechnique Fédérale de Lausanne (EPFL)>CHE>education,"The enormous growth of digital music databases has led to a comparable growth in the need for methods that help users organize and access such information. One area in particular that has seen much recent research activity is the use of automated techniques to describe audio content and to allow for its identification, browsing and retrieval. Conventional approaches to music content description rely on features characterizing the shape of the signal spectrum in relatively short-term frames. In the context of Automatic Speech Recognition (ASR), Hermansky described an interesting alternative to short-term spectrum features, the TRAP-TANDEM approach which uses long-term band-limited features trained in a supervised fashion. We adapt this idea to the specific case of music signals and propose a generic system for the description of temporal patterns. The same system with different settings is able to extract features describing either timbre or rhythmic content. The quality of the generated features is demonstrated in a set of music retrieval experiments and compared to other state-of-the-art models."
96,Patrick Flanagan,Quantifying Metrical Ambiguity.,2008,https://doi.org/10.5281/zenodo.1417117,Patrick Flanagan+Unknown>Unknown>Unknown,"This paper explores how data generated by meter induction models may be recycled to quantify metrical ambiguity, which is calculated by measuring the dispersion of metrical induction strengths across a population of possible meters. A measure of dispersion commonly used in economics to measure income inequality, the Gini coefficient, is introduced for this purpose. The value of this metric as a rhythmic descriptor is explored by quantifying the ambiguity of several common clave patterns and comparing the results to other metrics of rhythmic complexity and syncopation."
97,Ernesto Trajano de Lima;Geber Ramalho,On Rhythmic Pattern Extraction in Bossa Nova Music.,2008,https://doi.org/10.5281/zenodo.1417959,Ernesto Trajano de Lima+Centro de Informática (CIn)—Univ. Federal de Pernambuco>BRA>education;Geber Ramalho+Centro de Informática (CIn)—Univ. Federal de Pernambuco>BRA>education,"The analysis of expressive performance, an important research topic in Computer Music, is almost exclusively devoted to the study of Western Classical piano music. Instruments like the acoustic guitar and styles like Bossa Nova and Samba have been little studied, despite their harmonic and rhythmic richness. This paper describes some experimental results obtained with the extraction of rhythmic patterns from the guitar accompaniment of Bossa Nova songs. The songs, played by two different performers and recorded with the help of a MIDI guitar, were represented as strings and processed by FlExPat, a string matching algorithm. The results obtained were then compared to a previously acquired catalogue of “good” patterns."
98,Matthew Wright;W. Andrew Schloss;George Tzanetakis,Analyzing Afro-Cuban Rhythms using Rotation-Aware Clave Template Matching with Dynamic Programming.,2008,https://doi.org/10.5281/zenodo.1416356,Matthew Wright+University of Victoria>CAN>education;W. Andrew Schloss+University of Victoria>CAN>education;George Tzanetakis+University of Victoria>CAN>education,"The majority of existing research in Music Information Retrieval (MIR) has focused on either popular or classical music and frequently makes assumptions that do not generalize to other music cultures. We use the term Computational Ethnomusicology (CE) to describe the use of computer tools to assist the analysis and understanding of musics from around the world. Although existing MIR techniques can serve as a good starting point for CE, the design of effective tools can benefit from incorporating domain-specific knowledge about the musical style and culture of interest. In this paper we describe our realization of this approach in the context of studying Afro-Cuban rhythm. More specifically we show how computer analysis can help us characterize and appreciate the complexities of tracking tempo and analyzing micro-timing in these particular music styles. A novel template-based method for tempo tracking in rhythmically complex Afro-Cuban music is proposed. Although our approach is domain-specific, we believe that the concepts and ideas used could also be used for studying other music cultures after some adaptation."
99,Andre Holzapfel;Yannis Stylianou,Beat Tracking using Group Delay Based Onset Detection.,2008,https://doi.org/10.5281/zenodo.1415516,"Andre Holzapfel+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education;Yannis Stylianou+Institute of Computer Science, FORTH>GRC>facility|University of Crete>GRC>education","""This paper introduces a novel approach to estimate onsets in musical signals based on the phase spectrum and specifically using the average of the group delay function. A frame-by-frame analysis of a music signal provides the evolution of group delay over time, referred to as phase slope function. Onsets are then detected simply by locating the positive zero-crossings of the phase slope function. The proposed approach is compared to an amplitude-based onset detection approach in the framework of a state-of-the-art system for beat tracking. On a data set of music with less percussive content, the beat tracking accuracy achieved by the system is improved by 82% when the suggested phase-based onset detection approach is used instead of the amplitude-based approach, while on a set of music with stronger percussive characteristics both onset detection approaches provide comparable results of accuracy."""
100,Linxing Xiao;Aibo Tian;Wen Li;Jie Zhou,Using Statistic Model to Capture the Association between Timbre and Perceived Tempo.,2008,https://doi.org/10.5281/zenodo.1414922,Linxing Xiao+Tsinghua University>CHN>education;Aibo Tian+Tsinghua University>CHN>education;Wen Li+Tsinghua University>CHN>education;Jie Zhou+Tsinghua University>CHN>education,"The estimation of the perceived tempo is required in many MIR applications. However, automatic tempo estimation itself is still an open problem due to the insufficient understanding of the inherent mechanisms of the tempo perception. Published methods only use the information of rhythm pattern, so they may meet the half/double tempo error problem. To solve this problem, we propose to use a statistic model to investigate the association between timbre and tempo and use timbre information to improve the performance of tempo estimation. Experiment results show that this approach performs at least comparably to existing tempo extraction algorithms."
101,Eric Thul;Godfried T. Toussaint,Rhythm Complexity Measures: A Comparison of Mathematical Models of Human Perception and Performance.,2008,https://doi.org/10.5281/zenodo.1416218,Eric Thul+McGill University>CAN>education;Godfried T. Toussaint+McGill University>CAN>education,"Thirty two measures of rhythm complexity are compared using three widely different rhythm data sets. Twenty-two of these measures have been investigated in a limited context in the past, and ten new measures are explored here. Some of these measures are mathematically inspired, some were designed to measure syncopation, some were intended to predict various measures of human performance, some are based on constructs from music theory, such as Pressing’s cognitive complexity, and others are direct measures of different aspects of human performance, such as perceptual complexity, meter complexity, and performance complexity. In each data set the rhythms are ranked either according to increasing complexity using the judgements of human subjects, or using calculations with the computational models. Spearman rank correlation coefficients are computed between all pairs of rhythm rankings. Then phylogenetic trees are used to visualize and cluster the correlation coefficients. Among the many conclusions evident from the results, there are several observations common to all three data sets that are worthy of note. The syncopation measures form a tight cluster far from other clusters. The human performance measures fall in the same cluster as the syncopation measures. The complexity measures based on statistical properties of the inter-onset-interval histograms are poor predictors of syncopation or human performance complexity. Finally, this research suggests several open problems."
102,Mitsunori Ogihara;Tao Li 0001,N-Gram Chord Profiles for Composer Style Representation.,2008,https://doi.org/10.5281/zenodo.1415956,Mitsunori Ogihara+University of Miami>USA>education;Tao Li+Florida International University>USA>education,"This paper studies the problem of using weighted N-grams of chord sequences to construct the profile of a composer. The N-gram profile of a chord sequence is the collection of all N-grams appearing in a sequence where each N-gram is given a weight proportional to its beat count. The N-gram profile of a collection of chord sequences is the simple average of the N-gram profile of all the chord sequences in the collection. Similarity of two composers is measured by the cosine of their respective profiles, which has a value in the range [0, 1]. Using the cosine-based similarity, a group of composers is clustered into a hierarchy, which appears to be explicable. Also, the composition style can be identified using N-gram signatures."
103,Kjell Lemström;Niko Mikkilä;Veli Mäkinen,Fast Index Based Filters for Music Retrieval.,2008,https://doi.org/10.5281/zenodo.1416774,Kjell Lemström+University of Helsinki>FIN>education;Niko Mikkilä+University of Helsinki>FIN>education;Veli Mäkinen+University of Helsinki>FIN>education,"We consider two content-based music retrieval problems where the music is modeled as sets of points in the Euclidean plane, formed by the (on-set time, pitch) pairs. We introduce fast filtering methods based on indexing the underlying database. The filters run in a sublinear time in the length of the database, and they are lossless if a quadratic space may be used. By taking into account the application, the search space can be narrowed down, obtaining practically lossless filters using linear size index structures. For the checking phase, which dominates the overall running time, we exploit previously designed algorithms suitable for local checking. In our experiments on a music database, our best filter-based methods performed several orders of a magnitude faster than previous solutions."
104,Rainer Typke;Agatha Walczak-Typke,A Tunneling-Vantage Indexing Method for Non-Metrics.,2008,https://doi.org/10.5281/zenodo.1417307,R. Typke+Austrian Research Institute for Artificial Intelligence (OFAI)>AUT>facility|University of Vienna>AUT>education;A. C. Walczak-Typke+University of Vienna>AUT>education,"We consider an instance of the Earth Mover’s Distance (EMD) useful for comparing rhythmical patterns. To make searches for r-near neighbours efficient, we decompose our search space into disjoint metric subspaces, in each of which the EMD reduces to the l1 norm. We then use a combined approach of two methods, one for searching within the subspaces, the other for searching between them. For the former, we show how one can use vantage indexing without false positives nor false negatives for solving the exact r-near neighbour problem, and find an optimum number and placement of vantage objects for this result. For searching between subspaces, where the EMD is not a metric, we show how one can guarantee that still no false negatives occur, and the percentage of false positives is reduced as the search radius is increased."
