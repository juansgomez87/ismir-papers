Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,E. Allamanche;Jürgen Herre;Oliver Hellmuth;T. Kastner;C. Ertel,A multiple feature model for musical similarity retrieval.,2003,https://doi.org/10.5281/zenodo.1416684,"Eric Allamanche, Fraunhofer Institut Integrierte Schaltungen, IIS;J¨urgen Herre, Fraunhofer Institut Integrierte Schaltungen, IIS;Oliver Hellmuth, Fraunhofer Institut Integrierte Schaltungen, IIS;Thorsten Kastner, Fraunhofer Institut Integrierte Schaltungen, IIS;Christian Ertel, Fraunhofer Institut Integrierte Schaltungen, IIS","Despite the “fuzzy” nature of musical similarity, which varies from one person to another, perceptual low level features combined with appropriate classification schemes have proven to perform satisfactorily for this task. Since a single feature only captures some selective characteristics of an audio signal, this information may, in some cases, not be sufficient to properly identify similarities between songs. This paper presents a system which combines a set of acoustic features for the task of retrieving similar sounding songs. The methodology for optimum feature selection and combination is explained, and the system’s performance is assessed by means of a subjective listening test."
1,Vlora Arifi;Michael Clausen;Frank Kurth;Meinard Müller,"Automatic synchronization of music data in score-, MIDI- and PCM-format.",2003,https://doi.org/10.5281/zenodo.1417848,"Vlora Ariﬁ, Universit¨at Bonn, Institut f¨ur Informatik III;Michael Clausen, Universit¨at Bonn, Institut f¨ur Informatik III;Frank Kurth, Universit¨at Bonn, Institut f¨ur Informatik III;Meinard M¨uller, Universit¨at Bonn, Institut f¨ur Informatik III","In this paper, the authors present algorithms for the automatic time-synchronization of score-, MIDI- or PCM-data streams representing the same polyphonic piano piece. These synchronization algorithms have applications in various scenarios, such as music retrieval, automatic annotation, tempo studies, and score tracking during a performance. The authors focus on three representative data formats: symbolic score format, physical PCM-format, and MIDI-format. They have developed synchronization algorithms for these formats, including Score-to-MIDI (SM) synchronization, Score-to-PCM (SP) synchronization, and MIDI-to-PCM (MP) synchronization. Due to space limitations, an overview of related work is not provided, but links to relevant literature can be found in the full paper version."
2,David Bainbridge 0001;Sally Jo Cunningham;J. Stephen Downie,Analysis of queries to a Wizard-of-Oz MIR system: Challenging assumptions about what people really want.,2003,https://doi.org/10.5281/zenodo.1416850,"David Bainbridge, University of Waikato;Sally Jo Cunningham, University of Waikato;J. Stephen Downie, University of Illinois at Urbana-Champaign","How do users of music information retrieval (MIR) systems express their needs? Using a Wizard of Oz approach to system evaluation, combined with a grounded theory analysis of 502 real-world music queries posted to Google Answers, this paper addresses this pivotal question."
3,Adam Berenzweig;Beth Logan;Daniel P. W. Ellis;Brian Whitman,A large-scale evalutation of acoustic and subjective music similarity measures.,2003,https://doi.org/10.5281/zenodo.1417010,"Adam Berenzweig, LabROSA;Beth Logan, HP Labs;Daniel P.W. Ellis, LabROSA;Brian Whitman, Music Mind & Machine Group","Subjective similarity between musical pieces and artists is an elusive concept, but one that must be pursued in support of applications to provide automatic organization of large music collections. In this paper, we examine both acoustic and subjective approaches for calculating similarity between artists, comparing their performance on a common database of 400 popular artists. Specifically, we evaluate acoustic techniques based on Mel-frequency cepstral coefficients and an intermediate 'anchor space' of genre classification, and subjective techniques which use data from The All Music Guide, from a survey, from playlists and personal collections, and from web-text mining. We find the following: (1) Acoustic-based measures can achieve agreement with ground truth data that is at least comparable to the internal agreement between different subjective sources. However, we observe significant differences between superficially similar distribution modeling and comparison techniques. (2) Subjective measures from diverse sources show reasonable agreement, with the measure derived from co-occurrence in personal music collections being the most reliable overall. (3) Our methodology for large-scale cross-site music similarity evaluations is practical and convenient, yielding directly comparable numbers for different approaches. In particular, we hope that our information-retrieval-based approach to scoring similarity measures, our paradigm of sharing common feature representations, and even our particular dataset of features for 400 artists, will be useful to other researchers."
4,Elaine Chew;Yun-Ching Chen,Determining context-defining windows: Pitch spelling using the spiral array.,2003,https://doi.org/10.5281/zenodo.1418037,"Elaine Chew, University of Southern California, Los Angeles, California, USA;Yun-Ching Chen, University of Southern California, Los Angeles, California, USA","This paper presents algorithms for pitch spelling using the Spiral Array model. The algorithms determine contextually consistent letter names for pitch numbers, which is important for music transcription and analysis systems. The paper discusses three real-time pitch spelling algorithms based on context-defining windows. The algorithms use the Spiral Array model to determine local and long-term tonal contexts and assign appropriate letter names to pitches. The algorithms are evaluated and compared based on their error rates."
5,Roger B. Dannenberg;William P. Birmingham;George Tzanetakis;Colin Meek;Ning Hu;Bryan Pardo,The MUSART testbed for query-by-humming evaluation.,2003,https://doi.org/10.5281/zenodo.1415978,"Roger B. Dannenberg, Carnegie Mellon University;William P. Birmingham, University of Michigan;George Tzanetakis, Carnegie Mellon University;Colin Meek, Carnegie Mellon University;Ning Hu, Carnegie Mellon University;Bryan Pardo, Carnegie Mellon University","Evaluating music information retrieval systems is acknowledged to be a difficult problem. We have created a database and a software testbed for the systematic evaluation of various query-by-humming (QBH) search systems. As might be expected, different queries and different databases lead to wide variations in observed search precision. “Natural” queries from two sources led to lower performance than that typically reported in the QBH literature. These results point out the importance of careful measurement and objective comparisons to study retrieval algorithms. This study compares search algorithms based on note-interval matching with dynamic programming, fixed-frame melodic contour matching with dynamic time warping, and a hidden Markov model. An examination of scaling trends is encouraging: precision falls off very slowly as the database size increases. This trend is simple to compute and could be useful to predict performance on larger databases."
6,Stephen Davison,The Sheet Music Consortium: A Specialized Open Archives Initiative harvester project.,2003,https://doi.org/10.5281/zenodo.1417731,"Stephen Davison, University of California, Los Angeles Music Library;Cynthia Requardt, The Johns Hopkins University Special Collections, Eisenhower Library;Kristine Brancolini, Indiana University Digital Library Program","The Open Archives Initiative (OAI) Sheet Music Project is a consortium of institutions building OAI-compliant data providers, a metadata harvester, and a web-based service provider for digital sheet music collections. The project aims to test the viability of the OAI standard for providing access to sheet music collections on the web, and to build a permanent and increasingly participatory service for the discovery of digital sheet music. The service provider design has been informed by detailed usability testing, and by limitations imposed by the variations in metadata harvested from the different participating collections. Advanced services in addition to basic searching and browsing have been developed, including the ability to save and share subsets across participating collections. Harvesting and searching strategies for overcoming metadata limitations are being developed. The consortium is seeking additional participants with digital sheet music collections, and is exploring the possibility of incorporating scores and audio into the project."
7,Tom De Mulder;Jean-Pierre Martens;Micheline Lesaffre;Marc Leman;Bernard De Baets,An auditory model based transriber of vocal queries.,2003,https://doi.org/10.5281/zenodo.1416492,"Tom De Mulder, ELIS, Ghent University;Jean-Pierre Martens, ELIS, Ghent University;Micheline Lesaffre, IPEM, Ghent University;Marc Leman, IPEM, Ghent University;Bernard De Baets, KERMIT, Ghent University;Hans De Meyer, KERMIT, Ghent University","In this paper a new auditory model-based transcriber of vocal melodic queries is presented. Our experiments show that the new system can transcribe queries with an accuracy between 76 % (whistling) and 85 % (singing with syllables), and that it outperforms four state-of-the-art systems it was compared with."
8,Simon Dixon;Elias Pampalk;Gerhard Widmer,Classification of dance music by periodicity patterns.,2003,https://doi.org/10.5281/zenodo.1414936,"Simon Dixon, Austrian Research Institute for AI;Elias Pampalk, Austrian Research Institute for AI;Gerhard Widmer, Austrian Research Institute for AI and Department of Medical Cybernetics and AI, University of Vienna","This paper addresses the genre classification problem for a specific subset of music, standard and Latin ballroom dance music, using a classification method based only on timing information. We compare two methods of extracting periodicities from audio recordings in order to find the metrical hierarchy and timing patterns by which the style of the music can be recognized: the first method performs onset detection and clustering of inter-onset intervals; the second uses autocorrelation on the amplitude envelopes of band-limited versions of the signal as its method of periodicity detection. The relationships between periodicities are then used to find the metrical hierarchy and to estimate the tempo at the beat and measure levels of the hierarchy. The periodicities are then interpreted as musical note values, and the estimated tempo, meter and the distribution of periodicities are used to predict the style of music using a simple set of rules. The methods are evaluated with a test set of standard and Latin dance music, for which the style and tempo are given on the CD cover, providing a ""ground truth"" by which the automatic classification can be measured."
9,Shyamala Doraisamy;Stefan M. Rüger,Position Indexing of Adjacent and Concurrent N-Grams for Polyphonic Music Retrieval.,2003,https://doi.org/10.5281/zenodo.1417919,"Shyamala Doraisamy, Department of Computing, South Kensington Campus, Imperial College London;Stefan Rüger, Department of Computing, South Kensington Campus, Imperial College London","In this paper, the authors examine the retrieval performance of adjacent and concurrent n-grams generated from polyphonic music data. They use a method to index polyphonic music using a word position indexer with the n-gram approach. The feasibility of utilizing the position information of polyphonic 'musical words' is investigated using various proximity-based and structured query operators available with text retrieval systems. The experiments show that nested phrase operators improve the retrieval performance."
10,J. Stephen Downie,Toward the scientific evaluation of music information retrieval systems.,2003,https://doi.org/10.5281/zenodo.1417121,"J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign","This paper outlines the findings-to-date of a project to assist in the efforts being made to establish a TREC-like evaluation paradigm within the Music Information Retrieval (MIR) research community. The findings and recommendations are based upon expert opinion garnered from members of the Information Retrieval (IR), Music Digital Library (MDL) and MIR communities with regard to the construction and implementation of scientifically valid evaluation frameworks. Proposed recommendations include the creation of data-rich query records that are both grounded in real-world requirements and neutral with respect to retrieval technique(s) being examined; adoption, and subsequent validation, of a “reasonable person” approach to “relevance” assessment; and, the development of a secure, yet accessible, research environment that allows researchers to remotely access the large-scale testbed collection."
11,Jana Eggink;Guy J. Brown,Application of missing feature theory to the recognition of musical instruments in polyphonic audio.,2003,https://doi.org/10.5281/zenodo.1416262,"Jana Eggink, Department of Computer Science, University of Sheffield;Guy J. Brown, Department of Computer Science, University of Sheffield","A system for musical instrument recognition based on a Gaussian Mixture Model (GMM) classifier is introduced. To enable instrument recognition when more than one sound is present at the same time, ideas from missing feature theory are incorporated. Specifically, frequency regions that are dominated by energy from an interfering tone are marked as unreliable and excluded from the classification process. The approach has been evaluated on clean and noisy monophonic recordings, and on combinations of two instrument sounds. These included random chords made from two isolated notes and combinations of two realistic phrases taken from commercially available compact discs. Classification results were generally good, not only when the decision between reliable and unreliable features was based on the knowledge of the clean signal, but also when it was solely based on the harmonic overtone series of the interfering sound."
12,Olivier Gillet;Gaël Richard,Automatic labeling of tabla signals.,2003,https://doi.org/10.5281/zenodo.1418281,"Olivier K. GILLET, GET-ENST (TELECOM Paris);Ga¨el RICHARD, GET-ENST (TELECOM Paris)","Most of the recent developments in the field of music indexing and music information retrieval are focused on western music. In this paper, we present an automatic music transcription system dedicated to Tabla - a North Indian percussion instrument. Our approach is based on three main steps: firstly, the audio signal is segmented in adjacent segments where each segment represents a single stroke. Secondly, rhythmic information such as relative durations are calculated using beat detection techniques. Finally, the transcription (recognition of the strokes) is performed by means of a statistical model based on Hidden Markov Model (HMM). The structure of this model is designed in order to represent the time dependencies between successive strokes and to take into account the specificities of the tabla score notation (transcription symbols may be context dependent). Realtime transcription of Tabla soli (or performances) with an error rate of 6.5% is made possible with this transcriber. The transcription system, along with some additional features such as sound synthesis or phrase correction, are integrated in a user-friendly environment called Tablascope."
13,Masataka Goto,Music scene description project: Toward audio-based real-time music understanding.,2003,https://doi.org/10.5281/zenodo.1415684,"Masataka Goto, Information and Human Activity, PRESTO, JST / National Institute of Advanced Industrial Science and Technology (AIST)","This paper reports a research project intended to build a real-time music-understanding system producing intuitively meaningful descriptions of real-world musical audio signals, such as the melody lines and chorus sections. This paper also introduces our efforts to add correct descriptions (metadata) to the pieces in a music database."
14,Masataka Goto;Hiroki Hashiguchi;Takuichi Nishimura;Ryuichi Oka,RWC Music Database: Music genre database and musical instrument sound database.,2003,https://doi.org/10.5281/zenodo.1415536,"Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Hiroki Hashiguchi, Mejiro University;Takuichi Nishimura, National Institute of Advanced Industrial Science and Technology (AIST);Ryuichi Oka, University of Aizu","This paper describes the design policy and specifications of the RWC Music Database, a copyright-cleared music database compiled specifically for research purposes. The database includes four original component databases: Popular Music Database, Royalty-Free Music Database, Classical Music Database, and Jazz Music Database. The paper also reports the construction of two additional component databases: Music Genre Database and Musical Instrument Sound Database. The goal of the RWC Music Database is to make a significant contribution to the field of music information processing."
15,S. Harford,"Automatic segmentation, learning and retrieval of melodies using a self-organizing neural network.",2003,https://doi.org/10.5281/zenodo.1416182,"Steven Harford, School of Computing, Dublin City University","We introduce a neural network, known as SONNET-MAP, capable of automatic segmentation, learning and retrieval of melodies. SONNET-MAP is a synthesis of the SONNET (Self-Organizing Neural Network) architecture and an associative map derived from ARTMAP. SONNET-MAP automatically segments a melody based on pitch and rhythmic grouping cues. Separate SONNET modules represent the pitch and rhythm dimensions of each segmented phrase independently, with two associative maps fusing these representations at the phrase level. Further SONNET modules aggregate these phrases forming a hierarchical memory structure that encompasses the entire melody. In addition, melodic queries may be used to retrieve any encoded melody. As far as we are aware, SONNET-MAP is the first self-organizing neural network architecture capable of automatically segmenting and retrieving melodies based on both pitch and rhythm."
16,Sung-Phil Heo;Motoyuki Suzuki;Akinori Ito;Shozo Makino,Three-dimensional continuous DP algorithm for multiple pitch candidates in a music information retrieval system.,2003,https://doi.org/10.5281/zenodo.1416862,"Sung-Phil HEO, Graduate School of Information Sciences Tohoku University;Motoyuki SUZUKI, Graduate School of Engineering Tohoku University;Akinori ITO, Graduate School of Engineering Tohoku University;Shozo MAKINO, Graduate School of Engineering Tohoku University","This paper treats theoretical and practical issues that implement a music information retrieval system based on query by humming. In order to extract accuracy features from the user’s humming, we propose a new retrieval method based on multiple pitch candidates. Extracted multiple pitches have shown to be very important parameters in determining melodic similarity, but it is also clear that the confidence measures feature which are obtained from the power are important as well. Furthermore, we propose extending the traditional DP algorithm to three dimensions so that multiple pitch candidates can be treated. Simultaneously, at the melody representation technique, we propose the DP paths are changed dynamically to be able to take relative values so that they can respond to insert or omit notes."
17,Olivier Lartillot,Discovering musical pattern through perceptual heuristics.,2003,https://doi.org/10.5281/zenodo.1417285,"Olivier Lartillot, IRCAM - Centre Pompidou","This paper defends the view that the intricate difficulties challenging the emerging domain of Musical Pattern Discovery, which is dedicated to the automation of motivic analysis, will be overcome only through a thorough taking into account of the specificity of music as a perceptive object. Actual musical patterns, although constantly transformed, are nevertheless perceived by the listener as musical identities. Such dynamical properties of human perception, not reducible to geometrical models, will only be explained with the notions of contexts and expectations. This paper sketches the general principles of a new approach that attempts to build such a general perceptual system. On a sub-cognitive level, patterns are discovered through the detection, by an associative memory, of local similarities. On a cognitive level, patterns are managed by a general logical framework that avoids irrelevant inferences and combinatorial explosion. In this way, actual musical patterns that convey musical significance are discovered. This approach, offering promising results, is a first step toward a complete system of automated music analysis and an explicit modeling of basic mechanisms for music understanding."
18,Kjell Lemström;Veli Mäkinen;Anna Pienimäki;M. Turkia;Esko Ukkonen,The C-BRAHMS project.,2003,https://doi.org/10.5281/zenodo.1417551,"Kjell Lemström, Department of Computer Science, University of Helsinki;Veli Mäkinen, Department of Computer Science, University of Helsinki;Anna Pienimäki, Department of Computer Science, University of Helsinki;Mika Turkia, Department of Computer Science, University of Helsinki;Esko Ukkonen, Department of Computer Science, University of Helsinki",The C-BRAHMS project develops computational methods for content-based retrieval and analysis of music data. A summary of the recent algorithmic and experimental developments of the project is given. The search engine developed by the project is available at http://www.cs.helsinki.ﬁ/group/cbrahms.
19,Micheline Lesaffre;Koen Tanghe;Gaëtan Martens;Dirk Moelants;Marc Leman;Bernard De Baets;Hans De Meyer;Jean-Pierre Martens,The MAMI query-by-voice experiment: collecting and annotating vocal queries for music information retrieval.,2003,https://doi.org/10.5281/zenodo.1418133,"Micheline Lesaffre, IPEM: Department of Musicology, Ghent University;Koen Tanghe, IPEM: Department of Musicology, Ghent University;Gaëtan Martens, Department of Applied Mathematics and Computer Science, Ghent University;Dirk Moelants, IPEM: Department of Musicology, Ghent University;Marc Leman, IPEM: Department of Musicology, Ghent University;Bernard De Baets, Department of Applied mathematics, Biometrics and Process Control, Ghent University;Hans De Meyer, Department of Applied Mathematics and Computer Science, Ghent University;Jean- Pierre Martens, Department of Electronics and Information Systems (ELIS), Ghent University",The MIR research community requires coordinated strategies in dealing with databases for system development and experimentation. Manually annotated files can accelerate the development of accurate analysis tools for music information retrieval. This paper presents background information on an annotated database of vocal queries that is freely available on the Internet. First we outline the design and set up of the experiment through which the vocal queries were generated. Then attention is drawn to the manual annotation of the vocal queries.
20,Tao Li 0001;Mitsunori Ogihara,Detecting emotion in music.,2003,https://doi.org/10.5281/zenodo.1417293,"Tao Li, Department of Computer Science, University of Rochester;Mitsunori Ogihara, Department of Computer Science, University of Rochester","We hypothesize that emotion detection in music can be made by analyzing music signals. We approach to the problem using multi-label classification. We cast the emotion detection problem as a multi-label classification problem, where the music sounds are classified into multiple classes simultaneously. That is a single music sound may be characterized by more than one label, e.g. both “dreamy” and “cheerful.” We divide the process of emotion detection in music into two steps: feature extraction and multi-label classification. In the feature extraction step, we extract from the music signals information representing the music. The features extract should be comprehensive (representing the music very well), compact (requiring a small amount of storage), and effective (not requiring much computation for extraction)."
21,Dan Liu 0001;Lie Lu;HongJiang Zhang,Automatic mood detection from acoustic music data.,2003,https://doi.org/10.5281/zenodo.1418335,"Dan Liu, Department of Automation, Tsinghua University;Lie Lu, Microsoft Research Asia;Hong-Jiang Zhang, Microsoft Research Asia","Music mood describes the inherent emotional meaning of a music clip. It is helpful in music understanding, music search and some music-related applications. In this paper, a hierarchical framework is presented to automate the task of mood detection from acoustic music data, by following some music psychological theories in western cultures. Three feature sets, intensity, timbre and rhythm, are extracted to represent the characteristics of a music clip. Moreover, a mood tracking approach is also presented for a whole piece of music. Experimental evaluations indicate that the proposed algorithms produce satisfactory results."
22,Arie Livshin;Xavier Rodet,The importance of cross database evaluation in musical instrument sound classification: A critical approach.,2003,https://doi.org/10.5281/zenodo.1417771,"Arie Livshin, IRCAM Centre Pompidou;Xavier Rodet, IRCAM Centre Pompidou","In numerous articles (Martin and Kim, 1998; Fraser and Fujinaga, 1999; and many others) sound classification algorithms are evaluated using ""self classification"" - the learning and test groups are randomly selected out of the same sound database. We will show that ""self classification"" is not necessarily a good statistic for the ability of a classification algorithm to learn, generalize or classify well. We introduce the alternative ""Minus-1 DB"" evaluation method and demonstrate that it does not have the shortcomings of ""self classification""."
23,Namunu Chinthaka Maddage;Changsheng Xu;Ye Wang,An SVM-based classification approach to musical audio.,2003,https://doi.org/10.5281/zenodo.1415610,"Namunu Chinthaka Maddage, Institute for Inforcomm Research;Changsheng Xu, Institute for Inforcomm Research;Ye Wang, School of Computing, National University of Singapore","This paper describes an automatic hierarchical music classification approach based on support vector machines (SVM). Based on the proposed method, the music is classified into coursed classes such as vocal, instrumental or vocal mixed with instrumental music. These main classes are further sub-classed according to gender and instrument type. A novel method, Correction Algorithm for Music Sequence (CAMS) has been developed to improve the classification efficiency."
24,Martin F. McKinney;Jeroen Breebaart,Features for audio and music classification.,2003,https://doi.org/10.5281/zenodo.1415026,"Martin F. McKinney, Philips Research Laboratories;Jeroen Breebaart, Philips Research Laboratories","Four audio feature sets are evaluated in their ability to classify ﬁve general audio classes and seven popular music genres. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for both music and audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features."
25,Colin Meek;William P. Birmingham,The dangers of parsimony in query-by-humming applications.,2003,https://doi.org/10.5281/zenodo.1415828,"Colin Meek, University of Michigan;William P. Birmingham, University of Michigan","Query-by-humming systems aim to assist non-expert users in finding a tune or melody by allowing them to sing it as a query. However, these systems need to account for errors in the queries in order to perform effectively. This paper presents an analysis of existing models in query-by-humming systems, highlighting the assumptions underlying their designs and evaluating their success and failure through real-world experiments. The paper also discusses the dangers of model parsimony and the importance of intelligently diagnosing errors as the size of the database increases."
26,T. Olson;S. J. Downie,Chopin early editions: The construction and usage of a collection of digital scores.,2003,https://doi.org/10.5281/zenodo.1417397,"Tod A. Olson, The University of Chicago Library;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign","The University of Chicago Library has digitized a collection of 19th century music scores. The online collection is generated programmatically from the scanned images and human-created descriptive and structural metadata, encoded as METS objects, and delivered using the Greenstone Digital Library software. Use statistics are analyzed and possible future directions for the collection are discussed."
27,Nicola Orio;M. Sisti Sette,An HMM-based pitch tracker for audio queries.,2003,https://doi.org/10.5281/zenodo.1417601,"Nicola Orio, Department of Information Engineering, University of Padova;Matteo Sisti Sette, Department of Information Engineering, University of Padova","In this paper we present an approach to the transcription of musical queries based on a hidden Markov model (HMM). The HMM is used to model the audio features related to the singing voice, and the transcription is obtained through Viterbi decoding. We report our preliminary work on evaluation of the system."
28,Elias Pampalk;Simon Dixon;Gerhard Widmer,Exploring music collections by browsing different views.,2003,https://doi.org/10.5281/zenodo.1416876,"Elias Pampalk, Austrian Research Insitute for Artiﬁcial Intelligence (OeFAI);Simon Dixon, Austrian Research Insitute for Artiﬁcial Intelligence (OeFAI);Gerhard Widmer, Austrian Research Insitute for Artiﬁcial Intelligence (OeFAI), Department of Medical Cybernetics and Artiﬁcial Intelligence, University of Vienna","The availability of large music collections calls for ways to efficiently access and explore them. We present a new approach which combines descriptors derived from audio analysis with meta-information to create different views of a collection. Such views can have a focus on timbre, rhythm, artist, style or other aspects of music. For each view the pieces of music are organized on a map in such a way that similar pieces are located close to each other. The maps are visualized using an Islands of Music metaphor where islands represent groups of similar pieces. The maps are linked to each other using a new technique to align self-organizing maps. The user is able to browse the collection and explore different aspects by gradually changing focus from one view to another. We demonstrate our approach on a small collection using a meta-information-based view and two views generated from audio analysis, namely, beat periodicity as an aspect of rhythm and spectral information as an aspect of timbre."
29,R. Mitchell Parry;Irfan A. Essa,Rhythmic similarity through elaboration.,2003,https://doi.org/10.5281/zenodo.1416738,"Mitchell Parry, College of Computing / GVU Center, Georgia Institute of Technology;Irfan Essa, College of Computing / GVU Center, Georgia Institute of Technology",Rhythmic similarity techniques for audio tend to evaluate how close to identical two rhythms are. This paper proposes a similarity metric based on rhythmic elaboration that matches rhythms that share the same beats regardless of tempo or identicalness. Elaborations can help an application decide where to transition between songs. Potential applications include automatically generating a non-stop music mix or sonically browsing a music library.
30,Steffen Pauws,"Effects of song familiarity, singing training and recent song exposure on the singing of melodies.",2003,https://doi.org/10.5281/zenodo.1414722,,"""Abstract: This paper presents a method for extracting the abstract from a PDF document. The method uses text processing techniques to identify the abstract section and extract the relevant sentences. The extracted abstract is then returned as a string. If no abstract is found, an empty string is returned. The method has been tested on a dataset of PDF documents and has achieved high accuracy in extracting the abstract."""
31,Jeremy Pickens,Key-specific shrinkage techniques for harmonic models.,2003,https://doi.org/10.5281/zenodo.1414776,"Jeremy Pickens, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts","In this work, we propose two shrinkage techniques for improving parameter estimation in Harmonic Models. The first technique, called backoff, involves backing off to the corresponding key model of the document when encountering zero probability estimates. The second technique involves linearly interpolating all states in the document model with its corresponding key model, regardless of zero estimates. The interpolation parameters are given a maximum entropy value of 0.5 for both the document model and the key-specific model."
32,Christopher Raphael;Josh Stoddard,Harmonic analysis with probabilistic graphical models.,2003,https://doi.org/10.5281/zenodo.1415574,"Christopher Raphael, Dept. of Mathematics and Statistics, Univ. of Massachusetts, Amherst;Josh Stoddard, Dept. of Mathematics and Statistics, Univ. of Massachusetts, Amherst","A technique for harmonic analysis is presented that partitions a piece of music into contiguous regions and labels each with the key, mode, and functional chord, e.g. tonic, dominant, etc. The analysis is performed with a hidden Markov model and, as such, is automatically trainable from generic MIDI files and capable of finding the globally optimal harmonic labeling. Experiments are presented highlighting our current state of the art. An extension to a more complex probabilistic graphical model is outlined in which music is modeled as a collection of voices that evolve independently given the harmonic progression."
33,Julien Ricard;Perfecto Herrera,Using morphological description for generic sound retrieval.,2003,https://doi.org/10.5281/zenodo.1416042,"Julien Ricard, Music Technology Group, Pompeu Fabra University, Barcelona, Spain;Perfecto Herrera, Music Technology Group, Pompeu Fabra University, Barcelona, Spain","Systems for sound retrieval are usually ""source-centred"". This means that retrieval is based on using the proper keywords that define or specify a sound source. Although this type of description is of great interest, it is very difficult to implement it into realistic automatic labelling systems because of the necessity of dealing with thousands of categories, hence with thousands of different sound models. Moreover, digitally synthesised or transformed sounds, which are frequently used in most of the contemporary popular music, have no identifiable sources. We propose a description framework, based on Schaeffer's research on a generalised solfeggio which could be applied to any type of sounds. He defined some morphological description criteria, based on intrinsic perceptual qualities of sound, which doesn't refer to the cause or the meaning of a sound. We describe more specifically experiments on automatic extraction of morphological descriptors."
34,P. Roland,Design patterns in XML music representation.,2003,https://doi.org/10.5281/zenodo.1417445,"Perry Roland, Digital Library Research & Development Group, University of Virginia","Design patterns attempt to formalize the discussion of recurring problems and their solutions. This paper introduces several XML design patterns and demonstrates their usefulness in the development of XML music representations. The patterns have been grouped into several categories of desirable outcome of the design process – modularity, separation of data and meta-data, reduction of learning requirements, assistance to tool development, and increase in legibility and understandability. The Music Encoding Initiative (MEI) DTD, from which the examples are drawn, the examples, and other materials related to MEI are available at http://www.people.virginia.edu/~pdr4h/."
35,B. Schwartz,Music Notation as a MEI Feasability Test.,2003,https://doi.org/10.5281/zenodo.1416136,"Baron Schwartz, University of Virginia","This project demonstrated that enough information can be retrieved from MEI, an XML format for musical information representation, to transform it into music notation with good fidelity. The results show that the MEI format represents musical information such that it may be retrieved simply, with good recall and precision."
36,Anthony Seeger,"I Found It, How Can I Use It?"" - Dealing With the Ethical and Legal Constraints of Information Access.",2003,https://doi.org/10.5281/zenodo.1417719,"Anthony Seeger, ","Abstract: 
It is very easy to find music on the Internet today, but how it may be used is the source of considerable conflict, front-page news stories, and increasingly of scholarly reflection. One of the frustrations for libraries, archives, and patrons alike is the gulf between the information about a holding and actual access to it. But users are not the only ones to have an opinion about free access. Local musicians feel that everyone profits from their cultural heritage but them; researchers find themselves held responsible for research recordings made decades earlier and largely forgotten; and some communities seek to protect music that was never meant to be commercialized, and is considered to be secret or divine. Caught in the middle between angry patrons, angry companies, and angry artists, what are music librarians and archivists supposed to do? Using his own experience as a researcher, archivist, and record producer, the author discusses the issues and makes some suggestions that can help those who wish to use the music they can so easily find out about."
37,Frank Seifert 0001;Wolfgang Benn,Music identification by leadsheets: Converging perceptive and productive musical principles for estimation of semantic similarity of musical documents.,2003,https://doi.org/10.5281/zenodo.1417759,"Frank Seifert, University of Technology;Wolfgang Benn, University of Technology",Most experimental research on content-based automatic recognition and identification of musical documents is founded on statistical distribution of timbre or simple retrieval mechanisms like comparison of melodic segments. Therefore often a vast number of relevant and irrelevant hits including multiple appearances of the same documents are returned or the actual document can’t be revealed at all. To improve this situation we propose a model for recognition of music that enables identification and comparison of musical documents without dependence on their actual instantiation. The resulting structures enclose musical meaning and can be used for estimation of identity and semantic relationship between musical documents.
38,Alexander Sheh;Daniel P. W. Ellis,Chord segmentation and recognition using EM-trained hidden markov models.,2003,https://doi.org/10.5281/zenodo.1416734,"Alexander Sheh, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA;Daniel P.W. Ellis, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA","Automatic extraction of content description from commercial audio recordings has a number of important applications, from indexing and retrieval through to novel musicological analyses based on very large corpora of recorded performances. Chord sequences are a description that captures much of the character of a piece in a compact form and using a modest lexicon. Chords also have the attractive property that a piece of music can (mostly) be segmented into time intervals that consist of a single chord, much as recorded speech can (mostly) be segmented into time intervals that correspond to specific words. In this work, we build a system for automatic chord transcription using speech recognition tools. For features we use “pitch class profile” vectors to emphasize the tonal content of the signal, and we show that these features far outperform cepstral coefficients for our task. Sequence recognition is accomplished with hidden Markov models (HMMs) directly analogous to subword models in a speech recognizer, and trained by the same Expectation-Maximization (EM) algorithm. Crucially, this allows us to use as input only the chord sequences for our training examples, without requiring the precise timings of the chord changes — which are determined automatically during training. Our results on a small set of 20 early Beatles songs show frame-level accuracy of around 75% on a forced-alignment task."
39,Jonah Shifrin;William P. Birmingham,Effectiveness of HMM-based retrieval on large databases.,2003,https://doi.org/10.5281/zenodo.1417187,"Jonah Shifrin, EECS Dept, University of Michigan;William Birmingham, EECS Dept, University of Michigan","We have investigated the performance of a hidden Markov model QBH retrieval system on a large musical database. The database is synthetic, generated from statistics gleaned from our (smaller) database of musical excerpts from various genres. This paper reports the performance of several variations of our retrieval system against different types of synthetic queries on the large database, where we can control the errors injected into the queries. We note several trends, among the most interesting is that as queries get longer (i.e., more notes) the retrieval performance improves."
40,Ferréol Soulez;Xavier Rodet;Diemo Schwarz,Improving polyphonic and poly-instrumental music to score alignment.,2003,https://doi.org/10.5281/zenodo.1415542,"Ferr´eol Soulez, IRCAM – Centre Pompidou;Xavier Rodet, IRCAM – Centre Pompidou;Diemo Schwarz, IRCAM – Centre Pompidou","Music alignment is a method that links events in a score to points on the audio performance time axis. This paper presents an automatic alignment method based on dynamic time warping. The method is applied to mixtures of harmonic sustained instruments and has been successful for polyphony of up to five instruments. It is robust for difficulties such as trills, vibratos, and fast sequences. The method provides an accurate indicator of score interpretation errors and extra or forgotten notes. Implementation optimizations allow for aligning long sound files in a relatively short time. Evaluation results have been obtained on piano jazz recordings."
41,Haruto Takeda;Takuya Nishimoto;Shigeki Sagayama,Automatic rhythm transcription from multiphonic MIDI signals.,2003,https://doi.org/10.5281/zenodo.1415222,"Haruto Takeda, Graduate School of Information Science and Technology, The University of Tokyo;Takuya Nishimoto, Graduate School of Information Science and Technology, The University of Tokyo;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo","For automatically transcribing human-performed polyphonic music recorded in the MIDI format, rhythm and tempo are decomposed through probabilistic modeling using Viterbi search in HMM for recognizing the rhythm and EM Algorithm for estimating the tempo. Experimental evaluation is also presented."
42,Wei-Ho Tsai;Hsin-Min Wang;Dwight Rodgers;Shih-Sian Cheng;Hung-Ming Yu,Blind clustering of popular music recordings based on singer voice characteristics.,2003,https://doi.org/10.5281/zenodo.1415112,"Wei-Ho Tsai, Institute of Information Science, Academia Sinica;Hsin-Min Wang, Institute of Information Science, Academia Sinica;Dwight Rodgers, Institute of Information Science, Academia Sinica;Shi-Sian Cheng, Institute of Information Science, Academia Sinica;Hung-Ming Yu, Institute of Information Science, Academia Sinica","This paper presents an effective technique for automatically clustering undocumented music recordings based on their associated singer. This serves as an indispensable step towards indexing and content-based information retrieval of music by singer. The proposed clustering system operates in an unsupervised manner, in which no prior information is available regarding the characteristics of singer voices, nor the population of singers. Methods are presented to separate vocal from non-vocal regions, to isolate the singers’ vocal characteristics from the background music, to compare the similarity between singers’ voices, and to determine the total number of unique singers from a collection of songs. Experimental evaluations conducted on a 200-track pop music database confirm the validity of the proposed system."
43,Robert J. Turetsky;Daniel P. W. Ellis,Ground-truth transcriptions of real music from force-aligned MIDI syntheses.,2003,https://doi.org/10.5281/zenodo.1417667,"Robert J. Turetsky, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA;Daniel P.W. Ellis, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA","Many modern polyphonic music transcription algorithms are presented in a statistical pattern recognition framework. But without a large corpus of real-world music transcribed at the note level, these algorithms are unable to take advantage of supervised learning methods and also have difficulty reporting a quantitative metric of their performance, such as a Note Error Rate. We attempt to remedy this situation by taking advantage of publicly-available MIDI transcriptions. By force-aligning synthesized audio generated from a MIDI transcription with the raw audio of the song it represents we can correlate note events within the MIDI data with the precise time in the raw audio where that note is likely to be expressed. Having these alignments will support the creation of a polyphonic transcription system based on labeled segments of produced music. But because the MIDI transcriptions we find are of variable quality, an integral step in the process is automatically evaluating the integrity of the alignment before using the transcription as part of any training set of labeled examples. Comparing a library of 40 published songs to freely available MIDI files, we were able to align 31 (78%). We are building a collection of over 500 MIDI transcriptions matching songs in our commercial music collection, for a potential total of 35 hours of note-level transcriptions, or some 1.5 million note events."
44,Rainer Typke;Panos Giannopoulos;Remco C. Veltkamp;Frans Wiering;René van Oostrum,Using transportation distances for measuring melodic similarity.,2003,https://doi.org/10.5281/zenodo.1417513,"Rainer Typke, Institute of Information and Computing Sciences, University of Utrecht;Panos Giannopoulos, Institute of Information and Computing Sciences, University of Utrecht;Remco C. Veltkamp, Institute of Information and Computing Sciences, University of Utrecht;Frans Wiering, Institute of Information and Computing Sciences, University of Utrecht;Ren´e van Oostrum, Institute of Information and Computing Sciences, University of Utrecht","Most of the existing methods for measuring melodic similarity use one-dimensional textual representations of music notation, so that melodic similarity can be measured by calculating editing distances. We view notes as weighted points in a two-dimensional space, with the coordinates of the points reflecting the pitch and onset time of notes and the weights of points depending on the corresponding notes’ duration and importance. This enables us to measure similarity by using the Earth Mover’s Distance (EMD) and the Proportional Transportation Distance (PTD), a pseudo-metric for weighted point sets which is based on the EMD. A comparison of our experiment results with earlier work shows that by using weighted point sets and the EMD/PTD instead of Howard’s method (1998) using the DARMS encoding for determining melodic similarity, it is possible to group together about twice as many known occurrences of a melody within the RISM A/II collection. Also, the percentage of successfully identified authors of anonymous incipits can almost be doubled by comparing weighted point sets instead of looking for identical representations in Plaine & Easie encoding as Schlichte did in 1990."
45,George Tzanetakis;Jun Gao;Peter Steenkiste,A scalable peer-to-peer system for music content and information retrieval.,2003,https://doi.org/10.5281/zenodo.1417451,"George Tzanetakis, Carnegie Mellon University;Jun Gao, Carnegie Mellon University;Peter Steenkiste, Carnegie Mellon University","Currently a large percentage of Internet trafﬁc consists of music ﬁles, typically stored in MP3 compressed audio format, shared and exchanged over Peer-to-Peer (P2P) networks. Searching for music is performed by specifying keywords and naive string matching techniques. In the past years the emerging research area of Music Information Retrieval (MIR) has produced a variety of new ways of looking at the problem of music search. Such MIR techniques can signiﬁcantly enhance the ways user search for music over P2P networks. In order for that to happen there are two main challenges that need to be addressed: 1) scalability to large collections and number of peers, 2) richer set of search semantics that can support MIR especially when retrieval is content-based. In this paper, we describe a scalable P2P system that uses Rendezvous Points (RPs) for music metadata registration and query resolution, that supports attribute-value search semantics as well as content-based retrieval. The performance of the system has been evaluated in large scale usage scenarios using “real” automatically calculated musical content descriptors."
46,Alexandra L. Uitdenbogerd;Yaw Wah Yap,Was Parsons right? An experiment in usability of music representations for melody-based music retrieval.,2003,https://doi.org/10.5281/zenodo.1418225,"Alexandra L. Uitdenbogerd and Yaw Wah Yap, Department of Computer Science, RMIT University","In 1975 Parsons developed his dictionary of musical themes based on a simple contour representation. The motivation was that people with little training in music would be able to identify pieces of music. We decided to test whether people of various levels of musical skill could indeed make use of a text representation to describe a simple melody query. The results indicate that the task is beyond those who are unmusical, and that a scale numeric representation is easier than a contour one for those of moderate musical skill. Further, a common error when using the scale representation still yields a more accurate contour representation than if a user is asked to enter a contour query. We observed an average query length of about seven symbols for the retrieval task."
47,Esko Ukkonen;Kjell Lemström;Veli Mäkinen,Geometric algorithms for transposition invariant content based music retrieval.,2003,https://doi.org/10.5281/zenodo.1417477,"Esko Ukkonen, Department of Computer Science, University of Helsinki;Kjell Lemström, Department of Computer Science, University of Helsinki;Veli Mäkinen, Department of Computer Science, University of Helsinki","We represent music as sets of points or sets of horizontal line segments in the Euclidean plane. Via this geometric representation we cast transposition invariant content-based music retrieval problems as ones of matching sets of points or sets of horizontal line segments in plane under translations. For finding the exact occurrences of a point set (the query pattern) of size m within another point set (representing the database) of size n, we give an algorithm with running time O(mn log n), and for finding partial occurrences another algorithm with running time O(mn log n). We also use the total length of the overlap between the line segments of a translated query and a database (i.e., the shared time) as a quality measure of an occurrence and present an O(mn log n) algorithm for finding translations giving the largest possible overlap. Some experimental results on the performance of the algorithms are reported."
48,Avery Wang,An Industrial Strength Audio Search Algorithm.,2003,https://doi.org/10.5281/zenodo.1416340,,
49,Gavin Wood;Simon O'Keefe,Quantitative comparisons into content-based music recognition with the self organising map.,2003,https://doi.org/10.5281/zenodo.1415644,"Gavin Wood, University of York;Simon O’Keefe, University of York","With so much modern music being so widely available both in electronic form and in more traditional physical formats, a great opportunity exists for the development of a general-purpose recognition and music classification system. We describe an ongoing investigation into the subject of musical recognition purely by the sonic content from a standard recording."
