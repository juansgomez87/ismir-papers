Authors,Title,Year,Link,Authors with Affiliations,Abstract
Taenzer M.; Abeßer J.; Mimilakis S.I.; Weiß C.; Müller M.; Lukashevich H.,Investigating CNN-based instrument family recognition for western classical music recordings,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094413&partnerID=40&md5=c6991436626fd2085c47d97f045f9ea2,"Taenzer M., Fraunhofer IDMT, Ilmenau, Germany; Abeßer J., Fraunhofer IDMT, Ilmenau, Germany; Mimilakis S.I., Fraunhofer IDMT, Ilmenau, Germany; Weiß C., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany; Lukashevich H., Fraunhofer IDMT, Ilmenau, Germany","Western classical music comprises a rich repertoire composed for different ensembles. Often, these ensembles consist of instruments from one or two of the families woodwinds, brass, piano, vocals, and strings. In this paper, we consider the task of automatically recognizing instrument families from music recordings. As one main contribution, we investigate the influence of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models. We report on experiments using three datasets of monotimbral recordings covering different levels of timbral complexity: isolated notes, isolated melodies, and polyphonic pieces. While data augmentation and the normalization of spectral patches turned out to be beneficial, pre-processing strategies such as logarithmic compression and channel-energy normalization did not lead to substantial improvements. Furthermore, our cross-dataset experiments indicate the necessity of further optimization routines such as domain adaptation. © 2020 International Society for Music Information Retrieval. All rights reserved."
Zangerle E.; Huber R.; Vötter M.; Yang Y.-H.,Hit song prediction: Leveraging low- and high-level audio features,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094653&partnerID=40&md5=7502e17f9f434aa0c563b059a3af7477,"Zangerle E., University of Innsbruck, Austria; Huber R., University of Innsbruck, Austria; Vötter M., University of Innsbruck, Austria; Yang Y.-H., Academia Sinica, Taipei, Taiwan","Assessing the potential success of a given song based on its acoustic characteristics is an important task in the music industry. This task has mostly been approached from an internal perspective, utilizing audio descriptors to predict the success of a given song, where either low- or high-level audio features have been utilized separately. In this work, we aim to jointly exploit low- and high-level audio features and model the prediction as a regression task. Particularly, we make use of a wide and deep neural network architecture that allows for jointly exploiting low- and high-level features. Furthermore, we enrich the set of features with information about the release year of tracks. We evaluate our approach based on the Million Song Dataset and characterize a song as a hit if it is contained in the Billboard Hot 100 at any point in time. Our findings suggest that the proposed approach is able to outperform baseline approaches as well as approaches utilizing low- or high-level features individually. Furthermore, we find that incorporating the release year as well as features describing the mood and vocals of a song improve prediction results. © 2020 International Society for Music Information Retrieval. All rights reserved."
Tsuchida S.; Fukayama S.; Hamasaki M.; Goto M.,"Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing",2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096677&partnerID=40&md5=02a801b1660cde92300e5279251a7510,"Tsuchida S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Hamasaki M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We describe the AIST Dance Video Database (AIST Dance DB), a shared database containing original street dance videos with copyright-cleared dance music. Although dancing is highly related to dance music and dance information can be considered an important aspect of music information, research on dance information processing has not yet received much attention in the Music Information Retrieval (MIR) community. We therefore developed the AIST Dance DB as the first large-scale shared database focusing on street dances to facilitate research on a variety of tasks related to dancing to music. It consists of 13,939 dance videos covering 10 major dance genres as well as 60 pieces of dance music composed for those genres. The videos were recorded by having 40 professional dancers (25 male and 15 female) dance to those pieces. We carefully designed this database so that it can cover both solo dancing and group dancing as well as both basic choreography moves and advanced moves originally choreographed by each dancer. Moreover, we used multiple cameras surrounding a dancer to simultaneously shoot from various directions. The AIST Dance DB will foster new MIR tasks such as dance-motion genre classification, dancer identification, and dance-technique estimation. We propose a dance-motion genre-classification task and developed four baseline methods of identifying dance genres of videos in this database. We evaluated these methods by extracting dancer body motions and training their classifiers on the basis of long short-term memory (LSTM) recurrent neural network models and support-vector machine (SVM) models. © Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, Masataka Goto."
Cano E.; Beveridge S.,Microtiming analysis in traditional shetland fiddle music,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096159&partnerID=40&md5=851ea31c6dd6d45bd736db4957010344,"Cano E., Fraunhofer IDMT, Germany; Beveridge S., Songquito, Germany","This work aims to characterize microtiming variations in traditional Shetland fiddle music. These microtiming variations dictate the rhythmic flow of a performed melody, and contribute, among other things, to the suitability of this music as an accompaniment to dancing. In the context of Shetland fiddle music, these microtiming variations are often referred to as lilt. Using a corpus of 27 traditional fiddle tunes from the Shetland Isles, we examine inter-beat timing deviations, as well as inter-onset timing deviations of eighth note sequences. Results show a number of distinct inter-beat and inter-onset rhythmic patterns that may characterize lilt, as well as idiosyncratic patterns for each performer. This paper presents a first step towards the use of Music Information Retrieval (MIR) techniques for modelling lilt in traditional Scottish fiddle music, and highlights its implications in the field of ethnomusicology. © 2020 International Society for Music Information Retrieval. All rights reserved."
Wang C.; Benetos E.; Lostanlen V.; Chew E.,Adaptive time-frequency scattering for periodic modulation recognition in music signals,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096038&partnerID=40&md5=2df319cc5a5908e14ed975b551795808,"Wang C., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Lostanlen V., Music and Audio Research Laboratory, New York University, NY, United States; Chew E., CNRS-UMR9912/STMS IRCAM, Paris, France","Vibratos, tremolos, trills, and flutter-tongue are techniques frequently found in vocal and instrumental music. A common feature of these techniques is the periodic modulation in the time-frequency domain. We propose a representation based on time-frequency scattering to model the interclass variability for fine discrimination of these periodic modulations. Time-frequency scattering is an instance of the scattering transform, an approach for building invariant, stable, and informative signal representations. The proposed representation is calculated around the wavelet subband of maximal acoustic energy, rather than over all the wavelet bands. To demonstrate the feasibility of this approach, we build a system that computes the representation as input to a machine learning classifier. Whereas previously published datasets for playing technique analysis focus primarily on techniques recorded in isolation, for ecological validity, we create a new dataset to evaluate the system. The dataset, named CBF-periDB, contains fulllength expert performances on the Chinese bamboo flute that have been thoroughly annotated by the players themselves. We report F-measures of 99% for flutter-tongue, 82% for trill, 69% for vibrato, and 51% for tremolo detection, and provide explanatory visualisations of scattering coefficients for each of these techniques. © 2020 International Society for Music Information Retrieval. All rights reserved."
Bretan M.; Heck L.,Learning semantic similarity in music via self-supervision,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095586&partnerID=40&md5=63555b89cb41fcaa853a8bd14e406968,"Bretan M., Samsung Research America, United States; Heck L., Samsung Research America, United States","Neural networks have been used to learn a latent ""musical space"" or ""embedding"" to encode meaningful features and provide a method of measuring semantic similarity between two musical passages. An ideal embedding is one that both captures features useful for downstream tasks and conforms to a distribution suitable for sampling and meaningful interpolation. We present two new methods for learning musical embeddings that leverage context while simultaneously imposing a shape on the feature space distribution via backpropagation using an adversarial component. We focus on the symbolic domain and target short polyphonic musical units consisting of 40 note sequences. The goal is to project these units into a continuous low dimensional space that has semantic relevance. We evaluate relevance based on the learned features' abilities to complete various musical tasks and show improvement over baseline models including variational autoencoders, adversarial autoencoders, and deep structured semantic models. We use a dataset consisting of classical piano and demonstrate the robustness of our methods across multiple input representations. © 2020 International Society for Music Information Retrieval. All rights reserved."
Maezawa A.; Yamamoto K.; Fujishima T.,Rendering music performance with interpretation variations using conditional variational RNN,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095620&partnerID=40&md5=69363d5f1ea6ab9d528f2391d26129a6,"Maezawa A., Yamaha Corporation, Japan; Yamamoto K., Yamaha Corporation, Japan; Fujishima T., Yamaha Corporation, Japan","Capturing and generating a wide variety of musical expression is important in music performance rendering, but current methods fail to model such a variation. This paper presents a music performance rendering method that explicitly models the variability in interpretations for a given piece of music. Conditional variational recurrent neural network is used to jointly train, conditioned on the music score, an encoder from a music performance to a latent representation of interpretation and a decoder from the latent interpretation back to the music performance. Evaluation demonstrates the method is capable of predicting and generating an expressive performance, and that the decoder learns a latent space of musical interpretation that is consistent with human perception of interpretation. © 2020 International Society for Music Information Retrieval. All rights reserved."
Balke S.; Dorfer M.; Carvalho L.; Arzt A.; Widmer G.,Learning Soft-attention models for Tempo-invariant audio-sheet music retrieval,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097246&partnerID=40&md5=58a80f35caee243e68bc6025429e0aaa,"Balke S., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Dorfer M., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Carvalho L., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks. © 2020 International Society for Music Information Retrieval. All rights reserved."
Lerch A.; Arthur C.; Pati A.; Gururani S.,Music performance analysis: A survey,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096431&partnerID=40&md5=fec7b5d1da0c7fa75c2dc3d986021cf9,"Lerch A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Arthur C., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Pati A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Gururani S., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States","Music Information Retrieval (MIR) tends to focus on the analysis of audio signals. Often, a single music recording is used as representative of a ""song"" even though different performances of the same song may reveal different properties. A performance is distinct in many ways from a (arguably more abstract) representation of a ""song,"" ""piece,"" or musical score. The characteristics of the (recorded) performance-as opposed to the score or musical idea- can have a major impact on how a listener perceives music. The analysis of music performance, however, has been traditionally only a peripheral topic for the MIR research community. This paper surveys the field of Music Performance Analysis (MPA) from various perspectives, discusses its significance to the field of MIR, and points out opportunities for future research in this field. © 2020 International Society for Music Information Retrieval. All rights reserved."
De Valk R.; Ahmed R.; Crawford T.,Josquintab: A dataset for content-based computational analysis of music in lute tablature,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095629&partnerID=40&md5=29f2df0d8716713acc930bb154185302,"De Valk R., Department of Computing, Goldsmiths, University of London, United Kingdom; Ahmed R., Digital Humanities, MIT, United States; Crawford T., Department of Computing, Goldsmiths, University of London, United Kingdom","An enormous corpus of music for the lute, spanning some two and half centuries, survives today. Unlike other musical corpora from the same period, this corpus has undergone only limited musicological study. The main reason for this is that it is written down exclusively in lute tablature, a prescriptive form of notation that is difficult to understand for non-specialists as it reveals little structural information. In this paper we present JOSQUINTAB, a dataset of automatically created enriched diplomatic transcriptions in MIDI and MEI format of 64 sixteenthcentury lute intabulations, instrumental arrangements of vocal compositions. Such a dataset enables large-scale content-based computational analysis of music in lute tablature hitherto impossible. We describe the dataset, the mapping algorithm used to create it, as well as a method to quantitatively evaluate the degree of arrangement (goodness of fit) of an intabulation. Furthermore, we present two use cases, demonstrating the usefulness of the dataset for both music information retrieval and musicological research. We make the dataset, the source code, and an implementation of the mapping algorithm, runnable as a command line tool, publicly available. © 2020 International Society for Music Information Retrieval. All rights reserved."
Condit-Schultz N.; Arthur C.,Humdrumr: A new take on an old approach to computational musicology,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093848&partnerID=40&md5=448d2fc6d98ac391270135dc86dd4f10,"Condit-Schultz N., Georgia Institute of Technology, United States; Arthur C., Georgia Institute of Technology, United States","Musicology research is a fundamentally humanistic endeavor. However, despite the productive work of a small niche of humanities-trained computational musicologists, most cutting-edge digital music research is pursued by scholars whose primary training is scientific or computational, not humanistic. This unfortunate situation is prolonged, at least in part, by the daunting barrier that computer coding presents to humanities scholars with no technical training. In this paper, we present humdrumR (""hum-drummer""), a software package designed to afford computational musicology research for both advanced and novice computer coders. Humdrum is a powerful and influential existing computational musicology framework including the humdrum syntax-a flexible text data format with tens of thousands of extant scores available-and the Bash-based humdrum toolkit. HumdrumR is a modern replacement for the humdrum toolkit, based in the dataanalysis/ statistical programming language R. By combining the flexibility and transparency of the humdrum syntax with the powerful data analysis tools and concise syntax of R, humdrumR offers an appealing new approach to would-be computational musicologists. HumdrumR leverages R's powerful metaprogramming capabilities to create an extremely expressive and composable syntax, allowing novices to achieve usable analyses quickly while avoiding many coding concepts that are commonly challenging for beginners. © 2020 International Society for Music Information Retrieval. All rights reserved."
Pauwels J.; O'Hanlon K.; Gómez E.; Sandler M.B.,20 years of automatic chord recognition from audio,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096966&partnerID=40&md5=e8660a5914a46ee26ca639e84511f02f,"Pauwels J., Centre for Digital Music, Queen Mary University of London, United Kingdom; O'Hanlon K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","In 1999, Fujishima published Realtime Chord Recognition of Musical Sound: a System using Common Lisp Music. This paper kickstarted an active research topic that has been popular in and around the ISMIR community. The field of Automatic Chord Recognition (ACR) has evolved considerably from early knowledge-based systems towards data-driven methods, with neural network approaches arguably being central to current ACR research. Nonetheless, many of its core issues were already addressed or referred to in the Fujishima paper. In this paper, we review those twenty years of ACR according to these issues. We furthermore attempt to frame current directions in the field in order to establish some perspective for future research. © 2020 International Society for Music Information Retrieval. All rights reserved."
Fuentes M.; Maia L.S.; Rocamora M.; Biscainho L.W.P.; Crayencour H.C.; Essid S.; Bello J.P.,Tracking beats and microtiming in Afro-latin American music using conditional random fields and deep learning,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094424&partnerID=40&md5=2292d1e040914b7e59bb2835cd0de512,"Fuentes M., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France, LTCI, Télécom Paris, Institut Polytechnique de Paris 0 France, France; Maia L.S., LTCI, Télécom Paris, Institut Polytechnique de Paris 0 France, France, Federal University of Rio de Janeiro, Brazil; Rocamora M., Universidad de la República, Uruguay; Biscainho L.W.P., Federal University of Rio de Janeiro, Brazil; Crayencour H.C., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France; Essid S., LTCI, Télécom Paris, Institut Polytechnique de Paris 0 France, France; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Events in music frequently exhibit small-scale temporal deviations (microtiming), with respect to the underlying regular metrical grid. In some cases, as in music from the Afro-Latin American tradition, such deviations appear systematically, disclosing their structural importance in rhythmic and stylistic configuration. In this work we explore the idea of automatically and jointly tracking beats and microtiming in timekeeper instruments of Afro-Latin American music, in particular Brazilian samba and Uruguayan candombe. To that end, we propose a language model based on conditional random fields that integrates beat and onset likelihoods as observations. We derive those activations using deep neural networks and evaluate its performance on manually annotated data using a scheme adapted to this task. We assess our approach in controlled conditions suitable for these timekeeper instruments, and study the microtiming profiles' dependency on genre and performer, illustrating promising aspects of this technique towards a more comprehensive understanding of these music traditions. © 2020 International Society for Music Information Retrieval. All rights reserved."
"Pacha A.; Calvo-Zaragoza J.; Hajič J., Jr.",Learning notation graph construction for full-pipeline optical music recognition,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094788&partnerID=40&md5=8af972845e0918cb776d970ee03c2b19,"Pacha A., Institute of Information Systems Engineering, TU Wien, Austria; Calvo-Zaragoza J., Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain; Hajič J., Jr., Institute of Formal and Applied Linguistics, Charles University, Prague, Czech Republic","Optical Music Recognition (OMR) promises great benefits to Music Information Retrieval by reducing the costs of making sheet music available in a symbolic format. Recent advances in deep learning have turned typical OMR obstacles into clearly solvable problems, especially the stages that visually process the input image, such as staff line removal or detection of music-notation objects. However, merely detecting objects is not enough for retrieving the actual content, as music notation is a configurational writing system where the semantic of a primitive is defined by its relationship to other primitives. Thus, OMR systems must employ a notation assembly stage to infer such relationships among the detected objects. So far, this stage has been addressed by devising a set of predefined rules or grammars, which hardly generalize well. In this work, we formulate the notation assembly stage from a set of detected primitives as a machine learning problem. Our notation assembly is modeled as a graph that stores syntactic relationships among primitives, which allows us to capture the configuration of symbols in a music-notation document. Our results over the handwritten sheet music corpus MUSCIMA++ show 95.2% precision, 96.0% recall, and an F-score of 95.6% in establishing the correct syntactic relationships. When inferring relationships on data from a music object detector, the model achieves 93.2% precision, 91.5% recall and an F-score of 92.3%. © 2020 International Society for Music Information Retrieval. All rights reserved."
Jeong D.; Kwon T.; Kim Y.; Lee K.; Nam J.,VirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094425&partnerID=40&md5=013edc9262b5bd28d6984412396f5b45,"Jeong D., Graduate School of Culture Technology, KAIST, South Korea; Kwon T., Graduate School of Culture Technology, KAIST, South Korea; Kim Y., Graduate School of Culture Technology, KAIST, South Korea; Lee K., Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models.We also share the dataset we used for the experiment. © 2020 International Society for Music Information Retrieval. All rights reserved."
Simonetta F.; Cancino-Chacón C.; Ntalampiras S.; Widmer G.,A convolutional approach to melody line identification in symbolic scores,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094302&partnerID=40&md5=fe9cb282d4f2a655c9da74e7947c2b7d,"Simonetta F., Music Informatics Laboratory, Dept. of Computer Science, University of Milano, Italy; Cancino-Chacón C., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Ntalampiras S., Music Informatics Laboratory, Dept. of Computer Science, University of Milano, Italy; Widmer G., Austrian Research Institute for Artificial Intelligence, Vienna, Austria, Dept. of Computational Perception, Johannes Kepler University Linz, Austria","In many musical traditions, the melody line is of primary significance in a piece. Human listeners can readily distinguish melodies from accompaniment; however, making this distinction given only the written score - i.e. without listening to the music performed - can be a difficult task. Solving this task is of great importance for both Music Information Retrieval and musicological applications. In this paper, we propose an automated approach to identifying the most salient melody line in a symbolic score. The backbone of the method consists of a convolutional neural network (CNN) estimating the probability that each note in the score (more precisely: each pixel in a piano roll encoding of the score) belongs to the melody line. We train and evaluate the method on various datasets, using manual annotations where available and solo instrument parts where not. We also propose a method to inspect the CNN and to analyze the influence exerted by notes on the prediction of other notes; this method can be applied whenever the output of a neural network has the same size as the input. © Federico Simonetta, Carlos Cancino-Chacón, Stavros Ntalampiras, Gerhard Widmer."
Lemaire Q.; Holzapfel A.,Temporal convolutional networks for speech and music detection in radio broadcast,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097259&partnerID=40&md5=7d9a242103c9a6d5f2e5bfc4dbee0169,"Lemaire Q., KTH Royal Institute of Technology, Sweden; Holzapfel A., KTH Royal Institute of Technology, Sweden","The task of speech and music detection aims at the automatic annotation of potentially overlapping speech and music segments in audio recordings. This metadata extraction process finds important applications in royalty collection for broadcast audio. This study focuses on deep neural network architectures made to process sequential data, and a series of recent architectures that have not yet been applied for this task are evaluated, extended and compared with a state-of-the-art architecture. Moreover, different training strategies are evaluated, and we demonstrate the advantages of a pre-training procedure with lowquality data that facilitates the combination of heterogeneous datasets. The study shows that Temporal Convolution Network (TCN) architectures can outperform state-ofthe- art architectures. In specific, the novel non-causal TCN extension introduced in this paper leads to a significant improvement of the accuracy. © 2020 International Society for Music Information Retrieval. All rights reserved."
Porcaro L.; Gomez E.,20 years of playlists: A statistical analysis on popularity and diversity,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095993&partnerID=40&md5=0acf8bbe0234f13efc0a0701918b4055,"Porcaro L., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gomez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, Joint Research Centre, European Commission, Seville, Spain","Grouping songs together, according to music preferences, mood or other characteristics, is an activity which reflects personal listening behaviours and tastes. In the last two decades, due to the increasing size of music catalogue accessible and to improvements of recommendation algorithms, people have been exposed to new ways for creating playlists. In this work, through the statistical analysis of more than 400K playlists from four datasets, created in different temporal and technological contexts, we aim to understand if it is possible to extract information about the evolution of humans strategies for playlist creation. We focus our analysis on two driving concepts of the Music Information Retrieval literature: popularity and diversity. © 2020 International Society for Music Information Retrieval. All rights reserved."
Hantrakul L.; Engel J.; Roberts A.; Gu C.,Fast and flexible neural audio synthesis,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095752&partnerID=40&md5=a1db328ccd3fb2ec15d70d83c883123d,"Hantrakul L., Google Brain, United States; Engel J., Google Brain, United States; Roberts A., Google Brain, United States; Gu C., Google DeepMind, United Kingdom","Autoregressive neural networks, such as WaveNet, have opened up new avenues for expressive audio synthesis. High-quality speech synthesis utilizes detailed linguistic features for conditioning, but comparable levels of control have yet to be realized for neural synthesis of musical instruments. Here, we demonstrate an autoregressive model capable of synthesizing realistic audio that closely follows fine-scale temporal conditioning for loudness and fundamental frequency. We find the appropriate choice of conditioning features and architectures improves both the quantitative accuracy of audio resynthesis and qualitative responsiveness to creative manipulation of conditioning. While large autoregressive models generate audio much slower than real-time, we achieve these results with a more efficient WaveRNN model, opening the door for exploring real-time interactive audio synthesis with neural networks. © Lamtharn Hantrakul, Jesse Engel, Adam Roberts, Chenjie Gu."
Ju Y.; Howes S.; McKay C.; Condit-Schultz N.; Calvo-Zaragoza J.; Fujinaga I.,An interactive workflow for generating chord labels for homorhythmic music in symbolic formats,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095631&partnerID=40&md5=f2693b813dd34d06da8d5f307524613f,"Ju Y., Schulich School of Music, McGill University, Canada; Howes S., Schulich School of Music, McGill University, Canada; McKay C., Department of Liberal and Creative Arts, Marianopolis College, Canada; Condit-Schultz N., School of Music, Georgia Institute of Technology, United States; Calvo-Zaragoza J., Department of Software and Computing Systems, University of Alicante, Spain; Fujinaga I., Schulich School of Music, McGill University, Canada","Automatic harmonic analysis is challenging: rule-based models cannot account for every possible edge case, and manual annotation is expensive and sometimes inconsistent, undermining the training and evaluation of machine learning models. We present an interactive workflow to address these problems, and test it on Bach chorales. First, a rule-based model was used to generate preliminary, consistent chord labels in order to pre-train three machine learning models. These four models were grouped into an ensemble that generated chord labels by voting, achieving 91.4% accuracy on a reserved test set. A domain expert then corrected only those chords that the ensemble did not agree on unanimously (20.9% of the generated labels). Finally, we used these corrected annotations to re-train the machine learning models, and the resulting ensemble attained an accuracy of 93.5% on the reserved test set, a 24.4% reduction in the number of errors. This versatile interactive workflow can either work in a fully automatic way, or can capitalize on relatively minimal human involvement to generate higher-quality chord labels. It combines the consistency of rule-based models with the nuance of manual analysis to generate relatively inexpensive highquality ground truth for training effective machine learning models. © 2020 International Society for Music Information Retrieval. All rights reserved."
Elowsson A.; Friberg A.,Modeling music modality with a key-class invariant pitch chroma CNN,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094660&partnerID=40&md5=bb2e1c454563d0dce9e0e0fc2be0e673,"Elowsson A., KTH Royal Institute of Technology, Sweden; Friberg A., KTH Royal Institute of Technology, Sweden","This paper presents a convolutional neural network (CNN) that uses input from a polyphonic pitch estimation system to predict perceived minor/major modality in music audio. The pitch activation input is structured to allow the first CNN layer to compute two pitch chromas focused on different octaves. The following layers perform harmony analysis across chroma and time scales. Through max pooling across pitch, the CNN becomes invariant with regards to the key class (i.e., key disregarding mode) of the music. A multilayer perceptron combines the modality activation output with spectral features for the final prediction. The study uses a dataset of 203 excerpts rated by around 20 listeners each, a small challenging data size requiring a carefully designed parameter sharing. With an R2 of about 0.71, the system clearly outperforms previous systems as well as individual human listeners. A final ablation study highlights the importance of using pitch activations processed across longer time scales, and using pooling to facilitate invariance with regards to the key class. © 2020 International Society for Music Information Retrieval. All rights reserved."
Driedger J.; Schreiber H.; De Haas W.B.; Müller M.,Towards automatically correcting tapped beat annotations for music recordings,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096560&partnerID=40&md5=e370b71974d44c23a2f7244b7520abbb,"Driedger J., Chordify; Schreiber H., International Audio Laboratories Erlangen; De Haas W.B., Chordify; Müller M., International Audio Laboratories Erlangen","A common method to create beat annotations for music recordings is to let a human annotator tap along with them. However, this method is problematic due to the limited human ability to temporally align taps with audio cues for beats accurately. In order to create accurate beat annotations, it is therefore typically necessary to manually correct the recorded taps in a subsequent step, which is a cumbersome task. In this work we aim to automate this correction step by ""snapping"" the taps to close-by audio cues-a strategy that is often used by beat tracking algorithms to refine their beat estimates. The main contributions of this paper can be summarized as follows. First, we formalize the automated correction procedure mathematically. Second, we introduce a novel visualization method that serves as a tool to analyze the results of the correction procedure for potential errors. Third, we present a new dataset consisting of beat annotations for 101 music recordings. Fourth, we use this dataset to perform a listening experiment as well as a quantitative study to show the effectiveness of our snapping procedure. © 2020 International Society for Music Information Retrieval. All rights reserved."
Feisthauer L.; Bigo L.; Giraud M.,Modeling and learning structural breaks in sonata forms,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095081&partnerID=40&md5=54f7f988d552cf026762670e856e057e,"Feisthauer L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Bigo L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Giraud M., CRIStAL, UMR 9189, CNRS, Université de Lille, France","Expositions of Sonata Forms are structured towards two cadential goals, one being the Medial Caesura (MC). The MC is a gap in the musical texture between the Transition zone (TR) and the Secondary thematic zone (S). It appears as a climax of energy accumulation initiated by the TR, dividing the Exposition in two parts. We introduce highlevel features relevant to formalize this energy gain and to identify MCs. These features concern rhythmic, harmonic and textural aspects of the music and characterize either the MC, its preparation or the texture contrast between TR and S. They are used to train a LSTM neural network on a corpus of 27 movements of string quartets written by Mozart. The model correctly locates the MCs on 14 movements within a leave-one-piece-out validation strategy. We discuss these results and how the network manages to model such structural breaks. © 2020 International Society for Music Information Retrieval. All rights reserved."
Waloschek S.; Hadjakos A.; Pacha A.,Identification and Cross-document alignment of measures in music score images,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095915&partnerID=40&md5=6de799025a105370ef4ff21488d7a1e0,"Waloschek S., Center of Music and Film Informatics, Detmold University of Music, Germany; Hadjakos A., Center of Music and Film Informatics, Detmold University of Music, Germany; Pacha A., Institute of Information Systems Engineering, TU Wien, Austria","In the course of editing musical works, musicologists regularly compare multiple sources of the same musical piece, such as composers' autographs, handwritten copies, and various prints. For efficient comparison, cross-source navigation is essential, enabling to quickly jump back and forth between multiple sources without losing the current musical position. In practice, measures are first annotated by hand in the individual source images and then related to each other. Our approach automates this time-consuming and error-prone process with the help of deep learning. For this purpose, we train a neural network that automatically finds bounding boxes of all measures in images. A second network is trained to compute the similarity between two measures to determine if they have the same musical content and should, therefore, be linked for navigation. Sequences of outputs from the second network are matched using Dynamic TimeWarping to provide the final proposal of measure relationships, so-called concordances. In addition to cross-source navigation, the results can be used to spot structural differences across the sources which are essential for editorial work, so that musicologists can focus more on analytical tasks. © 2020 International Society for Music Information Retrieval. All rights reserved."
Gadermaier T.; Widmer G.,A study of annotation and alignment accuracy for performance comparison in complex orchestral music,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095759&partnerID=40&md5=aeabcb6cd0f941e1597fa035ef4e7907,"Gadermaier T., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Quantitative analysis of commonalities and differences between recorded music performances is an increasingly common task in computational musicology. A typical scenario involves manual annotation of different recordings of the same piece along the time dimension, for comparative analysis of, e.g., the musical tempo, or for mapping other performance-related information between performances. This can be done by manually annotating one reference performance, and then automatically synchronizing other performances, using audio-to-audio alignment algorithms. In this paper we address several questions related to those tasks. First, we analyze different annotations of the same musical piece, quantifying timing deviations between the respective human annotators. A statistical evaluation of the marker time stamps will provide (a) an estimate of the expected timing precision of human annotations and (b) a ground truth for subsequent automatic alignment experiments. We then carry out a systematic evaluation of different audio features for audio-to-audio alignment, quantifying the degree of alignment accuracy that can be achieved, and relate this to the results from the annotation study. © 2020 International Society for Music Information Retrieval. All rights reserved."
Li L.; Toda T.; Morikawa K.; Kobayashi K.; Makino S.,Improving singing aid system for laryngectomees with statistical voice conversion and VAE-space,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095914&partnerID=40&md5=7e3aa26951f162837e2f5599b881fcda,"Li L., University of Tsukuba, Japan; Toda T., Nagoya University, Japan; Morikawa K., Nagoya University, Japan; Kobayashi K., Nagoya University, Japan; Makino S., University of Tsukuba, Japan","This paper proposes an improved singing aid system for laryngectomees that converts electrolaryngeal (EL) speech produced using an electrolarynx to a more naturally sounding singing voice. Although the previously proposed system employing a noise suppression process and a rulebased pitch control approach has achieved preliminary success in converting EL speech into a singing voice, there are still two major limitations. First, the converted singing voice still sounds mechanical and unnatural owing to the adverse impacts of spectrograms extracted from EL speeches, also making the effect of pitch control limited. Second, the capability and flexibility of the rulebased pitch control in modeling various singing styles are insufficient, causing the converted singing voices to lack variety. To address these limitations, this paper proposes an improved system that uses 1) a statistical voice conversion approach to convert spectrograms extracted from EL speeches into those of natural speeches and 2) a deep generative model-based approach called VAE-SPACE for pitch modification, which generates pitch patterns in a data-driven manner instead of following manually designed rules. The experimental results revealed that 1) the conversion of spectrograms was effective in improving the naturalness of singing voices, and 2) the statistical pitch control approach was able to achieve comparable results with the rule-based approach, which was very carefully designed to be specialized in singing. © 2020 International Society for Music Information Retrieval. All rights reserved."
Weiß C.; Schlecht S.J.; Rosenzweig S.; Müller M.,Towards measuring intonation quality of choir recordings: A case study on bruckner's locus iste,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095518&partnerID=40&md5=158dab249af3f0891f74baf71e801d7c,"Weiß C., International Audio Laboratories Erlangen, Germany; Schlecht S.J., International Audio Laboratories Erlangen, Germany; Rosenzweig S., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Unaccompanied vocal music is a central part of Western art music, yet it requires excellent skills for singers to achieve proper intonation. In this paper, we analyze intonation deficiencies by introducing an intonation cost measure that can be computed from choir recordings and may help to assess the singers' intonation quality. With our approach, we measure the deviation between the recording's local salient frequency content and an adaptive reference grid based on the equal-tempered scale. The adaptivity introduces invariance of the local intonation measure to global intonation drifts. In our experiments, we compute this measure for several recordings of Anton Bruckner's choir piece Locus Iste. We demonstrate the robustness of the proposed measure by comparing scenarios of different complexity regarding the availability of aligned scores and multi-track recordings, as well as the number of singers per part. Even without using score information, our cost measure shows interesting trends, thus indicating the potential of our method for real-world applications. © 2020 International Society for Music Information Retrieval. All rights reserved."
Kelz R.; Widmer G.,Towards interpretable polyphonic transcription with invertible neural networks,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096814&partnerID=40&md5=5ecb198b255cf61763dcd41959bf65fc,"Kelz R., Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Austria, Institute of Computational Perception, Johannes Kepler University Linz, Austria","We explore a novel way of conceptualising the task of polyphonic music transcription, using so-called invertible neural networks. Invertible models unify both discriminative and generative aspects in one function, sharing one set of parameters. Introducing invertibility enables the practitioner to directly inspect what the discriminative model has learned, and exactly determine which inputs lead to which outputs. For the task of transcribing polyphonic audio into symbolic form, these models may be especially useful as they allow us to observe, for instance, to what extent the concept of single notes could be learned from a corpus of polyphonic music alone (which has been identified as a serious problem in recent research). This is an entirely new approach to audio transcription, which first of all necessitates some groundwork. In this paper, we begin by looking at the simplest possible invertible transcription model, and then thoroughly investigate its properties. Finally, we will take first steps towards a more sophisticated and capable version. We use the task of piano transcription, and specifically the MAPS dataset, as a basis for these investigations. © 2020 International Society for Music Information Retrieval. All rights reserved."
Yesiler F.; Tralie C.; Correya A.; Silva D.F.; Tovstogan P.; Gómez E.; Serra X.,Da-tacos: A dataset for cover song identification and understanding,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095724&partnerID=40&md5=cba491877fe5116f20bcc46ca925d908,"Yesiler F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Tralie C., Department of Mathematics and Computer Science, Ursinus College, United States; Correya A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Silva D.F., Departamento de Computação, Universidade Federal de São Carlos, Brazil; Tovstogan P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Joint Research Centre, European Commission, Seville, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper focuses on Cover Song Identification (CSI), an important research challenge in content-based Music Information Retrieval (MIR). Although the task itself is interesting and challenging for both academia and industry scenarios, there are a number of limitations for the advancement of current approaches. We specifically address two of them in the present study. First, the number of publicly available datasets for this task is limited, and there is no publicly available benchmark set that is widely used among researchers for comparative algorithm evaluation. Second, most of the algorithms are not publicly shared and reproducible, limiting the comparison of approaches. To overcome these limitations we propose Da-TACOS, a DaTAset for COver Song Identification and Understanding, and two frameworks for feature extraction and benchmarking to facilitate reproducibility. Da-TACOS contains 25K songs represented by unique editorial metadata plus 9 low- and mid-level features pre-computed with open source libraries, and is divided into two subsets. The Cover Analysis subset contains audio features (e.g. key, tempo) that can serve to study how musical characteristics vary for cover songs. The Benchmark subset contains the set of features that have been frequently used in CSI research, e.g. chroma, MFCC, beat onsets etc. Moreover, we provide initial benchmarking results of a selected number of state-of-the-art CSI algorithms using our dataset, and for reproducibility, we share a GitHub repository containing the feature extraction and benchmarking frameworks. © 2020 International Society for Music Information Retrieval. All rights reserved."
Yang D.; Tanprasert T.; Jenrungrot T.; Shan M.; Tsai T.J.,Midi passage retrieval using cell phone pictures of sheet music,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097109&partnerID=40&md5=6e95ad826c3332e79a04b45a5d92fb93,"Yang D., Harvey Mudd College, Claremont, CA, United States; Tanprasert T., Harvey Mudd College, Claremont, CA, United States; Jenrungrot T., Harvey Mudd College, Claremont, CA, United States; Shan M., Harvey Mudd College, Claremont, CA, United States; Tsai T.J., Harvey Mudd College, Claremont, CA, United States","This paper investigates a cross-modal retrieval problem in which a user would like to retrieve a passage of music from a MIDI file by taking a cell phone picture of a physical page of sheet music. While audio-sheet music retrieval has been explored by a number of works, this scenario is novel in that the query is a cell phone picture rather than a digital scan. To solve this problem, we introduce a midlevel feature representation called a bootleg score which explicitly encodes the rules of Western musical notation. We convert both the MIDI and the sheet music into bootleg scores using deterministic rules of music and classical computer vision techniques for detecting simple geometric shapes. Once the MIDI and cell phone image have been converted into bootleg scores, we estimate the alignment using dynamic programming. The most notable characteristic of our system is that it does test-time adaptation and has no trainable weights at all-only a set of about 30 hyperparameters. On a dataset containing 1000 cell phone pictures taken of 100 scores of classical piano music, our system achieves an F measure score of:869 and outperforms baseline systems based on commercial optical music recognition software. © 2020 International Society for Music Information Retrieval. All rights reserved."
Choi K.; Cho K.,Deep unsupervised drum transcription,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095689&partnerID=40&md5=6b50dc9a384ced8bcf6d345331214477,"Choi K., Spotify, Sweden; Cho K., New York University, Facebook AI Research, United States","We introduce DrummerNet, a drum transcription system that is trained in an unsupervised manner. DrummerNet does not require any ground-truth transcription and, with the data-scalability of deep neural networks, learns from a large unlabeled dataset. In DrummerNet, the target drum signal is first passed to a (trainable) transcriber, then reconstructed in a (fixed) synthesizer according to the transcription estimate. By training the system to minimize the distance between the input and the output audio signals, the transcriber learns to transcribe without ground truth transcription. Our experiment shows that DrummerNet performs favorably compared to many other recent drum transcription systems, both supervised and unsupervised. © 2020 International Society for Music Information Retrieval. All rights reserved."
Luo Y.-J.; Agres K.; Herremans D.,Learning disentangled representations of timbre and pitch for musical instrument sounds using gaussian mixture variational autoencoders,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097188&partnerID=40&md5=d075e5d929a4e7e0243ab08c2f197c64,"Luo Y.-J., Singapore University of Technology and Design, Singapore, Institute of High Performance Computing, A STAR, Singapore; Agres K., Institute of High Performance Computing, A STAR, Singapore, Yong Siew Toh Conservatory of Music, National University of Singapore, Singapore; Herremans D., Singapore University of Technology and Design, Singapore, Institute of High Performance Computing, A STAR, Singapore","In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model's efficacy using latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high F-scores when tested on our synthesized sounds, which verifies the model's performance of controllable realistic timbre/pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single encoder-decoder architecture, which is evaluated by measuring the shift in the posterior of instrument classification. Our in-depth evaluation confirms the model's ability to successfully disentangle timbre and pitch. © 2020 International Society for Music Information Retrieval. All rights reserved."
Lee J.H.; Choi H.-S.; Lee K.,Audio Query-based music source separation,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096770&partnerID=40&md5=9025313df061f9d46110ac2aaaa82201,"Lee J.H., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Choi H.-S., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Lee K., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea","In recent years, music source separation has been one of the most intensively studied research areas in music information retrieval. Improvements in deep learning lead to a big progress in music source separation performance. However, most of the previous studies are restricted to separating a few limited number of sources, such as vocals, drums, bass, and other. In this study, we propose a network for audio query-based music source separation that can explicitly encode the source information from a query signal regardless of the number and/or kind of target signals. The proposed method consists of a Query-net and a Separator: given a query and a mixture, the Query-net encodes the query into the latent space, and the Separator estimates masks conditioned by the latent vector, which is then applied to the mixture for separation. The Separator can also generate masks using the latent vector from the training samples, allowing separation in the absence of a query. We evaluate our method on the MUSDB18 dataset, and experimental results show that the proposed method can separate multiple sources with a single network. In addition, through further investigation of the latent space we demonstrate that our method can generate continuous outputs via latent vector interpolation. © 2020 International Society for Music Information Retrieval. All rights reserved."
Chen T.-P.; Su L.,Harmony transformer: Incorporating chord segmentation into harmony recognition,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097185&partnerID=40&md5=81ac54128b45e5f80afcf86ffdabfe77,"Chen T.-P., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","Musical harmony analysis is usually a process of unfolding and interpreting the hierarchical structure of music. Computational approaches to such structural analysis are still challenging, owing to the fact that the boundary between different harmonic states (such as chord functions) is not explicitly defined in the audio or symbolic music data. It is a novel approach to improve chord recognition by jointly identifying chord change using end-to-end sequence learning. In this paper, we propose the Harmony Transformer, a multi-task music harmony analysis model aiming to improve chord recognition through incorporating chord segmentation into the recognition process. The integration of chord segmentation and chord recognition is implemented with the Transformer, a deep sequential learning model yielding fruitful results in the field of natural language processing. A non-autoregressive decoding framework is also adopted here in aid of concatenating the two highly correlated tasks. Experiments of both chord symbol recognition and functional harmony recognition on audio and symbolic datasets demonstrate that explicitly learning the hierarchical structural information of musical data can facilitate and improve the harmony recognition. © 2020 International Society for Music Information Retrieval. All rights reserved."
Katsiavalos A.; Collins T.; Battey B.,An initial computational model for musical schemata theory,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094518&partnerID=40&md5=3528a71690278f53a16a1faae7e6fd4d,"Katsiavalos A., Music, Technology and Innovation Research Centre, De Montfort University, United Kingdom; Collins T., Music, Science and Technology Research Cluster, Department of Music, University of York, United Kingdom, Music Artificial Intelligence Algorithms, Inc., Davis, CA, United States; Battey B., Music, Technology and Innovation Research Centre, De Montfort University, United Kingdom","Musical schemata theory entails the classification of subphrase-length progressions in melodic, harmonic and metric feature-sets as named entities (e.g., 'Romanesca', 'Meyer', 'Cadence', etc.), where a musical schema is characterized by factors such as music content and form, position and tonal function within phrase structure, and interrelation with other schemata. To examine and automate the task of musical schemata classification, we developed a novel musical schemata classifier. First, we tested methods for exact and approximate matching of user-defined schemata prototypes, to establish the notions of identity and similarity between composite music patterns. Next, we examined methods for schemata prototype extraction from collections of same-labelled annotated examples, performing training and testing sessions similar to supervised learning approaches. The performance of the above tasks was verified using the same annotated dataset of 40 keyboard sonata excerpts from pre-Classical and Classical periods. Our evaluation of the classifier sheds light on: (a) ability to parse and interpret music information, (b) similarity methods for composite music patterns, (c) categorization methods for polyphonic music. © 2020 International Society for Music Information Retrieval. All rights reserved."
Koutlis C.; Schinas M.; Gkatziaki V.; Papadopoulos S.; Kompatsiaris Y.,Data-driven song recognition estimation using collective memory dynamics models,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095615&partnerID=40&md5=042ba6529aae2f1707659c2cc043856d,"Koutlis C., CERTH-ITI, Thessaloniki, Greece; Schinas M., CERTH-ITI, Thessaloniki, Greece; Gkatziaki V., CERTH-ITI, Thessaloniki, Greece; Papadopoulos S., CERTH-ITI, Thessaloniki, Greece; Kompatsiaris Y., CERTH-ITI, Thessaloniki, Greece","Cultural products such as music tracks intend to be appreciated and recognized by a portion of the audience. However, no matter how highly recognized a song might be at the beginning of its life, its recognition will inevitably and progressively decay. The mechanism that governs this decreasing trajectory could be modelled as a forgetting curve or a collective memory decay process. Here, we propose a composite model, termed T-REC, that involves chart data, YouTube views, Spotify popularity of tracks and forgetting curve dynamics with the purpose of estimating song recognition levels. We also present a comparative study, involving state-of-the-art and baseline models based on ground truth data from a survey that we conducted regarding the recognition level of 100 songs in Sweden. Our method is found to perform best among this ensemble of models. A remarkable finding of our study pertains to the role of the number of weeks a song remains in the charts, which is found to be a major factor for the accurate estimation of the song recognition level. © 2020 International Society for Music Information Retrieval. All rights reserved."
Epure E.V.; Khlif A.; Hennequin R.,Leveraging knowledge bases and parallel annotations for music genre translation,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094405&partnerID=40&md5=6d9c190a81f384894448a29acbf06d75,"Epure E.V., Deezer R&D; Khlif A., Deezer R&D; Hennequin R., Deezer R&D","Prevalent efforts have been put in automatically inferring genres of musical items. Yet, the propose solutions often rely on simplifications and fail to address the diversity and subjectivity of music genres. Accounting for these has, though, many benefits for aligning knowledge sources, integrating data and enriching musical items with tags. Here, we choose a new angle for the genre study by seeking to predict what would be the genres of musical items in a target tag system, knowing the genres assigned to them within source tag systems. We call this a translation task and identify three cases: 1) no common annotated corpus between source and target tag systems exists, 2) such a large corpus exists, 3) only few common annotations exist. We propose the related solutions: a knowledge-based translation modeled as taxonomy mapping, a statistical translation modeled with maximum likelihood logistic regression; a hybrid translation modeled with maximum a posteriori logistic regression with priors given by the knowledge-based translation. During evaluation, the solutions fit well the identified cases and the hybrid translation is systematically the most effective w.r.t. multilabel classification metrics. This is a first attempt to unify genre tag systems by leveraging both representation and interpretation diversity. © 2020 International Society for Music Information Retrieval. All rights reserved."
Donahue C.; Mao H.H.; Li Y.E.; Cottrell G.W.; McAuley J.,LakhNES: Improving multi-instrumental music generation with cross-domain pre-training,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093830&partnerID=40&md5=1680add114525a3993be791d414c6091,"Donahue C., Department of Music, UC San Diego, United States; Mao H.H., Department of Computer Science, UC San Diego, United States; Li Y.E., Department of Computer Science, UC San Diego, United States; Cottrell G.W., Department of Computer Science, UC San Diego, United States; McAuley J., Department of Computer Science, UC San Diego, United States","We are interested in the task of generating multiinstrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation-here we adapt it to the multiinstrumental setting. Transformers are complex, highdimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-instrument scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music, namely the Lakh MIDI dataset. Despite differences between the two corpora, we find that this transfer learning procedure improves both quantitative and qualitative performance for our primary task. © 2020 International Society for Music Information Retrieval. All rights reserved."
Chowdhury S.; Vall A.; Haunschmid V.; Widmer G.,Towards explainable music emotion recognition: The route via Mid-level features,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094248&partnerID=40&md5=cc71ebfffe1a6bb3ed17e525e6f36f7f,"Chowdhury S., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Vall A., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Haunschmid V., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria","Emotional aspects play an important part in our interaction with music. However, modelling these aspects in MIR systems have been notoriously challenging since emotion is an inherently abstract and subjective experience, thus making it difficult to quantify or predict in the first place, and to make sense of the predictions in the next. In an attempt to create a model that can give a musically meaningful and intuitive explanation for its predictions, we propose a VGG-style deep neural network that learns to predict emotional characteristics of a musical piece together with (and based on) human-interpretable, mid-level perceptual features. We compare this to predicting emotion directly with an identical network that does not take into account the mid-level features and observe that the loss in predictive performance of going through the mid-level features is surprisingly low, on average. The design of our network allows us to visualize the effects of perceptual features on individual emotion predictions, and we argue that the small loss in performance in going through the midlevel features is justified by the gain in explainability of the predictions. © 2020 International Society for Music Information Retrieval. All rights reserved."
Verma H.; Thickstun J.,Convolutional composer classification,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094400&partnerID=40&md5=fb32a8003b262b92ca566ed44a049762,"Verma H., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Thickstun J., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States","This paper investigates end-to-end learnable models for attributing composers to musical scores. We introduce several pooled, convolutional architectures for this task and draw connections between our approach and classical learning approaches based on global and n-gram features. We evaluate models on a corpus of 2,500 scores from the KernScores collection, authored by a variety of composers spanning the Renaissance era to the early 20th century. This corpus has substantial overlap with the corpora used in several previous, smaller studies; we compare our results on subsets of the corpus to these previous works. © 2020 International Society for Music Information Retrieval. All rights reserved."
Gillick J.; Cella C.-E.; Bamman D.,Estimating unobserved audio features for Target-based orchestration,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096544&partnerID=40&md5=54719e9d05a725a7db88bbd8e5f068e7,"Gillick J., School of Information, University of California, Berkeley, United States; Cella C.-E., CNMAT, University of California, Berkeley, United States; Bamman D., School of Information, University of California, Berkeley, United States","Target-based assisted orchestration can be thought of as the process of searching for optimal combinations of sounds to match a target sound, given a database of samples, a similarity metric, and a set of constraints. A typical solution to this problem is a proposed orchestral score where candidates are ranked by similarity in some feature space between the target sound and the mixture of audio samples in the database corresponding to the notes in the score; in the orchestral setting, valid scores may contain dozens of instruments sounding simultaneously. Generally, target-based assisted orchestration systems consist of a combinatorial optimization algorithm and a constraint solver that are jointly optimized to find valid solutions. A key step in the optimization involves generating a large number of combinations of sounds from the database and then comparing the features of each mixture of sounds with the target sound. Because of the high computational cost required to synthesize a new audio file and then compute features for every combination of sounds, in practice, existing systems instead estimate the features of each new mixture using precomputed features of the individual source files making up the combination. Currently, state-of-the-art systems use a simple linear combination to make these predictions, even if the features in use are not themselves linear. In this work, we explore neural models for estimating the features of a mixture of sounds from the features of the component sounds, finding that standard features can be estimated with accuracy significantly better than that of the methods currently used in assisted orchestration systems. We present quantitative comparisons and discuss the implications of our findings for target-based orchestration problems. © 2020 International Society for Music Information Retrieval. All rights reserved."
Park S.Y.; Laplante A.; Lee J.H.; Kaneshiro B.,Tunes together: Perception and experience of collaborative playlists,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094640&partnerID=40&md5=e9cd4e3a65fac039af8c33c378800428,"Park S.Y., Center for Computer Research in Music and Acoustics, Stanford University, United States; Laplante A., École de Bibliothéconomie et des Sciences de l'Information, Université de Montréal, Canada; Lee J.H., Information School, University of Washington, United States; Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, United States","Music is well established as a means of social connection. In the age of streaming platforms, personalized playlists and recommendations are popular topics in music information retrieval. We bring the focus of music enjoyment back to social connection and examine how technologies can enhance interpersonal relationships, specifically through the context of the collaborative playlist (CP).We conducted an exploratory study of CP users and non-users (N = 65) and examined speculative and experienced purposes and outcomes of CPs, as well as general perspectives on music and social connectedness. We derived a CP Framework with three purposes-Practical, Cognitive, and Social- and two connotations-Utility and Orientation. Both users and non-users shared similar perspectives on music-related activities and CP user outcomes. Projected and actual CP purposes differed between groups, however, as did perception of music's role in connectedness in recent years. These results highlight the importance of music-based social interactions for both groups. © 2020 International Society for Music Information Retrieval. All rights reserved."
Watanabe K.; Goto M.,"Query-by-blending: A music exploration system blending latent vector representations of lyric word, song audio, and artist",2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095697&partnerID=40&md5=3fea99410f50f6ac88db3eee68847713,"Watanabe K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents Query-by-Blending, a novel music exploration system that enables users to find unfamiliar music content by flexibly combining three musical aspects: lyric word, song audio, and artist. Although there are various systems for music retrieval based on the similarity between songs or artists and for music browsing based on visualized songs, it is still difficult to explore unfamiliar content by flexibly combining multiple musical aspects. Query-by- Blending overcomes this difficulty by representing each of the aspects as a latent vector representation (called a ""flavor"" in this paper) that is a distinctive quality felt to be characteristic of a given word/song/artist. By giving a lyric word as a query, for example, a user can find songs and artists whose flavors are similar to the flavor of the query word. Moreover, by giving a query combining (blending) lyric-word and song-audio flavors, the user can interactively explore unfamiliar content containing the blended flavor. This multi-aspect blending was achieved by constructing a novel vector space model into which all of the lyric words, song audio tracks, and artist IDs of a collection can be embedded. In our experiments, we embedded 14,505 lyric words, 433,936 songs, and 44,696 artists into the same shared vector space and found that the system can appropriately calculate similarities between different aspects and blend flavors to find related lyric words, songs, and artists. © 2020 International Society for Music Information Retrieval. All rights reserved."
Ycart A.; Stoller D.; Benetos E.,A comparative study of neural models for polyphonic music sequence transduction,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094121&partnerID=40&md5=54c5e11862c3a49de2b38b6af8198785,"Ycart A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Stoller D., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","Automatic transcription of polyphonic music remains a challenging task in the field of Music Information Retrieval. One under-investigated point is the post-processing of timepitch posteriograms into binary piano rolls. In this study, we investigate this task using a variety of neural network models and training procedures. We introduce an adversarial framework, that we compare against more traditional training losses. We also propose the use of binary neuron outputs and compare them to the usual real-valued outputs in both training frameworks. This allows us to train networks directly using the F-measure as training objective. We evaluate these methods using two kinds of transduction networks and two different multi-pitch detection systems, and compare the results against baseline note-tracking methods on a dataset of classical piano music. Analysis of results indicates that (1) convolutional models improve results over baseline models, but no improvement is reported for recurrent models; (2) supervised losses are superior to adversarial ones; (3) binary neurons do not improve results; (4) cross-entropy loss results in better or equal performance compared to the F-measure loss. © 2020 International Society for Music Information Retrieval. All rights reserved."
Ducher J.F.; Esling P.,Folded CQT rcnn for real-time recognition of instrument playing techniques,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094122&partnerID=40&md5=e86df6fabe3985f487def2638c647755,"Ducher J.F., CICM - MUSIDANSE, Université Paris 8, IRCAM (UMR9912 STMS), France; Esling P., CICM - MUSIDANSE, Université Paris 8, IRCAM (UMR9912 STMS), France","In the past years, deep learning has produced state-of-theart performance in timbre and instrument classification. However, only a few models currently deal with the recognition of advanced Instrument Playing Techniques (IPT). None of them have a real-time approach of this problem. Furthermore, most studies rely on a single sound bank for training and testing. Their methodology provides no assurance as to the generalization of their results to other sounds. In this article, we extend state-ofthe- art convolutional neural networks to the classification of IPTs. We build the first IPT corpus from independent sound banks, annotate it with the JAMS standard and make it freely available. Our models yield consistently high accuracies on a homogeneous subset of this corpus. However, only a proper taxonomy of IPTs and specifically defined input transforms offer proper resilience when addressing the ""minus-1db"" methodology, which assesses the ability of the models to generalize. In particular, we introduce a novel Folded Constant Q-Transform adjusted to the requirements of IPT classification. Finally we discuss the use of our classifier in real-time. © Jean-François Ducher, Philippe Esling."
Wei I.-C.; Wu C.-W.; Su L.,Generating structured drum pattern using variational autoencoder and self-similarity matrix,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096142&partnerID=40&md5=de8ddedd999e9b3efb1351f3185b6eed,"Wei I.-C., Institute of Information Science, Academia Sinica, Taiwan; Wu C.-W., Netflix, Inc., United States; Su L., Institute of Information Science, Academia Sinica, Taiwan","Drum pattern generation is a task that focuses on the rhythmic aspect of music and aims at generating percussive sequences. With the advancement of machine learning techniques, several models have been proven useful in producing compelling results. However, one of the main challenges is to generate structurally cohesive sequences. In this study, a drum pattern generation model based on Variational Autoencoders (VAEs) is presented; Specifically, the proposed model is built to generate symbolic drum patterns given an accompaniment that consists of melodic sequences. A self-similarity matrix (SSM) is incorporated in the process for encapsulating structural information. Both the objective evaluation and the subjective listening test highlight the model's capability of creating musically meaningful transitions on structural boundaries. © 2020 International Society for Music Information Retrieval. All rights reserved."
Shi Z.; Sapp C.S.; Arul K.; McBride J.; Smith J.O.,Supra: Digitizing the stanford university piano roll archive,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096382&partnerID=40&md5=45f693d3810b985d8bb6f3eac6bc47f6,"Shi Z., Center for Computer Research in Music and Acoustics, Stanford University, United States; Sapp C.S., Center for Computer Assisted Research in the Humanities, Stanford University, United States; Arul K., Department of Music, Stanford University, United States; McBride J., Music Library and Archive of Recorded Sound, Stanford University, United States; Smith J.O., Center for Computer Research in Music and Acoustics, Stanford University, United States","This paper describes the digitization process of a large collection of historical piano roll recordings held in the Stanford University Piano Roll Archive (SUPRA), which has resulted in an initial dataset of 478 performances of pianists from the early twentieth century transcribed to MIDI format. The process includes scanning paper rolls, digitizing the hole punches, and translating the pneumatic expression codings into MIDI format to create expressive performance files. We offer derivative files from each step of this process, including a high resolution image of the roll, a ""raw"" MIDI file of hole data, an ""expressive"" MIDI file that translates hole data into dynamics, and an audio file rendering of the expressive MIDI file on a digital piano sample. This provides digital access to the rolls for researchers in a flexible, searchable online database. We currently offer an initial dataset, ""SUPRA-RW"" from a selection of ""red Welte""-type rolls in the SUPRA. This dataset provides roll scans and MIDI transcriptions of important historical piano performances, many being made available widely for the first time. © 2020 International Society for Music Information Retrieval. All rights reserved."
Janssen B.; Collins T.; Yuping I.R.,Algorithmic ability to predict the musical future: Datasets and evaluation,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093835&partnerID=40&md5=bddd2a972592f9433ba4f9b66f050f08,"Janssen B., Digital Humanities Lab, Department of Humanities, Utrecht University, Netherlands; Collins T., Music, Science and Technology Research Cluster, Department of Music, University of York, United Kingdom, Music Artificial Intelligence Algorithms, Inc., Davis, CA, United States; Yuping I.R., Department of Information and Computing Sciences, Utrecht University, Netherlands","Music prediction and generation have been of recurring interest in the field of music informatics: many models that emulate listeners' musical expectancies, or that produce novel musical content have been introduced over the past few decades. So far, these models have mostly been evaluated in isolation, following diverse evaluation strategies. Our paper provides an overview of the new MIREX task Patterns for Prediction. We introduce a dataset, which contains monophonic and polyphonic data, both in symbolic and audio representations. We suggest a standardized evaluation procedure to compare algorithmic musical predictions. We compare two neural network models to a baseline model and show that algorithmic approaches can correctly predict about a third of a monophonic segment, and around half of a polyphonic segment, with one of the neural network models achieving best results. However, other approaches to algorithmic music prediction are needed to achieve a more rounded picture of the potential of state-of-the-art methods of music prediction. © 2020 International Society for Music Information Retrieval. All rights reserved."
Wiggins A.; Kim Y.,Guitar tablature estimation with a convolutional neural network,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095304&partnerID=40&md5=aa87f623b26c66a68e916b8b33581a99,"Wiggins A., Drexel University, Dept. of Electrical and Computer Engineering, United States; Kim Y., Drexel University, Dept. of Electrical and Computer Engineering, United States","Guitar tablature is a popular notation guitarists use to learn and share music. As it stands, most tablatures are created by an experienced guitarist taking the time and effort to annotate a song. As the process is time consuming and requires expertise, we are interested in automating this task. Previous approaches to automatic tablature transcription break the problem into two steps: 1) polyphonic pitch estimation, followed by 2) tablature fingering arrangement. Using a convolutional neural network (CNN) model, we can jointly solve both steps by learning a mapping directly from audio data to tablature. The model can simultaneously leverage physical playability constraints and differences in string timbres implicit in the data to determine the actual fingerings being used by the guitarist. We propose TabCNN, a CNN for estimating guitar tablature from audio of a solo acoustic guitar performance. We train and test our network using microphone recordings from the GuitarSet dataset [24], and TabCNN outperforms a state-of-the-art multipitch estimation algorithm. We also introduce a set of metrics to evaluate guitar tablature estimation. © 2020 International Society for Music Information Retrieval. All rights reserved."
Akama T.,Controlling symbolic music generation based on concept learning from domain knowledge,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094730&partnerID=40&md5=c10209e7e7e6519a49e2dc1f812e51c6,"Akama T., Sony Computer Science Laboratories, Tokyo, Japan","Machine learning allows automatic construction of generative models for music. However, they are learned from only the succession of notes itself without explicitly employing domain knowledge of musical concepts such as rhythm, contour, and fragmentation & consolidation. We approximate such musical domain knowledge as a function, and feed it into our model. Then, two decoupled spaces are learned: the extraction space that captures the target concept, and the residual space that captures the remainder. For monophonic symbolic music, our model exhibits high decoupling/modeling performance. Controllability in generation is improved: (i) our interpolation enables concept-aware flexible control over blending two musical fragments, and (ii) our variation generation enables users to make concept-aware adjustable variations. © 2020 International Society for Music Information Retrieval. All rights reserved."
Huang J.; Lerch A.,Automatic assessment of sight-reading exercises,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096996&partnerID=40&md5=579bdaa350b175f5b0c7208433cb01e1,"Huang J., Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, United States","Sight-reading requires a musician to decode, process, and perform a musical score quasi-instantaneously and without rehearsal. Due to the complexity of this task, it is difficult to assess the proficiency of a sight-reading performance, and it is even more challenging to model its human assessment. This study aims at evaluating and identifying effective features for automatic assessment of sight-reading performance. The evaluated set of features comprises taskspecific, hand-crafted, and interpretable features designed to represent various aspect of sight-reading performance covering parameters such as intonation, timing, dynamics, and score continuity. The most relevant features are identified by Principal Component Analysis and forward feature selection. For context, the same features are also applied to the assessment of rehearsed student music performances and compared across different assessment categories. The results show potential of automatic assessment models for sight-reading and the relevancy of different features as well as the contribution of different feature groups to different assessment categories. © 2020 International Society for Music Information Retrieval. All rights reserved."
Grachten M.; Deruty E.; Tanguy A.,Auto-adaptive resonance equalization using dilated residual networks,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097445&partnerID=40&md5=6d6b22c5f3ea3c4bccb215e2ae9c38db,"Grachten M., Contractor for Sony CSL Paris, France; Deruty E., Sony CSL Paris, France; Tanguy A., Yascore, Paris, France","In music and audio production, attenuation of spectral resonances is an important step towards a technically correct result. In this paper we present a two-component system to automate the task of resonance equalization. The first component is a dynamic equalizer that automatically detects resonances, to be attenuated by a user-specified factor. The second component is a deep neural network that predicts the optimal attenuation factor based on the windowed audio. The network is trained and validated on empirical data gathered from a listening experiment. We test two distinct network architectures for the predictive model and find that an agnostic network architecture operating directly on the audio signal is on a par with a network architecture that relies on hand-designed features. Both architectures significantly improve a baseline approach to predicting human-preferred resonance attenuation factors. © 2020 International Society for Music Information Retrieval. All rights reserved."
Nuttall T.; Casado M.G.; Tarifa V.N.; Repetto R.C.; Serra X.,Contributing to new musicological theories with computational methods: The case of centonization in arab-andalusian music,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097203&partnerID=40&md5=ab903a208cb3b79d23bd615d0c1a9ce2,"Nuttall T., Music Technology Group, Universitat Pompeu Fabra, Spain; Casado M.G., Music Technology Group, Universitat Pompeu Fabra, Spain; Tarifa V.N., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Arab-Andalusian music was formed in the medieval Islamic territories of the Iberian Peninsula, drawing on local traditions and assuming Arabic influences. The expert performer and researcher of the Moroccan tradition of this music, Amin Chaachoo, is developing a theory whose last formulation was recently published in La Musique Hispano-Arabe, al-Ala (2016), which argues that centonization, a melodic composition technique used in Gregorian chant, was also utilized for the creation of this repertoire. In this paper we aim to contribute to Chaachoo's theory by means of tf-idf analysis. A high-order n-gram model is applied to a corpus of 149 prescriptive transcriptions of heterophonic recordings, representing each as an unordered multiset of patterns. Computing the tf-idf statistic of each pattern in this corpus provides a means by which we can rank and compare motivic content across nawabat, distinct musical forms of the tradition. For each nawba, an empirical comparison is made between patterns identified as significant via our approach and those proposed by Chaachoo. Ultimately we observe considerable agreement between the two pattern sets and go further in proposing new, unique and as yet undocumented patterns that occur at least as frequently and with at least as much importance as those in Chaachoo's proposals. © 2020 International Society for Music Information Retrieval. All rights reserved."
Ens J.; Pasquier P.,Quantifying musical style: Ranking symbolic music based on similarity to a style,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095386&partnerID=40&md5=d92bd09e0808b9b97f9f8dba3b9880c4,"Ens J., Simon Fraser University, Canada; Pasquier P., Simon Fraser University, Canada","Modelling human perception of musical similarity is critical for the evaluation of generative music systems, musicological research, and many Music Information Retrieval tasks. Although human similarity judgments are the gold standard, computational analysis is often preferable, since results are often easier to reproduce, and computational methods are much more scalable. Moreover, computation based approaches can be calculated quickly and on demand, which is a prerequisite for use with an online system. We propose StyleRank, a method to measure the similarity between a MIDI file and an arbitrary musical style delineated by a collection of MIDI files. MIDI files are encoded using a novel set of features and an embedding is learned using Random Forests. Experimental evidence demonstrates that StyleRank is highly correlated with human perception of stylistic similarity, and that it is precise enough to rank generated samples based on their similarity to the style of a corpus. In addition, similarity can be measured with respect to a single feature, allowing specific discrepancies between generated samples and a particular musical style to be identified. © 2020 International Society for Music Information Retrieval. All rights reserved."
Böck S.; Davies M.E.P.; Knees P.,Multi-task learning of tempo and beat: Learning one to improve the other,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093880&partnerID=40&md5=696893156b001e35005534e129bd962c,"Böck S., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, TU Wien, Vienna, Austria; Davies M.E.P., INESC TEC, Porto, Portugal; Knees P., TU Wien, Vienna, Austria","We propose a multi-task learning approach for simultaneous tempo estimation and beat tracking of musical audio. The system shows state-of-the-art performance for both tasks on a wide range of data, but has another fundamental advantage: due to its multi-task nature, it is not only able to exploit the mutual information of both tasks by learning a common, shared representation, but can also improve one by learning only from the other. The multi-task learning is achieved by globally aggregating the skip connections of a beat tracking system built around temporal convolutional networks, and feeding them into a tempo classification layer. The benefit of this approach is investigated by the inclusion of training data for which tempo-only annotations are available, and which is shown to provide improvements in beat tracking accuracy. © 2020 International Society for Music Information Retrieval. All rights reserved."
Lattner S.; Dorfler M.; Arzt A.,Learning complex basis functions for invariant representations of audio,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094175&partnerID=40&md5=eb7a137853f3948ff15fa106b4e4761d,"Lattner S., Sony Computer Science Laboratories (CSL), Paris, France; Dorfler M., NuHAG, Faculty of Mathematics, University Vienna, Austria; Arzt A., Institute of Computational Perception, JKU Linz, Austria","Learning features from data has shown to be more successful than using hand-crafted features for many machine learning tasks. In music information retrieval (MIR), features learned from windowed spectrograms are highly variant to transformations like transposition or time-shift. Such variances are undesirable when they are irrelevant for the respective MIR task. We propose an architecture called Complex Autoencoder (CAE) which learns features invariant to orthogonal transformations. Mapping signals onto complex basis functions learned by the CAE results in a transformation-invariant ""magnitude space"" and a transformation-variant ""phase space"". The phase space is useful to infer transformations between data pairs. When exploiting the invariance-property of the magnitude space, we achieve state-of-the-art results in audio-to-score alignment and repeated section discovery for audio. A PyTorch implementation of the CAE, including the repeated section discovery method, is available online. © 2020 International Society for Music Information Retrieval. All rights reserved."
Tanprasert T.; Jenrungrot T.; Müller M.; Tsai T.J.,Midi-sheet music alignment using bootleg score synthesis,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094219&partnerID=40&md5=b7227cc6d3bc836c41b70c84eb970e21,"Tanprasert T., Harvey Mudd College, Claremont, CA, United States; Jenrungrot T., Harvey Mudd College, Claremont, CA, United States; Müller M., International Audio Laboratories Erlangen, Germany; Tsai T.J., Harvey Mudd College, Claremont, CA, United States","MIDI-sheet music alignment is the task of finding correspondences between a MIDI representation of a piece and its corresponding sheet music images. Rather than using optical music recognition to bridge the gap between sheet music and MIDI, we explore an alternative approach: projecting the MIDI data into pixel space and performing alignment in the image domain. Our method converts the MIDI data into a crude representation of the score that only contains rectangular floating notehead blobs, a process we call bootleg score synthesis. Furthermore, we project sheet music images into the same bootleg space by applying a deep watershed notehead detector and filling in the bounding boxes around each detected notehead. Finally, we align the bootleg representations using a simple variant of dynamic time warping. On a dataset of 68 real scanned piano scores from IMSLP and corresponding MIDI performances, our method achieves a 97:3% accuracy at an error tolerance of one second, outperforming several baseline systems that employ optical music recognition. © 2020 International Society for Music Information Retrieval. All rights reserved."
De Reuse T.; Fujinaga I.,Pattern clustering in monophonic music by learning a non-linear embedding from human annotations,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096639&partnerID=40&md5=fa763b64196c865e2eca87ca9affca42,"De Reuse T., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Canada","Musical pattern discovery algorithms find instances of repetition in symbolic music, allowing for some userspecifiable amount of variation between identified repetitions; however, they can yield an intractably large number of discovered patterns when allowing for even small amounts of variation. This is commonly addressed by defining some heuristic notion of pattern significance, and returning only the most significant patterns. This paper develops a method of pattern discovery that models human judgement of what constitutes a significant pattern by incorporating annotations of repeated patterns, avoiding the need to design heuristics. We take pattern discovery as a clustering task, where the input is a set of passages of monophonic music, represented as vectors of extracted features, and the output clusters correspond to discovered patterns. The human annotations are used to train a neural network to learn a lowdimensional embedding of the feature space that maps passages of music close together when they are occurrences of the same ground-truth pattern. The results of this approach match up with the annotations significantly better than the results of an approach using clustering without subspace learning. We provide examples of the types of patterns that this method tends to discover and discuss its feasibility and practicality as a tool for extracting useful information about repetitive structure in music. © 2020 International Society for Music Information Retrieval. All rights reserved."
Holzapfel A.; Benetos E.,Automatic music transcription and ethnomusicology: A user study,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096647&partnerID=40&md5=3b951f0fdce645f74f36b81d683ae2c2,"Holzapfel A., KTH Royal Institute of Technology, Sweden; Benetos E., Queen Mary University of London, United Kingdom","Converting an acoustic music signal into music notation using a computer program has been at the forefront of music information research for several decades, as a task referred to as automatic music transcription (AMT). However, current AMT research is still constrained to system development followed by quantitative evaluations; it is still unclear whether the performance of AMT methods is considered sufficient to be used in the everyday practice of music scholars. In this paper, we propose and carry out a user study on evaluating the usefulness of automatic music transcription in the context of ethnomusicology. As part of the study, we recruited 16 participants who were asked to transcribe short musical excerpts either from scratch or using the output of an AMT system as a basis. We collect and analyze quantitative measures such as transcription time and effort, and a range of qualitative feedback from study participants, which includes user needs, criticisms of AMT technologies, and links between perceptual and quantitative evaluations on AMT outputs. The results show no quantitative advantage of using AMT, but important indications regarding appropriate user groups and evaluation measures are provided. © 2020 International Society for Music Information Retrieval. All rights reserved."
Harasim D.; O'Donnell T.J.; Rohrmeier M.,Harmonic syntax in time rhythm improves grammatical models of harmony,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096645&partnerID=40&md5=983d49e900b8f8049c018a95b872606f,"Harasim D., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland; O'Donnell T.J., Department of Linguistics, McGill University, Canada; Rohrmeier M., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland","Music is hierarchically structured, both in how it is perceived by listeners and how it is composed. Such structure can be elegantly captured using probabilistic grammatical models similar to those used to study natural language. They address the complexity of the structure using abstract categories in a recursive formalism. Most existing grammatical models of musical structure focus on one single dimension of music - such as melody, harmony, or rhythm. While these grammar models often work well on short musical excerpts, accurate analysis of longer pieces requires taking into account the constraints from multiple domains of structure. The present paper proposes abstract product grammars - a formalism which integrates multiple dimensions of musical structure into a single grammatical model - along with efficient parsing and inference algorithms for this formalism. We use this model to study the combination of hierarchically-structured harmonic syntax and hierarchically-structured rhythmic information. The latter is modeled by a novel grammar of rhythm that is capable of expressing temporal regularities in musical phrases. It integrates grouping structure and meter. The combined model of harmony and rhythm outperforms both single-dimension models in computational experiments. All models are trained and evaluated on a treebank of hand-annotated Jazz standards. © Daniel Harasim, Timothy J. O'Donnell, Martin Rohrmeier ."
Lee J.H.; Pritchard L.; Hubbles C.,Can we listen to it together?: Factors influencing reception of music recommendations and post-recommendation behavior,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095313&partnerID=40&md5=3dce03abf980ee0303e0aa89a7a6094f,"Lee J.H., University of Washington, United States; Pritchard L., University of Washington, United States; Hubbles C., University of Washington, United States","Few prior studies on music recommendations investigate the context in which users receive the recommendations, and what impact the recommendation has on the user. In this paper, we aim to better understand the factors that affect people's decisions as to whether they choose to listen to music recommendations and how the recommendations impact their music-listening behaviors. We conducted an online survey asking about people's past experiences on giving and receiving music recommendations. We found that in addition to the aesthetic qualities of music and the respondent's taste, expectations regarding the delivery (e.g., timing, persistence) of the recommendations, familiarity, trust in the recommender's abilities, and the rationale for suggestions were important factors. We discuss the implications for the design of music recommenders based on the findings, including better rationale for and accessibility of recommended music, improved saving options, and more targeted delivery at specific times. The data also suggests disparities in how people wish to receive music recommendations and what will influence them to listen to recommendations, versus how they would like to offer recommendations to others. In addition, the findings highlight the importance of music recommendations in people's existing social relationships and their role in building/improving new relationships. © 2020 International Society for Music Information Retrieval. All rights reserved."
Thickstun J.; Harchaoui Z.; Foster D.P.; Kakade S.M.,Coupled recurrent models for polyphonic music composition,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095276&partnerID=40&md5=ab7de4eb33c91039ede91fd9aa4c1433,"Thickstun J., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Harchaoui Z., Department of Statistics, University of Washington, Seattle, WA, United States; Foster D.P., Amazon, NY, United States; Kakade S.M., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States, Department of Statistics, University of Washington, Seattle, WA, United States","This paper introduces a novel recurrent model for music composition that is tailored to the structure of polyphonic music. We propose an efficient new conditional probabilistic factorization of musical scores, viewing a score as a collection of concurrent, coupled sequences: i.e. voices. To model the conditional distributions, we borrow ideas from both convolutional and recurrent neural models; we argue that these ideas are natural for capturing music's pitch invariances, temporal structure, and polyphony. We train models for single-voice and multi-voice composition on 2,300 scores from the KernScores dataset. © 2020 International Society for Music Information Retrieval. All rights reserved."
Chen W.; Keast J.; Moody J.; Moriarty C.; Villalobos F.; Winter V.; Zhang X.; Lyu X.; Freeman E.; Wang J.; Cai S.; Kinnaird K.M.,Data usage in MIR: History & future recommendations,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094220&partnerID=40&md5=25ac68120cda610b1db1cf291f7b8163,"Chen W., Smith College, Northampton, MA, United States; Keast J., Smith College, Northampton, MA, United States; Moody J., Smith College, Northampton, MA, United States; Moriarty C., Smith College, Northampton, MA, United States; Villalobos F., Smith College, Northampton, MA, United States; Winter V., Smith College, Northampton, MA, United States; Zhang X., Smith College, Northampton, MA, United States; Lyu X., Smith College, Northampton, MA, United States; Freeman E., Smith College, Northampton, MA, United States; Wang J., Smith College, Northampton, MA, United States; Cai S., Smith College, Northampton, MA, United States; Kinnaird K.M., Smith College, Northampton, MA, United States","The MIR community faces unique challenges in terms of data access, due in large part to country-specific copyright laws. As a result, there is an emerging divide in the MIR research community between labs that have access to music through large companies with abundant funds, and independent labs at smaller institutions who do not have such expansive access. This paper explores how independent researchers have worked to overcome limitations of access to music data without contributing to the crisis of reproducibility. Acknowledging that there is no single solution for every data access problem that smaller labs face, we propose a number of possibilities for how the MIR community can bridge the gap between advancements from large companies and those within academia. As MIR looks towards the next 20 years, democratizing and expanding access to MIR research and music data is critical. Future solutions could include a distributed MIREX system, an API designed for MIR researchers, and community-led advocacy to stakeholders. © 2020 International Society for Music Information Retrieval. All rights reserved."
Falcão F.; Bozkurt B.; Serra X.; Andrade N.; Baysal O.,A dataset of rhythmic pattern reproductions and baseline automatic assessment system,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096761&partnerID=40&md5=528c64465e5be0b8cb1a2c8a6b6e7bb6,"Falcão F., Universidade Federal de Campina Grande, Brazil, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bozkurt B., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, Izmir Demokrasi University, Turkey; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Andrade N., Universidade Federal de Campina Grande, Brazil; Baysal O., Istanbul Technical University, Turkey","This work presents a novel dataset comprised of audio and jury evaluations for rhythmic pattern reproduction performances by students applying for a conservatory. Data was collected in-loco during entrance exams where students were asked to imitate a set of rhythmic patterns played by teachers. In addition to the pass or fail grades provided by the members of the jury during the exam sessions, a subset of the data was also evaluated by external annotators on a 4-level scale. A baseline automatic assessment system is presented to demonstrate the usefulness of the dataset. Preliminary results deliver an accuracy of 76% for a simple pass/fail logistic regression classifier and a mean average error of 0.59 for a linear regression grade estimator. The implementation is also made publicly available to serve as baseline for alternative assessments systems that may leverage the dataset. © 2020 International Society for Music Information Retrieval. All rights reserved."
Nieto O.; McCallum M.; Davies M.E.P.; Robertson A.; Stark A.; Egozy E.,"The harmonix set: Beats, downbeats, and functional segment annotations of western popular music",2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095398&partnerID=40&md5=69e01d6c9b04cb915c21333dd39b1565,"Nieto O., Pandora Media, Inc., Oakland, CA, United States; McCallum M., Pandora Media, Inc., Oakland, CA, United States; Davies M.E.P., INESC TEC, Porto, Portugal; Robertson A., Ableton AG, Berlin, Germany; Stark A., MIMU, London, United Kingdom; Egozy E., MIT, Cambridge, MA, United States","We introduce the Harmonix set: a collection of annotations of beats, downbeats, and functional segmentation for over 900 full tracks that covers a wide range of western popular music. Given the variety of annotated music information types in this set, and how strongly these three types of data are typically intertwined, we seek to foster research that focuses on multiple retrieval tasks at once. The dataset includes additional metadata such as MusicBrainz identifiers to support the linking of the dataset to third-party information or audio data when available. We describe the methodology employed in acquiring this set, including the annotation process and song selection. In addition, an initial data exploration of the annotations and actual dataset content is conducted. Finally, we provide a series of baselines of the Harmonix set with reference beat-trackers, downbeat estimation, and structural segmentation algorithms. © 2020 International Society for Music Information Retrieval. All rights reserved."
Parmer T.; Ahn Y.Y.,Evolution of the informational complexity of contemporary western music,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095277&partnerID=40&md5=721060746755e34d402c63c85bf1cb1a,"Parmer T., School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States; Ahn Y.Y., School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States","We measure the complexity of songs in the Million Song Dataset (MSD) in terms of pitch, timbre, loudness, and rhythm to investigate their evolution from 1960 to 2010. By comparing the Billboard Hot 100 with random samples, we find that the complexity of popular songs tends to be more narrowly distributed around the mean, supporting the idea of an inverted U-shaped relationship between complexity and hedonistic value. We then examine the temporal evolution of complexity, reporting consistent changes across decades, such as a decrease in average loudness complexity since the 1960s, and an increase in timbre complexity overall but not for popular songs. We also show, in contrast to claims that popular songs sound more alike over time, that they are not more similar than they were 50 years ago in terms of pitch or rhythm, although similarity in timbre shows distinctive patterns across eras and similarity in loudness has been increasing. Finally, we show that musical genres can be differentiated by their distinctive complexity profiles. © 2020 International Society for Music Information Retrieval. All rights reserved."
Jiang J.; Chen K.; Li W.; Xia G.,Large-vocabulary chord transcription via chord structure decomposition,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096760&partnerID=40&md5=b35e745d612d917cf77160dee0519d8a,"Jiang J., Computer Science Department, Fudan University, China, Music X Lab, NYU Shanghai, China, Machine Learning Department, Carnegie Mellon University, United States; Chen K., Computer Science Department, Fudan University, China, Music X Lab, NYU Shanghai, China; Li W., Computer Science Department, Fudan University, China; Xia G., Music X Lab, NYU Shanghai, China","While audio chord recognition systems have acquired considerable accuracy on small vocabularies (e.g., major/ minor chords), the large-vocabulary chord recognition problem still remains unsolved. This problem hinders the practical usages of audio recognition systems. The difficulty mainly lies in the intrinsic long-tail distribution of chord qualities, and most chord qualities have too few samples for model training. In this paper, we propose a new model for audio chord recognition under a huge chord vocabulary. The core concept is to decompose any chord label into a set of musically meaningful components (e.g., triad, bass, seventh), each with a much smaller vocabulary compared to the size of the overall chord vocabulary. A multitask classifier is then trained to recognize all the components given the audio feature, and then labels of individual components are reassembled to form the final chord label. Experiments show that the proposed system not only achieves state-of-the-art results on traditional evaluation metrics but also performs well on a large vocabulary. © 2020 International Society for Music Information Retrieval. All rights reserved."
Foroughmand H.; Peeters G.,Deep-rhythm for tempo estimation and rhythm pattern recognition,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093890&partnerID=40&md5=0b6040b8707040a0b89530d145343cfd,"Foroughmand H., IRCAM Lab, CNRS, Sorbonne Université, France; Peeters G., LTCI, Télécom Paris, Institut Polytechnique de Paris, France","It has been shown that the harmonic series at the tempo frequency of the onset-strength-function of an audio signal accurately describes its rhythm pattern and can be used to perform tempo or rhythm pattern estimation. Recently, in the case of multi-pitch estimation, the depth of the input layer of a convolutional network has been used to represent the harmonic series of pitch candidates. We use a similar idea here to represent the harmonic series of tempo candidates. We propose the Harmonic-Constant-Q-Modulation which represents, using a 4D-tensors, the harmonic series of modulation frequencies (considered as tempo frequencies) in several acoustic frequency bands over time. This representation is used as input to a convolutional network which is trained to estimate tempo or rhythm pattern classes. Using a large number of datasets, we evaluate the performance of our approach and compare it with previous approaches. We show that it slightly increases Accuracy-1 for tempo estimation but not the average-mean-Recall for rhythm pattern recognition. © 2020 International Society for Music Information Retrieval. All rights reserved."
Li B.; Kumar A.,Query by video: Cross-modal music retrieval,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096929&partnerID=40&md5=a12c9ab681ee3a332b9705820c115c1e,"Li B., University of Rochester, Rochester, NY, United States; Kumar A., Spotify, New York, NY, United States","Cross-modal retrieval learns the relationship between the two types of data in a common space so that an input from one modality can retrieve data from a different modality. We focus on modeling the relationship between two highly diverse data, music and real-world videos. We learn crossmodal embeddings using a two-stream network trained with music-video pairs. Each branch takes one modality as the input and it is constrained with emotion tags. Then the constraints allow the cross-modal embeddings to be learned with significantly fewer music-video pairs. To retrieve music for an input video, the trained model ranks tracks in the music database by cross-modal distances to the query video. Quantitative evaluations show high accuracy of audio/video emotion tagging when evaluated on each branch independently and high performance for cross-modal music retrieval. We also present crossmodal music retrieval experiments on Spotify music using user-generated videos from Instagram and Youtube as queries, and subjective evaluations show that the proposed model can retrieve relevant music. We present the music retrieval results at: http://www.ece.rochester. edu/~bli23/projects/query.html. © 2020 International Society for Music Information Retrieval. All rights reserved."
Doras G.; Peeters G.,Cover detection using dominant melody embeddings,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096926&partnerID=40&md5=2a914a267a14d6dc0bfb24320421fe7a,"Doras G., Sacem & Ircam Lab, CNRS, Sorbonne Université, France; Peeters G., LTCI, Telecom Paris, Institut Polytechnique de Paris, France","Automatic cover detection - the task of finding in an audio database all the covers of one or several query tracks- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use. In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction - that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds. © 2020 International Society for Music Information Retrieval. All rights reserved."
Flexer A.; Lallai T.,Can we increase inter- and intra-rater agreement in modeling general music similarity?,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096736&partnerID=40&md5=ccb08895e949ecc84c77ac2a4942693e,"Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Lallai T., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","We present a pilot study on ways to increase inter- and intra-rater agreement in quantification of general similarity between pieces of music. By using a more controlled group of human subjects and carefully curating song material, we try to increase overall agreement between raters concerning the perceived general similarity of songs. Repeated conduction of the experiment with a two week lag shows that intra-rater agreement is higher than inter-rater agreement. Analysis of the results and interviews with test subjects suggests that the genre of songs was a major factor in judging similarity between songs. We discuss the impacts of our results on evaluation of respective machine learning models and question the validity of experiments on general music similarity. © 2020 International Society for Music Information Retrieval. All rights reserved."
Kim J.W.; Bello J.P.,Adversarial learning for improved onsets and frames music transcription,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095765&partnerID=40&md5=e66e619ce896b4dcbc82970ebd0555ad,"Kim J.W., Music and Audio Research Lab, New York University, United States; Bello J.P., Music and Audio Research Lab, New York University, United States","Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements on transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the timefrequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both framelevel and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis. © 2020 International Society for Music Information Retrieval. All rights reserved."
Bittner R.; Bosch J.J.,Generalized metrics for Single-F0 estimation evaluation,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097028&partnerID=40&md5=281a2dffe18de0638312a88e1ec167de,"Bittner R., Spotify, United States; Bosch J.J., Spotify, United States","Single-f0 estimation methods, including pitch trackers and melody estimators, have historically been evaluated using a set of common metrics which score estimates frame-wise in terms of pitch and voicing accuracy. ""Voicing"" refers to whether or not a pitch is active, and has historically been regarded as a binary value. However, this has limitations because it is often ambiguous whether a pitch is present or absent, making a binary choice difficult for humans and algorithms alike. For example, when a source fades out or reverberates, the exact point where the pitch is no longer present is unclear. Many single-f0 estimation algorithms select a threshold for when a pitch is active or not, and different choices of threshold drastically affect the results of standard metrics. In this paper, we present a refinement on the existing single-f0 metrics, by allowing the estimated voicing to be represented as a continuous likelihood, and introducing a weighting on frame level pitch accuracy, which considers the energy of the source producing the f0 relative to the energy of the rest of the signal. We compare these metrics experimentally with the previous metrics using a number of algorithms and datasets and discuss the fundamental differences. We show that, compared to the previous metrics, our proposed metrics allow threshold-independent algorithm comparisons. © 2020 International Society for Music Information Retrieval. All rights reserved."
Madhusudhan S.T.; Chowdhary G.,Deepsrgm - Sequence classification and ranking in Indian classical music with deep learning,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096972&partnerID=40&md5=d1304545153ee30a9da6b2723c24a337,"Madhusudhan S.T., University of Illinois, Urbana Champaign, United States; Chowdhary G., University of Illinois, Urbana Champaign, United States","A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient prepossessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence. © 2020 International Society for Music Information Retrieval. All rights reserved."
Muller M.; Zalkow F.,FMP notebooks: Educational material for teaching and learning fundamentals of music processing,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096730&partnerID=40&md5=3e40bd3fafe6c56693938ad2c7b94014,"Muller M., International Audio Laboratories Erlangen, Germany; Zalkow F., International Audio Laboratories Erlangen, Germany","In this paper, we introduce a novel collection of educational material for teaching and learning fundamentals of music processing (FMP) with a particular focus on the audio domain. This collection, referred to as FMP notebooks, discusses well-established topics in Music Information Retrieval (MIR) as motivating application scenarios. The FMP notebooks provide detailed textbook-like explanations of central techniques and algorithms in combination with Python code examples that illustrate how to implement the theory. All components including the introductions of MIR scenarios, illustrations, sound examples, technical concepts, mathematical details, and code examples are integrated into a consistent and comprehensive framework based on Jupyter notebooks. The FMP notebooks are suited for studying the theory and practice, for generating educational material for lectures, as well as for providing baseline implementations for many MIR tasks, thus addressing students, teachers, and researchers. © Meinard Müller, Frank Zalkow."
Zhou Y.; Chu W.; Young S.; Chen X.,"Bandnet: A neural network-based, multi-instrument beatles-style midi music composition machine",2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094399&partnerID=40&md5=bd038e7ceafde648265fe52d5ada76b0,"Zhou Y., Snap Inc., 63 Market St, Venice, 90291, CA, United States, Department of EECS, University of California, Berkeley, United States; Chu W., Snap Inc., 63 Market St, Venice, 90291, CA, United States; Young S., Snap Inc., 63 Market St, Venice, 90291, CA, United States, Herb Alpert School of Music, University of California, Los Angeles, United States; Chen X., Snap Inc., 63 Market St, Venice, 90291, CA, United States","In this paper, we propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' music and generate full songs in the style of the Beatles with little human intervention. In the learning stage, a sequence of stylistically uniform, multiple-channel music samples was modeled by an RNN. In the composition stage, a short clip of randomly-generated music was used as a seed for the RNN to start music score prediction. To form structured music, segments of generated music from different seeds were concatenated together. To improve the quality and structure of the generated music, we integrated music theory knowledge into the model, such as controlling the spacing of gaps in the vocal melody, normalizing the timing of chord changes, and requiring notes to be related to the song's key (C major, for example). This integration improved the quality of the generated music as verified by a professional composer. We also conducted a subjective listening test that showed our generated music was close to original music by the Beatles in terms of style similarity, professional quality, and interestingness. The generated music samples can be downloaded at https://goo.gl/uaLXoB. © 2020 International Society for Music Information Retrieval. All rights reserved."
Behrooz M.; Mennicken S.; Thom J.; Cramer R.K.H.,Augmenting music listening experiences on voice assistants,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095122&partnerID=40&md5=44c7870d3f0a600178333bccd2793ef6,"Behrooz M., University of California Santa Cruz, Santa Cruz, United States; Mennicken S., Spotify USA, Somerville, MA, United States; Thom J., Spotify USA, Somerville, MA, United States; Cramer R.K.H., Spotify USA, Somerville, MA, United States","Voice interfaces have rapidly gained popularity, introducing the opportunity for new ways to explore new interaction paradigms for music. However, most interactions with music in current consumer voice devices are still relatively transactional; primarily allowing for keyword-based commands and basic content playback controls. They are less likely to contextualize content or support content discovery beyond what users think to ask for. We present an approach to dynamically augment the voice-based music experience with background information using story generation techniques. Our findings indicate that augmentation can have positive effects on voice-based music experiences, given the right user context and mindset. © 2020 International Society for Music Information Retrieval. All rights reserved."
Meseguer-Brocal G.; Peeters G.,Conditioned-U-Net: Introducing a control mechanism in the U-net for multiple source separations,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094154&partnerID=40&md5=7641e990bd7b4a9e5926a5293077f772,"Meseguer-Brocal G., Ircam Lab, CNRS, Sorbonne Université, Paris, 75004, France; Peeters G., LTCI, Télécom Paris, Institut Polytechnique de Paris, Paris, 75013, France","Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically trained for a single task, e.g. a particular instrument isolation. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net) which adds a control mechanism to the standard U-Net. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The CU- Net decides the instrument to isolate according to a onehot- encoding input vector. The input vector is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost. © 2020 International Society for Music Information Retrieval. All rights reserved."
Parada-Cabaleiro E.; Batliner A.; Schuller B.,Diplomatic edition of Il Lauro secco: Ground truth for OMR of white mensural notation,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096057&partnerID=40&md5=7b72a28157995f28e9730c69465ad0bb,"Parada-Cabaleiro E., ZD.B Dipartment of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, Instituto Complutense de Ciencias Musicales (ICCMU), Universidad Complutense de Madrid, Spain; Batliner A., ZD.B Dipartment of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schuller B., ZD.B Dipartment of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, GLAM - Group on Language, Audio & Music, Imperial College London, United Kingdom","Early musical sources in white mensural notation-the most common notation in European printed music during the Renaissance-are nowadays preserved by libraries worldwide trough digitalisation. Still, the application of music information retrieval to this repertoire is restricted by the use of digitalisation techniques which produce an uncodified output. Optical Music Recognition (OMR) automatically generates a symbolic representation of imagebased musical content, thus making this repertoire reachable from the computational point of view; yet, further improvements are often constricted by the limited ground truth available. We address this lacuna by presenting a symbolic representation in original notation of Il Lauro Secco, an anthology of Italian madrigals in white mensural notation. For musicological analytic purposes, we encoded the repertoire in ∗∗mens and MEI formats; for OMR ground truth, we automatically codified the repertoire in agnostic and semantic formats, via conversion from the ∗∗mens files. © Emilia Parada-Cabaleiro, Anton Batliner, Björn Schuller."
Román M.A.; Pertusa A.; Calvo-Zaragoza J.,Holistic approach to polyphonic music transcription with neural networks,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095528&partnerID=40&md5=1e59b95d558ce169e6afb25b9793e0b4,"Román M.A., U.I. for Computing Research, University of Alicante, Spain; Pertusa A., U.I. for Computing Research, University of Alicante, Spain; Calvo-Zaragoza J., U.I. for Computing Research, University of Alicante, Spain","We present a framework based on neural networks to extract music scores directly from polyphonic audio in an end-to-end fashion. Most previous Automatic Music Transcription (AMT) methods seek a piano-roll representation of the pitches, that can be further transformed into a score by incorporating tempo estimation, beat tracking, key estimation or rhythm quantization. Unlike these methods, our approach generates music notation directly from the input audio in a single stage. For this, we use a Convolutional Recurrent Neural Network (CRNN) with Connectionist Temporal Classification (CTC) loss function which does not require annotated alignments of audio frames with the score rhythmic information. We trained our model using as input Haydn, Mozart, and Beethoven string quartets and Bach chorales synthesized with different tempos and expressive performances. The output is a textual representation of four-voice music scores based on ∗∗kern format. Although the proposed approach is evaluated in a simplified scenario, results show that this model can learn to transcribe scores directly from audio signals, opening a promising avenue towards complete AMT. © 2020 International Society for Music Information Retrieval. All rights reserved."
Park S.; Kwon T.; Lee J.; Kim J.; Nam J.,A Cross-scape plot representation for visualizing symbolic melodic similarity,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096627&partnerID=40&md5=28b6a381b35d5de0164482a56d1b6c71,"Park S., Graduate School of Culture Technology, KAIST, South Korea; Kwon T., Graduate School of Culture Technology, KAIST, South Korea; Lee J., Graduate School of Culture Technology, KAIST, South Korea; Kim J., Graduate School of Culture Technology, KAIST, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Symbolic melodic similarity is based on measuring a pairwise distance between two songs from diverse perspectives. The distance is usually summarized as a single value for song retrieval. This obscures observing the details of similarity patterns within the two songs. In this paper, we propose a cross-scape plot representation to visualize multi-scaled melody similarity between two symbolic music encodings. The cross-scape plot is computed by stacking up a minimum local distance between two segments from each of the two songs. As the layer goes up, the segment size increases and it computes incrementally more long-term distances. This hierarchical representation allows for capturing the location and length of similar segments between two songs in a visually intuitive manner. We show the effectiveness of the cross-scape plot by evaluating it on examples from folk music collections with similarity-based categories and plagiarism cases. © 2020 International Society for Music Information Retrieval. All rights reserved."
Huang Y-F.; Chen T.-P.; Moran N.; Coleman S.; Su L.,Identifying expressive semantics in orchestral conducting kinematics,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096506&partnerID=40&md5=55314d2e04b4a634efbed2e552a4022e,"Huang Y-F., Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan; Chen T.-P., Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan; Moran N., Reid School of Music, University of Edinburgh, United Kingdom; Coleman S., Institute for Sport, Physical Education and Health Sciences, University of Edinburgh, United Kingdom; Su L., Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan","Existing kinematic research on orchestral conducting movement contributes to beat-tracking and the delivery of performance dynamics. Methodologically, such movement cues have been treated as distinct, isolated events. Yet as practicing musicians and music pedagogues know, conductors' expressive instructions are highly flexible and dependent on the musical context. We seek to demonstrate an approach to search for effective descriptors to express musical features in conducting movement in a valid music context, and to extract complex expressive semantics from elementary conducting kinematic variations. This study therefore proposes a multi-task learning model to jointly identify dynamic, articulation, and phrasing cues from conducting kinematics. A professional conducting movement dataset is compiled using a high-resolution motion capture system. The ReliefF algorithm is applied to select significant features from conducting movement, and recurrent neural network (RNN) is implemented to identify multiple movement cues. The experimental results disclose key elements in conducting movement which communicate musical expressiveness; the results also highlight the advantage of multi-task learning in the complete musical context over single-task learning. To the best of our knowledge, this is the first attempt to use recurrent neural network to explore multiple semantic expressive cuing in conducting movement kinematics. © 2020 International Society for Music Information Retrieval. All rights reserved."
Lee K.; Nam J.,Learning a joint embedding space of monophonic and mixed music signals for singing voice,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096988&partnerID=40&md5=02a013fa7cf720ee5af0d40f014154ae,"Lee K., Graduate School of Culture Technology, KAIST, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and query-by-singer in both the in-domain and cross-domain tasks. © 2020 International Society for Music Information Retrieval. All rights reserved."
MacKinlay D.; Botev Z.,Mosaic style transfer using sparse autocorrelograms,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095657&partnerID=40&md5=033acde8f374b7dba2d0ecfbd0de080f,"MacKinlay D., School of Mathematics and Statistics, UNSW Sydney, Australia; Botev Z., School of Mathematics and Statistics, UNSW Sydney, Australia","We introduce a novel mosaic synthesis algorithm for musical style transfer using the autocorrelogram as a feature map. We decompose the autocorrelogram feature map sparsely in a decaying sinusoid basis, using that decomposition as an interpolation scheme in feature space. This efficiently provides gradient information in the mosaicing optimization, including gradients of the challenging time-scale parameters, which are usually computationally intractable for discretely sampled signals. The required calculations are straightforward to parallelize on vectorprocessing hardware. Our implementation of the method provides good quality output and novel musical effects in example tasks by itself and can also be integrated into alternative mosaicing methods. © 2020 International Society for Music Information Retrieval. All rights reserved."
Rosenzweig S.; Scherbaum F.; Müller M.,Detecting stable regions in frequency trajectories for tonal analysis of traditional georgian vocal music,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096985&partnerID=40&md5=52a47ca274d620d895d97b0f5be364fb,"Rosenzweig S., International Audio Laboratories Erlangen, Germany; Scherbaum F., University of Potsdam, Potsdam, Germany; Müller M., International Audio Laboratories Erlangen, Germany","While Georgia has a long history of orally transmitted polyphonic singing, there is still an ongoing controversial discussion among ethnomusicologists on the tuning system underlying this type of music. First attempts have been made to analyze tonal properties (e. g., harmonic and melodic intervals) based on fundamental frequency (F0) trajectories. One major challenge in F0-based tonal analysis is introduced by unstable regions in the trajectories due to pitch slides and other frequency fluctuations. In this paper, we describe two approaches for detecting stable regions in frequency trajectories: the first algorithm uses morphological operations inspired by image processing, and the second one is based on suitably defined binary time-frequency masks. To avoid undesired distortions in subsequent analysis steps, both approaches keep the original F0-values unmodified, while only removing F0-values in unstable trajectory regions. We evaluate both approaches against manually annotated stable regions and discuss their potential in the context of interval analysis for traditional three-part Georgian singing. © 2020 International Society for Music Information Retrieval. All rights reserved."
Shibata G.; Nishikimi R.; Nakamura E.; Yoshii K.,"Statistical music structure analysis based on a Homogeneity-, Repetitiveness-, and regularity-aware hierarchical hidden Semi-markov model",2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096105&partnerID=40&md5=9771ef48f007564005bd7b0b157954ad,"Shibata G., Graduate School of Informatics, Kyoto University, Japan; Nishikimi R., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","This paper describes a music structure analysis method that splits music audio signals into meaningful segments such as musical sections and clusters them. In this task, how to model the four fundamental aspects of musical sections, i.e., homogeneity, repetitiveness, novelty, and regularity, in a unified way is still an open problem. Here we propose a solid statistical approach based on a homogeneity-, repetitiveness-, and regularity-aware hierarchical hidden semi-Markov model. The higher-level semi-Markov chain represents a sequence of sections that tend to have regularly spaced boundaries. The timbral features in each section are assumed to follow emission distributions that are homogeneous over time. The lower-level left-to-right Markov chain in each section represents a chord sequence whose sequential order is constrained to be a repetition of a chord sequence in another section of the same cluster. The whole model can be trained unsupervisedly based on Bayesian sparse learning where unnecessary sections automatically degenerate. The proposed method outperformed representative methods in segmentation and clustering accuracies with estimated sections having similar statistical properties as the ground truth data. © G. Shibata, R. Nishikimi, E. Nakamura, and K. Yoshii."
Choi J.; Lee J.; Park J.; Nam J.,Zero-shot learning for audio-based music classification and tagging,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094321&partnerID=40&md5=7aee708efe36f96743500d8040fae2de,"Choi J., Graduate School of Culture Technology, KAIST, South Korea; Lee J., Graduate School of Culture Technology, KAIST, South Korea; Park J., NAVER Corp, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where side information about the labels is used to unveil the relationship between each other. In this work, we investigate the zero-shot learning in the music domain and organize two different setups of side information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations. Considering a music track is usually multilabeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning. Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in the music domain. © 2020 International Society for Music Information Retrieval. All rights reserved."
Karsdorp F.; Van Kranenburg P.; Manjavacas E.,Learning similarity metrics for melody retrieval,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094564&partnerID=40&md5=acdd16aa53ad3c8c52f8f38615a7d0bf,"Karsdorp F., KNAW Meertens Instituut, Netherlands; Van Kranenburg P., KNAW Meertens Instituut, Netherlands, Utrecht University, Netherlands; Manjavacas E., University of Antwerp, Belgium","Similarity measures are indispensable in music information retrieval. In recent years, various proposals have been made for measuring melodic similarity in symbolically encoded scores. Many of these approaches are ultimately based on a dynamic programming approach such as sequence alignment or edit distance, which has various drawbacks. First, the similarity scores are not necessarily metrics and are not directly comparable. Second, the algorithms are mostly first-order and of quadratic timecomplexity, and finally, the features and weights need to be defined precisely. We propose an alternative approach which employs deep neural networks for end-to-end similarity metric learning. We contrast and compare different recurrent neural architectures (LSTM and GRU) for representing symbolic melodies as continuous vectors, and demonstrate how duplet and triplet loss functions can be employed to learn compact distributional representations of symbolic music in an induced melody space. This approach is contrasted with an alignment-based approach. We present results for the Meertens Tune Collections, which consists of a large number of vocal and instrumental monophonic pieces from Dutch musical sources, spanning five centuries, and demonstrate the robustness of the learned similarity metrics. © 2020 International Society for Music Information Retrieval. All rights reserved."
Finkensiep C.; Widdess R.; Rohrmeier M.,Modelling the syntax of north indian melodies with a generalized graph grammar,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094289&partnerID=40&md5=f90b583179067d59d3ffdaa51fc969f3,"Finkensiep C., EPFL, Switzerland; Widdess R., SOAS University of London, United Kingdom; Rohrmeier M., EPFL, Switzerland","Hierarchical models of music allow explanation of highly complex musical structure based on the general principle of recursive elaboration and a small set of orthogonal operations. Recent approaches to melodic elaboration have converged to a representation based on intervals, which allows the elaboration of pairs of notes. However, two problems remain: First, an interval-first representation obscures one-sided operations like neighbor notes. Second, while models of Western melody styles largely agree on stepwise operations such as neighbors and passing notes, larger intervals are either attributed to latent harmonic properties or left unexplained. This paper presents a grammar for melodies in North Indian raga music, showing not only that recursively applied neighbor and passing note operations underlie this style as well, but that larger intervals are generated as generalized neighbors, based on the tonal hierarchy of the underlying scale structure. The notion of a generalized neighbor is not restricted to ragas but can be transferred to other musical styles, opening new perspectives on latent structure behind melodies and music in general. The presented grammar is based on a graph representation that allows one to express elaborations on both notes and intervals, unifying and generalizing previous graphand tree-based approaches. © 2020 International Society for Music Information Retrieval. All rights reserved."
Maia L.S.; Fuentes M.; Biscainho L.W.P.; Rocamora M.; Essid S.,Sambaset: A dataset of historical samba de enredo recordings for computational music analysis,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096223&partnerID=40&md5=4801fcc978ab2020621c02f29e0de32c,"Maia L.S., Federal University of Rio de Janeiro, Brazil, LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Fuentes M., LTCI, Télécom Paris, Institut Polytechnique de Paris, France, L2S, CNRS-Université Paris-Sud-Centrale Supélec, France; Biscainho L.W.P., Federal University of Rio de Janeiro, Brazil; Rocamora M., Universidad de la República, Uruguay; Essid S., LTCI, Télécom Paris, Institut Polytechnique de Paris, France","In the last few years, several datasets have been released to meet the requirements of ""hungry"" yet promising datadriven approaches in music technology research. Since, for historical reasons, most investigations conducted in the field still revolve around music of the so-called ""Western"" tradition, the corresponding data, methodology and conclusions carry a strong cultural bias. Music of non- ""Western"" background, whenever present, is usually underrepresented, poorly labeled, or even mislabeled, the exception being projects that aim at specifically describing such music. In this paper we present SAMBASET, a dataset of Brazilian samba music that contains over 40 hours of historical and modern samba de enredo commercial recordings. To the best of our knowledge, this is the first dataset of this genre. We describe the collection of metadata (e.g. artist, composer, release date) and outline our semiautomatic approach to the challenging task of annotating beats in this large dataset, which includes the assessment of the performance of state-of-the-art beat tracking algorithms for this specific case. Finally, we present a study on tempo and beat tracking that illustrates SAMBASET's value, and we comment on other tasks for which it could be used. © 2020 International Society for Music Information Retrieval. All rights reserved."
Rigaux P.; Travers N.,Scalable searching and ranking for melodic pattern queries,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095650&partnerID=40&md5=57dd03b0a326db8c6e9672520614738d,"Rigaux P., Cedric Lab, CNAM, France; Travers N., Leonard de Vinci, Research Center, Cedric Lab, CNAM, France","We present the design and implementation of a scalable search engine for large Digital Score Libraries. It covers the core features expected from an information retrieval system. Music representation is pre-processed, simplified and normalized. Collections are searched for scores that match a melodic pattern, results are ranked on their similarity with the pattern, and matching fragments are finally identified on the fly. Moreover, all these features are designed to be integrated in a standard search engine and thus benefit from the horizontal scalability of such systems. Our method is fully implemented, and relies on ELASTICSEARCH for collection indexing. We describe its main components, report and study its performances. © 2020 International Society for Music Information Retrieval. All rights reserved."
Smith J.B.L.; Kawasaki Y.; Goto M.,Unmixer: An interface for extracting and remixing loops,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096067&partnerID=40&md5=d01f287f713b75a60d3186620e52f603,"Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Kawasaki Y., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","To create their art, remix artists would like to have segmented stem tracks at their disposal; that is, isolated instances of the loops and sounds that the original composer used to create a track. We present Unmixer, a web service that will analyze and extract loops from any audio uploaded by a user. The loops are presented in an interface that allows users to immediately remix the loops; if users upload multiple tracks, they can easily create mashups with the loops, which are automatically matched in tempo. To analyze the audio, we use a recently-proposed method of source separation based on the nonnegative Tucker decomposition of the spectrum. To reduce interference among the extracted loops, we propose an extra factorization step with a sparseness constraint and demonstrate that it improves the source separation result. We also propose a method for selecting the best instances of the extracted loops and demonstrate its effectiveness in an evaluation. Both of these improvements are incorporated into the backend of the interface. Finally, we discuss the feedback collected in a set of user evaluations. © 2020 International Society for Music Information Retrieval. All rights reserved."
Park J.; Choi K.; Jeon S.; Kim D.; Park J.,A Bi-directional transformer for musical chord recognition,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095098&partnerID=40&md5=4d5fd2adc9517635d878caa5a61453cf,"Park J., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Choi K., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Jeon S., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Kim D., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Park J., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea","Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model. In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance. © 2020 International Society for Music Information Retrieval. All rights reserved."
Ferreira L.N.; Whitehead J.,Learning to generate music with sentiment,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094164&partnerID=40&md5=b2d2008d0b9de3429f6e72c801abab8d,"Ferreira L.N., University of California, Santa Cruz, Department of Computational Media, United States; Whitehead J., University of California, Santa Cruz, Department of Computational Media, United States","Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy. A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous. © 2020 International Society for Music Information Retrieval. All rights reserved."
Cífka O.; Şimşekli U.; Richard G.,Supervised symbolic music style translation using synthetic data,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095493&partnerID=40&md5=8b453fef8d3b9cfe92f45dc35d5b6ac3,"Cífka O., LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Şimşekli U., LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Richard G., LTCI, Télécom Paris, Institut Polytechnique de Paris, France","Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style. More recently, several attempts have been made to extend such approaches to music (both symbolic and audio) in order to enable transforming musical style in a similar manner. In this study, we focus on symbolic music with the goal of altering the 'style' of a piece while keeping its original 'content'. As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of 'aligned' data (i.e. the same musical piece played in multiple styles), we develop the first fully supervised algorithm for this task. At the core of our approach lies a synthetic data generation scheme which allows us to produce virtually unlimited amounts of aligned data, and hence avoid the above issue. In view of this data generation scheme, we propose an encoder-decoder model for translating symbolic music accompaniments between a number of different styles. Our experiments show that our models, although trained entirely on synthetic data, are capable of producing musically meaningful accompaniments even for real (non-synthetic) MIDI recordings. © 2020 International Society for Music Information Retrieval. All rights reserved."
Hu X.; Que Y.; Kando N.; Lian W.,Analyzing user interactions with music information retrieval system: An eye-tracking approach,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095095&partnerID=40&md5=5f3e9348bf00b092e8bbb1b3a7653144,"Hu X., University of Hong Kong, Hong Kong; Que Y., University of Hong Kong, Hong Kong; Kando N., National Institute of Informatics, Japan; Lian W., University of Hong Kong, Hong Kong","There has been little research considering eye movement as a measure when assessing user interactions with music information retrieval (MIR) systems, whereas many studies have adopted conventional user-centered measures such as user effectiveness and user perception. To bridge this research gap, this study investigates users' eye movement patterns and measures with two music retrieval tasks and two interface presentation modes. A user experiment was conducted with 16 participants whose eye movement and mouse click behaviors were recorded through professional eye trackers. Through analyzing visual patterns of eye gazes and movements as well as various metrics in prominent Areas of Interest (AOI), it is found that users' eye movement behaviors were related to task type. Besides, the results also disclosed that some eye movement metrics were related to both user effectiveness and user perception, and influenced by user characteristics. It is also found that some eye movement and user effectiveness metrics can be used to predict user perception. This study allows researchers to gain a deeper insight into user interactions with MIR systems from the perspective of eye movement measure. © 2020 International Society for Music Information Retrieval. All rights reserved."
McFee B.; Kinnaird K.M.,Improving structure evaluation through automatic hierarchy expansion,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093919&partnerID=40&md5=41ae65872090ab34e73629208e5781fc,"McFee B., Music and Audio Research Lab, Center for Data Science, New York University, United States; Kinnaird K.M., Department of Computer Science, Statistical and Data Sciences Program, Smith College, United States","Structural segmentation is the task of partitioning a recording into non-overlapping time intervals, and labeling each segment with an identifying marker such as A, B, or verse. Hierarchical structure annotation expands this idea to allow an annotator to segment a song with multiple levels of granularity. While there has been recent progress in developing evaluation criteria for comparing two hierarchical annotations of the same recording, the existing methods have known deficiencies when dealing with inexact label matchings and sequential label repetition. In this article, we investigate methods for automatically enhancing structural annotations by inferring (and expanding) hierarchical information from the segment labels. The proposed method complements existing techniques for comparing hierarchical structural annotations by coarsening or refining labels with variation markers to either collapse similarly labeled segments together, or separate identically labeled segments from each other. Using the multi-level structure annotations provided in the SALAMI dataset, we demonstrate that automatic hierarchy expansion allows structure comparison methods to more accurately assess similarity between annotations. © 2020 International Society for Music Information Retrieval. All rights reserved."
Low T.; Hentschel C.; Polley S.; Das A.; Sack H.; Nürnberger A.; Stober S.,The Ismir explorer - A visual interface for exploring 20 years of Ismir publications,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095814&partnerID=40&md5=a8ce6f4d76ba3ef028eb2ca94c3c5321,"Low T., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Hentschel C., Hasso Plattner Institute for IT Systems Engineering, University of Potsdam, Germany; Polley S., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Das A., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Sack H., FIZ Karlsruhe, Leibniz Institute for Information Infrastructure, Karlsruhe, Germany; Nürnberger A., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Stober S., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany","Ever since the first International SymposiumonMusic InformationRetrieval in 2000, the proceedings have beenmade publicly available to interested researchers. After 20 years of annual conferences and workshops, this number has grown to an impressive amount of almost 2,000 papers. When restricted to linear search and retrieval in a document collection of this size, it becomes inherently hard to identify topics, related work and trends in scientific research. Therefore, this paper presents and evaluates a map-based user interface for exploring 20 years of ISMIR publications. The interface visualizes k-nearest neighbor subsets of semantically similar papers. Users may jump from one neighborhood to the next by selecting another paper from the current subset. Through animated transitions between local k-nnmaps, the interface creates the impression of panning a large global map. Evaluation results of a small user study suggest that users are able to discover interesting links between papers. Due to its generic approach, the interface is easily applicable to other document collections as well. The search interface and its source code are made publicly available. © 2020 International Society for Music Information Retrieval. All rights reserved."
Fu Z.-S.; Su L.,Hierarchical classification networks for singing voice segmentation and transcription,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095418&partnerID=40&md5=16ee32256c9860dd21b6939b78cc553f,"Fu Z.-S., Dept. EE, National Taiwan University, Taiwan; Su L., IIS, Academia Sinica, Taiwan","Identifying the onset and offset time of a note is a challenging step in singing voice transcription, as the soft onset/ offset, portamento, and vibrato phenomena are rich in singing voice signals. In this work, we utilize various types of signal representations with deep learning for onset and offset detection of monophonic singing voice. We consider onset and offset detection as a hierarchical classification problem, where every input segment is classified into one of all the possible states in monophonic singing, namely the silence, activation, and transition states,where the transition state is further classified into the onset and offset states. An objective function based on this hierarchical taxonomy nicely guides the model to capture complicated temporal dynamics of note sequences. Multiple input signal representations containing spectral differences and pitch saliency are employed to jointly enhance such temporal patterns. The proposed method implemented with residual networks provides improved performance over prior art in onset and offset detection. Moreover, by integrating with a pitch detection framework, the proposed method also outperforms previous singing voice transcription methods. This result emphasizes the importance of note segmentation in singing voice transcription. © 2020 International Society for Music Information Retrieval. All rights reserved."
Sioros G.; Câmara G.S.; Danielsen A.,Mapping timing strategies in drum performance,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094847&partnerID=40&md5=e524b8943bc6886e73b08b9a20007ec8,"Sioros G., RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, Department of Musicology, University of Oslo, Norway; Câmara G.S., RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, Department of Musicology, University of Oslo, Norway; Danielsen A., RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, Department of Musicology, University of Oslo, Norway","How do drummers express different timing styles? We conducted an experiment in which we asked twenty-two professional drummers to perform a simple rhythmic pattern while listening to a metronome. Here, we investigate the strategies they employed to express three different instructed timing profiles for the same pattern: ""On"" ""Pushed"" and Laid-back. Our analysis of the recordings follows three stages. First, we compute sixteen boolean features that capture the microtiming relations of the kick, snare and hi-hat drum onsets, between each other and with regards to the metrical grid. Second, we construct a microtiming profile (mtP) for every performance by averaging the boolean features across the recording. An mtP codifies the frequency with which the various features were found in a performance. Third, through a ""similarity profiles"" hierarchical clustering analysis, we identify groups of recordings with significant similarities in their mtPs. We found distinct strategies to express each intended timing profile that employ specific combinations of relations between the instruments and with regards to the meter. Finally, we created a map that summarizes the main characteristics of the strategies and their relations using a phylogenetic tree visualization. © 2020 International Society for Music Information Retrieval. All rights reserved."
Tymoczko D.; Gotham M.; Cuthbert M.S.; Ariza C.,The romantext format: A flexible and standard method for representing roman numeral analyses,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076722082&partnerID=40&md5=1de6fd68b9ff4a66d90078b979185bf7,"Tymoczko D., Princeton University, NJ, United States; Gotham M., Cornell University, NY, United States; Cuthbert M.S., M.I.T., MA, United States; Ariza C.","Roman numeral analysis has been central to the Western musician's toolkit since its emergence in the early nineteenth century: it is an extremely popular method for recording subjective analytical decisions about the chords and keys implied by a passage of music. Disagreements about these judgments have led to extensive theoretical debates and ongoing controversies. Such debates are exacerbated by the absence of a public corpus of expert Roman numeral analyses, and by the more fundamental lack of an agreed-upon, computer-readable syntax in which those analyses might be expressed. This paper specifies such a standard, along with an associated code library in music21, and a preliminary set of example corpora. To frame the project, we review some of the motivations for doing harmonic analysis, some reasons why it resists automation, and some prospective uses for our tools. © 2020 International Society for Music Information Retrieval. All rights reserved."
Manzelli R.; Thakkar V.; Siahkamari A.; Kulis B.,Conditioning deep generative raw audio models for structured automatic music,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067130314&partnerID=40&md5=45f8c9d17e572c0c79dacd1e9eadfd76,"Manzelli R., ECE Department, Boston University, United States; Thakkar V., ECE Department, Boston University, United States; Siahkamari A., ECE Department, Boston University, United States; Kulis B., ECE Department, Boston University, United States","Existing automatic music generation approaches that feature deep learning can be broadly classified into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind’s WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work. © Rachel Manzelli, Vijay Thakkar, Ali Siahkamari, Brian Kulis."
McGuirl M.R.; Kinnaird K.M.; Savard C.; Bugbee E.H.,SE and SNL diagrams: Flexible data structures for MIR,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069872104&partnerID=40&md5=00ecac4542bfb81e37db3fedd38cb3d1,"McGuirl M.R., Division of Applied Mathematics, Brown University, United States; Kinnaird K.M., Division of Applied Mathematics, Brown University, United States; Savard C., Department of Mathematics, University of Michigan, United States; Bugbee E.H., Department of Biostatistics, Brown University, United States","The matrix-based representations commonly used in MIR tasks are often difficult to interpret. This work introduces start-end (SE) diagrams and start(normalized)length (SNL) diagrams, two novel structure-based representations for sequential music data. Inspired by methods from topological data analysis, both SE and SNL diagrams come equipped with efficiently computable and stable metrics. Utilizing SE or SNL diagrams as input, we address the cover song task for score-based data with high accuracy. While both representations are concisely defined and flexible, SNL diagrams in particular address issues introduced by commonly used resampling methods. © Melissa R. McGuirl, Katherine M. Kinnaird, Claire Savard, Erin H. Bugbee."
Hu X.; Li F.; Ng J.T.D.,On the relationships between music-induced emotion and physiological signals,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069877632&partnerID=40&md5=f95811834b5b3f1f500ddcfb97a0958e,"Hu X., University of Hong Kong, Shenzhen Institute of Research and Innovation, Hong Kong; Li F., Sichuan University, China; Ng J.T.D., University of Hong Kong, Hong Kong","Emotion-aware music information retrieval (MIR) has been difficult due to the subjectivity and temporality of emotion responses to music. Physiological signals are regarded as related to emotion and thus could potentially be exploited in emotion-aware music discovery. This study explored the possibility of using physiological signals to detect users’ emotion responses to music, with consideration of individual characteristics (personality, music preferences, etc.). A user experiment was conducted with 23 participants who searched for music in a novel MIR system. Users’ listening behaviors and self-reported emotion responses to a total of 628 music pieces were collected. During music listening, a series of peripheral physiological signals (e.g., heart rate, skin conductance) were recorded from participants unobtrusively using a research-grade wearable wristband. A set of features in the time- and frequency- domains were extracted from the physiological signals and analyzed using statistical and machine learning methods. Results reveal 1) significant differences in some physiological features between positive and negative arousal and mood categories, and 2) effective classification of emotion responses based on physiological signals for some individuals. The findings can contribute to further improvement of emotion-aware intelligent MIR systems exploiting physiological signals as an objective and personalized input. © Hu, X. Li, F. Ng, J."
Lattner S.; Grachten M.; Widmer G.,A predictive model for music based on learned interval representations,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069858678&partnerID=40&md5=87366d8dbf02b2082681f8bede336da3,"Lattner S., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Grachten M., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Widmer G., Institute of Computational Perception, JKU Linz, Austria","Connectionist sequence models (e.g., RNNs) applied to musical sequences suffer from two known problems: First, they have strictly “absolute pitch perception”. Therefore, they fail to generalize over musical concepts which are commonly perceived in terms of relative distances between pitches (e.g., melodies, scale types, modes, cadences, or chord types). Second, they fall short of capturing the concepts of repetition and musical form. In this paper we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network which learns and operates on interval representations of musical sequences. The relative pitch modeling increases generalization and reduces sparsity in the input data. Furthermore, it can learn sequences of copy-and-shift operations (i.e. chromatically transposed copies of musical fragments)—a promising capability for learning musical repetition structure. We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably. Furthermore, we show that the relative pitch processing of the RGAE naturally facilitates the learning and the generation of sequences of copy-and-shift operations, wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural network on this task. © Stefan Lattner, Maarten Grachten, Gerhard Widmer."
Panda R.; Malheiro R.; Paiva R.P.,Musical texture and expressivity features for music emotion recognition,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069857501&partnerID=40&md5=3a693b38be058f8246e6ac38f44ead08,"Panda R., CISUC – Centre for Informatics and Systems, University of Coimbra, Portugal; Malheiro R., CISUC – Centre for Informatics and Systems, University of Coimbra, Portugal; Paiva R.P., CISUC – Centre for Informatics and Systems, University of Coimbra, Portugal","We present a set of novel emotionally-relevant audio features to help improving the classification of emotions in audio music. First, a review of the state-of-the-art regarding emotion and music was conducted, to understand how the various music concepts may influence human emotions. Next, well known audio frameworks were analyzed, assessing how their extractors relate with the studied musical concepts. The intersection of this data showed an unbalanced representation of the eight musical concepts. Namely, most extractors are low-level and related with tone color, while musical form, musical texture and expressive techniques are lacking. Based on this, we developed a set of new algorithms to capture information related with musical texture and expressive techniques, the two most lacking concepts. To validate our work, a public dataset containing 900 30-second clips, annotated in terms of Russell’s emotion quadrants was created. The inclusion of our features improved the F1-score obtained using the best 100 features by 8.6% (to 76.0%), using support vector machines and 20 repetitions of 10-fold cross-validation. © Renato Panda, Ricardo Malheiro, Rui Pedro Paiva."
Schreiber H.; Müller M.,A single-step approach to musical tempo estimation using a convolutional neural network,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068964919&partnerID=40&md5=1cf2d643dda9761eb9b6f030d440498c,"Schreiber H., Tagtraum Industries Incorporated, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","We present a single-step musical tempo estimation system based solely on a convolutional neural network (CNN). Contrary to existing systems, which typically first identify onsets or beats and then derive a tempo, our system estimates the tempo directly from a conventional mel-spectrogram in a single step. This is achieved by framing tempo estimation as a multi-class classification problem using a network architecture that is inspired by conventional approaches. The system’s CNN has been trained with the union of three datasets covering a large variety of genres and tempi using problem-specific data augmentation techniques. Two of the three ground-truths are novel and will be released for research purposes. As input the system requires only 11.9 s of audio and is therefore suitable for local as well as global tempo estimation. When used as a global estimator, it performs as well as or better than other state-of-the-art algorithms. Especially the exact estimation of tempo without tempo octave confusion is significantly improved. As local estimator it can be used to identify and visualize tempo drift in musical performances. © Hendrik Schreiber, Meinard Müller."
Viraraghavan V.S.; Murthy H.A.; Aravind R.,Precision of sung notes in carnatic music,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862016&partnerID=40&md5=4221d577ca0ac260e2470459441c5837,"Viraraghavan V.S., TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India, Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Murthy H.A., Department of Computer Science and Engineering, Indian Institute of Technology, Madras, India; Aravind R., Department of Electrical Engineering, Indian Institute of Technology, Madras, India","Carnatic music is replete with continuous pitch movement called gamakas and can be viewed as consisting of constant-pitch notes (CPNs) and transients. The stationary points (STAs) of transients – points where the pitch curve changes direction – also carry melody information. In this paper, the precision of sung notes in Carnatic music is studied in detail by treating CPNs and STAs separately. There is variation among the nineteen musicians considered, but on average, the precision of CPNs increases exponentially with duration and settles at about 10 cents for CPNs longer than 0.5 seconds. For analyzing STAs, in contrast to Western music, rāga (melody) information is found to be necessary, and errors in STAs show a significantly larger standard deviation of about 60 cents. To corroborate these observations, the music was automatically transcribed and re-synthesized using CPN and STA information using two interpolation techniques. The results of perceptual tests clearly indicate that the grammar is highly flexible. We also show that the precision errors are not due to poor pitch tracking, singer deficiencies or delay in auditory feedback. © V S Viraraghavan, H A Murthy, R Aravind."
Eremenko V.; Demirel E.; Bozkurt B.; Serra X.,Audio-aligned jazz harmony dataset for automatic chord transcription and corpus-based research,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069883510&partnerID=40&md5=924b9f758a7c370c44cacea998a9d065,"Eremenko V., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Demirel E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bozkurt B., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we present a new dataset of time-aligned jazz harmony transcriptions. This dataset is a useful resource for content-based analysis, especially for training and evaluating chord transcription algorithms. Most of the available chord transcription datasets only contain annotations for rock and pop, and the characteristics of jazz, such as the extensive use of seventh chords, are not represented. Our dataset consists of annotations of 113 tracks selected from “The Smithsonian Collection of Classic Jazz” and “Jazz: The Smithsonian Anthology,” covering a range of performers, subgenres, and historical periods. Annotations were made by a jazz musician and contain information about the meter, structure, and chords for entire audio tracks. We also present evaluation results of this dataset using state-of-the-art chord estimation algorithms that support seventh chords. The dataset is valuable for jazz scholars interested in corpus-based research. To demonstrate this, we extract statistics for symbolic data and chroma features from the audio tracks. © Vsevolod Eremenko, Emir Demirel, Baris Bozkurt, Xavier Serra."
Vatolkin I.; Rudolph G.,Comparison of audio features for recognition of western and ethnic instruments in polyphonic mixtures,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069858324&partnerID=40&md5=135018cdc72030d379166fe4df5af471,"Vatolkin I., TU Dortmund, Department of Computer Science, Germany; Rudolph G., TU Dortmund, Department of Computer Science, Germany","Studies on instrument recognition are almost always restricted to either Western or ethnic music. Only little work has been done to compare both musical worlds. In this paper, we analyse the performance of various audio features for recognition of Western and ethnic instruments in chords. The feature selection is done with the help of a minimum redundancy - maximum relevance strategy and a multi-objective evolutionary algorithm. We compare the features found to be the best for individual categories and propose a novel strategy based on non-dominated sorting to evaluate and select trade-off features which may contribute as best as possible to the recognition of individual and all instruments. © Igor Vatolkin, Günter Rudolph."
Yan Y.; Lustig E.; VanderStel J.; Duan Z.,Part-invariant model for music generation and harmonization,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862810&partnerID=40&md5=e791d92f474abea76c8a215aebff7d99,"Yan Y., Electrical and Computer Engineering, University of Rochester, United States; Lustig E., Eastman School of Music, University of Rochester, United States; VanderStel J., Eastman School of Music, University of Rochester, United States; Duan Z., Electrical and Computer Engineering, University of Rochester, United States","Automatic music generation has been gaining more attention in recent years. Existing approaches, however, are mostly ad hoc to specific rhythmic structures or instrumentation layouts, and lack music-theoretic rigor in their evaluations. In this paper, we present a neural language (music) model that tries to model symbolic multi-part music. Our model is part-invariant, i.e., it can process/generate any part (voice) of a music score consisting of an arbitrary number of parts, using a single trained model. For better incorporating structural information of pitch spaces, we use a structured embedding matrix to encode multiple aspects of a pitch into a vector representation. The generation is performed by Gibbs Sampling. Meanwhile, our model directly generates note spellings to make outputs human-readable. We performed objective (grading) and subjective (listening) evaluations by recruiting music theorists to compare the outputs of our algorithm with those of music students on the task of bassline harmonization (a traditional pedagogical task). Our experiment shows that errors of our algorithm and students are differently distributed, and the range of ratings for generated pieces overlaps with students’ to varying extents for our three provided basslines. This experiment suggests some future research directions. © Yujia Yan, Ethan Lustig, Joseph VanderStel, Zhiyao Duan ."
Cumming J.E.; McKay C.; Stuchbery J.; Fujinaga I.,Methodologies for creating symbolic corpora of Western music before 1600,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069843974&partnerID=40&md5=502b02a4425e917f9fe768b6d5f6e4b5,"Cumming J.E., McGill University, Canada; McKay C., Marianopolis College, Canada; Stuchbery J., McGill University, Canada; Fujinaga I., McGill University, Canada","The creation of a corpus of compositions in symbolic formats is an essential step for any project in systematic research. There are, however, many potential pitfalls, especially in early music, where scores are edited in different ways: variables include clefs, note values, types of barline, and editorial accidentals. Different score editors and optical music recognition software have their own ways of storing and exporting musical data. Choice of software and file formats, and their various parameters, can thus unintentionally bias data, as can decisions on how to interpret potentially ambiguous markings in original sources. This becomes especially problematic when data from different corpora are combined for computational processing, since observed regularities and irregularities may in fact be linked with inconsistent corpus collection methodologies, internal and external, rather than the underlying music. This paper proposes guidelines, templates, and work-flows for the creation of consistent early music corpora, and for detecting encoding biases in existing corpora. We have assembled a corpus of Renaissance duos as a sample implementation, and present machine learning experiments demonstrating how inconsistent or naïve encoding methodologies for corpus collection can distort results. © Julie E. Cumming, Cory McKay, Jonathan Stuchbery, Ichiro Fujinaga."
Schreiber H.; Müller M.,A crowdsourced experiment for tempo estimation of electronic dance music,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069804549&partnerID=40&md5=1196739f405131e9d1f5681c0bdf674e,"Schreiber H., tagtraum industries incorporated, United States; Müller M., International Audio Laboratories Erlangen, Germany","Relative to other datasets, state-of-the-art tempo estimation algorithms perform poorly on the GiantSteps Tempo dataset for electronic dance music (EDM). In order to investigate why, we conducted a large-scale, crowdsourced experiment involving 266 participants from two distinct groups. The quality of the collected data was evaluated with regard to the participants’ input devices and background. In the data itself we observed significant tempo ambiguities, which we attribute to annotator subjectivity and tempo instability. As a further contribution, we then constructed new annotations consisting of tempo distributions for each track. Using these annotations, we reevaluated two recent state-of-the-art tempo estimation systems achieving significantly improved results. The main conclusions of this investigation are that current tempo estimation systems perform better than previously thought and that evaluation quality needs to be improved. The new crowdsourced annotations will be released for evaluation purposes. © Hendrik Schreiber, Meinard Müller."
Dorfer M.; Henkel F.; Widmer G.,"Learning to listen, read, and follow: Score following as a reinforcement learning game",2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069801375&partnerID=40&md5=5d87093dfa8032d617301902a4f72f74,"Dorfer M., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Henkel F., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Score following is the process of tracking a musical performance (audio) with respect to a known symbolic representation (a score). We start this paper by formulating score following as a multimodal Markov Decision Process, the mathematical foundation for sequential decision making. Given this formal definition, we address the score following task with state-of-the-art deep reinforcement learning (RL) algorithms such as synchronous advantage actor critic (A2C). In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. All this behavior is learned entirely from scratch, based on a weak and potentially delayed reward signal that indicates to the agent how close it is to the correct position in the score. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images. © Matthias Dorfer, Florian Henkel, Gerhard Widmer."
Tsukuda K.; Fukayama S.; Goto M.,Listener anonymizer: Camouflaging play logs to preserve user’s demographic anonymity,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069851815&partnerID=40&md5=6b4d313e7a549b059b2d3b53dc9dc324,"Tsukuda K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","When a user signs up with an online music service, she is often requested to register her demographic attributes such as age, gender, and nationality. Even if she does not input such information, it has been reported that user attributes can be predicted with high accuracy by using her play log. How can users enjoy music when using an online music service while preserving their demographic anonymity? To solve this problem, we propose a system called Listener Anonymizer. Listener Anonymizer monitors the user’s play log. When it detects that her confidential attributes can be predicted, it selects songs that can decrease the prediction accuracy and recommends them to her. The user can camouflage her play logs by playing these songs to preserve her demographic anonymity. Since such songs do not always match her music taste, selecting as few songs as possible that can effectively anonymize her attributes is required. Listener Anonymizer realizes this by selecting songs based on feature ablation analysis. Our experimental results using Last.fm play logs showed that Listener Anonymizer was able to preserve anonymity with fewer songs than a method that randomly selected songs. © Kosetsu Tsukuda, Satoru Fukayama, Masataka Goto."
Spinelli L.; Lau J.; Pritchard L.; Lee J.H.,Influences on the social practices surrounding commercial music services: A model for rich interactions,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069865386&partnerID=40&md5=0473760382aadd9b84bfd8459e42c3ef,"Spinelli L., Information School, University of Washington, Seattle, United States; Lau J., Information School, University of Washington, Seattle, United States; Pritchard L., Information School, University of Washington, Seattle, United States; Lee J.H., Information School, University of Washington, Seattle, United States","Music can play an important role in social experiences and interactions. Technologies in-use affect these experiences and interactions and as they continue to evolve, social behaviors and norms surrounding them also evolve. In this paper, we explore the social aspects of commercial music services through focus group observation and interview data. We seek to better understand how existing services are used for social music practices and can be improved. We identified 9 social practices and 24 influences surrounding commercial music services. Based on the user data, we created a model of these practices and influences that provides a lens through which social experiences surrounding commercial music services can be understood. An understanding of these social practices within their contextual ecosystem help inform what influences should be considered when designing new technologies. Our findings include the identification of: the underlying relationships between practices and their influences; practices and influences that inform the weight of relationships in social networks; social norms to be considered when designing social features; influences that add additional insight to previously observed behaviors; and a detailed explanation of how music selection and listening practices can be supported by commercial music services.[1] L. Barrington, R. Oda, and G.R.G. Lanckriet. Smarter than Genius? Human Evaluation of Music Recom-mender Systems. In Proc. ISMIR, pages 357–362, 2009. © Louis Spinelli, Josephine Lau, Liz Pritchard, Jin Ha Lee."
Krasanakis E.; Schinas E.; Papadopoulos S.; Kompatsiaris Y.; Mitkas P.,VenueRank: Identifying venues that contribute to artist popularity,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069864049&partnerID=40&md5=01069617b622f38ab74591deab6b6c4e,"Krasanakis E., CERTH-ITI, Thessaloniki, Greece; Schinas E., CERTH-ITI, Thessaloniki, Greece, AUTH, Thessaloniki, Greece; Papadopoulos S., CERTH-ITI, Thessaloniki, Greece; Kompatsiaris Y., CERTH-ITI, Thessaloniki, Greece; Mitkas P., AUTH, Thessaloniki, Greece","An important problem in the live music industry is finding venues that help expose artists to wider audiences. However, it is often difficult to obtain live music audience data to tackle this task. In this work, we investigate whether important venues can instead be inferred through social media data. Our approach consists of employing bipartite graph ranking algorithms to help discover important venues in artist-venue graphs mined from Facebook. We use both well-established algorithms, such as BiRank, and a modification of their common iterative scheme that avoids the impact of possibly erroneous heuristics to the ranking, which we call VenueRank. Resulting venue ranks are compared to those obtained from feature extraction for predicting the most listened artists and large listener increments in Spotify. This comparison yields high correlation between venue importance for listener prediction and bipartite graph ranking algorithms, with VenueRank found more robust against overfitting. © Emmanouil Krasanakis, Emmanouil Schinas, Symeon Papadopoulos, Yiannis Kompatsiaris, Pericles Mitkas."
Arzt A.; Lattner S.,Audio-to-score alignment using transposition-invariant features,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069809162&partnerID=40&md5=8de70ca97538629875f9d098090c1d85,"Arzt A., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Lattner S., Institute of Computational Perception, Johannes Kepler University, Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France","Audio-to-score alignment is an important pre-processing step for in-depth analysis of classical music. In this paper, we apply novel transposition-invariant audio features to this task. These low-dimensional features represent local pitch intervals and are learned in an unsupervised fashion by a gated autoencoder. Our results show that the proposed features are indeed fully transposition-invariant and enable accurate alignments between transposed scores and performances. Furthermore, they can even outperform widely used features for audio-to-score alignment on ‘untransposed data’, and thus are a viable and more flexible alternative to well-established features for music alignment and matching. © Andreas Arzt, Stefan Lattner."
Gebhardt R.B.; Lykartsis A.; Stein M.,A confidence measure for key labelling,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069883464&partnerID=40&md5=c2320a2c29b20bb3eaa86af824ae8b34,"Gebhardt R.B., Audio Communication Group, TU Berlin, Germany; Lykartsis A., Audio Communication Group, TU Berlin, Germany; Stein M., Native Instruments GmbH, Germany","We present a new measure for automatically estimating the confidence of musical key classification. Our approach leverages the degree of harmonic information held within a musical audio signal (its “keyness”) as well as the steadiness of local key detections across the its duration (its “stability”). Using this confidence measure, musical tracks which are likely to be misclassified, i.e. those with low confidence, can then be handled differently from those analysed by standard, fully automatic key detection methods. By means of a listening test, we demonstrate that our developed features significantly correlate with listeners’ ratings of harmonic complexity, steadiness and the uniqueness of key. Furthermore, we demonstrate that tracks which are incorrectly labelled using an existing key detection system obtain low confidence values. Finally, we introduce a new method called “root note heuristics” for the special treatment of tracks with low confidence. We show that by applying these root note heuristics, key detection results can be improved for minimalistic music. © Roman B. Gebhardt, Athanasios Lykartsis, Michael Stein."
Carsault T.; Nika J.; Esling P.,Using musical relationships between chord labels in automatic chord extraction tasks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850608&partnerID=40&md5=d6331c0115d183f60b8d14e175f203eb,"Carsault T., Ircam, CNRS, Sorbonne Université, UMR 9912 STMS, France; Nika J., Ircam, CNRS, Sorbonne Université, UMR 9912 STMS, France, L3i Lab, University of La Rochelle, France; Esling P., Ircam, CNRS, Sorbonne Université, UMR 9912 STMS, France","Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors. © Tristan Carsault, Jérôme Nika, Philippe Esling."
Kelkar T.; Roy U.; Jensenius A.R.,Evaluating a collection of sound-tracing data of melodic phrases,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065563922&partnerID=40&md5=f1f1ab594e3192e859ac7865ac6047b5,"Kelkar T., RITMO, Dept. of Musicology, University of Oslo, Norway; Roy U., RITMO, Dept. of Musicology, University of Oslo, Norway; Jensenius A.R., RITMO, Dept. of Musicology, University of Oslo, Norway","Melodic contour, the ‘shape’ of a melody, is a common way to visualize and remember a musical piece. The purpose of this paper is to explore the building blocks of a future ‘gesture-based’ melody retrieval system. We present a dataset containing 16 melodic phrases from four musical styles and with a large range of contour variability. This is accompanied by full-body motion capture data of 26 participants performing sound-tracing to the melodies. The dataset is analyzed using canonical correlation analysis (CCA), and its neural network variant (Deep CCA), to understand how melodic contours and sound tracings relate to each other. The analyses reveal non-linear relationships between sound and motion. The link between pitch and verticality does not appear strong enough for complex melodies. We also find that descending melodic contours have the least correlation with tracings. © Tejaswinee Kelkar, Udit Roy, Alexander Refsum Jensenius."
Gómez J.S.; Abeßer J.; Cano E.,"Jazz solo instrument classification with convolutional neural networks, source separation, and transfer learning",2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069006100&partnerID=40&md5=b04928c84667dc2cb4b790b39914554c,"Gómez J.S., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Cano E., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany","Predominant instrument recognition in ensemble recordings remains a challenging task, particularly if closely-related instruments such as alto and tenor saxophone need to be distinguished. In this paper, we build upon a recently-proposed instrument recognition algorithm based on a hybrid deep neural network: a combination of convolutional and fully connected layers for learning characteristic spectral-temporal patterns. We systematically evaluate harmonic/percussive and solo/accompaniment source separation algorithms as pre-processing steps to reduce the overlap among multiple instruments prior to the instrument recognition step. For the particular use-case of solo instrument recognition in jazz ensemble recordings, we further apply transfer learning techniques to fine-tune a previously trained instrument recognition model for classifying six jazz solo instruments. Our results indicate that both source separation as pre-processing step as well as transfer learning clearly improve recognition performance, especially for smaller subsets of highly similar instruments. © Juan S. Gómez, Jakob Abeßer, Estefanía Cano."
Xi Q.; Bittner R.M.; Pauwels J.; Ye X.; Bello J.P.,Guitarset: A dataset for guitar transcription,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069828888&partnerID=40&md5=9fad38df11b933c11790ebd05a3cb366,"Xi Q., Music and Audio Research Lab, New York University, United States; Bittner R.M., Music and Audio Research Lab, New York University, United States; Pauwels J., Center for Digital Music, Queen Mary University of London, United Kingdom; Ye X., Music and Audio Research Lab, New York University, United States; Bello J.P., Music and Audio Research Lab, New York University, United States","The guitar is a popular instrument for a variety of reasons, including its ability to produce polyphonic sound and its musical versatility. The resulting variability of sounds, however, poses significant challenges to automated methods for analyzing guitar recordings. As data driven methods become increasingly popular for difficult problems like guitar transcription, sets of labeled audio data are highly valuable resources. In this paper we present GuitarSet, a dataset that provides high quality guitar recordings alongside rich annotations and metadata. In particular, by recording guitars using a hexaphonic pickup, we are able to not only provide recordings of the individual strings but also to largely automate the expensive annotation process. The dataset contains recordings of a variety of musical excerpts played on an acoustic guitar, along with time-aligned annotations of string and fret positions, chords, beats, downbeats, and playing style. We conclude with an analysis of new challenges presented by this data, and see that it is interesting for a wide variety of tasks in addition to guitar transcription, including performance analysis, beat/downbeat tracking, and chord estimation. © Qingyang Xi, Rachel Bittner, Johan Pauwels, Xuzhou Ye, Juan Bello."
Figueiredo F.; Andrade N.,Quantifying disruptive influence in the allmusic guide,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093667&partnerID=40&md5=9c9a17c137cac2256fc62ec76214c1ad,"Figueiredo F., Universidade Federal de Minas Gerais, Brazil; Andrade N., Universidade Federal de Campina Grande, Brazil","Understanding how influences shape musical creation provides rich insight into cultural trends. As such, there have been several efforts to create quantitative complex network methods that support the analysis of influence networks among artists in a music corpus. We contribute to this body of work by examining how disruption happens in a corpus about music influence from the All Music Guide. A disruptive artist is one that creates a new stream of influences; this artist builds on prior efforts but influences subsequent artists that do not build on the same prior efforts. We leverage methods devised to study disruption in Science and Technology and apply them to the context of music creation. Our results point that such methods identify innovative artists and that disruption is mostly uncorrelated with network centrality. © 2020 International Society for Music Information Retrieval. All rights reserved."
Bauer C.; Schedl M.,Investigating cross-country relationship between users’ social ties and music mainstreaminess,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069877333&partnerID=40&md5=34cb4ff88a61298e0fb0b56f548bf59c,"Bauer C., Johannes Kepler University Linz, Austria; Schedl M., Johannes Kepler University Linz, Austria","We investigate the complex relationship between the factors (i) preference for music mainstream, (ii) social ties in an online music platform, and (iii) demographics. We define (i) on a global and a country level, (ii) by several network centrality measures such as Jaccard index among users’ connections, closeness centrality, and betweenness centrality, and (iii) by country and age information. Using the LFM-1b dataset of listening events of Last.fm users, we are able to uncover country-dependent differences in consumption of mainstream music as well as in user behavior with respect to social ties and users’ centrality. We could identify that users inclined to mainstream music tend to have stronger connections than the group of less mainstreamy users. Furthermore, our analysis revealed that users typically have less connections within a country than cross-country ones, with the first being stronger social ties, though. Results will help building better user models of listeners and in turn improve personalized music retrieval and recommendation algorithms. © Christine Bauer, Markus Schedl."
Cheng T.; Fukayama S.; Goto M.,Comparing RNN parameters for melodic similarity,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069829696&partnerID=40&md5=774ee016a53f8a2b8542459dd8b1fe56,"Cheng T., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Melodic similarity is an important task in the Music Information Retrieval (MIR) domain, with promising applications including query by example, music recommendation and visualisation. Most current approaches compute the similarity between two melodic sequences by comparing their local features (distance between pitches, intervals, etc.) or by comparing the sequences after aligning them. In order to find a better feature representing global characteristics of a melody, we propose to represent the melodic sequence of each musical piece by the parameters of a generative Recurrent Neural Network (RNN) trained on its sequence. Because the trained RNN can generate the identical melodic sequence of each piece, we can expect that the RNN parameters contain the temporal information within the melody. In our experiment, we first train an RNN on all melodic sequences, and then use it as an initialisation to train an individual RNN on each melodic sequence. The similarity between two melodies is computed by using the distance between their individual RNN parameters. Experimental results showed that the proposed RNN-based similarity outperformed the baseline similarity obtained by directly comparing melodic sequences. © Tian Cheng, Satoru Fukayama, Masataka Goto."
Jeong D.; Kwon T.; Nam J.,A timbre-based approach to estimate key velocity from polyphonic piano recordings,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069873811&partnerID=40&md5=95e5b8d30b3b48a2b48322287728431c,"Jeong D., Graduate School of Culture Technology, KAIST, South Korea; Kwon T., Graduate School of Culture Technology, KAIST, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Estimating the key velocity of each note from polyphonic piano music is a highly challenging task. Previous work addressed the problem by estimating note intensity using a polyphonic note model. However, they are limited because the note intensity is vulnerable to various factors in a recording environment. In this paper, we propose a novel method to estimate the key velocity focusing on timbre change which is another cue associated with the key velocity. To this end, we separate individual notes of polyphonic piano music using non-negative matrix factorization (NMF) and feed them into a neural network that is trained to discriminate the timbre change according to the key velocity. Combining the note intensity from the separated notes with the statistics of the neural network prediction, the proposed method estimates the key velocity in the dimension of MIDI note velocity. The evaluation on Saarland Music Data and the MAPS dataset shows promising results in terms of robustness to changes in the recording environment. © Dasaem Jeong, Taegyun Kwon, Juhan Nam."
Tralie C.J.,Cover song synthesis by analogy,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837883&partnerID=40&md5=dbf6b7af7297b0a69e134fd25a0b997f,"Tralie C.J., Duke University, Department of Mathematics, United States","In this work, we pose and address the following “cover song analogies” problem: given a song A by artist 1 and a cover song A’ of this song by artist 2, and given a different song B by artist 1, synthesize a song B’ which is a cover of B in the style of artist 2. Normally, such a polyphonic style transfer problem would be quite challenging, but we show how the cover songs example constrains the problem, making it easier to solve. First, we extract the longest common beat-synchronous subsequence between A and A’, and we time stretch the corresponding beat intervals in A’ so that they align with A. We then derive a version of joint 2D convolutional NMF, which we apply to the constant-Q spectrograms of the synchronized segments to learn a translation dictionary of sound templates from A to A’. Finally, we apply the learned templates as filters to the song B, and we mash up the translated filtered components into the synthesized song B’ using audio mosaicing. We showcase our algorithm on several examples, including a synthesized cover version of Michael Jackson’s “Bad” by Alien Ant Farm, learned from the latter’s “Smooth Criminal” cover. © Christopher J. Tralie."
de Valk R.; Weyde T.,Deep neural networks with voice entry estimation heuristics for voice separation in symbolic music representations,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842543&partnerID=40&md5=3ba72db0c8766844cc88d12fb0eb10f3,"de Valk R., Jukedeck Ltd, United Kingdom; Weyde T., Department of Computer Science City, University of London, United Kingdom","In this study we explore the use of deep feedforward neural networks for voice separation in symbolic music representations. We experiment with different network architectures, varying the number and size of the hidden layers, and with dropout. We integrate two voice entry estimation heuristics that estimate the entry points of the individual voices in the polyphonic fabric into the models. These heuristics serve to reduce error propagation at the beginning of a piece, which, as we have shown in previous work, can seriously hamper model performance. The models are evaluated on the 48 fugues from Johann Sebastian Bach’s The Well-Tempered Clavier and his 30 inventions—a dataset that we curated and make publicly available. We find that a model with two hidden layers yields the best results. Using more layers does not lead to a significant performance improvement. Furthermore, we find that our voice entry estimation heuristics are highly effective in the reduction of error propagation, improving performance significantly. Our best-performing model outperforms our previous models, where the difference is significant, and, depending on the evaluation metric, performs close to or better than the reported state of the art. © Reinier de Valk, Tillman Weyde."
Lu W.-T.; Su L.,Transferring the style of homophonic music using recurrent neural networks and autoregressive models,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069003444&partnerID=40&md5=095223d6ba07d585b36e60460c82e486,"Lu W.-T., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","Utilizing deep learning techniques to generate musical contents has caught wide attention in recent years. Within this context, this paper investigates a specific problem related to music generation, music style transfer. This practical problem aims to alter the style of a given music piece from one to another while preserving the essence of that piece, such as melody and chord progression. In particular, we discuss the style transfer of homophonic music, composed of a predominant melody part and an accompaniment part, where the latter is modified through Gibbs sampling on a generative model combining recurrent neural networks and autoregressive models. Both objective and subjective test experiment are performed to assess the performance of transferring the style of an arbitrary music piece having a homophonic texture into two different distinct styles, Bachs chorales and Jazz. © Wei-Tsung Lu and Li Su."
Bigoni F.; Dahl S.,Timbre discrimination for brief instrument sounds,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069856128&partnerID=40&md5=9db954ad57a674b76ee69ae6c8a1c002,"Bigoni F., Sound and Music Computing Aalborg University, Copenhagen, Denmark; Dahl S., Dept. of Architecture, Design and Media Technology, Aalborg University, Copenhagen, Denmark","Timbre discrimination, even for very brief sounds, allows identification and separation of different sound sources. The existing literature on the effect of duration on timbre recognition shows high performance for remarkably short time window lengths, but does not address the possible effect of musical training. In this study, we applied an adaptive procedure to investigate the effect of musical training on individual thresholds for instrument identification. A timbre discrimination task consisting of a 4-alternative forced choice (4AFC) of brief instrument sounds with varying duration was assigned to 16 test subjects using an adaptive staircase method. The effect of musical training has been investigated by dividing the participants into two groups: musicians and non-musicians. The experiment showed lowest thresholds for the guitar sound and highest for the violin sound, with a high overall performance level, but no significant difference between the two groups. It is suggested that the test subjects adjust the weightings of the perceptual dimensions of timbre according to different degrees of acoustic degradation of the stimuli, which are evaluated both by plotting extracted audio features in a feature space and by considering the timbral specificities of the four instruments. © Francesco Bigoni, Sofia Dahl."
Gouvert O.; Oberlin T.; Févotte C.,Matrix co-factorization for cold-start recommendation,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069849853&partnerID=40&md5=3ca127bbab4189deff4ab91ec238f618,"Gouvert O., IRIT, Université de Toulouse, CNRS, France; Oberlin T., IRIT, Université de Toulouse, CNRS, France; Févotte C., IRIT, Université de Toulouse, CNRS, France","Song recommendation from listening counts is now a classical problem, addressed by different kinds of collaborative filtering (CF) techniques. Among them, Poisson matrix factorization (PMF) has raised a lot of interest, since it seems well-suited to the implicit data provided by listening counts. Additionally, it has proven to achieve state-of-the-art performance while being scalable to big data. Yet, CF suffers from a critical issue, usually called cold-start problem: the system cannot recommend new songs, i.e., songs which have never been listened to. To alleviate this, one should complement the listening counts with another modality. This paper proposes a multi-modal extension of PMF applied to listening counts and tag labels extracted from the Million Song Dataset. In our model, every song is represented by the same activation pattern in each modality but with possibly different scales. As such, the method is not prone to the cold-start problem, i.e., it can learn from a single modality when the other one is not informative. Our model is symmetric (it equally uses both modalities) and we evaluate it on two tasks: new songs recommendation and tag labeling. © Olivier Gouvert, Thomas Oberlin, Cédric Févotte."
Hennequin R.; Royo-Letelier J.; Moussallam M.,Audio based disambiguation of music genre tags,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069804969&partnerID=40&md5=f25c465b60612921495749536dc496f1,"Hennequin R., Deezer R and D, Paris, France; Royo-Letelier J., Deezer R and D, Paris, France; Moussallam M., Deezer R and D, Paris, France","In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres. We show that such embeddings can be used for disambiguating genre tags (identification of different labels for the same genre, tag translation from a tag system to another, inference of hierarchical taxonomies on these genre tags). These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system. We show empirically that they makes it possible to retrieve the original taxonomy of a tag system, spot duplicates tags and translate tags from a tag system to another. © Romain Hennequin, Jimena Royo-Letelier, Manuel Moussallam."
Bigo L.; Feisthauer L.; Giraud M.; Levé F.,Relevance of musical features for cadence detection,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069812504&partnerID=40&md5=e6dbeb86bc9e7add5148cc9f30f5fdd7,"Bigo L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Feisthauer L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Giraud M., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Levé F., CRIStAL, UMR 9189, CNRS, Université de Lille, France, MIS, Université de Picardie Jules Verne, Amiens, France","Cadences, as breaths in music, are felt by the listener or studied by the theorist by combining harmony, melody, texture and possibly other musical aspects. We formalize and discuss the significance of 44 cadential features, correlated with the occurrence of cadences in scores. These features describe properties at the arrival beat of a cadence and its surroundings, but also at other onsets heuristically identified to pinpoint chords preparing the cadence. The representation of each beat of the score as a vector of cadential features makes it possible to reformulate cadence detection as a classification task. An SVM classifier was run on two corpora from Bach and Haydn totaling 162 perfect authentic cadences and 70 half cadences. In these corpora, the classifier correctly identified more than 75% of perfect authentic cadences and 50% of half cadences, with low false positive rates. The experiment results are consistent with common knowledge that classification is more complex for half cadences than for authentic cadences. © Louis Bigo, Laurent Feisthauer, Mathieu Giraud, Florence Levé."
Tsushima H.; Nakamura E.; Itoyama K.; Yoshii K.,Interactive arrangement of chords and melodies based on a tree-structured generative model,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069828371&partnerID=40&md5=e0a4ffb1a7858976601d1b760c6ecf62,"Tsushima H., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","We describe an interactive music composition system that assists a user in refining chords and melodies by generating chords for melodies (harmonization) and vice versa (melodization). Since these two tasks have been dealt with independently, it is difficult to jointly estimate chords and melodies that are optimal in both tasks. Another problem is developing an interactive GUI that enables a user to partially update chords and melodies by considering the latent tree structure of music. To solve these problems, we propose a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chord symbols, (2) a metrical Markov model for chord boundaries, (3) a Markov model for melody pitches, and (4) a metrical Markov model for melody onsets. The harmonic functions (syntactic roles) and repetitive structure of chords are learned by the PCFG. Any variables specified by a user can be optimized or sampled in a principled manner according to a unified posterior distribution. For improved melodization, a long short-term memory (LSTM) network can also be used. The subjective experimental result showed the effectiveness of the proposed system. © Hiroaki Tsushima, Katsutoshi Itoyama, Eita Nakamura, Kazuyoshi Yoshii."
McLeod A.; Steedman M.,Evaluating automatic polyphonic music transcription,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069839804&partnerID=40&md5=188c425fbacd254dbde234072e4d9662,"McLeod A., University of Edinburgh, United Kingdom; Steedman M., University of Edinburgh, United Kingdom","Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a time-frequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce MV 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation—for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H. © Andrew McLeod, Mark Steedman."
Ren I.Y.; Volk A.; Swierstra W.; Veltkamp R.C.,Analysis by classification: A comparative study of annotated and algorithmically extracted patterns in symbolic music data,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069814533&partnerID=40&md5=72eb6d250e4c94141ba1a895c1b8e3eb,"Ren I.Y., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands; Swierstra W., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands","Musical patterns are salient passages that repeatedly appear in music. Such passages are vital for compression, classification and prediction tasks in MIR, and algorithms employing different techniques have been proposed to find musical patterns automatically. Human-annotated patterns have been collected and used to evaluate pattern discovery algorithms, e.g., in the Discovery of Repeated Themes & Sections MIREX task. However, state-of-the-art algorithms are not yet able to reproduce human-annotated patterns. To understand what gives rise to the discrepancy between algorithmically extracted patterns and human-annotated patterns, we use jSymbolic2 to extract features from patterns, visualise the feature space using PCA and perform a comparative analysis using classification techniques. We show that it is possible to classify algorithmically extracted patterns, human-annotated patterns and randomly sampled passages. This implies: (a) Algorithmically extracted patterns possess different properties than human-annotated patterns (b) Algorithmically extracted patterns have different structures than randomly sampled passages (c) Human-annotated patterns contain more information than randomly sampled passages despite subjectivity involved in the annotation process. We further discover that rhythmic features are of high importance in the classification process, which should influence future research on automatic pattern discovery. © Iris Yuping Ren, Anja Volk, Wouter Swierstra, Remco C. Veltkamp."
Park J.; Lee J.; Park J.; Ha J.-W.; Nam J.,Representation learning of music using artist labels,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069814372&partnerID=40&md5=08c39aadfe4c07c867c14c3d3be2d141,"Park J., NAVER Corp, South Korea; Lee J., Graduate School of Culture Technology, KAIST, South Korea; Park J., NAVER Corp, South Korea; Ha J.-W., NAVER Corp, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models. © Jiyoung Park, Jongpil Lee, Jangyeon Park, Jung-Woo Ha, Juhan Nam."
de Padua R.; Oliveira de Carvalho V.; de Oliveira Rezende S.; Silva D.F.,Exploring musical relations using association rule networks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837234&partnerID=40&md5=8ad3c36d1db5075224680a5814ae22ff,"de Padua R., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil, Data Science Team, Itaú-Unibanco, São Paulo, Brazil; Oliveira de Carvalho V., Instituto de Geociências e Ciências Exatas, Universidade Estadual Paulista, Rio Claro, Brazil; de Oliveira Rezende S., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil; Silva D.F., Departamento de Computação, Universidade Federal de São Carlos, São Carlos, Brazil","Music information retrieval (MIR) has been gaining increasing attention in both industry and academia. While many algorithms for MIR rely on assessing feature subsequences, the user normally has no resources to interpret the significance of these patterns. Interpreting the relations between these temporal patterns and some aspects of the assessed songs can help understanding not only some algorithms’ outcomes but the kind of patterns which better defines a set of similarly labeled recordings. In this work, we present a novel method to assess these relations, constructing an association rule network from temporal patterns obtained by a simple quantization process. With an empirical evaluation, we illustrate how we can use our method to explore these relations in a varied set of data and labels. © Renan de Padua, Vernica Oliveira de Carvalho, Solange Rezende, Diego Furtado Silva."
Bogdanov D.; Porter A.; Schreiber H.; Urbano J.; Oramas S.,"The acousticbrainz genre dataset: Multi-source, multi-level, multi-label, and large-scale",2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076102558&partnerID=40&md5=4692a4371583b34fadafa4813882feeb,"Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Spain; Porter A., Music Technology Group, Universitat Pompeu Fabra, Spain; Schreiber H., Tagtraum Industries Incorporated, United States; Urbano J., Multimedia Computing Group, Delft University of Technology, Netherlands; Oramas S., Pandora, United States","This paper introduces the AcousticBrainz Genre Dataset, a large-scale collection of hierarchical multi-label genre annotations from different metadata sources. It allows researchers to explore how the same music pieces are annotated differently by different communities following their own genre taxonomies, and how this could be addressed by genre recognition systems. Genre labels for the dataset are sourced from both expert annotations and crowds, permitting comparisons between strict hierarchies and folksonomies. Music features are available via the Acoustic- Brainz database. To guide research, we suggest a concrete research task and provide a baseline as well as an evaluation method. This task may serve as an example of the development and validation of automatic annotation algorithms on complementary datasets with different taxonomies and coverage. With this dataset, we hope to contribute to developments in content-based music genre recognition as well as cross-disciplinary studies on genre metadata analysis. © 2020 International Society for Music Information Retrieval. All rights reserved."
Subramanian V.; Lerch A.,Concert stitch: Organization and synchronization of crowd-sourced recordings,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069881019&partnerID=40&md5=b0a1c42de5334ea5e30f8c4d285e00fe,"Subramanian V., Center for Music Technology, Georgia Institute of Technology, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","The number of audience recordings of concerts on the internet has exploded with the advent of smartphones. This paper proposes a method to organize and align these recordings in order to create one or more complete renderings of the concert. The process comprises two steps: first, using audio fingerprints to represent the recordings, identify overlapping segments, and compute an approximate alignment using a modified Dynamic Time Warping (DTW) algorithm and second, applying a cross-correlation around the approximate alignment points in order to improve the accuracy of the alignment. The proposed method is compared to two baseline systems using approaches previously proposed for similar tasks. One baseline cross-correlates the audio fingerprints directly without DTW. The second baseline replaces the audio fingerprints with pitch chroma in the DTW algorithm. A new dataset annotating real-world data obtained from the Live Music Archive is presented and used for evaluation of the three systems. © Vinod Subramanian, Alexander Lerch."
Bemman B.; Meredith D.,Backtracking search heuristics for solving the all-partition array problem,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093761&partnerID=40&md5=746e2893bc792aa57247ca4783d2425f,"Bemman B., Aalborg University, Aalborg, Denmark; Meredith D., Aalborg University, Aalborg, Denmark","Recent efforts to model the compositional processes of Milton Babbitt have yielded a number of computationally challenging problems. One of these problems, known as the all-partition array problem, is a particularly hard variant of set covering, and several different approaches, including mathematical optimization, constraint satisfaction, and greedy backtracking, have been proposed for solving it. Of these previous approaches, only constraint programming has led to a successful solution. Unfortunately, this solution is expensive in terms of computation time. We present here two new search heuristics and a modification to a previously proposed heuristic, that, when applied to a greedy backtracking algorithm, allow the all-partition array problem to be solved in a practical running time. We demonstrate the success of our heuristics by solving for three different instances of the problem found in Babbitt's music, including one previously solved with constraint programming and one Babbitt himself was unable to solve. Use of the new heuristics allows each instance of the problem to be solved more quickly than was possible with previous approaches. © 2020 International Society for Music Information Retrieval. All rights reserved."
Pati A.; Lerch A.; Hadjeres G.,Learning to traverse latent spaces for musical score inpainting,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083466364&partnerID=40&md5=9a56b4e2cbd0393af640f73c64a74766,"Pati A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Hadjeres G., Sony CSL, Paris, France","Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models. © 2020 International Society for Music Information Retrieval. All rights reserved."
Huang C.-Z.A.; Hawthorne C.; Roberts A.; Dinculescu M.; Wexler J.; Hong L.; Howcroft J.,The bach doodle: Approachable music composition with machine learning at scale,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082396519&partnerID=40&md5=da8675f209460ffafe1e7a602281bb9d,"Huang C.-Z.A., Google Brain Magenta; Hawthorne C., Google Brain Magenta; Roberts A., Google Brain Magenta; Dinculescu M., Google Brain Magenta; Wexler J., Google Brain Pair; Hong L., Google Doodle; Howcroft J., Google Doodle","To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle [1], where users can create their own melody and have it harmonized by a machine learning model (Coconet [22]) in the style of Bach. For users to input melodies, we designed a simplified sheet-music based interface. To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js [32] to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depthwise separable convolutions and fusing operations. We also reduced the model download size to approximately 400KB through post-training weight quantization. We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models. © 2020 International Society for Music Information Retrieval. All rights reserved."
Sarfati M.; Hu A.; Donier J.,Community-based cover song detection,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079218664&partnerID=40&md5=4720e9c6188b6ccb5011257eafff5a85,"Sarfati M., Spotify, Sweden; Hu A., Spotify, Sweden; Donier J., Spotify, Sweden","Audio-based cover song detection has received much attention in the MIR community in the recent years. To date, the most popular formulation of the problem has been to compare the audio signals of two tracks and to make a binary decision based on this information only. However, leveraging additional signals might be key if one wants to solve the problem at an industrial scale. In this paper, we introduce an ensemble-based method that approaches the problem from a many-to-many perspective. Instead of considering pairs of tracks in isolation, we consider larger sets of potential versions for a given composition, and create and exploit the graph of relationships between these tracks. We show that this can result in a significant improvement in performance, in particular when the number of existing versions of a given composition is large. © 2020 International Society for Music Information Retrieval. All rights reserved."
Korzeniowski F.; Widmer G.,Genre-agnostic key classification with convolutional neural networks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842568&partnerID=40&md5=df04e8bb8bb4df69d2cf0bb23744b342,"Korzeniowski F., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","We propose modifications to the model structure and training procedure to a recently introduced Convolutional Neural Network for musical key classification. These modifications enable the network to learn a genre-independent model that performs better than models trained for specific music styles, which has not been the case in existing work. We analyse this generalisation capability on three datasets comprising distinct genres. We then evaluate the model on a number of unseen data sets, and show its superior performance compared to the state of the art. Finally, we investigate the model’s performance on short excerpts of audio. From these experiments, we conclude that models need to consider the harmonic coherence of the whole piece when classifying the local key of short segments of audio. © Filip Korzeniowski and Gerhard Widmer."
Wu C.-W.; Lerch A.,From labeled to unlabeled data – On the data challenge in automatic drum transcription,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842320&partnerID=40&md5=7cb64d5dc368ef8901a69bc40a6bb995,"Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","Automatic Drum Transcription (ADT), like many other music information retrieval tasks, has made progress in the past years through the integration of machine learning and audio signal processing techniques. However, with the increasing popularity of data-hungry approaches such as deep learning, the insufficient amount of data becomes more and more a challenge that concerns the generality of the resulting models and the validity of the evaluation. To address this challenge in ADT, this paper first examines the existing labeled datasets and how representative they are of the research problem. Next, possibilities of using unlabeled data to improve general ADT systems are explored. Specifically, two paradigms that harness information from unlabeled data, namely feature learning and student-teacher learning, are applied to two major types of ADT systems. All systems are evaluated on four different drum datasets. The results highlight the necessity of more and larger annotated datasets and indicate the feasibility of exploiting unlabeled data for improving ADT systems. © Chih-Wei Wu, Alexander Lerch."
Kinnaird K.M.,Aligned sub-hierarchies: A structure-based approach to the cover song task,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069856585&partnerID=40&md5=52423734ab002b56fb4c2d055476da17,"Kinnaird K.M., Data Sciences Initiative, Division of Applied Mathematics, Brown University, United States","Extending previous structure-based approaches to the song comparison tasks such as the fingerprint and cover song tasks, this paper introduces the aligned sub-hierarchies (AsH) representation. Built by applying a post-processing technique to the aligned hierarchies of a song, the AsH representation is the set of unique aligned hierarchies for repeats (called AHR) encoded in the original aligned hierarchies of the whole song. Effectively each AHR within AsH is a section of the aligned hierarchies for the original song. Like aligned hierarchies, the AsH representation can be embedded into a classification space with a natural metric that makes inter-song comparisons based on sections of the songs. Experiments addressing a version of the cover song task on score-based data using AsH as the basis of inter-song comparison demonstrate potential of AsH-based approaches for MIR tasks. © Katherine M. Kinnaird."
Parada-Cabaleiro E.; Schmitt M.; Batliner A.; Schuller B.W.,Musical-linguistic annotations of Il Lauro Secco,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069823889&partnerID=40&md5=b2b30dd77eb77a393dcf0110301c8cb2,"Parada-Cabaleiro E., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schmitt M., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Batliner A., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schuller B.W., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, GLAM – Group on Language, Audio and Music, Imperial College London, United Kingdom","The Italian madrigal, a polyphonic secular a cappella composition of the 16th century, is characterised by a strong musical-linguistic relationship, which has made it an icon of the ‘Renaissance humanism’. In madrigals, lyrical meaning is mimicked by the music, through the utilisation of a composition technique known as madrigalism. The synergy between Renaissance music and poetry makes madrigals of great value to musicologists, linguists, and historians—thus, it is a promising repertoire for computational musicology. However, the application of computational techniques for automatic detection of madrigalisms within scores of such repertoire is limited by the lack of annotations to refer to. In this regard, we present 30 madrigals of the anthology Il Lauro Secco encoded in two symbolic formats, MEI and **kern, with hand-encoded annotations of madrigalisms. This work aims to encourage the development of algorithms for madrigalism detection, a composition procedure typical of early music, but still underrepresented in music information retrieval research. © Emilia Parada-Cabaleiro, Maximilian Schmitt, Anton Batliner, Björn W. Schuller."
Manilow E.; Seetharaman P.; Pardo B.,The northwestern university source separation library,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069852021&partnerID=40&md5=9debebfb15d7805595437852775ee34b,"Manilow E., Northwestern University, United States; Seetharaman P., Northwestern University, United States; Pardo B., Northwestern University, United States","Audio source separation is the process of isolating individual sonic elements from a mixture or auditory scene. We present the Northwestern University Source Separation Library, or nussl for short. nussl (pronounced ‘nuzzle’) is an open-source, object-oriented audio source separation library implemented in Python. nussl provides implementations for many existing source separation algorithms and a platform for creating the next generation of source separation algorithms. By nature of its design, nussl easily allows new algorithms to be benchmarked against existing algorithms on established data sets and facilitates development of new variations on algorithms. Here, we present the design methodologies in nussl, two experiments using it, and use nussl to showcase benchmarks for some algorithms contained within. © Ethan Manilow, Prem Seetharaman, Bryan Pardo."
Korzeniowski F.; Widmer G.,Improved chord recognition by combining duration and harmonic language models,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069841915&partnerID=40&md5=c892295e22bc4e16f6d49675ba67dc22,"Korzeniowski F., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model—to be applied on chord sequences—and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results. © Filip Korzeniowski and Gerhard Widmer."
Luo Y.-J.; Su L.,Learning domain-adaptive latent representations of music signals using variational autoencoders,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069883438&partnerID=40&md5=d3f2b1e8c1f67ee7fa2a9d6ff7b09821,"Luo Y.-J., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","In this paper, we tackle the problem of domain-adaptive representation learning for music processing. Domain adaptation is an approach aiming to eliminate the distributional discrepancy of the modeling data, so as to transfer learnable knowledge from one domain to another. With its great success in the fields of computer vision and natural language processing, domain adaptation also shows great potential in music processing, for music is essentially a highly-structured semantic system having domain-dependent information. Our proposed model contains a Variational Autoencoder (VAE) that encodes the training data into a latent space, and the resulting latent representations along with its model parameters are then reused to regularize the representation learning of the downstream task where the data are in the other domain. The experiments on cross-domain music alignment, namely an audio-to-MIDI alignment, and a monophonic-to-polyphonic music alignment of singing voice show that the learned representations lead to better higher alignment accuracy than that using conventional features. Furthermore, a preliminary experiment on singing voice source separation, by regarding the mixture and the voice as two distinct domains, also demonstrates the capability to solve music processing problems from the perspective of domain-adaptive representation learning. © Yin-Jyun Luo and Li Su."
Crawford T.; Badkobeh G.; Lewis D.,Searching page-images of early music scanned with OMR: A scalable solution using minimal absent words,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067862161&doi=10.1002%2facp.1463&partnerID=40&md5=36a6560b3365673a6f01b4bb7ab35649,"Crawford T., Goldsmiths, University of London, United Kingdom; Badkobeh G., Goldsmiths, University of London, United Kingdom; Lewis D., Oxford eResearch Centre, United Kingdom","We define three retrieval tasks requiring efficient search of the musical content of a collection of ~32k page-images of 16th-century music to find: duplicates; pages with the same musical content; pages of related music. The images are subjected to Optical Music Recognition (OMR), introducing inevitable errors. We encode pages as strings of diatonic pitch intervals, ignoring rests, to reduce the effect of such errors. We extract indices comprising lists of two kinds of ‘word’. Approximate matching is done by counting the number of common words between a query page and those in the collection. The two word-types are (a) normal ngrams and (b) minimal absent words (MAWs). The latter have three important properties for our purpose: they can be built and searched in linear time, the number of MAWs generated tends to be smaller, and they preserve the structure and order of the text, obviating the need for expensive sorting operations. We show that retrieval performance of MAWs is comparable with ngrams, but with a marked speed improvement. We also show the effect of word length on retrieval. Our results suggest that an index of MAWs of mixed length provides a good method for these tasks which is scalable to larger collections. © Tim Crawford, Golnaz Badkobeh, David Lewis."
Condit-Schultz N.; Ju Y.; Fujinaga I.,A flexible approach to automated harmonic analysis: Multiple annotations of chorales by Bach and Prætorius,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069866814&partnerID=40&md5=2c06de5c771c572b7839c04d699badea,"Condit-Schultz N., McGill University, Canada; Ju Y., McGill University, Canada; Fujinaga I., McGill University, Canada","Despite being a core component of Western music theory, harmonic analysis remains a subjective endeavor, resistant automation. This subjectivity arises from disagreements regarding, among other things, the interpretation of contrapuntal figures, the set of “legal” harmonies, and how harmony relates to more abstract features like tonal function. In this paper, we provide a formal specification of harmonic analysis. We then present a novel approach to computational harmonic analysis: rather than computing harmonic analyses based on one specific set of rules, we compute all possible analyses which satisfy only basic, uncontroversial constraints. These myriad interpretations can later be filtered to extract preferred analyses; for instance, to forbid 7th chords or to prefer analyses with fewer non-chord tones. We apply this approach to two concrete musical datasets: existing encodings of 371 chorales by J.S. Bach and new encodings of 200 chorales by M. Prætorius. Through an online API users can filter and download numerous harmonic interpretations of these 571 chorales. This dataset will serve as a useful resource in the study of harmonic/functional progression, voice-leading, and the relationship between melody and harmony, and as a stepping stone towards automated harmonic analysis of more complex music. © Nathaniel Condit-Schultz, Yaolong Ju, Ichiro Fujinaga."
Lattner S.; Grachten M.; Widmer G.,Learning transposition-invariant interval features from symbolic music and audio,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069853194&partnerID=40&md5=389e2eac20fa4864b77755ba551323aa,"Lattner S., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Grachten M., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Widmer G., Institute of Computational Perception, JKU Linz, Austria","Many music theoretical constructs (such as scale types, modes, cadences, and chord types) are defined in terms of pitch intervals—relative distances between pitches. Therefore, when computer models are employed in music tasks, it can be useful to operate on interval representations rather than on the raw musical surface. Moreover, interval representations are transposition-invariant, valuable for tasks like audio alignment, cover song detection and music structure analysis. We employ a gated autoencoder to learn fixed-length, invertible and transposition-invariant interval representations from polyphonic music in the symbolic domain and in audio. An unsupervised training method is proposed yielding an organization of intervals in the representation space which is musically plausible. Based on the representations, a transposition-invariant self-similarity matrix is constructed and used to determine repeated sections in symbolic music and in audio, yielding competitive results in the MIREX task”Discovery of Repeated Themes and Sections”. © Stefan Lattner, Maarten Grachten, Gerhard Widmer."
Gotham M.,"Taking form: A representation standard, conversion code, and example corpus for recording, visualizing, and studying analyses of musical form",2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076737785&partnerID=40&md5=0e23e8738e586d910e3438460684460b,"Gotham M., Cornell University, Department of Music, United States, University of Cambridge, Sidney Sussex College, United Kingdom","We report on new specification standards for representing human analyses of musical form which enable musicians to represent their analytical view of a piece either on the score (where an encoded version is available) or on a spreadsheet. Both of these representations are simple, intuitive, and highly human-readable. Further, we provide code for converting between these formats, as well as a nested bracket representation adopted from computational linguistics which, in turn, can be visualised in familiar tree diagrams to provide 'at a glance' introductions to works. Finally, we provide an initial corpus of analyses/ annotations in these formats, report on the practicalities of amassing them, and offer tools for automatic comparison of the works in the corpus based on the content and structure of the annotations. We intend for this resource to be useful to computational musicologists, enabling study of form at scale, and also useful pedagogically to all teachers, students, and appreciators of music from whom projects of this kind can be rather disconnected. The code and corpus can be found at https://github.com/MarkGotham/Taking-Form. © 2020 International Society for Music Information Retrieval. All rights reserved."
Abeßer J.; Balke S.; Müller M.,Improving bass saliency estimation using label propagation and transfer learning,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850763&partnerID=40&md5=e7fd0c18aa858a49a626f7986de3a9d1,"Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Balke S., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","In this paper, we consider two methods to improve an algorithm for bass saliency estimation in jazz ensemble recordings which are based on deep neural networks. First, we apply label propagation to increase the amount of training data by transferring pitch labels from our labeled dataset to unlabeled audio recordings using a spectral similarity measure. Second, we study in several transfer learning experiments, whether isolated note recordings can be beneficial for pre-training a model which is later fine-tuned on ensemble recordings. Our results indicate that both strategies can improve the performance on bass saliency estimation by up to five percent in accuracy. © Jakob Abeßer, Stefan Balke, Meinard Müller."
Delbouys R.; Hennequin R.; Piccoli F.; Royo-Letelier J.; Moussallam M.,Music mood detection based on audio and lyrics with deep neural net,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069855379&partnerID=40&md5=b4436be475571caff4965ae8959a6c80,"Delbouys R., Deezer, 12 rue d’Athènes, Paris, 75009, France; Hennequin R., Deezer, 12 rue d’Athènes, Paris, 75009, France; Piccoli F., Deezer, 12 rue d’Athènes, Paris, 75009, France; Royo-Letelier J., Deezer, 12 rue d’Athènes, Paris, 75009, France; Moussallam M., Deezer, 12 rue d’Athènes, Paris, 75009, France","We consider the task of multimodal music mood prediction based on the audio signal and the lyrics of a track. We reproduce the implementation of traditional feature engineering based approaches and propose a new model based on deep learning. We compare the performance of both approaches on a database containing 18,000 tracks with associated valence and arousal values and show that our approach outperforms classical models on the arousal detection task, and that both approaches perform equally on the valence prediction task. We also compare the a poste-riori fusion with fusion of modalities optimized simultaneously with each unimodal model, and observe a significant improvement of valence prediction. We release part of our database for comparison purposes. © Rémi Delbouys, Romain Hennequin, Francesco Piccoli, Jimena Royo-Letelier, Manuel Moussallam."
Frieler K.; Höger F.; Pfleiderer M.; Dixon S.,Two web applications for exploring melodic patterns in jazz solos,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850925&partnerID=40&md5=7b9b24c202f6783dd250d62a0bf6575a,"Frieler K., Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; Höger F., Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; Pfleiderer M., Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; Dixon S., Center for Digital Music, Queen Mary University of London, United Kingdom","This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project “Dig That Lick” is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The first one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans. © Klaus Frieler, Frank Höger, Martin Pfleiderer, Simon Dixon."
Mishra S.; Sturm B.L.; Dixon S.,Understanding a deep machine listening model through feature inversion,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068994776&partnerID=40&md5=c0824b9329b09073d5f844a05666582d,"Mishra S., Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom; Sturm B.L., Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom","Methods for interpreting machine learning models can help one understand their global and/or local behaviours, and thereby improve them. In this work, we apply a global analysis method to a machine listening model, which essentially inverts the features generated in a model back into an interpretable form like a sonogram. We demonstrate this method for a state-of-the-art singing voice detection model. We train up-convolutional neural networks to invert the feature generated at each layer of the model. The results suggest that the deepest fully connected layer of the model does not preserve temporal and harmonic structures, but that the inverted features from the deepest convolutional layer do. Moreover, a qualitative analysis of a large number of inputs suggests that the deepest layer in the model learns a decision function as the information it preserves depends on the class label associated with an input. © Saumitra Mishra, Bob L. Sturm, Simon Dixon."
McKay C.; Cumming J.E.; Fujinaga I.,jSymbolic 2.2: Extracting features from symbolic music for use in musicological and MIR research,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069809082&partnerID=40&md5=c34da26245ae7902215179cb5550e00d,"McKay C., Marianopolis College, Canada; Cumming J.E., McGill University, Canada; Fujinaga I., McGill University, Canada","jSymbolic is an open-source platform for extracting features from symbolic music. These features can serve as inputs to machine learning algorithms, or they can be analyzed statistically to derive musicological insights. jSymbolic implements 246 unique features, comprising 1497 different values, making it by far the most extensive symbolic feature extractor to date. These features are designed to be applicable to a diverse range of musics, and may be extracted from both symbolic music files as a whole and from windowed subsets of them. Researchers can also use jSymbolic as a platform for developing and distributing their own bespoke features, as it has an easily extensible plug-in architecture. In addition to implementing 135 new unique features, version 2.2 of jSymbolic places a special focus on functionality for avoiding biases associated with how symbolic music is encoded. In addition, new interface elements and documentation improve convenience, ease-of-use and accessibility to researchers with diverse ranges of technical expertise. jSymbolic now includes a GUI, command-line interface, API, flexible configuration file format, extensive manual and detailed tutorial. The enhanced effectiveness of jSymbolic 2.2’s features is demonstrated in two sets of experiments: 1) genre classification and 2) Renaissance composer attribution. © Cory McKay, Julie E. Cumming, Ichiro Fujinaga."
Sears D.R.W.; Korzeniowski F.; Widmer G.,Evaluating language models of tonal harmony,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862591&partnerID=40&md5=91a52b68f33ebb04f97520d35d9e0d12,"Sears D.R.W., College of Visual and Performing Arts, Texas Tech University, Lubbock, United States; Korzeniowski F., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most state-of-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types. © Sears, Korzeniowski, Widmer."
Castellanos F.J.; Calvo-Zaragoza J.; Vigliensoni G.; Fujinaga I.,Document analysis of music score images with selectional auto-encoders,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842280&partnerID=40&md5=848e94ed88f658846c4df86895025540,"Castellanos F.J., Software and Computing Systems, University of Alicante, Spain; Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Spain; Vigliensoni G., Schulich School of Music, McGill University, Canada; Fujinaga I., Schulich School of Music, McGill University, Canada","The document analysis of music score images is a key step in the development of successful Optical Music Recognition systems. The current state of the art considers the use of deep neural networks trained to classify every pixel of the image according to the image layer it belongs to. This process, however, involves a high computational cost that prevents its use in interactive machine learning scenarios. In this paper, we propose the use of a set of deep selectional auto-encoders, implemented as fully-convolutional networks, to perform image-to-image categorizations. This strategy retains the advantages of using deep neural networks, which have demonstrated their ability to perform this task, while dramatically increasing the efficiency by processing a large number of pixels in a single step. The results of an experiment performed with a set of high-resolution images taken from Medieval manuscripts successfully validate this approach, with a similar accuracy to that of the state of the art but with a computational time orders of magnitude smaller, making this approach appropriate for being used in interactive applications. © Francisco J. Castellanos, Jorge Calvo-Zaragoza, Gabriel Vigliensoni, Ichiro Fujinaga."
Gururani S.; Sharma M.; Lerch A.,An attention mechanism for musical instrument recognition,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086245656&partnerID=40&md5=b346594bfe647bc8b3e1c3d148098fd8,"Gururani S., Center for Music Technology, Georgia Institute of Technology, United States; Sharma M., School of Interactive Computing, Georgia Institute of Technology, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, such as MedleyDB, have strong per-frame instrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or 'attend to') specific time segments in the audio relevant to each instrument label leading to interpretable results. © 2020 International Society for Music Information Retrieval. All rights reserved."
Parada-Cabaleiro E.; Schmitt M.; Batliner A.; Hantke S.; Costantini G.; Scherer K.; Schuller B.W.,Identifying emotions in opera singing: Implications of adverse acoustic conditions,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069882450&partnerID=40&md5=fb4e6b031e10f8b2432cf9b63f125c68,"Parada-Cabaleiro E., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schmitt M., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Batliner A., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Hantke S., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, Machine Intelligence and Signal Processing Group, Technische Universität München, Germany; Costantini G., Department of Electronic Engineering, University of Rome Tor Vergata, Italy; Scherer K., Department of Psychology, University of Geneva, Switzerland; Schuller B.W., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, GLAM – Group on Language, Audio and Music, Imperial College London, United Kingdom","The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners’ and machines’ identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners’ gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion. © Emilia Parada-Cabaleiro, Maximilian Schmitt, Anton Batliner, Simone Hantke, Giovanni Costantini, Klaus Scherer, Björn W. Schuller."
Harasim D.; Rohrmeier M.; O’Donnell T.J.,A generalized parsing framework for generative models of harmonic syntax,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837251&partnerID=40&md5=a1e79474513d6701d253f80a7c9847d4,"Harasim D., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland, Institut für Kunst- und Musikwissenschaft, TU Dresden, Germany; Rohrmeier M., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland, Institut für Kunst- und Musikwissenschaft, TU Dresden, Germany; O’Donnell T.J., Department of Linguistics, McGill University, Canada","Modeling the structure of musical pieces constitutes a central research problem for music information retrieval, music generation, and musicology. At the present, models of harmonic syntax face challenges on the tasks of detecting local and higher-level modulations (most previous models assume a priori knowledge of key), computing connected parse trees for long sequences, and parsing sequences that do not end with tonic chords, but in turnarounds. This paper addresses those problems by proposing a new generative formalism Probabilistic Abstract Context-Free Grammars (PACFGs) to address these issues, and presents variants of standard parsing algorithms that efficiently enumerate all possible parses of long chord sequences and to estimate their probabilities. PACFGs specifically allow for structured non-terminal symbols in rich and highly flexible feature spaces. The inference procedure moreover takes advantage of these abstractions by sharing probability mass between grammar rules over joint features. The paper presents a model of the harmonic syntax of Jazz using this formalism together with stochastic variational inference to learn the probabilistic parameters of a grammar from a corpus of Jazz-standards. The PACFG model outperforms the standard context-free approach while reducing the number of free parameters and performing key finding on the fly. © Daniel Harasim, Martin Rohrmeier, Timothy J. O’Donnell."
Li B.; Maezawa A.; Duan Z.,Skeleton plays piano: Online generation of pianist body movements from MIDI performance,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069805601&partnerID=40&md5=1ca341be9eabece48f888d7b71990ca0,"Li B., University of Rochester, United States; Maezawa A., Yamaha Corporation, Japan; Duan Z., University of Rochester, United States","Generating expressive body movements of a pianist for a given symbolic sequence of key depressions is important for music interaction, but most existing methods cannot incorporate musical context information and generate movements of body joints that are further away from the fingers such as head and shoulders. This paper addresses such limitations by directly training a deep neural network system to map a MIDI note stream and additional metric structures to a skeleton sequence of a pianist playing a keyboard instrument in an online fashion. Experiments show that (a) incorporation of metric information yields in 4% smaller error, (b) the model is capable of learning the motion behavior of a specific player, and (c) no significant difference between the generated and real human movements is observed by human subjects in 75% of the pieces. © Bochen Li, Akira Maezawa, Zhiyao Duan."
Donahue C.; Mao H.H.; McAuley J.,The NES music database: A multi-instrumental dataset with expressive performance attributes,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069846035&partnerID=40&md5=eb0cc71e1b8ae6d19698b133ad6a7d03,"Donahue C., UC San Diego, United States; Mao H.H., UC San Diego, United States; McAuley J., UC San Diego, United States","Existing research on music generation focuses on composition, but often ignores the expressive performance characteristics required for plausible renditions of resultant pieces. In this paper, we introduce the Nintendo Entertainment System Music Database (NES-MDB), a large corpus allowing for separate examination of the tasks of composition and performance. NES-MDB contains thousands of multi-instrumental songs composed for playback by the compositionally-constrained NES audio synthesizer. For each song, the dataset contains a musical score for four instrument voices as well as expressive attributes for the dynamics and timbre of each voice. Unlike datasets comprised of General MIDI files, NES-MDB includes all of the information needed to render exact acoustic performances of the original compositions. Alongside the dataset, we provide a tool that renders generated compositions as NES-style audio by emulating the device’s audio processor. Additionally, we establish baselines for the tasks of composition, which consists of learning the semantics of composing for the NES synthesizer, and performance, which involves finding a mapping between a composition and realistic expressive attributes. © Chris Donahue, Huanru Henry Mao, Julian McAuley."
Shih S.-Y.; Chi H.-Y.,"Automatic, personalized, and flexible playlist generation using reinforcement learning",2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069876448&partnerID=40&md5=b8331ad4d9685b715a131a79a4bec1a4,"Shih S.-Y., National Taiwan University, Taiwan; Chi H.-Y., KKBOX Inc., Taipei, Taiwan","Songs can be well arranged by professional music curators to form a riveting playlist that creates engaging listening experiences. However, it is time-consuming for curators to timely rearrange these playlists for fitting trends in future. By exploiting the techniques of deep learning and reinforcement learning, in this paper, we consider music playlist generation as a language modeling problem and solve it by the proposed attention language model with policy gradient. We develop a systematic and interactive approach so that the resulting playlists can be tuned flexibly according to user preferences. Considering a playlist as a sequence of words, we first train our attention RNN language model on baseline recommended playlists. By optimizing suitable imposed reward functions, the model is thus refined for corresponding preferences. The experimental results demonstrate that our approach not only generates coherent playlists automatically but is also able to flexibly recommend personalized playlists for diversity, novelty and freshness. © Shun-Yao Shih, Heng-Yu Chi."
Bittner R.M.; Fuentes M.; Rubinstein D.; Jansson A.; Choi K.; Kell T.,Mirdata: Software for reproducible usage of datasets,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093741&partnerID=40&md5=598e0700db937030d6dce36f39ccc3bb,"Bittner R.M., Spotify, United States; Fuentes M., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France, LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Rubinstein D., Spotify, United States; Jansson A., Spotify, United States; Choi K., Spotify, United States; Kell T., Spotify, United States","There are a number of efforts in the MIR community towards increased reproducibility, such as creating more open datasets, publishing code, and the use of common software libraries, e.g. for evaluation. However, when it comes to datasets, there is usually little guarantee that researchers are using the exact same data in the same way, which among other issues, makes comparisons of different methods on the ""same"" datasets problematic. In this paper, we first show how (often unknown) differences in datasets can lead to significantly different experimental results. We propose a solution to these problems in the form of an open source library, mirdata, which handles datasets in their current distribution modes, but controls for possible variability. In particular, it contains tools which: (1) validate if the user's data (e.g. audio, annotations) is consistent with a canonical version of the dataset; (2) load annotations in a consistent manner; (3) download or give instructions for obtaining data; and (4) make it easy to perform track metadata-specific analysis. © 2020 International Society for Music Information Retrieval. All rights reserved."
Lee J.; Kim S.; Lee K.,Automatic choreography generation with convolutional encoder-decoder network,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082393507&partnerID=40&md5=ee1dd1e93d25b9553e2516d19c19fd3e,"Lee J., Music & Audio Research Group, Seoul National University, South Korea; Kim S., Music & Audio Research Group, Seoul National University, South Korea; Lee K., Music & Audio Research Group, Seoul National University, South Korea","Automatic choreography generation is a challenging task because it often requires an understanding of two abstract concepts - music and dance - which are realized in the two different modalities, namely audio and video, respectively. In this paper, we propose a music-driven choreography generation system using an auto-regressive encoderdecoder network. To this end, we first collected a set of multimedia clips that include both music and corresponding dance motion. We then extract the joint coordinates of the dancer from video and the mel-spectrogram of music from audio and train our network using musicchoreography pairs as input. Finally, a novel dance motion is generated at the inference time when only music is given as an input. We performed a user study for a qualitative evaluation of the proposed method, and the results show that the proposed model is able to generate musically meaningful and natural dance movements given an unheard song. We also revealed through quantitative evaluation that the network has created a movement that correlates with the beat of music. © 2020 International Society for Music Information Retrieval. All rights reserved."
Southall C.; Stables R.; Hockman J.,Player vs transcriber: A game approach to data manipulation for automatic drum transcription,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069851541&partnerID=40&md5=9e16d4a474c67a1a36c1f6dbf0178d17,"Southall C., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Stables R., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Hockman J., DMT Lab, Birmingham City University, Birmingham, United Kingdom","State-of-the-art automatic drum transcription (ADT) approaches utilise deep learning methods reliant on time-consuming manual annotations and require congruence between training and testing data. When these conditions are not held, they often fail to generalise. We propose a game approach to ADT, termed player vs transcriber (PvT), in which a player model aims to reduce transcription accuracy of a transcriber model by manipulating training data in two ways. First, existing data may be augmented, allowing the transcriber to be trained using recordings with modified timbres. Second, additional individual recordings from sample libraries are included to generate rare combinations. We present three versions of the PvT model: AugExist, which augments pre-existing recordings; AugAddExist, which adds additional samples of drum hits to the AugExist system; and Generate, which generates training examples exclusively from individual drum hits from sample libraries. The three versions are evaluated alongside a state-of-the-art deep learning ADT system using two evaluation strategies. The results demonstrate that including the player network improves the ADT performance and suggests that this is due to improved generalisability. The results also indicate that although the Generate model achieves relatively low results, it is a viable choice when annotations are not accessible. © Carl Southall, Ryan Stables and Jason Hockman."
McLeod A.; Steedman M.,Meter detection and alignment of MIDI performance,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069443617&partnerID=40&md5=dc635a34cf83a07dea536c11585ebf77,"McLeod A., University of Edinburgh, United Kingdom; Steedman M., University of Edinburgh, United Kingdom","Metrical alignment is an integral part of any complete automatic music transcription (AMT) system. In this paper, we present an HMM for both detecting the metrical structure of given live performance MIDI data, and aligning that structure with the underlying notes. The model takes as input only a list of the notes present in a performance, and labels bars, beats, and sub beats in time. We also present an incremental algorithm which can perform inference on the model efficiently using a modified Viterbi search. We propose a new metric designed for the task, and using it, we show that our model achieves state-of-the-art performance on a corpus of metronomically aligned MIDI data, as well as a second corpus of live performance MIDI data. The code for the model described in this paper is available at https://www.github.com/apmcleod/met-align. © Andrew McLeod, Mark Steedman."
"Hajič J., Jr.; Dorfer M.; Widmer G.; Pecina P.",Towards full-pipeline handwritten OMR with musical symbol detection by U-NETS,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069830366&partnerID=40&md5=4054c1072a8e469ef7e70acf260eb853,"Hajič J., Jr., Institute of Formal and Applied Linguistics, Charles University, Czech Republic; Dorfer M., Institute of Computational Perception, Johannes Kepler University, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Austria; Pecina P., Institute of Formal and Applied Linguistics, Charles University, Czech Republic","Detecting music notation symbols is the most immediate unsolved subproblem in Optical Music Recognition for musical manuscripts. We show that a U-Net architecture for semantic segmentation combined with a trivial detector already establishes a high baseline for this task, and we propose tricks that further improve detection performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81. Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach. © Jan Hajič jr., Matthias Dorfer, Gerhard Widmer, Pavel Pecina."
Gupta C.; Tong R.; Li H.; Wang Y.,Semi-supervised lyrics and solo-singing alignment,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068989719&partnerID=40&md5=53182bda445245c2aec1219126b3822a,"Gupta C., NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore, School of Computing, National University of Singapore, Singapore; Tong R., Alibaba Inc., Singapore R and D Center, Singapore; Li H., Electrical and Computer Engineering Dept., National University of Singapore, Singapore; Wang Y., NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore, School of Computing, National University of Singapore, Singapore","We propose a semi-supervised algorithm to align lyrics to the corresponding singing vocals. The proposed method transcribes and aligns lyrics to solo-singing vocals using the imperfect transcripts from an automatic speech recognition (ASR) system and the published lyrics. The ASR provides time alignment between vocals and hypothesized lyrical content, while the non-aligned published lyrics correct the hypothesized lyrical content. The effectiveness of the proposed method is validated through three experiments. First, a human listening test shows that 73.32% of our automatically aligned sentence-level transcriptions are correct. Second, the automatically aligned sung segments are used for singing acoustic model adaptation, which reduces the word error rate (WER) of automatic transcription of sung lyrics from 72.08% to 37.15% in an open test. Third, another iteration of decoding and model adaptation increases the amount of reliably decoded segments from 44.40% to 91.96% and further reduces the WER to 36.32%. The proposed framework offers an automatic way to generate reliable alignments between lyrics and solo-singing. A large-scale solo-singing and lyrics aligned corpus can be derived with the proposed method, which will be beneficial for music and singing voice related research. © Chitralekha Gupta, Rong Tong, Haizhou, Ye Wang."
Lagrange M.; Rossignol M.; Lafay G.,Visualization of audio data using stacked graphs,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069858615&partnerID=40&md5=55fb0905a6433262515fe79cb69f33d3,"Lagrange M., LS2N, CNRS, École Centrale de Nantes, France; Rossignol M.; Lafay G., LS2N, CNRS, École Centrale de Nantes, France","In this paper, we study the benefit of considering stacked graphs to display audio data. Thanks to a careful use of layering of the spectral information, the resulting display is both concise and intuitive. Compared to the spectrogram display, it allows the reader to focus more on the temporal aspect of the time/frequency decomposition while keeping an abstract view of the spectral information. The use of such a display is validated using two perceptual experiments that demonstrate the potential of the approach. The first considers the proposed display to perform an identification task of the musical instrument and the second considers the proposed display to evaluate the technical level of a musical performer. Both experiments show the potential of the display and potential applications scenarios in musical training are discussed. © Mathieu Lagrange∗, Mathias Rossignol, Grégoire Lafay∗."
Román M.A.; Pertusa A.; Calvo-Zaragoza J.,An end-to-end framework for audio-to-score music transcription on monophonic excerpts,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065988943&partnerID=40&md5=50156a8652dbb089f3930a0dc63d7a9b,"Román M.A., U.I. for Computing Research, University of Alicante, Alicante, Spain; Pertusa A., U.I. for Computing Research, University of Alicante, Alicante, Spain; Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Valencia, Spain","In this work, we present an end-to-end framework for audio-to-score transcription. To the best of our knowledge, this is the first automatic music transcription approach which obtains directly a symbolic score from audio, instead of performing separate stages for piano-roll estimation (pitch detection and note tracking), meter detection or key estimation. The proposed method is based on a Convolutional Recurrent Neural Network architecture directly trained with pairs of spectrograms and their corresponding symbolic scores in Western notation. Unlike standard pitch estimation methods, the proposed architecture does not need the music symbols to be aligned with their audio frames thanks to a Connectionist Temporal Classification loss function. Training and evaluation were performed using a large dataset of short monophonic scores (incipits) from the RISM collection, that were synthesized to get the ground-truth data. Although there is still room for improvement, most musical symbols were correctly detected and the evaluation results validate the proposed approach. We believe that this end-to-end framework opens new avenues for automatic music transcription. © Miguel A. Román, Antonio Pertusa, Jorge Calvo-Zaragoza."
Lisena P.; Todorov K.; Cecconi C.; Leresche F.; Canno I.; Puyrenier F.; Voisin M.; Le Meur T.; Troncy R.,Controlled vocabularies for music metadata,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069830249&partnerID=40&md5=7594e58f5ef462ab63f9faddf62f0bb8,"Lisena P., EURECOM, Sophia Antipolis, France; Todorov K., LIRMM, University of Montpellier, CNRS, France; Cecconi C., Philharmonie de Paris, France; Leresche F., Bibliothèque Nationale de France, France; Canno I., Radio France, France; Puyrenier F., Bibliothèque Nationale de France, France; Voisin M., Radio France, France; Le Meur T., Philharmonie de Paris, France; Troncy R., EURECOM, Sophia Antipolis, France","We present a set of music-specific controlled vocabularies, formalized using Semantic Web languages, describing topics like musical genres, keys, or medium of performance. We have collected a number of existing vocabularies in various formats, converted them to SKOS and performed the interconnection of their equivalent terms. In addition, novel vocabularies, not available online before, have been designed by an editorial team. Next to multilingual labels and definitions, we provide hierarchical relations as well as links to external resources. We also show the application of those vocabularies for the production of vector embeddings, allowing for the calculation of distances between keys or between instruments. © Lisena et al."
Lee K.; Choi K.; Nam J.,Revisiting singing voice detection: A quantitative review and the future outlook,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068955461&partnerID=40&md5=498af8dc20563758a396c6072302fca4,"Lee K., School of Computing, KAIST, South Korea; Choi K., Spotify Inc., United States; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval. Although several proposed algorithms have shown high performances, we argue that there is still room for improving the singing voice detection system. In order to identify the area of improvement, we first perform an error analysis on three recent singing voice detection systems. Based on the analysis, we design novel methods to test the systems on multiple sets of internally curated and generated data to further examine the pitfalls, which are not clearly revealed with the currently available datasets. From the experiment results, we also propose several directions towards building a more robust singing voice detector. © Kyungyun Lee, Keunwoo Choi, Juhan Nam."
Mustaine M.; Ibrahim K.M.; Gupta C.; Wang Y.,Empirically weighing the importance of decision factors when selecting music to sing,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069864335&partnerID=40&md5=399b6d91282468bda5aa9a1850b1f580,"Mustaine M., School of Computing, National University of Singapore, Singapore; Ibrahim K.M., School of Computing, National University of Singapore, Singapore; Gupta C., School of Computing, National University of Singapore, Singapore, NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore","Although music cognition and music information retrieval have many common areas of research interest, relatively little work utilizes a combination of signal- and human-centric approaches when assessing complex cognitive phenomena. This work explores the importance of four cognitive decision-making factors (familiarity, genre preference, ease of vocal reproducibility, and overall preference) influence in the perception of “singability”, how attractive a song is to sing. In Experiment One, we develop a model to validate and empirically determine to what degree these factors are important when evaluating its singability. Results indicate that evaluations of how these four factors impact singability strongly correlate with pairwise evaluations (ρ = 0.692, p < 0.0001), supporting the notion that singability is a measurable cognitive process. Experiment Two examines the degree to which timbral and rhythmic features contribute to singability. Regression and random forest analysis find that some selected features are more significant than others. We discuss the method we use to empirically assess the complex decisions, and provide a preliminary exploration regarding what acoustic features may motivate these choices. © Michael Mustaine, Karim M. Ibrahim, Chitralekha Gupta, Ye Wang."
Stoller D.; Ewert S.; Dixon S.,Wave-U-Net: A multi-scale neural network for end-to-end audio source separation,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069811693&partnerID=40&md5=cf793041e06ab9c01b06bddb0212daae,"Stoller D., Queen Mary University of London, United Kingdom; Ewert S., Spotify, Sweden; Dixon S., Queen Mary University of London, United Kingdom","Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem. © Daniel Stoller, Sebastian Ewert, Simon Dixon."
Weiß C.; Balke S.; Abeßer J.; Müller M.,Computational corpus analysis: A case study on jazz solos,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069836077&partnerID=40&md5=91da748135f152dda08eaff51c8c1741,"Weiß C., International Audio Laboratories, Erlangen, Germany; Balke S., International Audio Laboratories, Erlangen, Germany; Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","For musicological studies on large corpora, the compilation of suitable data constitutes a time-consuming step. In particular, this is true for high-quality symbolic representations that are generated manually in a tedious process. A recent study on Western classical music has shown that musical phenomena such as the evolution of tonal complexity over history can also be analyzed on the basis of audio recordings. As our first contribution, we transfer this corpus analysis method to jazz music using the Weimar Jazz Database, which contains high-level symbolic transcriptions of jazz solos along with the audio recordings. Second, we investigate the influence of the input representation type on the corpus-level observations. In our experiments, all representation types led to qualitatively similar results. We conclude that audio recordings can build a reasonable basis for conducting such type of corpus analysis. © Christof Weiß, Stefan Balke, Jakob Abeßer, Meinard Müller."
Takahashi T.; Fukayama S.; Goto M.,Instrudive: A music visualization system based on automatically recognized instrumentation,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069820716&partnerID=40&md5=2c3a42d2f9e82f7bfcdaf0ce790536d7,"Takahashi T., University of Tsukuba, Japan, National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","A music visualization system called Instrudive is presented that enables users to interactively browse and listen to musical pieces by focusing on instrumentation. Instrumentation is a key factor in determining musical sound characteristics. For example, a musical piece performed with vocals, electric guitar, electric bass, and drums can generally be associated with pop/rock music but not with classical or electronic. Therefore, visualizing instrumentation can help listeners browse music more efficiently. Instrudive visualizes musical pieces by illustrating instrumentation with multi-colored pie charts and displays them on a map in accordance with the similarity in instrumentation. Users can utilize three functions. First, they can browse musical pieces on a map by referring to the visualized instrumentation. Second, they can interactively edit a playlist that showing the items to be played later. Finally, they can discern the temporal changes in instrumentation and skip to a preferable part of a piece with a multi-colored graph. The instruments are identified using a deep convolutional neural network that has four convolutional layers with different filter shapes. Evaluation of the proposed model against conventional and state-of-the-art methods showed that it has the best performance. © Takumi Takahashi, Satoru Fukayama, Masataka Goto."
Ycart A.; McLeod A.; Benetos E.; Yoshii K.,Blending acoustic and language model predictions for automatic music transcription,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084934683&partnerID=40&md5=55e51c2d0d5012fe546d38a81f91db7e,"Ycart A., Queen Mary University of London, United Kingdom; McLeod A., Kyoto University, Japan; Benetos E., Queen Mary University of London, United Kingdom; Yoshii K., Kyoto University, Japan","In this paper, we introduce a method for converting an input probabilistic piano roll (the output of a typical multipitch detection model) into a binary piano roll. The task is an important step for many automatic music transcription systems with the goal of converting an audio recording into some symbolic format. Our model has two components: an LSTM-based music language model (MLM) which can be trained on any MIDI data, not just that aligned with audio; and a blending model used to combine the probabilities of the MLM with those of the input probabilistic piano roll given by an acoustic multi-pitch detection model, which must be trained on (a comparably small amount of) aligned data. We use scheduled sampling to make the MLM robust to noisy sequences during testing. We analyze the performance of our model on the MAPS dataset using two different timesteps (40ms and 16th-note), comparing it against a strong baseline hidden Markov model with a training method not used before for the task to our knowledge. We report a statistically significant improvement over HMM decoding in terms of notewise F-measure with both timesteps, with 16th note timesteps improving further compared to 40ms timesteps. © Ycart, McLeod, Benetos, Yoshii."
Esling P.; Chemla–Romeu-Santos A.; Bitton A.,"Bridging audio analysis, perception and synthesis with perceptually-regularized variational timbre spaces",2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069823548&partnerID=40&md5=6202826c7ae7f050525b45e8da84c886,"Esling P., Institut de Recherche et Coordination Acoustique-Musique (IRCAM), CNRS - UMR 9912, UPMC - Sorbonne Universite, 1 Place Igor Stravinsky, Paris, F-75004, France; Chemla–Romeu-Santos A., Institut de Recherche et Coordination Acoustique-Musique (IRCAM), CNRS - UMR 9912, UPMC - Sorbonne Universite, 1 Place Igor Stravinsky, Paris, F-75004, France; Bitton A., Institut de Recherche et Coordination Acoustique-Musique (IRCAM), CNRS - UMR 9912, UPMC - Sorbonne Universite, 1 Place Igor Stravinsky, Paris, F-75004, France","Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception. © Philippe Esling, Axel Chemla, Adrien Bitton."
Royo-Letelier J.; Hennequin R.; Tran V.-A.; Moussallam M.,Disambiguating music artists at scale with audio metric learning,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850621&partnerID=40&md5=d27f45ec1b718cf5fa71ddf60dab9bd1,"Royo-Letelier J., Deezer, 12 rue d’Athènes, Paris, 75009, France; Hennequin R., Deezer, 12 rue d’Athènes, Paris, 75009, France; Tran V.-A., Deezer, 12 rue d’Athènes, Paris, 75009, France; Moussallam M., Deezer, 12 rue d’Athènes, Paris, 75009, France","We address the problem of disambiguating large scale catalogs through the definition of an unknown artist clustering task. We explore the use of metric learning techniques to learn artist embeddings directly from audio, and using a dedicated homonym artists dataset, we compare our method with a recent approach that learn similar embeddings using artist classifiers. While both systems have the ability to disambiguate unknown artists relying exclusively on audio, we show that our system is more suitable in the case when enough audio data is available for each artist in the train dataset. We also propose a new negative sampling method for metric learning that takes advantage of side information such as music genre during the learning phase and shows promising results for the artist clustering task. © First author, Second author, Third author, Fourth author."
Andreux M.; Mallat S.,Music generation and transformation with moment matching-scattering inverse networks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069866092&partnerID=40&md5=407f1ac7d2d84e9dedf9bffd2950b042,"Andreux M., Département d’informatique de l’ENS, École normale supérieure, CNRS, PSL Research University, Paris, 75005, France; Mallat S., Département d’informatique de l’ENS, École normale supérieure, CNRS, PSL Research University, Paris, 75005, France","We introduce a Moment Matching-Scattering Inverse Network (MM-SIN) to generate and transform musical sounds. The MM-SIN generator is similar to a variational autoencoder or an adversarial network. However, the encoder or the discriminator are not learned, but computed with a scattering transform defined from prior information on sparse time-frequency audio properties. The generator is trained by jointly minimizing the reconstruction loss of an inverse problem, and a generation loss which computes a distance over scattering moments. It has a similar causal architecture as a WaveNet and provides a simpler mathematical model related to time-frequency decompositions. Numerical experiments demonstrate that this MM-SIN generates new realistic musical signals. It can transform low-level musical attributes such as pitch with a linear transformation in the embedding space of scattering coefficients. © Mathieu Andreux and Stéphane Mallat."
Silva D.F.; Falcão F.V.; Andrade N.,Summarizing and comparing music data and its application on cover song identification,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069856327&partnerID=40&md5=9a645180ad19a1a429002cbc6885481a,"Silva D.F., Departamento de Computação, Universidade Federal de São Carlos, São Carlos, Brazil; Falcão F.V., Departamento de Sistemas e Computação, Universidade Federal de Campina Grande, Campina Grande, Brazil; Andrade N., Departamento de Sistemas e Computação, Universidade Federal de Campina Grande, Campina Grande, Brazil","While there is a multitude of music information retrieval algorithms that have distance functions as their core procedure, comparing the similarity between recordings is a costly procedure. At the same, the recent growth of digital music repositories makes necessary the development of novel time- and memory-efficient algorithms to deal with music data. One particularly interesting idea on the literature is transforming the music data into reduced representations, improving the memory usage and reducing the time necessary to assess the similarity. However, these techniques usually add other issues, such as an expensive preprocessing or a reduced retrieval performance. In this paper, we propose a novel method to summarize a recording in small snippets based on its self-similarity information. Besides, we present a simple way to compare other recordings to these summaries. We demonstrate, in the scenario of cover song identification, that our method is more than one order of magnitude faster than state-of-the-art adversaries, at the same time that the retrieval performance is not affected significantly. Additionally, our method is incremental, which allows the easy and fast update of the database when a new song needs to be inserted into the retrieval system. © Diego Furtado Silva, Felipe Vieira Falcão, Nazareno Andrade."
Harrison P.M.C.; Pearce M.T.,An energy-based generative sequence model for testing sensory theories of Western harmony,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069853333&partnerID=40&md5=664ca62846d3c81df92bf3f9af32e05b,"Harrison P.M.C., Queen Mary University of London, Cognitive Science Research Group, United Kingdom; Pearce M.T., Queen Mary University of London, Cognitive Science Research Group, United Kingdom","The relationship between sensory consonance and Western harmony is an important topic in music theory and psychology. We introduce new methods for analysing this relationship, and apply them to large corpora representing three prominent genres of Western music: classical, popular, and jazz music. These methods centre on a generative sequence model with an exponential-family energy-based form that predicts chord sequences from continuous features. We use this model to investigate one aspect of instantaneous consonance (harmonicity) and two aspects of sequential consonance (spectral distance and voice-leading distance). Applied to our three musical genres, the results generally support the relationship between sensory consonance and harmony, but lead us to question the high importance attributed to spectral distance in the psychological literature. We anticipate that our methods will provide a useful platform for future work linking music psychology to music theory. © Peter M. C. Harrison, Marcus T. Pearce."
Fuentes M.; McFee B.; Crayencour H.C.; Essid S.; Bello J.P.,Analysis of common design choices in deep learning systems for downbeat tracking,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065981409&partnerID=40&md5=77c171ee98a5b550d7533dea69eba1c3,"Fuentes M., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France, LTCI, Télécom ParisTech, Univ. Paris-Saclay, France; McFee B., Music and Audio Research Laboratory, New York University, United States, Center of Data Science, New York University, United States; Crayencour H.C., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France; Essid S., LTCI, Télécom ParisTech, Univ. Paris-Saclay, France; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Downbeat tracking consists of annotating a piece of musical audio with the estimated position of the first beat of each bar. In recent years, increasing attention has been paid to applying deep learning models to this task, and various architectures have been proposed, leading to a significant improvement in accuracy. However, there are few insights about the role of the various design choices and the delicate interactions between them. In this paper we offer a systematic investigation of the impact of largely adopted variants. We study the effects of the temporal granularity of the input representation (i.e. beat-level vs tatum-level) and the encoding of the networks outputs. We also investigate the potential of convolutional-recurrent networks, which have not been explored in previous downbeat tracking systems. To this end, we exploit a state-of-the-art recurrent neural network where we introduce those variants, while keeping the training data, network learning parameters and post-processing stages fixed. We find that temporal granularity has a significant impact on performance, and we analyze its interaction with the encoding of the networks outputs. © Magdalena Fuentes, Brian McFee, Hélène C. Crayencour, Slim Essid, Juan P. Bello."
Pons J.; Nieto O.; Prockup M.; Schmidt E.; Ehmann A.; Serra X.,End-to-end learning for music audio tagging at scale,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065977708&partnerID=40&md5=8e4ab4351e7290a6c22723594079e75f,"Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Nieto O., Pandora Media Inc., Oakland, CA, United States; Prockup M., Pandora Media Inc., Oakland, CA, United States; Schmidt E., Pandora Media Inc., Oakland, CA, United States; Ehmann A., Pandora Media Inc., Oakland, CA, United States; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios. © Jordi Pons, Oriol Nieto, Matthew Prockup, Erik Schmidt, Andreas Ehmann, Xavier Serra."
Aljanaki A.; Soleymani M.,A data-driven approach to mid-level perceptual musical feature modeling,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069866131&partnerID=40&md5=6c09f65bafa636da83e7530c9dcd1334,"Aljanaki A., Institute of Computational Perception, Johannes Kepler University, Austria; Soleymani M., Swiss Center for Affective Sciences, University of Geneva, Switzerland","Musical features and descriptors could be coarsely divided into three levels of complexity. The bottom level contains the basic building blocks of music, e.g., chords, beats and timbre. The middle level contains concepts that emerge from combining the basic blocks: tonal and rhythmic stability, harmonic and rhythmic complexity, etc. High-level descriptors (genre, mood, expressive style) are usually modeled using the lower level ones. The features belonging to the middle level can both improve automatic recognition of high-level descriptors, and provide new music retrieval possibilities. Mid-level features are subjective and usually lack clear definitions. However, they are very important for human perception of music, and on some of them people can reach high agreement, even though defining them and therefore, designing a hand-crafted feature extractor for them can be difficult. In this paper, we derive the mid-level descriptors from data. We collect and release a dataset1 of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation. We then compare several approaches to predicting these descriptors from spectrograms using deep-learning. We also demonstrate the usefulness of these mid-level features using music emotion recognition as an application. © Anna Aljanaki,, Mohammad Soleymani."
Yang R.; Wang D.; Wang Z.; Chen T.; Jiang J.; Xia G.,Deep music analogy via latent representation disentanglement,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083435998&partnerID=40&md5=ed4cb69e1c72deb9f0287edfaf51e710,"Yang R., Music X Lab, NYU Shanghai, China; Wang D., Music X Lab, NYU Shanghai, China; Wang Z., Music X Lab, NYU Shanghai, China; Chen T., Music X Lab, NYU Shanghai, China; Jiang J., Music X Lab, NYU Shanghai, China, Machine Learning Department, Carnegie Mellon University, United States; Xia G., Music X Lab, NYU Shanghai, China","Analogy-making is a key method for computer algorithms to generate both natural and creative music pieces. In general, an analogy is made by partially transferring the music abstractions, i.e., high-level representations and their relationships, from one piece to another; however, this procedure requires disentangling music representations, which usually takes little effort for musicians but is non-trivial for computers. Three sub-problems arise: extracting latent representations from the observation, disentangling the representations so that each part has a unique semantic interpretation, and mapping the latent representations back to actual music. In this paper, we contribute an explicitlyconstrained variational autoencoder (EC2-VAE) as a unified solution to all three sub-problems. We focus on disentangling the pitch and rhythm representations of 8-beat music clips conditioned on chords. In producing music analogies, this model helps us to realize the imaginary situation of ""what if"" a piece is composed using a different pitch contour, rhythm pattern, or chord progression by borrowing the representations from other pieces. Finally, we validate the proposed disentanglement method using objective measurements and evaluate the analogy examples by a subjective study. © 2020 International Society for Music Information Retrieval. All rights reserved."
Liebman E.; White C.N.; Stone P.,On the impact of music on decision making in cooperative tasks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069857537&partnerID=40&md5=5555737395f3467ec673c8836b6cccc4,"Liebman E., University of Texas at Austin, Computer Science Department, United States; White C.N., Missouri Western State University, Department of Psychology, United States; Stone P., University of Texas at Austin, Computer Science Department, United States","Numerous studies have demonstrated that mood affects emotional and cognitive processing. Previous work has established that music-induced mood can measurably alter people’s behavior in different contexts. However, the nature of how decision-making is affected by music in social settings hasn’t been sufficiently explored. The goal of this study is to examine which aspects of people’s decision making in inter-social tasks are affected when exposed to music. For this purpose, we devised an experiment in which people drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results indicate that music indeed alters people’s behavior with respect to this social task. To further understand the correspondence between auditory features and decision making, we have also studied how individual aspects of music affected response patterns. © Elad Liebman, Corey N. White, Peter Stone."
Southall C.; Stables R.; Hockman J.,Improving peak-picking using multiple time-step loss functions,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068974002&partnerID=40&md5=304530281415beb7624f2345e3072442,"Southall C., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Stables R., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Hockman J., DMT Lab, Birmingham City University, Birmingham, United Kingdom","The majority of state-of-the-art methods for music information retrieval (MIR) tasks now utilise deep learning methods reliant on minimisation of loss functions such as cross entropy. For tasks that include framewise binary classification (e.g., onset detection, music transcription) classes are derived from output activation functions by identifying points of local maxima, or peaks. However, the operating principles behind peak picking are different to that of the cross entropy loss function, which minimises the absolute difference between the output and target values for a single frame. To generate activation functions more suited to peak-picking, we propose two versions of a new loss function that incorporates information from multiple time-steps: 1) multi-individual, which uses multiple individual time-step cross entropies; and 2) multi-difference, which directly compares the difference between sequential time-step outputs. We evaluate the newly proposed loss functions alongside standard cross entropy in the popular MIR tasks of onset detection and automatic drum transcription. The results highlight the effectiveness of these loss functions in the improvement of overall system accuracies for both MIR tasks. Additionally, directly comparing the output from sequential time-steps in the multi-difference approach achieves the highest performance. © Carl Southall, Ryan Stables and Jason Hockman."
Zangerle E.; Pichl M.,Content-based user models: Modeling the many faces of musical preference,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069857301&partnerID=40&md5=944bb3e16c41e9499e64367444589aee,"Zangerle E., Universität Innsbruck, Department of Computer Science, Austria; Pichl M., Universität Innsbruck, Department of Computer Science, Austria","User models that capture the musical preferences of users are central for many tasks in music information retrieval and music recommendation, yet, it has not been fully explored and exploited. To this end, the musical preferences of users in the context of music recommender systems have mostly been captured in collaborative filtering-based approaches. Alternatively, users can be characterized by their average listening behavior and hence, by the mean values of a set of content descriptors of tracks the users listened to. However, a user may listen to highly different tracks and genres. Thus, computing the average of all tracks does not capture the user’s listening behavior well. We argue that each user may have many different preferences that depend on contextual aspects (e.g., listening to classical music when working and hard rock when doing sports) and that user models should account for these different sets of preferences. In this paper, we provide a detailed analysis and evaluation of different user models that describe a user’s musical preferences based on acoustic features of tracks the user has listened to. © Eva Zangerle, Martin Pichl."
Ofner A.; Stober S.,Shared generative representation of auditory concepts and EEG to reconstruct perceived and imagined music,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069852890&partnerID=40&md5=2a9387c1269a25309e34e1fc585745a8,"Ofner A., Research Focus Cognitive Sciences, University of Potsdam, Germany; Stober S., Research Focus Cognitive Sciences, University of Potsdam, Germany","Retrieving music information from brain activity is a challenging and still largely unexplored research problem. In this paper we investigate the possibility to reconstruct perceived and imagined musical stimuli from electroencephalography (EEG) recordings based on two datasets. One dataset contains multichannel EEG of subjects listening to and imagining rhythmical patterns presented both as sine wave tones and short looped spoken utterances. These utterances leverage the well-known speech-to-song illusory transformation which results in very catchy and easy to reproduce motifs. A second dataset provides EEG recordings for the perception of 10 full length songs. Using a multi-view deep generative model we demonstrate the feasibility of learning a shared latent representation of brain activity and auditory concepts, such as rhythmical motifs appearing across different instrumentations. Introspection of the model trained on the rhythm dataset reveals disentangled rhythmical and timbral features within and across subjects. The model allows continuous interpolation between representations of different observed variants of the presented stimuli. By decoding the learned embeddings we were able to reconstruct both perceived and imagined music. Stimulus complexity and the choice of training data shows strong effect on the reconstruction quality. © André Ofner, Sebastian Stober."
Finkensiep C.; Neuwirth M.; Rohrmeier M.,Generalized skipgrams for pattern discovery in polyphonic streams,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069805622&partnerID=40&md5=c1548ff266aa1b49fa53727f9bc18e88,"Finkensiep C., École Polytechnique Fédérale de Lausanne, Switzerland; Neuwirth M., École Polytechnique Fédérale de Lausanne, Switzerland; Rohrmeier M., École Polytechnique Fédérale de Lausanne, Switzerland","The discovery of patterns using a minimal set of assumptions constitutes a central challenge in the modeling of polyphonic music and complex streams in general. Skipgrams have been found to be a powerful model for capturing semi-local dependencies in sequences of entities when dependencies may not be directly adjacent (see, for instance, the problems of modeling sequences of words or letters in computational linguistics). Since common skipgrams define locality based on indices, they can only be applied to a single stream of non-overlapping entities. This paper proposes a generalized skipgram model that allows arbitrary cost functions (defining locality), efficient filtering, recursive application (skipgrams over skipgrams), and memory efficient streaming. Further, a sampling mechanism is proposed that flexibly controls runtime and output size. These generalizations and optimizations make it possible to employ skipgrams for the discovery of repeated patterns of close, nonsimultaneous events or notes. The extensions to the skipgram model provided here do not only apply to musical notes but to any list of entities that is monotonic with respect to a given cost function. © Christoph Finkensiep, Markus Neuwirth, Martin."
Demetriou A.; Jansson A.; Kumar A.; Bittner R.M.,Vocals in music matter: The relevance of vocals in the minds of listeners,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067311732&partnerID=40&md5=6f34a519a7f6def1956e962e2fc44373,"Demetriou A., Multimedia Computing Group, TU Delft, Netherlands, Spotify Inc., New York City, United States; Jansson A., Spotify Inc., New York City, United States; Kumar A., Spotify Inc., New York City, United States; Bittner R.M., Spotify Inc., New York City, United States","In music information retrieval, we often make assertions about what features of music are important to study, one of which is vocals. While the importance of vocals in music preference is both intuitive and anticipated by psychological theory, we have not found any survey studies that confirm this commonly held assertion. We address two questions: (1) what components of music are most salient to people’s musical taste, and (2) how do vocals rank relative to other components of music, in regards to whether people like or dislike a song. Lastly, we explore the aspects of the voice that listeners find important. Two surveys of Spotify users were conducted. The first gathered open-format responses that were then card-sorted into semantic categories by the team of researchers. The second asked respondents to rank the semantic categories derived from the first survey. Responses indicate that vocals were a salient component in the minds of listeners. Further, vocals ranked high as a self-reported factor for a listener liking or disliking a track, among a statistically significant ranking of musical attributes. In addition, we open several new interesting problem areas that have yet to be explored in MIR. © Andrew Demetriou, Andreas Jansson, Aparna Kumar, Rachel M. Bittner."
Knees P.; Schedl M.; Goto M.,Intelligent user interfaces for music discovery: The past 20 years and what's to come,2019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085359140&partnerID=40&md5=64d28be6e457ea5ab4b4f7a52e0c8d93,"Knees P., Faculty of Informatics, TU Wien, Vienna, Austria; Schedl M., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Providing means to assist the user in finding music is one of the original motivations underlying the research field known as Music Information Retrieval (MIR). Therefore, already the first edition of ISMIR in the year 2000 called for papers addressing the topic of ""User interfaces for music IR"". Since then, the way humans interact with technology to access and listen to music has substantially changed, not least driven by the advances of MIR and related research fields such as machine learning and recommender systems. In this paper, we reflect on the evolution of MIR-driven user interfaces for music browsing and discovery over the past two decades. We argue that three major developments have transformed and shaped user interfaces during this period, each connected to a phase of new listening practices: first, connected to personal music collections, intelligent audio processing and content description algorithms that facilitate the automatic organization of repositories and finding music according to sound qualities; second, connected to collective web platforms, the exploitation of user-generated metadata pertaining to semantic descriptions; and third, connected to streaming services, the collection of online music interaction traces on a large scale and their exploitation in recommender systems. We review and contextualize work from ISMIR and related venues from all three phases and extrapolate current developments to outline possible scenarios of music recommendation and listening interfaces of the future. © 2020 International Society for Music Information Retrieval. All rights reserved."
Kedyte V.; Panteli M.; Weyde T.; Dixon S.,Geographical origin prediction of folk music recordings from the United Kingdom,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053915605&partnerID=40&md5=aaa785d172b91ae8c622e56bfc574b1d,"Kedyte V., Department of Computer Science, City University of London, United Kingdom; Panteli M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Weyde T., Department of Computer Science, City University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Field recordings from ethnomusicological research since the beginning of the 20th century are available today in large digitised music archives. The application of music information retrieval and data mining technologies can aid large-scale data processing leading to a better understanding of the history of cultural exchange. In this paper we focus on folk and traditional music from the United Kingdom and study the correlation between spatial origins and musical characteristics. In particular, we investigate whether the geographical location of music recordings can be predicted solely from the content of the audio signal. We build a neural network that takes as input a feature vector capturing musical aspects of the audio signal and predicts the latitude and longitude of the origins of the music recording. We explore the performance of the model for different sets of features and compare the prediction accuracy between geographical regions of the UK. Our model predicts the geographical coordinates of music recordings with an average error of less than 120 km. The model can be used in a similar manner to identify the origins of recordings in large unlabelled music collections and reveal patterns of similarity in music from around the world. © 2019 Vytaute Kedyte, Maria Panteli, Tillman Weyde, Simon Dixon."
Ross J.C.; Mishra A.; Ganguli K.K.; Bhattacharyya P.; Rao P.,Identifying raga similarity through embeddings learned from compositions' notation,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068065865&partnerID=40&md5=d2be6db29c90d2c4e2ce577e7d841eb4,"Ross J.C., Dept. of Computer Science and Engineering, India; Mishra A., IBM Research India, India; Ganguli K.K., Dept. of Electrical Engineering, Indian Institute of Technology Bombay, India; Bhattacharyya P., Dept. of Computer Science and Engineering, India; Rao P., Dept. of Electrical Engineering, Indian Institute of Technology Bombay, India","Identifying similarities between ragas in Hindustani music impacts tasks like music recommendation, music information retrieval and automatic analysis of large-scale musical content. Quantifying raga similarity becomes extremely challenging as it demands assimilation of both intrinsic (viz., notes, tempo) and extrinsic (viz. raga singingtime, emotions conveyed) properties of ragas. This paper introduces novel frameworks for quantifying similarities between ragas based on their melodic attributes alone, available in the form of bandish (composition) notation. Based on the hypothesis that notes in a particular raga are characterized by the company they keep, we design and train several deep recursive neural network variants with Long Short-term Memory (LSTM) units to learn distributed representations of notes in ragas from bandish notations. We refer to these distributed representations as note-embeddings. Note-embeddings, as we observe, capture a raga's identity, and thus the similarity between note-embeddings signifies the similarity between the ragas. Evaluations with perplexity measure and clustering based method show the performance improvement in identifying similarities using note-embeddings over n-gram and unidirectional LSTM baselines. While our metric may not capture similarity between ragas in their entirety, it could be quite useful in various computational music settings that heavily rely on melodic information. © 2019 Joe Cheri Ross, Abhijit Mishra, Kaustuv Kanti Ganguli, Pushpak Bhattacharyya, Preeti Rao."
Oliveira R.S.; Nóbrega C.; Marinho L.B.; Andrade N.,A multiobjective music recommendation approach for aspect-based diversification,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069942935&partnerID=40&md5=56f8557a455506b3ec2fbab3c7860fa7,"Oliveira R.S., UFCG - Federal University of Campina Grande, Brazil; Nóbrega C., UFCG - Federal University of Campina Grande, Brazil; Marinho L.B., UFCG - Federal University of Campina Grande, Brazil; Andrade N., UFCG - Federal University of Campina Grande, Brazil","Many successful recommendation approaches are based on the optimization of some explicit utility function defined in terms of the misfit between the predicted and the actual items of the user. Although effective, this approach may lead to recommendations that are relevant but obvious and uninteresting. Many approaches investigate this problem by trying to avoid recommendation lists in which items are very similar to each other (aka diversification) with respect to some aspect of the item. However, users may have very different preferences concerning what aspects should be diversified and what should match their past/current preferences. In this paper we take this into consideration by proposing a solution based on multiobjective optimization for generating recommendation lists featuring the optimal balance between the aspects that should be held fixed (maximize similarity with users actual items) and the ones that should be diversified (minimize similarity with other items in the recommendation list). We evaluate our proposed approach on real data from Last.fm and demonstrate its effectiveness in contrast to state-of-the-art approaches. © 2019 Ricardo S. Oliveira, Caio Nóbrega, Leandro B. Marinho, Nazareno Andrade."
Cogliati A.; Duan Z.,A metric for music notation transcription accuracy,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053807815&partnerID=40&md5=7e3f896faa8b50687883c9fba888d1bb,"Cogliati A., University of Rochester, Electrical and Computer Engineering, United States; Duan Z., University of Rochester, Electrical and Computer Engineering, United States","Automatic music transcription aims at transcribing musical performances into music notation. However, most existing transcription systems only focus on parametric transcription, i.e., they output a symbolic representation in absolute terms, showing frequency and absolute time (e.g., a pianoroll representation), but not in musical terms, with spelling distinctions (e.g., A versus G#) and quantized meter. Recent attempts at producing full music notation output have been hindered by the lack of an objective metric to measure the adherence of the results to the ground truth music score, and had to rely on time-consuming human evaluation by music theorists. In this paper, we propose an edit distance, similar to the Levenshtein Distance used for measuring the difference between two sequences, typically strings of characters. The metric treats a music score as a sequence of sets of musical objects, ordered by their onsets. The metric reports the differences between two music scores based on twelve aspects: barlines, clefs, key signatures, time signatures, notes, note spelling, note durations, stem directions, groupings, rests, rest duration, and staff assignment. We also apply a linear regression model to the metric in order to predict human evaluations on a dataset of short music excerpts automatically transcribed into music notation. © 2019 Andrea Cogliati, Zhiyao Duan."
Defferrard M.; Benzi K.; Vandergheynst P.; Bresson X.,FMA: A dataset for music analysis,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068955365&partnerID=40&md5=52ec35cfef56f2b0a0851dd8d52c1502,"Defferrard M., LTS2, EPFL, Switzerland; Benzi K., LTS2, EPFL, Switzerland; Vandergheynst P., LTS2, EPFL, Switzerland; Bresson X., SCSE, NTU, Singapore, Singapore","We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commonslicensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma. © 2019 Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson."
Dai J.; Dixon S.,Analysis of interactive intonation in unaccompanied SATB ensembles,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059844430&partnerID=40&md5=2b9a6516411eab5208d0a33e1e8feada,"Dai J., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Unaccompanied ensemble singing is common in many musical cultures, yet it requires great skill for singers to listen to each other and adjust their pitch to stay in tune. The aim of this research is to investigate interaction in four-part (SATB) singing from the point of view of pitch accuracy (intonation). In particular we compare intonation accuracy of individual singers and collaborative ensembles. 20 participants (five groups of four) sang two pieces of music in three different listening conditions: solo, with one vocal part missing and with all vocal parts. After semi-automatic pitch extraction and manual correction, we annotated the recordings and calculated the pitch error, melodic interval error, harmonic interval error and note stability. We observed significant differences between individual and interactional intonation, more specifically: 1) Singing without the bass part has less mean absolute pitch error than singing with all vocal parts; 2) Mean absolute melodic interval error increases when participants can hear the other parts; 3) Mean absolute harmonic interval error is higher in the one-way interaction condition than the two-way interaction condition; and 4) Singers produce more stable notes when singing solo than with their partners. © 2019 Jiajie Dai, Simon Dixon."
Hawthorne C.; Elsen E.; Song J.; Roberts A.; Simon I.; Raffel C.; Engel J.; Oore S.; Eck D.,Onsets and frames: Dual-objective piano transcription,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059752323&partnerID=40&md5=8eee573801f54d440814958b249a119b,"Hawthorne C., Google Brain Team, Mountain View, CA, United States; Elsen E., Google Brain Team, Mountain View, CA, United States; Song J., Google Brain Team, Mountain View, CA, United States; Roberts A., Google Brain Team, Mountain View, CA, United States; Simon I., Google Brain Team, Mountain View, CA, United States; Raffel C., Google Brain Team, Mountain View, CA, United States; Engel J., Google Brain Team, Mountain View, CA, United States; Oore S., Google Brain Team, Mountain View, CA, United States; Eck D., Google Brain Team, Mountain View, CA, United States","We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions. © Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, Douglas Eck."
Bogdanov D.; Serra X.,Quantifying music trends and facts using editorial metadata from the discogs database,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069834588&partnerID=40&md5=5e4a4c81d1a6b99d9a619349ebc94524,"Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","While a vast amount of editorial metadata is being actively gathered and used by music collectors and enthusiasts, it is often neglected by music information retrieval and musicology researchers. In this paper we propose to explore Discogs, one of the largest databases of such data available in the public domain. Our main goal is to show how largescale analysis of its editorial metadata can raise questions and serve as a tool for musicological research on a number of example studies. The metadata that we use describes music releases, such as albums or EPs. It includes information about artists, tracks and their durations, genre and style, format (such as vinyl, CD, or digital files), year and country of each release. Using this data we study correlations between different genre and style labels, assess their specificity and analyze typical track durations. We estimate trends in prevalence of different genres, styles, and formats across different time periods. In our analysis of styles we use electronic music as an example. Our contribution also includes the tools we developed for our analysis and the generated datasets that can be re-used by MIR researchers and musicologists. © 2019 Dmitry Bogdanov, Xavier Serra."
Gururani S.; Lerch A.,Automatic sample detection in polyphonic music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069859408&partnerID=40&md5=00178da32a1564171d91b210377ea5c3,"Gururani S., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","The term 'sampling' refers to the usage of snippets or loops from existing songs or sample libraries in new songs, mashups, or other music productions. The ability to automatically detect sampling in music is, for instance, beneficial for studies tracking artist influences geographically and temporally. We present a method based on Non-negative Matrix Factorization (NMF) and Dynamic Time Warping (DTW) for the automatic detection of a sample in a pool of songs. The method comprises of two processing steps: first, the DTW alignment path between NMF activations of a song and query sample is computed. Second, features are extracted from this path and used to train a Random Forest classifier to detect the presence of the sample. The method is able to identify samples that are pitch shifted and/or time stretched with approximately 63% F-measure. We evaluate this method against a new publicly available dataset of real-world sample and song pairs. © 2019 Siddharth Gururani, Alexander Lerch."
Hu X.; Choi K.; Hao Y.; Cunningham S.J.; Lee J.H.; Laplante A.; Bainbridge D.; Downie J.S.,Exploring the music library association mailing list: A text mining approach,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062741595&partnerID=40&md5=e8dd9bde3918517143d72517048cdd61,"Hu X., University of Hong Kong, Hong Kong, Hong Kong; Choi K., University of Illinois, United States; Hao Y., University of Illinois, United States; Cunningham S.J., University of Waikato, New Zealand; Lee J.H., Univeristy of Washington, United States; Laplante A., Université de Montréal, Canada; Bainbridge D., University of Waikato, New Zealand; Downie J.S., University of Illinois, United States","Music librarians and people pursuing music librarianship have exchanged emails via the Music Library Association Mailing List (MLA-L) for decades. The list archive is an invaluable resource to discover new insights on music information retrieval from the perspective of the music librarian community. This study analyzes a corpus of 53,648 emails posted on MLA-L from 2000 to 2016 by using text mining and quantitative analysis methods. In addition to descriptive analysis, main topics of discussions and their trends over the years are identified through topic modeling. We also compare messages that stimulated discussions to those that did not. Inspection of semantic topics reveals insights complementary to previous topic analyses of other Music Information Retrieval (MIR) related resources. © 2019 Xiao Hu, Kahyun Choi, Yun Hao, Sally Jo Cunningham, Jin Ha Lee, Audrey Laplante, David Bainbridge and J. Stephen Downie."
Schlüter J.; Lehner B.,Zero-mean convolutions for level-invariant singing voice detection,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057115155&partnerID=40&md5=29e8164278e76e4d1ea9fcd5cb1328c5,"Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Lehner B., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","State-of-the-art singing voice detectors are based on classifiers trained on annotated examples. As recently shown, such detectors have an important weakness: Since singing voice is correlated with sound level in training data, classifiers learn to become sensitive to input magnitude, and give different predictions for the same signal at different sound levels. Starting from a Convolutional Neural Network (CNN) trained on logarithmic-magnitude mel spectrogram excerpts, we eliminate this dependency by forcing each first-layer convolutional filter to be zero-mean – that is, to have its coefficients sum to zero. In contrast to four other methods – data augmentation, instance normalization, spectral delta features, and per-channel energy normalization (PCEN) – that we evaluated on a large-scale public dataset, zero-mean convolutions achieve perfect sound level invariance without any impact on prediction accuracy or computational requirements. We assume that zero-mean convolutions would be useful for other machine listening tasks requiring robustness to level changes. © Jan Schlüter, Bernhard Lehner."
Kinnaird K.M.,Examining musical meaning in similarity thresholds,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069826319&partnerID=40&md5=be00e1ca527451c69d6b4121726e64bb,"Kinnaird K.M., Brown University, United States","Many approaches to Music Information Retrieval tasks rely on correctly determining if two segments of a given musical recording are repeats of each other. Repetitions in recordings are rarely exact, and identifying the appropriate threshold for these pairwise decisions is crucial for tuning MIR algorithms. However, current approaches for determining and reporting this threshold parameter are devoid of contextual meaning and interpretations, which makes comparing previous results difficult and which requires access to specific datasets. This paper highlights weaknesses in current approaches to choosing similarity thresholds, provides a framework using the proportion of orthogonal musical change to tie thresholds back to feature spaces with the cosine dissimilarity measure, and introduces new research possibilities given a music-centered approach for selecting similarity thresholds. © 2019 Katherine M. Kinnaird."
Tsaptsinos A.,Lyrics-based music genre classification using a hierarchical attention network,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055006420&partnerID=40&md5=35e86b38936a8004a8160f22d88ace00,"Tsaptsinos A., ICME, Stanford University, United States","Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure-in which words combine to form lines, lines form segments, and segments form a complete song-we adapt a hierarchical attention network (HAN) to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres. © 2019 Alexandros Tsaptsinos."
Cazau D.; Wang Y.; Adam O.; Wang Q.; Nuel G.,Improving note segmentation in automatic piano music transcription systems with a two-state pitch-wise HMM method,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068963394&partnerID=40&md5=10988ae7d9f3c079217185f09258949f,"Cazau D., Lab-STICC, ENSTA-Bretagne, France; Wang Y., School of Information Science and Engineering, Southeast, China; Adam O., Institut d'Alembert, UPMC, France; Wang Q., School of Information Science and Engineering, Southeast, China; Nuel G., LPMA, UPMC, France","Many methods for automatic piano music transcription involve a multi-pitch estimation method that estimates an activity score for each pitch. A second processing step, called note segmentation, has to be performed for each pitch in order to identify the time intervals when the notes are played. In this study, a pitch-wise two-state on/off first-order Hidden Markov Model (HMM) is developed for note segmentation. A complete parametrization of the HMM sigmoid function is proposed, based on its original regression formulation, including a parameter α of slope smoothing and β of thresholding contrast. A comparative evaluation of different note segmentation strategies was performed, differentiated according to whether they use a fixed threshold, called ""Hard Thresholding"" (HT), or a HMM-based thresholding method, called ""Soft Thresholding"" (ST). This evaluation was done following MIREX standards and using the MAPS dataset. Also, different transcription and recording scenarios were tested using three units of the Audio Degradation toolbox. Results show that note segmentation through a HMM soft thresholding with a data-based optimization of the {α, β} parameter couple significantly enhances transcription performance. © 2019 Dorian Cazau, Yuancheng Wang, Olivier Adam, Qiao Wang, Grégory Nuel."
Basaran D.; Essid S.; Peeters G.,Main melody extraction with source-filter NMF and CRNN,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064135681&partnerID=40&md5=e2cf64d437a92902fded7f33a7ac83f9,"Basaran D., CNRS, Ircam Lab, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; Essid S., LTCI, Télécom ParisTech, Université Paris Saclay, Paris, France; Peeters G., CNRS, Ircam Lab, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France","Estimating the main melody of a polyphonic audio recording remains a challenging task. We approach the task from a classification perspective and adopt a convolutional recurrent neural network (CRNN) architecture that relies on a particular form of pretraining by source-filter nonnegative matrix factorisation (NMF). The source-filter NMF decomposition is chosen for its ability to capture the pitch and timbre content of the leading voice/instrument, providing a better initial pitch salience than standard time-frequency representations. Starting from such a musically motivated representation, we propose to further enhance the NMF-based salience representations with CNN layers, then to model the temporal structure by an RNN network and to estimate the dominant melody with a final classification layer. The results show that such a system achieves state-of-the-art performance on the MedleyDB dataset without any augmentation methods or large training sets. © Dogac Basaran, Slim Essid, Geoffroy Peeters."
Bittner R.M.; McFee B.; Salamon J.; Li P.; Bello J.P.,Deep salience representations for F0 estimation in polyphonic music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069924285&partnerID=40&md5=b305708df8196d6969b417b8c9f42431,"Bittner R.M., Music and Audio Research Laboratory, New York University, United States; McFee B., Music and Audio Research Laboratory, New York University, United States, Center for Data Science, New York University, United States; Salamon J., Music and Audio Research Laboratory, New York University, United States; Li P., Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Estimating fundamental frequencies in polyphonic music remains a notoriously difficult task in Music Information Retrieval. While other tasks, such as beat tracking and chord recognition have seen improvement with the application of deep learning models, little work has been done to apply deep learning methods to fundamental frequency related tasks including multi-f0 and melody tracking, primarily due to the scarce availability of labeled data. In this work, we describe a fully convolutional neural network for learning salience representations for estimating fundamental frequencies, trained using a large, semi-automatically generated f0 dataset. We demonstrate the effectiveness of our model for learning salience representations for both multi-f0 and melody tracking in polyphonic audio, and show that our models achieve state-of-the-art performance on several multi-f0 and melody datasets. We conclude with directions for future research. © 2019 Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, Juan P. Bello."
Heo H.; Kim H.J.; Kim W.S.; Lee K.,Cover song identification with metric learning using distance as a feature,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054236711&partnerID=40&md5=89c431190a732f2ade3b6e25b39b643a,"Heo H., Music and Audio Research Group, Seoul National University, South Korea; Kim H.J., Department of Computer Sciences, University of Wisconsin-Madison, United States; Kim W.S., Music and Audio Research Group, Seoul National University, South Korea; Lee K., Music and Audio Research Group, Seoul National University, South Korea","Most of cover song identification algorithms are based on the pairwise (dis)similarity between two songs which are represented by harmonic features such as chroma, and therefore the choice of a distance measure and a feature has a significant impact on performance. Furthermore, since the similarity measure is query-dependent, it cannot represent an absolute distance measure. In this paper, we present a novel approach to tackle the cover song identification problem from a new perspective. We first construct a set of core songs, and represent each song in a high-dimensional space where each dimension indicates the pairwise distance between the given song and the other in the pre-defined core set. There are several advantages to this. First, using a number of reference songs in the core set, we make the most of relative distances to many other songs. Second, as all songs are transformed into the same high-dimensional space, kernel methods and metric learning are exploited for distance computation. Third, our approach does not depend on the computation method for the pairwise distance, and thus can use any existing algorithms. Experimental results confirm that the proposed approach achieved a large performance gain compared to the state-of-the-art methods. © 2019 Hoon Heo, Hyunwoo J. Kim,Wan Soo Kim, Kyogu Lee."
Vigliensoni G.; Fujinaga I.,The music listening histories dataset,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066910167&partnerID=40&md5=1c498c50ef6bf9dbd25c551a875183c1,"Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","We introduce the Music Listening Histories Dataset (MLHD), a large-scale collection of music listening events assembled from more than 27 billion time-stamped logs extracted from Last.fm. The logs are organized in the form of listening histories per user, and have been conveniently preprocessed and cleaned. Attractive features of the MLHD are the self-declared metadata provided by users at the moment of registration whose identities have been anonymized, MusicBrainz identifiers for the music entities in each of the logs that allows for an easy linkage to other existing resources, and a set of user profiling features designed to describe aspects of their music listening behavior and activity. We describe the process of assembling the dataset, its content, its demographic characteristics, and discuss about the possible uses of this collection, which, currently, is the largest research dataset of this kind in the field. © 2019 Gabriel Vigliensoni and Ichiro Fujinaga."
Tsushima H.; Nakamura E.; Itoyama K.; Yoshii K.,Function- And rhythm-aware melody harmonization based on tree-structured parsing and split-merge sampling of chord sequences,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051998287&partnerID=40&md5=6dbad753bb5804715e1798ddf06eeb33,"Tsushima H., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan, RIKEN AIP, Japan","This paper presents an automatic harmonization method that, for a given melody (sequence of musical notes), generates a sequence of chord symbols in the style of existing data. A typical way is to use hidden Markov models (HMMs) that represent chord transitions on a regular grid (e.g., bar or beat grid). This approach, however, cannot explicitly describe the rhythms, harmonic functions (e.g., tonic, dominant, and subdominant), and the hierarchical structure of chords, which are supposedly important in traditional harmony theories. To solve this, we formulate a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chords incorporating their syntactic functions, (2) a metrical Markov model describing chord rhythms, and (3) a Markov model generating melodies conditionally on a chord sequence. To estimate a variable-length chord sequence for a given melody, we iteratively refine the latent tree structure and the chord symbols and rhythms using a Metropolis-Hastings sampler with split-merge operations. Experimental results show that the proposed method outperformed the HMM-based method in terms of predictive abilities. © 2019 Hiroaki Tsushima, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii."
Liang F.; Gotham M.; Johnson M.; Shotton J.,Automatic stylistic composition of bach chorales with deep LSTM,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055992374&partnerID=40&md5=eebbfebf781bab11de68bc83996ad11f,"Liang F., Department of Engineering, University of Cambridge, United Kingdom; Gotham M., Faculty of Music, University of Cambridge, United Kingdom; Johnson M., Microsoft, United States; Shotton J., Microsoft, United States","This paper presents ""BachBot"": an end-to-end automatic composition system for composing and completing music in the style of Bach's chorales using a deep long short-term memory (LSTM) generative model. We propose a new sequential encoding scheme for polyphonic music and a model for both composition and harmonization which can be efficiently sampled without expensive Markov Chain Monte Carlo (MCMC). Analysis of the trained model provides evidence of neurons specializing without prior knowledge or explicit supervision to detect common music-theoretic concepts such as tonics, chords, and cadences. To assess BachBot's success, we conducted one of the largest musical discrimination tests on 2336 participants. Among the results, the proportion of responses correctly differentiating BachBot from Bach was only 1% better than random guessing. © 2019 Feynman Liang, Mark Gotham, Matthew Johnson, Jamie Shotton."
Van Der Wel E.; Ullrich K.,Optical Music Recognition with Convolutional Sequence-to-Sequence models,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052817526&partnerID=40&md5=9168ca43222c17b6e48ee8d548bb3f9c,"Van Der Wel E., University of Amsterdam, Netherlands; Ullrich K., University of Amsterdam, Netherlands","Optical Music Recognition (OMR) is an important technology within Music Information Retrieval. Deep learning models show promising results on OMR tasks, but symbol-level annotated data sets of sufficient size to train such models are not available and difficult to develop. We present a deep learning architecture called a Convolutional Sequence-to-Sequence model to both move towards an end-to-end trainable OMR pipeline, and apply a learning process that trains on full sentences of sheet music instead of individually labeled symbols. The model is trained and evaluated on a human generated data set, with various image augmentations based on real-world scenarios. This data set is the first publicly available set in OMR research with sufficient size to train and evaluate deep learning models. With the introduced augmentations a pitch recognition accuracy of 81% and a duration accuracy of 94% is achieved, resulting in a note level accuracy of 80%. Finally, the model is compared to commercially available methods, showing a large improvements over these applications. © 2019 Eelco van derWel, Karen Ullrich."
Pacha A.; Calvo-Zaragoza J.,Optical music recognition in mensural notation with region-based convolutional neural networks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062446867&partnerID=40&md5=873254c405ad63072fad568054685977,"Pacha A., Institute of Visual Computing and Human-Centered Technology, TU Wien, Austria; Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Spain","In this work, we present an approach for the task of optical music recognition (OMR) using deep neural networks. Our intention is to simultaneously detect and categorize musical symbols in handwritten scores, written in mensural notation. We propose the use of region-based convolutional neural networks, which are trained in an end-to-end fashion for that purpose. Additionally, we make use of a convolutional neural network that predicts the relative position of a detected symbol within the staff, so that we cover the entire image-processing part of the OMR pipeline. This strategy is evaluated over a set of 60 ancient scores in mensural notation, with more than 15000 annotated symbols belonging to 32 different classes. The results reflect the feasibility and capability of this approach, with a weighted mean average precision of around 76% for symbol detection, and over 98% accuracy for predicting the position. © Alexander Pacha, Jorge Calvo-Zaragoza."
Choi K.; Fazekas G.; Sandler M.; Cho K.,Transfer learning for music classification and regression tasks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069930310&partnerID=40&md5=63fc69c5a839b0e691d27bd198f46419,"Choi K., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Cho K., Center for Data Science, New York University, New York, NY, United States","In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features. © 2019 Keunwoo Choi, György Fazekas, Mark Sandler, Kyunghyun Cho."
Tsukuda K.; Ishida K.; Goto M.,Lyric Jumper: A lyrics-based music exploratory web service by modeling lyrics generative process,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069930276&partnerID=40&md5=c099849e1a743e2cb5f851754fd1e2d5,"Tsukuda K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Ishida K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Each artist has their own taste for topics of lyrics such as ""love"" and ""friendship."" Considering such artist's taste brings new applications in music information retrieval: choosing an artist based on topics of lyrics and finding un- familiar artists who have similar taste to a favorite artist. Although previous studies applied latent Dirichlet allocation (LDA) to lyrics to analyze topics, LDA was not able to capture the artist's taste. In this paper, we propose a topic model that can deal with the artist's taste for topics of lyrics. Our model assumes each artist has a topic distribution and a topic is assigned to each song according to the distribution. Our experimental results using a real- world dataset show that our model outperforms LDA in terms of the perplexity. By applying our model to estimate topics of 147,990 lyrics by 3,722 artists, we implement a web service called Lyric Jumper that enables users to explore lyrics based on the estimated topics. Lyric Jumper provides functions such as artist's topic taste visualization and topic-similarity-based artist recommendation. We also analyze operation logs obtained from 12,353 users on Lyric Jumper and show the usefulness of Lyric Jumper especially in recommending topic-related phrases in lyrics. © 2019 Kosetsu Tsukuda, Keisuke Ishida, Masataka Goto."
Louboutin C.; Bimbot F.,Modeling the multiscale structure of chord sequences using polytopic graphs,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069899562&partnerID=40&md5=c2ade27a8802210f4c2ac31977d49de2,"Louboutin C., Université Rennes 1, IRISA, France; Bimbot F., CNRS - UMR 6074, IRISA, France","Chord sequences are an essential source of information in a number of MIR tasks. However, beyond the sequential nature of musical content, relations and dependencies within a music segment can be more efficiently modeled as a graph. Polytopic Graphs have been recently introduced to model music structure so as to account for multiscale relationships between events located at metrically homologous instants. In this paper, we focus on the description of chord sequences and we study a specific set of graph configurations, called Primer Preserving Permutations (PPP). For sequences of 16 chords, PPPs account for 6 different latent systems of relations, corresponding to 6 main structural patterns (Prototypical Carrier Sequences or PCS). Observed chord sequences can be viewed as distorted versions of these PCS and the corresponding optimal PPP is estimated by minimizing a description cost over the latent relations. After presenting the main concepts of this approach, the article provides a detailed study of PPPs across a corpus of 727 chord sequences annotated from the RWC POP database (100 pop songs). Our results illustrate both qualitatively and quantitatively the potential of the proposed model for capturing long-term multiscale structure in musical data, which remains a challenge in computational music modeling and in Music Information Retrieval. © 2019 Corentin Louboutin, Frédéric Bimbot."
Jansson A.; Humphrey E.; Montecchio N.; Bittner R.; Kumar A.; Weyde T.,Singing voice separation with deep U-Net convolutional networks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069911577&partnerID=40&md5=38cca5edb51e737d961412edb9db91c6,"Jansson A., City, University of London, United Kingdom, Spotify, Sweden; Humphrey E., Spotify, Sweden; Montecchio N., Spotify, Sweden; Bittner R., Spotify, Sweden; Kumar A., Spotify, Sweden; Weyde T., City, University of London, United Kingdom","The decomposition of a music audio signal into its vocal and backing track components is analogous to image-toimage translation, where a mixed spectrogram is transformed into its constituent sources. We propose a novel application of the U-Net architecture - initially developed for medical imaging - for the task of source separation, given its proven capacity for recreating the fine, low-level detail required for high-quality audio reproduction. Through both quantitative evaluation and subjective assessment, experiments demonstrate that the proposed algorithm achieves state-of-the-art performance. © 2019 Andreas Jansson, Eric Humphrey, Nicola Montecchio, Rachel Bittner, Aparna Kumar, Tillman Weyde."
Dzhambazov G.; Holzapfel A.; Srinivasamurthy A.; Serra X.,Metrical-accent aware vocal onset detection in polyphonic audio,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069903194&partnerID=40&md5=1fc63dddad78f535bc6bd4e77aa249b9,"Dzhambazov G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Media Technology and Interaction Design, KTH Royal Institute of Technology, Stockholm, Sweden; Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The goal of this study is the automatic detection of onsets of the singing voice in polyphonic audio recordings. Starting with a hypothesis that the knowledge of the current position in a metrical cycle (i.e. metrical accent) can improve the accuracy of vocal note onset detection, we propose a novel probabilistic model to jointly track beats and vocal note onsets. The proposed model extends a state of the art model for beat and meter tracking, in which a-priori probability of a note at a specific metrical accent interacts with the probability of observing a vocal note onset. We carry out an evaluation on a varied collection of multi-instrument datasets from two music traditions (English popular music and Turkish makam) with different types of metrical cycles and singing styles. Results confirm that the proposed model reasonably improves vocal note onset detection accuracy compared to a baseline model that does not take metrical position into account. © 2019 Georgi Dzhambazov, Andre Holzapfel, Ajay Srinivasamurthy, Xavier Serra."
Peperkamp J.; Hildebrandt K.; Liem C.C.S.,A formalization of relative local tempo variations in collections of performances,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936515&partnerID=40&md5=05d15da2630f810a1f8e5d6fe83cd40a,"Peperkamp J., Delft University of Technology, Delft, Netherlands; Hildebrandt K., Delft University of Technology, Delft, Netherlands; Liem C.C.S., Delft University of Technology, Delft, Netherlands","Multiple performances of the same piece share similarities, but also show relevant dissimilarities. With regard to the latter, analyzing and quantifying variations in collections of performances is useful to understand how a musical piece is typically performed, how naturally sounding new interpretations could be rendered, or what is peculiar about a particular performance. However, as there is no formal ground truth as to what these variations should look like, it is a challenge to provide and validate analysis methods for this. In this paper, we focus on relative local tempo variations in collections of performances. We propose a way to formally represent relative local tempo variations, as encoded in warping paths of aligned performances, in a vector space. This enables using statistics for analyzing tempo variations in collections of performances. We elaborate the computation and interpretation of the mean variation and the principal modes of variation. To validate our analysis method despite the absence of a ground truth, we present results on artificially generated data, representing several categories of local tempo variations. Finally, we show how our method can be used for analyzing to realworld data and discuss potential applications. © 2019 Jeroen Peperkamp, Klaus Hildebrandt, Cynthia C. S."
Zhang S.; Repetto R.C.; Serra X.,Understanding the expressive functions of jingju metrical patterns through lyrics text mining,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056652704&partnerID=40&md5=d37f7468ec710a04cab8f60afc903f02,"Zhang S., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","The emotional content of jingju (aka Beijing or Peking opera) arias is conveyed through pre-defined metrical patterns known as banshi, each of them associated with a specific expressive function. In this paper, we first report the work on a comprehensive corpus of jingju lyrics that we built, suitable for text mining and text analysis in a data-driven framework. Utilizing this corpus, we propose a novel approach to study the expressive functions of banshi by applying text analysis techniques on lyrics. First we apply topic modeling techniques to jingju lyrics text documents grouped at different levels according to the banshi they are associated with. We then experiment with several different document vector representations of lyrics in a series of document classification experiments. The topic modeling results showed that sentiment polarity (positive or negative) is better distinguished between different shengqiang-banshi (a more fine grained partition of banshi) than banshi alone, and we are able to achieve high accuracy scores in classifying lyrics documents into different banshi categories. We discuss the technical and musicological implications and possible future improvements. © 2019 Shuo Zhang, Rafael Caro Repetto, Xavier Serra."
Mishra S.; Sturm B.L.; Dixon S.,Local interpretable model-agnostic explanations for music content analysis,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063089874&partnerID=40&md5=de590301375d4acbd16a48a5aaed2c98,"Mishra S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sturm B.L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","The interpretability of a machine learning model is essential for gaining insight into model behaviour. While some machine learning models (e.g., decision trees) are transparent, the majority of models used today are still black-boxes. Recent work in machine learning aims to analyse these models by explaining the basis of their decisions. In this work, we extend one such technique, called local interpretable model-agnostic explanations, to music content analysis. We propose three versions of explanations: one version is based on temporal segmentation, and the other two are based on frequency and time-frequency segmentation. These explanations provide meaningful ways to understand the factors that influence the classification of specific input data. We apply our proposed methods to three singing voice detection systems: the first two are designed using decision tree and random forest classifiers, respectively; the third system is based on convolutional neural network. The explanations we generate provide insights into the model behaviour. We use these insights to demonstrate that despite achieving 71.4% classification accuracy, the decision tree model fails to generalise. We also demonstrate that the model-agnostic explanations for the neural network model agree in many cases with the model-dependent saliency maps. The experimental code and results are available online.1 © 2019 Saumitra Mishra, Bob L. Sturm, Simon Dixon."
Huang C.-Z.A.; Cooijmans T.; Roberts A.; Courville A.; Eck D.,Counterpoint by convolution,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055966041&partnerID=40&md5=15ee7ac379aaf084211bb8b3dc1cfa96,"Huang C.-Z.A., MILA, Université de Montréal, Canada; Cooijmans T., MILA, Université de Montréal, Canada; Roberts A., Google Brain; Courville A., MILA, Université de Montréal, Canada; Eck D., Google Brain","Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition. Our model is an instance of orderless NADE [36], which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample quality, which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from [40] yields better samples than ancestral sampling, based on both log-likelihood and human evaluation. © 2019 Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck."
Humphrey E.J.; Montecchio N.; Bittner R.; Jansson A.; Jehan T.,Mining labeled data from web-scale collections for vocal activity detection in music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058518447&partnerID=40&md5=dc5971a63012a034d5105acd9cfc0552,"Humphrey E.J., Spotify, New York, NY, United States; Montecchio N., Spotify, New York, NY, United States; Bittner R., Spotify, New York, NY, United States, Music, Audio and Research Lab (MARL), New York University, United States; Jansson A., Spotify, New York, NY, United States, City University, London, United Kingdom; Jehan T., Spotify, New York, NY, United States","This work demonstrates an approach to generating strongly labeled data for vocal activity detection by pairing instrumental versions of songs with their original mixes. Though such pairs are rare, we find ample instances in a massive music collection for training deep convolutional networks at this task, achieving state of the art performance with a fraction of the human effort required previously. Our error analysis reveals two notable insights: imperfect systems may exhibit better temporal precision than human annotators, and should be used to accelerate annotation; and, machine learning from mined data can reveal subtle biases in the data source, leading to a better understanding of the problem itself. We also discuss future directions for the design and evolution of benchmarking datasets to rigorously evaluate AI systems. © 2019 Eric J. Humphrey, Nicola Montecchio, Rachel Bittner, Andreas Jansson, Tristan Jehan."
Zalkow F.; Weiß C.; Müller M.,Exploring tonal-dramatic relationships in richard Wagner's ring cycle,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069920600&partnerID=40&md5=032a612d32cbdca5adad559351fba115,"Zalkow F., International Audio Laboratories Erlangen, Germany; Weiß C., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Richard Wagner's cycle Der Ring des Nibelungen, consisting of four music dramas, constitutes a comprehensive work of high importance forWestern music history. In this paper, we indicate how MIR methods can be applied to explore this large-scale work with respect to tonal properties. Our investigations are based on a data set that contains 16 audio recordings of the entire Ring as well as extensive annotations including measure positions, singer activities, and leitmotif regions. As a basis for the tonal analysis, we make use of common audio features, which capture local chord and scale information. Employing a crossversion approach, we show that global histogram representations can reflect certain tonal relationships in a robust way. Based on our annotations, a musicologist may easily select and compare passages associated with dramatic aspects, for example, the appearance of specific characters or the presence of particular leitmotifs. Highlighting and investigating such passages may provide insights into the role of tonality for the dramatic conception of Wagner's Ring. By giving various concrete examples, we indicate how our approach may open up new ways for exploring large musical corpora in an intuitive and interactive way. © 2019 Frank Zalkow, Christof Weiß, Meinard Müller."
McFee B.; Bello J.P.,Structured training for large-vocabulary chord recognition,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053714526&partnerID=40&md5=90af5c2dc10aecb4914dc23913b94745,"McFee B., Center for Data Science, New York University, United States, Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Automatic chord recognition systems operating in the large-vocabulary regime must overcome data scarcity: certain classes occur much less frequently than others, and this presents a significant challenge when estimating model parameters. While most systems model the chord recognition task as a (multi-class) classification problem, few attempts have been made to directly exploit the intrinsic structural similarities between chord classes. In this work, we develop a deep convolutional-recurrent model for automatic chord recognition over a vocabulary of 170 classes. To exploit structural relationships between chord classes, the model is trained to produce both the time-varying chord label sequence as well as binary encodings of chord roots and qualities. This binary encoding directly exposes similarities between related classes, allowing the model to learn a more coherent representation of simultaneous pitch content. Evaluations on a corpus of 1217 annotated recordings demonstrate substantial improvements compared to previous models. © 2019 Brian McFee, Juan Pablo Bello."
Schreiber H.; Müller M.,A post-processing procedure for improving music tempo estimates using supervised learning,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069870428&partnerID=40&md5=9f82a4b963797f8de12fd08fec12a2bf,"Schreiber H., Tagtraum Industries Incorporated; Müller M., International Audio Laboratories Erlangen, Germany","Tempo estimation is a fundamental problem in music information retrieval and has been researched extensively. One problem still unsolved is the tendency of tempo estimation algorithms to produce results that are wrong by a small number of known factors (so-called octave errors). We propose a method that uses supervised learning to predict such tempo estimation errors. In a post-processing step, these predictions can then be used to correct an algorithm's tempo estimates. While being simple and relying only on a small number of features, our proposed method significantly increases accuracy for state-of-the-art tempo estimation methods. © 2019 Hendrik Schreiber, Meinard Müller."
Shi Z.; Arul K.; Smith J.O.,Modeling and digitizing reproducing piano rolls,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069821605&partnerID=40&md5=3a139b13ae7aecb4dcff6d998f782f05,"Shi Z., CCRMA, Stanford University, United States; Arul K., Department of Music, Stanford University, United States; Smith J.O., CCRMA, Stanford University, United States","Reproducing piano rolls are among the early music storage mediums, preserving fine details of a piano or organ performance on a continuous roll of paper with holes punched onto them. While early acoustic recordings suffer from poor quality sound, reproducing piano rolls benefit from the fidelity of a live piano for playback, and capture all features of a performance in what amounts to an early digital data format. However, due to limited availability of well maintained playback instruments and the condition of fragile paper, rolls have remained elusive and generally inaccessible for study. This paper proposes methods for modeling and digitizing reproducing piano rolls. Starting with an optical scan, we convert the raw image data into the MIDI file format by applying histogram-based image processing and building computational models of the musical expressions encoded on the rolls. Our evaluations show that MIDI emulations from our computational models are accurate on note level and proximate the musical expressions when compared with original playback recordings. © 2019 Zhengshan Shi, Kumaran Arul, Julius O. Smith."
Oramas S.; Nieto O.; Barbieri F.; Serra X.,"Multi-label music genre classification from audio, text, and images using deep features",2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069917282&partnerID=40&md5=6a745cc023df02a48901b1f2befd0319,"Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Nieto O., Pandora Media Inc., United States; Barbieri F., TALN Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Music genres allow to categorize musical items that share common characteristics. Although these categories are not mutually exclusive, most related research is traditionally focused on classifying tracks into a single class. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for certain applications. In this work we aim to expand this task by categorizing musical items into multiple and fine-grained labels, using three different data modalities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes. For every album we have collected the cover image, text reviews, and audio tracks. Additionally, we propose an approach for multi-label genre classification based on the combination of feature embeddings learned with state-of-the-art deep learning methodologies. Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields improved results. © 2019 Sergio Oramas, Oriol Nieto, Francesco Barbieri, Xavier Serra."
Humphrey E.J.; Durand S.; McFee B.,OpenMIC-2018: An open dataset for multiple instrument recognition,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056621572&partnerID=40&md5=662a690a001f004f12487c05710a7d1e,"Humphrey E.J., Spotify, Sweden; Durand S., Spotify, Sweden; McFee B., New York University, United States","Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music information retrieval. While there has been significant progress in developing predictive models for this and related classification tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset contains 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the dataset was sampled and annotated, and compare its characteristics to similar, previous data-sets. Finally, we present experimental results and baseline model performance to motivate future work. © Eric J. Humphrey, Simon Durand, Brian McFee."
Syue J.-L.; Su L.; Lin Y.-J.; Li P.-C.; Lu Y.-K.; Wang Y.-L.; Su A.W.Y.,Accurate audio-to-score alignment for expressive violin recordings,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069915547&partnerID=40&md5=3ece1309ceca3c84960011487932c2b4,"Syue J.-L., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Su L., Music and Culture Technology Lab., IIS, Academia Sinica, Taiwan; Lin Y.-J., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Li P.-C., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Lu Y.-K., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Wang Y.-L., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Su A.W.Y., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan","An audio-to-score alignment system adaptive to various playing styles and techniques, and also with high accuracy for onset/offset annotation is the key step toward advanced research on automatic music expression analysis. Technical barriers include the processing of overlapped notes, repeated note sequences, and silence. Most of these characteristics vary with expressions. In this paper, the audio-toscore alignment problem of expressive violin performance is addressed. We propose a two-stage alignment system composed of the dynamic time warping (DTW) algorithm, simulation of overlapped sustain notes, background noise model, silence detection, and refinement process, to better capture the onset. More importantly, we utilize the nonnegative matrix factorization (NMF) method for synthesis of the reference signal in order to deal with highly diverse timbre in real-world performance. A dataset of annotated expressive violin recordings in which each piece is played with various expressive musical terms is used. The optimal choice of basic parameters considered in conventional alignment systems, such as features, distance functions in DTW, synthesis methods for the reference signal, and energy ratios, is analyzed. Different settings on different expressions are compared and discussed. Results show that the proposed methods notably improve the conventional DTW-based alignment method. © 2019 Jia-Ling Syue, Li Su, Yi-Ju Lin, Pei-Ching Li, Yen-Kuang Lu, Yu-Lin Wang, Alvin W. Y. Su."
Bittner R.M.; Gu M.; Hernandez G.; Humphrey E.J.; Jehan T.; McCurry P.H.; Montecchio N.,Automatic playlist sequencing and transitions,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069859516&partnerID=40&md5=f8aeda058978dce63d9362aef4bceefc,"Bittner R.M., Spotify Inc., United States; Gu M., Spotify Inc., United States; Hernandez G., Spotify Inc., United States; Humphrey E.J., Spotify Inc., United States; Jehan T., Spotify Inc., United States; McCurry P.H., Spotify Inc., United States; Montecchio N., Spotify Inc., United States","Professional music curators and DJs artfully arrange and mix recordings together to create engaging, seamless, and cohesive listening experiences, a craft enjoyed by audiences around the world. The average listener, however, lacks both the time and the skill necessary to create comparable experiences, despite access to same source material. As a result, user-generated listening sessions often lack the sophistication popularized by modern artists, e.g. tracks are played in their entirety with little or no thought given to their ordering. To these ends, this paper presents methods for automatically sequencing existing playlists and adding DJ-style crossfade transitions between tracks: the former is modeled as a graph traversal problem, and the latter as an optimization problem. Our approach is motivated by an analysis of listener data on a large music catalog, and subjectively evaluated by professional curators. © 2019 Rachel M. Bittner, Minwei Gu, Gandalf Hernandez, Eric J. Humphrey."
Ens J.; Riecke B.E.; Pasquier P.,The significance of the low complexity dimension in music similarity judgements,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069942343&partnerID=40&md5=6f2a5922cdb00b2413c4a9296af08620,"Ens J., Simon Fraser University, Canada; Riecke B.E., Simon Fraser University, Canada; Pasquier P., Simon Fraser University, Canada","Previous research has demonstrated that similarity judgements are context specific, as they are shaped by cultural exposure, familiarity, and the musical aesthetic of the content being compared. Although such research suggests that the criterion for similarity judgement varies with respect to the musical style of the content being compared, the specific musical factors which shape this criterion are unknown. Since dimensional complexity differentiates musical genres, and has been shown to affect similarity judgements following lifelong exposure, this experiment investigates the short-term influence of dimensional complexity on similarity judgements. Rhythmic and pitch sequences with two levels of complexity were factorially combined to create four distinct types of prototype melodies. 51 participants rated the similarity of each type of prototype melody (M) to two variations, one in which the pitch content was modified (Mp), and another in which the rhythmic content was modified (Mr). The results indicate that rhythm and pitch complexity both play a significant role, influencing the perceived similarity of Mp, and Mr. The dimension bearing low complexity information was found to be the predominant factor in similarity judgements, as participants found modifications to this dimension to significantly decrease perceived similarity. © 2019 Jeff Ens, Bernhard E. Riecke, Philippe Pasquier."
Deng J.; Kwok Y.-K.,Large vocabulary automatic chord estimation with an even chance training scheme,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057936249&partnerID=40&md5=e25576f9b0458ad1fbc4ceb538c76990,"Deng J., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, Hong Kong; Kwok Y.-K., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, Hong Kong","This paper presents a large vocabulary automatic chord estimation system implemented using a bidirectional long short-term memory recurrent neural network trained with a skewed-class-aware scheme. This scheme gives the uncommon chord types much more exposure during the training process. The evaluation results indicate that: compared with a normal training scheme, the proposed scheme can boost the weighted chord symbol recalls of some uncommon chords and significantly improve the average chord quality accuracy, at the expense of the overall weighted chord symbol recall. © 2019 Junqi Deng and Yu-Kwong Kwok."
Pons J.; Gong R.; Serra X.,Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054982924&partnerID=40&md5=9ea1d5a2be43efd7ce78670134968982,"Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gong R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper introduces a new score-informed method for the segmentation of jingju a cappella singing phrase into syllables. The proposed method estimates the most likely sequence of syllable boundaries given the estimated syllable onset detection function (ODF) and its score. Throughout the paper, we first examine the jingju syllables structure and propose a definition of the term ""syllable onset"". Then, we identify which are the challenges that jingju a cappella singing poses. Further, we investigate how to improve the syllable ODF estimation with convolutional neural networks (CNNs). We propose a novel CNN architecture that allows to efficiently capture different timefrequency scales for estimating syllable onsets. Besides, we propose using a score-informed Viterbi algorithm - instead of thresholding the onset function-, because the available musical knowledge we have (the score) can be used to inform the Viterbi algorithm to overcome the identified challenges. The proposed method outperforms the state-of-the-art in syllable segmentation for jingju a cappella singing. We further provide an analysis of the segmentation errors which points possible research directions. © 2019 Jordi Pons, Rong Gong and Xavier Serra."
Viraraghavan V.S.; Aravind R.; Murthy H.A.,A statistical analysis of gamakas in Carnatic music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064663524&partnerID=40&md5=696515bdd523802e790dd9c79f6f4dfb,"Viraraghavan V.S., TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India, Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Aravind R., Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Murthy H.A., Department of Computer Science and Engineering, Indian Institute of Technology, Madras, India","Carnatic Music, a form of classical music prevalent in South India, has a central concept called rāgas, defined as melodic scales and/or a set of characteristic melodic phrases. These definitions also account for the continuous pitch movement in gamakas and micro-tonal adjustments to pitch values. In this paper, we present several statistics of gamakas to arrive at a model of Carnatic music. We draw upon the two-component model of Carnatic Music, which splits it into a slowly varying 'stage' and a detail, called 'dance'. Based on the statistics, we propose slightly altered definitions of two similar components called constant-pitch notes and transients. An automated implementation of these definitions is used in collecting statistics from 84 concert renditions. We then suggest that the constant-pitch notes and transients can be considered as context and detail respectively of the rāga, but add that both are necessary for defining the rāga. This is verified by performing listening tests on only the constant-pitch notes and transients independently. © 2019 Venkata Subramanian Viraraghavan, R Aravind, Hema A Murthy."
Yang L.-C.; Chou S.-Y.; Yang Y.-H.,Midinet: A convolutional generative adversarial network for symbolic-domain music generation,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069920583&partnerID=40&md5=5df3b16fa43783161ba76ee75d1796da,"Yang L.-C., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan; Chou S.-Y., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan","Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting. © 2019 Li-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang."
Tsai T.J.; Tjoa S.K.; Müller M.,Make your own accompaniment: Adapting full-mix recordings to match solo-only user recordings,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069946795&partnerID=40&md5=e2b7d197bbe81d8de90d485dbd9c8f95,"Tsai T.J., Harvey Mudd College, Claremont, CA, United States; Tjoa S.K., Galvanize, Inc., San Francisco, CA, United States; Müller M., International Audio Laboratories Erlangen, Erlangen, Germany","We explore the task of generating an accompaniment track for a musician playing the solo part of a known piece. Unlike previous work in real-time accompaniment, we focus on generating the accompaniment track in an off-line fashion by adapting a full-mix recording (e.g. a professional CD recording or Youtube video) to match the user's tempo preferences. The input to the system is a set of recorded passages of a solo part played by the user (e.g. solo part in a violin concerto). These recordings are contiguous segments of music where the soloist part is active. Based on this input, the system identifies the corresponding passages within a full-mix recording of the same piece (i.e. contains both solo and accompaniment parts), and these passages are temporally warped to run synchronously to the soloonly recordings. The warped passages can serve as accompaniment tracks for the user to play along with at a tempo that matches his or her ability or desired interpretation. As the main technical contribution, we introduce a segmental dynamic time warping algorithm that simultaneously solves both the passage identification and alignment problems. We demonstrate the effectiveness of the proposed system on a pilot data set for classical violin. © 2019 TJ Tsai, Steven K. Tjoa, Meinard Müller."
Ranjani H.G.; Paramashivan D.; Sreenivas T.V.,Quantized melodic contours in indian art music perception: Application to transcription,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061030421&partnerID=40&md5=64f3ed6c637296b9329e3301d3b139ac,"Ranjani H.G., Dept of ECE, Indian Institute of Science, Bangalore, India; Paramashivan D., Dept of Music, University of Alberta, Canada; Sreenivas T.V., Dept of ECE, Indian Institute of Science, Bangalore, India","Rāgas in Indian Art Music have a florid dynamism associated with them. Owing to their inherent structural intricacies, the endeavor of mapping melodic contours to musical notation becomes cumbersome. We explore the potential of mapping, through quantization of melodic contours and listening test of synthesized music, to capture the nuances of rāgas. We address both Hindustani and Carnatic music forms of Indian Art Music. Two quantization schemes are examined using stochastic models of melodic pitch. We attempt to quantify the salience of rāga perception from reconstructed melodic contours. Perception experiments verify that much of the rāga nuances inclusive of the gamaka (subtle ornamentation) structures can be retained by sampling and quantizing critical points of melodic contours. Further, we show application of this result to automatically transcribe melody of Indian Art Music. © 2019 Ranjani, H. G., Deepak Paramashivan, Thippur V. Sreenivas."
Schnell N.; Schwarz D.; Larralde J.; Borghesi R.,"PiPo, a plugin interface for afferent data stream processing modules",2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057034057&partnerID=40&md5=5160611dd6cab35216d14fc8efaf06aa,"Schnell N., UMR STMS, IRCAM-CNRS-UPMC, France; Schwarz D., UMR STMS, IRCAM-CNRS-UPMC, France; Larralde J., UMR STMS, IRCAM-CNRS-UPMC, France; Borghesi R., UMR STMS, IRCAM-CNRS-UPMC, France","We present PiPo, a plugin API for data stream processing with applications in interactive audio processing and music information retrieval as well as potentially other domains of signal processing. The development of the API has been motivated by our recurrent need to use a set of signal processing modules that extract low-level descriptors from audio and motion data streams in the context of different authoring environments and end-user applications. The API is designed to facilitate both, the development of modules and the integration of modules or module graphs into applications. It formalizes the processing of streams of multidimensional data frames which may represent regularly sampled signals as well as time-tagged events or numeric annotations. As we found it sufficient for the processing of incoming (i.e. afferent) data streams, PiPo modules have a single input and output and can be connected to sequential and parallel processing paths. After laying out the context and motivations, we present the concept and implementation of the PiPo API with a set of modules that allow for extracting low-level descriptors from audio streams. In addition, we describe the integration of the API into host environments such as Max, Juce, and OpenFrameworks. © 2019 Norbert Schnell, Diemo Schwarz, Joseph Larralde, Riccardo Borghesi."
Pauwels J.; O'Hanlon K.; Fazekas G.; Sandler M.B.,Confidence measures and their applications in music labelling systems based on hidden markov models,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060938609&partnerID=40&md5=ce677d3c7aed8f3da2c82933140e2017,"Pauwels J., Centre for Digital Music, Queen Mary University of London, United Kingdom; O'Hanlon K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","Inspired by previous work on confidence measures for tempo estimation in loops, we explore ways to add confidence measures to other music labelling tasks. We start by reflecting on the reasons why the work on loops was successful and argue that it is an example of the ideal scenario in which it is possible to define a confidence measure independently of the estimation algorithm. This requires additional domain knowledge not used by the estimation algorithm, which is rarely available. Therefore we move our focus to defining confidence measures for hidden Markov models, a technique used in multiple music information retrieval systems and beyond. We propose two measures that are oblivious to the specific labelling task, trading off performance for computational requirements. They are experimentally validated by means of a chord estimation task. Finally, we have a look at alternative uses of confidence measures, besides those applications that require a high precision rather than a high recall, such as most query retrievals. © 2019 Johan Pauwels, Ken O'Hanlon, György Fazekas, Mark B. Sandler."
Salamon J.; Bittner R.M.; Bonada J.; Bosch J.J.; Gómez E.; Bello J.P.,An analysis/synthesis framework for automatic f0 annotation of multitrack datasets,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054290573&partnerID=40&md5=dda0f1252a3f1530a84f32afd85b8c56,"Salamon J., Music and Audio Research Laboratory, New York University, United States; Bittner R.M., Music and Audio Research Laboratory, New York University, United States; Bonada J., Music Technology Group, Universitat Pompeu Fabra, Spain; Bosch J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Generating continuous f0 annotations for tasks such as melody extraction and multiple f0 estimation typically involves running a monophonic pitch tracker on each track of a multitrack recording and manually correcting any estimation errors. This process is labor intensive and time consuming, and consequently existing annotated datasets are very limited in size. In this paper we propose a framework for automatically generating continuous f0 annotations without requiring manual refinement: the estimate of a pitch tracker is used to drive an analysis/synthesis pipeline which produces a synthesized version of the track. Any estimation errors are now reflected in the synthesized audio, meaning the tracker's output represents an accurate annotation. Analysis is performed using a wide-band harmonic sinusoidal modeling algorithm which estimates the frequency, amplitude and phase of every harmonic, meaning the synthesized track closely resembles the original in terms of timbre and dynamics. Finally the synthesized track is automatically mixed back into the multitrack. The framework can be used to annotate multitrack datasets for training learning-based algorithms. Furthermore, we show that algorithms evaluated on the automatically generated/annotated mixes produce results that are statistically indistinguishable from those they produce on the original, manually annotated, mixes. We release a software library implementing the proposed framework, along with new datasets for melody, bass and multiple f0 estimation. © 2019 Justin Salamon, Rachel M. Bittner, Jordi Bonada, Juan J. Bosch, Emilia Gómez."
Losorelli S.; Nguyen D.T.; Dmochowski J.P.; Kaneshiro B.,NMED-T: A tempo-focused dataset of cortical and behavioral responses to naturalistic music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059990722&partnerID=40&md5=3e0c9adba1ddd1da48c1069bfbc565a5,"Losorelli S., Center for the Study of Language and Information, Stanford University, United States, Center for Computer Research in Music and Acoustics, Stanford University, United States; Nguyen D.T., Center for the Study of Language and Information, Stanford University, United States, Center for Computer Research in Music and Acoustics, Stanford University, United States; Dmochowski J.P., Department of Biomedical Engineering, City College of New York, United States; Kaneshiro B., Center for the Study of Language and Information, Stanford University, United States, Center for Computer Research in Music and Acoustics, Stanford University, United States","Understanding human perception of music is foundational to many research topics in Music Information Retrieval (MIR). While the field of MIR has shown a rising interest in the study of brain responses, access to data remains an obstacle. Here we introduce the Naturalistic Music EEG Dataset-Tempo (NMED-T), an open dataset of electrophysiological and behavioral responses collected from 20 participants who heard a set of 10 commercially available musical works. Song stimuli span various genres and tempos, and all contain electronically produced beats in duple meter. Preprocessed and aggregated responses include dense-array EEG and sensorimotor synchronization (tapping) responses, behavioral ratings of the songs, and basic demographic information. These data, along with illustrative analysis code, are published in Matlab format. Raw EEG and tapping data are also made available. In this paper we describe the construction of the dataset, present results from illustrative analyses, and document the format and attributes of the published data. This dataset facilitates reproducible research in neuroscience and cognitive MIR, and points to several possible avenues for future studies on human processing of naturalistic music. © 2019 Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, and Blair Kaneshiro."
Arzt A.; Widmer G.,Piece identification in classical piano music without reference scores,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892150&partnerID=40&md5=85022d5c99f0d0b8a9508851115708f0,"Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In this paper we describe an approach to identify the name of a piece of piano music, based on a short audio excerpt of a performance. Given only a description of the pieces in text format (i.e. no score information is provided), a reference database is automatically compiled by acquiring a number of audio representations (performances of the pieces) from internet sources. These are transcribed, preprocessed, and used to build a reference database via a robust symbolic fingerprinting algorithm, which in turn is used to identify new, incoming queries. The main challenge is the amount of noise that is introduced into the identification process by the music transcription algorithm and the automatic (but possibly suboptimal) choice of performances to represent a piece in the reference database. In a number of experiments we show how to improve the identification performance by increasing redundancy in the reference database and by using a preprocessing step to rate the reference performances regarding their suitability as a representation of the pieces in question. As the results show this approach leads to a robust system that is able to identify piano music with high accuracy - without any need for data annotation or manual data preparation. © 2019 Andreas Arzt, Gerhard Widmer."
Narang K.; Rao P.,Acoustic features for determining goodness of tabla strokes,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055008895&partnerID=40&md5=dd81871d4a946df6a1cd25eadc8647a7,"Narang K., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","The tabla is an essential component of the Hindustani classical music ensemble and therefore a popular choice with musical instrument learners. Early lessons typically target the mastering of individual strokes from the inventory of bols (spoken syllables corresponding to the distinct strokes) via training in the required articulatory gestures on the right and left drums. Exploiting the close links between the articulation, acoustics and perception of tabla strokes, this paper presents a study of the different timbral qualities that correspond to the correct articulation and to identified common misarticulations of the different bols. We present a dataset created out of correctly articulated and distinct categories of misarticulated strokes, all perceptually verified by an expert. We obtain a system that automatically labels a recording as a good or bad sound, and additionally identifies the precise nature of the misarticulation with a view to providing corrective feedback to the player. We find that acoustic features that are sensitive to the relatively small deviations from the good sound due to poorly articulated strokes are not necessarily the features that have proved successful in the recognition of strokes corresponding to distinct tabla bols as required for music transcription. © 2019 Krish Narang and Preeti Rao."
Van Kranenburg P.; Maessen G.,Comparing offertory melodies of five medieval christian chant traditions,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069935360&partnerID=40&md5=9084ed2e4ccb6c2428048b125c40b4e6,"Van Kranenburg P., Utrecht University, Meertens Institute, Netherlands; Maessen G.","In this study, we compare the melodies of five medieval chant traditions: Gregorian, Old Roman, Milanese, Beneventan, and Mozarabic. We present a newly created dataset containing several hundreds of offertory melodies, which are the longest and most complex within the total body of chant melodies. For each tradition, we train n-gram language models on a representation of the chants as sequence of chromatic intervals. By computing perplexities of the melodies, we get an indication of the relations between the traditions, revealing the melodies of the Gregorian tradition as most diverse. Next, we perform a classification experiment using global features of the melodies. The choice of features is informed by expert knowledge. We use properties of the intervallic content of the melodies, and properties of the melismas, revealing that significant differences exist between the traditions. For example, the Gregorian melodies contain less step-wise intervals compared to the other repertoires. Finally, we train a classifier on the perplexities as computed with the n-gram models, resulting in a very reliable classifier. © 2019 Peter van Kranenburg, Geert Maessen."
Dorfer M.; Arzt A.; Widmer G.,Learning audio - Sheet music correspondences for score identification and offline alignment,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056619422&partnerID=40&md5=9ed5fe5a5b04cf24255aca3e53f1cdf2,"Dorfer M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria","This work addresses the problem of matching short excerpts of audio with their respective counterparts in sheet music images. We show how to employ neural networkbased cross-modality embedding spaces for solving the following two sheet music-related tasks: retrieving the correct piece of sheet music from a database when given a music audio as a search query; and aligning an audio recording of a piece with the corresponding images of sheet music. We demonstrate the feasibility of this in experiments on classical piano music by five different composers (Bach, Haydn, Mozart, Beethoven and Chopin), and additionally provide a discussion on why we expect multi-modal neural networks to be a fruitful paradigm for dealing with sheet music and audio at the same time. © 2019 Matthias Dorfer, Andreas Arzt, Gerhard Widmer."
Wilkins J.; Seetharaman P.; Wahl A.; Pardo B.,Vocalset: A singing voice dataset,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059773103&partnerID=40&md5=cb8373b6c8893782076b2d15a5da7ddf,"Wilkins J., Computer Science, Northwestern University, Evanston, IL, United States, School of Music, Northwestern University, Evanston, IL, United States; Seetharaman P., Computer Science, Northwestern University, Evanston, IL, United States; Wahl A., School of Music, Northwestern University, Evanston, IL, United States, School of Music, Ithaca College, Ithaca, NY, United States; Pardo B., Computer Science, Northwestern University, Evanston, IL, United States","We present VocalSet, a singing voice dataset of a capella singing. Existing singing voice datasets either do not capture a large range of vocal techniques, have very few singers, or are single-pitch and devoid of musical context. VocalSet captures not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts. VocalSet has recordings of 10.1 hours of 20 professional singers (11 male, 9 female) performing 17 different different vocal techniques. This data will facilitate the development of new machine learning models for singer identification, vocal technique identification, singing generation and other related applications. To illustrate this, we establish baseline results on vocal technique classification and singer identification by training convolutional network classifiers on VocalSet to perform these tasks. © Julia Wilkins, Prem Seetharaman, Alison Wahl, Bryan Pardo."
Fonseca E.; Pons J.; Favory X.; Font F.; Bogdanov D.; Ferraro A.; Oramas S.; Porter A.; Serra X.,Freesound datasets: A platform for the creation of open audio datasets,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059741914&partnerID=40&md5=8e1d8b0655e4094c812c10221e2b28f0,"Fonseca E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Favory X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Font F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Ferraro A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Oramas S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Porter A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Openly available datasets are a key factor in the advancement of data-driven research approaches, including many of the ones used in sound and music computing. In the last few years, quite a number of new audio datasets have been made available but there are still major shortcomings in many of them to have a significant research impact. Among the common shortcomings are the lack of transparency in their creation and the difficulty of making them completely open and sharable. They often do not include clear mechanisms to amend errors and many times they are not large enough for current machine learning needs. This paper introduces Freesound Datasets, an online platform for the collaborative creation of open audio datasets based on principles of transparency, openness, dynamic character, and sustainability. As a proof-of-concept, we present an early snapshot of a large-scale audio dataset built using this platform. It consists of audio samples from Freesound organised in a hierarchy based on the AudioSet Ontology. We believe that building and maintaining datasets following the outlined principles and using open tools and collaborative approaches like the ones presented here will have a significant impact in our research community. © 2019 Eduardo Fonseca, Jordi Pons, Xavier Favory, Frederic Font, Dmitry Bogdanov, Andres Ferraro, Sergio Oramas, Alastair Porter, Xavier Serra."
Gupta C.; Grunberg D.; Rao P.; Wang Y.,Towards automatic mispronunciation detection in singing,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054967342&partnerID=40&md5=74ae712ec1f5028fd0c4c2b9c149e773,"Gupta C., School of Computing, National University of Singapore, Singapore, Singapore, NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore, Singapore; Grunberg D., School of Computing, National University of Singapore, Singapore, Singapore; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore","A tool for automatic pronunciation evaluation of singing is desirable for those learning a second language. However, efforts to obtain pronunciation rules for such a tool have been hindered by a lack of data; while many spokenword datasets exist that can be used in developing the tool, there are relatively few sung-lyrics datasets for such a purpose. In this paper, we demonstrate a proof-of-principle for automatic pronunciation evaluation in singing using a knowledge-based approach with limited data in an automatic speech recognition (ASR) framework. To demonstrate our approach, we derive mispronunciation rules specific to South-East Asian English accents in singing based on a comparative study of the pronunciation error patterns in singing versus speech. Using training data restricted to American English speech, we evaluate different methods involving the deduced L1-specific (native language) rules for singing. In the absence of L1 phone models, we incorporate the derived pronunciation variations in the ASR framework via a novel approach that combines acoustic models for sub-phonetic segments to represent the missing L1 phones. The word-level assessment achieved by the system on singing and speech is similar, indicating that it is a promising scheme for realizing a full-fledged pronunciation evaluation system for singing in future. © 2019 Chitralekha Gupta, David Grunberg, Preeti Rao, Ye Wang."
Li B.; Dinesh K.; Sharma G.; Duan Z.,Video-based vibrato detection and analysis for polyphonic string music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059783148&partnerID=40&md5=71c777de0a17aae33b61bbf82eae610d,"Li B., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States; Dinesh K., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States; Sharma G., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States; Duan Z., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States","In music performance, vibrato is an important artistic effect, where slight variations in pitch are introduced to add expressiveness and warmth. Automatic vibrato detection and analysis, although well studied for monophonic music, has rarely been explored for polyphonic music, because of the challenge in multi-pitch analysis. We propose a video-based approach for detecting and analyzing vibrato in polyphonic string music. Specifically, we capture the fine motion of the left hand of string players through optical flow analysis of video frames. We explore two methods. The first uses a feature extraction and SVM classification pipeline, and the second is an unsupervised technique based on autocorrelation analysis of the principal motion component. The proposed methods are compared with audio-only methods applied to individual instrument tracks separated from original audio mixture using the score. Experiments show that the proposed video-based methods achieve a significantly higher vibrato detection accuracy than the audio-based methods especially in high polyphony cases. Further experiments also demonstrate the utility of the approach in vibrato rate and extent analysis. © 2019 Bochen Li, Karthik Dinesh, Gaurav Sharma, Zhiyao Duan."
Maezawa A.,Fast and accurate: Improving a simple beat tracker with a selectively-applied deep beat identification,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068999934&partnerID=40&md5=800d39b6d3160ae17f5ed628f18e1240,"Maezawa A., Yamaha Corporation, Japan","In music applications, audio beat tracking is a central component that requires both speed and accuracy, but a fast beat tracker typically has many beat phase errors, while an accurate one typically requires more computation. This paper achieves a fast tracking speed and a low beat phase error by applying a slow but accurate beat phase detector at only the most informative spots in a given song, and interpolating the rest by a fast tatum-level tracker. We present (1) a framework for selecting a small subset of the tatum indices that information-theoretically best describes the beat phases of the song, (2) a fast HMM-based beat tracker for tatum tracking, and (3) an accurate but slow beat detector using a deep neural network (DNN). The evaluations demonstrate that the proposed DNN beat phase detection halves the beat phase error of the HMM-based tracker and enables a 98% decrease in the required number of DNN invocations without dropping the accuracy. © 2019 Akira Maezawa."
Tuggener L.; Elezi I.; Schmidhuber J.; Stadelmann T.,Deep watershed detector for music object recognition,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053595487&partnerID=40&md5=93bb00b1a0e946d0fa2f213b1132c173,"Tuggener L., ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland, Faculty of Informatics, Università della Svizzera Italiana, Lugano, Switzerland; Elezi I., ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland, Dept. of Environmental Sciences, Informatics and Statistics, Ca’Foscari University of Venice, Italy; Schmidhuber J., Faculty of Informatics, Università della Svizzera Italiana, Lugano, Switzerland; Stadelmann T., ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland","Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD’s ability to work with synthetic scores equally well as with handwritten music. © Lukas Tuggener, Ismail Elezi, Jürgen Schmidhuber, Thilo Stadelmann."
Parada-Cabaleiro E.; Batliner A.; Baird A.; Schuller B.W.,The SEILS dataset: Symbolically encoded scores in modern-early notation for computational musicology,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069876193&partnerID=40&md5=f75a55e317e04b29e7605884022b82f8,"Parada-Cabaleiro E., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany; Batliner A., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany; Baird A., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany; Schuller B.W., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany, GLAM - Group on Language, Audio and Music, Imperial College London, United Kingdom","The automatic analysis of notated Renaissance music is restricted by a shortfall in codified repertoire. Thousands of scores have been digitised by music libraries across the world, but the absence of symbolically codified information makes these inaccessible for computational evaluation. Optical Music Recognition (OMR) made great progress in addressing this issue, however, early notation is still an on-going challenge for OMR. To this end, we present the Symbolically Encoded Il Lauro Secco (SEILS) dataset, a new dataset of codified scores for use within computational musicology. We focus on a collection of Italian madrigals from the 16th century, a polyphonic secular a cappella composition characterised by strong musical-linguistic synergies. Thirty madrigals for five unaccompanied voices are presented in modern and early notation, considering a variety of digital formats: Lilypond, Music XML, MIDI, and Finale (a total of 150 symbolically codified scores). Given the musical and poetic value of the chosen repertoire, we aim to promote synergies between computational musicology and linguistics. © 2019 Emilia Parada-Cabaleiro, Anton Batliner, Alice Baird, Björn W. Schuller."
Smith J.B.L.; Goto M.,Multi-part pattern analysis: Combining structure analysis and source separation to discover intra-part repeated sequences,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892661&partnerID=40&md5=ba40552dfbb04060b97c4ba83fba5ab0,"Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Structure is usually estimated as a single-level phenomenon with full-texture repeats and homogeneous sections. However, structure is actually multi-dimensional: in a typical piece of music, individual instrument parts can repeat themselves in independent ways, and sections can be homogeneous with respect to several parts or only one part. We propose a novel MIR task, multi-part pattern analysis, that requires the discovery of repeated patterns within instrument parts. To discover repeated patterns in individual voices, we propose an algorithm that applies source separation and then tailors the structure analysis to each estimated source, using a novel technique to resolve transitivity errors. Creating ground truth for this task by hand would be infeasible for a large corpus, so we generate a synthetic corpus from MIDI files. We synthesize audio and produce measure-by-measure descriptions of which instruments are active and which repeat themselves exactly. Lastly, we present a set of appropriate evaluation metrics, and use them to compare our approach to a set of baselines. © 2019 Jordan B. L. Smith, Masataka Goto."
Medeot G.; Cherla S.; Kosta K.; McVicar M.; Abdallah S.; Selvi M.; Newton-Rex E.; Webster K.,StructureNet: Inducing structure in generated melodies,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064131047&partnerID=40&md5=344b6d04b7d994cb24e239e13d6a1308,"Medeot G., Jukedeck Ltd., London, United Kingdom; Cherla S., Jukedeck Ltd., London, United Kingdom; Kosta K., Jukedeck Ltd., London, United Kingdom; McVicar M., Jukedeck Ltd., London, United Kingdom; Abdallah S., Jukedeck Ltd., London, United Kingdom; Selvi M., Jukedeck Ltd., London, United Kingdom; Newton-Rex E., Jukedeck Ltd., London, United Kingdom; Webster K., Imperial College London, London, United Kingdom","We present the StructureNet - a recurrent neural network for inducing structure in machine-generated compositions. This model resides in a musical structure space and works in tandem with a probabilistic music generation model as a modifying agent. It favourably biases the probabilities of those notes that result in the occurrence of structural elements it has learnt from a dataset. It is extremely flexible in that it is able to work with any such probabilistic model, it works well when training data is limited, and the types of structure it can be made to induce are highly customisable. We demonstrate through our experiments on a subset of the Nottingham dataset that melodies generated by a recurrent neural network based melody model are indeed more structured in the presence of the StructureNet. © McVicar, Samer Abdallah, Marco Selvi, Ed Newton-Rex, Kevin Webster. McVicar, Samer Abdallah, Marco Selvi, Ed Newton-Rex, Kevin Webster."
Suzuki J.; Kitahara T.,A music player with song selection function for a group of people,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069937952&partnerID=40&md5=8b6c9b2f15e00c0557287125e6ce9ca6,"Suzuki J., College of Humanities and Sciences, Nihon University, Tokyo, 156-8550, Japan; Kitahara T., College of Humanities and Sciences, Nihon University, Tokyo, 156-8550, Japan","There are often situations in which a group of people gather and listen to the same songs. However, majority of existing studies related to music information retrieval (MIR) have focused on personalization for individual users, and there have been only a few studies related to MIR intended for a group of people. Here, we present an Android music player with a music selection function for people who are listening to the same songs in the same place. We assume that each user owns his/her favorite songs on his/her Android device. Once a group of users gathers each user can launch this player on his/her smart- phone. Then, the player running on each device starts to communicate with other devices via Bluetooth. Information about songs stored in every device, along with the playback history, is collected to a device referred to as the master device. Then, the master device estimates each user's preference for every song based on playback history and music similarity. The master device then extracts songs that are highly preferred and sends a command to start playback to the devices storing these songs. Our experimental results demonstrate the successful estimation of music preferences based on music similarity. © 2019 Jun'ichi Suzuki and Tetsuro Kitahara."
Nishikimi R.; Nakamura E.; Goto M.; Itoyama K.; Yoshii K.,Scale- And rhythm-aware musical note estimation for vocal F0 trajectories based on a semi-tatum-synchronous hierarchical hidden semi-Markov model,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063509726&partnerID=40&md5=7da047090fed1b9eb692a6107f7bba13,"Nishikimi R., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan, RIKEN AIP, Japan","This paper presents a statistical method that estimates a sequence of musical notes from a vocal F0 trajectory. Since the onset times and F0s of sung notes are considerably deviated from the discrete tatums and pitches indicated in a musical score, a score model is crucial for improving timefrequency quantization of the F0s. We thus propose a hierarchical hidden semi-Markov model (HHSMM) that combines a score model representing the rhythms and pitches of musical notes with musical scales with an F0 model representing time-frequency deviations from a note sequence specified by a score. In the score model, musical scales are generated stochastically. Note pitches are then generated according to the scales and note onsets are generated according to a Markov process defined on the tatum grid. In the F0 model, onset deviations, smooth note-to-note F0 transitions, and F0 deviations are generated stochastically and added to the note sequence. Given an F0 trajectory, our method estimates the most likely sequence of musical notes while giving more importance on the score model than the F0 model. Experimental results showed that the proposed method outperformed an HMM-based method having no models of scales and rhythms. © 2019 Ryo Nishikimi, Eita Nakamura, Masataka Goto, Kat-sutoshi Itoyama, Kazuyoshi Yoshii."
Smith J.B.L.; Chew E.,Automatic interpretation of music structure analyses: A validated technique for post-hoc estimation of the rationale for an annotation,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069907682&partnerID=40&md5=912dbf23bd2d27b7aa193b65feaa9577,"Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Chew E., Queen Mary University of London, United Kingdom","Annotations of musical structure usually provide a low level of detail: they include boundary locations and section labels, but do not indicate what makes the sections similar or distinct, or what changes in the music at each boundary. For those studying annotated corpora, it would be useful to know the rationale for each annotation, but collecting this information from listeners is burdensome and difficult. We propose a new algorithm for estimating which musical features formed the basis for each part of an annotation. To evaluate our approach, we use a synthetic dataset of music clips, all designed to have ambiguous structure, that was previously used and validated in a psychology experiment. We find that, compared to a previous optimization-based algorithm, our correlation-based approach is better able to predict the rationale for an analysis. Using the best version of our algorithm, we process examples from the SALAMI dataset and demonstrate how we can augment the structure annotation data with estimated rationales, inviting new ways to research and use the data. © 2019 Jordan B. L. Smith, Elaine Chew."
Hung Y.-N.; Yang Y.-H.,Frame-level instrument recognition by timbre and pitch,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062241227&partnerID=40&md5=79e566e865fe4b81becd5d2567e550d2,"Hung Y.-N., Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan","Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at https://biboamy.github.io/instrument-recognition/. © Yun-Ning Hung and Yi-Hsuan Yang."
Ren I.Y.; Koops H.V.; Volk A.; Swierstra W.,In search of the consensus among musical pattern discovery algorithms,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069839829&partnerID=40&md5=d730d8c0d494e13fc0f9307eaa007281,"Ren I.Y., University Utrecht, Netherlands; Koops H.V., University Utrecht, Netherlands; Volk A., University Utrecht, Netherlands; Swierstra W., University Utrecht, Netherlands","Patterns are an essential part of music and there are many different algorithms that aim to discover them. Based on the improvements brought by using data fusion methods to find the consensus of algorithms on other MIR tasks, we hypothesize that fusing the output from musical pattern discovery algorithms will improve the pattern discovery results. In this paper, we explore two methods to combine the pattern output from ten state-of-the-art algorithms using two datasets. Both provide human-annotated patterns as ground truth. We show that finding the consensus among the output of different musical pattern discovery algorithms is challenging for two reasons: First, the number of patterns found by the algorithms exceeds patterns in human annotations by several orders of magnitude, with little agreement on what constitutes a pattern. Second, the algorithms perform inconsistently across different pieces. We show that algorithms lack a consensus with each other. Therefore, it is difficult to harness the collective wisdom of the algorithms to find ground truth patterns. The main contribution of this paper is a meta-analysis of the (dis)similarities among pattern discovery algorithms' output and using the output in two fusion methods. Furthermore, we discuss the implication of our results for the MIREX task. © 2019 Ris Yuping Ren, Hendrik Vincent Koops, Anja Volk, Wouter Swierstra."
Laplante A.; Bowman T.D.; Aamar N.,"""I'm at #osheaga!"": Listening to the backchannel of a music festival on twitter",2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055284501&partnerID=40&md5=a51abd74e4682ac675a8f3541e059507,"Laplante A., Université de Montréal, Canada; Bowman T.D., Wayne State University, United States; Aamar N., Université de Montréal, Canada","It has become common practice for audience members to use social media to connect, share, and communicate with each other during events (e.g., sport events, elections, award ceremonies). But how is this backchannel used during a musical event and what does it say about how people engage with the music and the artists performing it? In this paper, we present the result of a study of a dataset composed of 31,140 tweets posted during and around the 10th edition of Osheaga, an important music festival held annually in Montreal. A combination of statistics and qualitative content analysis is used to examine the postings. This allows us to describe the content of these postings (i.e., topics, shared media), the type of message being shared (i.e., opinion, expression, information), and who the authors of these tweets are. © 2019 Audrey Laplante, Timothy D. Bowman, Nawel Aammar."
Wang C.-I.; Mysore G.J.; Dubnov S.,Re-visiting the music segmentation problem with crowdsourcing,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068973259&partnerID=40&md5=6941d245f354ac3a1040d26888baec1d,"Wang C.-I., UCSD, United States; Mysore G.J., Adobe Research, United States; Dubnov S., UCSD, United States","Identifying boundaries in music structural segmentation is a well studied music information retrieval problem. The goal is to develop algorithms that automatically identify segmenting time points in music that closely matches human annotated data. The annotation itself is challenging due to its subjective nature, such as the degree of change that constitutes a boundary, the location of such boundaries, and whether a boundary should be assigned to a single time frame or a range of frames. Existing datasets have been annotated by small number of experts and the annotators tend to be constrained to specific definitions of segmentation boundaries. In this paper, we re-examine the annotation problem. We crowdsource the problem to a large number of annotators and present an analysis of the results. Our preliminary study suggests that although there is a correlation to existing datasets, this form of annotations reveals additional information such as stronger vs. weaker boundaries, gradual vs. sudden boundaries, and the difference in perception of boundaries between musicians and non-musicians. The study suggests that it could be worth re-defining certain aspects of the boundary identification in music structural segmentation problem with a broader definition. © 2019 Cheng-I Wang, Gautham J. Mysore, Shlomo Dubnov."
Meseguer-Brocal G.; Cohen-Hadria A.; Peeters G.,"DALI: A large dataset of synchronized audio, lyrics and notes, automatically created using teacher-student machine learning paradigm",2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064115409&partnerID=40&md5=a013152de6c6a5a93e8af19c843c97af,"Meseguer-Brocal G., Ircam Lab, CNRS, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; Cohen-Hadria A., Ircam Lab, CNRS, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; Peeters G., Ircam Lab, CNRS, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France","The goal of this paper is twofold. First, we introduce DALI, a large and rich multimodal dataset containing 5358 audio tracks with their time-aligned vocal melody notes and lyrics at four levels of granularity. The second goal is to explain our methodology where dataset creation and learning models interact using a teacher-student machine learning paradigm that benefits each other. We start with a set of manual annotations of draft time-aligned lyrics and notes made by non-expert users of Karaoke games. This set comes without audio. Therefore, we need to find the corresponding audio and adapt the annotations to it. To that end, we retrieve audio candidates from the Web. Each candidate is then turned into a singing-voice probability over time using a teacher, a deep convolutional neural network singing-voice detection system (SVD), trained on cleaned data. Comparing the time-aligned lyrics and the singing-voice probability, we detect matches and update the time-alignment lyrics accordingly. From this, we obtain new audio sets. They are then used to train new SVD students used to perform again the above comparison. The process could be repeated iteratively. We show that this allows to progressively improve the performances of our SVD and get better audio-matching and alignment. © Gabriel Meseguer-Brocal, Alice Cohen-Hadria, Geoffroy Peeters."
Nakamura E.; Yoshii K.; Katayose H.,Performance error detection and post-processing for fast and accurate symbolic music alignment,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054228461&partnerID=40&md5=60c66c609d3d4bd7912509aad4d40323,"Nakamura E., Kyoto University, Japan; Yoshii K., Kyoto University, RIKEN AIP, Japan; Katayose H., Kwansei Gakuin University, Japan","This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typically have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by performance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned signals, to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in local regions around performance errors. To remove the dependence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously proposed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results. © 2019 Eita Nakamura, Kazuyoshi Yoshii, Haruhiro Katayose."
Srinivasamurthy A.; Holzapfel A.; Serra X.,Informed automatic meter analysis of music recordings,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069915655&partnerID=40&md5=ece231d57c47d9c8e641ea1bf5fb3790,"Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Media Technology and Interaction Design Department, KTH Royal Institute of Technology, Stockholm, Sweden; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Automatic meter analysis aims to annotate a recording of a metered piece of music with its metrical structure. This analysis subsumes correct estimation of the type of meter, the tempo, and the alignment of the metrical structure with the music signal. Recently, Bayesian models have been successfully applied to several of meter analysis tasks, but depending on the musical context, meter analysis still poses significant challenges. In this paper, we investigate if there are benefits to automatic meter analysis from additional a priori information about the metrical structure of music. We explore informed automatic meter analysis, in which varying levels of prior information about the metrical structure of the music piece is available to analysis algorithms. We formulate different informed meter analysis tasks and discuss their practical applications, with a focus on Indian art music. We then adapt state of the art Bayesian meter analysis methods to these tasks and evaluate them on corpora of Indian art music. The experiments show that the use of additional information aids meter analysis and improves automatic meter analysis performance, with significant gains for analysis of downbeats. © 2019 Ajay Srinivasamurthy, Andre Holzapfel, Xavier Serra."
Waloschek S.; Hadjakos A.,Driftin’ down the scale: Dynamic time warping in the presence of pitch drift and transpositions,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060935316&partnerID=40&md5=c530b58b4c75b4efe0a1342e90031d57,"Waloschek S., Center of Music and Film Informatics, Detmold University of Music, Germany; Hadjakos A., Center of Music and Film Informatics, Detmold University of Music, Germany","Recordings of a cappella music often exhibit significant pitch drift. This drift may accumulate over time to a total transposition of several semitones, which renders the canonical 2-dimensional Dynamic Time Warping (DTW) useless. We propose Transposition-Aware Dynamic Time Warping (TA-DTW), an approach that introduces a 3rd dimension to DTW. Steps in this dimension represent changes in transposition. Paired with suitable input features, TA-DTW computes an optimal alignment path between a symbolic score and a corresponding audio recording in the presence of pitch drift or arbitrary transpositions. © Simon Waloschek, Aristotelis Hadjakos."
Park S.; Kim T.; Lee K.; Kwak N.,Music source separation using stacked hourglass networks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062230883&partnerID=40&md5=d0c1e1d786a2e509b5a00999fd3fb15b,"Park S., Graduate School of Convergence Science and Technology, Seoul National University; Kim T., Graduate School of Convergence Science and Technology, Seoul National University; Lee K., Graduate School of Convergence Science and Technology, Seoul National University; Kwak N., Graduate School of Convergence Science and Technology, Seoul National University","In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks. © Sungheon Park, Taehoon Kim, Kyogu Lee, Nojun Kwak."
Ganguli K.K.; Rao P.,Towards computational modeling of the ungrammatical in a raga performance,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065210909&partnerID=40&md5=6d75df9c5217d063ddb8f0f090f00c91,"Ganguli K.K., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","Raga performance allows for considerable flexibility in interpretation of the raga grammar in order to incorporate elements of creativity via improvisation. It is therefore of much interest in pedagogy to understand what ungrammaticality might mean in the context of a given raga, and possibly develop means to detect this in an audio recording of the raga performance. One prominent notion is that ungrammaticality is considered to occur only when the performer ""treads"" on another, possibly allied, raga in a listener's perception. With this view, we consider modeling the technical boundary of a raga as that which separates it from another raga that is closest to it in its distinctive features. We wish to find computational models that can indicate ungrammaticality using a data-driven estimation of the model parameters; i.e. the raga performances of great artists are used to obtain representations that discriminate most between same and different raga performances. We choose a well-known pair of allied ragas (Deshkar and Bhupali in north Indian classical music) for an empirical study of computational representations for the distinctive attributes of tonal hierarchy and melodic shape of a chosen common descending phrase. © 2019 Kaustuv Kanti Ganguli and Preeti Rao."
Lu W.-T.; Su L.,Vocal melody extraction with semantic segmentation and audio-symbolic domain transfer learning,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063526017&partnerID=40&md5=682a10ca9e511cef1676f3ffcdecf263,"Lu W.-T., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","The melody extraction problem is analogue to semantic segmentation on a time-frequency image, in which every pixel on the image is classified as a part of a melody object or not. Such an approach can benefit from a signal processing method that helps to enhance the true pitch contours on an image, and, a music language model with structural information on large-scale symbolic music data to be transfer into an audio-based model. In this paper, we propose a novel melody extraction system, using a deep convolutional neural network (DCNN) with dilated convolution as the semantic segmentation tool. The candidate pitch contours on the time-frequency image are enhanced by combining the spectrogram and cepstral-based features. Moreover, an adaptive progressive neural network is employed to transfer the semantic segmentation model in the symbolic domain to the one in the audio domain. This paper makes an attempt to bridge the semantic gaps between signal-level features and perceived melodies, and between symbolic data and audio data. Experiments show competitive accuracy of the proposed method on various datasets. © Wei-Tsung Lu and Li Su."
Sebastian J.; Murthy H.A.,Onset detection in composition items of Carnatic music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069819332&partnerID=40&md5=0026186adf8b4d288a0374292fd5e6bf,"Sebastian J., Indian Institute of Technology, Madras, India; Murthy H.A., Indian Institute of Technology, Madras, India","Complex rhythmic patterns associated with Carnatic music are revealed from the stroke locations of percussion instruments. However, a comprehensive approach for the detection of these locations from composition items is lacking. This is a challenging problem since the melodic sounds (typically vocal and violin) generate soft-onset locations which result in a number of false alarms. In this work, a separation-driven onset detection approach is proposed. Percussive separation is performed using a Deep Recurrent Neural Network (DRNN) in the first stage. A single model is used to separate the percussive vs the non-percussive sounds using discriminative training and time-frequency masking. This is then followed by an onset detection stage based on group delay (GD) processing on the separated percussive track. The proposed approach is evaluated on a large dataset of live Carnatic music concert recordings and compared against percussive separation and onset detection baselines. The separation performance is significantly better than that of Harmonic- Percussive Separation (HPS) algorithm and onset detection performance is better than the state-of-the-art Convolutional Neural Network (CNN) based algorithm. The proposed approach has an absolute improvement of 18.4% compared with the detection algorithm applied directly on the composition items. © 2019 Jilt Sebastian, Hema A. Murthy."
Langhabel J.; Lieck R.; Toussaint M.; Rohrmeier M.,Feature discovery for sequential prediction of monophonic music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069848051&partnerID=40&md5=e1f9d6f70082ef76cab942bb5e12d561,"Langhabel J., Department of Computer Science, TU Berlin, Germany; Lieck R., Machine Learning and Robotics Lab, University of Stuttgart, Germany, Systematic Musicology and Music Cognition, TU Dresden, Germany; Toussaint M., Machine Learning and Robotics Lab, University of Stuttgart, Germany; Rohrmeier M., Systematic Musicology and Music Cognition, TU Dresden, Germany, Digital and Cognitive Musicology Lab, EPFL, Switzerland","Learning a model for sequential prediction of symbolic music remains an open challenge. An important special case is the prediction of pitch sequences based on a corpus of monophonic music. We contribute to this line of research in two respects: (1) Our models improve the stateof- the-art performance. (2) Our method affords learning interpretable models by discovering an explicit set of relevant features. We discover features using the PULSE learning framework, which repetitively suggests new candidate features using a generative operation and selects features while optimizing the underlying model. Defining a domain-specific generative operation allows to combine multiple music-theoretically motivated features in a unified model and to control their interaction on a fine-grained level. We evaluate our models on a set of benchmark corpora of monophonic chorales and folk songs, outperforming previous work. Finally, we discuss the characteristics of the discovered features from a musicological perspective, giving concrete examples. © 2019 Jonas Langhabel, Robert Lieck, Marc Toussaint, Martin Rohrmeier."
Tralie C.J.,Early MFCC and HPCP fusion for robust cover song identification,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058527360&partnerID=40&md5=f91d16fbc397dc58204cbb4242c8c5eb,"Tralie C.J., Duke University Department of Electrical and Computer Engineering, United States","While most schemes for automatic cover song identification have focused on note-based features such as HPCP and chord profiles, a few recent papers surprisingly showed that local self-similarities of MFCC-based features also have classification power for this task. Since MFCC and HPCP capture complementary information, we design an unsupervised algorithm that combines normalized, beatsynchronous blocks of these features using cross-similarity fusion before attempting to locally align a pair of songs. As an added bonus, our scheme naturally incorporates structural information in each song to fill in alignment gaps where both feature sets fail. We show a striking jump in performance over MFCC and HPCP alone, achieving a state of the art mean reciprocal rank of 0.87 on the Covers80 dataset. We also introduce a new medium-sized hand designed benchmark dataset called ""Covers 1000,"" which consists of 395 cliques of cover songs for a total of 1000 songs, and we show that our algorithm achieves an MRR of 0.9 on this dataset for the first correctly identified song in a clique. We provide the precomputed HPCP and MFCC features, as well as beat intervals, for all songs in the Covers 1000 dataset for use in further research. © 2019 Christopher J. Tralie."
Gkiokas A.; Katsouros V.,Convolutional neural networks for real-time beat tracking: A dancing robot application,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069811176&partnerID=40&md5=d6777e697f1645628d9924a4f04652cf,"Gkiokas A., Institute for Language and Speech Processing, Athena Research and Innovation Center, Athens, Greece; Katsouros V., Institute for Language and Speech Processing, Athena Research and Innovation Center, Athens, Greece","In this paper a novel approach that adopts Convolutional Neural Networks (CNN) for the Beat Tracking task is proposed. The proposed architecture involves 2 convolutional layers with the CNN filter dimensions corresponding to time and band frequencies, in order to learn a Beat Activation Function (BAF) from a time-frequency representation. The output of each convolutional layer is computed only over the past values of the previous layer, to enable the computation of the BAF in an online fashion. The output of the CNN is post-processed by a dynamic programming algorithm in combination with a bank of resonators for calculating the salient rhythmic periodicities. The proposed method has been designed to be computational efficient in order to be embedded on a dancing NAO robot application, where the dance moves of the choreography are synchronized with the beat tracking output. The proposed system was submitted to the Signal Processing Cup Challenge 2017 and ranked among the top third algorithms. © 2019 Aggelos Gkiokas and Vassilis Katsouros."
Dong H.-W.; Yang Y.-H.,Convolutional generative adversarial networks with binary neurons for polyphonic music generation,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062221767&partnerID=40&md5=93ca92bd23325fb4bb16ad53b747e79f,"Dong H.-W., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan","It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binary-valued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binary-valued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445.github.io/bmusegan/. © Hao-Wen Dong and Yi-Hsuan Yang."
Teng Y.; Zhao A.; Goudeseune C.,Generating nontrivial melodies for music as a service,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069906814&partnerID=40&md5=efb76f10627bc620d85e7e97c599944f,"Teng Y., U. of Illinois, Dept. of ECE, United States; Zhao A., U. of Illinois, Dept. of ECE, United States; Goudeseune C., U. of Illinois, Beckman Inst., United States","We present a hybrid neural network and rule-based system that generates pop music. Music produced by pure rule-based systems often sounds mechanical. Music produced by machine learning sounds better, but still lacks hierarchical temporal structure. We restore temporal hierarchy by augmenting machine learning with a temporal production grammar, which generates the music's overall structure and chord progressions. A compatible melody is then generated by a conditional variational recurrent autoencoder. The autoencoder is trained with eight-measure segments from a corpus of 10,000 MIDI files, each of which has had its melody track and chord progressions identified heuristically. The autoencoder maps melody into a multi-dimensional feature space, conditioned by the underlying chord progression. A melody is then generated by feeding a random sample from that space to the autoencoder's decoder, along with the chord progression generated by the grammar. The autoencoder can make musically plausible variations on an existing melody, suitable for recurring motifs. It can also reharmonize a melody to a new chord progression, keeping the rhythm and contour. The generated music compares favorably with that generated by other academic and commercial software designed for the music-as-a-service industry. © 2019 Yifei Teng, Anny Zhao, Camille Goudeseune."
Li S.; Dixon S.; Plumbley M.D.,Clustering expressive timing with regressed polynomial coefficients demonstrated by a model selection test,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056144567&partnerID=40&md5=cdc8e3fde30ded30671464e84955e369,"Li S., Beijing University of Posts and Telecommunications, China; Dixon S., Queen Mary University of London, United Kingdom; Plumbley M.D., University of Surrey, United Kingdom","Though many past works have tried to cluster expressive timing within a phrase, there have been few attempts to cluster features of expressive timing with constant dimensions regardless of phrase lengths. For example, used as a way to represent expressive timing, tempo curves can be regressed by a polynomial function such that the number of regressed polynomial coefficients remains constant with a given order regardless of phrase lengths. In this paper, clustering the regressed polynomial coefficients is proposed for expressive timing analysis. A model selection test is presented to compare Gaussian Mixture Models (GMMs) fitting regressed polynomial coefficients and fitting expressive timing directly. As there are no expected results of clustering expressive timing, the proposed method is demonstrated by how well the expressive timing are approximated by the centroids of GMMs. The results show that GMMs fitting the regressed polynomial coefficients outperform GMMs fitting expressive timing directly. This conclusion suggests that it is possible to use regressed polynomial coefficients to represent expressive timing within a phrase and cluster expressive timing within phrases of different lengths. © 2019 Shengchen Li, Simon Dixon, Mark D. Plumbley."
Crestel L.; Esling P.; Heng L.; McAdams S.,A database linking piano and orchestral midi scores with application to automatic projective orchestration,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936840&partnerID=40&md5=640a5e75390a5f0747977b3e324f1577,"Crestel L., Music Representations, IRCAM, Paris, France; Esling P., Music Representations, IRCAM, Paris, France; Heng L., Schulich School of Music, McGill University, Montréal, QC, Canada; McAdams S., Schulich School of Music, McGill University, Montréal, QC, Canada","This article introduces the Projective Orchestral Database (POD), a collection of MIDI scores composed of pairs linking piano scores to their corresponding orchestrations. To the best of our knowledge, this is the first database of its kind, which performs piano or orchestral prediction, but more importantly which tries to learn the correlations between piano and orchestral scores. Hence, we also introduce the projective orchestration task, which consists in learning how to perform the automatic orchestration of a piano score. We show how this task can be addressed using learning methods and also provide methodological guidelines in order to properly use this database. © 2019 Léopold Crestel, Philippe Esling, Lena Heng, Stephen McAdams."
Lim H.; Rhyu S.; Lee K.,Chord generation from symbolic melody using blstm networks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062227482&partnerID=40&md5=e8f1136a5e56f618044ffab8e4d23305,"Lim H., Music and Audio Research Group, Graduate School of Convergence Science and Technology, South Korea, Center for Super Intelligence, Seoul National University, South Korea; Rhyu S., Music and Audio Research Group, Graduate School of Convergence Science and Technology, South Korea; Lee K., Music and Audio Research Group, Graduate School of Convergence Science and Technology, South Korea, Center for Super Intelligence, Seoul National University, South Korea","Generating a chord progression from a monophonic melody is a challenging problem because a chord progression requires a series of layered notes played simultaneously. This paper presents a novel method of generating chord sequences from a symbolic melody using bidirectional long short-term memory (BLSTM) networks trained on a lead sheet database. To this end, a group of feature vectors composed of 12 semitones is extracted from the notes in each bar of monophonic melodies. In order to ensure that the data shares uniform key and duration characteristics, the key and the time signatures of the vectors are normalized. The BLSTM networks then learn from the data to incorporate the temporal dependencies to produce a chord progression. Both quantitative and qualitative evaluations are conducted by comparing the proposed method with the conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8% and 11.4% performance increase from the other models, respectively. User studies further confirm that the chord sequences generated by the proposed method are preferred by listeners. © 2019 Hyungui Lim, Seungyeon Ryu and Kyogu Lee."
Calvo-Zaragoza J.; Rizo D.,Camera-primus: Neural end-to-end optical music recognition on realistic monophonic scores,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052792251&partnerID=40&md5=961ee6797dd26984831c13c62328f514,"Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Spain; Rizo D., Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana (ISEA.CV), Universidad de Alicante, Spain","The optical music recognition (OMR) field studies how to automate the process of reading the musical notation present in a given image. Among its many uses, an interesting scenario is that in which a score captured with a camera is to be automatically reproduced. Recent approaches to OMR have shown that the use of deep neural networks allows important advances in the field. However, these approaches have been evaluated on images with ideal conditions, which do not correspond to the previous scenario. In this work, we evaluate the performance of an end-to-end approach that uses a deep convolutional recurrent neural network (CRNN) over non-ideal image conditions of music scores. Consequently, our contribution also consists of Camera-PrIMuS, a corpus of printed monophonic scores of real music synthetically modified to resemble camera-based realistic scenarios, involving distortions such as irregular lighting, rotations, or blurring. Our results confirm that the CRNN is able to successfully solve the task under these conditions, obtaining an error around 2% at music-symbol level, thereby representing a groundbreaking piece of research towards useful OMR systems. © Jorge Calvo-Zaragoza, David Rizo."
Fang J.; Grunberg D.; Litman D.; Wang Y.,Discourse analysis of lyric and lyric-based classification of music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064572335&partnerID=40&md5=347f757b17524cff597812105e807780,"Fang J., School of Computing, National University of Singapore, Singapore, Singapore; Grunberg D., School of Computing, National University of Singapore, Singapore, Singapore; Litman D., Department of Computer Science, University of Pittsburgh, United States; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore","Lyrics play an important role in the semantics and the structure of many pieces of music. However, while many existing lyric analysis systems consider each sentence of a given set of lyrics separately, lyrics are more naturally understood as multi-sentence units, where the relations between sentences is a key factor. Here we describe a series of experiments using discourse-based features, which describe the relations between different sentences within a set of lyrics, for several common Music Information Retrieval tasks. We first investigate genre recognition and present evidence that incorporating discourse features allow for more accurate genre classification than singlesentence lyric features do. Similarly, we examine the problem of release date estimation by passing features to classifiers to determine the release period of a particular song, and again determine that an assistance from discoursebased features allow for superior classification relative to single-sentence lyric features alone. These results suggest that discourse-based features are potentially useful for Music Information Retrieval tasks. © 2019 Jiakun Fang, David Grunberg, Diane Litman, Ye Wang."
Liu M.; Hu X.; Schedl M.,"Artist preferences and cultural, socio-economic distances across countries: A big data perspective",2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058481801&partnerID=40&md5=d07d14f90eb3591577dcb61f1c76af99,"Liu M., University of Hong Kong, Hong Kong, Hong Kong; Hu X., University of Hong Kong, Hong Kong, Hong Kong; Schedl M., Johannes Kepler University Linz, Austria","Users in different countries may have different music preferences, possibly due to geographical, economic, linguistic, and cultural factors. Revealing the relationship between music preference and cultural socio-economic differences across countries is of great importance for music information retrieval in a cross-country or cross-cultural context. Existing works are usually based on small samples in one or several countries or take only one or two socio-economic aspects into account. To bridge the gap, this study makes use of a large-scale music listening dataset, LFM-1b with more than one billion music listening logs, to explore possible associations between a variety of cultural and socio-economic measurements and artist preferences in 20 countries. From a big data perspective, the results reveal: 1) there is a highly uneven distribution of preferred artists across countries; 2) the linguistic differences among these countries are positively associated with the distances in artist preferences; 3) country differences in three of the six cultural dimensions considered in this study have positive influences on the difference of artist preferences among the countries; and 4) geographical and economic distances among the countries have no significant relationship with their artist preferences. © 2019 Meijun Liu, Xiao Hu, Markus Schedl."
Chen T.-P.; Su L.,Functional harmony recognition of symbolic music data with multi-task recurrent neural networks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064106041&partnerID=40&md5=4d0f25fa01bfdec99d55f509fdc23b96,"Chen T.-P., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","Previous works on chord recognition mainly focus on chord symbols but overlook other essential features that matter in musical harmony. To tackle the functional harmony recognition problem, we compile a new professionally annotated dataset of symbolic music encompassing not only chord symbols, but also various interrelated chord functions such as key modulation, chord inversion, secondary chords, and chord quality. We further present a novel holistic system in functional harmony recognition; a multi-task learning (MTL) architecture is implemented with the recurrent neural network (RNN) to jointly model chord functions in an end-to-end scenario. Experimental results highlight the capability of the proposed recognition system, and a promising improvement of the system by employing multi-task learning instead of single-task learning. This is one attempt to challenge the end-to-end chord recognition task from the perspective of functional harmony so as to uncover the grand structure ruling the flow of musical sound. The dataset and the source code of the proposed system is announced at https://github.com/Tsung-Ping/functional-harmony. © Tsung-Ping Chen and Li Su."
Ariga S.; Fukayama S.; Goto M.,Song2Guitar: A difficulty-aware arrangement system for generating guitar solo covers from polyphonic audio of popular music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069872079&partnerID=40&md5=87fb63b36810c00aa026a2e6e682f63f,"Ariga S., University of Tokyo, Japan; Fukayama S., AIST, Japan; Goto M., AIST, Japan","This paper describes Song2Guitar which automatically generates difficulty-aware guitar solo cover of popular music from its acoustic signals. Previous research has utilized hidden Markov models (HMMs) to generate playable guitar piece from music scores. Our Song2Guitar extends the framework by leveraging MIR technologies so that it can handle beats, chords and melodies extracted from polyphonic audio. Furthermore, since it is important to generate a guitar piece to meet the skill of a player, Song2Guitar generates guitar solo covers in consideration of playing difficulty. We conducted a data-driven investigation to find what factor makes a guitar piece difficult to play, and restricted Song2Guitar to use certain hand forms adaptively so that the player can play the piece without experiencing too much difficulty. The user interface of Song2Guitar is also implemented and is used to conduct user tests. The results indicated that Song2Guitar succeeded in generating guitar solo covers from polyphonic audio with various playing difficulties. © 2019 Shunya Ariga, Satoru Fukayama, Masataka Goto."
Brunner G.; Konrad A.; Wang Y.; Wattenhofer R.,MiDI-VAE: Modeling dynamics and instrumentation of music with applications to style transfer,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062228576&partnerID=40&md5=7a67171304a500b145090efddd3cb82f,"Brunner G., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland; Konrad A., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland; Wang Y., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland; Wattenhofer R., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland","We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions. © Gino Brunner, Andres Konrad, Yuyi Wang, Roger Wattenhofer."
Sears D.R.W.; Arzt A.; Frostel H.; Sonnleitner R.; Widmer G.,Modeling harmony with skip-grams,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069869482&partnerID=40&md5=ae9de89945ed6e185b9a9e6033c9a29b,"Sears D.R.W., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Frostel H., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Sonnleitner R., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","String-based (or viewpoint) models of tonal harmony often struggle with data sparsity in pattern discovery and prediction tasks, particularly when modeling composite events like triads and seventh chords, since the number of distinct n-note combinations in polyphonic textures is potentially enormous. To address this problem, this study examines the efficacy of skip-grams in music research, an alternative viewpoint method developed in corpus linguistics and natural language processing that includes sub-sequences of n events (or n-grams) in a frequency distribution if their constituent members occur within a certain number of skips. Using a corpus consisting of four datasets of Western classical music in symbolic form, we found that including skip-grams reduces data sparsity in n-gram distributions by (1) minimizing the proportion of n-grams with negligible counts, and (2) increasing the coverage of contiguous n-grams in a test corpus. What is more, skip-grams significantly outperformed contiguous n-grams in discovering conventional closing progressions (called cadences). © 2019 Sears, Arzt, Frostel, Sonnleitner, Widmer."
Gururani S.; Summers C.; Lerch A.,Instrument activity detection in polyphonic music using deep neural networks,2018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064129027&partnerID=40&md5=11a113040c15a61a3e54701af82f704d,"Gururani S., Center for Music Technology, Georgia Institute of Technology, United States; Summers C., Gracenote, Emeryville, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","Although instrument recognition has been thoroughly research, recognition in polyphonic music still faces challenges. While most research in polyphonic instrument recognition focuses on predicting the predominant instruments in a given audio recording, instrument activity detection represents a generalized problem of detecting the presence or activity of instruments in a track on a fine-grained temporal scale. We present an approach for instrument activity detection in polyphonic music with temporal resolution ranging from one second to the track level. This system allows, for instance, to retrieve specific areas of interest such as guitar solos. Three classes of deep neural networks are trained to detect up to 18 instruments. The architectures investigated in this paper are: multi-layer perceptrons, convolutional neural networks, and convolutional-recurrent neural networks. An in-depth evaluation on publicly available multi-track datasets using methods such as AUC-ROC and Label Ranking Average Precision highlights different aspects of the model performance and indicates the importance of using multiple evaluation metrics. Furthermore, we propose a new visualization to discuss instrument confusion in a multi-label scenario. © Siddharth Gururani, Cameron Summers, Alexander Lerch."
Masada K.; Bunescu R.,Chord recognition in symbolic music using semi-Markov conditional random fields,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069863654&partnerID=40&md5=a866a9326c092ab8fef393bebb57c600,"Masada K., School of EECS, Ohio University, Athens, OH, United States; Bunescu R., School of EECS, Ohio University, Athens, OH, United States","Chord recognition is a fundamental task in the harmonic analysis of tonal music, in which music is processed into a sequence of segments such that the notes in each segment are consistent with a corresponding chord label. We propose a machine learning model for chord recognition that uses semi-Markov Conditional Random Fields (semi-CRFs) to perform a joint segmentation and labeling of symbolic music. One benefit of using a semi-Markov model is that it enables the utilization of segment-level features, such as segment purity and chord coverage, that capture the extent to which the events in an entire segment of music are compatible with a candidate chord label. Correspondingly, we develop a rich set of segment-level features for a semi-CRF model that also incorporates the likelihood of a large number of chord-to-chord transitions. Evaluations on a dataset of Bach chorales and a corpus of theme and variations for piano by Beethoven and Mozart show that the proposed semi-CRF model outperforms a discriminatively trained Hidden Markov Model (HMM) that does sequential labeling of sounding events, thus demonstrating the suitability of semi-Markov models for joint segmentation and labeling of music. © 2019 Kristen Masada, Razvan Bunescu."
Gang N.; Kaneshiro B.; Berger J.; Dmochowski J.P.,Decoding neurally relevant musical features using canonical correlation analysis,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069887845&partnerID=40&md5=12188daff92a0cc61288572253f24fc5,"Gang N., Center for Computer Research in Music and Acoustics, Stanford University, United States; Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, United States; Berger J., Center for Computer Research in Music and Acoustics, Stanford University, United States; Dmochowski J.P., Department of Biomedical Engineering, City College of New York, United States","Music Information Retrieval (MIR) has been dominated by computational approaches. The possibility of leveraging neural systems via brain-computer interfaces is an alternative approach to annotating music. Here we test this idea by measuring correlations between musical features and brain responses in a statistically optimal fashion. Using an extensive dataset of electroencephalographic (EEG) responses to a variety of natural music stimuli, we employed Canonical Correlation Analysis to identify spatial EEG components that track temporal stimulus components. We found multiple statistically significant dimensions of stimulus-response correlation (SRC) for all songs studied. The temporal filters that maximize correlation with the neural response highlight harmonics and subharmonics of that song's beat frequency, with different harmonics emphasized by different components. The most stimulus-driven component of the EEG has an anatomically plausible, symmetric frontocentral topography that is preserved across stimuli. Our results suggest that different neural circuits encode different temporal hierarchies of natural music. Moreover, as techniques for decoding EEG advance, it may be possible to automatically label music via brain-computer interfaces that capture neural responses that are then translated into stimulus annotations. © 2019 Nick Gang, Blair Kaneshiro, Jonathan Berger, and Jacek P. Dmochowski."
Bigo L.; Giraud M.; Groult R.; Guiomard-Kagan N.; Levé F.,Sketching sonata form structure in selected classical string quartets,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069824769&partnerID=40&md5=d53f10b4c96a97fadff34ee29ae943fb,"Bigo L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Giraud M., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Groult R., MIS, Université de Picardie Jules Verne, Amiens, France; Guiomard-Kagan N., MIS, Université de Picardie Jules Verne, Amiens, France; Levé F., CRIStAL, UMR 9189, CNRS, Université de Lille, France, MIS, Université de Picardie Jules Verne, Amiens, France","Many classical works from 18th and 19th centuries are sonata forms, exhibiting a piece-level tonal path through an exposition, a development and a recapitulation and involving two thematic zones as well as other elements. The computational music analysis of scores with such a largescale structure is a challenge for the MIR community and should gather different analysis techniques. We propose first steps in that direction, combining analysis features on symbolic scores on patterns, harmony, and other elements into a structure estimated by a Viterbi algorithm on a Hidden Markov Model. We test this strategy on a set of first movements of Haydn and Mozart string quartets. The proposed computational analysis strategy finds some pertinent features and sketches the sonata form structure in some pieces that have a simple sonata form. © 2019 Louis Bigo, Mathieu Giraud, Richard Groult, Nicolas Guiomard-Kagan, Florence Levé."
Chen N.; Wang S.,High-level music descriptor extraction algorithm based on combination of multi-channel CNNs and LSTM,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061061065&partnerID=40&md5=2e00b4dba9ae41486902f067b0ffebb4,"Chen N., East China University of Science and Technology, China; Wang S., East China University of Science and Technology, China","Although Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTM) have yielded impressive performances in a variety of Music Information Retrieval (MIR) tasks, the complementarity among the CNNs of different architectures and that between CNNs and LSTM are seldom considered. In this paper, multichannel CNNs with different architectures and LSTM are combined into one unified architecture (Multi-Channel Convolutional LSTM, MCCLSTM) to extract high-level music descriptors. First, three channels of CNNs with different shapes of filter are applied on each spectrogram image chunk to extract the pitch-, tempo-, and bassrelevant descriptors, respectively. Then, the outputs of each CNNs channel are concatenated and then passed through a fully connected layer to obtain the fused descriptor. Finally, LSTM is applied on the fused descriptor sequence of the whole track to extract its long-term structure property to obtain the high-level descriptor. To prove the efficiency of the MCCLSTM model, the obtained high-level music descriptor is applied to the music genre classification and emotion prediction task. Experimental results demonstrate that, when compared with the hand-crafted schemes or conventional deep learning (Multi Layer Perceptrons (MLP), CNNs, and LSTM) based ones, MCCLSTM achieves higher prediction accuracy on three music collections with different kinds of semantic tags. © 2019 Ning Chen, Shijun Wang."
Vogl R.; Dorfer M.; Widmer G.; Knees P.,Drum transcription via joint beat and drum modeling using convolutional recurrent neural networks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069918587&partnerID=40&md5=73e91ae536f07943fcf44c0319a040f0,"Vogl R., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria, Dept. of Computational Perception, Johannes Kepler University Linz, Austria; Dorfer M., Dept. of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University Linz, Austria; Knees P., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","Existing systems for automatic transcription of drum tracks from polyphonic music focus on detecting drum instrument onsets but lack consideration of additional meta information like bar boundaries, tempo, and meter. We address this limitation by proposing a system which has the capability to detect drum instrument onsets along with the corresponding beats and downbeats. In this design, the system has the means to utilize information on the rhythmical structure of a song which is closely related to the desired drum transcript. To this end, we introduce and compare different architectures for this task, i.e., recurrent, convolutional, and recurrent-convolutional neural networks. We evaluate our systems on two well-known data sets and an additional new data set containing both drum and beat annotations. We show that convolutional and recurrentconvolutional neural networks perform better than state-ofthe- art methods and that learning beats jointly with drums can be beneficial for the task of drum detection. © 2019 Richard Vogl, Matthias Dorfer, Gerhard Widmer, Peter Knees."
Laroche C.; Papadopoulos H.; Kowalski M.; Richard G.,Genre specific dictionaries for harmonic/percussive source separation,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030224512&partnerID=40&md5=4ae57f001aa4c4d7dafe8d12a0a3fdb3,"Laroche C., LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay, Paris, France, Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France; Papadopoulos H., Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France; Kowalski M., Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France, Parietal project-team, INRIA, CEA-Saclay, France; Richard G., LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay, Paris, France","Blind source separation usually obtains limited performance on real and polyphonic music signals. To overcome these limitations, it is common to rely on prior knowledge under the form of side information as in Informed Source Separation or on machine learning paradigms applied on a training database. In the context of source separation based on factorization models such as the Non-negative Matrix Factorization, this supervision can be introduced by learning specific dictionaries. However, due to the large diversity of musical signals it is not easy to build sufficiently compact and precise dictionaries that will well characterize the large array of audio sources. In this paper, we argue that it is relevant to construct genre-specific dictionaries. Indeed, we show on a task of harmonic/percussive source separation that the dictionaries built on genre-specific training subsets yield better performances than cross-genre dictionaries. © Clément Laroche, Hélène Papadopoulos, Matthieu Kowalski, Gaël Richard."
Repetto R.C.; Serra X.,A collection of music scores for corpus based jingju singing research,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041393599&partnerID=40&md5=bd39ff80e9b15bf5f36cd41bc2ec940d,"Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The MIR research on jingju (also known as Beijing or Peking opera) music has taken audio as the main source of information. Music scores are an important resource for the musicological research of this tradition, but no machine readable ones have been available for computational analysis. In order to explore the potential of symbolic score data for jingju music research, we have expanded the Comp- Music Jingju Music Corpus, which contains mostly audio, with a collection of 92 machine readable scores, for a total of 897 melodic lines. Since our purpose is the study of jingju singing in terms of its musical system elements, we have selected the arias used as examples in reference jingju music textbooks. The collection is accompanied by scores metadata, curated annotations per score and melodic line, and a set of software tools for extracting statistical information from it. All the gathered data and developed software are available for research purposes. In this paper we first discuss the culture specific concepts that are needed for understanding the contents of the collection, followed by a detailed description of it. We then present a series of computational analyses performed on the scores and discuss some musicological findings. © 2019 Rafael Caro Repetto, Xavier Serra."
Ewert S.; Wang S.; Müller M.; Sandler M.,Score-informed identification of missing and extra notes in piano recordings,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029561826&partnerID=40&md5=90170cc9a72954350067f47057f229ba,"Ewert S., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Wang S., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Müller M., International Audio Laboratories, Erlangen, Germany; Sandler M., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom","A main goal in music tuition is to enable a student to play a score without mistakes, where common mistakes include missing notes or playing additional extra ones. To automatically detect these mistakes, a first idea is to use a music transcription method to detect notes played in an audio recording and to compare the results with a corresponding score. However, as the number of transcription errors produced by standard methods is often considerably higher than the number of actual mistakes, the results are often of limited use. In contrast, our method exploits that the score already provides rough information about what we seek to detect in the audio, which allows us to construct a tailored transcription method. In particular, we employ score-informed source separation techniques to learn for each score pitch a set of templates capturing the spectral properties of that pitch. After extrapolating the resulting template dictionary to pitches not in the score, we estimate the activity of each MIDI pitch over time. Finally, making again use of the score, we choose for each pitch an individualized threshold to differentiate note onsets from spurious activity in an optimized way. We indicate the accuracy of our approach on a dataset of piano pieces commonly used in education. © Sebastian Ewert, Siying Wang, Meinard Müller and Mark Sandler."
Tanaka T.; Bemman B.; Meredith D.,Integer programming formulation of the problem of generating Milton Babbitt’s all-partition arrays,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052203043&partnerID=40&md5=9628f58e544418bc1f1f3ffb97ef39fe,"Tanaka T., STMS Lab, IRCAM, CNRS, UPMC, Paris, France; Bemman B., Aalborg University, Aalborg, Denmark; Meredith D., Aalborg University, Aalborg, Denmark","Milton Babbitt (1916–2011) was a composer of twelve-tone serial music noted for creating the all-partition array. The problem of generating an all-partition array involves finding a rectangular array of pitch-class integers that can be partitioned into regions, each of which represents a distinct integer partition of 12. Integer programming (IP) has proven to be effective for solving such combinatorial problems, however, it has never before been applied to the problem addressed in this paper. We introduce a new way of viewing this problem as one in which restricted overlaps between integer partition regions are allowed. This permits us to describe the problem using a set of linear constraints necessary for IP. In particular, we show that this problem can be defined as a special case of the well-known problem of set-covering (SCP), modified with additional constraints. Due to the difficulty of the problem, we have yet to discover a solution. However, we assess the potential practicality of our method by running it on smaller similar problems. © Tsubasa Tanaka, Brian Bemman, David Meredith."
Scholz R.; Ramalho G.; Cabral G.,Cross task study on mirex recent results: An index for evolution measurement and some stagnation hypotheses,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042800257&partnerID=40&md5=9ebf43182cbf962361d7118869a66559,"Scholz R.; Ramalho G., Universidade Federal de Pernambuco, Recife, PE, Brazil; Cabral G.","In the last 20 years, Music Information Retrieval (MIR) has been an expanding research field, and the MIREX competition has become the main evaluation venue in MIR field. Analyzing recent results for various tasks of MIREX (MIR Evaluation eXchange), we observed that the evolution of task solutions follows two different patterns: for some tasks, the results apparently hit stagnation, whereas for others, they seem getting better over time. In this paper, (a) we compile the MIREX results of the last 6 years, (b) we propose a configurable quantitative index for evolution trend measurement of MIREX tasks, and (c) we discuss possible explanations or hypotheses for the stagnation phenomena hitting some of them. This paper hopes to incite a debate in the MIR research community about the progress in the field and how to adequately measure evolution trends. © Ricardo Scholz, Geber Ramalho, Giordano Cabral."
Fields B.; Rhodes C.,Listen to me – Don’t listen to me: What can communities of critics tell us about music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069988936&partnerID=40&md5=a799cb5e630077b37db21edc1df23ad5,"Fields B., Dept. of Computing Goldsmiths, University of London, New Cross London, SE14 6NW, United Kingdom; Rhodes C., Dept. of Computing Goldsmiths, University of London, New Cross London, SE14 6NW, United Kingdom","Social knowledge and data sharing on the Web takes many forms. So too do the ways people share ideas and opinions. In this paper we examine one such emerging form: the amateur critic. In particular, we examine genius.com, a website which allows its users to annotate and explain the meaning of segments of lyrics in music and other written works. We describe a novel dataset of approximately 700,000 users’ activity on genius.com, their social connections, and song annotation activity. The dataset encompasses over 120,000 songs, with more than 3 million unique annotations. Using this dataset, we model overlap in interest or expertise through the proxy of co-annotation. This is the basis for a complex network model of the activity on genius.com, which is then used for community detection. We introduce a new measure of network community activity: community skew. Through this analysis we draw a comparison of between co-annotation and notions of genre and categorisation in music. We show a new view on the social constructs of genre in music. © Ben Fields, Christophe Rhodes."
Schramm R.; McLeod A.; Steedman M.; Benetos E.,Multi-pitch detection and voice assignment for a cappella recordings of multiple singers,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037744474&partnerID=40&md5=7d5949e95df1dce57b442efcc961b012,"Schramm R., Department of Music, Universidade Federal do Rio Grande do Sul, Brazil, Centre for Digital Music, Queen Mary University of London, United Kingdom; McLeod A., School of Informatics, University of Edinburgh, United Kingdom; Steedman M., School of Informatics, University of Edinburgh, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","This paper presents a multi-pitch detection and voice assignment method applied to audio recordings containing a cappella performances with multiple singers. A novel approach combining an acoustic model for multi-pitch detection and a music language model for voice separation and assignment is proposed. The acoustic model is a spectrogram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov models that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part compositions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70% for frame-based multipitch detection and over 45% for four-voice assignment. © 2019 Rodrigo Schramm, Andrew McLeod, Mark Steedman, Emmanouil Benetos."
Eghbal-Zadeh H.; Widmer G.,Noise robust music artist recognition using I-vector features,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057552407&partnerID=40&md5=4b1922d50d1d492b384f0f84eb486c5a,"Eghbal-Zadeh H., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","In music information retrieval (MIR), dealing with different types of noise is important and the MIR models are frequently used in noisy environments such as live performances. Recently, i-vector features have shown great promise for some major tasks in MIR, such as music similarity and artist recognition. In this paper, we introduce a novel noise-robust music artist recognition system using i-vector features. Our method uses a short sample of noise to learn the parameters of noise, then using a Maximum A Postriori (MAP) estimation it estimates clean i-vectors given noisy i-vectors. We examine the performance of multiple systems confronted with different kinds of additive noise in a clean training - noisy testing scenario. Using open-source tools, we have synthesized 12 different noisy versions from a standard 20-class music artist recognition dataset encountered with 4 different kinds of additive noise with 3 different Signal-to-Noise-Ratio (SNR). Using these datasets, we carried out music artist recognition experiments comparing the proposed method with the state-of-the-art. The results suggest that the proposed method outperforms the state-of-the-art. © Hamid Eghbal-zadeh Gerhard Widmer."
Kirlin P.B.,Global properties of expert and algorithmic hierarchical music analyses,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069990178&partnerID=40&md5=4ce64e2912489fee953365ea867ded40,"Kirlin P.B., Department of Mathematics and Computer Science, Rhodes College, United States","In recent years, advances in machine learning and increases in data set sizes have produced a number of viable algorithms for analyzing music in a hierarchical fashion according to the guidelines of music theory. Many of these algorithms, however, are based on techniques that rely on a series of local decisions to construct a complete music analysis, resulting in analyses that are not guaranteed to resemble ground-truth analyses in their large-scale organizational shapes or structures. In this paper, we examine a number of hierarchical music analysis data sets — drawing from Schenkerian analysis and other analytical systems based on A Generative Theory of Tonal Music — to study three global properties calculated from the shapes of the analyses. The major finding presented in this work is that it is possible for an algorithm that only makes local decisions to produce analyses that resemble expert analyses with regards to the three global properties in question. We also illustrate specific similarities and differences in these properties across both ground-truth and algorithmically-produced analyses. © Phillip B. Kirlin."
Bas de Haas W.; Volk A.,Meter detection in symbolic music using inner metric analysis,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029331271&partnerID=40&md5=f759e40d6e9a6abf9ac12e8d29a4abc7,"Bas de Haas W., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands","In this paper we present PRIMA: a new model tailored to symbolic music that detects the meter and the first downbeat position of a piece. Given onset data, the metrical structure of a piece is interpreted using the Inner Metric Analysis (IMA) model. IMA identifies the strong and weak metrical positions in a piece by performing a periodicity analysis, resulting in a weight profile for the entire piece. Next, we reduce IMA to a feature vector and model the detection of the meter and its first downbeat position probabilistically. In order to solve the meter detection problem effectively, we explore various feature selection and parameter optimisation strategies, including Genetic, Maximum Likelihood, and Expectation-Maximisation algorithms. PRIMA is evaluated on two datasets of MIDI files: a corpus of ragtime pieces, and a newly assembled pop dataset. We show that PRIMA outperforms autocorrelation-based meter detection as implemented in the MIDItoolbox on these datasets. © W. Bas de Haas, Anja Volk."
Panteli M.; Dixon S.,On the evaluation of rhythmic and melodic descriptors for music similarity,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038969135&partnerID=40&md5=69d5f7fed2d41dc3f3b2ad6fb711f47d,"Panteli M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","In exploratory studies of large music collections where often no ground truth is available, it is essential to evaluate the suitability of the underlying methods prior to drawing any conclusions. In this study we focus on the evaluation of audio features that can be used for rhythmic and melodic content description and similarity estimation. We select a set of state-of-the-art rhythmic and melodic descriptors and assess their invariance with respect to transformations of timbre, recording quality, tempo and pitch. We create a dataset of synthesised audio and investigate which features are invariant to the aforementioned transformations and whether invariance is affected by characteristics of the music style and the monophonic or polyphonic character of the audio recording. From the descriptors tested, the scale transform performed best for rhythm classification and retrieval and pitch bihistogram performed best for melody. The proposed evaluation strategy can inform decisions in the feature design process leading to significant improvement in the reliability of the features. © Maria Panteli, Simon Dixon."
Miron M.; Janer J.; Gómez E.,Monaural score-informed source separation for classical music using convolutional neural networks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048592475&partnerID=40&md5=cec65d90e0571d4eb0eeee0f07876500,"Miron M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Janer J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Score information has been shown to improve music source separation when included into non-negative matrix factorization (NMF) frameworks. Recently, deep learning approaches have outperformed NMF methods in terms of separation quality and processing time, and there is scope to extend them with score information. In this paper, we propose a score-informed separation system for classical music that is based on deep learning. We propose a method to derive training features from audio files and the corresponding coarsely aligned scores for a set of classical music pieces. Additionally, we introduce a convolutional neural network architecture (CNN) with the goal of estimating time-frequency masks for source separation. Our system is trained with synthetic renditions derived from the original scores and can be used to separate real-life performances based on the same scores, provided a coarse audio-to-score alignment. The proposed system achieves better performance (SDR and SIR) and is less computationally intensive than a score-informed NMF system on a dataset comprising Bach chorales. © 2019 Marius Miron, Jordi Janer, Emilia Gómez."
Hedges T.; Wiggins G.,Improving predictions of derived viewpoints in multiple viewpoint systems,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070023128&partnerID=40&md5=c965997cd99b5ceaeb5b0dc3aa1ec5fc,"Hedges T., Queen Mary University of London, United Kingdom; Wiggins G., Queen Mary University of London, United Kingdom","This paper presents and tests a method for improving the predictive power of derived viewpoints in multiple viewpoints systems. Multiple viewpoint systems are a well established method for the statistical modelling of sequential symbolic musical data. A useful class of viewpoints known as derived viewpoints map symbols from a basic event space to a viewpoint-specific domain. Probability estimates are calculated in the derived viewpoint domain before an inverse function maps back to the basic event space to complete the model. Since an element in the derived viewpoint domain can potentially map onto multiple basic elements, probability mass is distributed between the basic elements with a uniform distribution. As an alternative, this paper proposes a distribution weighted by zero-order frequencies of the basic elements to inform this probability mapping. Results show this improves the predictive performance for certain derived viewpoints, allowing them to be selected in viewpoint selection. © Thomas Hedges, Geraint Wiggins."
Hu X.; Choi K.; Lee J.H.; Laplante A.; Hao Y.; Cunningham S.J.; Downie J.S.,WIMIR: An informetric study on women authors in ISMIR,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069898466&partnerID=40&md5=0191e6d8a4e09e8c82df4f2787a9f1bb,"Hu X., University of Hong Kong, China; Choi K., University of Illinois, United States; Lee J.H., Univeristy of Washington, United States; Laplante A., Université de Montréal, Canada; Hao Y., University of Illinois, United States; Cunningham S.J., University of Waikato, New Zealand; Downie J.S., University of Illinois, United States","The Music Information Retrieval (MIR) community is becoming increasingly aware of a gender imbalance evident in ISMIR participation and publication. This paper reports upon a comprehensive informetric study of the publication, authorship and citation characteristics of female researchers in the context of the ISMIR conferences. All 1,610 papers in the ISMIR proceedings written by 1,910 unique authors from 2000 to 2015 were collected and analyzed. Only 14.1% of all papers were led by female researchers. Temporal analysis shows that the percentage of lead female authors has not improved over the years, but more papers have appeared with female coauthors in very recent years. Topics and citation numbers are also analyzed and compared between female and male authors to identify research emphasis and to measure impact. The results show that the most prolific authors of both genders published similar numbers of ISMIR papers and the citation counts of lead authors in both genders had no significant difference. We also analyzed the collaboration patterns to discover whether gender is related to the number of collaborators. Implications of these findings are discussed and suggestions are proposed on how to continue encouraging and supporting female participation in the MIR field. © Xiao Hu, Kahyun Choi, Jin Ha Lee, Audrey Laplante, Yun Hao, Sally Jo Cunningham and J. Stephen Downie."
Sonnleitner R.; Arzt A.; Widmer G.,Landmark-based audio fingerprinting for DJ mix monitoring,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030224868&partnerID=40&md5=9f44d3cde3307389a7cf5ff647b3300e,"Sonnleitner R., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Recently, the media monitoring industry shows increased interest in applying automated audio identification systems for revenue distribution of DJ performances played in discotheques. DJ mixes incorporate a wide variety of signal modifications, e.g. pitch shifting, tempo modifications, cross-fading and beat-matching. These signal modifications are expected to be more severe than what is usually encountered in the monitoring of radio and TV broadcasts. The monitoring of DJ mixes presents a hard challenge for automated music identification systems, which need to be robust to various signal modifications while maintaining a high level of specificity to avoid false revenue assignment. In this work we assess the fitness of three landmark-based audio fingerprinting systems with different properties on real-world data – DJ mixes that were performed in discotheques. To enable the research community to evaluate systems on DJ mixes, we also create and publish a freely available, creative-commons licensed dataset of DJ mixes along with their reference tracks and song-border annotations. Experiments on these datasets reveal that a recent quad-based method achieves considerably higher performance on this task than the other methods. © Reinhard Sonnleitner, Andreas Arzt, Gerhard Widmer."
Valle R.; Fremont D.J.; Akkaya I.; Donze A.; Freed A.; Seshia S.S.,Learning and visualizing music specifications using pattern graphs,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069998573&partnerID=40&md5=06fbde7a02a51a177f1825c5763dfe1d,"Valle R., UC Berkeley, CNMAT, United States; Fremont D.J., UC Berkeley, United States; Akkaya I., UC Berkeley, United States; Donze A., UC Berkeley, United States; Freed A., UC Berkeley, CNMAT, United States; Seshia S.S., UC Berkeley, United States","We describe a system to learn and visualize specifications from song(s) in symbolic and audio formats. The core of our approach is based on a software engineering procedure called specification mining. Our procedure extracts patterns from feature vectors and uses them to build pattern graphs. The feature vectors are created by segmenting song(s) and extracting time and and frequency domain features from them, such as chromagrams, chord degree and interval classification. The pattern graphs built on these feature vectors provide the likelihood of a pattern between nodes, as well as start and ending nodes. The pattern graphs learned from a song(s) describe formal specifications that can be used for human interpretable quantitatively and qualitatively song comparison or to perform supervisory control in machine improvisation. We offer results in song summarization, song and style validation and machine improvisation with formal specifications. © Rafael Valle, Daniel J. Fremont, Ilge Akkaya, Alexandre Donze, Adrian Freed, Sanjit S. Seshia."
Michael Winters R.; Gururani S.; Lerch A.,"Automatic practice logging: Introduction, dataset & preliminary study",2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070013832&partnerID=40&md5=27084b9fdf0baa61047c95ba623e10ec,"Michael Winters R., Georgia Tech Center for Music Technology (GTCMT), Georgia; Gururani S., Georgia Tech Center for Music Technology (GTCMT), Georgia; Lerch A., Georgia Tech Center for Music Technology (GTCMT), Georgia","Musicians spend countless hours practicing their instruments. To document and organize this time, musicians commonly use practice charts to log their practice. However, manual techniques require time, dedication, and experience to master, are prone to fallacy and omission, and ultimately can not describe the subtle variations in each repetition. This paper presents an alternative: by analyzing and classifying the audio recorded while practicing, logging could occur automatically, with levels of detail, accuracy, and ease that would not be possible otherwise. Towards this goal, we introduce the problem of Automatic Practice Logging (APL), including a discussion of the benefits and unique challenges it raises. We then describe a new dataset of over 600 annotated recordings of solo piano practice, which can be used to design and evaluate APL systems. After framing our approach to the problem, we present an algorithm designed to align short segments of practice audio with reference recordings using pitch chroma and dynamic time warping. © R. Michael Winters, Siddharth Gururani, Alexander Lerch."
Ibrahim K.M.; Grunberg D.; Agres K.; Gupta C.; Wang Y.,Intelligibility of sung lyrics: A pilot study,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048552209&partnerID=40&md5=f30f7afa8849eeb3c83e472bd63620eb,"Ibrahim K.M., Department of Computer Science, National University of Singapore, Singapore, Singapore; Grunberg D., Department of Computer Science, National University of Singapore, Singapore, Singapore; Agres K., Institute of High Performance Computing, ASTAR, Singapore, Singapore; Gupta C., Department of Computer Science, National University of Singapore, Singapore, Singapore; Wang Y., Department of Computer Science, National University of Singapore, Singapore, Singapore","We propose a system to automatically assess the intelligibility of sung lyrics. We are particularly interested in being able to identify songs which are intelligible to second language learners, as such individuals often sing along the song to help them learn their second language, but this is only helpful if the song is intelligible enough for them to understand. As no automatic system for identifying 'intelligible' songs currently exists, songs for second language learners are generally selected by hand, a timeconsuming and onerous process. We conducted an experiment in which test subjects, all of whom are learning English as a second language, were presented with 100 excerpts of songs drawn from five different genres. The test subjects listened to and transcribed the excerpts and the intelligibility of each excerpt was assessed based on average transcription accuracy across subjects. Excerpts that were more accurately transcribed on average were considered to be more intelligible than those less accurately transcribed on average. We then tested standard acoustic features to determine which were most strongly correlated with intelligibility. Our final system classifies the intelligibility of the excerpts and achieves 66% accuracy for 3 classes of intelligibility. © 2019 Karim M. Ibrahim, David Grunberg, Kat Agres, Chitralekha Gupta, Ye Wang."
Rodríguez-Algarra F.; Sturm B.L.; Maruri-Aguilar H.,Analysing scattering-based music content analysis systems: Where’s the music?,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936094&partnerID=40&md5=f288f83d3c4164a42d3c1d378c37b77b,"Rodríguez-Algarra F., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sturm B.L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Maruri-Aguilar H., School of Mathematical Sciences, Queen Mary University of London, United Kingdom","Music content analysis (MCA) systems built using scattering transform features are reported quite successful in the GTZAN benchmark music dataset. In this paper, we seek to answer why. We first analyse the feature extraction and classification components of scattering-based MCA systems. This guides us to perform intervention experiments on three factors: train/test partition, classifier and recording spectrum. The partition intervention shows a decrease in the amount of reproduced ground truth by the resulting systems. We then replace the learning algorithm with a binary decision tree, and identify the impact of specific feature dimensions. We finally alter the spectral content related to such dimensions, which reveals that these scattering-based systems exploit acoustic information below 20 Hz to reproduce GTZAN ground truth. The source code to reproduce our experiments is available online. 1 © Francisco Rodríguez-Algarra, Bob L. Sturm, Hugo Maruri-Aguilar."
Weigl D.M.; Page K.R.,"A framework for distributed semantic annotation of musical score: ""Take it to the bridge!""",2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046934871&partnerID=40&md5=4f1e62012e0f4dd9ee68ed8da70a43c8,"Weigl D.M., Oxford e-Research Centre, University of Oxford, United Kingdom; Page K.R., Oxford e-Research Centre, University of Oxford, United Kingdom","Music notation expresses performance instructions in a way commonly understood by musicians, but printed paper parts are limited to encodings of static, a priori knowledge. In this paper we present a platform for multi-way communication between collaborating musicians through the dynamic modification of digital parts: the Music Encoding and Linked Data (MELD) framework for distributed real-time annotation of digital music scores. MELD users and software agents create semantic annotations of music concepts and relationships, which are associated with musical structure specified by the Music Encoding Initiative schema (MEI). Annotations are expressed in RDF, allowing alternative music vocabularies (e.g., popular vs. classical music structures) to be applied. The same underlying framework retrieves, distributes, and processes information that addresses semantically distinguishable music elements. Further knowledge is incorporated from external sources through the use of Linked Data. The RDF is also used to match annotation types and contexts to rendering actions which display the annotations upon the digital score. Here, we present a MELD implementation and deployment which augments the digital music scores used by musicians in a group performance, collaboratively changing the sequence within and between pieces in a set list. © 2019 David M. Weigl and Kevin R. Page."
Calvo-Zaragoza J.; Valero-Mas J.J.; Pertusa A.,End-to-end Optical Music Recognition using neural networks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045320607&partnerID=40&md5=e30bb7f46f89022d6c317f5e8616a0f2,"Calvo-Zaragoza J., Centre for Interdisciplinary Research in Music, Media and Technology, McGill University, Montreal, QC, Canada; Valero-Mas J.J., Software and Computing Systems, University of Alicante, Alicante, Spain; Pertusa A., Software and Computing Systems, University of Alicante, Alicante, Spain","This work addresses the Optical Music Recognition (OMR) task in an end-to-end fashion using neural networks. The proposed architecture is based on a Recurrent Convolutional Neural Network topology that takes as input an image of a monophonic score and retrieves a sequence of music symbols as output. In the first stage, a series of convolutional filters are trained to extract meaningful features of the input image, and then a recurrent block models the sequential nature of music. The system is trained using a Connectionist Temporal Classification loss function, which avoids the need for a frame-by-frame alignment between the image and the ground-truth music symbols. Experimentation has been carried on a set of 90,000 synthetic monophonic music scores with more than 50 different possible labels. Results obtained depict classification error rates around 2 % at symbol level, thus proving the potential of the proposed end-to-end architecture for OMR. The source code, dataset, and trained models are publicly released for reproducible research and future comparison purposes. © 2019 Jorge Calvo-Zaragoza, Jose J. Valero-Mas, Antonio Pertusa."
Deng J.; Kwok Y.-K.,A hybrid Gaussian-HMM-deep-learning approach for automatic chord estimation with very large vocabulary,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033574575&partnerID=40&md5=ae1d73a1d6ae663e7c25005e2334f19b,"Deng J., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong; Kwok Y.-K., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong","We propose a hybrid Gaussian-HMM-Deep-Learning approach for automatic chord estimation with very large chord vocabulary. The Gaussian-HMM part is similar to Chordino, which is used as a segmentation engine to divide input audio into note spectrogram segments. Two types of deep learning models are proposed to classify these segments into chord labels, which are then connected as chord sequences. Two sets of evaluations are conducted with two large chord vocabularies. The first evaluation is conducted in a recent MIREX standard way. Results show that our approach has obvious advantage over the state-of-the-art large-vocabulary-with-inversions supportable ACE system in terms of large vocabularies, although is outperformed by in small vocabularies. Through analyzing and deducing system behaviors behind the results, we see interesting chord confusion patterns made by different systems, which conceivably point to a demand of more balanced and consistent annotated datasets for training and testing. The second evaluation preliminarily demonstrates our approach’s superiority on a jazz chord vocabulary with 36 chord types, compared with a Chordino-like Gaussian-HMM baseline system with augmented vocabulary capacity. © Junqi Deng and Yu-Kwong Kwok."
Bandiera G.; Picas O.R.; Tokuda H.; Hariya W.; Oishi K.; Serra X.,Good-sounds.org: A framework to explore goodness in instrumental sounds,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046885523&partnerID=40&md5=9ec7970529a543d52c6bab9156e44778,"Bandiera G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Picas O.R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Tokuda H., Technology Development Dept., KORG Inc., Tokyo, Japan; Hariya W., Technology Development Dept., KORG Inc., Tokyo, Japan; Oishi K., Technology Development Dept., KORG Inc., Tokyo, Japan; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We introduce good-sounds.org, a community driven framework based on freesound.org to explore the concept of goodness in instrumental sounds. Goodness is considered here as the common agreed basic sound quality of an instrument without taking into consideration musical expressiveness. Musicians upload their sounds and vote on existing sounds, and from the collected data the system is able to develop sound goodness measures of relevance for music education applications. The core of the system is a database of sounds, together with audio features extracted from them using MTG’s Essentia library and user annotations related to the goodness of the sounds. The web front-end provides useful data visualizations of the sound attributes and tools to facilitate user interaction. To evaluate the framework, we carried out an experiment to rate sound goodness of single notes of nine orchestral instruments. In it, users rated the sounds using an AB vote over a set of sound attributes defined to be of relevance in the characterization of single notes of instrumental sounds. With the obtained votes we built a ranking of the sounds for each attribute and developed a model that rates the goodness for each of the selected sound attributes. Using this approach, we have succeeded in obtaining results comparable to a model that was built from expert generated evaluations. © Giuseppe Bandiera, Oriol Romani Picas, Hiroshi Tokuda, Wataru Hariya, Koji Oishi, Xavier Serra."
Raffel C.; Ellis D.P.W.,Extracting ground truth information from MIDI files: A MIDIfesto,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069988770&partnerID=40&md5=2cf606f83c5c25ff20791200e202ed5b,"Raffel C., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States",MIDI files abound and provide a bounty of information for music informatics. We enumerate the types of information available in MIDI files and describe the steps necessary for utilizing them. We also quantify the reliability of this data by comparing it to human-annotated ground truth. The results suggest that developing better methods to leverage information present in MIDI files will facilitate the creation of MIDI-derived ground truth for audio content-based MIR. © Colin Raffel and Daniel P. W. Ellis.
Holzapfel A.; Benetos E.,The sousta corpus: Beat-informed automatic transcription of traditional dance tunes,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028026832&partnerID=40&md5=48483ce8cfb745b095dc3dd1c082fac9,"Holzapfel A., Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this paper, we present a new corpus for research in computational ethnomusicology and automatic music transcription, consisting of traditional dance tunes from Crete. This rich dataset includes audio recordings, scores transcribed by ethnomusicologists and aligned to the audio performances, and meter annotations. A second contribution of this paper is the creation of an automatic music transcription system able to support the detection of multiple pitches produced by lyra (a bowed string instrument). Furthermore, the transcription system is able to cope with deviations from standard tuning, and provides temporally quantized notes by combining the output of the multi-pitch detection stage with a state-of-the-art meter tracking algorithm. Experiments carried out for note tracking using 25ms onset tolerance reach 41.1% using information from the multi-pitch detection stage only, 54.6% when integrating beat information, and 57.9% when also supporting tuning estimation. The produced meter aligned transcriptions can be used to generate staff notation, a fact that increases the value of the system for studies in ethnomusicology. © Andre Holzapfel, Emmanouil Benetos."
Stasiak B.,DTV-based melody cutting for DTW-based melody search and indexing in QBH systems,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069995242&partnerID=40&md5=9ca698b5ac128b0f194c649980b6d064,"Stasiak B., Institute of Information Technology, Lodz University of Technology, Poland","Melody analysis is an important processing step in several areas of Music Information Retrieval (MIR). Processing the pitch values extracted from raw input audio signal may be computationally complex as it requires substantial effort to reduce the uncertainty resulting i.a. from tempo variability and transpositions. A typical example is the melody matching problem in Query-by-Humming (QbH) systems, where Dynamic Time Warping (DTW) and note-based approaches are typically applied. In this work we present a new, simple and efficient method of investigating the melody content which may be used for approximate, preliminary matching of melodies irrespective of their tempo and length. The proposed solution is based on Discrete Total Variation (DTV) of the melody pitch vector, which may be computed in linear time. We demonstrate its practical application for finding the appropriate melody cutting points in the R*-tree-based DTW indexing framework. The experimental validation is based on a dataset of 4431 queries and over 4000 template melodies, constructed specially for testing Query-by-Humming systems. © Bartłomiej Stasiak."
Atherton J.; Kaneshiro B.,I said it first: Topological analysis of lyrical influence networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892722&partnerID=40&md5=273b7d5b98a8ab55fdf58909457e4fbe,"Atherton J., Center for Computer Research in Music and Acoustics, Stanford University, United States; Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, United States","We present an analysis of musical influence using intact lyrics of over 550,000 songs, extending existing research on lyrics through a novel approach using directed networks. We form networks of lyrical influence over time at the level of three-word phrases, weighted by tf-idf. An edge reduction analysis of strongly connected components suggests highly central artist, songwriter, and genre network topologies. Visualizations of the genre network based on multidimensional scaling confirm network centrality and provide insight into the most influential genres at the heart of the network. Next, we present metrics for influence and self-referential behavior, examining their interactions with network centrality and with the genre diversity of songwriters. Here, we uncover a negative correlation between songwriters’ genre diversity and the robustness of their connections. By examining trends among the data for top genres, songwriters, and artists, we address questions related to clustering, influence, and isolation of nodes in the networks. We conclude by discussing promising future applications of lyrical influence networks in music information retrieval research. The networks constructed in this study are made publicly available for research purposes. © Jack Atherton and Blair Kaneshiro."
Liebman E.; Stone P.; White C.N.,Impact of music on decision making in quantitative tasks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862638&partnerID=40&md5=ea2e89d4329b12c86125617d57345d35,"Liebman E., Department of Computer Science, University of Texas at Austin, Austria; Stone P., Department of Computer Science, University of Texas at Austin, Austria; White C.N., Department of Psychology, Syracuse University, United States","The goal of this study is to explore which aspects of people’s analytical decision making are affected when exposed to music. To this end, we apply a stochastic sequential model of simple decisions, the drift-diffusion model (DDM), to understand risky decision behavior. Numerous studies have demonstrated that mood can affect emotional and cognitive processing, but the exact nature of the impact music has on decision making in quantitative tasks has not been sufficiently studied. In our experiment, participants decided whether to accept or reject multiple bets with different risk vs. reward ratios while listening to music that was chosen to induce positive or negative mood. Our results indicate that music indeed alters people’s behavior in a surprising way - happy music made people make better choices. In other words, it made people more likely to accept good bets and reject bad bets. The DDM decomposition indicated the effect focused primarily on both the caution and the information processing aspects of decision making. To further understand the correspondence between auditory features and decision making, we studied how individual aspects of music affect response patterns. Our results are particularly interesting when compared with recent results regarding the impact of music on emotional processing, as they illustrate that music affects analytical decision making in a fundamentally different way, hinting at a different psychological mechanism that music impacts. © Elad Liebman, Peter Stone, Corey N. White."
Besson V.; Gurrieri M.; Rigaux P.; Tacaille A.; Thion V.,A methodology for quality assessment in collaborative score libraries,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028053774&partnerID=40&md5=2223293dfaa3cb66b6d012ca07cab58a,"Besson V., CESR, Univ. Tours, France; Gurrieri M., CESR, Univ. Tours, France; Rigaux P., CEDRIC/CNAM, Paris, France; Tacaille A., IReMus, Sorbonne Universités, Paris, France; Thion V., IRISA, Univ. Rennes 1, Lannion, France","We examine quality issues raised by the development of XML-based Digital Score Libraries. Based on the authors’ practical experience, the paper exposes the quality shortcomings inherent to the complexity of music encoding, and the lack of support from state-of-the-art formats. We also identify the various facets of the “quality” concept with respect to usages and motivations. We finally propose a general methodology to introduce quality management as a first-level concern in the management of score collections. © Vincent Besson, Marco Gurrieri, Philippe Rigaux, Alice Tacaille, Virginie Thion."
Dorfer M.; Arzt A.; Widmer G.,Towards score following in sheet music images,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030557163&partnerID=40&md5=b4887c8ab38eabce742ae6d8ae4c91e4,"Dorfer M., Department of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","This paper addresses the matching of short music audio snippets to the corresponding pixel location in images of sheet music. A system is presented that simultaneously learns to read notes, listens to music and matches the currently played music to its corresponding notes in the sheet. It consists of an end-to-end multi-modal convolutional neural network that takes as input images of sheet music and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (covering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks – which have proven to be powerful image processing models – working with sheet music becomes feasible and a promising future research direction. © Matthias Dorfer, Andreas Arzt, Gerhard Widmer."
Cardoso J.P.V.; Pontello L.F.; Holanda P.H.F.; Guilherme B.; Goussevskaia O.; da Silva A.P.C.,Mixtape: Direction-based navigation in large media collections,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027164189&partnerID=40&md5=142c523b8e614c1982dd9255859b0b08,"Cardoso J.P.V., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Pontello L.F., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Holanda P.H.F., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Guilherme B., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Goussevskaia O., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; da Silva A.P.C., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil","In this work we explore the increasing demand for novel user interfaces to navigate large media collections. We implement a scalable data structure to store and retrieve similarity information and propose a novel navigation framework that uses geometric vector operations and real-time user feedback to direct the outcome. In particular, we implement this framework in the domain of music. To evaluate the effectiveness of the navigation process, we propose an automatic evaluation framework, based on synthetic user profiles, which allows to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape 1 , a simple web application that allows users to create playlists by providing real-time feedback through liking and skipping patterns. © João Paulo V. Cardoso, Luciana Fujii Pontello, Pedro H. F. Holanda, Bruno Guilherme, Olga Goussevskaia, Ana Paula Couto da Silva."
Andrade N.; Figueiredo F.,Exploring the latent structure of collaborations in music recordings: A case study in jazz,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069998639&partnerID=40&md5=1e41d98f34feb87804f155799c196dc7,"Andrade N., Universidade Federal de Campina Grande, Brazil; Figueiredo F., IBM Research, Brazil","Music records are largely a byproduct of collaborative efforts. Understanding how musicians collaborate to create records provides a step to understand the social production of music. This work leverages recent methods from trajectory mining to investigate how musicians have collaborated over time to record albums. Our case study analyzes data from the Discogs.com database from the Jazz domain. Our analysis examines how to explore the latent structure of collaboration between leading artists or bands and instrumentists over time. Moreover, we leverage the latent structure of our dataset to perform large-scale quantitative analyses of typical collaboration dynamics in different artist communities. © Nazareno Andrade, Flavio Figueiredo."
Liu I.-T.; Randall R.,Predicting missing music components with bidirectional long short-term memory neural networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069995647&partnerID=40&md5=350cb6ccf0e5558b3ae2c93c1b3f30a2,"Liu I.-T., Carnegie Mellon University, School of Music, United States; Randall R., Carnegie Mellon University, School of Music, Center for the Neural Basis of Cognition, United States","Successfully predicting missing components (entire parts or voices) from complex multipart musical textures has attracted researchers of music information retrieval and music theory. However, these applications were limited to either two-part melody and accompaniment (MA) textures or four-part Soprano-Alto-Tenor-Bass (SATB) textures. This paper proposes a robust framework applicable to both textures using a Bidirectional Long-Short Term Memory (BLSTM) recurrent neural network. The BLSTM system was evaluated using frame-wise accuracies on the Nottingham Folk Song dataset and J. S. Bach Chorales. Experimental results demonstrated that adding bidirectional links to the neural network improves prediction accuracy by 3% on average. Specifically, BLSTM outperforms other neural-network based methods by 4.6% on average for four-part SATB and two-part MA textures (employing a transition matrix). The high accuracies obtained with BLSTM on both two-part and four-part textures indicated that BLSTM is the most robust and applicable structure for predicting missing components from multi-part musical textures. © I-Ting Liu, Richard Randall."
Schedl M.; Eghbal-Zadeh H.; Gómez E.; Tkalčič M.,An analysis of agreement in classical music perception and its relationship to listener characteristics,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066072685&partnerID=40&md5=d5c945c1b0914926d857410fc06bc5c6,"Schedl M., Johannes Kepler University, Linz, Austria; Eghbal-Zadeh H., Johannes Kepler University, Linz, Austria; Gómez E., Universitat Pompeu Fabra, Barcelona, Spain; Tkalčič M., Free University of Bozen–Bolzano, Italy","We present a study, carried out on 241 participants, which investigates on classical music material the agreement of listeners on perceptual music aspects (related to emotion, tempo, complexity, and instrumentation) and the relationship between listener characteristics and these aspects. For the currently popular task of music emotion recognition, the former question is particularly important when defining a ground truth of emotions perceived in a given music collection. We characterize listeners via a range of factors, including demographics, musical inclination, experience, and education, and personality traits. Participants rate the music material under investigation, i.e., 15 expert-defined segments of Beethoven’s 3rd symphony, “Eroica”, in terms of 10 emotions, perceived tempo, complexity, and number of instrument groups. Our study indicates only slight agreement on most perceptual aspects, but significant correlations between several listener characteristics and perceptual qualities. © Markus Schedl, Hamid Eghbal-Zadeh, Emilia Gómez, Marko Tkalčič."
Sturm B.L.,Revisiting priorities: Improving MIR evaluation practices,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038946615&partnerID=40&md5=2ffb7fbe000fe85c83d5039e39baf20b,"Sturm B.L., Centre for Digital Music, Queen Mary University of London, United Kingdom","While there is a consensus that evaluation practices in music informatics (MIR) must be improved, there is no consensus about what should be prioritised in order to do so. Priorities include: 1) improving data; 2) improving figures of merit; 3) employing formal statistical testing; 4) employing cross-validation; and/or 5) implementing transparent, central and immediate evaluation. In this position paper, I argue how these priorities treat only the symptoms of the problem and not its cause: MIR lacks a formal evaluation framework relevant to its aims. I argue that the principal priority is to adapt and integrate the formal design of experiments (DOE) into the MIR research pipeline. Since the aim of DOE is to help one produce the most reliable evidence at the least cost, it stands to reason that DOE will make a significant contribution to MIR. Accomplishing this, however, will not be easy, and will require far more effort than is currently being devoted to it. © Bob L. Sturm."
Panteli M.; Benetos E.; Dixon S.,Learning a feature space for similarity in world music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027104271&partnerID=40&md5=a877ba8582188006b323054f960fb110,"Panteli M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this study we investigate computational methods for assessing music similarity in world music. We use state-of-the-art audio features to describe musical content in world music recordings. Our music collection is a subset of the Smithsonian Folkways Recordings with audio examples from 31 countries from around the world. Using supervised and unsupervised dimensionality reduction techniques we learn feature representations for music similarity. We evaluate how well music styles separate in this learned space with a classification experiment. We obtained moderate performance classifying the recordings by country. Analysis of misclassifications revealed cases of geographical or cultural proximity. We further evaluate the learned space by detecting outliers, i.e. identifying recordings that stand out in the collection. We use a data mining technique based on Mahalanobis distances to detect outliers and perform a listening experiment in the ‘odd one out’ style to evaluate our findings. We are able to detect, amongst others, recordings of non-musical content as outliers as well as music with distinct timbral and harmonic content. The listening experiment reveals moderate agreement between subjects’ ratings and our outlier estimation. © Maria Panteli, Emmanouil Benetos, Simon Dixon."
Southall C.; Stables R.; Hockman J.,Automatic drum transcription for polyphonic recordings using soft attention mechanisms and convolutional neural networks,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047935133&partnerID=40&md5=1bff37502f4a0a6dbd40abd5ba7004ef,"Southall C., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Stables R., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Hockman J., DMT Lab, Birmingham City University, Birmingham, United Kingdom","Automatic drum transcription is the process of generating symbolic notation for percussion instruments within audio recordings. To date, recurrent neural network (RNN) systems have achieved the highest evaluation accuracies for both drum solo and polyphonic recordings, however the accuracies within a polyphonic context still remain relatively low. To improve accuracy for polyphonic recordings, we present two approaches to the ADT problem: First, to capture the dynamism of features in multiple time-step hidden layers, we propose the use of soft attention mechanisms (SA) and an alternative RNN configuration containing additional peripheral connections (PC). Second, to capture these same trends at the input level, we propose the use of a convolutional neural network (CNN), which uses a larger set of time-step features. In addition, we propose the use of a bidirectional recurrent neural network (BRNN) in the peak-picking stage. The proposed systems are evaluated along with two state-of-the-art ADT systems in five evaluation scenarios, including a newly-proposed evaluation methodology designed to assess the generalisability of ADT systems. The results indicate that all of the newly proposed systems achieve higher accuracies than the stateof- the-art RNN systems for polyphonic recordings and that the additional BRNN peak-picking stage offers slight improvement in certain contexts. © 2019 Carl Southall, Ryan Stables and Jason Hockman."
Lostanlen V.; Cella C.-E.,Deep convolutional networks on the pitch spiral for music instrument recognition,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050035309&partnerID=40&md5=410f8b9f3cd57c797e7b2bded031f941,"Lostanlen V., École Normale Supérieure, PSL Research University, CNRS, Paris, France; Cella C.-E., École Normale Supérieure, PSL Research University, CNRS, Paris, France","Musical performance combines a wide range of pitches, nuances, and expressive techniques. Audio-based classification of musical instruments thus requires to build signal representations that are invariant to such transformations. This article investigates the construction of learned convolutional architectures for instrument recognition, given a limited amount of annotated training data. In this context, we benchmark three different weight sharing strategies for deep convolutional networks in the time-frequency domain: temporal kernels; time-frequency kernels; and a linear combination of time-frequency kernels which are one octave apart, akin to a Shepard pitch spiral. We provide an acoustical interpretation of these strategies within the source-filter framework of quasi-harmonic sounds with a fixed spectral envelope, which are archetypal of musical notes. The best classification accuracy is obtained by hybridizing all three convolutional layers into a single deep learning architecture. © Vincent Lostanlen and Carmine-Emanuele Cella."
Bantula H.; Giraldo S.; Ramirez R.,Jazz ensemble expressive performance modeling,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069999043&partnerID=40&md5=845c7115930b6185bf90e986b7a6e588,"Bantula H., Music Technology Group, Universitat . Pompeu Fabra, Barcelona, Spain; Giraldo S., Music Technology Group, Universitat . Pompeu Fabra, Barcelona, Spain; Ramirez R., Music Technology Group, Universitat . Pompeu Fabra, Barcelona, Spain","Computational expressive music performance studies the analysis and characterisation of the deviations that a musician introduces when performing a musical piece. It has been studied in a classical context where timing and dynamic deviations are modeled using machine learning techniques. In jazz music, work has been done previously on the study of ornament prediction in guitar performance, as well as in saxophone expressive modeling. However, little work has been done on expressive ensemble performance. In this work, we analysed the musical expressivity of jazz guitar and piano from two different perspectives: solo and ensemble performance. The aim of this paper is to study the influence of piano accompaniment into the performance of a guitar melody and vice versa. Based on a set of recordings made by professional musicians, we extracted descriptors from the score, we transcribed the guitar and the piano performances and calculated performance actions for both instruments. We applied machine learning techniques to train models for each performance action, taking into account both solo and ensemble descriptors. Finally, we compared the accuracy of the induced models. The accuracy of most models increased when ensemble information was considered, which can be explained by the interaction between musicians. © 2016 Bantula, Helena, Giraldo, Sergio, Ramirez, Rafael."
Ycart A.; Benetos E.,A study on LSTM networks for polyphonic music sequence modelling,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044325645&partnerID=40&md5=2ee7901090da14dffc1d209988cec37f,"Ycart A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","Neural networks, and especially long short-term memory networks (LSTM), have become increasingly popular for sequence modelling, be it in text, speech, or music. In this paper, we investigate the predictive power of simple LSTM networks for polyphonic MIDI sequences, using an empirical approach. Such systems can then be used as a music language model which, combined with an acoustic model, can improve automatic music transcription (AMT) performance. As a first step, we experiment with synthetic MIDI data, and we compare the results obtained in various settings, throughout the training process. In particular, we compare the use of a fixed sample rate against a musically-relevant sample rate. We test this system both on synthetic and real MIDI data. Results are compared in terms of note prediction accuracy. We show that the higher the sample rate is, the better the prediction is, because self transitions are more frequent. We suggest that for AMT, a musically-relevant sample rate is crucial in order to model note transitions, beyond a simple smoothing effect. © 2019 Adrien Ycart and Emmanouil Benetos."
Calvo-Zaragoza J.; Vigliensoni G.; Fujinaga I.,"One-step detection of background, staff lines, and symbols in medieval music manuscripts with convolutional neural networks",2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046125397&partnerID=40&md5=52460a0e0a2e606b51aabb02dec73b68,"Calvo-Zaragoza J., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, QC, Canada; Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, QC, Canada","One of the most complex stages of optical music recognition workflows is the detection and isolation of musical symbols. Traditionally, this goal is achieved by performing preprocesses of binarization and staff-line removal. However, these are commonly performed using heuristics that do not generalize widely when applied to different types of documents such as medieval scores. In this paper we propose an effective and generalizable approach to address this problem in one step. Our proposal classifies each pixel of the image among background, staff lines, and symbols using supervised learning techniques, namely convolutional neural networks. Experiments on a set of medieval music pages proved that the proposed approach is very accurate, achieving a performance upwards of 90% and outperforming common ensembles of binarization and staffline removal algorithms. © 2019 Jorge Calvo-Zaragoza, Gabriel Vigliensoni, and Ichiro Fujinaga."
Kinnaird K.M.,Aligned hierarchies: A multi-scale structure-based representation for music-based data streams,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069817882&partnerID=40&md5=081924e949af655380ba6104cc3bf1a9,"Kinnaird K.M., Department of Mathematics, Statistics, and Computer Science, Macalester College, Saint Paul, MN, United States","We introduce aligned hierarchies, a low-dimensional representation for music-based data streams, such as recordings of songs or digitized representations of scores. The aligned hierarchies encode all hierarchical decompositions of repeated elements from a high-dimensional and noisy music-based data stream into one object. These aligned hierarchies can be embedded into a classification space with a natural notion of distance. We construct the aligned hierarchies by finding, encoding, and synthesizing all repeated structure present in a music-based data stream. For a data set of digitized scores, we conducted experiments addressing the fingerprint task that achieved perfect precision-recall values. These experiments provide an initial proof of concept for the aligned hierarchies addressing MIR tasks. © Katherine M. Kinnaird."
Chung C.-H.; Chen Y.; Chen H.,Exploiting playlists for representation of songs and words for text-based music retrieval,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044545821&partnerID=40&md5=c93a824634c8bcd2fb922ea46209addd,"Chung C.-H., National Taiwan University, Taiwan; Chen Y., KKBOX Inc., Taiwan; Chen H., National Taiwan University, Taiwan","As a result of the growth of online music streaming services, a large number of playlists have been created by users and service providers. The title of each playlist provides useful information, such as the theme and listening context, of the songs in the playlist. In this paper, we investigate how to exploit the words extracted from playlist titles for text-based music retrieval. The main idea is to represent songs and words in a common latent space so that the music retrieval is converted to the problem of selecting songs that are the nearest neighbors of the query word in the latent space. Specifically, an unsupervised learning method is proposed to generate a latent representation of songs and words, where the learning objects are the co-occurring songs and words in playlist titles. Five metrics (precision, recall, coherence, diversity, and popularity) are considered for performance evaluation of the proposed method. Qualitative results demonstrate that our method is able to capture the semantic meaning of songs and words, owning to the proximity property of related songs and words in the latent space. © 2019 Chia-Hao Chung, Yian Chen, Homer Chen."
Balke S.; Driedger J.; Abeßer J.; Dittmar C.; Müller M.,Towards evaluating multiple predominant melody annotations in jazz recordings,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066076805&partnerID=40&md5=4a28bce98c0ba461e00483d1253014ce,"Balke S., International Audio Laboratories, Erlangen, Germany; Driedger J., International Audio Laboratories, Erlangen, Germany; Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Dittmar C., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","Melody estimation algorithms are typically evaluated by separately assessing the task of voice activity detection and fundamental frequency estimation. For both subtasks, computed results are typically compared to a single human reference annotation. This is problematic since different human experts may differ in how they specify a predominant melody, thus leading to a pool of equally valid reference annotations. In this paper, we address the problem of evaluating melody extraction algorithms within a jazz music scenario. Using four human and two automatically computed annotations, we discuss the limitations of standard evaluation measures and introduce an adaptation of Fleiss’ kappa that can better account for multiple reference annotations. Our experiments not only highlight the behavior of the different evaluation measures, but also give deeper insights into the melody extraction task. © Stefan Balke, Jakob Abeßer, Jonathan Driedger, Christian Dittmar, Meinard Müller."
Fan J.; Tatar K.; Thorogood M.; Pasquier P.,Ranking-based emotion recognition for experimental music,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047331324&partnerID=40&md5=bdc2424c16a3c81c3ce0e31db11e51bf,"Fan J., Simon Fraser University, Vancouver, BC, Canada; Tatar K., Simon Fraser University, Vancouver, BC, Canada; Thorogood M., Simon Fraser University, Vancouver, BC, Canada; Pasquier P., Simon Fraser University, Vancouver, BC, Canada","Emotion recognition is an open problem in Affective Computing the field. Music emotion recognition (MER) has challenges including variability of musical content across genres, the cultural background of listeners, reliability of ground truth data, and the modeling human hearing in computational domains. In this study, we focus on experimental music emotion recognition. First, we present a music corpus that contains 100 experimental music clips and 40 music clips from 8 musical genres. The dataset (the music clips and annotations) is publicly available at: http://metacreation.net/project/emusic/. Then, we present a crowdsourcing method that we use to collect ground truth via ranking the valence and arousal of music clips. Next, we propose a smoothed RankSVM (SRSVM) method. The evaluation has shown that the SRSVM outperforms four other ranking algorithms. Finally, we analyze the distribution of perceived emotion of experimental music against other genres to demonstrate the difference between genres. © 2019 Jianyu Fan, Kivanç Tatar, Miles Thorogood, Philippe Pasquier."
Cancino-Chacón C.; Grachten M.; Agres K.,From bach to the beatles: The simulation of human tonal expectation using ecologically-trained predictive models,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050636761&partnerID=40&md5=608e41877763431bd867c4560edb3317,"Cancino-Chacón C., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Grachten M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Agres K., Institute of High Performance Computing, ASTAR, Singapore, Singapore","Tonal structure is in part conveyed by statistical regularities between musical events, and research has shown that computational models reflect tonal structure in music by capturing these regularities in schematic constructs like pitch histograms. Of the few studies that model the acquisition of perceptual learning from musical data, most have employed self-organizing models that learn a topology of static descriptions of musical contexts. Also, the stimuli used to train these models are often symbolic rather than acoustically faithful representations of musical material. In this work we investigate whether sequential predictive models of musical memory (specifically, recurrent neural networks), trained on audio from commercial CD recordings, induce tonal knowledge in a similar manner to listeners (as shown in behavioral studies in music perception). Our experiments indicate that various types of recurrent neural networks produce musical expectations that clearly convey tonal structure. Furthermore, the results imply that although implicit knowledge of tonal structure is a necessary condition for accurate musical expectation, the most accurate predictive models also use other cues beyond the tonal structure of the musical context. © 2019 Carlos Cancino-Chacón, Maarten Grachten, Kat Agres."
Hori G.; Sagayama S.,Minimax viterbi algorithm for HMM-based guitar fingering decision,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936599&partnerID=40&md5=09a9c80d2f036fd5932d3e7e917cd522,"Hori G., Asia University, RIKEN, Japan; Sagayama S., Meiji University, Japan","Previous works on automatic fingering decision for string instruments have been mainly based on path optimization by minimizing the difficulty of a whole phrase that is typically defined as the sum of the difficulties of moves required for playing the phrase. However, from a practical viewpoint of beginner players, it is more important to minimize the maximum difficulty of a move required for playing the phrase, that is, to make the most difficult move easier. To this end, we introduce a variant of the Viterbi algorithm (termed the “minimax Viterbi algorithm”) that finds the path of the hidden states that maximizes the minimum transition probability (not the product of the transition probabilities) and apply it to HMM-based guitar fingering decision. We compare the resulting fingerings by the conventional Viterbi algorithm and our proposed minimax Viterbi algorithm to show the appropriateness of our new method. © Gen Hori, Shigeki Sagayama."
"Hajič J., jr.; Novotný J.; Pecina P.; Pokorný J.",Further steps towards a standard testbed for optical music recognition,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045196546&partnerID=40&md5=25c08e64173a7a53e49b49c9e71d8ed4,"Hajič J., jr., Charles University, Institute of Formal and Applied Linguistics, Czech Republic; Novotný J., Charles University, Department of Software Engineering, Czech Republic; Pecina P., Charles University, Institute of Formal and Applied Linguistics, Czech Republic; Pokorný J., Charles University, Department of Software Engineering, Czech Republic","Evaluating Optical Music Recognition (OMR) is notoriously difficult and automated end-to-end OMR evaluation metrics are not available to guide development. In “Towards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images”, Byrd and Simonsen recently stress that a benchmarking standard is needed in the OMR community, both with regards to data and evaluation metrics. We build on their analysis and definitions and present a prototype of an OMR benchmark. We do not, however, presume to present a complete solution to the complex problem of OMR benchmarking. Our contributions are: (a) an attempt to define a multilevel OMR benchmark dataset and a practical prototype implementation for both printed and handwritten scores, (b) a corpus-based methodology for assessing automated evaluation metrics, and an underlying corpus of over 1000 qualified relative cost-to-correct judgments. We then assess several straightforward automated MusicXML evaluation metrics against this corpus to establish a baseline over which further metrics can improve. © Jan Hajič jr., Jiří Novotný, Pavel Pecina, Jaroslav Pokorný."
Dzhambazov G.; Srinivasamurthy A.; Şentürk S.; Serra X.,On the use of note onsets for improved lyrics-to-audio alignment in Turkish Makam music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069940317&partnerID=40&md5=2133b87ad055e74d67a6ff4fd716acfe,"Dzhambazov G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Lyrics-to-audio alignment aims to automatically match given lyrics and musical audio. In this work we extend a state of the art approach for lyrics-to-audio alignment with information about note onsets. In particular, we consider the fact that transition to next lyrics syllable usually implies transition to a new musical note. To this end we formulate rules that guide the transition between consecutive phonemes when a note onset is present. These rules are incorporated into the transition matrix of a variable-time hidden Markov model (VTHMM) phonetic recognizer based on MFCCs. An estimated melodic contour is input to an automatic note transcription algorithm, from which the note onsets are derived. The proposed approach is evaluated on 12 a cappella audio recordings of Turkish Makam music using a phrase-level accuracy measure. Evaluation of the alignment is also presented on a polyphonic version of the dataset in order to assess how degradation in the extracted onsets affects performance. Results show that the proposed model outperforms a baseline approach unaware of onset transition rules. To the best of our knowledge, this is the one of the first approaches tackling lyrics tracking, which combines timbral features with a melodic feature in the alignment process itself. © Georgi Dzhambazov, Ajay Srinivasamurthy, Sertan Şentürk, Xavier Serra ."
Wu C.-W.; Lerch A.,Automatic drum transcription using the student-teacher learning paradigm with unlabeled music data,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047918161&partnerID=40&md5=9dd53c41582b87036079647dcc8a3453,"Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","Automatic drum transcription is a sub-task of automatic music transcription that converts drum-related audio events into musical notation. While noticeable progress has been made in the past by combining pattern recognition methods with audio signal processing techniques, the major limitation of many state-of-the-art systems still originates from the difficulty of obtaining a meaningful amount of annotated data to support the data-driven algorithms. In this work, we address the challenge of insufficiently labeled data by exploring the possibility of utilizing unlabeled music data from online resources. Specifically, a student neural network is trained using the labels generated from multiple teacher systems. The performance of the model is evaluated on a publicly available dataset. The results show the general viability of using unlabeled music data to improve the performance of drum transcription systems. © 2019 Chih-Wei Wu, Alexander Lerch."
Kum S.; Oh C.; Nam J.,Melody extraction on vocal segments using multi-column deep neural networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070016484&partnerID=40&md5=14cb76caf431c119af14d673c745d607,"Kum S., Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, South Korea; Oh C., Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, South Korea; Nam J., Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, South Korea","Singing melody extraction is a task that tracks pitch contour of singing voice in polyphonic music. While the majority of melody extraction algorithms are based on computing a saliency function of pitch candidates or separating the melody source from the mixture, data-driven approaches based on classification have been rarely explored. In this paper, we present a classification-based approach for melody extraction on vocal segments using multi-column deep neural networks. In the proposed model, each of neural networks is trained to predict a pitch label of singing voice from spectrogram, but their outputs have different pitch resolutions. The final melody contour is inferred by combining the outputs of the networks and post-processing it with a hidden Markov model. In order to take advantage of the data-driven approach, we also augment training data by pitch-shifting the audio content and modifying the pitch label accordingly. We use the RWC dataset and vocal tracks of the MedleyDB dataset for training the model and evaluate it on the ADC 2004, MIREX 2005 and MIR-1k datasets. Through several settings of experiments, we show incremental improvements of the melody prediction. Lastly, we compare our best result to those of previous state-of-the-arts. © Sangeun Kum, Changheun Oh, Juhan Nam."
Fournier-S’niehotta R.; Rigaux P.; Travers N.,Querying XML score databases: Xquery is not enough!,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038881602&partnerID=40&md5=7bfc87285504dd3b6f4780d1de133e30,"Fournier-S’niehotta R., CNAM, France; Rigaux P., CNAM, France; Travers N., CNAM, France","The paper addresses issues related to the design of query languages for searching and restructuring collections of XML-encoded music scores. We advocate against a direct approach based on XQuery, and propose a more powerful strategy that first extracts a structured representation of music notation from score encodings, and then manipulates this representation in closed form with dedicated operators. The paper exposes the content model, the resulting language, and describes our implementation on top of a large Digital Score Library (DSL). © Raphaël Fournier-S’niehotta, Philippe Rigaux, Nicolas Travers."
Hennequin R.; Rigaud F.,Long-term reverberation modeling for under-determined audio source separation with application to vocal melody extraction,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046989676&partnerID=40&md5=dca5f48be8ccb5eb8ae18eaec1dbf0ae,"Hennequin R., Deezer R and D, 10 rue d’Athènes, Paris, 75009, France; Rigaud F., Audionamix R and D, 171 quai de Valmy, Paris, 75010, France","In this paper, we present a way to model long-term reverberation effects in under-determined source separation algorithms based on a non-negative decomposition framework. A general model for the sources affected by reverberation is introduced and update rules for the estimation of the parameters are presented. Combined with a well-known source-filter model for singing voice, an application to the extraction of reverberated vocal tracks from polyphonic music signals is proposed. Finally, an objective evaluation of this application is described. Performance improvements are obtained compared to the same model without reverberation modeling, in particular by significantly reducing the amount of interference between sources. © Romain Hennequin, François Rigaud."
Font F.; Serra X.,Tempo estimation for music loops and a simple confidence measure,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060925975&partnerID=40&md5=bca0555eaf9d785918b14868202908ab,"Font F., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Tempo estimation is a common task within the music information retrieval community, but existing works are rarely evaluated with datasets of music loops and the algorithms are not tailored to this particular type of content. In addition to this, existing works on tempo estimation do not put an emphasis on providing a confidence value that indicates how reliable their tempo estimations are. In current music creation contexts, it is common for users to search for and use loops shared in online repositories. These loops are typically not produced by professionals and lack annotations. Hence, the existence of reliable tempo estimation algorithms becomes necessary to enhance the reusability of loops shared in such repositories. In this paper, we test six existing tempo estimation algorithms against four music loop datasets containing more than 35k loops. We also propose a simple and computationally cheap confidence measure that can be applied to any existing algorithm to estimate the reliability of their tempo predictions when applied to music loops. We analyse the accuracy of the algorithms in combination with our proposed confidence measure, and see that we can significantly improve the algorithms’ performance when only considering music loops with high estimated confidence. © Frederic Font and Xavier Serra."
Nakano T.; Mochihashi D.; Yoshii K.; Goto M.,Musical typicality: How many similar songs exist?,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035074897&partnerID=40&md5=b3b7496b30e484557b24c3cbb030c105,"Nakano T., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Mochihashi D., Institute of Statistical Mathematics, Japan; Yoshii K., Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We propose a method for estimating the musical “typicality” of a song from an information theoretic perspective. While musical similarity compares just two songs, musical typicality quantifies how many of the songs in a set are similar. It can be used not only to express the uniqueness of a song but also to recommend one that is representative of a set. Building on the type theory in information theory (Cover & Thomas 2006), we use a Bayesian generative model of musical features and compute the typicality of a song as the sum of the probabilities of the songs that share the type of the given song. To evaluate estimated results, we focused on vocal timbre which can be evaluated quantitatively by using the singer’s gender. Estimated typicality is evaluated against the Pearson correlation coefficient between the computed typicality and the ratio of the number of male singers to female singers of a song-set. Our result shows that the proposed measure works more effectively to estimate musical typicality than the previous model based simply on generative probabilities. © Tomoyasu Nakano, Daichi Mochihashi, Kazuyoshi Yoshii, Masataka Goto."
Su L.; Chuang T.-Y.; Yang Y.-H.,"Exploiting frequency, periodicity and harmonicity using advanced time-frequency concentration techniques for multipitch estimation of choir and symphony",2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070011351&partnerID=40&md5=d7ed69ab00ba0b62515a6cfc1189e988,"Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Chuang T.-Y., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","To advance research on automatic music transcription (AMT), it is important to have labeled datasets with sufficient diversity and complexity that support the creation and evaluation of robust algorithms to deal with issues seen in real-world polyphonic music signals. In this paper, we propose new datasets and investigate signal processing algorithms for multipitch estimation (MPE) in choral and symphony music, which have been seldom considered in AMT research. We observe that MPE in these two types of music is challenging because of not only the high polyphony number, but also the possible imprecision in pitch for notes sung or played by multiple singers or musicians in unison. To improve the robustness of pitch estimation, experiments show that it is beneficial to measure pitch saliency by jointly considering frequency, periodicity and harmonicity information. Moreover, we can improve the localization and stability of pitch by the multi-taper methods and nonlinear time-frequency reassignment techniques such as the Concentration of Time and Frequency (ConceFT) transform. We show that the proposed unsupervised methods to MPE compare favorably with, if not superior to, state-of-the-art supervised methods in various types of music signals from both existing and the newly created datasets. © Li Su, Tsung-Ying Chuang and Yi-Hsuan Yang."
Chen L.; Stolterman E.; Raphael C.,Human-interactive optical music recognition,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041410266&partnerID=40&md5=19200f47abaaa99d5728f065db68d332,"Chen L., Indiana University Bloomington, United States; Stolterman E., Indiana University Bloomington, United States; Raphael C., Indiana University Bloomington, United States","We propose a human-driven Optical Music Recognition (OMR) system that creates symbolic music data from common Western notation scores. Despite decades of development, OMR still remains largely unsolved as state-of-the-art automatic systems are unable to give reliable and useful results on a wide range of documents. For this reason our system, Ceres, combines human input and machine recognition to efficiently generate high-quality symbolic data. We propose a scheme for human-in-the-loop recognition allowing the user to constrain the recognition in two ways. The human actions allow the user to impose either a pixel labeling or model constraint, while the system re-recognizes subject to these constraints. We present evaluation based on different users’ log data using both Ceres and Sibelius software to produce the same music documents. We conclude that our system shows promise for transcribing complicated music scores with high accuracy. © Liang Chen, Erik Stolterman, Christopher Raphael."
Korzeniowski F.; Widmer G.,Feature learning for chord recognition: The deep chroma extractor,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054251070&partnerID=40&md5=729ab5cf5412bfe2ff378ae994c3cba4,"Korzeniowski F., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","We explore frame-level audio feature learning for chord recognition using artificial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artificial neural networks. It is trained to compute chroma features that encode harmonic information important for chord recognition, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and resolve harmonic ambiguities. We compare the resulting features to hand-crafted ones by using a simple linear frame-wise classifier for chord recognition on various data sets. The results show that the learned feature extractor produces superior chroma vectors for chord recognition. © Filip Korzeniowski and Gerhard Widmer."
Guiomard-Kagan N.; Giraud M.; Groult R.; Levé F.,Improving voice separation by better connecting contigs,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063466257&partnerID=40&md5=7b0b659d18c05c4a879322ee7dbfc232,"Guiomard-Kagan N., MIS, Univ. Picardie Jules Verne, Amiens, France; Giraud M., CRIStAL, UMR CNRS 9189, Univ. Lille, Lille, France; Groult R., MIS, Univ. Picardie Jules Verne, Amiens, France; Levé F., MIS, Univ. Picardie Jules Verne, Amiens, France, CRIStAL, UMR CNRS 9189, Univ. Lille, Lille, France","Separating a polyphonic symbolic score into monophonic voices or streams helps to understand the music and may simplify further pattern matching. One of the best ways to compute this separation, as proposed by Chew and Wu in 2005 [2], is to first identify contigs that are portions of the music score with a constant number of voices, then to progressively connect these contigs. This raises two questions: Which contigs should be connected first? And, how should these two contigs be connected? Here we propose to answer simultaneously these two questions by considering a set of musical features that measures the quality of any connection. The coefficients weighting the features are optimized through a genetic algorithm. We benchmark the resulting connection policy on corpora containing fugues of the Well-Tempered Clavier by J. S. Bach as well as on string quartets, and we compare it against previously proposed policies [2, 9]. The contig connection is improved, particularly when one takes into account the whole content of voice fragments to assess the quality of their possible connection. © Nicolas Guiomard-Kagan, Mathieu Giraud, Richard Groult, Florence Levé."
Wu C.-W.; Lerch A.,On drum playing technique detection in polyphonic mixtures,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047948804&partnerID=40&md5=dd940b0faf169ae77c52270a917f0dc1,"Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","In this paper, the problem of drum playing technique detection in polyphonic mixtures of music is addressed. We focus on the identification of 4 rudimentary techniques: strike, buzz roll, flam, and drag. The specifics and the challenges of this task are being discussed, and different sets of features are compared, including various features extracted from NMF-based activation functions, as well as baseline spectral features. We investigate the capabilities and limitations of the presented system in the case of real-world recordings and polyphonic mixtures. To design and evaluate the system, two datasets are introduced: a training dataset generated from individual drum hits, and additional annotations of the well-known ENST drum dataset minus one subset as test dataset. The results demonstrate issues with the traditionally used spectral features, and indicate the potential of using NMF activation functions for playing technique detection, however, the performance of polyphonic music still leaves room for future improvement. © Chih-Wei Wu, Alexander Lerch."
Buccoli M.; Zanoni M.; Fazekas G.; Sarti A.; Sandler M.,A higher-dimensional expansion of affective norms for english terms for music tagging,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070017560&partnerID=40&md5=d09acb6afd7f8d2125a60ef108893364,"Buccoli M., Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy; Zanoni M., Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy; Fazekas G., Centre For Digital Music, Queen Mary, University of London, United Kingdom; Sarti A., Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy; Sandler M., Centre For Digital Music, Queen Mary, University of London, United Kingdom","The Valence, Arousal and Dominance (VAD) model for emotion representation is widely used in music analysis. The ANEW dataset is composed of more than 2000 emotion related descriptors annotated in the VAD space. However, due to the low number of dimensions of the VAD model, the distribution of terms of the ANEW dataset tends to be compact and cluttered. In this work, we aim at finding a possibly higher-dimensional transformation of the VAD space, where the terms of the ANEW dataset are better organised conceptually and bear more relevance to music tagging. Our approach involves the use of a kernel expansion of the ANEW dataset to exploit a higher number of dimensions, and the application of distance learning techniques to find a distance metric that is consistent with the semantic similarity among terms. In order to train the distance learning algorithms, we collect information on the semantic similarity from human annotation and editorial tags. We evaluate the quality of the method by clustering the terms in the found high-dimensional domain. Our approach exhibits promising results with objective and subjective performance metrics, showing that a higher dimensional space could be useful to model semantic similarity among terms of the ANEW dataset. © Michele Buccoli, Massimiliano Zanoni, György Fazekas, Augusto Sarti, Mark Sandler."
Demetriou A.; Larson M.; Liem C.C.S.,Go with the flow: When listeners use music as technology,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045467222&partnerID=40&md5=9e51a92df9f326b27eee48fe29380e62,"Demetriou A., Delft University of Technology, Delft, Netherlands; Larson M., Delft University of Technology, Delft, Netherlands, Radboud University, Nijmegen, Netherlands; Liem C.C.S., Delft University of Technology, Delft, Netherlands","Music has been shown to have a profound effect on listeners’ internal states as evidenced by neuroscience research. Listeners report selecting and listening to music with specific intent, thereby using music as a tool to achieve desired psychological effects within a given context. In light of these observations, we argue that music information retrieval research must revisit the dominant assumption that listening to music is only an end unto itself. Instead, researchers should embrace the idea that music is also a technology used by listeners to achieve a specific desired internal state, given a particular set of circumstances and a desired goal. This paper focuses on listening to music in isolation (i.e., when the user listens to music by themselves with headphones) and surveys research from the fields of social psychology and neuroscience to build a case for a new line of research in music information retrieval on the ability of music to produce flow states in listeners. We argue that interdisciplinary collaboration is necessary in order to develop the understanding and techniques necessary to allow listeners to exploit the full potential of music as psychological technology. © Andrew Demetriou, Martha Larson, Cynthia C. S. Liem."
Schreiber H.,Genre ontology learning: Comparing curated with crowd-sourced ontologies,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069803074&partnerID=40&md5=9c606212d21409daaf427cdfbd95d166,"Schreiber H., Tagtraum Industries Incorporated, United States","The Semantic Web has made it possible to automatically find meaningful connections between musical pieces which can be used to infer their degree of similarity. Similarity in turn, can be used by recommender systems driving music discovery or playlist generation. One useful facet of knowledge for this purpose are fine-grained genres and their inter-relationships. In this paper we present a method for learning genre ontologies from crowd-sourced genre labels, exploiting genre co-occurrence rates. Using both lexical and conceptual similarity measures, we show that the quality of such learned ontologies is comparable with manually created ones. In the process, we document properties of current reference genre ontologies, in particular a high degree of disconnectivity. Further, motivated by shortcomings of the established taxonomic precision measure, we define a novel measure for highly disconnected ontologies. © Hendrik Schreiber."
Pachet F.; Papadopoulos A.; Roy P.,Sampling variations of sequences for structured music generation,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051464943&partnerID=40&md5=2b5ad1d5d2f03b1ee5164376e686aae5,"Pachet F., Sony CSL Paris, France; Papadopoulos A., UPMC Univ Paris 06, UMR 7606, LIP6, France; Roy P., Sony CSL Paris, France","Recently, machine-learning techniques have been successfully used for the generation of complex artifacts such as music or text. However, these techniques are still unable to capture and generate artifacts that are convincingly structured. In particular, musical sequences do not exhibit pattern structure, as typically found in human composed music. We present an approach to generate structured sequences, based on a mechanism for sampling efficiently variations of musical sequences. Given an input sequence and a statistical model, this mechanism uses belief propagation to sample a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity function defined by Mongeau and Sankoff. We then show how this mechanism can be used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem. We illustrate our approach with a convincingly structured generated lead sheet in the style of the Beatles. © 2019 François Pachet, Alexandre Papadopoulos, Pierre Roy."
Silva D.F.; Yeh C.-C.M.; Batista G.E.A.P.A.; Keogh E.,SiMPle: Assessing music similarity using subsequences joins,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038877908&partnerID=40&md5=1d84c7b57299a5295a85bca200a9cbad,"Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil, Department of Computer Science and Engineering, University of California, Riverside, United States; Yeh C.-C.M., Department of Computer Science and Engineering, University of California, Riverside, United States; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Keogh E., Department of Computer Science and Engineering, University of California, Riverside, United States","Most algorithms for music information retrieval are based on the analysis of the similarity between feature sets extracted from the raw audio. A common approach to assessing similarities within or between recordings is by creating similarity matrices. However, this approach requires quadratic space for each comparison and typically requires a costly post-processing of the matrix. In this work, we propose a simple and efficient representation based on a subsequence similarity join, which may be used in several music information retrieval tasks. We apply our method to the cover song recognition problem and demonstrate that it is superior to state-of-the-art algorithms. In addition, we demonstrate how the proposed representation can be exploited for multiple applications in music processing. © Diego F. Silva, Chin-Chia M. Yeh, Gustavo E. A. P. A. Batista, Eamonn Keogh."
Driedger J.; Balke S.; Ewert S.; Müller M.,Template-based vibrato analysis of music signals,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028042493&partnerID=40&md5=d7e926624fb2b3273d92860ebcee69de,"Driedger J., International Audio Laboratories, Erlangen, Germany; Balke S., International Audio Laboratories, Erlangen, Germany; Ewert S., Queen Mary University of London, United Kingdom; Müller M., International Audio Laboratories, Erlangen, Germany","The automated analysis of vibrato in complex music signals is a highly challenging task. A common strategy is to proceed in a two-step fashion. First, a fundamental frequency (F0) trajectory for the musical voice that is likely to exhibit vibrato is estimated. In a second step, the trajectory is then analyzed with respect to periodic frequency modulations. As a major drawback, however, such a method cannot recover from errors made in the inherently difficult first step, which severely limits the performance during the second step. In this work, we present a novel vibrato analysis approach that avoids the first error-prone F0-estimation step. Our core idea is to perform the analysis directly on a signal’s spectrogram representation where vibrato is evident in the form of characteristic spectro-temporal patterns. We detect and parameterize these patterns by locally comparing the spectrogram with a predefined set of vibrato templates. Our systematic experiments indicate that this approach is more robust than F0-based strategies. © Jonathan Driedger, Stefan Balke, Sebastian Ewert, Meinard Müller."
Velarde G.; Weyde T.; Chacón C.C.; Meredith D.; Grachten M.,Composer recognition based on 2D-filtered piano-rolls,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037338117&partnerID=40&md5=32669c6d7ebafd83c1ed24991c4d1650,"Velarde G., Department of Architecture Design and Media Technology, Aalborg University, Denmark; Weyde T., Department of Computer Science, City University London, United Kingdom; Chacón C.C., Austrian Research Institute for Artificial Intelligence, Austria; Meredith D., Department of Architecture Design and Media Technology, Aalborg University, Denmark; Grachten M., Austrian Research Institute for Artificial Intelligence, Austria","We propose a method for music classification based on the use of convolutional models on symbolic pitch–time representations (i.e. piano-rolls) which we apply to composer recognition. An excerpt of a piece to be classified is first sampled to a 2D pitch–time representation which is then subjected to various transformations, including convolution with predefined filters (Morlet or Gaussian) and classified by means of support vector machines. We combine classifiers based on different pitch representations (MIDI and morphetic pitch) and different filter types and configurations. The method does not require parsing of the music into separate voices, or extraction of any other predefined features prior to processing; instead it is based on the analysis of texture in a 2D pitch–time representation. We show that filtering significantly improves recognition and that the method proves robust to encoding, transposition and amount of information. On discriminating between Haydn and Mozart string quartet movements, our best classifier reaches state-of-the-art performance in leave-one-out cross validation. © Gissel Velarde, Carlos Cancino Chacón, Tillman Weyde, David Meredith, Maarten Grachten."
Yang L.; Rajab K.Z.; Chew E.,AVA: An interactive system for visual and quantitative analyses of vibrato and portamento performance styles,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065063893&partnerID=40&md5=3d9524b37b598b845eefe352beae4445,"Yang L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Rajab K.Z., Antennas and Electromagnetics Group, Queen Mary University of London, United Kingdom; Chew E., Centre for Digital Music, Queen Mary University of London, United Kingdom","Vibratos and portamenti are important expressive features for characterizing performance style on instruments capable of continuous pitch variation such as strings and voice. Accurate study of these features is impeded by time consuming manual annotations. We present AVA, an interactive tool for automated detection, analysis, and visualization of vibratos and portamenti. The system implements a Filter Diagonalization Method (FDM)-based and a Hidden Markov Model-based method for vibrato and portamento detection. Vibrato parameters are reported directly from the FDM, and portamento parameters are given by the best fit Logistic Model. The graphical user interface (GUI) allows the user to edit the detection results, to view each vibrato or portamento, and to read the output parameters. The entire set of results can also be written to a text file for further statistical analysis. Applications of AVA include music summarization, similarity assessment, music learning, and musicological analysis. We demonstrate AVA’s utility by using it to analyze vibratos and portamenti in solo performances of two Beijing opera roles and two string instruments, erhu and violin. © Luwei Yang, Khalid Z. Rajab and Elaine Chew."
Koops H.V.; Bas de Haas W.; Bountouridis D.; Volk A.,Integration and quality assessment of heterogeneous chord sequences using data fusion,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069003712&partnerID=40&md5=3120e3369143d07e6124d76fab5c2113,"Koops H.V., Department of Information and Computing Sciences, Utrecht University, Netherlands; Bas de Haas W., Chordify, Netherlands; Bountouridis D., Department of Information and Computing Sciences, Utrecht University, Netherlands; Volk A., Department of Information and Computing Sciences, Utrecht University, Netherlands","Two heads are better than one, and the many are smarter than the few. Integrating knowledge from multiple sources has shown to increase retrieval and classification accuracy in many domains. The recent explosion of crowd-sourced information, such as on websites hosting chords and tabs for popular songs, calls for sophisticated algorithms for data-driven quality assessment and data integration to create better, and more reliable data. In this paper, we propose to integrate the heterogeneous output of multiple automatic chord extraction algorithms using data fusion. First we show that data fusion creates significantly better chord label sequences from multiple sources, outperforming its source material, majority voting and random source integration. Second, we show that data fusion is capable of assessing the quality of sources with high precision from source agreement, without any ground-truth knowledge. Our study contributes to a growing body of work showing the benefits of integrating knowledge from multiple sources in an advanced way. © Hendrik Vincent Koops, W. Bas de Haas, Dimitrios Bountouridis, Anja Volk."
Andersen K.; Knees P.,Conversations with expert users in music retrieval and research challenges for creative MIR,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027199576&partnerID=40&md5=6be4653e8e0d91848b0ec351c83e5654,"Andersen K., Studio for Electro Instrumental Music (STEIM), Amsterdam, Netherlands; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","Sample retrieval remains a central problem in the creative process of making electronic dance music. This paper describes the findings from a series of interview sessions involving users working creatively with electronic music. We conducted in-depth interviews with expert users on location at the Red Bull Music Academies in 2014 and 2015. When asked about their wishes and expectations for future technological developments in interfaces, most participants mentioned very practical requirements of storing and retrieving files. A central aspect of the desired systems is the need to provide increased flow and unbroken periods of concentration and creativity. From the interviews, it becomes clear that for Creative MIR, and in particular, for music interfaces for creative expression, traditional requirements and paradigms for music and audio retrieval differ to those from consumer-centered MIR tasks such as playlist generation and recommendation and that new paradigms need to be considered. Despite all technical aspects being controllable by the experts themselves, searching for sounds to use in composition remains a largely semantic process. From the outcomes of the interviews, we outline a series of possible conclusions and areas and pose two research challenges for future developments of sample retrieval interfaces in the creative domain. © Kristina Andersen, Peter Knees."
Tian M.; Sandler M.B.,Music structural segmentation across genres with Gammatone features,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070001383&partnerID=40&md5=324c63ddce05fa3518d262bfb023f51a,"Tian M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","Music structural segmentation (MSS) studies to date mainly employ audio features describing the timbral, harmonic or rhythmic aspects of the music and are evaluated using datasets consisting primarily of Western music. A new dataset of Chinese traditional Jingju music with structural annotations is introduced in this paper to complement the existing evaluation framework. We discuss some statistics of the annotations analysing the inter-annotator agreements. We present two auditory features derived from the Gammatone filters based respectively on the cepstral analysis and the spectral contrast description. The Gammatone features and two commonly used features, Mel-frequency cepstral coefficients (MFCCs) and chromagram, are evaluated on the Jingju dataset as well as two existing used ones using several state-of-the-art algorithms. The investigated Gammatone features outperform MFCCs and chromagram when evaluated on the Jingju dataset and show similar performance with the Western datasets. We identify the presented Gammatone features as effective structure descriptors, especially for music lacking notable timbral or harmonic sectional variations. Results also indicate that the design of audio features and segmentation algorithms should be adapted to specific music genres to interpret individual structural patterns. © Mi Tian, Mark B. Sandler."
Chung C.-H.; Lou J.-K.; Chen H.,"A latent representation of users, sessions, and songs for listening behavior analysis",2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027147737&partnerID=40&md5=0d34afa36d3d3a8c8e5c1ab7c8605165,"Chung C.-H., National Taiwan University, Taiwan; Lou J.-K., KKBOX Inc, Taiwan; Chen H., National Taiwan University, Taiwan","Understanding user listening behaviors is important to the personalization of music recommendation. In this paper, we present an approach that discovers user behavior from a large-scale, real-world listening record. The proposed approach generates a latent representation of users, listening sessions, and songs, where each of these objects is represented as a point in the multi-dimensional latent space. Since the distance between two points is an indication of the similarity of the two corresponding objects, it becomes extremely simple to evaluate the similarity between songs or the matching of songs with the user preference. By exploiting this feature, we provide a two-dimensional user behavior analysis framework for music recommendation. Exploring the relationships between user preference and the contextual or temporal information in the session data through this framework significantly facilitates personalized music recommendation. We provide experimental results to illustrate the strengths of the proposed approach for user behavior analysis. © Chia-Hao Chung, Jing-Kai Lou, Homer Chen."
Gray P.; Bunescu R.,A neural greedy model for voice separation in symbolic music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037737122&partnerID=40&md5=05b01292792a1dacc6ad52ec939ae421,"Gray P., School of EECS, Ohio University, Athens, OH, United States; Bunescu R., School of EECS, Ohio University, Athens, OH, United States","Music is often experienced as a simultaneous progression of multiple streams of notes, or voices. The automatic separation of music into voices is complicated by the fact that music spans a voice-leading continuum ranging from monophonic, to homophonic, to polyphonic, often within the same work. We address this diversity by defining voice separation as the task of partitioning music into streams that exhibit both a high degree of external perceptual separation from the other streams and a high degree of internal perceptual consistency, to the maximum degree that is possible in the given musical input. Equipped with this task definition, we manually annotated a corpus of popular music and used it to train a neural network with one hidden layer that is connected to a diverse set of perceptually informed input features. The trained neural model greedily assigns notes to voices in a left to right traversal of the input chord sequence. When evaluated on the extraction of consecutive within voice note pairs, the model obtains over 91% F-measure, surpassing a strong baseline based on an iterative application of an envelope extraction function. © Patrick Gray, Razvan Bunescu."
Creager E.; Stein N.D.; Badeau R.; Depalle P.,Nonnegative tensor factorization with frequency modulation cues for blind audio source separation,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061218406&partnerID=40&md5=36a0456f4f6dce8d512df53ca69e07ab,"Creager E., Analog Devices Lyric Labs, Cambridge, MA, United States, CIRMMT, McGill University, Montréal, Canada; Stein N.D., Analog Devices Lyric Labs, Cambridge, MA, United States; Badeau R., LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, Paris, France, CIRMMT, McGill University, Montréal, Canada; Depalle P., CIRMMT, McGill University, Montréal, Canada","We present Vibrato Nonnegative Tensor Factorization, an algorithm for single-channel unsupervised audio source separation with an application to separating instrumental or vocal sources with nonstationary pitch from music recordings. Our approach extends Nonnegative Matrix Factorization for audio modeling by including local estimates of frequency modulation as cues in the separation. This permits the modeling and unsupervised separation of vibrato or glissando musical sources, which is not possible with the basic matrix factorization formulation. The algorithm factorizes a sparse nonnegative tensor comprising the audio spectrogram and local frequency-slope-to-frequency ratios, which are estimated at each time-frequency bin using the Distributed Derivative Method. The use of local frequency modulations as separation cues is motivated by the principle of common fate partial grouping from Auditory Scene Analysis, which hypothesizes that each latent source in a mixture is characterized perceptually by coherent frequency and amplitude modulations shared by its component partials. We derive multiplicative factor updates by Minorization-Maximization, which guarantees convergence to a local optimum by iteration. We then compare our method to the baseline on two separation tasks: one considers synthetic vibrato notes, while the other considers vibrato string instrument recordings. © Elliot Creager, Noah D. Stein, Roland Badeau, Philippe Depalle."
Kelz R.; Dorfer M.; Korzeniowski F.; Böck S.; Arzt A.; Widmer G.,On the potential of simple framewise approaches to piano transcription,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069983069&partnerID=40&md5=b76eabad7894eeb717e52cc82a4bce82,"Kelz R., Department of Computational Perception, Johannes Kepler University Linz, Austria; Dorfer M., Department of Computational Perception, Johannes Kepler University Linz, Austria; Korzeniowski F., Department of Computational Perception, Johannes Kepler University Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset – without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve. © Rainer Kelz, Matthias Dorfer, Filip Korzeniowski, Sebastian Böck, Andreas Arzt, Gerhard Widmer."
Holzapfel A.; Grill T.,Bayesian meter tracking on learned signal representations,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069900283&partnerID=40&md5=f22dfcb22728a02e4e76a93aa11d29e1,"Holzapfel A., Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Grill T., Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Most music exhibits a pulsating temporal structure, known as meter. Consequently, the task of meter tracking is of great importance for the domain of Music Information Retrieval. In our contribution, we specifically focus on Indian art musics, where meter is conceptualized at several hierarchical levels, and a diverse variety of metrical hierarchies exist, which poses a challenge for state of the art analysis methods. To this end, for the first time, we combine Convolutional Neural Networks (CNN), allowing to transcend manually tailored signal representations, with subsequent Dynamic Bayesian Tracking (BT), modeling the recurrent metrical structure in music. Our approach estimates meter structures simultaneously at two metrical levels. The results constitute a clear advance in meter tracking performance for Indian art music, and we also demonstrate that these results generalize to a set of Ballroom dances. Furthermore, the incorporation of neural network output allows a computationally efficient inference. We expect the combination of learned signal representations through CNNs and higher-level temporal modeling to be applicable to all styles of metered music, provided the availability of sufficient training data. © Andre Holzapfel, Thomas Grill."
Gregorio J.; Kim Y.E.,Phrase-level audio segmentation of jazz improvisations informed by symbolic data,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064122871&partnerID=40&md5=549a3db6029d9e29471bfbb62cd7c8e7,"Gregorio J., Drexel University, Dept. of Electrical and Computer Engineering, United States; Kim Y.E., Drexel University, Dept. of Electrical and Computer Engineering, United States","Computational music structure analysis encompasses any model attempting to organize music into qualitatively salient structural units, which can include anything in the heirarchy of large scale form, down to individual phrases and notes. While much existing audio-based segmentation work attempts to capture repetition and homogeneity cues useful at the form and thematic level, the time scales involved in phrase-level segmenation and the avoidance of repetition in improvised music necessitate alternate approaches in approaching jazz structure analysis. Recently, the Weimar Jazz Database has provided transcriptions of solos by a variety of eminent jazz performers. Utilizing a subset of these transcriptions aligned to their associated audio sources, we propose a model based on supervised training of a Hidden Markov Model with ground-truth state sequences designed to encode melodic contours appearing frequently in jazz improvisations. Results indicate that representing likely melodic contours in this way allows a low-level audio feature set containing primarily timbral and harmonic information to more accurately predict phrase boundaries. © Jeff Gregorio and Youngmoo E. Kim."
Figueiredo F.; Ribeiro B.; Faloutsos C.; Andrade N.; Almeida J.M.,Mining online music listening trajectories,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047727481&partnerID=40&md5=dd34c240b533b3a7f89f2c7ca2202f0a,"Figueiredo F., IBM Research, Brazil; Ribeiro B., Purdue University, India; Faloutsos C., Carnegie Mellon University, United States; Andrade N., Universidade Federal de Campina Grande, Brazil; Almeida J.M., Universidade Federal de Minas Gerais, Brazil","Understanding the listening habits of users is a valuable undertaking for musicology researchers, artists, consumers and online businesses alike. With the rise of Online Music Streaming Services (OMSSs), large amounts of user behavioral data can be exploited for this task. In this paper, we present SWIFT-FLOWS, an approach that models user listening habits in regards to how user attention transitions between artists. SWIFT-FLOWS combines recent advances in trajectory mining, coupled with modulated Markov models as a means to capture both how users switch attention from one artist to another, as well as how users fixate their attention in a single artist over short or large periods of time. We employ SWIFT-FLOWS on OMSSs datasets showing that it provides: (1) semantically meaningful representation of habits; (2) accurately models the attention span of users. © Figueiredo, Ribeiro, Faloutsos, Andrade, Almeida."
Lambert A.J.; Weyde T.; Armstrong N.,Adaptive frequency neural networks for dynamic pulse and metre perception,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069605671&partnerID=40&md5=fa161a71a0da450a4cfc190f07192301,"Lambert A.J., City University London, United Kingdom; Weyde T., City University London, United Kingdom; Armstrong N., City University London, United Kingdom","Beat induction, the means by which humans listen to music and perceive a steady pulse, is achieved via a perceptual and cognitive process. Computationally modelling this phenomenon is an open problem, especially when processing expressive shaping of the music such as tempo change. To meet this challenge we propose Adaptive Frequency Neural Networks (AFNNs), an extension of Gradient Frequency Neural Networks (GFNNs). GFNNs are based on neurodynamic models and have been applied successfully to a range of difficult music perception problems including those with syncopated and polyrhythmic stimuli. AFNNs extend GFNNs by applying a Hebbian learning rule to the oscillator frequencies. Thus the frequencies in an AFNN adapt to the stimulus through an attraction to local areas of resonance, and allow for a great dimensionality reduction in the network. Where previous work with GFNNs has focused on frequency and amplitude responses, we also consider phase information as critical for pulse perception. Evaluating the time-based output, we find significantly improved responses of AFNNs compared to GFNNs to stimuli with both steady and varying pulse frequencies. This leads us to believe that AFNNs could replace the linear filtering methods commonly used in beat tracking and tempo estimation systems, and lead to more accurate methods. © Andrew J. Lambert, Tillman Weyde, and Newton Armstrong."
Nieto O.; Bello J.P.,Systematic exploration of computational music structure research,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070008001&partnerID=40&md5=78d35f0a7d87a874a5d25de30c8f71d4,"Nieto O., Pandora Media, Inc., United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","In this work we present a framework containing open source implementations of multiple music structural segmentation algorithms and employ it to explore the hyper parameters of features, algorithms, evaluation metrics, datasets, and annotations of this MIR task. Besides testing and discussing the relative importance of the moving parts of the computational music structure eco-system, we also shed light on its current major challenges. Additionally, a new dataset containing multiple structural annotations for tracks that are particularly ambiguous to analyze is introduced, and used to quantify the impact of specific annotators when assessing automatic approaches to this task. Results suggest that more than one annotation per track is necessary to fully address the problem of ambiguity in music structure research. © Oriol Nieto, Juan Pablo Bello."
Southall C.; Stables R.; Hockman J.,Automatic drum transcription using bi-directional recurrent neural networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044052239&partnerID=40&md5=8c3f8aac96e563f8fc3e7a61da9d2a27,"Southall C., Digital Media Technology Laboratory (DMT Lab), Birmingham City University, Birmingham, United Kingdom; Stables R., Digital Media Technology Laboratory (DMT Lab), Birmingham City University, Birmingham, United Kingdom; Hockman J., Digital Media Technology Laboratory (DMT Lab), Birmingham City University, Birmingham, United Kingdom","Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive instruments in audio recordings. Neural networks have already been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We propose the use of neural networks for ADT in order to exploit their ability to capture a complex configuration of features associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neural network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suitable for online operation. In both systems, a separate network is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilising the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respectively. The results demonstrate the effectiveness of the presented methods for solo percussion and a capacity for identifying snare drums, which are historically the most difficult drum class to detect. © Carl Southall, Ryan Stables, Jason Hockman."
Tse T.; Salamon J.; Williams A.; Jiang H.; Law E.,Ensemble: A hybrid human-machine system for generating melody scores from audio,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070001294&partnerID=40&md5=f7cbe786b0f15f9e2f20b7e547f011eb,"Tse T., University of Waterloo, Canada; Salamon J., New York University, United States; Williams A., University of Waterloo, Canada; Jiang H., University of Waterloo, Canada; Law E., University of Waterloo, Canada","Music transcription is a highly complex task that is difficult for automated algorithms, and equally challenging to people, even those with many years of musical training. Furthermore, there is a shortage of high-quality datasets for training automated transcription algorithms. In this research, we explore a semi-automated, crowdsourced approach to generate music transcriptions, by first running an automatic melody transcription algorithm on a (polyphonic) song to produce a series of discrete notes representing the melody, and then soliciting the crowd to correct this melody. We present a novel web-based interface that enables the crowd to correct transcriptions, report results from an experiment to understand the capabilities of non-experts to perform this challenging task, and characterize the characteristics and actions of workers and how they correlate with transcription performance. © Tim Tse, Justin Salamon, Alex Williams, Helga Jiang and Edith Law."
Groves R.,Automatic melodic reduction using a supervised probabilistic context-free grammar,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064348639&partnerID=40&md5=d2193efca7deec1969459261fd766bdb,,"This research explores a Natural Language Processing technique utilized for the automatic reduction of melodies: the Probabilistic Context-Free Grammar (PCFG). Automatic melodic reduction was previously explored by means of a probabilistic grammar [11] [1]. However, each of these methods used unsupervised learning to estimate the probabilities for the grammar rules, and thus a corpus-based evaluation was not performed. A dataset of analyses using the Generative Theory of Tonal Music (GTTM) exists [13], which contains 300 Western tonal melodies and their corresponding melodic reductions in tree format. In this work, supervised learning is used to train a PCFG for the task of melodic reduction, using the tree analyses provided by the GTTM dataset. The resulting model is evaluated on its ability to create accurate reduction trees, based on a node-by-node comparison with ground-truth trees. Multiple data representations are explored, and example output reductions are shown. Motivations for performing melodic reduction include melodic identification and similarity, efficient storage of melodies, automatic composition, variation matching, and automatic harmonic analysis. © Ryan Groves."
Nishikimi R.; Nakamura E.; Itoyama K.; Yoshii K.,Musical note estimation for F0 trajectories of singing voices based on a Bayesian semi-beat-synchronous HMM,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063527235&partnerID=40&md5=65be7635bb30aace1f9cac219ccc0d2a,"Nishikimi R., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","This paper presents a statistical method that estimates a sequence of discrete musical notes from a temporal trajectory of vocal F0s. Since considerable effort has been devoted to estimate the frame-level F0s of singing voices from music audio signals, we tackle musical note estimation for those F0s to obtain a symbolic musical score. A naïve approach to musical note estimation is to quantize the vocal F0s at a semitone level in every time unit (e.g., half beat). This approach, however, fails when the vocal F0s are significantly deviated from those specified by a musical score. The onsets of musical notes are often delayed or advanced from beat times and the vocal F0s fluctuate according to singing expressions. To deal with these deviations, we propose a Bayesian hidden Markov model that allows musical notes to change in semi-synchronization with beat times. Both the semitone-level F0s and onset deviations of musical notes are regarded as latent variables and the frequency deviations are modeled by an emission distribution. The musical notes and their onset and frequency deviations are jointly estimated by using Gibbs sampling. Experimental results showed that the proposed method improved the accuracy of musical note estimation against baseline methods. © Ryo Nishikimi, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii."
Ojima Y.; Nakamura E.; Itoyama K.; Yoshii K.,"A hierarchical bayesian model of chords, pitches, and spectrograms for multipitch analysis",2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038407976&partnerID=40&md5=e1bbcea5d8cf9383237c26b601f069d1,"Ojima Y., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","This paper presents a statistical multipitch analyzer that can simultaneously estimate pitches and chords (typical pitch combinations) from music audio signals in an unsupervised manner. A popular approach to multipitch analysis is to perform nonnegative matrix factorization (NMF) for estimating the temporal activations of semitone-level pitches and then execute thresholding for making a piano-roll representation. The major problems of this cascading approach are that an optimal threshold is hard to determine for each musical piece and that musically inappropriate pitch combinations are allowed to appear. To solve these problems, we propose a probabilistic generative model that fuses an acoustic model (NMF) for a music spectrogram with a language model (hidden Markov model; HMM) for pitch locations in a hierarchical Bayesian manner. More specifically, binary variables indicating the existences of pitches are introduced into the framework of NMF. The latent grammatical structures of those variables are regulated by an HMM that encodes chord progressions and pitch co-occurrences (chord components). Given a music spectrogram, all the latent variables (pitches and chords) are estimated jointly by using Gibbs sampling. The experimental results showed the great potential of the proposed method for unified music transcription and grammar induction. © Yuta Ojima, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii."
Stoller D.; Dixon S.,Analysis and classification of phonation modes in singing,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070021047&partnerID=40&md5=7b813e6dd44000f7e88da3dfb4a7f9a5,"Stoller D., Queen Mary University of London, United Kingdom; Dixon S., Queen Mary University of London, United Kingdom","Phonation mode is an expressive aspect of the singing voice and can be described using the four categories neutral, breathy, pressed and flow. Previous attempts at automatically classifying the phonation mode on a dataset containing vowels sung by a female professional have been lacking in accuracy or have not sufficiently investigated the characteristic features of the different phonation modes which enable successful classification. In this paper, we extract a large range of features from this dataset, including specialised descriptors of pressedness and breathiness, to analyse their explanatory power and robustness against changes of pitch and vowel. We train and optimise a feed-forward neural network (NN) with one hidden layer on all features using cross validation to achieve a mean F-measure above 0.85 and an improved performance compared to previous work. Applying feature selection based on mutual information and retaining the nine highest ranked features as input to a NN results in a mean F-measure of 0.78, demonstrating the suitability of these features to discriminate between phonation modes. Training and pruning a decision tree yields a simple rule set based only on cepstral peak prominence (CPP), temporal flatness and average energy that correctly categorises 78% of the recordings. © Daniel Stoller, Simon Dixon."
Seetharaman P.; Pardo B.,Simultaneous separation and segmentation in layered music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054225782&partnerID=40&md5=f82bff8340695ff281677d00d2685afa,"Seetharaman P., Northwestern University, United States; Pardo B., Northwestern University, United States","In many pieces of music, the composer signals how individual sonic elements (samples, loops, the trumpet section) should be grouped by introducing sources or groups in a layered manner. We propose to discover and leverage the layering structure and use it for both structural segmentation and source separation. We use reconstruction error from non-negative matrix factorization (NMF) to guide structure discovery. Reconstruction error spikes at moments of significant sonic change. This guides segmentation and also lets us group basis sets for NMF. The number of sources, the types of sources, and when the sources are active are not known in advance. The only information is a specific type of layering structure. There is no separate training phase to learn a good basis set. No prior seeding of the NMF matrices is required. Unlike standard approaches to NMF there is no need for a post-processor to partition the learned basis functions by group. Source groups are learned automatically from the data. We evaluate our method on mixtures consisting of looping source groups. This separation approach outperforms a standard clustering NMF source separation approach on such mixtures. We find our segmentation approach is competitive with state-of-the-art segmentation methods on this dataset. © Prem Seetharaman, Bryan Pardo."
Gong R.; Pons J.; Serra X.,Audio to score matching by combining phonetic and duration information,2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041405755&partnerID=40&md5=d796cd5b3a2dbca343f2f389f06f7eca,"Gong R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We approach the singing phrase audio to score matching problem by using phonetic and duration information - with a focus on studying the jingju a cappella singing case. We argue that, due to the existence of a basic melodic contour for each mode in jingju music, only using melodic information (such as pitch contour) will result in an ambiguous matching. This leads us to propose a matching approach based on the use of phonetic and duration information. Phonetic information is extracted with an acoustic model shaped with our data, and duration information is considered with the Hidden Markov Models (HMMs) variants we investigate. We build a model for each lyric path in our scores and we achieve the matching by ranking the posterior probabilities of the decoded most likely state sequences. Three acoustic models are investigated: (i) convolutional neural networks (CNNs), (ii) deep neural networks (DNNs) and (iii) Gaussian mixture models (GMMs). Also, two duration models are compared: (i) hidden semi-Markov model (HSMM) and (ii) post-processor duration model. Results show that CNNs perform better in our (small) audio dataset and also that HSMM outperforms the post-processor duration model. © 2019 Rong Gong, Jordi Pons and Xavier Serra."
Hsu K.-C.; Lin C.-S.; Chi T.-S.,Sparse coding based music genre classification using spectro-temporal modulations,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048391041&partnerID=40&md5=fa52dbc1256373ba84dd0d063c8d006c,"Hsu K.-C., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Lin C.-S., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Chi T.-S., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","Spectro-temporal modulations (STMs) of the sound convey timbre and rhythm information so that they are intuitively useful for automatic music genre classification. The STMs are usually extracted from a time-frequency representation of the acoustic signal. In this paper, we investigate the efficacy of two kinds of STM features, the Gabor features and the rate-scale (RS) features, selectively extracted from various time-frequency representations, including the short-time Fourier transform (STFT) spectrogram, the constant-Q transform (CQT) spectrogram and the auditory (AUD) spectrogram, in recognizing the music genre. In our system, the dictionary learning and sparse coding techniques are adopted for training the support vector machine (SVM) classifier. Both spectral-type features and modulation-type features are used to test the system. Experiment results show that the RS features extracted from the log. magnituded CQT spectrogram produce the highest recognition rate in classifying the music genre. © Kai-Chun Hsu, Chih-Shan Lin, Tai-Shih Chi."
Calvo-Zaragoza J.; Rizo D.; Iñesta J.M.,Two (note) heads are better than one: Pen-based multimodal interaction with music scores,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045189375&partnerID=40&md5=b6f4e2800c3be5a5f219f73a7545002e,"Calvo-Zaragoza J., Pattern Recognition and Artificial Intelligence Group, Department of Software and Computing Systems, University of Alicante, Spain; Rizo D., Pattern Recognition and Artificial Intelligence Group, Department of Software and Computing Systems, University of Alicante, Spain; Iñesta J.M., Pattern Recognition and Artificial Intelligence Group, Department of Software and Computing Systems, University of Alicante, Spain","Digitizing early music sources requires new ways of dealing with musical documents. Assuming that current technologies cannot guarantee a perfect automatic transcription, our intention is to develop an interactive system in which user and software collaborate to complete the task. Since conventional score post-editing might be tedious, the user is allowed to interact using an electronic pen. Although this provides a more ergonomic interface, this interaction must be decoded as well. In our framework, the user traces the symbols using the electronic pen over a digital surface, which provides both the underlying image (offline data) and the drawing made by the e-pen (online data) to improve classification. Applying this methodology over 70 scores of the target musical archive, a dataset of 10 230 bimodal samples of 30 different symbols was obtained and made available for research purposes. This paper presents experimental results on classification over this dataset, in which symbols are recognized by combining the two modalities. This combination of modes has demonstrated its good performance, decreasing the error rate of using each modality separately and achieving an almost error-free performance. © Jorge Calvo-Zaragoza, David Rizo, Jose M. Iñesta."
Lewis D.; Crawford T.; Müllensiefen D.,Instrumental idiom in the 16th century: Embellishment patterns in arrangements of vocal music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069810926&partnerID=40&md5=ebf544073735aa07561c8e726dcf51a4,"Lewis D., Goldsmiths, University of London, United Kingdom; Crawford T., Goldsmiths, University of London, United Kingdom; Müllensiefen D., Goldsmiths, University of London, United Kingdom","Much surviving 16th-century instrumental music consists of arrangements (‘intabulations’) of vocal music, in tablature for solo lute. Intabulating involved deciding what to omit from a score to fit the instrument, and making it fit under the hand. Notes were usually added as embellishments to the original plain score, using idiomatic patterns, typically at cadences, but often filling simple intervals in the vocal parts with faster notes. Here we test whether such patterns are both characteristic of lute intabulations as a class (vs original lute music) and of different genres within that class. We use patterns identified in the musicological literature to search two annotated corpora of encoded lute music using the SIA(M)ESE algorithm. Diatonic patterns occur in many chromatic forms, accidentals being added depending how the arranger applied the conventions of musica ficta. Rhythms must be applied at three different scales as notation is inconsistent across the repertory. This produced over 88,000 short melodic queries to search in two corpora totalling just over 6,000 encodings of lute pieces. We show that our method clearly discriminates between intabulations and original music for the lute (p < .001); it also can distinguish sacred and secular genres within the vocal models (p < .001). © David Lewis, Tim Crawford, Daniel Müllensiefen."
López-Serrano P.; Dittmar C.; Driedger J.; Müller M.,Towards modeling and decomposing loop-based electronic music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028067276&partnerID=40&md5=af70192bade8a2f68137b13028e08a19,"López-Serrano P., International Audio Laboratories Erlangen, Germany; Dittmar C., International Audio Laboratories Erlangen, Germany; Driedger J., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Electronic Music (EM) is a popular family of genres which has increasingly received attention as a research subject in the field of MIR. A fundamental structural unit in EM are loops—audio fragments whose length can span several seconds. The devices commonly used to produce EM, such as sequencers and digital audio workstations, impose a musical structure in which loops are repeatedly triggered and overlaid. This particular structure allows new perspectives on well-known MIR tasks. In this paper we first review a prototypical production technique for EM from which we derive a simplified model. We then use our model to illustrate approaches for the following task: given a set of loops that were used to produce a track, decompose the track by finding the points in time at which each loop was activated. To this end, we repurpose established MIR techniques such as fingerprinting and non-negative matrix factor deconvolution. © Patricio López-Serrano, Christian Dittmar, Jonathan Driedger, Meinard Müller."
Fuller J.; Hubener L.; Kim Y.-S.; Lee J.H.,Elucidating user behavior in music services through persona and gender,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061411382&partnerID=40&md5=dbd3d0b2a33ed4bc0f20cbb8d393a447,"Fuller J., University of Washington, United States; Hubener L., University of Washington, United States; Kim Y.-S., University of Washington, United States; Lee J.H., University of Washington, United States","Prior user studies in the music information retrieval field have identified different personas representing the needs, goals, and characteristics of specific user groups for a user-centered design of music services. However, these personas were derived from a qualitative study involving a small number of participants and their generalizability has not been tested. The objectives of this study are to explore the applicability of seven user personas, developed in prior research, with a larger group of users and to identify the correlation between personas and the use of different types of music services. In total, 962 individuals were surveyed in order to understand their behaviors and preferences when interacting with music streaming services. Using a stratified sampling framework, key characteristics of each persona were extracted to classify users into specific persona groups. Responses were also analyzed in relation to gender, which yielded significant differences. Our findings support the development of more targeted approaches in music services rather than a universal service model. © John Fuller, Lauren Hubener, Yea-Seul Kim, Jin Ha Lee."
Choi K.; Fazekas G.; Sandler M.,Automatic tagging using deep convolutional neural networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069993895&partnerID=40&md5=a04f30f5a33ff5beff624d3978e643a5,"Choi K., Queen Mary University of London, United Kingdom; Fazekas G., Queen Mary University of London, United Kingdom; Sandler M., Queen Mary University of London, United Kingdom","We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data. © Keunwoo Choi, György Fazekas, Mark Sandler."
Lu Y.-C.; Wu C.-W.; Lu C.-T.; Lerch A.,Automatic outlier detection in music genre datasets,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038962698&partnerID=40&md5=2b0fa8b48b6e0c2918fb1d70031207d8,"Lu Y.-C., Department of Computer Science, Virginia Tech, United States; Wu C.-W., Center for Music Technology, Georgia Institute of Technology, United States; Lu C.-T., Department of Computer Science, Virginia Tech, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","Outlier detection, also known as anomaly detection, is an important topic that has been studied for decades. An outlier detection system is able to identify anomalies in a dataset and thus improve data integrity by removing the detected outliers. It has been successfully applied to different types of data in various fields such as cyber-security, finance, and transportation. In the field of Music Information Retrieval (MIR), however, the number of related studies is small. In this paper, we introduce different state-of-the-art outlier detection techniques and evaluate their viability in the context of music datasets. More specifically, we present a comparative study of 6 outlier detection algorithms applied to a Music Genre Recognition (MGR) dataset. It is determined how well algorithms can identify mislabeled or corrupted files, and how much the quality of the dataset can be improved. Results indicate that state-of-the-art anomaly detection systems have problems identifying anomalies in MGR datasets reliably. © Yen-Cheng Lu, Chih-Wei Wu, Chang-Tien Lu, Alexander Lerch."
Jeong I.-Y.; Lee K.,Learning temporal features using a deep neural network and its application to music genre classification,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028393149&partnerID=40&md5=30f563318989fb30ba78478958fdae7d,"Jeong I.-Y., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Lee K., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea","In this paper, we describe a framework for temporal feature learning from audio with a deep neural network, and apply it to music genre classification. To this end, we revisit the conventional spectral feature learning framework, and reformulate it in the cepstral modulation spectrum domain, which has been successfully used in many speech and music-related applications for temporal feature extraction. Experimental results using the GTZAN dataset show that the temporal features learned from the proposed method are able to obtain classification accuracy comparable to that of the learned spectral features. © Il-Young Jeong and Kyogu Lee."
Waloschek S.; Berndt A.; Bohl B.W.; Hadjakos A.,Interactive scores in classical music production,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060930767&partnerID=40&md5=082aa24f57331470658e960f5cf05cf4,"Waloschek S., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany; Berndt A., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany; Bohl B.W., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany; Hadjakos A., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany","The recording of classical music is mostly centered around the score of a composition. During editing of these recordings, however, further technical visualizations are used. Introducing digital interactive scores to the recording and editing process can enhance the workflow significantly and speed up the production process. This paper gives a short introduction to the recording process and outlines possibilities that arise with interactive scores. Current related music information retrieval research is discussed, showing a potential path to score-based editing. © Simon Waloschek, Axel Berndt, Benjamin W. Bohl, Aristotelis Hadjakos."
Shanahan D.; Neubarth K.; Conklin D.,Mining musical traits of social functions in native American music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039416796&partnerID=40&md5=ff5af630087d59b31bc8a187a4fb7f3e,"Shanahan D., Louisiana State University, Baton Rouge, LA, United States; Neubarth K., Canterbury Christ Church University, United Kingdom; Conklin D., University of the Basque Country UPV/EHU, San Sebastian, Spain, IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","Native American music is perhaps one of the most documented repertoires of indigenous folk music, being the subject of empirical ethnomusicological analyses for significant portions of the early 20th century. However, it has been largely neglected in more recent computational research, partly due to a lack of encoded data. In this paper we use the symbolic encoding of Frances Densmore’s collection of over 2000 songs, digitized between 1998 and 2014, to examine the relationship between internal musical features and social function. More specifically, this paper applies contrast data mining to discover global feature patterns that describe generalized social functions. Extracted patterns are discussed with reference to early ethnomusicological work and recent approaches to music, emotion, and ethology. A more general aim of this paper is to provide a methodology in which contrast data mining can be used to further examine the interactions between musical features and external factors such as social function, geography, language, and emotion. © Daniel Shanahan, Kerstin Neubarth, Darrell Conklin."
Hyrkas J.; Howe B.,MusicDB: A platform for longitudinal music analytics,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070007303&partnerID=40&md5=8db7049dff0909075761355c3cb35159,"Hyrkas J., University of Washington, United States; Howe B., University of Washington, United States","With public data sources such as Million Song dataset, researchers can now study longitudinal questions about the patterns of popular music, but the scale and complexity of the data complicate analysis. We propose MusicDB, a new approach for longitudinal music analytics that adapts techniques from relational databases to the music setting. By representing song timeseries data relationally, we aim to dramatically decrease the programming effort required for complex analytics while significantly improving scalability. We show how our platform can improve performance by reducing the amount of data accessed for many common analytics tasks, and how such tasks can be implemented quickly in relational languages — variants of SQL. We further show that expressing music analytics tasks over relational representations allows the system to automatically parallelize and optimize the resulting programs to improve performance. We evaluate our system by expressing complex analytics tasks including calculating song density and beat-aligning features and showing significant performance improvements over previous results. Finally, we evaluate expressiveness by reproducing the results from a recent analysis of longitudinal music trends using the Million Song dataset. © Jeremy Hyrkas, Bill Howe."
Beauguitte P.; Duggan B.; Kelleher J.,A corpus of annotated Irish traditional dance music recordings: Design and benchmark evaluations,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056639280&partnerID=40&md5=fcbcb5f6e1bc24b287fe472923be65de,"Beauguitte P., School of Computing, Dublin Institute of Technology, Ireland; Duggan B., School of Computing, Dublin Institute of Technology, Ireland; Kelleher J., School of Computing, Dublin Institute of Technology, Ireland","An emerging trend in music information retrieval (MIR) is the use of supervised machine learning to train automatic music transcription models. A prerequisite of adopting a machine learning methodology is the availability of annotated corpora. However, different genres of music have different characteristics and modelling these characteristics is an important part of creating state of the art MIR systems. Consequently, although some music corpora are available the use of these corpora is tied to the specific music genre, instrument type and recording context the corpus covers. This paper introduces the first corpus of annotations of audio recordings of Irish traditional dance music that covers multiple instrument types and both solo studio and live session recordings. We first discuss the considerations that motivated our design choices in developing the corpus. We then benchmark a number of automatic music transcription algorithms against the corpus. © Pierre Beauguitte, Bryan Duggan and John Kelleher."
Bogdanov D.; Porter A.; Herrera P.; Serra X.,Cross-collection evaluation for music classification tasks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020298303&partnerID=40&md5=d2501a0704d71226ca108a4b03c2e7c1,"Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Spain; Porter A., Music Technology Group, Universitat Pompeu Fabra, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Many studies in music classification are concerned with obtaining the highest possible cross-validation result. However, some studies have noted that cross-validation may be prone to biases and that additional evaluations based on independent out-of-sample data are desirable. In this paper we present a methodology and software tools for cross-collection evaluation for music classification tasks. The tools allow users to conduct large-scale evaluations of classifier models trained within the AcousticBrainz platform, given an independent source of ground-truth annotations, and its mapping with the classes used for model training. To demonstrate the application of this methodology we evaluate five models trained on genre datasets commonly used by researchers for genre classification, and use collaborative tags from Last.fm as an independent source of ground truth. We study a number of evaluation strategies using our tools on validation sets from 240,000 to 1,740,000 music recordings and discuss the results. © Dmitry Bogdanov, Alastair Porter, Perfecto Herrera, Xavier Serra."
Osmalskyj J.; Foster P.; Dixon S.; Embrechts J.-J.,Combining features for cover song identification,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069981861&partnerID=40&md5=4f9f7466843a406fa51ecce8b0d0f25f,"Osmalskyj J., University of Liège, Belgium; Foster P., Queen Mary University of London, United Kingdom; Dixon S., Queen Mary University of London, United Kingdom; Embrechts J.-J., University of Liège, Belgium","In this paper, we evaluate a set of methods for combining features for cover song identification. We first create multiple classifiers based on global tempo, duration, loudness, beats and chroma average features, training a random forest for each feature. Subsequently, we evaluate standard combination rules for merging these single classifiers into a composite classifier based on global features. We further obtain two higher level classifiers based on chroma features: one based on comparing histograms of quantized chroma features, and a second one based on computing cross-correlations between sequences of chroma features, to account for temporal information. For combining the latter chroma-based classifiers with the composite classifier based on global features, we use standard rank aggregation methods adapted from the information retrieval literature. We evaluate performance with the Second Hand Song dataset, where we quantify performance using multiple statistics. We observe that each combination rule outperforms single methods in terms of the total number of identified queries. Experiments with rank aggregation methods show an increase of up to 23.5 % of the number of identified queries, compared to single classifiers. © Julien Osmalskyj, Peter Foster, Simon Dixon, Jean-Jacques Embrechts."
Park J.; Lee K.,Harmonic-percussive source separation using harmonicity and sparsity constraints,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013834233&partnerID=40&md5=a09d867bedf3a54e8f4f07bccb150276,"Park J., Music and Audio Research Group, Seoul National University, Seoul, South Korea; Lee K., Music and Audio Research Group, Seoul National University, Seoul, South Korea","In this paper, we propose a novel approach to harmonic-percussive sound separation (HPSS) using Non-negative Matrix Factorization (NMF) with sparsity and harmonicity constraints. Conventional HPSS methods have focused on temporal continuity of harmonic components and spectral continuity of percussive components. However, it may not be appropriate to use them to separate time-varying harmonic signals such as vocals, vibratos, and glissandos, as they lack in temporal continuity. Based on the observation that the spectral distributions of harmonic and percussive signals differ – i.e., harmonic components have harmonic and sparse structure while percussive components are broadband – we propose an algorithm that successfully separates the rapidly time-varying harmonic signals from the percussive ones by imposing different constraints on the two groups of spectral bases. Experiments with real recordings as well as synthesized sounds show that the proposed method outperforms the conventional methods. © Jeongsoo Park, Kyogu Lee."
Otsuka M.; Kitahara T.,Improving MIDI guitar’s accuracy with NMF and neural net,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069983120&partnerID=40&md5=091d6db78e5620a3abb9d98234189b1d,"Otsuka M., Graduate School of Integrated Basic Sciences, Nihon University, Japan; Kitahara T., Graduate School of Integrated Basic Sciences, Nihon University, Japan","In this paper, we propose a method for improving the accuracy of MIDI guitars. MIDI guitars are useful tools for various purposes from inputting MIDI data to enjoying a jam session system, but existing MIDI guitars do not have sufficient accuracy in converting the performance to an MIDI form. In this paper, we make an attempt on improving the accuracy of a MIDI guitar by integrating it with an audio transcription method based on non-negative matrix factorization (NMF). First, we investigate an NMF-based algorithm for transcribing guitar performances. Although the NMF is a promising method, an effective post-process (i.e., converting the NMF’s output to an MIDI form) is a non-trivial problem. We propose use of a neural network for this conversion. Next, we investigate a method for integrating the outputs of the MIDI guitar and NMF. Because they have different tendencies in wrong outputs, we take an policy of outputting only common parts in the two outputs. Experimental results showed that the F-score of our method was 0.626 whereas those of the MIDI-guitar-only and NMF-and-neural-network-only methods were 0.347 and 0.526, respectively. © Masaki Otsuka and Tetsuro Kitahara."
Ellis R.J.; Xing Z.; Fang J.; Wang Y.,Quantifying lexical novelty in song lyrics,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048580257&partnerID=40&md5=99e60122b96313b88b8754717080f0e2,"Ellis R.J., School of Computing, National University of Singapore, Singapore; Xing Z., School of Computing, National University of Singapore, Singapore; Fang J., School of Computing, National University of Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore","Novelty is an important psychological construct that affects both perceptual and behavioral processes. Here, we propose a lexical novelty score (LNS) for a song’s lyric, based on the statistical properties of a corpus of 275,905 lyrics (available at www.smcnus.org/lyrics/). A lyric-level LNS was derived as a function of the inverse document frequencies of its unique words. An artist-level LNS was then computed using the LNSs of lyrics uniquely associated with each artist. Statistical tests were performed to determine whether lyrics and artists on Billboard Magazine’s lists of “All-Time Top 100” songs and artists had significantly lower LNSs than “non-top” songs and artists. An affirmative and highly consistent answer was found in both cases. These results highlight the potential utility of the LNS as a feature for MIR. © Robert J Ellis, Zhe Xing, Jiakun Fang, and Ye Wang."
Dittmar C.; Lehner B.; Prätzlich T.; Müller M.; Widmer G.,Cross-version singing voice detection in classical opera recordings,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028076913&partnerID=40&md5=4e5f407e0be1b87a10e7cc97913aa719,"Dittmar C., International Audio Laboratories, Erlangen, Germany; Lehner B., Johannes Kepler University, Linz, Austria; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany; Widmer G., Johannes Kepler University, Linz, Austria","In the field of Music Information Retrieval (MIR), the automated detection of the singing voice within a given music recording constitutes a challenging and important research problem. In this study, our goal is to find those segments within a classical opera recording, where one or several singers are active. As our main contributions, we first propose a novel audio feature that extends a state-of-the-art feature set that has previously been applied to singing voice detection in popular music recordings. Second, we describe a simple bootstrapping procedure that helps to improve the results in the case that the test data is not reflected well by the training data. Third, we show that a cross-version approach can help to stabilize the results even further. © Christian Dittmar, Bernhard Lehner, Thomas Prätzlich, Meinard Müller, Gerhard Widmer."
Vinutha T.P.; Sankagiri S.; Ganguli K.K.; Rao P.,Structural segmentation and visualization of sitar and sarod concert audio,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021954900&partnerID=40&md5=e207ac0a02f0a738a64c040ee77eb1b4,"Vinutha T.P., Department of Electrical Engineering, IIT Bombay, India; Sankagiri S., Department of Electrical Engineering, IIT Bombay, India; Ganguli K.K., Department of Electrical Engineering, IIT Bombay, India; Rao P., Department of Electrical Engineering, IIT Bombay, India","Hindustani classical instrumental concerts follow an episodic development that, musicologically, is described via changes in the rhythmic structure. Uncovering this structure in a musically relevant form can provide for powerful visual representations of the concert audio that is of potential value in music appreciation and pedagogy. We investigate the structural analysis of the metered section (gat) of concerts of two plucked string instruments, the sitar and sarod. A prominent aspect of the gat is the interplay between the melody soloist and the accompanying drummer (tabla). The tempo as provided by the tabla together with the rhythmic density of the sitar/sarod plucks serve as the main dimensions that predict the transition between concert sections. We present methods to access the stream of tabla onsets separately from the sitar/sarod onsets, addressing challenges that arise in the instrument separation. Further, the robust detection of tempo and the estimation of rhythmic density of sitar/sarod plucks are discussed. A case study of a fully annotated concert is presented, and is followed by results of achieved segmentation accuracy on a database of sitar and sarod gats across artists. © Vinutha T.P., Suryanarayana Sankagiri, Kaustuv Kanti Ganguli, Preeti Rao."
Wolff D.; MacFarlane A.; Weyde T.,Comparative music similarity modelling using transfer learning across user groups,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023772102&partnerID=40&md5=5cec125cb4e7d3be1a411c9245f9c97b,"Wolff D., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; MacFarlane A., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","We introduce a new application of transfer learning for training and comparing music similarity models based on relative user data: The proposed Relative Information-Theoretic Metric Learning (RITML) algorithm adapts a Mahalanobis distance using an iterative application of the ITML algorithm, thereby extending it to relative similarity data. RITML supports transfer learning by training models with respect to a given template model that can provide prior information for regularisation. With this feature we use information from larger datasets to build better models for more specific datasets, such as user groups from different cultures or of different age. We then evaluate what model parameters, in this case acoustic features, are relevant for the specific models when compared to the general user data. We to this end introduce the new CASimIR dataset, the first openly available relative similarity dataset with user attributes. With two age-related subsets, we show that transfer learning with RITML leads to better age-specific models. RITML here improves learning on small datasets. Using the larger MagnaTagATune dataset, we show that RITML performs as well as state-of-the-art algorithms in terms of general similarity estimation. © Daniel Wolff, Andrew MacFarlane and Tillman Weyde."
Vatolkin I.; Rudolph G.; Weihs C.,Evaluation of album effect for feature selection in music genre recognition,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017563262&partnerID=40&md5=c77b2f46dad9895439ffb9519acfe7dc,"Vatolkin I., TU Dortmund, Department of Computer Science, Germany; Rudolph G., TU Dortmund, Department of Computer Science, Germany; Weihs C., TU Dortmund, Faculty of Statistics, Germany","With an increasing number of available music characteristics, feature selection becomes more important for various categorisation tasks, helping to identify relevant features and remove irrelevant and redundant ones. Another advantage is the decrease of runtime and storage demands. However, sometimes feature selection may lead to “over-optimisation” when data in the optimisation set is too different from data in the independent validation set. In this paper, we extend our previous work on feature selection for music genre recognition and focus on so-called “album effect” meaning that optimised classification models may overemphasize relevant characteristics of particular artists and albums rather than learning relevant properties of genres. For that case we examine the performance of classification models on two validation sets after the optimisation with feature selection: the first set with tracks not used for training and feature selection but randomly selected from the same albums, and the second set with tracks selected from other albums. As it can be expected, the classification performance on the second set decreases. Nevertheless, in almost all cases the feature selection remains beneficial compared to complete feature sets and a baseline using MFCCs, if applied for an ensemble of classifiers, proving robust generalisation performance. © Igor Vatolkin, Günter Rudolph, Claus Weihs."
Schlüter J.,Learning to pinpoint singing voice from weakly labeled examples,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016639226&partnerID=40&md5=0aa22bfd458650cf02218e0dc9a64e89,"Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Building an instrument detector usually requires temporally accurate ground truth that is expensive to create. However, song-wise information on the presence of instruments is often easily available. In this work, we investigate how well we can train a singing voice detection system merely from song-wise annotations of vocal presence. Using convolutional neural networks, multiple-instance learning and saliency maps, we can not only detect singing voice in a test signal with a temporal accuracy close to the state-of-the-art, but also localize the spectral bins with precision and recall close to a recent source separation method. Our recipe may provide a basis for other sequence labeling tasks, for improving source separation or for inspecting neural networks trained on auditory spectrograms. © Jan Schlüter."
Percival G.; Fukayama S.; Goto M.,SONG2QUARTET: A system for generating string quartet cover songs from polyphonic audio of popular music,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029925880&partnerID=40&md5=c88bc89774b89f989cc13c87c091b4c3,"Percival G., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We present Song2Quartet, a system for generating string quartet versions of popular songs by combining probabilistic models estimated from a corpus of symbolic classical music with the target audio file of any song. Song2Quartet allows users to add novelty to listening experience of their favorite songs and gain familiarity with string quartets. Previous work in automatic arrangement of music only used symbolic scores to achieve a particular musical style; our challenge is to also consider audio features of the target popular song. In addition to typical audio music content analysis such as beat and chord estimation, we also use time-frequency spectral analysis in order to better reflect partial phrases of the song in its cover version. Song2Quartet produces a probabilistic network of possible musical notes at every sixteenth note for each accompanying instrument of the quartet by combining beats, chords, and spectrogram from the target song with Markov chains estimated from our corpora of quartet music. As a result, the musical score of the cover version can be generated by finding the optimal paths through these networks. We show that the generated results follow the conventions of classical string quartet music while retaining some partial phrases and chord voicings from the target audio. © Graham Percival, Satoru Fukayama, Masataka Goto."
Durand S.; Essid S.,Downbeat detection with conditional random fields and deep learned features,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013873952&partnerID=40&md5=164da3952003b41df67291a78127a269,"Durand S., LTCI, CNRS, Télécom ParisTech Université Paris-Saclay, Paris, 75013, France; Essid S., LTCI, CNRS, Télécom ParisTech Université Paris-Saclay, Paris, 75013, France","In this paper, we introduce a novel Conditional Random Field (CRF) system that detects the downbeat sequence of musical audio signals. Feature functions are computed from four deep learned representations based on harmony, rhythm, melody and bass content to take advantage of the high-level and multi-faceted aspect of this task. Downbeats being dynamic, the powerful CRF classification system allows us to combine our features with an adapted temporal model in a fully data-driven fashion. Some meters being under-represented in our training set, we show that data augmentation enables a statistically significant improvement of the results by taking into account class imbalance. An evaluation of different configurations of our system on nine datasets shows its efficiency and potential over a heuristic based approach and four downbeat tracking algorithms. © Simon Durand, Slim Essid."
Janssen B.; van Kranenburg P.; Volk A.,A comparison of symbolic similarity measures for finding occurrences of melodic segments,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019356178&partnerID=40&md5=dff8351b3be2ad3241181338d17dce44,"Janssen B., Meertens Institute, Amsterdam, Netherlands; van Kranenburg P., Meertens Institute, Amsterdam, Netherlands; Volk A., Utrecht University, Netherlands","To find occurrences of melodic segments, such as themes, phrases and motifs, in musical works, a well-performing similarity measure is needed to support human analysis of large music corpora. We evaluate the performance of a range of melodic similarity measures to find occurrences of phrases in folk song melodies. We compare the similarity measures correlation distance, city-block distance, Euclidean distance and alignment, proposed for melody comparison in computational ethnomusicology; furthermore Implication-Realization structure alignment and B-spline alignment, forming successful approaches in symbolic melodic similarity; moreover, wavelet transform and the geometric approach Structure Induction, having performed well in musical pattern discovery. We evaluate the success of the different similarity measures through observing retrieval success in relation to human annotations. Our results show that local alignment and SIAM perform on an almost equal level to human annotators. © Berit Janssen, Peter van Kranenburg, Anja Volk."
Padilla V.; McLean A.; Marsden A.; Ng K.,Improving optical music recognition by combining outputs from multiple sources,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069947431&partnerID=40&md5=1fd611726ce7e48ca5497bf9307a9af9,"Padilla V., Lancaster University, United Kingdom; McLean A., University of Leeds, United Kingdom; Marsden A., Lancaster University, United Kingdom; Ng K., University of Leeds, United Kingdom","Current software for Optical Music Recognition (OMR) produces outputs with too many errors that render it an unrealistic option for the production of a large corpus of symbolic music files. In this paper, we propose a system which applies image pre-processing techniques to scans of scores and combines the outputs of different commercial OMR programs when applied to images of different scores of the same piece of music. As a result of this procedure, the combined output has around 50% fewer errors when compared to the output of any one OMR program. Image pre-processing splits scores into separate movements and sections and removes ossia staves which confuse OMR software. Post-processing aligns the outputs from different OMR programs and from different sources, rejecting outputs with the most errors and using majority voting to determine the likely correct details. Our software produces output in MusicXML, concentrating on accurate pitch and rhythm and ignoring grace notes. Results of tests on the six string quartets by Mozart dedicated to Joseph Haydn and the first six piano sonatas by Mozart are presented, showing an average recognition rate of around 95%. © Victor Padilla, Alex McLean, Alan Marsden & Kia Ng."
Gasser M.; Arzt A.; Gadermaier T.; Grachten M.; Widmer G.,Classical music on the web – User interfaces and data representations,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056638022&partnerID=40&md5=094acbf767e1faa7458206f14b4f7953,"Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Arzt A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria; Gadermaier T., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Grachten M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria","We present a set of web-based user interfaces for explorative analysis and visualization of classical orchestral music and a web API that serves as a backend to those applications; we describe use cases that motivated our developments within the PHENICX project, which promotes a vital interaction between Music Information Retrieval research groups and a world-renowned symphony orchestra. Furthermore, we describe two real-world applications that involve the work presented here. Firstly, our web applications are used in the editorial stage of a periodically released subscription-based mobile app by the Royal Concertgebouw Orchestra (RCO) 1 , which serves as a content-distribution channel for multi-modally enhanced recordings of classical concerts. Secondly, our web API and user interfaces have been successfully used to provide real-time information (such as the score, and explanatory comments from musicologists) to the audience during a live concert of the RCO. © Martin Gasser, Andreas Arzt, Thassilo Gadermaier, Maarten Grachten, Gerhard Widmer."
Kruspe A.M.,Training phoneme models for singing with “songified” speech data,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054284624&partnerID=40&md5=4b433df6952625f646ede2eb348928b9,"Kruspe A.M., Fraunhofer IDMT, Ilmenau, Germany","Speech recognition in singing is a task that has not been widely researched so far. Singing possesses several characteristics that differentiate it from speech. Therefore, algorithms and models that were developed for speech usually perform worse on singing. One of the bottlenecks in many algorithms is the recognition of phonemes in singing. We noticed that this recognition step can be improved when using singing data in model training, but to our knowledge, there are no large datasets of singing data annotated with phonemes. However, such data does exist for speech. We therefore propose to make phoneme recognition models more robust for singing by training them on speech data that has artificially been made more “song-like”. We test two main modifications on speech data: Time stretching and pitch shifting. Artificial vibrato is also tested. We then evaluate models trained on different combinations of these modified speech recordings. The utilized modeling algorithms are Neural Networks and Deep Belief Networks. © Anna M. Kruspe."
Dai J.; Mauch M.; Dixon S.,Analysis of intonation trajectories in solo singing,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030988394&partnerID=40&md5=0c9ee521b06c0ea2b02625f0cbd9e544,"Dai J., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We present a new dataset for singing analysis and modelling, and an exploratory analysis of pitch accuracy and pitch trajectories. Shortened versions of three pieces from The Sound of Music were selected: “Edelweiss”, “Do-Re-Mi” and “My Favourite Things”. 39 participants sang three repetitions of each excerpt without accompaniment, resulting in a dataset of 21762 notes in 117 recordings. To obtain pitch estimates we used the Tony software’s automatic transcription and manual correction tools. Pitch accuracy was measured in terms of pitch error and interval error. We show that singers’ pitch accuracy correlates significantly with self-reported singing skill and musical training. Larger intervals led to larger errors, and the tritone interval in particular led to average errors of one third of a semitone. Note duration (or inter-onset interval) had a significant effect on pitch accuracy, with greater accuracy on longer notes. To model drift in the tonal centre over time, we present a sliding window model which reveals patterns in the pitch errors of some singers. Based on the trajectory, we propose a measure for the magnitude of drift: tonal reference deviation (TRD). The data and software are freely available. 1. © Jiajie Dai, Matthias Mauch, Simon Dixon."
Ringwalt D.; Dannenberg R.B.,Image quality estimation for multi-score OMR,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070005553&partnerID=40&md5=333110eabd6982005d838cc07cd8ce04,"Ringwalt D., Carnegie Mellon University, School of Computer Science, United States; Dannenberg R.B., Carnegie Mellon University, School of Computer Science, United States","Optical music recognition (OMR) is the recognition of images of musical scores. Recent research has suggested aligning the results of OMR from multiple scores of the same work (multi-score OMR, MS-OMR) to improve accuracy. As a simpler alternative, we have developed features which predict the quality of a given score, allowing us to select the highest-quality score to use for OMR. Furthermore, quality may be used to weight each score in an alignment, which should improve existing systems’ robustness. Using commercial OMR software on a test set of MIDI recordings and multiple corresponding scores, our predicted OMR accuracy is weakly but significantly correlated with the true accuracy. Improved features should be able to produce highly consistent results. © Dan Ringwalt, Roger B. Dannenberg."
Vall A.; Skowron M.; Knees P.; Schedl M.,Improving music recommendations with a weighted factorization of the tagging activity,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058982056&partnerID=40&md5=94907ed0dce04059083d898e393b8fb2,"Vall A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Skowron M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Collaborative filtering systems for music recommendations are often based on implicit feedback derived from listening activity. Hybrid approaches further incorporate additional sources of information in order to improve the quality of the recommendations. In the context of a music streaming service, we present a hybrid model based on matrix factorization techniques that fuses the implicit feedback derived from the users’ listening activity with the tags that users have given to musical items. In contrast to existing work, we introduce a novel approach to exploit tags by performing a weighted factorization of the tagging activity. We evaluate the model for the task of artist recommendation, using the expected percentile rank as metric, extended with confidence intervals to enable the comparison between models. Thus, our contribution is twofold: (1) we introduce a novel model that uses tags to improve music recommendations and (2) we extend the evaluation methodology to compare the performance of different recommender systems. © Andreu Vall, Marcin Skowron, Peter Knees, Markus Schedl."
Miron M.; Carabias-Orti J.J.; Janer J.,Improving score-informed source separation for classical music through note refinement,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057593513&partnerID=40&md5=f83953b89a1276da98058beb4457003f,"Miron M., Music Technology Group, Universitat Pompeu Fabra, Spain; Carabias-Orti J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Janer J., Music Technology Group, Universitat Pompeu Fabra, Spain","Signal decomposition methods such as Non-negative Matrix Factorization (NMF) demonstrated to be a suitable approach for music signal processing applications, including sound source separation. To better control this decomposition, NMF has been extended using prior knowledge and parametric models. In fact, using score information considerably improved separation results. Nevertheless, one of the main problems of using score information is the misalignment between the score and the actual performance. A potential solution to this problem is the use of audio to score alignment systems. However, most of them rely on a tolerance window that clearly affects the separation results. To overcome this problem, we propose a novel method to refine the aligned score at note level by detecting both, onset and offset for each note present in the score. Note refinement is achieved by detecting shapes and contours in the estimated instrument-wise time activation (gains) matrix. Decomposition is performed in a supervised way, using training instrument models and coarsely-aligned score information. The detected contours define time-frequency note boundaries, and they increase the sparsity. Finally, we have evaluated our method for informed source separation using a dataset of Bach chorales obtaining satisfactory results, especially in terms of SIR. © Marius Miron, Julio José Carabias-Orti, Jordi Janer."
Koops H.V.; Volk A.; Bas de Haas W.,Corpus-based rhythmic pattern analysis of ragtime syncopation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022071068&partnerID=40&md5=ca211c15f944184ba3d7470c3ecdbe4c,"Koops H.V., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands; Bas de Haas W., Utrecht University, Netherlands","This paper presents a corpus-based study on rhythmic patterns in the RAG-collection of approximately 11.000 symbolically encoded ragtime pieces. While characteristic musical features that define ragtime as a genre have been debated since its inception, musicologists argue that specific syncopation patterns are most typical for this genre. Therefore, we investigate the use of syncopation patterns in the RAG-collection from its beginnings until the present time in this paper. Using computational methods, this paper provides an overview on the use of rhythmical patterns of the ragtime genre, thereby offering valuable new insights that complement musicological hypotheses about this genre. Specifically, we measure the amount of syncopation for each bar using Longuet-Higgins and Lee’s model of syncopation, determine the most frequent rhythmic patterns, and discuss the role of a specific short-long-short syncopation pattern that musicologists argue is characteristic for ragtime. A comparison between the ragtime (pre-1920) and modern (post-1920) era shows that the two eras differ in syncopation pattern use. Onset density and amount of syncopation increase after 1920. Moreover, our study confirms the musicological hypothesis on the important role of the short-long-short syncopation pattern in ragtime. These findings are pivotal in developing ragtime genre-specific features. © Hendrik Vincent Koops, Anja Volk, W. Bas de Haas."
Nakamura E.; Cuvillier P.; Cont A.; Ono N.; Sagayama S.,Autoregressive hidden semi-Markov model of symbolic music performance for score following,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064341619&partnerID=40&md5=9c6f69a155dc4179d52199033da8cf0c,"Nakamura E., National Institute of Informatics, Tokyo, 101-8430, Japan; Cuvillier P., Inria MuTant Project-Team, Ircam/UPMC, CNRS UMR STMS, Paris, 75004, France; Cont A., Inria MuTant Project-Team, Ircam/UPMC, CNRS UMR STMS, Paris, 75004, France; Ono N., National Institute of Informatics, Tokyo, 101-8430, Japan; Sagayama S., Meiji University, Tokyo, 164-8525, Japan","A stochastic model of symbolic (MIDI) performance of polyphonic scores is presented and applied to score following. Stochastic modelling has been one of the most successful strategies in this field. We describe the performance as a hierarchical process of performer’s progression in the score and the production of performed notes, and represent the process as an extension of the hidden semi-Markov model. The model is compared with a previously studied model based on hidden Markov model (HMM), and reasons are given that the present model is advantageous for score following especially for scores with trills, tremolos, and arpeggios. This is also confirmed empirically by comparing the accuracy of score following and analysing the errors. We also provide a hybrid of this model and the HMM-based model which is computationally more efficient and retains the advantages of the former model. The present model yields one of the state-of-the-art score following algorithms for symbolic performance and can possibly be applicable for other music recognition problems. © Eita Nakamura, Philippe Cuvillier, Arshia Cont, Nobutaka Ono, Shigeki Sagayama."
Zacharakis A.; Kaliakatsos-Papakostas M.; Cambouropoulos E.,Conceptual blending in music cadences: A formal model and subjective evaluation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045905435&partnerID=40&md5=a883bd6891b97a1c89409906b53be498,"Zacharakis A., School of Music Studies, Aristotle University of Thessaloniki, Greece; Kaliakatsos-Papakostas M., School of Music Studies, Aristotle University of Thessaloniki, Greece; Cambouropoulos E., School of Music Studies, Aristotle University of Thessaloniki, Greece","Conceptual blending is a cognitive theory whereby elements from diverse, but structurally-related, mental spaces are ‘blended’ giving rise to new conceptual spaces. This study focuses on structural blending utilising an algorithmic formalisation for conceptual blending applied to harmonic concepts. More specifically, it investigates the ability of the system to produce meaningful blends between harmonic cadences, which arguably constitute the most fundamental harmonic concept. The system creates a variety of blends combining elements of the penultimate chords of two input cadences and it further estimates the expected relationships between the produced blends. Then, a preliminary subjective evaluation of the proposed blending system is presented. A pairwise dissimilarity listening test was conducted using original and blended cadences as stimuli. Subsequent multidimensional scaling analysis produced spatial configurations for both behavioural data and dissimilarity estimations by the algorithm. Comparison of the two configurations showed that the system is capable of making fair predictions of the perceived dissimilarities between the blended cadences. This implies that this conceptual blending approach is able to create perceptually meaningful blends based on self-evaluation of its outcome. © Asterios Zacharakis, Maximos Kaliakatsos-Papakostas, Emilios Cambouropoulos."
Sigler A.; Wild J.; Handelman E.,Schematizing the treatment of dissonance in 16th-century counterpoint,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069881724&partnerID=40&md5=01632c4a9f20890f5cb45d087db7c760,"Sigler A., School of Computer Science, McGill University, Computing Music, Canada; Wild J., Schulich School of Music, McGill University, Canada; Handelman E., Computing Music, Canada","We describe a computational project concerning labeling of dissonance treatments – schematic descriptions of the uses of dissonances. We use automatic score annotation and database methods to develop schemata for a large corpus of 16th-century polyphonic music. We then apply structural techniques to investigate coincidence of schemata, and to extrapolate from found structures to unused possibilities. © Andie Sigler, Jon Wild, Eliot Handelman."
Matz D.; Cano E.; Abeßer J.,New sonorities for early jazz recordings using sound source separation and automatic mixing tools,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028051686&partnerID=40&md5=6347908a0601c0132787ee9d5ce37364,"Matz D., University of Applied Sciences, Düsseldorf, Germany; Cano E., Fraunhofer IDMT, Ilmenau, Germany; Abeßer J., Fraunhofer IDMT, Ilmenau, Germany","In this paper, a framework for automatic mixing of early jazz recordings is presented. In particular, we propose the use of sound source separation techniques as a pre-processing step of the mixing process. In addition to an initial solo and accompaniment separation step, the proposed mixing framework is composed of six processing blocks: harmonic-percussive separation (HPS), cross-adaptive multi-track scaling (CAMTS), cross-adaptive equalizer (CAEQ), cross-adaptive dynamic spectral panning (CADSP), automatic excitation (AE), and time-frequency selective panning (TFSP). The effects of the different processing steps in the final quality of the mix are evaluated through a listening test procedure. The results show that the desired quality improvements in terms of sound balance, transparency, stereo impression, timbre, and overall impression can be achieved with the proposed framework. © Daniel Matz, Estefanía Cano, Jakob Abeßer."
Kruspe A.M.,Bootstrapping a system for phoneme recognition and keyword spotting in unaccompanied singing,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994195825&partnerID=40&md5=53784a3b9757e8446f4326f4675e64b5,"Kruspe A.M., Fraunhofer IDMT, Ilmenau, Germany","Speech recognition in singing is still a largely unsolved problem. Acoustic models trained on speech usually produce unsatisfactory results when used for phoneme recognition in singing. On the flipside, there is no phonetically annotated singing data set that could be used to train more accurate acoustic models for this task. In this paper, we attempt to solve this problem using the DAMP data set which contains a large number of recordings of amateur singing in good quality. We first align them to the matching textual lyrics using an acoustic model trained on speech. We then use the resulting phoneme alignment to train new acoustic models using only subsets of the DAMP singing data. These models are then tested for phoneme recognition and, on top of that, keyword spotting. Evaluation is performed for different subsets of DAMP and for an unrelated set of the vocal tracks of commercial pop songs. Results are compared to those obtained with acoustic models trained on the TIMIT speech data set and on a version of TIMIT augmented for singing. Our new approach shows significant improvements over both. © Anna M. Kruspe."
Gulati S.; Serrà J.; Ganguli K.K.; Şentürk S.; Serra X.,Time-delayed melody surfaces for rāga recognition,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985928576&partnerID=40&md5=f08f4061995c33ed10a51d07ee725653,"Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Telefonica Research, Barcelona, Spain; Ganguli K.K., Dept. of Electrical Engg., Indian Institute of Technology Bombay, Mumbai, India; Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Rāga is the melodic framework of Indian art music. It is a core concept used in composition, performance, organization, and pedagogy. Automatic rāga recognition is thus a fundamental information retrieval task in Indian art music. In this paper, we propose the time-delayed melody surface (TDMS), a novel feature based on delay coordinates that captures the melodic outline of a rāga. A TDMS describes both the tonal and the temporal characteristics of a melody, using only an estimation of the predominant pitch. Considering a simple k-nearest neighbor classifier, TDMSs outperform the state-of-the-art for rāga recognition by a large margin. We obtain 98% accuracy on a Hindustani music dataset of 300 recordings and 30 rāgas, and 87% accuracy on a Carnatic music dataset of 480 recordings and 40 rāgas. TDMSs are simple to implement, fast to compute, and have a musically meaningful interpretation. Since the concepts and formulation behind the TDMS are generic and widely applicable, we envision its usage in other music traditions beyond Indian art music. © Sankalp Gulati, Joan Serrà, Kaustuv K Ganguli, Sertan Şentürk and Xavier Serra."
Jančovič P.; Köküer M.; Baptiste W.,Automatic transcription of ornamented Irish traditional flute music using hidden Markov models,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062000686&partnerID=40&md5=782733db2a3efe7a558e592ebbed23ae,"Jančovič P., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom; Köküer M., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom, School of Digital Media Technology, Birmingham City University, United Kingdom; Baptiste W., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom, School of Digital Media Technology, Birmingham City University, United Kingdom","This paper presents an automatic system for note transcription of Irish traditional flute music containing ornamentation. This is a challenging problem due to the soft nature of onsets and short durations of ornaments. The proposed automatic transcription system is based on hidden Markov models, with separate models being built for notes and for single-note ornaments. Mel-frequency cepstral coefficients are employed to represent the acoustic signal. Different setups of parameters in feature extraction and acoustic modelling are explored. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD. The performance of the system is evaluated in terms of the transcription of notes as well as detection of onsets. It is demonstrated that the proposed system can achieve a very good note transcription and onset detection performance. Over 28% relative improvement in terms of the F-measure is achieved for onset detection in comparison to conventional onset detection methods based on signal energy and fundamental frequency. © Peter Jančovič, Münevver Köküer, Wrena Baptiste."
Manaris B.; Stoudenmier S.,Specter: Combining music information retrieval with sound spatialization,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070006592&partnerID=40&md5=0a904a1ba47e53e1aed3909d27681401,"Manaris B., Computer Science Dept., College of Charleston, United States; Stoudenmier S., Computer Science Dept., College of Charleston, United States","Specter combines music information retrieval (MIR) with sound spatialization to provide a simple, yet versatile environment to experiment with sound spatialization for music composition and live performance. Through various interfaces and sensors, users may position sounds at arbitrary locations and trajectories in a three-dimensional plane. The system utilizes the JythonMusic environment for symbolic music processing, music information retrieval, and live audio manipulation. It also incorporates Iannix, a 3D graphical, open-source sequencer, for real-time generation, manipulation, and storing of sound trajectory scores. Finally, through Glaser, a sound manipulation instrument, Specter renders the various sounds in space. The system architecture supports different sound spatialization techniques including Ambisonics and Vector Based Amplitude Panning. Various interfaces are discussed, including a Kinect-based sensor system, a LeapMotion-based hand-tracking interface, and a smartphone-based OSC controller. Finally, we present Migrant, a music composition, which utilizes and demonstrates Specter’s ability to combine MIR techniques with sound spatialization through inexpensive, minimal hardware. © Bill Manaris and Seth Stoudenmier."
Page K.R.; Nurmikko-Fuller T.; Rindfleisch C.; Weigl D.M.; Lewis R.; Dreyfus L.; De Roure D.,A toolkit for live annotation of opera performance: Experiences capturing wagner’s ring cycle,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041057870&partnerID=40&md5=5c58749dfebd505b5431f3bf65b5a05c,"Page K.R., Oxford e-Research Centre, University of Oxford, United Kingdom; Nurmikko-Fuller T., Oxford e-Research Centre, University of Oxford, United Kingdom; Rindfleisch C., Faculty of Music, University of Oxford, United Kingdom; Weigl D.M., Oxford e-Research Centre, University of Oxford, United Kingdom; Lewis R., Department of Computing Goldsmiths, University of London, United Kingdom; Dreyfus L., Faculty of Music, University of Oxford, United Kingdom; De Roure D., Oxford e-Research Centre, University of Oxford, United Kingdom","Performance of a musical work potentially provides a rich source of multimedia material for future investigation, both for musicologists’ study of reception and perception, and in improvement of computational methods applied to its analysis. This is particularly true of music theatre, where a traditional recording cannot sufficiently capture the ephemeral phenomena unique to each staging. In this paper we introduce a toolkit developed with, and used by, a musicologist throughout a complete multi-day production of Richard Wagner’s Der Ring des Nibelungen. The toolkit is centred on a tablet-based score interface through which the scholar makes notes on the scenic setting of the performance as it unfolds, supplemented by a variety of digital data gathered to structure and index the annotations. We report on our experience developing a system suitable for real-time use by the musicologist, structuring the data for reuse and further investigation using semantic web technologies, and of the practical challenges and compromises of fieldwork within a working theatre. Finally we consider the utility of our tooling from both a user perspective and through an initial quantitative investigation of the data gathered. © K. R. Page, T. Nurmikko-Fuller, C. Rindfleisch, D. M. Weigl, R. Lewis, L. Dreyfus, D. De Roure."
Devaney J.; Arthur C.; Condit-Schultz N.; Nisula K.,Theme and variation encodings with roman numerals (TaVERn): A new data set for symbolic music analysis,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025465927&partnerID=40&md5=cdf6631744730f6500c55627d7c57427,"Devaney J., School of Music, Ohio State University, United States; Arthur C., School of Music, Ohio State University, United States; Condit-Schultz N., School of Music, Ohio State University, United States; Nisula K., School of Music, Ohio State University, United States","The Theme And Variation Encodings with Roman Numerals (TAVERN) dataset consists of 27 complete sets of theme and variations for piano composed between 1765 and 1810 by Mozart and Beethoven. In these theme and variation sets, comparable harmonic structures are realized in different ways. This facilitates an evaluation of the effectiveness of automatic analysis algorithms in generalizing across different musical textures. The pieces are encoded in standard **kern format, with analyses jointly encoded using an extension to **kern. The harmonic content of the music was analyzed with both Roman numerals and function labels in duplicate by two different expert analyzers. The pieces are divided into musical phrases, allowing for multiple-levels of automatic analysis, including chord labeling and phrase parsing. This paper describes the content of the dataset in detail, including the types of chords represented, and discusses the ways in which the analyzers sometimes disagreed on the lower-level harmonic content (the Roman numerals) while converging at similar high-level structures (the function of the chords within the phrase). © Johanna Devaney, Claire Arthur, Nathaniel Condit-Schultz, and Kirsten Nisula."
Rigaud F.; Radenen M.,Singing voice melody transcription using deep neural networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008470713&partnerID=40&md5=454f25e5abb25152c80261764522b751,"Rigaud F., Audionamix R and D, 171 quai de Valmy, Paris, 75010, France; Radenen M., Audionamix R and D, 171 quai de Valmy, Paris, 75010, France","This paper presents a system for the transcription of singing voice melodies in polyphonic music signals based on Deep Neural Network (DNN) models. In particular, a new DNN system is introduced for performing the f0 estimation of the melody, and another DNN, inspired from recent studies, is learned for segmenting vocal sequences. Preparation of the data and learning configurations related to the specificity of both tasks are described. The performance of the melody f0 estimation system is compared with a state-of-the-art method and exhibits highest accuracy through a better generalization on two different music databases. Insights into the global functioning of this DNN are proposed. Finally, an evaluation of the global system combining the two DNNs for singing voice melody transcription is presented. © François Rigaud and Mathieu Radenen."
Eghbal-zadeh H.; Lehner B.; Schedl M.; Widmer G.,I-vectors for timbre-based music similarity and music artist classification,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050540904&partnerID=40&md5=c241a0547ba0f1f9bc02175180b17a45,"Eghbal-zadeh H., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Lehner B., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","In this paper, we present a novel approach to extract song-level descriptors built from frame-level timbral features such as Mel-frequency cepstral coefficient (MFCC). These descriptors are called identity vectors or i-vectors and are the results of a factor analysis procedure applied on frame-level features. The i-vectors provide a low-dimensional and fixed-length representation for each song and can be used in a supervised and unsupervised manner. First, we use the i-vectors for an unsupervised music similarity estimation, where we calculate the distance between i-vectors in order to predict the genre of songs. Second, for a supervised artist classification task we report the performance measures using multiple classifiers trained on the i-vectors. Standard datasets for each task are used to evaluate our method and the results are compared with the state of the art. By only using timbral information, we already achieved the state of the art performance in music similarity (which uses extra information such as rhythm). In artist classification using timbre descriptors, our method outperformed the state of the art. © Hamid Eghbal-zadeh, Bernhard Lehner, Markus Schedl, Gerhard Widmer."
Tralie C.J.; Bendich P.,Cover song identification with timbral shape sequences,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757346&partnerID=40&md5=3b60e074e2af1564d612d6c6ce5cb3f3,"Tralie C.J., Duke University, Department of Electrical and Computer Engineering, United States; Bendich P., Duke University, Department of Mathematics, United States","We introduce a novel low level feature for identifying cover songs which quantifies the relative changes in the smoothed frequency spectrum of a song. Our key insight is that a sliding window representation of a chunk of audio can be viewed as a time-ordered point cloud in high dimensions. For corresponding chunks of audio between different versions of the same song, these point clouds are approximately rotated, translated, and scaled copies of each other. If we treat MFCC embeddings as point clouds and cast the problem as a relative shape sequence, we are able to correctly identify 42/80 cover songs in the “Covers 80” dataset. By contrast, all other work to date on cover songs exclusively relies on matching note sequences from Chroma derived features. © Christopher J. Tralie, Paul Bendich."
Liang D.; Zhan M.; Ellis D.P.W.,Content-aware collaborative music recommendation using pre-trained neural networks,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070002034&partnerID=40&md5=307511cccf0368671db312f3579ae81a,"Liang D., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; Zhan M., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; Ellis D.P.W., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States","Although content is fundamental to our music listening preferences, the leading performance in music recommendation is achieved by collaborative-filtering-based methods which exploit the similarity patterns in user’s listening history rather than the audio content of songs. Meanwhile, collaborative filtering has the well-known “cold-start” problem, i.e., it is unable to work with new songs that no one has listened to. Efforts on incorporating content information into collaborative filtering methods have shown success in many non-musical applications, such as scientific article recommendation. Inspired by the related work, we train a neural network on semantic tagging information as a content model and use it as a prior in a collaborative filtering model. Such a system still allows the user listening data to “speak for itself”. The proposed system is evaluated on the Million Song Dataset and shows comparably better result than the collaborative filtering approaches, in addition to the favorable performance in the cold-start case. © Dawen Liang, Minshu Zhan, Daniel P. W. Ellis."
Makris D.; Kaliakatsos-Papakostas M.; Cambouropoulos E.,Probabilistic modular bass voice leading in melodic harmonisation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047461620&partnerID=40&md5=34cfd942165b96441adb3425c8e8be16,"Makris D., Department of Informatics, Ionian University, Corfu, Greece; Kaliakatsos-Papakostas M., School of Music Studies, Aristotle University of Thessaloniki, Greece; Cambouropoulos E., School of Music Studies, Aristotle University of Thessaloniki, Greece","Probabilistic methodologies provide successful tools for automated music composition, such as melodic harmonisation, since they capture statistical rules of the music idioms they are trained with. Proposed methodologies focus either on specific aspects of harmony (e.g., generating abstract chord symbols) or incorporate the determination of many harmonic characteristics in a single probabilistic generative scheme. This paper addresses the problem of assigning voice leading focussing on the bass voice, i.e., the realisation of the actual bass pitches of an abstract chord sequence, under the scope of a modular melodic harmonisation system where different aspects of the generative process are arranged by different modules. The proposed technique defines the motion of the bass voice according to several statistical aspects: melody voice contour, previous bass line motion, bass-to-melody distances and statistics regarding inversions and note doublings in chords. The aforementioned aspects of voicing are modular, i.e., each criterion is defined by independent statistical learning tools. Experimental results on diverse music idioms indicate that the proposed methodology captures efficiently the voice layout characteristics of each idiom, whilst additional analyses on separate statistically trained modules reveal distinctive aspects of each idiom. The proposed system is designed to be flexible and adaptable (for instance, for the generation of novel blended melodic harmonisations). © Dimos Makris, Maximos Kaliakatsos-Papakostas, Emilios Cambouropoulos."
Kaliakatsos-Papakostas M.; Zacharakis A.; Tsougras C.; Cambouropoulos E.,Evaluating the general chord type representation in tonal music and organising GCT chord labels in functional chord categories,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042477544&partnerID=40&md5=743d24571f868bab9f84e8c8724b878e,"Kaliakatsos-Papakostas M., School of Music Studies, Aristotle University of Thessaloniki, Greece; Zacharakis A., School of Music Studies, Aristotle University of Thessaloniki, Greece; Tsougras C., School of Music Studies, Aristotle University of Thessaloniki, Greece; Cambouropoulos E., School of Music Studies, Aristotle University of Thessaloniki, Greece","The General Chord Type (GCT) representation is appropriate for encoding tone simultaneities in any harmonic context (such as tonal, modal, jazz, octatonic, atonal). The GCT allows the re-arrangement of the notes of a harmonic sonority such that abstract idiom-specific types of chords may be derived. This encoding is inspired by the standard roman numeral chord type labelling and is, therefore, ideal for hierarchic harmonic systems such as the tonal system and its many variations; at the same time, it adjusts to any other harmonic system such as post-tonal, atonal music, or traditional polyphonic systems. In this paper the descriptive potential of the GCT is assessed in the tonal idiom by comparing GCT harmonic labels with human expert annotations (Kostka & Payne harmonic dataset). Additionally, novel methods for grouping and clustering chords, according to their GCT encoding and their functional role in chord sequences, are introduced. The results of both harmonic labelling and functional clustering indicate that the GCT representation constitutes a suitable scheme for representing effectively harmony in computational systems. © Maximos Kaliakatsos-Papakostas, Asterios Zacharakis, Costas Tsougras, Emilios Cambouropoulos."
Elowsson A.,Beat tracking with a cepstroid invariant neural network,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016607210&partnerID=40&md5=0d1575b17ef00ae0e5be25aad149919a,"Elowsson A., KTH Royal Institute of Technology, Sweden","We present a novel rhythm tracking architecture that learns how to track tempo and beats through layered learning. A basic assumption of the system is that humans understand rhythm by letting salient periodicities in the music act as a framework, upon which the rhythmical structure is interpreted. Therefore, the system estimates the cepstroid (the most salient periodicity of the music), and uses a neural network that is invariant with regards to the cepstroid length. The input of the network consists mainly of features that capture onset characteristics along time, such as spectral differences. The invariant properties of the network are achieved by subsampling the input vectors with a hop size derived from a musically relevant subdivision of the computed cepstroid of each song. The output is filtered to detect relevant periodicities and then used in conjunction with two additional networks, which estimates the speed and tempo of the music, to predict the final beat positions. We show that the architecture has a high performance on music with public annotations. © Anders Elowsson."
Dittmar C.; Pfleiderer M.; Müller M.,Automated estimation of ride cymbal swing ratios in jazz recordings,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020404260&partnerID=40&md5=110c3786b410b8b93854cbc4ca91b7f7,"Dittmar C., International Audio Laboratories Erlangen, Germany; Pfleiderer M., Liszt University of Music Weimar, Germany; Müller M., International Audio Laboratories Erlangen, Germany","In this paper, we propose a new method suitable for the automatic analysis of microtiming played by drummers in jazz recordings. Specifically, we aim to estimate the drummers’ swing ratio in excerpts of jazz recordings taken from the Weimar Jazz Database. A first approach is based on automatic detection of ride cymbal (RC) onsets and evaluation of relative time intervals between them. However, small errors in the onset detection propagate considerably into the swing ratio estimates. As our main technical contribution, we propose to use the log-lag autocorrelation function (LLACF) as a mid-level representation for estimating swing ratios, circumventing the error-prone detection of RC onsets. In our experiments, the LLACF-based swing ratio estimates prove to be more reliable than the ones based on RC onset detection. Therefore, the LLACF seems to be the method of choice to process large amounts of jazz recordings. Finally, we indicate some implications of our method for microtiming studies in jazz research. © Christian Dittmar, Martin Pfleiderer, Meinard Müller."
Cherla S.; Tran S.N.; Weyde T.; d’Avila Garcez A.,Hybrid long- and short-term models of folk melodies,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069910924&partnerID=40&md5=6383bfd7ae3920afe46e11f8e3961dba,"Cherla S., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Tran S.N., Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; d’Avila Garcez A., Machine Learning Group, Department of Computer Science, City University London, United Kingdom","In this paper, we present the results of a study on dynamic models for predicting sequences of musical pitch in melodies. Such models predict a probability distribution over the possible values of the next pitch in a sequence, which is obtained by combining the prediction of two components (1) a long-term model (LTM) learned offline on a corpus of melodies, as well as (2) a short-term model (STM) which incorporates context-specific information available during prediction. Both the LTM and the STM learn regularities in pitch sequences solely from data. The models are combined in an ensemble, wherein they are weighted by the relative entropies of their respective predictions. Going by previous work that demonstrates the success of Connectionist LTMs, we employ the recently proposed Recurrent Temporal Discriminative Restricted Boltzmann Machine (RTDRBM) as the LTM here. While it is indeed possible for the same model to also serve as an STM, our experiments showed that n-gram models tended to learn faster than the RTDRBM in an online setting and that the hybrid of an RTDRBM LTM and an n-gram STM gives us the best predictive performance yet on a corpus of monophonic chorale and folk melodies. © Srikanth Cherla, Son N. Tran, Tillman Weyde, Artur d’Avila Garcez."
Savage P.E.; Atkinson Q.D.,Automatic tune family identification by musical sequence alignment,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045054433&partnerID=40&md5=10c51c61a65df7055ad8e7c67cd6f2a4,"Savage P.E., Tokyo University of the Arts, Dept. of Musicology, Japan; Atkinson Q.D., Auckland University, Dept. of Psychology, New Zealand","Musics, like languages and genes, evolve through a process of transmission, variation, and selection. Evolution of musical tune families has been studied qualitatively for over a century, but quantitative analysis has been hampered by an inability to objectively distinguish between musical similarities that are due to chance and those that are due to descent from a common ancestor. Here we propose an automated method to identify tune families by adapting genetic sequence alignment algorithms designed for automatic identification and alignment of protein families. We tested the effectiveness of our method against a high-quality ground-truth dataset of 26 folk tunes from four diverse tune families (two English, two Japanese) that had previously been identified and aligned manually by expert musicologists. We tested different combinations of parameters related to sequence alignment and to modeling of pitch, rhythm, and text to find the combination that best matched the ground-truth classifications. The best-performing automated model correctly grouped 100% (26/26) of the tunes in terms of overall similarity to other tunes, identifying 85% (22/26) of these tunes as forming distinct tune families. The success of our approach on a diverse, cross-cultural ground-truth dataset suggests promise for future automated reconstruction of musical evolution on a wide scale. © Patrick E. Savage, Quentin D. Atkinson."
Ganguli K.K.; Gulati S.; Serra X.; Rao P.,Data-driven exploration of melodic structures in hindustani music,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022075371&partnerID=40&md5=476635bfbb0be4d91966e20dff1c312f,"Ganguli K.K., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","Indian art music is quintessentially an improvisatory music form in which the line between ‘fixed’ and ‘free’ is extremely subtle. In a rāga performance, the melody is loosely constrained by the chosen composition but otherwise improvised in accordance with the rāga grammar. One of the melodic aspects that is governed by this grammar is the manner in which a melody evolves in time in the course of a performance. In this work, we aim to discover such implicit patterns or regularities present in the temporal evolution of vocal melodies of Hindustani music. We start by applying existing tools and techniques used in music information retrieval to a collection of concerts recordings of ālāp performances by renowned khayal vocal artists. We use svara-based and svara duration-based melodic features to study and quantify the manifestation of concepts such as vādi, samvādi, nyās and graha svara in the vocal performances. We show that the discovered patterns corroborate the musicological findings that describe the “unfolding” of a rāga in vocal performances of Hindustani music. The patterns discovered from the vocal melodies might help music students to learn improvisation and can complement the oral music pedagogy followed in this music tradition. © Kaustuv Kanti Ganguli, Sankalp Gulati, Xavier Serra, Preeti Rao."
Dzhambazov G.; Şentürk S.; Serra X.,Searching lyrical phrases in a-capella turkish makam recordings,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069989891&partnerID=40&md5=493354f73ca8ab6a9bca5baa7a7daa77,"Dzhambazov G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Search by lyrics, the problem of locating the exact occurrences of a phrase from lyrics in musical audio, is a recently emerging research topic. Unlike key-phrases in speech, lyrical key-phrases have durations that bear important relation to other musical aspects like the structure of a composition. In this work we propose an approach that address the differences of syllable durations, specific for singing. First a phrase is expanded to MFCC-based phoneme models, trained on speech. Then, we apply dynamic time warping between the phrase and audio to estimate candidate audio segments in the given audio recording. Next, the retrieved audio segments are ranked by means of a novel score-informed hidden Markov model, in which durations of the syllables within a phrase are explicitly modeled. The proposed approach is evaluated on 12 a-capella audio recordings of Turkish Makam music. Relying on standard speech phonetic models, we arrive at promising results that outperform a baseline approach unaware of lyrics durations. To the best of our knowledge, this is the first work tackling the problem of search by lyrical key-phrases. We expect that it can serve as a baseline for further research on singing material with similar musical characteristics. © Georgi Dzhambazov, Sertan Şentürk, Xavier Serra."
Sordo M.; Ogihara M.; Wuchty S.,Analysis of the evolution of research groups and topics in the ISMIR conference,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043707399&partnerID=40&md5=186fcb0bbb4e2cb1f031fdacc793b2eb,"Sordo M., Center for Computational Science, University of Miami, United States; Ogihara M., Dept. of Computer Science, University of Miami, United States; Wuchty S., Dept. of Computer Science, University of Miami, United States","We present an analysis of the topics and research groups that participated in the ISMIR conference over the last 15 years, based on its proceedings. While we first investigate the topological changes of the co-authorship network as well as topics over time, we also identify groups of researchers, allowing us to investigate their evolution and topic dependence. Notably, we find that large groups last longer if they actively alter their membership. Furthermore, such groups tend to cover a wider selection of topics, suggesting that a change of members as well as of research topics increases their adaptability. In turn, smaller groups show the opposite behavior, persisting longer if their membership is altered minimally and focus on a smaller set of topics. Finally, by analyzing the effect of group size and lifespan on research impact, we observed that papers penned by medium sized and long lasting groups tend to have a citation advantage. © Mohamed Sordo, Mitsunori Ogihara, Stefan Wuchty."
Smith J.B.L.; Goto M.,Using priors to improve estimates of music structure,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009854686&partnerID=40&md5=606741b8131adff841f4c4ed599062a8,"Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Existing collections of annotations of musical structure possess many strong regularities: for example, the lengths of segments are approximately log-normally distributed, as is the number of segments per annotation; and the lengths of two adjacent segments are highly likely to have an integer ratio. Since many aspects of structural annotations are highly regular, but few of these regularities are taken into account by current algorithms, we propose several methods of improving predictions of musical structure by using their likelihood according to prior distributions. We test the use of priors to improve a committee of basic segmentation algorithms, and to improve a committee of cutting-edge approaches submitted to MIREX. In both cases, we are unable to improve on the best committee member, meaning that our proposed approach is outperformed by simple parameter tuning. The same negative result was found despite incorporating the priors in multiple ways. To explain the result, we show that although there is a correlation overall between output accuracy and prior likelihood, the weakness of the correlation in the high-likelihood region makes the proposed method infeasible. We suggest that to improve on the state of the art using prior likelihoods, these ought to be incorporated at a deeper level of the algorithm. © Jordan B. L. Smith, Masataka Goto."
Nuanáin C.Ó.; Herrera P.; Jordà S.,An evaluation framework and case study for rhythmic concatenative synthesis,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019968153&partnerID=40&md5=a59c0dd7f3f7860cb74d595d40970995,"Nuanáin C.Ó., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Jordà S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we present and report on a methodology for evaluating a creative MIR-based application of concatenative synthesis. After reviewing many existing applications of concatenative synthesis we have developed an application that specifically addresses loop-based rhythmic pattern generation. We describe how such a system could be evaluated with respect to its its objective retrieval performance and subjective responses of humans in a listener survey. Applying this evaluation strategy produced positive findings to help verify and validate the objectives of our system. We discuss the results of the evaluation and draw conclusions by contrasting the objective analysis with the subjective impressions of the users. © Cárthach Ó Nuanáin, Perfecto Herrera, Sergi Jordà."
Luo Y.-J.; Su L.; Yang Y.-H.; Chi T.-S.,Detection of common mistakes in novice violin playing,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044638302&partnerID=40&md5=4f567b580d4e4f29cdb88467cb705652,"Luo Y.-J., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan, Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Su L., Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Chi T.-S., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musical instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset comprising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are generated from the same feature set with different scales, including two note-level representations and three segment-level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vector machine for classification. Results show that the F-measures using different feature representations can vary up to 20% for two types of playing mistakes. It demonstrates the different sensitivities of each feature representation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed. © Yin-Jyun Luo Li Su Yi-Hsuan Yang and Tai-Shih Chi."
Cogliati A.; Temperley D.; Duan Z.,Transcribing human piano performances into music notation,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017519434&partnerID=40&md5=7a747ffa7f0903679a4a554c115ed932,"Cogliati A., Electrical and Computer Engineering, University of Rochester, United States; Temperley D., Eastman School of Music, University of Rochester, United States; Duan Z., Electrical and Computer Engineering, University of Rochester, United States","Automatic music transcription aims to transcribe musical performances into music notation. However, existing transcription systems that have been described in research papers typically focus on multi-F0 estimation from audio and only output notes in absolute terms, showing frequency and absolute time (a piano-roll representation), but not in musical terms, with spelling distinctions (e.g., Ab versus G#) and quantized meter. To complete the transcription process, one would need to convert the piano-roll representation into a properly formatted and musically meaningful musical score. This process is non-trivial and largely unre-searched. In this paper we present a system that generates music notation output from human-recorded MIDI performances of piano music. We show that the correct estimation of the meter, harmony and streams in a piano performance provides a solid foundation to produce a properly formatted score. In a blind evaluation by professional music theorists, the proposed method outperforms two commercial programs and an open source program in terms of pitch notation and rhythmic notation, and ties for the top in terms of overall voicing and staff placement. © Andrea Cogliati, David Temperley, Zhiyao Duan."
Jin R.; Raphael C.,Graph-based rhythm interpretation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069995407&partnerID=40&md5=5dfe2c8f9f8ec272ae24ade2bd9bf98e,"Jin R., Indiana University, School of Informatics and Computing, United States; Raphael C., Indiana University, School of Informatics and Computing, United States","We present a system that interprets the notated rhythm obtained from optical music recognition (OMR). Our approach represents the notes and rests in a system measure as the vertices of a graph. We connect the graph by adding voice edges and coincidence edges between pairs of vertices, while the rhythmic interpretation follows simply from the connected graph. The graph identification problem is cast as an optimization where each potential edge is scored according to its plausibility. We seek the optimally scoring graph where the score is represented as a sum of edge scores. Experiments were performed on about 60 score pages showing that our system can handle difficult rhythmic situations including multiple voices, voices that merge and split, voices spanning two staves, and missing tuplets. © Rong Jin, Christopher Raphael."
Zhang S.; Repetto R.C.; Serra X.,Predicting pairwise pitch contour relations based on linguistic tone information in Beijing opera singing,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069938667&partnerID=40&md5=e2bdfee592fae56294de8a6a0a13f41e,"Zhang S., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","The similarity between linguistic tones and melodic pitch contours in Beijing Opera can be captured either by the contour shape of single syllable units, or by the pairwise pitch height relations in adjacent syllable units. In this paper, we investigate the latter problem with a novel machine learning approach, using techniques from time series data mining. Approximately 1300 pairwise contour segments are extracted from a selection of 20 arias. We then formulate the problem as a supervised machine-learning task of predicting types of pairwise melodic relations based on linguistic tone information. The results give a comparative view of fixed and mixed-effects models that achieved around 70% of maximum accuracy. We discuss the superiority of the current method to that of the unsupervised learning in single-syllable-unit contour analysis of similarity in Beijing Opera. © Shuo Zhang, Rafael Caro Repetto, Xavier Serra."
Van Balen J.; Burgoyne J.A.; Bountouridis D.; Müllensiefen D.; Veltkamp R.C.,Corpus analysis tools for computational hook discovery,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066095143&partnerID=40&md5=a684729ec22c83848b81b8fd25830c71,"Van Balen J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Burgoyne J.A., Music Cognition Group, University of Amsterdam, Netherlands; Bountouridis D., Department of Information and Computing Sciences, Utrecht University, Netherlands; Müllensiefen D., Department of Psychology, Goldsmiths, University of London, United Kingdom; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Netherlands","Compared to studies with symbolic music data, advances in music description from audio have overwhelmingly focused on ground truth reconstruction and maximizing prediction accuracy, with only a small fraction of studies using audio description to gain insight into musical data. We present a strategy for the corpus analysis of audio data that is optimized for interpretable results. The approach brings two previously unexplored concepts to the audio domain: audio bigram distributions, and the use of corpus-relative or “second-order” descriptors. To test the real-world applicability of our method, we present an experiment in which we model song recognition data collected in a widely-played music game. By using the proposed corpus analysis pipeline we are able to present a cognitively adequate analysis that allows a model interpretation in terms of the listening history and experience of our participants. We find that our corpus-based audio features are able to explain a comparable amount of variance to symbolic features for this task when used alone and that they can supplement symbolic features profitably when the two types of features are used in tandem. Finally, we highlight new insights into what makes music recognizable. © Jan Van Balen, John Ashley Burgoyne, Dimitrios Bountouridis, Daniel Müllensiefen, Remco C. Veltkamp."
Weiß C.; Schaab M.,On the impact of key detection performance for identifying classical music styles,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069983258&partnerID=40&md5=8a7032e61465a68c40a92863461e2497,"Weiß C., Fraunhofer Institute for Digital Media Technology, Ilmenau, Germany; Schaab M., Fraunhofer Institute for Digital Media Technology, Ilmenau, Germany","We study the automatic identification of Western classical music styles by directly using chroma histograms as classification features. Thereby, we evaluate the benefits of knowing a piece’s global key for estimating key-related pitch classes. First, we present four automatic key detection systems. We compare their performance on suitable datasets of classical music and optimize the algorithms’ free parameters. Using a second dataset, we evaluate automatic classification into the four style periods Baroque, Classical, Romantic, and Modern. To that end, we calculate global chroma statistics of each audio track. We then split up the tracks according to major and minor keys and circularly shift the chroma histograms with respect to the tonic note. Based on these features, we train two individual classifier models for major and minor keys. We test the efficiency of four chroma extraction algorithms for classification. Furthermore, we evaluate the impact of key detection performance on the classification results. Additionally, we compare the key-related chroma features to other chroma-based features. We obtain improved performance when using an efficient key detection method for shifting the chroma histograms. © Christof Weiß, Maximilian Schaab."
Liebman E.; Stone P.; White C.N.,How music alters decision making - Impact of music stimuli on emotional classification,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837109&partnerID=40&md5=076e67e20fb02b27ed717edbf136cab2,"Liebman E., Computer Science Department, University of Texas, Austin, United States; Stone P., Computer Science Department, University of Texas, Austin, United States; White C.N., Department of Psychology, Syracuse University, United States","Numerous studies have demonstrated that mood can affect emotional processing. The goal of this study was to explore which components of the decision process are affected when exposed to music; we do so within the context of a stochastic sequential model of simple decisions, the drift-diffusion model (DDM). In our experiment, participants decided whether words were emotionally positive or negative while listening to music that was chosen to induce positive or negative mood. The behavioral results show that the music manipulation was effective, as participants were biased to label words positive in the positive music condition. The DDM shows that this bias was driven by a change in the starting point of evidence accumulation, which indicates an a priori response bias. In contrast, there was no evidence that music affected how participants evaluated the emotional content of the stimuli. To better understand the correspondence between auditory features and decision-making, we proceeded to study how individual aspects of music affect response patterns. Our results have implications for future studies of the connection between music and mood. © Elad Liebman, Peter Stone, Corey N. White."
Xia G.; Wang Y.; Dannenberg R.; Gordon G.,Spectral learning for expressive interactive ensemble music performance,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044844984&partnerID=40&md5=430990a40484b7c8c0c2a91c69ad2550,"Xia G., School of Computer Science, Carnegie Mellon University, United States; Wang Y., School of Computer Science, Carnegie Mellon University, United States; Dannenberg R., School of Computer Science, Carnegie Mellon University, United States; Gordon G., School of Computer Science, Carnegie Mellon University, United States","We apply machine learning to a database of recorded ensemble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers’ musical expression as co-evolving time series and learn their interactive relationship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspondence not only between different performers but also between the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to generate a more human-like interaction. © Guangyu Xia, Yun Wang, Roger Dannenberg, Geoffrey Gordon."
Oramas S.; Sordo M.; Espinosa-Anke L.; Serra X.,A semantic-based approach for artist similarity,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030750396&partnerID=40&md5=0be5c4795e0db187586203a7cb8c65a2,"Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Sordo M., Center for Computational Science, University of Miami, United States; Espinosa-Anke L., TALN Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","This paper describes and evaluates a method for computing artist similarity from a set of artist biographies. The proposed method aims at leveraging semantic information present in these biographies, and can be divided in three main steps, namely: (1) entity linking, i.e. detecting mentions to named entities in the text and linking them to an external knowledge base; (2) deriving a knowledge representation from these mentions in the form of a semantic graph or a mapping to a vector-space model; and (3) computing semantic similarity between documents. We test this approach on a corpus of 188 artist biographies and a slightly larger dataset of 2,336 artists, both gathered from Last.fm. The former is mapped to the MIREX Audio and Music Similarity evaluation dataset, so that its similarity judgments can be used as ground truth. For the latter dataset we use the similarity between artists as provided by the Last.fm API. Our evaluation results show that an approach that computes similarity over a graph of entities and semantic categories clearly outperforms a baseline that exploits word co-occurrences and latent factors. © Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke, Xavier Serra."
Weiß C.; Arifi-Müller V.; Prätzlich T.; Kleinertz R.; Müller M.,Analyzing measure annotations for western classical music recordings,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026841879&partnerID=40&md5=a02732cdd121e1f7d91fe1e05440d870,"Weiß C., International Audio Laboratories, Erlangen, Germany; Arifi-Müller V., International Audio Laboratories, Erlangen, Germany; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Kleinertz R., Institut für Musikwissenschaft, Saarland University, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","This paper approaches the problem of annotating measure positions in Western classical music recordings. Such annotations can be useful for navigation, segmentation, and cross-version analysis of music in different types of representations. In a case study based on Wagner’s opera “Die Walküre”, we analyze two types of annotations. First, we report on an experiment where several human listeners generated annotations in a manual fashion. Second, we examine computer-generated annotations which were obtained by using score-to-audio alignment techniques. As one main contribution of this paper, we discuss the inconsistencies of the different annotations and study possible musical reasons for deviations. As another contribution, we propose a kernel-based method for automatically estimating confidences of the computed annotations which may serve as a first step towards improving the quality of this automatic method. © Christof Weiß, Vlora Arifi-Müller, Thomas Prätzlich, Rainer Kleinertz, Meinard Müller."
Bosch J.J.; Bittner R.M.; Salamon J.; Gómez E.,A comparison of melody extraction methods based on source-filter modelling,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023743808&partnerID=40&md5=8232c0c50e0e1ae57635845be7f88291,"Bosch J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Bittner R.M., Music and Audio Research Laboratory, New York University, United States; Salamon J., Music and Audio Research Laboratory, New York University, United States; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain","This work explores the use of source-filter models for pitch salience estimation and their combination with different pitch tracking and voicing estimation methods for automatic melody extraction. Source-filter models are used to create a mid-level representation of pitch that implicitly incorporates timbre information. The spectrogram of a musical audio signal is modelled as the sum of the leading voice (produced by human voice or pitched musical instruments) and accompaniment. The leading voice is then modelled with a Smoothed Instantaneous Mixture Model (SIMM) based on a source-filter model. The main advantage of such a pitch salience function is that it enhances the leading voice even without explicitly separating it from the rest of the signal. We show that this is beneficial for melody extraction, increasing pitch estimation accuracy and reducing octave errors in comparison with simpler pitch salience functions. The adequate combination with voicing detection techniques based on pitch contour characterisation leads to significant improvements over state-of-the-art methods, for both vocal and instrumental music. © First author, Second author, Third author, Fourth author, Fifth author, Sixth author."
McFee B.; Humphrey E.J.; Urbano J.,A plan for sustainable MIR evaluation,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013331154&partnerID=40&md5=48203c5ba896ab616933c771aebeaacf,"McFee B., Center for Data Science, MARL, New York University, United States; Humphrey E.J., Spotify, Ltd, Sweden; Urbano J., Music Technology Group, Universitat Pompeu Fabra, Spain","The Music Information Retrieval Evaluation eXchange (MIREX) is a valuable community service, having established standard datasets, metrics, baselines, methodologies, and infrastructure for comparing MIR methods. While MIREX has managed to successfully maintain operations for over a decade, its long-term sustainability is at risk. The imposed constraint that input data cannot be made freely available to participants necessitates that all algorithms run on centralized computational resources, which are administered by a limited number of people. This incurs an approximately linear cost with the number of submissions, exacting significant tolls on both human and financial resources, such that the current paradigm becomes less tenable as participation increases. To alleviate the recurring costs of future evaluation campaigns, we propose a distributed, community-centric paradigm for system evaluation, built upon the principles of openness, transparency, reproducibility, and incremental evaluation. We argue that this proposal has the potential to reduce operating costs to sustainable levels. Moreover, the proposed paradigm would improve scalability, and eventually result in the release of large, open datasets for improving both MIR techniques and evaluation methods. © Brian McFee, Eric J. Humphrey, Julián Urbano."
Chen N.; Downie J.S.; Xiao H.; Zhu Y.; Zhu J.,Modified perceptual linear prediction liftered cepstrum (MPLPLC) model for pop cover song recognition,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019656044&partnerID=40&md5=3a704b55d6f2ce0284b3fdffd3bec89b,"Chen N., Dept. of Elec. and Comm. Eng., East China Univ. of Sci. and Tech, China; Downie J.S., Graduate School of Library and Information Science, UIUC, United States; Xiao H., Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, China; Zhu Y., Dept. of Elec. and Comm. Eng., East China Univ. of Sci. and Tech, China; Zhu J., Dept. of Electronic Engineering, Shanghai Jiao Tong University, China","Most of the features of Cover Song Identification (CSI), for example, Pitch Class Profile (PCP) related features, are based on the musical facets shared among cover versions: melody evolution and harmonic progression. In this work, the perceptual feature was studied for CSI. Our idea was to modify the Perceptual Linear Prediction (PLP) model in the field of Automatic Speech Recognition (ASR) by (a) introducing new research achievements in psychophysics, and (b) considering the difference between speech and music signals to make it consistent with human hearing and more suitable for music signal analysis. Furthermore, the obtained Linear Prediction Coefficients (LPCs) were mapped to LPC cepstrum coefficients, on which liftering was applied, to boost the timbre invariance of the resultant feature: Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC). Experimental results showed that both LPC cepstrum coefficients mapping and cepstrum liftering were crucial in ensuring the identification power of the MPLPLC feature. The MPLPLC feature outperformed state-of-the-art features in the context of CSI and in resisting instrumental accompaniment variation. This study verifies that the mature techniques in the ASR or Computational Auditory Scene Analysis (CASA) fields may be modified and included to enhance the performance of the Music Information Retrieval (MIR) scheme. © Ning Chen, J. Stephen Downie, Haidong Xiao, Yu Zhu, Jie Zhu."
Guiomard-Kagan N.; Giraud M.; Groult R.; Levé F.,Comparing voice and stream segmentation algorithms,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063425793&partnerID=40&md5=19582ebbd7660bb954005ac1ab246807,"Guiomard-Kagan N., MIS, U. Picardie Jules Verne (UPJV), Amiens, France; Giraud M., CRIStAL, CNRS, U. Lille, Lille, France; Groult R., MIS, U. Picardie Jules Verne (UPJV), Amiens, France; Levé F., MIS, U. Picardie Jules Verne (UPJV), Amiens, France","Voice and stream segmentation algorithms group notes from polyphonic data into relevant units, providing a better understanding of a musical score. Voice segmentation algorithms usually extract voices from the beginning to the end of the piece, whereas stream segmentation algorithms identify smaller segments. In both cases, the goal can be to obtain mostly monophonic units, but streams with polyphonic data are also relevant. These algorithms usually cluster contiguous notes with close pitches. We propose an independent evaluation of four of these algorithms (Temperley, Chew and Wu, Ishigaki et al., and Rafailidis et al.) using several evaluation metrics. We benchmark the algorithms on a corpus containing the 48 fugues of Well-Tempered Clavier by J. S. Bach as well as 97 files of popular music containing actual polyphonic information. We discuss how to compare together voice and stream segmentation algorithms, and discuss their strengths and weaknesses. © Nicolas Guiomard-Kagan, Mathieu Giraud, Richard Groult, Florence Levé."
Rodríguez-López M.E.; Volk A.,Selective acquisition techniques for enculturation-based melodic phrase segmentation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063015268&partnerID=40&md5=b32895ae4940458b4a601aa885f87f9f,"Rodríguez-López M.E., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands","Automatic melody segmentation is an important yet unsolved problem in Music Information Retrieval. Research in the field of Music Cognition suggests that previous listening experience plays a considerable role in the perception of melodic segment structure. At present automatic melody segmenters that model listening experience commonly do so using unsupervised statistical learning with ‘non-selective’ information acquisition techniques, i.e. the learners gather and store information indiscriminately into memory. In this paper we investigate techniques for ‘selective’ information acquisition, i.e. our learning model uses a goal-oriented approach to select what to store in memory. We test the usefulness of the segmentations produced using selective acquisition learning in a melody classification experiment involving melodies of different cultures. Our results show that the segments produced by our selective learner segmenters substantially improve classification accuracy when compared to segments produced by a non-selective learner segmenter, two local segmentation methods, and two naïve baselines. © Marcelo E. Rodríguez-López, Anja Volk."
Huang Y.-H.; Chen X.; Beck S.; Burn D.; Van Gool L.,Automatic handwritten mensural notation interpreter: From manuscript to MIDI performance,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012864133&partnerID=40&md5=7bf9364a6e8ad469c070904fa27c0509,"Huang Y.-H., ESAT-PSI, iMinds, KU Leuven, Germany; Chen X., ESAT-PSI, iMinds, KU Leuven, Germany; Beck S., Department of Musicology, KU Leuven, Germany; Burn D., Department of Musicology, KU Leuven, Germany; Van Gool L., D-ITET, ETH Zürich, Germany","This paper presents a novel automatic recognition framework for hand-written mensural music. It takes a scanned manuscript as input and yields as output modern music scores. Compared to the previous mensural Optical Music Recognition (OMR) systems, ours shows not only promising performance in music recognition, but also works as a complete pipeline which integrates both recognition and transcription. There are three main parts in this pipeline: i) region-of-interest detection, ii) music symbol detection and classification, and iii) transcription to modern music. In addition to the output in modern notation, our system can generate a MIDI file as well. It provides an easy platform for the musicologists to analyze old manuscripts. Moreover, it renders these valuable cultural heritage resources available to non-specialists as well, as they can now access such ancient music in a better understandable form. c Yu-Hui Huang?, Xuanli Chen?, Serafina Beck†, David Burn†, and Luc Van Gool."
Risk L.; Mok L.; Hankinson A.; Cumming J.,Melodic similarity in traditional French-Canadian instrumental dance tunes,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069998541&partnerID=40&md5=daa788e61fdb22c070f22bfd63beaec0,"Risk L., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Mok L., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Hankinson A., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Cumming J., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada","Commercial recordings of French-Canadian instrumental dance tunes represent a varied and complex corpus of study. This was a primarily aural tradition, transmitted from performer to performer with few notated sources until the late 20th century. Practitioners routinely combined tune segments to create new tunes and personalized settings of existing tunes. This has resulted in a corpus that exhibits an extreme amount of variation, even among tunes with the same name. In addition, the same tune or tune segment may appear under several different names. Previous attempts at building systems for automated retrieval and ranking of instrumental dance tunes perform well for near-exact matching of tunes, but do not work as well in retrieving and ranking, in order of most to least similar, variants of a tune; especially those with variations as extreme as this particular corpus. In this paper we will describe a new approach capable of ranked retrieval of variant tunes, and demonstrate its effectiveness on a transcribed corpus of incipits. © Laura Risk, Lillio Mok, Andrew Hankinson, Julie Cumming."
Humphrey E.J.; Bello J.P.,Four timely insights on automatic chord estimation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066087545&partnerID=40&md5=e6f0b5211ce4308bf7715b714c5d1a76,"Humphrey E.J., Music and Audio Research Laboratory, New York University, United States, MuseAmi, Inc., United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Automatic chord estimation (ACE) is a hallmark research topic in content-based music informatics, but like many other tasks, system performance appears to be converging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite arguably achieving some of the highest results to date, both approaches plateau well short of having solved the problem. Therefore, this work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that invalidate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, standard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conventional approaches conflate the competing goals of recognition and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjective, making the very notion of “ground truth” annotations tenuous. Synthesizing these observations, this paper offers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large. © Eric J. Humphrey, Juan P. Bello."
Cho H.-S.; Lee J.-Y.; Kim H.-G.,Singing voice separation from monaural music based on kernel back-fitting using beta-order spectral amplitude estimation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046986831&partnerID=40&md5=f10f64620217eb803dc6d78338cace30,"Cho H.-S., Kwangwoon University, Seoul, South Korea; Lee J.-Y., Kwangwoon University, Seoul, South Korea; Kim H.-G., Kwangwoon University, Seoul, South Korea","Separating the leading singing voice from the musical background from a monaural recording is a challenging task that appears naturally in several music processing applications. Recently, kernel additive modeling with generalized spatial Wiener filtering (GW) was presented for music/voice separation. In this paper, an adaptive auditory filtering based on β-order minimum mean-square error spectral amplitude estimation (bSA) is applied to the kernel additive modeling for improving the singing voice separation performance from monaural music signal. The proposed algorithm is composed of five modules: short time Fourier transform, music/voice separation based on bSA, determination of back-fitting, back-fitting, and inverse short time Fourier transform. In the proposed method, the Singular Value Decomposition (SVD)-based factorized spectral amplitude exponent β for each kernel component is adaptively calculated for effective bSA-based auditory filtering performance during kernel back-fitting. Using a back-fitting threshold, the kernel back-fitting process can automatically be iteratively performed until convergence. Experimental results show that the proposed method achieves better separation performance than GW based on kernel additive modeling. © Hye-Seung Cho, Jun-Yong Lee, Hyoung-Gook Kim."
Liang C.-Y.; Su L.; Yang Y.-H.; Lin H.-M.,Musical offset detection of pitched instruments: The case of violin,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070003057&partnerID=40&md5=eb79253f54ec57c15ee39cc8429c5759,"Liang C.-Y., Academia Sinica, Taiwan; Su L., Academia Sinica, Taiwan; Yang Y.-H., Academia Sinica, Taiwan; Lin H.-M., University of California, San Diego, United States","Musical offset detection is an integral part of a music signal processing system that requires complete characterization of note events. However, unlike onset detection, offset detection has seldom been the subject of an in-depth study in the music information retrieval community, possibly because of the ambiguity involved in the determination of offset times in music. This paper presents a preliminary study aiming at discussing ways to annotate and to evaluate offset times for pitched non-percussive instruments. Moreover, we conduct a case study of offset detection in violin recordings by evaluating a number of energy, spectral flux, and pitch based methods using a new dataset covering 6 different violin playing techniques. The new dataset, which is going to be shared with the research community, consists of 63 violin recordings that are thoroughly annotated based on perceptual loudness and note transition. The offset detection methods, which are adapted from well-known methods for onset detection, are evaluated using an onset-aware method we propose for this task. Result shows that the accuracy of offset detection is highly dependent on the playing techniques involved. Moreover, pitch-based methods can better get rid of the soft-decaying behavior of offsets and achieve the best result among others. © Che-Yuan Liang, Li Su, Yi-Hsuan Yang, Hsin-Ming Lin."
Lee J.H.; Hu X.; Choi K.; Downie J.S.,Mirex grand challenge 2014 user experience: Qualitative analysis of user feedback,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059545495&partnerID=40&md5=e234ddd75aaa1881354907f5e549dcdf,"Lee J.H., University of Washington, United States; Hu X., University of Hong Kong, Hong Kong; Choi K., University of Illinois, United States; Downie J.S., University of Illinois, United States","Evaluation has always been fundamental to the Music Information Retrieval (MIR) community, as evidenced by the popularity of the Music Information Retrieval Evaluation eXchange (MIREX). However, prior MIREX tasks have primarily focused on testing specialized MIR algorithms that sit on the back end of systems. Not until the Grand Challenge 2014 User Experience (GC14UX) task had the users’ overall interaction and experience with complete systems been formally evaluated. Three systems were evaluated based on five criteria. This paper reports the results of GC14UX, with a special focus on the qualitative analysis of 99 free text responses collected from evaluators. The analysis revealed additional user opinions, not fully captured by score ratings on the given criteria, and demonstrated the challenge of evaluating a variety of systems with different user goals. We conclude with a discussion on the implications of findings and recommendations for future UX evaluation tasks, including adding new criteria: Aesthetics, Performance, and Utility. © Jin Ha Lee, Xiao Hu, Kahyun Choi, J. Stephen Downie."
Schreiber H.,Improving genre annotations for the million song dataset,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070021704&partnerID=40&md5=0ee4870f78349133cd89b7e462258bf3,"Schreiber H., Tagtraum Industries Incorporated, United States","Any automatic music genre recognition (MGR) system must show its value in tests against a ground truth dataset. Recently, the public dataset most often used for this purpose has been proven problematic, because of mislabeling, duplications, and its relatively small size. Another dataset, the Million Song Dataset (MSD), a collection of features and metadata for one million tracks, unfortunately does not contain readily accessible genre labels. Therefore, multiple attempts have been made to add song-level genre annotations, which are required for supervised machine learning tasks. Thus far, the quality of these annotations has not been evaluated. In this paper we present a method for creating additional genre annotations for the MSD from databases, which contain multiple, crowd-sourced genre labels per song (Last.fm, beaTunes). Based on label co-occurrence rates, we derive taxonomies, which allow inference of top-level genres. These are most often used in MGR systems. We then combine multiple datasets using majority voting. This both promises a more reliable ground truth and allows the evaluation of the newly generated and preexisting datasets. To facilitate further research, all derived genre annotations are publicly available on our website. © Hendrik Schreiber."
Tsai T.J.; Prätzlich T.; Müller M.,Known-artist live song ID: A hashprint approach,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021309426&partnerID=40&md5=9aed1942b20bd5e739661c64b6542971,"Tsai T.J., University of California Berkeley, Berkeley, CA, United States; Prätzlich T., International Audio Laboratories Erlangen, Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Erlangen, Germany","The goal of live song identification is to recognize a song based on a short, noisy cell phone recording of a live performance. We propose a system for known-artist live song identification and provide empirical evidence of its feasibility. The proposed system represents audio as a sequence of hashprints, which are binary fingerprints that are derived from applying a set of spectro-temporal filters to a spectrogram representation. The spectro-temporal filters can be learned in an unsupervised manner on a small amount of data, and can thus tailor its representation to each artist. Matching is performed using a cross-correlation approach with downsampling and rescoring. We evaluate our approach on the Gracenote live song identification benchmark data set, and compare our results to five other baseline systems. Compared to the previous state-of-the-art, the proposed system improves the mean reciprocal rank from .68 to .79, while simultaneously reducing the average runtime per query from 10 seconds down to 0.9 seconds. © TJ Tsai, Thomas Prätzlich, Meinard Müller."
Dutta S.; Krishnaraj Sekhar P.V.; Murthy H.A.,Raga verification in carnatic music using longest common segment set,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039958134&partnerID=40&md5=84565ec51d738d5019204103167fcc06,"Dutta S., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India; Krishnaraj Sekhar P.V., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India; Murthy H.A., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India","There are at least 100 rāgas that are regularly performed in Carnatic music concerts. The audience determines the identity of rāgas within a few seconds of listening to an item. Most of the audience consists of people who are only avid listeners and not performers. In this paper, an attempt is made to mimic the listener. A rāga verification framework is therefore suggested. The rāga verification system assumes that a specific rāga is claimed based on similarity of movements and motivic patterns. The system then checks whether this claimed rāga is correct. For every rāga, a set of cohorts are chosen. A rāga and its cohorts are represented using pallavi lines of compositions. A novel approach for matching, called Longest Common Segment Set (LCSS), is introduced. The LCSS scores for a rāga are then normalized with respect to its cohorts in two different ways. The resulting systems and a baseline system are compared for two partitionings of a dataset. A dataset of 30 rāgas from Charsur Foundation 1 is used for analysis. An equal error rate (EER) of 12% is obtained. © Shrey Dutta, Krishnaraj Sekhar PV, Hema A. Murthy."
Szeto W.M.; Wong K.H.,A hierarchical Bayesian framework for score-informed source separation of piano music signals,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023761083&partnerID=40&md5=70991459d3809c447b6090472b1fc2f7,"Szeto W.M., Office of University General Education, Chinese University of Hong Kong, Hong Kong; Wong K.H., Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong","Here we propose a score-informed monaural source separation system to extract every tone from a mixture of piano tone signals. Two sinusoidal models in our earlier work are employed in the above-mentioned system to represent piano tones: the General Model and the Piano Model. The General Model, a variant of sinusoidal modeling, can represent a single tone with high modeling quality, yet it fails to separate mixtures of tones due to the overlapping partials. The Piano Model, on the other hand, is an instrument-specific model tailored for piano. Its modeling quality is lower but it can learn from training data (consisting entirely of isolated tones), resolve the overlapping partials and thus separate the mixtures. We formulate a new hierarchical Bayesian framework to run both Models in the source separation process so that the mixtures with overlapping partials can be separated with high quality. The results show that our proposed system gives robust and accurate separation of piano tone signal mixtures (including octaves) while achieving significantly better quality than those reported in related work done previously. © Wai Man SZETO, Kin Hong WONG."
Knees P.; Faraldo Á.; Herrera P.; Vogl R.; Böck S.; Hörschläger F.; Le Goff M.,Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023766135&partnerID=40&md5=60f22a23a7f178f17bd30a35788fb3e7,"Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Faraldo Á., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Vogl R., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Hörschläger F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Le Goff M., Native Instruments GmbH, Berlin, Germany","We present two new data sets for automatic evaluation of tempo estimation and key detection algorithms. In contrast to existing collections, both released data sets focus on electronic dance music (EDM). The data sets have been automatically created from user feedback and annotations extracted from web sources. More precisely, we utilize user corrections submitted to an online forum to report wrong tempo and key annotations on the Beatport website. Beatport is a digital record store targeted at DJs and focusing on EDM genres. For all annotated tracks in the data sets, samples of at least one-minute-length can be freely downloaded. For key detection, further ground truth is extracted from expert annotations manually assigned to Beatport tracks for benchmarking purposes. The set for tempo estimation comprises 664 tracks and the set for key detection 604 tracks. We detail the creation process of both data sets and perform extensive benchmarks using state-of-the-art algorithms from both academic research and commercial products. © Peter Knees, Ángel Faraldo, Perfecto Herrera, Richard Vogl, Sebastian Böck, Florian Hörschläger, Mickael Le Goff."
Lee J.H.; Kim Y.-S.; Hubbles C.,A look at the cloud from both sides now: An analysis of cloud music service usage,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021801859&partnerID=40&md5=b1a1f26b5c616fb0f77c92e727a04a1b,"Lee J.H., University of Washington, United States; Kim Y.-S., University of Washington, United States; Hubbles C., University of Washington, United States","Despite the increasing popularity of cloud-based music services, few studies have examined how users select and utilize these services, how they manage and access their music collections in the cloud, and the issues or challenges they are facing within these services. In this paper, we present findings from an online survey with 198 responses collected from users of commercial cloud music services, exploring their selection criteria, use patterns, perceived limitations, and future predictions. We also investigate differences in these aspects by age and gender. Our results elucidate previously under-studied changes in music consumption, music listening behaviors, and music technology adoption. The findings also provide insights into how to improve the future design of cloud-based music services, and have broader implications for any cloud-based services designed for managing and accessing personal media collections. © Jin Ha Lee, Yea-Seul Kim, Chris Hubbles."
Inskip C.; Wiering F.,In their own words: Using text analysis to identify musicologists' attitudes towards technology,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041402342&partnerID=40&md5=b6c222d88d9ea0b56f41321c95808516,"Inskip C., Department of Information Studies, University College London, United Kingdom; Wiering F., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands","A widely distributed online survey gathered quantitative and qualitative data relating to the use of technology in the research practices of musicologists. This survey builds on existing work in the digital humanities and provides insights into the specific nature of musicology in relation to use and perceptions of technology. Analysis of the data (n=621) notes the preferences in resource format and the digital skills of the survey participants. The themes of comments on rewards, benefits, frustrations, risks, and limitations are explored using an h-point approach derived from applied linguistics. It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities, and that efforts should be made into supporting development of their digital skills and providing usable, useful and reliable software created with a ‘musicology-centred’ design approach. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level. © Charles Inskip, Frans Wiering."
Vigliensoni G.; Fujinaga I.,"Automatic music recommendation systems: Do demographic, profiling, and contextual features improve their performance?",2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026763411&partnerID=40&md5=31518a6a4ee3ed9fa93a8a84d46a329c,"Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","Traditional automatic music recommendation systems’ performance typically rely on the accuracy of statistical models learned from past preferences of users on music items. However, additional sources of data such as demographic attributes of listeners, their listening behaviour, and their listening contexts encode information about listeners, and their listening habits, that may be used to improve the accuracy of music recommendation models. In this paper we introduce a large dataset of music listening histories with listeners’ demographic information, and a set of features to characterize aspects of people’s listening behaviour. The longevity of the collected listening histories, covering over two years, allows the retrieval of basic forms of listening context. We use this dataset in the evaluation of accuracy of a music artist recommendation model learned from past preferences of listeners on music items and their interaction with several combinations of people’s demographic, profiling, and contextual features. Our results indicate that using listeners’ self-declared age, country, and gender improve the recommendation accuracy by 8 percent. When a new profiling feature termed exploratoryness was added, the accuracy of the model increased by 12 percent. © Gabriel Vigliensoni and Ichiro Fujinaga."
Vieira F.; Andrade N.,Evaluating conflict management mechanisms for online social jukeboxes,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069986457&partnerID=40&md5=cefb6e298bf9e979071a09f228b2f9f4,"Vieira F., Department of Systems and Computing, Universidade Federal de Campina, Grande, Brazil; Andrade N., Department of Systems and Computing, Universidade Federal de Campina, Grande, Brazil","Social music listening is a prevalent and often fruitful experience. Social jukeboxes are systems that enable social music listening with listeners collaboratively choosing the music to be played. Naturally, because music tastes are diverse, using social jukeboxes often involves conflicting interests. Because of that, virtually all social jukeboxes incorporate conflict management mechanisms. In contrast with their widespread use, however, little attention has been given to evaluating how different conflict management mechanisms function to preserve the positive experience of music listeners. This paper presents an experiment with three conflict management mechanisms and three groups of listeners. The mechanisms were chosen to represent those most commonly used in the state of the practice. Our study employs a mixed-methods approach to quantitatively analyze listeners’ satisfaction and to examine their impressions and views on conflict, conflict management mechanisms, and social jukeboxing. © Felipe Vieira, Nazareno Andrade."
Barbancho I.; Tardón L.J.; Barbancho A.M.; Sbert M.,Benford’s law for music analysis,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070001844&partnerID=40&md5=9bfc6bd94e11e5e611a916197e59b73e,"Barbancho I., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Campus Teatinos, Málaga, 29071, Spain; Tardón L.J., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Campus Teatinos, Málaga, 29071, Spain; Barbancho A.M., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Campus Teatinos, Málaga, 29071, Spain; Sbert M., University of Girona, Institute of Informatics and Applications, Campus Montilivi, Girona, 17003, Spain","Benford’s law defines a peculiar distribution of the leading digits of a set of numbers. The behavior is logarithmic, with the leading digit 1 reflecting largest probability of occurrence and the remaining ones showing decreasing probabilities of appearance following a logarithmic trend. Many discussions have been carried out about the application of Benford’s law to many different fields. In this paper, a novel exploitation of Benford’s law for the analysis of audio signals is proposed. Three new audio features based on the evaluation of the degree of agreement of a certain audio dataset to Benford’s law are presented. These new proposed features are succesfully tested in two concrete audio tasks: the detection of artificially assembled chords and the estimation of the quality of the MIDI conversions. © Isabel Barbancho, Lorenzo J. Tardón, Ana M. Barbancho, Mateu Sbert."
Krebs F.; Böck S.; Dorfer M.; Widmer G.,Downbeat tracking using beat-synchronous features and recurrent neural networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013992385&partnerID=40&md5=fe179b9b7d4b5226703edcc9835b8ce8,"Krebs F., Department of Computational, Perception Johannes Kepler University, Linz, Austria; Böck S., Department of Computational, Perception Johannes Kepler University, Linz, Austria; Dorfer M., Department of Computational, Perception Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational, Perception Johannes Kepler University, Linz, Austria","In this paper, we propose a system that extracts the downbeat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results. © Florian Krebs, Sebastian Böck, Matthias Dorfer, Gerhard Widmer."
Stober S.; Sternin A.; Owen A.M.; Grahn J.A.,Towards music imagery information retrieval: Introducing the OpenMIIR dataset of EEG recordings from music perception and imagination,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023777656&partnerID=40&md5=9aaf552fc70b525a2ecfd693d1155cb6,"Stober S., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Sternin A., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Owen A.M., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Grahn J.A., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada","Music imagery information retrieval (MIIR) systems may one day be able to recognize a song from only our thoughts. As a step towards such technology, we are presenting a public domain dataset of electroencephalography (EEG) recordings taken during music perception and imagination. We acquired this data during an ongoing study that so far comprises 10 subjects listening to and imagining 12 short music fragments – each 7–16s long – taken from well-known pieces. These stimuli were selected from different genres and systematically vary along musical dimensions such as meter, tempo and the presence of lyrics. This way, various retrieval scenarios can be addressed and the success of classifying based on specific dimensions can be tested. The dataset is aimed to enable music information retrieval researchers interested in these new MIIR challenges to easily test and adapt their existing approaches for music analysis like fingerprinting, beat tracking, or tempo estimation on EEG data. © Sebastian Stober, Avital Sternin, Adrian M. Owen and Jessica A. Grahn."
Böck S.; Krebs F.; Widmer G.,Joint beat and downbeat tracking with recurrent neural networks,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013934982&partnerID=40&md5=dd15d9a9efd525c318f3371962379c4c,"Böck S., Department of Computational Perception Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception Johannes Kepler University, Linz, Austria","In this paper we present a novel method for jointly extracting beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectrograms is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and downbeat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles. © Sebastian Böck, Florian Krebs, and Gerhard Widmer."
Yoshii K.; Itoyama K.; Goto M.,Infinite superimposed discrete all-pole modeling for multipitch analysis of wavelet spectrograms,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045460223&partnerID=40&md5=8db5269edd46c1ae72b4a72b664b402c,"Yoshii K., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a statistical multipich analyzer based on a source-filter model that decomposes a target music audio signal in terms of three major kinds of sound quantities: pitch (fundamental frequency: F0), timbre (spectral envelope), and intensity (amplitude). If the spectral envelope of an isolated sound is represented by an all-pole filter, linear predictive coding (LPC) can be used for filter estimation in the linear-frequency domain. The main problem of LPC is that although only the amplitudes of harmonic partials are reliable samples drawn from the spectral envelope, the whole spectrum is used for filter estimation. To solve this problem, we propose an infinite superimposed discrete all-pole (iSDAP) model that, given a music signal, can estimate an appropriate number of superimposed harmonic structures whose harmonic partials are drawn from a limited number of spectral envelopes. Our nonparametric Bayesian source-filter model is formulated in the log-frequency domain that better suits the frequency characteristics of human audition. Experimental results showed that the proposed model outperformed the counterpart model formulated in the linear frequency domain. © Kazuyoshi Yoshii, Katsutoshi Itoyama, Masataka Goto."
Ganguli K.K.; Rastogi A.; Pandit V.; Kantan P.; Rao P.,Efficient melodic query based audio search for Hindustani vocal compositions,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039962722&partnerID=40&md5=134b1ff59448827eda7e64875086fbcc,"Ganguli K.K., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Rastogi A., Electrical Engineering, Stanford University, India; Pandit V., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Kantan P., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, India","Time-series pattern matching methods that incorporate time warping have recently been used with varying degrees of success on tasks of search and discovery of melodic phrases from audio for Indian classical vocal music. While these methods perform effectively due to the minimal assumptions they place on the nature of the sampled pitch temporal trajectories, their practical applicability to retrieval tasks on real-world databases is seriously limited by their prohibitively large computational complexity. While dimensionality reduction of the time-series to discrete symbol strings is a standard approach that can exploit computational gains from the data compression as well as the availability of efficient string matching algorithms, the compressed representation of the pitch time series itself is not well understood given the pervasiveness of pitch inflections in the melodic shape of the raga phrases. We propose methods that are informed by domain knowledge to design the representation and to optimize parameter settings for the subsequent string matching algorithm. The methods are evaluated in the context of an audio query based search for Hindustani vocal compositions in audio recordings via the mukhda (refrain of the song). We present results that demonstrate performance close to that achieved by time-series matching but at orders of magnitude reduction in complexity. © Kaustuv Kanti Ganguli, Abhinav Rastogi, Vedhas Pandit, Prithvi Kantan, Preeti Rao."
Lykartsis A.; Wu C.-W.; Lerch A.,Beat histogram features from NMF-based novelty functions for music classification,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059461998&partnerID=40&md5=01ddce35b01dbd64e2388e6fc62a6f0d,"Lykartsis A., Technische Universität Berlin, Audio Communication Group, Germany; Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","In this paper we present novel rhythm features derived from drum tracks extracted from polyphonic music and evaluate them in a genre classification task. Musical excerpts are analyzed using an optimized, partially fixed Non-Negative Matrix Factorization (NMF) method and beat histogram features are calculated on basis of the resulting activation functions for each one out of three drum tracks extracted (Hi-Hat, Snare Drum and Bass Drum). The features are evaluated on two widely used genre datasets (GTZAN and Ballroom) using standard classification methods, concerning the achieved overall classification accuracy. Furthermore, their suitability in distinguishing between rhythmically similar genres and the performance of the features resulting from individual activation functions is discussed. Results show that the presented NMF-based beat histogram features can provide comparable performance to other classification systems, while considering strictly drum patterns. © Athanasios Lykartsis, Chih-Wei Wu, Alexander Lerch."
Allik A.; Fazekas G.; Sandler M.,An ontology for audio features,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000779715&partnerID=40&md5=ef0f8d54b40ede353832243f63b14312,"Allik A., Queen Mary University of London, United Kingdom; Fazekas G., Queen Mary University of London, United Kingdom; Sandler M., Queen Mary University of London, United Kingdom","A plurality of audio feature extraction toolsets and feature datasets are used by the MIR community. Their different conceptual organisation of features and output formats however present difficulties in exchanging or comparing data, while very limited means are provided to link features with content and provenance. These issues are hindering research reproducibility and the use of multiple tools in combination. We propose novel Semantic Web ontologies (1) to provide a common structure for feature data formats and (2) to represent computational workflows of audio features facilitating their comparison. The Audio Feature Ontology provides a descriptive framework for expressing different conceptualisations of and designing linked data formats for content-based audio features. To accommodate different views in organising features, the ontology does not impose a strict hierarchical structure, leaving this open to task and tool specific ontologies that derive from a common vocabulary. The ontologies are based on the analysis of existing feature extraction tools and the MIR literature, which was instrumental in guiding the design process. They are harmonised into a library of modular interlinked ontologies that describe the different entities and activities involved in music creation, production and consumption. © Alo Allik, György Fazekas, Mark Sandler."
Osmalskyj J.; Van Droogenbroeck M.; Embrechts J.-J.,Enhancing cover song identification with hierarchical rank aggregation,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026665573&partnerID=40&md5=ddbfa346945bb1e28252fc1676f35ccd,"Osmalskyj J., INTELSIG Laboratory, University of Liège, Belgium; Van Droogenbroeck M., INTELSIG Laboratory, University of Liège, Belgium; Embrechts J.-J., INTELSIG Laboratory, University of Liège, Belgium","Cover song identification involves calculating pairwise similarities between a query audio track and a database of reference tracks. While most authors make exclusively use of chroma features, recent work tends to demonstrate that combining similarity estimators based on multiple audio features increases the performance. We improve this approach by using a hierarchical rank aggregation method for combining estimators based on different features. More precisely, we first aggregate estimators based on global features such as the tempo, the duration, the overall loudness, the number of beats, and the average chroma vector. Then, we aggregate the resulting composite estimator with four popular state-of-the-art methods based on chromas as well as timbre sequences. We further introduce a refinement step for the rank aggregation called “local Kemenization” and quantify its benefit for cover song identification. The performance of our method is evaluated on the Second Hand Song dataset. Our experiments show a significant improvement of the performance, up to an increase of more than 200 % of the number of queries identified in the Top-1, compared to previous results. © Julien Osmalskyj, Marc Van Droogenbroeck, Jean-Jaques Embrechts."
Jiang Y.; Raphael C.,Instrument identification in optical music recognition,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069994755&partnerID=40&md5=839d8eb818df2f2432c6204cbf4688ad,"Jiang Y., School of Informatics and Computing, Indiana University, Bloomington, United States; Raphael C., School of Informatics and Computing, Indiana University, Bloomington, United States","We present a method for recognizing and interpreting the text labels for the instruments in an orchestra score, thereby associating staves with instruments. This task is one of many necessary in optical music recognition. Our approach treats the score system as the basic unit of processing. A graph structure describes the possible orderings of instruments in the system. Each instrument may apply to several staves, may be represented with several possible text strings, and may appear at several possible positions relative to the staves. We find the optimal labeling of staves using a globally optimal dynamic programming approach that embeds simple template-based optical character reconition within the overall recognition scheme. When given an entire score, we simultaneously optimize on the text labeling for each system, as well as the character template models, thus adapting to the font at hand. Our implementation alternately optimizes over the text label identification and re-estimates the character templates. Experiments are presented on 10 different scores showing a significant improvement due to adaptation. © Yucong Jiang, Christopher Raphael."
Stober S.; Prätzlich T.; Müller M.,Brain beats: Tempo extraction from EEG data,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026847895&partnerID=40&md5=24169d84dca09a2626f634b3a7f15f53,"Stober S., Research Focus Cognititive Sciences, University of Potsdam, Germany; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","This paper addresses the question how music information retrieval techniques originally developed to process audio recordings can be adapted for the analysis of corresponding brain activity data. In particular, we conducted a case study applying beat tracking techniques to extract the tempo from electroencephalography (EEG) recordings obtained from people listening to music stimuli. We point out similarities and differences in processing audio and EEG data and show to which extent the tempo can be successfully extracted from EEG signals. Furthermore, we demonstrate how the tempo extraction from EEG signals can be stabilized by applying different fusion approaches on the mid-level tempogram features. © Sebastian Stober, Thomas Prätzlich, Meinard Müller."
Bansal J.; Woolhouse M.,Predictive power of personality on music-genre exclusivity,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016312813&partnerID=40&md5=4db0668d76a32ccceedc017495fd1a27,"Bansal J., McMaster University, Canada; Woolhouse M., McMaster University, Canada","Studies reveal a strong relationship between personality and preferred musical genre. Our study explored this relationship using a new methodology: genre dispersion among people’s mobile-phone music collections. By analyzing the download behaviours of genre-defined user subgroups, we investigated the following questions: (1) do genre-preferring subgroups show distinct patterns of genre consumption and genre exclusivity; (2) does genre exclusivity relate to Big Five personality factors? We hypothesized that genre-preferring subgroups would vary in genre exclusivity, and that their degree of exclusivity would be linearly associated with the openness personality factor (if people have open personalities, they should be “open” to different musical styles). Consistent with our hypothesis, results showed that greater genre inclusivity, i.e. many genres in people’s music collections, positively associated with openness and (unexpectedly) agreeableness, suggesting that individuals with high openness and agreeableness have wider musical tastes than those with low openness and agreeableness. Our study corroborated previous research linking genre preference and personality, and revealed, in a novel way, the predictive power of personality on music-consumption. © Jotthi Bansal, Matthew Woolhouse."
Cheng T.; Mauch M.; Benetos E.; Dixon S.,An attack/decay model for piano transcription,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017522507&partnerID=40&md5=fa1298a2ae508c51b4df50fbb4e805e6,"Cheng T., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activation that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcription is performed in a supervised way, with the training and test datasets produced by the same piano. First we train parameters for the attack and decay components on isolated notes, then update only the note activations for transcription. Experiments show that the proposed model achieves 82% on note-wise and 79% on frame-wise F-measures on the ‘ENSTDkCl’ subset of the MAPS database, outperforming the current published state of the art. © Tian Cheng, Matthias Mauch, Emmanouil Benetos and Simon Dixon."
Fu M.; Xia G.; Dannenberg R.; Wasserman L.,A statistical view on the expressive timing of piano rolled chords,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070021284&partnerID=40&md5=28edff1c3bd6eca05be5869cb5658196,"Fu M., School of Music, Carnegie Mellon University, United States; Xia G., School of Computer Science, Carnegie Mellon University, United States; Dannenberg R., School of Computer Science, Carnegie Mellon University, United States; Wasserman L., School of Computer Science, Carnegie Mellon University, United States","Rolled or arpeggiated chords are notated chords performed by playing the notes sequentially, usually from lowest to highest in pitch. Arpeggiation is a characteristic of musical expression, or expressive timing, in piano performance. However, very few studies have investigated rolled chord performance. In this paper, we investigate two expressive timing properties of piano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can functionally replace the onsets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We ask two research questions. First, what is the equivalent onset of a rolled chord? Second, are the onset spans of different chords interpreted in the same way? The first question is answered by local tempo estimation while the second question is answered by Analysis of Variance. Also, we contribute a piano duet dataset for rolled chords analysis and other studies on expressive music performance. The dataset contains three pieces of music, each performed multiple times by different pairs of musicians. © Mutian Fu, Guangyu Xia, Roger Dannenberg, Larry Wasserman."
Summers C.; Popp P.,Temporal music context identification with user listening data,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063021698&partnerID=40&md5=d7ec25e1ded6f02063364a5405a4e6eb,"Summers C., Gracenote, India; Popp P., Gracenote, India","The times when music is played can indicate context for listeners. From the peaceful song for waking up each morning to the traditional song for celebrating a holiday to an up-beat song for enjoying the summer, the relationship between the music and the temporal context is clearly important. For music search and recommendation systems, an understanding of these relationships provides a richer environment to discover and listen. But with the large number of tracks available in music catalogues today, manually labeling track-temporal context associations is difficult, time consuming, and costly. This paper examines track-day contexts with the purpose of identifying relationships with specific music tracks. Improvements are made to an existing method for classifying Christmas tracks and a generalization to the approach is shown that allows automated discovery of music for any day of the year. Analyzing the top 50 tracks obtained from this method for three well-known holidays, Halloween, Saint Patrick’s Day, and July 4th, precision@50 was 95%, 99%, and 73%, respectively. © Cameron Summers, Phillip Popp."
Kuribayashi T.; Asano Y.; Yoshikawa M.,Towards support for understanding classical music: Alignment of content descriptions on the web,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062743588&partnerID=40&md5=5ff82315785813346a2ddf3071172996,"Kuribayashi T., Graduate School of Informatics, Kyoto University, Japan; Asano Y., Graduate School of Informatics, Kyoto University, Japan; Yoshikawa M., Graduate School of Informatics, Kyoto University, Japan","Supporting the understanding of classical music is an important topic that involves various research fields such as text analysis and acoustics analysis. Content descriptions are explanations of classical music compositions that help a person to understand technical aspects of the music. Recently, Kuribayashi et al. proposed a method for obtaining content descriptions from the web. However, the content descriptions on a single page frequently explain a specific part of a composition only. Therefore, a person who wants to fully understand the composition suffers from a time-consuming task, which seems almost impossible for a novice of classical music. To integrate the content descriptions obtained from multiple pages, we propose a method for aligning each pair of paragraphs of such descriptions. Using dynamic time warping-based method along with our new ideas, (a) a distribution-based distance measure named w2DD, and (b) the concept of passage expressions, it is possible to align content descriptions of classical music better than when using cutting-edge text analysis methods. Our method can be extended in future studies to create applications systems to integrate descriptions with musical scores and performances. © Taku Kuribayashi, Yasuhito Asano, Masatoshi Yoshikawa."
Zangerle E.; Pichl M.; Hupfauf B.; Specht G.,Can microblogs predict music charts? An analysis of the relationship between #nowplaying tweets and music charts,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016319689&partnerID=40&md5=97e132a401b9768ad97d1666ee4d6704,"Zangerle E., Department of Computer Science, University of Innsbruck, Austria; Pichl M., Department of Computer Science, University of Innsbruck, Austria; Hupfauf B., Department of Computer Science, University of Innsbruck, Austria; Specht G., Department of Computer Science, University of Innsbruck, Austria","Twitter is one of the leading social media platforms, where hundreds of millions of tweets cover a wide range of topics, including the music a user is listening to. Such #nowplaying tweets may serve as an indicator for future charts, however, this has not been thoroughly studied yet. Therefore, we investigate to which extent such tweets correlate with the Billboard Hot 100 charts and whether they allow for music charts prediction. The analysis is based on #nowplaying tweets and the Billboard charts of the years 2014 and 2015. We analyze three different aspects in regards to the time series representing #nowplaying tweets and the Billboard charts: (i) the correlation of Twitter and the Billboard charts, (ii) the temporal relation between those two and (iii) the prediction performance in regards to charts positions of tracks. We find that while there is a mild correlation between tweets and the charts, there is a temporal lag between these two time series for 90% of all tracks. As for the predictive power of Twitter, we find that incorporating Twitter information in a multivariate model results in a significant decrease of both the mean RMSE as well as the variance of rank predictions. © Eva Zangerle, Martin Pichl, Benedikt Hupfauf, Günther Specht."
Kirlin P.B.; Thomas D.L.,Extending a model of monophonic hierarchical music analysis to homophony,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070012603&partnerID=40&md5=6fb419cf600883933e4f4125160066b9,"Kirlin P.B., Department of Mathematics and Computer Science, Rhodes College, United States; Thomas D.L., Department of Mathematics and Computer Science, Rhodes College, United States","Computers are now powerful enough and data sets large enough to enable completely data-driven studies of Schenkerian analysis, the most well-established variety of hierarchical music analysis. In particular, we now have probabilistic models that can be trained via machine learning algorithms to analyze music in a hierarchical fashion as a music theorist would. Most of these models, however, only analyze the monophonic melodic content of the music, as opposed to taking all of the musical voices into account. In this paper, we explore the feasibility of extending a probabilistic model developed for analyzing monophonic music to function with homophonic music. We present details of the new model, an algorithm for determining the most probable analysis of the music, and a number of experiments evaluating the quality of the analyses predicted by the model. We also describe how varying the way the model interprets rests in the input music affects the resulting analyses produced. © Phillip B. Kirlin and David L. Thomas."
Lee J.H.; Price R.,Understanding users of commercial music services through personas: Design implications,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044233522&partnerID=40&md5=6ad5f56841daf23d86ff235e08557e39,"Lee J.H., University of Washington, United States; Price R., University of Washington, United States","Most of the previous literature on music users’ needs, habits, and interactions with music information retrieval (MIR) systems focuses on investigating user groups of particular demographics or testing the usability of specific interfaces/systems. In order to improve our understanding of how users’ personalities and characteristics affect their needs and interactions with MIR systems, we conducted a qualitative user study across multiple commercial music services, utilizing interviews and think-aloud sessions. Based on the empirical user data, we have developed seven personas. These personas offer a deeper understanding of the different types of MIR system users and the relative importance of various design implications for each user type. Implications for system design include a renegotiation of our understanding of desired user engagement, especially with the habit of context-switching, designing systems for specialized uses, and addressing user concerns around privacy, transparency, and control. © Jin Ha Lee, Rachel Price."
Liem C.C.S.; Hanjalic A.,Comparative analysis of orchestral performance recordings: An image-based approach,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057958699&partnerID=40&md5=2f79861325d91508624e254dc293df00,"Liem C.C.S., Delft University of Technology, Multimedia Computing Group, Netherlands; Hanjalic A., Delft University of Technology, Multimedia Computing Group, Netherlands","Traditionally, the computer-assisted comparison of multiple performances of the same piece focused on performances on single instruments. Due to data availability, there also has been a strong bias towards analyzing piano performances, in which local timing, dynamics and articulation are important expressive performance features. In this paper, we consider the problem of analyzing multiple performances of the same symphonic piece, performed by different orchestras and different conductors. While differences between interpretations in this genre may include commonly studied features on timing, dynamics and articulation, the timbre of the orchestra and choices of balance within the ensemble are other important aspects distinguishing different orchestral interpretations from one another. While it is hard to model these higher-level aspects as explicit audio features, they can usually be noted visually in spectrogram plots. We therefore propose a method to compare orchestra performances by examining visual spectrogram characteristics. Inspired by eigenfaces in human face recognition, we apply Principal Components Analysis on synchronized performance fragments to localize areas of cross-performance variation in time and frequency. We discuss how this information can be used to examine performer differences, and how beyond pairwise comparison, relative differences can be studied between multiple performances in a corpus at once. © Cynthia C. S. Liem, Alan Hanjalic."
Gupta S.; Srinivasamurthy A.; Kumar M.; Murthy H.A.; Serra X.,Discovery of syllabic percussion patterns in tabla solo recordings,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022056941&partnerID=40&md5=d5a89ec6a8d3f318b602631dc3a997c0,"Gupta S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Kumar M., DONlab, Indian Institute of Technology Madras, Chennai, India; Murthy H.A., DONlab, Indian Institute of Technology Madras, Chennai, India; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We address the unexplored problem of percussion pattern discovery in Indian art music. Percussion in Indian art music uses onomatopoeic oral mnemonic syllables for the transmission of repertoire and technique. This is utilized for the task of percussion pattern discovery from audio recordings. From a parallel corpus of audio and expert curated scores for 38 tabla solo recordings, we use the scores to build a set of most frequent syllabic patterns of different lengths. From this set, we manually select a subset of musically representative query patterns. To discover these query patterns in an audio recording, we use syllable-level hidden Markov models (HMM) to automatically transcribe the recording into a syllable sequence, in which we search for the query pattern instances using a Rough Longest Common Subsequence (RLCS) approach. We show that the use of RLCS makes the approach robust to errors in automatic transcription, significantly improving the pattern recall rate and F-measure. We further propose possible enhancements to improve the results. © Swapnil Gupta, Ajay Srinivasamurthy, Manoj Kumar, Hema A. Murthy, Xavier Serra."
Oramas S.; Espinosa-Anke L.; Lawlor A.; Serra X.; Saggion H.,Exploring customer reviews for music genre classification and evolutionary studies,2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021923306&partnerID=40&md5=7897216fb862065646c7301ae3c34008,"Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Espinosa-Anke L., TALN Group, Universitat Pompeu Fabra, Spain; Lawlor A., Insight Centre for Data Analytics, University College of Dublin, Ireland; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain; Saggion H., TALN Group, Universitat Pompeu Fabra, Spain","In this paper, we explore a large multimodal dataset of about 65k albums constructed from a combination of Amazon customer reviews, MusicBrainz metadata and AcousticBrainz audio descriptors. Review texts are further enriched with named entity disambiguation along with polarity information derived from an aspect-based sentiment analysis framework. This dataset constitutes the cornerstone of two main contributions: First, we perform experiments on music genre classification, exploring a variety of feature types, including semantic, sentimental and acoustic features. These experiments show that modeling semantic information contributes to outperforming strong bag-of-words baselines. Second, we provide a diachronic study of the criticism of music genres via a quantitative analysis of the polarity associated to musical aspects over time. Our analysis hints at a potential correlation between key cultural and geopolitical events and the language and evolving sentiments found in music reviews. © Sergio Oramas, Luis Espinosa-Anke, Aonghus Lawlor, Xavier Serra, Horacio Saggion."
Silva D.F.; Souza V.M.A.; Batista G.E.A.P.A.,Music shapelets for fast cover song recognition,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029375687&partnerID=40&md5=6bb0f7bd4335c050493b3d9f7e99c3a4,"Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Souza V.M.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil","A cover song is a new performance or recording of a previously recorded music by an artist other than the original one. The automatic identification of cover songs is useful for a wide range of tasks, from fans looking for new versions of their favorite songs to organizations involved in licensing copyrighted songs. This is a difficult task given that a cover may differ from the original song in key, timbre, tempo, structure, arrangement and even language of the vocals. Cover song identification has attracted some attention recently. However, most of the state-of-the-art approaches are based on similarity search, which involves a large number of similarity computations to retrieve potential cover versions for a query recording. In this paper, we adapt the idea of time series shapelets for content-based music retrieval. Our proposal adds a training phase that finds small excerpts of feature vectors that best describe each song. We demonstrate that we can use such small segments to identify cover songs with higher identification rates and more than one order of magnitude faster than methods that use features to describe the whole music. © Diego F. Silva, Vinícius M. A. Souza, Gustavo E. A. P. A. Batista."
Repetto R.C.; Gong R.; Kroher N.; Serra X.,Comparison of the singing style of two Jingju schools,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021959833&partnerID=40&md5=b987caa32c80114e47bae860180cd7bd,"Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gong R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Kroher N., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Performing schools (liupai) in jingju (also known as Peking or Beijing opera) are one of the most important elements for the appreciation of this genre among connoisseurs. In the current paper, we study the potential of MIR techniques for supporting and enhancing musicological descriptions of the singing style of two of the most renowned jingju schools for the dan role-type, namely Mei and Cheng schools. To this aim, from the characteristics commonly used for describing singing style in musicological literature, we have selected those that can be studied using standard audio features. We have selected eight recordings from our jingju music research corpus and have applied current algorithms for the measurement of the selected features. Obtained results support the descriptions from musicological sources in all cases but one, and also add precision to them by providing specific measurements. Besides, our methodology suggests some characteristics not accounted for in our musicological sources. Finally, we discuss the need for engaging jingju experts in our future research and applying this approach for musicological and educational purposes as a way of better validating our methodology. © Rafael Caro Repetto, Rong Gong, Nadine Kroher, Xavier Serra."
Pugin L.; Zitellini R.; Roland P.,Verovio: A library for engraving MEI music notation into SVG,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034269070&partnerID=40&md5=7c7185988d10ba74f67d722168a0431f,"Pugin L., Swiss RISM Office, Switzerland; Zitellini R., Swiss RISM Office, Switzerland; Roland P., University of Virginia, United States","Rendering symbolic music notation is a common component of many MIR applications, and many tools are available for this task. There is, however, a need for a tool that can natively render the Music Encoding Initiative (MEI) notation encodings that are increasingly used in music research projects. In this paper, we present Verovio, a library and toolkit for rendering MEI. A significant advantage of Verovio is that it implements MEI’s structure internally, making it the best suited solution for rendering features that make MEI unique. Verovio is designed as a fast, portable, lightweight tool written in pure standard C++ with no dependencies on third-party frameworks or libraries. It can be used as a command-line rendering tool, as a library, or it can be compiled to JavaScript using the Emscripten LLVM-to-JavaScript compiler. This last option is particularly interesting because it provides a complete in-browser music MEI typesetter. The SVG output from Verovio is organized in such a way that the MEI structure is preserved as much as possible. Since every graphic in SVG is an XML element that is easily addressable, Verovio is particularly well-suited for interactive applications, especially in web browsers. Verovio is available under the GPL open-source license. © Laurent Pugin, Rodolfo Zitellini, Perry Roland."
Saurel P.; Rousseaux F.; Danger M.,On the changing regulations of privacy and personal information in MIR,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047245776&partnerID=40&md5=e99bb75f81933638fc72f112a215f6d2,"Saurel P., Université Paris-Sorbonne, France; Rousseaux F., IRCAM, France; Danger M., ADAMI, France","In recent years, MIR research has continued to focus more and more on user feedback, human subjects data, and other forms of personal information. Concurrently, the European Union has adopted new, stringent regulations to take effect in the coming years regarding how such information can be collected, stored and manipulated, with equally strict penalties for being found in violation of the law. Here, we provide a summary of these changes, consider how they relate to our data sources and research practices, and identify promising methodologies that may serve researchers well, both in order to be in compliance with the law and conduct more subject-friendly research. We additionally provide a case study of how such changes might affect a recent human subjects project on the topic of style, and conclude with a few recommendations for the near future. This paper is not intended to be legal advice: our personal legal interpretations are strictly mentioned for illustration purpose, and reader should seek proper legal counsel. © Pierre Saurel, Francis Rousseaux, Marc Danger."
Sutcliffe R.; Crawford T.; Fox C.; Root D.L.; Hovy E.; Lewis R.,Relating natural language text to musical passages,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006247222&partnerID=40&md5=0b007103d83e2ae4c3919c3a1b54d36b,"Sutcliffe R., School of CSEE, University of Essex, Colchester, United Kingdom; Crawford T., Dept of Computing Goldsmiths, University of London, United Kingdom; Fox C., School of CSEE, University of Essex, Colchester, United Kingdom; Root D.L., Department of Music, University of Pittsburgh, Pittsburgh, PA, United States; Hovy E., Lang Technologies Inst, Carnegie-Mellon Univ, Pittsburgh, PA, United States; Lewis R., Dept of Computing Goldsmiths, University of London, United Kingdom","There is a vast body of musicological literature containing detailed analyses of musical works. These texts make frequent references to musical passages in scores by means of natural language phrases. Our long-term aim is to investigate whether these phrases can be linked automatically to the musical passages to which they refer. As a first step, we have organised for two years running a shared evaluation in which participants must develop software to identify passages in a MusicXML score based on a short noun phrase in English. In this paper, we present the rationale for this work, discuss the kind of references to musical passages which can occur in actual scholarly texts, describe the first two years of the evaluation and finally appraise the results to establish what progress we have made. © R. Sutcliffe, T. Crawford, C. Fox, D.L. Root, E. Hovy and R. Lewis."
Wang C.-I.; Hsu J.; Dubnov S.,Music pattern discovery with variable Markov oracle: A unified approach to symbolic and audio representations,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005958076&partnerID=40&md5=09039250d364adf18168e8c6091a7356,"Wang C.-I., Music Department, University of California, San Diego, United States; Hsu J., Music Department, University of California, San Diego, United States; Dubnov S., Music Department, University of California, San Diego, United States","This paper presents a framework for automatically discovering patterns in a polyphonic music piece. The proposed framework is capable of handling both symbolic and audio representations. Chroma features are post-processed with heuristics stemming from musical knowledge and fed into the pattern discovery framework. The pattern-finding algorithm is based on Variable Markov Oracle. The Variable Markov Oracle data structure is capable of locating repeated suffixes within a time series, thus making it an appropriate tool for the pattern discovery task. Evaluation of the proposed framework is performed on the JKU Patterns Development Dataset with state of the art performance. © Cheng-i Wang, Jennifer Hsu and Shlomo Dubnov."
Barbancho I.; Tzanetakis G.; Tardón L.J.; Driessen P.F.; Barbancho A.M.,Estimation of the direction of strokes and arpeggios,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052464452&partnerID=40&md5=b56dbeeda14838cebe19d99e58378cdd,"Barbancho I., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Málaga, 29071, Spain; Tzanetakis G., University of Victoria, Department of Computer Science, Victoria, Canada; Tardón L.J., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Málaga, 29071, Spain; Driessen P.F., University of Victoria, Department of Computer Science, Victoria, Canada; Barbancho A.M., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Málaga, 29071, Spain","Whenever a chord is played in a musical instrument, the notes are not commonly played at the same time. Actually, in some instruments, it is impossible to trigger multiple notes simultaneously. In others, the player can consciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke). In this paper, we describe a system to automatically estimate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analysis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direction, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ. © Isabel Barbancho, George Tzanetakis, Lorenzo J. Tardón, Peter F. Driessen, Ana M. Barbancho."
Chen Y.-P.; Su L.; Yang Y.-H.,Electric guitar playing technique detection in real-world recordings based on F0 sequence pattern recognition,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990981376&partnerID=40&md5=c3f25d281fbd2b06e052aec4d4d4f227,"Chen Y.-P., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","For a complete transcription of a guitar performance, the detection of playing techniques such as bend and vibrato is important, because playing techniques suggest how the melody is interpreted through the manipulation of the guitar strings. While existing work mostly focused on playing technique detection for individual single notes, this paper attempts to expand this endeavor to recordings of guitar solo tracks. Specifically, we treat the task as a time sequence pattern recognition problem, and develop a two-stage framework for detecting five fundamental playing techniques used by the electric guitar. Given an audio track, the first stage identifies prominent candidates by analyzing the extracted melody contour, and the second stage applies a pre-trained classifier to the candidates for playing technique detection using a set of timbre and pitch features. The effectiveness of the proposed framework is validated on a new dataset comprising of 42 electric guitar solo tracks without accompaniment, each of which covers 10 to 25 notes. The best average F-score achieves 74% in two-fold cross validation. Furthermore, we also evaluate the performance of the proposed framework for bend detection in five studio mixtures, to discuss how it can be applied in transcribing real-world electric guitar solos with accompaniment. © Yuan-Ping Chen, Li Su, Yi-Hsuan Yang."
Bazzica A.; Liem C.C.S.; Hanjalic A.,Exploiting instrument-wise playing/non-playing labels for score synchronization of symphonic music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023765922&partnerID=40&md5=d42901b0000e8c5ef4e60739688e0ae4,"Bazzica A., Delft University of Technology, Netherlands; Liem C.C.S., Delft University of Technology, Netherlands; Hanjalic A., Delft University of Technology, Netherlands","Synchronization of a score to an audio-visual music performance recording is usually done by solving an audio-to-MIDI alignment problem. In this paper, we focus on the possibility to represent both the score and the performance using information about which instrument is active at a given time stamp. More specifically, we investigate to what extent instrument-wise “playing” (P) and “non-playing” (NP) labels are informative in the synchronization process and what role the visual channel can have for the extraction of P/NP labels. After introducing the P/NP-based representation of the music piece, both at the score and performance level, we define an efficient way of computing the distance between the two representations, which serves as input for the synchronization step based on dynamic time warping. In parallel with assessing the effectiveness of the proposed representation, we also study its robustness when missing and/or erroneous labels occur. Our experimental results show that P/NP-based music piece representation is informative for performance-to-score synchronization and may benefit the existing audio-only approaches. © Alessio Bazzica, Cynthia C. S. Liem, Alan Hanjalic."
Grill T.; Schlüter J.,Music boundary detection using neural networks on combined features and two-level annotations,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009059267&partnerID=40&md5=1570f1976d1c5536e1bb3fd9862a4fe2,"Grill T., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schlüter J., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","The determination of structural boundaries is a key task for understanding the structure of a musical piece, but it is also highly ambiguous. Recently, Convolutional Neural Networks (CNN) trained on spectrogram features and human annotations have been successfully used to tackle the problem, but still fall clearly behind human performance. We expand on the CNN approach by combining spectrograms with self-similarity lag matrices as audio features, thereby capturing more facets of the underlying structural information. Furthermore, in order to consider the hierarchical nature of structural organization, we explore different strategies to learn from the two-level annotations of main and secondary boundaries available in the SALAMI structural annotation dataset. We show that both measures improve boundary recognition performance, resulting in a significant improvement over the previous state of the art. As a side-effect, our algorithm can predict boundaries on two different structural levels, equivalent to the training data. © Thomas Grill and Jan Schlüter."
Bittner R.M.; Salamon J.; Essid S.; Bello J.P.,Melody extraction by contour classification,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008455727&partnerID=40&md5=2c82d44c632018bced7195a69f6a3df7,"Bittner R.M., Music and Audio Research Lab, New York University, United States; Salamon J., Music and Audio Research Lab, New York University, United States, Center for Urban Science and Progress, New York University, United States; Essid S., Télécom Paris-Tech, France; Bello J.P., Music and Audio Research Lab, New York University, United States","Due to the scarcity of labeled data, most melody extraction algorithms do not rely on fully data-driven processing blocks but rather on careful engineering. For example, the Melodia melody extraction algorithm employs a pitch contour selection stage that relies on a number of heuristics for selecting the melodic output. In this paper we explore the use of a discriminative model to perform purely data-driven melodic contour selection. Specifically, a discriminative binary classifier is trained to distinguish melodic from non-melodic contours. This classifier is then used to predict likelihoods for a track’s extracted contours, and these scores are decoded to generate a single melody output. The results are compared with the Melodia algorithm and with a generative model used in a previous study. We show that the discriminative model outperforms the generative model in terms of contour classification accuracy, and the melody output from our proposed system performs comparatively to Melodia. The results are complemented with error analysis and avenues for future improvements. © Rachel M. Bittner, Justin Salamon, Slim Essid, Juan P. Bello."
Vad B.; Boland D.; Williamson J.; Murray-Smith R.; Steffensen P.B.,Design and evaluation of a probabilistic music projection interface,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011647502&partnerID=40&md5=b23d96bddca43a90fea11118afc349b2,"Vad B., School of Computing Science, University of Glasgow, United Kingdom; Boland D., School of Computing Science, University of Glasgow, United Kingdom; Williamson J., School of Computing Science, University of Glasgow, United Kingdom; Murray-Smith R., School of Computing Science, University of Glasgow, United Kingdom; Steffensen P.B., Syntonetic A/S, Copenhagen, Denmark","We describe the design and evaluation of a probabilistic interface for music exploration and casual playlist generation. Predicted subjective features, such as mood and genre, inferred from low-level audio features create a 34-dimensional feature space. We use a nonlinear dimensionality reduction algorithm to create 2D music maps of tracks, and augment these with visualisations of probabilistic mappings of selected features and their uncertainty. We evaluated the system in a longitudinal trial in users’ homes over several weeks. Users said they had fun with the interface and liked the casual nature of the playlist generation. Users preferred to generate playlists from a local neighbourhood of the map, rather than from a trajectory, using neighbourhood selection more than three times more often than path selection. Probabilistic highlighting of subjective features led to more focused exploration in mouse activity logs, and 6 of 8 users said they preferred the probabilistic highlighting mode. © Beatrix Vad, Daniel Boland, John Williamson, Roderick Murray-Smith, Peter Berg Steffensen."
Lehner B.; Widmer G.,Monaural blind source separation in the context of vocal detection,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986187419&partnerID=40&md5=98dd86cc17a16eb871b41e209be71c2f,"Lehner B., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","In this paper, we evaluate the usefulness of several monaural blind source separation (BSS) algorithms in the context of vocal detection (VD). BSS is the problem of recovering several sources, given only a mixture. VD is the problem of automatically identifying the parts in a mixed audio signal, where at least one person is singing. We compare the results of three different strategies for utilising the estimated singing voice signals from four state-of-the-art source separation algorithms. In order to assess the performance of those strategies on an internal data set, we use two different feature sets, each fed to two different classifiers. After selecting the most promising approach, the results on two publicly available data sets are presented. In an additional experiment, we use the improved VD for a simple post-processing technique: For the final estimation of the source signals, we decide to use either silence, or the mixed, or the separated signals, according to the VD. The results of traditionally used BSS evaluation methods suggest that this is useful for both the estimated background signals, as well as for the estimated vocals. © Bernhard Lehner, Gerhard Widmer."
Schlüter J.; Grill T.,Exploring data augmentation for improved singing voice detection with neural networks,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973376414&partnerID=40&md5=5f6ffe91142a6f9fd5c15caf8de1ff15,"Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Grill T., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","In computer vision, state-of-the-art object recognition systems rely on label-preserving image transformations such as scaling and rotation to augment the training datasets. The additional training examples help the system to learn invariances that are difficult to build into the model, and improve generalization to unseen data. To the best of our knowledge, this approach has not been systematically explored for music signals. Using the problem of singing voice detection with neural networks as an example, we apply a range of label-preserving audio transformations to assess their utility for music data augmentation. In line with recent research in speech recognition, we find pitch shifting to be the most helpful augmentation method. Combined with time stretching and random frequency filtering, we achieve a reduction in classification error between 10 and 30%, reaching the state of the art on two public datasets. We expect that audio data augmentation would yield significant gains for several other sequence labelling and event detection tasks in music information retrieval. © Jan Schlüter and Thomas Grill."
Laplante A.,Improving music recommender systems: What can we learn from research on music tastes?,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060993339&partnerID=40&md5=7b6e06ac96b29531fb95a871d99acc6f,"Laplante A., École de Bibliothéconomie et des Sciences de l’Information, Université de Montréal, Canada","The success of a music recommender system depends on its ability to predict how much a particular user will like or dislike each item in its catalogue. However, such predictions are difficult to make accurately due to the complex nature of music tastes. In this paper, we review the literature on music tastes from social psychology and sociology of music to identify the correlates of music tastes and to understand how music tastes are formed and evolve through time. Research shows associations between music preferences and a wide variety of sociodem-ographic and individual characteristics, including personality traits, values, ethnicity, gender, social class, and political orientation. It also reveals the importance of social influences on music tastes, more specifically from family and peers, as well as the central role of music tastes in the construction of personal and social identities. Suggestions for the design of music recommender systems are made based on this literature review. © Audrey Laplante."
Panteli M.; Bogaards N.; Honingh A.,Modeling rhythm similarity for electronic dance music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030170247&partnerID=40&md5=b0ab11ec76fbdd3ef787b8f3fcc55ccf,"Panteli M., University of Amsterdam, Amsterdam, Netherlands; Bogaards N., Elephantcandy, Amsterdam, Netherlands; Honingh A., University of Amsterdam, Amsterdam, Netherlands","A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a ‘loop’, a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between segments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most notably: attack phase of onsets, periodicity of rhythmic elements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, after which the similarity between segments can be calculated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the overall performance of the model with perceptual ratings of rhythm similarity. © Maria Panteli, Niels Bogaards, Aline Honingh."
Singhi A.; Brown D.G.,"On cultural, textual and experiential aspects of music mood",2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041407959&partnerID=40&md5=1aaeff66d472aa9f750f71e4e0d0cc5f,"Singhi A., University of Waterloo, Cheriton School of Computer Science, Canada; Brown D.G., University of Waterloo, Cheriton School of Computer Science, Canada","We study the impact of the presence of lyrics on music mood perception for both Canadian and Chinese listeners by conducting a user study of Canadians not of Chinese origin, Chinese-Canadians, and Chinese people who have lived in Canada for fewer than three years. While our original hypotheses were largely connected to cultural components of mood perception, we also analyzed how stable mood assignments were when listeners could read the lyrics of recent popular English songs they were hearing versus when they only heard the songs. We also showed the lyrics of some songs to participants without playing the recorded music. We conclude that people assign different moods to the same song in these three scenarios. People tend to assign a song to the mood cluster that includes “melancholy” more often when they read the lyrics without listening to it, and having access to the lyrics does not help reduce the difference in music mood perception between Canadian and Chinese listeners significantly. Our results cause us to question the idea that songs have “inherent mood”. Rather, we suggest that the mood depends on both cultural and experiential context. © Abhishek Singhi, Daniel G. Brown."
Jun S.; Rho S.; Hwang E.,Geographical region mapping scheme based on musical preferences,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069933535&partnerID=40&md5=51ffbb27f76c10bd0aada4461d7714f4,"Jun S., Korea University, South Korea; Rho S., Sungkyul University, South Korea; Hwang E., Korea University, South Korea","Many countries and cities in the world tend to have different types of preferred or popular music, such as pop, K-pop, and reggae. Music-related applications utilize geographical proximity for evaluating the similarity of music preferences between two regions. Sometimes, this can lead to incorrect results due to other factors such as culture and religion. To solve this problem, in this paper, we propose a scheme for constructing a music map in which regions are positioned close to one another depending on the similarity of the musical preferences of their populations. That is, countries or cities in a traditional map are rearranged in the music map such that regions with similar musical preferences are close to one another. To do this, we collect users’ music play history and extract popular artists and tag information from the collected data. Similarities among regions are calculated using the tags and their frequencies. And then, an iterative algorithm for rearranging the regions into a music map is applied. We present a method for constructing the music map along with some experimental results. © Sanghoon Jun, Seungmin Rho, Eenjun Hwang."
Wang J.-C.; Yen M.-C.; Yang Y.-H.; Wang H.-M.,Automatic set list identification and song segmentation for full-length concert videos,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069917740&partnerID=40&md5=c2e3bdb30d23ec1a44109d096b602c2c,"Wang J.-C., Academia Sinica, Taipei, Taiwan, University of California, San Diego, CA, United States; Yen M.-C., Academia Sinica, Taipei, Taiwan; Yang Y.-H., Academia Sinica, Taipei, Taiwan; Wang H.-M., Academia Sinica, Taipei, Taiwan","Recently, plenty of full-length concert videos have become available on video-sharing websites such as YouTube. As each video generally contains multiple songs, natural questions that arise include “what is the set list?” and “when does each song begin and end?” Indeed, many full concert videos on YouTube contain song lists and timecodes contributed by uploaders and viewers. However, newly uploaded content and videos of lesser-known artists typically lack this metadata. Manually labeling such metadata would be labor-intensive, and thus an automated solution is desirable. In this paper, we define a novel research problem, automatic set list segmentation of full concert videos, which calls for techniques in music information retrieval (MIR) such as audio fingerprinting, cover song identification, musical event detection, music alignment, and structural segmentation. Moreover, we propose a greedy approach that sequentially identifies a song from a database of studio versions and simultaneously estimates its probable boundaries in the concert. We conduct preliminary evaluations on a collection of 20 full concerts and 1,152 studio tracks. Our result demonstrates the effectiveness of the proposed greedy algorithm. © Ju-Chiang Wang, Ming-Chi Yen, Yi-Hsuan Yang, and Hsin-Min Wang."
McFee B.; Humphrey E.J.; Bello J.P.,A software framework for musical data augmentation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996516893&partnerID=40&md5=3dc5ad87eb9e2ff67145aa40049237ac,"McFee B., Center for Data Science, New York University, United States, Music and Audio Research Laboratory, New York University, United States; Humphrey E.J., Music and Audio Research Laboratory, New York University, United States, MuseAmi, Inc., United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Predictive models for music annotation tasks are practically limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of “data augmentation” — supplementing a training set with carefully perturbed samples — has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated perturbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals. © Brian McFee, Eric J. Humphrey, Juan P. Bello."
Lee C.-L.; Lin Y.-T.; Yao Z.-R.; Lee F.-Y.; Wu J.-L.,Automatic mashup creation by considering both vertical and horizontal mashabilities,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966890182&partnerID=40&md5=e1060a261bb6719d627015b720d738bf,"Lee C.-L., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Lin Y.-T., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Yao Z.-R., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Lee F.-Y., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Wu J.-L., Communications and Multimedia Laboratory, National Taiwan University, Taiwan","In this paper, we proposed a system to effectively create music mashups – a kind of re-created music that is made by mixing parts of multiple existing music pieces. Unlike previous studies which merely generate mashups by overlaying music segments on one single base track, the proposed system creates mashups with multiple background (e.g. instrumental) and lead (e.g. vocal) track segments. So, besides the suitability between the vertically overlaid tracks (i.e. vertical mashability) used in previous studies, we proposed to further consider the suitability between the horizontally connected consecutive music segments (i.e. horizontal mashability) when searching for proper music segments to be combined. On the vertical side, two new factors: “harmonic change balance” and “volume weight” have been considered. On the horizontal side, the methods used in the studies of medley creation are incorporated. Combining vertical and horizontal mashabilities together, we defined four levels of mashability that may be encountered and found the proper solution to each of them. Subjective evaluations showed that the proposed four levels of mashability can appropriately reflect the degrees of listening enjoyment. Besides, by taking the newly proposed vertical mashability measurement into account, the improvement in user satisfaction is statistically significant. © Chuan-Lung Lee, Yin-Tzu Lin, Zun-Ren Yao, Feng-Yi Lee and Ja-Ling Wu."
Frisson C.; Dupont S.; Yvart W.; Riche N.; Siebert X.; Dutoit T.,A proximity grid optimization method to improve audio search for sound design,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069929331&partnerID=40&md5=18a7c3eeca3d6f917ca8b602751e1dd2,"Frisson C., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Dupont S., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Yvart W., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Riche N., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Siebert X., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Dutoit T., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium","Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques. © Christian Frisson, Stéphane Dupont, Willy Yvart, Nicolas Riche, Xavier Siebert, Thierry Dutoit."
Sturm B.L.; Bardeli R.; Langlois T.; Emiya V.,Formalizing the problem of music description,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069906272&partnerID=40&md5=acf0feacd6ca214cd10141a2407d73c3,"Sturm B.L., Aalborg University, Denmark; Bardeli R., Fraunhofer IAIS, Germany; Langlois T., Lisbon University, Portugal; Emiya V., Aix-Marseille Université, CNRS UMR 7279 LIF, France","The lack of a formalism for “the problem of music description” results in, among other things: ambiguity in what problem a music description system must address, how it should be evaluated, what criteria define its success, and the paradox that a music description system can reproduce the “ground truth” of a music dataset without attending to the music it contains. To address these issues, we formalize the problem of music description such that all elements of an instance of it are made explicit. This can thus inform the building of a system, and how it should be evaluated in a meaningful way. We provide illustrations of this formalism applied to three examples drawn from the literature. © Bob L. Sturm, Rolf Bardeli, Thibault Langlois, Valentin Emiya."
Hsu L.-C.; Wang Y.-L.; Lin Y.-J.; Su A.W.Y.; Metcalf C.D.,Detection of motor changes in violin playing by EMG signals,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069928086&partnerID=40&md5=4b5d878d16fab75a219cda34938809de,"Hsu L.-C., Department of CSIE, National Cheng-Kung University, Taiwan; Wang Y.-L., Department of CSIE, National Cheng-Kung University, Taiwan; Lin Y.-J., Department of CSIE, National Cheng-Kung University, Taiwan; Su A.W.Y., Department of CSIE, National Cheng-Kung University, Taiwan; Metcalf C.D., Faculty of Health Sciences, University of Southampton, United Kingdom","Playing a music instrument relies on the harmonious body movements. Motor sequences are trained to achieve the perfect performances in musicians. Thus, the information from audio signal is not enough to understand the sensorimotor programming in players. Recently, the investigation of muscular activities of players during performance has attracted our interests. In this work, we propose a multi-channel system that records the audio sounds and electromyography (EMG) signal simultaneously and also develop algorithms to analyze the music performance and discover its relation to player’s motor sequences. The movement segment was first identified by the information of audio sounds, and the direction of violin bowing was detected by the EMG signal. Six features were introduced to reveal the variations of muscular activities during violin playing. With the additional information of the audio signal, the proposed work could efficiently extract the period and detect the direction of motor changes in violin bowing. Therefore, the proposed work could provide a better understanding of how players activate the muscles to organize the multi-joint movement during violin performance. © L.C. Hsu, Y.J. Lin, Y.L. Wang, A.W.Y. Su, C.D. Metcalf."
Humphrey E.J.; Salamon J.; Nieto O.; Forsyth J.; Bittner R.M.; Bello J.P.,JAMS: A JSON annotated music specification for reproducible MIR research,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066080965&partnerID=40&md5=7373d22ea5c7ef9283fdecf00e1b3833,"Humphrey E.J., Music and Audio Research Lab, New York University, New York, United States; Salamon J., Music and Audio Research Lab, New York University, New York, United States, Center for Urban Science and Progress, New York University, New York, United States; Nieto O., Music and Audio Research Lab, New York University, New York, United States; Forsyth J., Music and Audio Research Lab, New York University, New York, United States; Bittner R.M., Music and Audio Research Lab, New York University, New York, United States; Bello J.P., Music and Audio Research Lab, New York University, New York, United States","The continued growth of MIR is motivating more complex annotation data, consisting of richer information, multiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of addressing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, comprehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we provide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and discuss how now is a crucial time to make a concerted effort toward sustainable annotation standards. © Eric J. Humphrey, Justin Salamon, Oriol Nieto, Jon Forsyth, Rachel M. Bittner, Juan P. Bello."
Hu X.; Lee J.H.; Choi K.; Downie J.S.,A cross-cultural study of mood in K-pop songs,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041843979&partnerID=40&md5=b7d7ff299a5b8f079ea2f85dcb8a10e9,"Hu X., University of Hong Kong, Hong Kong; Lee J.H., University of Washington, United States; Choi K., University of Illinois, United States; Downie J.S., University of Illinois, United States","Prior research suggests that music mood is one of the most important criteria when people look for music—but the perception of mood may be subjective and can be influenced by many factors including the listeners’ cultural background. In recent years, the number of studies of music mood perceptions by various cultural groups and of automated mood classification of music from different cultures has been increasing. However, there has yet to be a well-established testbed for evaluating cross-cultural tasks in Music Information Retrieval (MIR). Moreover, most existing datasets in MIR consist mainly of Western music and the cultural backgrounds of the annotators were mostly not taken into consideration or were limited to one cultural group. In this study, we built a collection of 1,892 K-pop (Korean Pop) songs with mood annotations collected from both Korean and American listeners, based on three different mood models. We analyze the differences and similarities between the mood judgments of the two listener groups, and propose potential MIR tasks that can be evaluated on this dataset. © Xiao Hu, Jin Ha Lee, Kahyun Choi, J. Stephen Downie."
Molina E.; Tardón L.J.; Barbancho I.; Barbancho A.M.,The importance of F0 tracking in query-by-singing-humming,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069901261&partnerID=40&md5=122920d022531fb6a042c2b3acc6ffbd,"Molina E., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Tardón L.J., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho I., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho A.M., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain","In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of query-by-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baseline method. For the evaluation, we measured the QBSH performance for 189 different combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In addition, we also found clear differences in robustness to F0 transcription errors between different matchers. © Emilio Molina, Lorenzo J. Tardón, Isabel Barbancho, Ana M. Barbancho."
Arzt A.; Widmer G.; Sonnleitner R.,Tempo- and transposition-invariant identification of piece and score position,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059771094&partnerID=40&md5=3ba1be6fc03f11e43e6b546d3d979650,"Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Sonnleitner R., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We present an algorithm that, given a very small snippet of an audio performance and a database of musical scores, quickly identifies the piece and the position in the score. The algorithm is both tempo- and transposition-invariant. We approach the problem by extending an existing tempo-invariant symbolic fingerprinting method, replacing the absolute pitch information in the fingerprints with a relative representation. Not surprisingly, this leads to a big decrease in the discriminative power of the fingerprints. To overcome this problem, we propose an additional verification step to filter out the introduced noise. Finally, we present a simple tracking algorithm that increases the retrieval precision for longer queries. Experiments show that both modifications improve the results, and make the new algorithm usable for a wide range of applications. © Andreas Arzt, Gerhard Widmer, Reinhard Sonnleitner."
McFee B.; Nieto O.; Bello J.P.,Hierarchical evaluation of segment boundary detection,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989225106&partnerID=40&md5=3a518ede1edf13c0ae18c25826495879,"McFee B., Center for Data Science, New York University, United States, Music and Audio Research Laboratory, New York University, United States; Nieto O., Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Structure in music is traditionally analyzed hierarchically: large-scale sections can be sub-divided and refined down to the short melodic ideas at the motivic level. However, typical algorithmic approaches to structural annotation produce flat temporal partitions of a track, which are commonly evaluated against a similarly flat, human-produced annotation. Evaluating structure analysis as represented by flat annotations effectively discards all notions of structural depth in the evaluation. Although collections of hierarchical structure annotations have been recently published, no techniques yet exist to measure an algorithm’s accuracy against these rich structural annotations. In this work, we propose a method to evaluate structural boundary detection with hierarchical annotations. The proposed method transforms boundary detection into a ranking problem, and facilitates the comparison of both flat and hierarchical annotations. We demonstrate the behavior of the proposed method with various synthetic and real examples drawn from the SALAMI dataset. © Brian McFee, Oriol Nieto, Juan P. Bello."
Wu C.-W.; Lerch A.,Drum transcription using partially fixed non-negative matrix factorization with template adaptation,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973278728&partnerID=40&md5=306c5ea7fbce6b8bcb7bca0d7c65a097,"Wu C.-W., Georgia Institute of Technology, Center for Music Technology, Georgia; Lerch A., Georgia Institute of Technology, Center for Music Technology, Georgia","In this paper, a template adaptive drum transcription algorithm using partially fixed Non-negative Matrix Factorization (NMF) is presented. The proposed method detects percussive events in complex mixtures of music with a minimal training set. The algorithm decomposes the music signal into two dictionaries: a percussive dictionary initialized with pre-defined drum templates and a harmonic dictionary initialized with undefined entries. The harmonic dictionary is adapted to the non-percussive music content in a standard NMF procedure. The percussive dictionary is adapted to each individual signal in an iterative scheme: it is fixed during the decomposition process, and is updated based on the result of the previous convergence. Two template adaptation methods are proposed to provide more flexibility and robustness in the case of unknown data. The performance of the proposed system has been evaluated and compared to state of the art systems. The results show that template adaptation improves the transcription performance, and the detection accuracy is in the same range as more complex systems. © Chih-Wei Wu, Alexander Lerch."
Antila C.; Cumming J.,The VIS framework: Analyzing counterpoint in large datasets,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069906709&partnerID=40&md5=9701a3a33b3db6107328b28a9d676c27,"Antila C., McGill University, Canada; Cumming J., McGill University, Canada","The VIS Framework for Music Analysis is a modular Python library designed for “big data” queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strategy in VIS, and the flexibility of this approach for future researchers. © Christopher Antila and Julie Cumming."
Gómez-Marín D.; Jordà S.; Herrera P.,Pad and sad: Two awareness-weighted rhythmic similarity distances,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003552787&partnerID=40&md5=c3d307c78ebb829a05033eb8a12e025f,"Gómez-Marín D., Universitat Pompeu Fabra, Spain; Jordà S., Universitat Pompeu Fabra, Spain; Herrera P., Universitat Pompeu Fabra, Spain","Measuring rhythm similarity is relevant for the analysis and generation of music. Existing similarity metrics tend to consider our perception of rhythms as being in time without discriminating the importance of some regions over others. In a previously reported experiment we observed that measures of similarity may differ given the presence or absence of a pulse inducing sound and the importance of those measures is not constant along the pattern. These results are now reinterpreted by refining the previously proposed metrics. We consider that the perceptual contribution of each beat to the measured similarity is non-homogeneous but might indeed depend on the temporal positions of the beat along the bar. We show that with these improvements, the correlation between the previously evaluated experimental similarity and predictions based on our metrics increases substantially. We conclude by discussing a possible new methodology for evaluating rhythmic similarity between audio loops. © Daniel Gómez-Marín, Sergi Jordà, Perfecto Herrera."
Arjannikov T.; Zhang J.Z.,An association-based approach to genre classification in music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061492495&partnerID=40&md5=c1919202ad8134a417906354ffbfaa80,"Arjannikov T., University of Lethbridge, Canada; Zhang J.Z., University of Lethbridge, Canada","Music Information Retrieval (MIR) is a multi-disciplin-ary research area that aims to automate the access to large-volume music data, including browsing, retrieval, storage, etc. The work that we present in this paper tackles a nontrivial problem in the field, namely music genre classification, which is one of the core tasks in MIR. In our proposed approach, we make use of association analysis to study and predict music genres based on the acoustic features extracted directly from music. In essence, we build an associative classifier, which finds inherent associations between content-based features and individual genres and then uses them to predict the genre(s) of a new music piece. We demonstrate the feasibility of our approach through a series of experiments using two publicly available music datasets. One of them is the largest available in MIR and contains real world data, while the other has been widely used and provides a good benchmarking basis. We show the effectiveness of our approach and discuss various related issues. In addition, due to its associative nature, our classifier can assign multiple genres to a single music piece; hopefully this would offer insights into the prevalent multi-label situation in genre classification. © Tom Arjannikov, John Z. Zhang."
Nieto O.; Farbood M.M.; Jehan T.; Bello J.P.,Perceptual analysis of the f-measure for evaluating section boundaries in music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066072277&partnerID=40&md5=a0c6a17271bb5b376cfbc4249f44664f,"Nieto O., Music and Audio Research Lab, New York University, United States; Farbood M.M., Music and Audio Research Lab, New York University, United States; Jehan T., Echo Nest, United States; Bello J.P., Music and Audio Research Lab, New York University, United States","In this paper, we aim to raise awareness of the limitations of the F-measure when evaluating the quality of the boundaries found in the automatic segmentation of music. We present and discuss the results of various experiments where subjects listened to different musical excerpts containing boundary indications and had to rate the quality of the boundaries. These boundaries were carefully generated from state-of-the-art segmentation algorithms as well as human-annotated data. The results show that humans tend to give more relevance to the precision component of the F-measure rather than the recall component, therefore making the classical F-measure not as perceptually informative as currently assumed. Based on the results of the experiments, we discuss the potential of an alternative evaluation based on the F-measure that emphasizes precision over recall, making the section boundary evaluation more expressive and reliable. © Oriol Nieto, Morwaread M. Farbood, Tristan Jehan, Juan Pablo Bello."
Freedman D.; Kohler E.; Tutschku H.,Correlating extracted and ground-truth harmonic data in music retrieval tasks,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988000027&partnerID=40&md5=1a9d924be03e1c99e85756b929373f97,"Freedman D., Harvard University, United States; Kohler E., Harvard University, United States; Tutschku H., Harvard University, United States","We show that traditional music information retrieval tasks with well-chosen parameters perform similarly using computationally extracted chord annotations and ground-truth annotations. Using a collection of Billboard songs with provided ground-truth chord labels, we use established chord identification algorithms to produce a corresponding extracted chord label dataset. We implement methods to compare chord progressions between two songs on the basis of their optimal local alignment scores. We create a set of chord progression comparison parameters defined by chord distance metrics, gap costs, and normalization measures and run a black-box global optimization algorithm to stochastically search for the best parameter set to maximize the rank correlation for two harmonic retrieval tasks across the ground-truth and extracted chord Billboard datasets. The first task evaluates chord progression similarity between all pairwise combinations of songs, separately ranks results for ground-truth and extracted chord labels, and returns a rank correlation coefficient. The second task queries the set of songs with fabricated chord progressions, ranks each query’s results across ground-truth and extracted chord labels, and returns rank correlations. The end results suggest that practical retrieval systems can be constructed to work effectively without the guide of human ground-truthing. © Dylan Freedman, Eddie Kohler, Hans Tutschku."
Flexer A.,Improving visualization of high-dimensional music similarity spaces,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987969516&partnerID=40&md5=38b6a4fa885f6defc3e948f60fb5cf13,"Flexer A., Austrian Research Institute for Artificial Intelligence, Austria",Visualizations of music databases are a popular form of interface allowing intuitive exploration of music catalogs. They are often based on lower dimensional projections of high dimensional music similarity spaces. Such similarity spaces have already been shown to be negatively impacted by so-called hubs and anti-hubs. These are points that appear very close or very far to many other data points due to a problem of measuring distances in high-dimensional spaces. We present an empirical study on how this phenomenon impacts three popular approaches to compute two-dimensional visualizations of music databases. We also show how the negative impact of hubs and anti-hubs can be reduced by re-scaling the high dimensional spaces before low dimensional projection. © Arthur Flexer.
Urbano J.; Bogdanov D.; Herrera P.; Gómez E.; Serra X.,What is the effect of audio quality on the robustness of MFCCs and chroma features?,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028044145&partnerID=40&md5=8b376f6c7390888f4386ce2977cf45b3,"Urbano J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical applications they are to be computed on music corpora containing audio files encoded in a variety of lossy formats. Such encodings distort the original signal and therefore may affect the computation of descriptors. This raises the question of the robustness of these descriptors across various audio encodings. We examine this assumption for the case of MFCCs and chroma features. In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre. Using two different audio analysis tools over a diverse collection of music tracks, we compute several statistics to quantify the robustness of the resulting descriptors, and then estimate the practical effects for a sample task like genre classification. © J.Urbano, D.Bogdanov, P.Herrera, E.Gómez and X.Serra."
Benetos E.; Weyde T.,An efficient temporally-constrained probabilistic model for multiple-instrument music transcription,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973291866&partnerID=40&md5=443d2e516e4b0a3526839b91e6a4b473,"Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Weyde T., Department of Computer Science, City University, London, United Kingdom","In this paper, an efficient, general-purpose model for multiple instrument polyphonic music transcription is proposed. The model is based on probabilistic latent component analysis and supports the use of sound state spectral templates, which represent the temporal evolution of each note (e.g. attack, sustain, decay). As input, a variable-Q transform (VQT) time-frequency representation is used. Computational efficiency is achieved by supporting the use of pre-extracted and pre-shifted sound state templates. Two variants are presented: without temporal constraints and with hidden Markov model-based constraints controlling the appearance of sound states. Experiments are performed on benchmark transcription datasets: MAPS, TRIOS, MIREX multiF0, and Bach10; results on multi-pitch detection and instrument assignment show that the proposed models outperform the state-of-the-art for multiple-instrument transcription and is more than 20 times faster compared to a previous sound state-based model. We finally show that a VQT representation can lead to improved multi-pitch detection performance compared with constant-Q representations. © Emmanouil Benetos, Tillman Weyde."
Prätzlich T.; Müller M.,Frame-level audio segmentation for abridged musical works,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017651109&partnerID=40&md5=71b6e676dff79c15ae9dc5c0792e0c64,"Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","Large-scale musical works such as operas may last several hours and typically involve a huge number of musicians. For such compositions, one often finds different arrangements and abridged versions (often lasting less than an hour), which can also be performed by smaller ensembles. Abridged versions still convey the flavor of the musical work containing the most important excerpts and melodies. In this paper, we consider the task of automatically segmenting an audio recording of a given version into semantically meaningful parts. Following previous work, the general strategy is to transfer a reference segmentation of the original complete work to the given version. Our main contribution is to show how this can be accomplished when dealing with strongly abridged versions. To this end, opposed to previously suggested segment-level matching procedures, we adapt a frame-level matching approach for transferring the reference segment information to the unknown version. Considering the opera “Der Freischütz” as an example scenario, we discuss how to balance out flexibility and robustness properties of our proposed frame-level segmentation procedure. © Thomas Prätzlich, Meinard Müller."
Grohganz H.; Clausen M.; Müller M.,Estimating musical time information from performed MIDI files,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059785340&partnerID=40&md5=3f45fd45cdf7f859dbb3ca23a8a7b5a1,"Grohganz H., Bonn University, Germany; Clausen M., Bonn University, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","Even though originally developed for exchanging control commands between electronic instruments, MIDI has been used as quasi standard for encoding and storing score-related parameters. MIDI allows for representing musical time information as specified by sheet music as well as physical time information that reflects performance aspects. However, in many of the available MIDI files the musical beat and tempo information is set to a preset value with no relation to the actual music content. In this paper, we introduce a procedure to determine the musical beat grid from a given performed MIDI file. As one main contribution, we show how the global estimate of the time signature can be used to correct local errors in the pulse grid estimation. Different to MIDI quantization, where one tries to map MIDI note onsets onto a given musical pulse grid, our goal is to actually estimate such a grid. In this sense, our procedure can be used in combination with existing MIDI quantization procedures to convert performed MIDI files into semantically enriched score-like MIDI files. © Harald Grohganz, Michael Clausen, Meinard Müller."
Cancino Chacón C.E.; Lattner S.; Grachten M.,Developing tonal perception through unsupervised learning,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058074677&partnerID=40&md5=bc189da49fa792e2350934ea65b9145c,"Cancino Chacón C.E., Austrian Research Institute for Artificial Intelligence, Austria; Lattner S., Austrian Research Institute for Artificial Intelligence, Austria; Grachten M., Austrian Research Institute for Artificial Intelligence, Austria","The perception of tonal structure in music seems to be rooted both in low-level perceptual mechanisms and in enculturation, the latter accounting for cross-cultural differences in perceived tonal structure. Unsupervised machine learning methods are a powerful tool for studying how musical concepts may emerge from exposure to music. In this paper, we investigate to what degree tonal structure can be learned from musical data by unsupervised training of a Restricted Boltzmann Machine, a generative stochastic neural network. We show that even based on a limited set of musical data, the model learns several aspects of tonal structure. Firstly, the model learns an organization of musical material from different keys that conveys the topology of the circle of fifths (CoF). Although such a topology can be learned using principal component analysis (PCA) when using pitch-only representations, we found that using a pitch-duration representation impedes the extraction of the CoF topology much more for PCA than for the RBM. Furthermore, we replicate probe-tone experiments by Krumhansl and Shepard, measuring the organization of tones within a key in human perception. We find that the responses of the RBM share qualitative characteristics with those of both trained and untrained listeners. © Carlos Eduardo Cancino Chacón, Stefan Lattner, Maarten Grachten."
Carabias-Orti J.J.; Rodriguez-Serrano F.J.; Vera-Candeas P.; Ruiz-Reyes N.; Cañadas-Quesada F.J.,An audio to score alignment framework using spectral factorization and dynamic time warping,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009484871&partnerID=40&md5=2d03876fd0b2ce167f87dad0a0f8b9df,"Carabias-Orti J.J., Music Technology Group (MTG), Universitat Pompeu Fabra, Spain; Rodriguez-Serrano F.J., Polytechnical School of Linares, Universidad de Jaen, Spain; Vera-Candeas P., Polytechnical School of Linares, Universidad de Jaen, Spain; Ruiz-Reyes N., Polytechnical School of Linares, Universidad de Jaen, Spain; Cañadas-Quesada F.J., Polytechnical School of Linares, Universidad de Jaen, Spain","In this paper, we present an audio to score alignment framework based on spectral factorization and online Dynamic Time Warping (DTW). The proposed framework has two separated stages: preprocessing and alignment. In the first stage, we use Non-negative Matrix Factorization (NMF) to learn spectral patterns (i.e. basis functions) associated to each combination of concurrent notes in the score. In the second stage, a low latency signal decomposition method with fixed spectral patterns per combination of notes is used over the magnitude spectrogram of the input signal resulting in a divergence matrix that can be interpreted as the cost of the matching for each combination of notes at each frame. Finally, a Dynamic Time Warping (DTW) approach has been used to find the path with the minimum cost and then determine the relation between the performance and the musical score times. Our framework have been evaluated using a dataset of baroque-era pieces and compared to other systems, yielding solid results and performance. © J.J. Carabias-Orti, F.J. Rodriguez-Serrano, P. Vera-Candeas, N. Ruiz-Reyes, F.J. Cañadas-Quesada."
Srinivasamurthy A.; Holzapfel A.; Cemgil A.T.; Serra X.,Particle filters for efficient meter tracking with dynamic Bayesian networks,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973317973&partnerID=40&md5=b2bbb4319247f3dcf342730d36b375bb,"Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Dept. of Computer Engineering, Boğaziçi University, Istanbul, Turkey; Cemgil A.T., Dept. of Computer Engineering, Boğaziçi University, Istanbul, Turkey; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Recent approaches in meter tracking have successfully applied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the application of exact inference is computationally demanding. More efficient approximate inference algorithms using particle filters (PF) can be developed to overcome this limitation. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an existing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Filter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter tracking accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We document that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music. © Ajay Srinivasamurthy, Andre Holzapfel, Ali Taylan Cemgil, Xavier Serra."
Sigtia S.; Boulanger-Lewandowski N.; Dixon S.,Audio chord recognition with a hybrid recurrent neural network,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971282541&partnerID=40&md5=3c29eee9fdbe01d45989f759384cd7dc,"Sigtia S., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Boulanger-Lewandowski N., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada; Dixon S., Centre for Digital Music, Queen Mary University of London, London, United Kingdom","In this paper, we present a novel architecture for audio chord estimation using a hybrid recurrent neural network. The architecture replaces hidden Markov models (HMMs) with recurrent neural network (RNN) based language models for modelling temporal dependencies between chords. We demonstrate the ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic signal, eliminating the need for a complex feature extraction stage. For the hybrid RNN architecture, inference over the output variables of interest is performed using beam search. In addition to the hybrid model, we propose a modification to beam search using a hash table which yields improved results while reducing memory requirements by an order of magnitude, thus making the proposed model suitable for real-time applications. We evaluate our model's performance on a dataset with publicly available annotations and demonstrate that the performance is comparable to existing state of the art approaches for chord recognition. © Siddharth Sigtia, Nicolas Boulanger-Lewandowski, Simon Dixon."
Giraud M.; Levé F.; Mercier F.; Rigaudière M.; Thorez D.,Towards modeling texture in symbolic data,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069909689&partnerID=40&md5=fe7ebf547c43889de89f5f47a8230b73,"Giraud M., LIFL, CNRS, Univ. Lille 1, Lille 3, France; Levé F., MIS, UPJV, Amiens LIFL, Univ. Lille 1, France; Mercier F., Univ. Lille 1, France; Rigaudière M., Univ. Lorraine, France; Thorez D., Univ. Lille 1, France","Studying texture is a part of many musicological analy-ses. The change of texture plays an important role in the cognition of musical structures. Texture is a feature commonly used to analyze musical audio data, but it is rarely taken into account in symbolic studies. We propose to formalize the texture in classical Western instrumental music as melody and accompaniment layers, and provide an algorithm able to detect homorhythmic layers in polyphonic data where voices are not separated. We present an evaluation of these methods for parallel motions against a ground truth analysis of ten instrumental pieces, including the first movements of the six quatuors op. 33 by Haydn. © Mathieu Giraud, Florence Levé, Florent Mercier, Marc Rigaudière, Donatien Thorez."
Li P.-C.; Su L.; Yang Y.-H.; Su A.W.Y.,Analysis of expressive musical terms in violin using score-informed and expression-based audio features,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989244870&partnerID=40&md5=0fb9ce09a01d2453e9e9aa90343afc70,"Li P.-C., SCREAM Lab, Department of CSIE, National Cheng-Kung University, Taiwan; Su L., MAC Lab., CITI, Academia Sinica, Taiwan; Yang Y.-H., MAC Lab., CITI, Academia Sinica, Taiwan; Su A.W.Y., SCREAM Lab, Department of CSIE, National Cheng-Kung University, Taiwan","The manipulation of different interpretational factors, including dynamics, duration, and vibrato, constitutes the realization of different expressions in music. Therefore, a deeper understanding of the workings of these factors is critical for advanced expressive synthesis and computer-aided music education. In this paper, we propose the novel task of automatic expressive musical term classification as a direct means to study the interpretational factors. Specifically, we consider up to 10 expressive musical terms, such as Scherzando and Tranquillo, and compile a new dataset of solo violin excerpts featuring the realization of different expressive terms by different musicians for the same set of classical music pieces. Under a score-informed scheme, we design and evaluate a number of note-level features characterizing the interpretational aspects of music for the classification task. Our evaluation shows that the proposed features lead to significantly higher classification accuracy than a baseline feature set commonly used in music information retrieval tasks. Moreover, taking the contrast of feature values between an expressive and its corresponding non-expressive version (if given) of a music piece greatly improves the accuracy in classifying the presented expressive one. We also draw insights from analyzing the feature relevance and the class-wise accuracy of the prediction. © Pei-Ching Li, Li Su, Yi-Hsuan Yang, Alvin W. Y. Su. Li-."
Kruspe A.M.,Keyword spotting in a-capella singing,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048382424&partnerID=40&md5=8057ef3027929d15c1ed2febdbbfd961,"Kruspe A.M., Fraunhofer IDMT, Ilmenau, Germany, Johns Hopkins University, Baltimore, MD, United States","Keyword spotting (or spoken term detection) is an interesting task in Music Information Retrieval that can be applied to a number of problems. Its purposes include topical search and improvements for genre classification. Keyword spotting is a well-researched task on pure speech, but state-of-the-art approaches cannot be easily transferred to singing because phoneme durations have much higher variations in singing. To our knowledge, no keyword spotting system for singing has been presented yet. We present a keyword spotting approach based on keyword-filler Hidden Markov Models (HMMs) and test it on a-capella singing and spoken lyrics. We test Mel-Frequency Cepstral Coefficents (MFCCs), Perceptual Linear Predictive Features (PLPs), and Temporal Patterns (TRAPs) as front ends. These features are then used to generate phoneme posteriors using Multilayer Perceptrons (MLPs) trained on speech data. The phoneme posteriors are then used as the system input. Our approach produces useful results on a-capella singing, but depend heavily on the chosen keyword. We show that results can be further improved by training the MLP on a-capella data. We also test two post-processing methods on our phoneme posteriors before the keyword spotting step. First, we average the posteriors of all three feature sets. Second, we run the three concatenated posteriors through a fusion classifier. © Anna M. Kruspe."
Chen C.-T.; Jang J.-S.R.; Lu C.-H.,Improved query-by-tapping via tempo alignment,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069933648&partnerID=40&md5=e20460c5d29abdb76982f481b98791a7,"Chen C.-T., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Department of Computer Science, National Taiwan University, Taipei, Taiwan; Lu C.-H., Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Institute for Information Industry, Taiwan","Query by tapping (QBT) is a content-based music retrieval method that can retrieve a song by taking the user’s tapping or clapping at the note onsets of the intended song in the database for comparison. This paper proposes a new query-by-tapping algorithm that aligns the IOI (inter-onset interval) vector of the query sequence with songs in the dataset by building an IOI ratio matrix, and then applies a dynamic programming (DP) method to compute the optimum path with minimum cost. Experiments on different datasets indicate that our algorithm outperforms other previous approaches in accuracy (top-10 and MRR), with a speedup factor of 3 in computation. With the advent of personal handheld devices, QBT provides an interesting and innovative way for music retrieval by shaking or tapping the devices, which is also discussed in the paper. © Chun-Ta Chen, Jyh-Shing Roger Jang, Chun-Hung Lu."
Cunningham S.J.; Nichols D.M.; Bainbridge D.; Ali H.,Social music in cars,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012024481&partnerID=40&md5=aabd53af66c77327c57bbef84bd4ccdc,"Cunningham S.J., Department of Computer Science, University of Waikato, New Zealand; Nichols D.M., Department of Computer Science, University of Waikato, New Zealand; Bainbridge D., Department of Computer Science, University of Waikato, New Zealand; Ali H., Department of Computer Science, University of Waikato, New Zealand","This paper builds an understanding of how music is currently experienced by a social group travelling together in a car—how songs are chosen for playing, how music both reflects and influences the group’s mood and social interaction, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of ethnographic data (participant observations and interviews) focusing primarily on the experience of in-car music on moderate length and long trips. We suggest features and functionality for music software to enhance the social experience when travelling in cars, and prototype and test a user interface based on design suggestions drawn from the data. © S.J. Cunningham, D.M. Nichols, D. Bainbridge, H. Ali."
Melenhorst M.S.; Liem C.C.S.,Put the concert attendee in the spotlight. A user-centered design and development approach for classical concert applications,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978652176&partnerID=40&md5=6d7b6e97c2acf02270b5cae67e576e0e,"Melenhorst M.S., Delft University of Technology, Multimedia Computing Group, Netherlands; Liem C.C.S., Delft University of Technology, Multimedia Computing Group, Netherlands","As the importance of real-life use cases in the music information retrieval (MIR) field is increasing, so does the importance of understanding user needs. The development of innovative real-life applications that draw on MIR technology requires a user-centered design and development approach that assesses user needs and aligns them with technological and academic ambitions in the MIR domain. In this paper we present such an approach, and apply it to the development of technological applications to enrich classical symphonic concerts. A user-driven approach is particularly important in this area, as orchestras need to innovate the concert experience to meet the needs and expectations of younger generations without alienating the current audience. We illustrate this approach with the results of five focus groups for three audience segments, which allow us to formulate informed user requirements for classical concert applications. © Mark S. Melenhorst, Cynthia C. S. Liem."
Caetano M.; Wiering F.,Theoretical framework of a computational model of auditory memory for music emotion recognition,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058018796&partnerID=40&md5=1af66fea94dcdc0bc1fc331af205bdf9,"Caetano M., Sound and Music Computing Group INESC TEC, Porto, Portugal; Wiering F., Dep. Information and Computing Sciences, Utrecht University, Netherlands","The bag of frames (BOF) approach commonly used in music emotion recognition (MER) has several limitations. The semantic gap is believed to be responsible for the glass ceiling on the performance of BOF MER systems. However, there are hardly any alternative proposals to address it. In this article, we introduce the theoretical framework of a computational model of auditory memory that incorporates temporal information into MER systems. We advocate that the organization of auditory memory places time at the core of the link between musical meaning and musical emotions. The main goal is to motivate MER researchers to develop an improved class of systems capable of overcoming the limitations of the BOF approach and coping with the inherent complexity of musical emotions. © Marcelo Caetano, Frans Wiering."
Raffel C.; McFee B.; Humphrey E.J.; Salamon J.; Nieto O.; Liang D.; Ellis D.P.W.,mir_eval: A transparent implementation of common MIR metrics,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066108850&partnerID=40&md5=b973b7b12c48b2e2c72a5a1c10737e8b,"Raffel C., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; McFee B., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States, Center for Jazz Studies, Columbia University, New York, United States; Humphrey E.J., Music and Audio Research Lab, New York University, New York, United States; Salamon J., Music and Audio Research Lab, New York University, New York, United States, Center for Urban Science and Progress, New York University, New York, United States; Nieto O., Music and Audio Research Lab, New York University, New York, United States; Liang D., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; Ellis D.P.W., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States","Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir_eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir_eval and quantitatively compare each to existing implementations. When the scores reported by mir_eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir_eval’s architecture, design, and intended use. © Colin Raffel, Brian McFee, Eric J. Humphrey, Justin Salamon, Oriol Nieto, Dawen Liang, Daniel P. W. Ellis."
Abeßer J.; Cano E.; Frieler K.; Pfleiderer M.; Zaddach W.-G.,Score-informed analysis of intonation and pitch modulation in jazz solos,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007458423&partnerID=40&md5=7ca526a1da69865f17d31fbf91ff8e8f,"Abeßer J., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Cano E., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Frieler K., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany; Pfleiderer M., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany; Zaddach W.-G., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany","The paper presents new approaches for analyzing the characteristics of intonation and pitch modulation of woodwind and brass solos in jazz recordings. To this end, we use score-informed analysis techniques for source separation and fundamental frequency tracking. After splitting the audio into a solo and a backing track, a reference tuning frequency is estimated from the backing track. Next, we compute the fundamental frequency contour for each tone in the solo and a set of features describing its temporal shape. Based on this data, we first investigate, whether the tuning frequencies of jazz recordings changed over the decades of the last century. Second, we analyze whether the intonation is artist-specific. Finally, we examine how the modulation frequency of vibrato tones depends on contextual parameters such as pitch, duration, and tempo as well as the performing artist. © Jakob Abeßer, Estefanía Cano, Klaus Frieler, Martin Pfleiderer, Wolf-Georg Zaddach."
Bittner R.; Salamon J.; Tierney M.; Mauch M.; Cannam C.; Bello J.,MedleyDB: A multitrack dataset for annotation-intensive MIR research,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057325970&partnerID=40&md5=ff8c9aa9802f15136c013065e48c5c5f,"Bittner R., Music and Audio Research Lab, New York University, United States; Salamon J., Music and Audio Research Lab, New York University, United States, Center for Urban Science and Progress, New York University, United States; Tierney M., Music and Audio Research Lab, New York University, United States; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Cannam C., Centre for Digital Music, Queen Mary University of London, United Kingdom; Bello J., Music and Audio Research Lab, New York University, United States","We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research. © Rachel Bittner, Justin Salamon, Mike Tierney, Matthias Mauch, Chris Cannam, Juan Bello."
Moore J.L.; Joachims T.; Turnbull D.,Taste space versus the world: An embedding analysis of listening habits and geography,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016290977&partnerID=40&md5=199525de708141f58f17ab9f5e2ee4c8,"Moore J.L., Cornell University, Dept. of Computer Science, United States; Joachims T., Cornell University, Dept. of Computer Science, United States; Turnbull D., Ithaca College, Dept. of Computer Science, United States","Probabilistic embedding methods provide a principled way of deriving new spatial representations of discrete objects from human interaction data. The resulting assignment of objects to positions in a continuous, low-dimensional space not only provides a compact and accurate predictive model, but also a compact and flexible representation for understanding the data. In this paper, we demonstrate how probabilistic embedding methods reveal the “taste space” in the recently released Million Musical Tweets Dataset (MMTD), and how it transcends geographic space. In particular, by embedding cities around the world along with preferred artists, we are able to distill information about cultural and geographical differences in listening patterns into spatial representations. These representations yield a similarity metric among city pairs, artist pairs, and city-artist pairs, which can then be used to draw conclusions about the similarities and contrasts between taste space and geographic location. © Joshua L. Moore, Thorsten Joachims, Douglas Turnbull."
Oramas S.; Gómez F.; Gómez E.; Mora J.,FlaBase: Towards the creation of a flamenco music knowledge base,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995603955&partnerID=40&md5=4c2924a968d3a2ae0d826f4c2314d22f,"Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Gómez F., Technical University of Madrid, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Mora J., Faculty of Psychology, University of Sevilla, Spain","Online information about flamenco music is scattered over different sites and knowledge bases. Unfortunately, there is no common repository that indexes all these data. In this work, information related to flamenco music is gathered from general knowledge bases (e.g., Wikipedia, DBpedia), music encyclopedias (e.g., MusicBrainz), and specialized flamenco websites, and is then integrated into a new knowledge base called FlaBase. As resources from different data sources do not share common identifiers, a process of pair-wise entity resolution has been performed. FlaBase contains information about 1,174 artists, 76 palos (flamenco genres), 2,913 albums, 14,078 tracks, and 771 Andalusian locations. It is freely available in RDF and JSON formats. In addition, a method for entity recognition and disambiguation for FlaBase has been created. The system can recognize and disambiguate FlaBase entity references in Spanish texts with an f-measure value of 0.77. We applied it to biographical texts present in Flabase. By using the extracted information, the knowledge base is populated with relevant information and a semantic graph is created connecting the entities of FlaBase. Artists relevance is then computed over the graph and evaluated according to a flamenco expert criteria. Accuracy of results shows a high degree of quality and completeness of the knowledge base. © Sergio Oramas1, Francisco Gómez2, Emilia Gómez1, Joaquín Mora3."
Porter A.; Bogdanov D.; Kaye R.; Tsukanov R.; Serra X.,AcousticBrainz: A community platform for gathering music information obtained from audio,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994009403&partnerID=40&md5=0f61acabc67e81bf4551601e5312e0f2,"Porter A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, MetaBrainz Foundation, United States; Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Kaye R., MetaBrainz Foundation, United States; Tsukanov R., MetaBrainz Foundation, United States; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We introduce the AcousticBrainz project, an open platform for gathering music information. At its core, AcousticBrainz is a database of music descriptors computed from audio recordings using a number of state-of-the-art Music Information Retrieval algorithms. Users run a supplied feature extractor on audio files and upload the analysis results to the AcousticBrainz server. All submissions include a MusicBrainz identifier allowing them to be linked to various sources of editorial information. The feature extractor is based on the open source Essentia audio analysis library. From the data submitted by the community, we run classifiers aimed at adding musically relevant semantic information. These classifiers can be developed by the community using tools available on the AcousticBrainz website. All data in AcousticBrainz is freely available and can be accessed through the website or API. For AcousticBrainz to be successful we need to have an active community that contributes to and uses this platform, and it is this community that will define the actual uses and applications of its data. © Alastair Porter, Dmitry Bogdanov, Robert Kaye, Roman Tsukanov, Xavier Serra."
Prockup M.; Ehmann A.F.; Gouyon F.; Schmidt E.M.; Celma O.; Kim Y.E.,Modeling genre with the music genome project: Comparing human-labeled attributes and audio features,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977927583&partnerID=40&md5=ce355609fcb2a44d34f48f37c8595115,"Prockup M., Drexel University, United States, Pandora Media Inc., United States; Ehmann A.F., Pandora Media Inc., United States; Gouyon F., Pandora Media Inc., United States; Schmidt E.M., Pandora Media Inc., United States; Celma O., Pandora Media Inc., United States; Kim Y.E., Drexel University, United States","Genre provides one of the most convenient categorizations of music, but it is often regarded as a poorly defined or largely subjective musical construct. In this work, we provide evidence that musical genres can to a large extent be objectively modeled via a combination of musical attributes. We employ a data-driven approach utilizing a subset of 48 hand-labeled musical attributes comprising instrumentation, timbre, and rhythm across more than one million examples from Pandora® Internet Radio’s Music Genome Project®. A set of audio features motivated by timbre and rhythm are then implemented to model genre both directly and through audio-driven models derived from the hand-labeled musical attributes. In most cases, machine learning models built directly from hand-labeled attributes outperform models based on audio features. Among the audio-based models, those that combine audio features and learned musical attributes perform better than those derived from audio features alone. © Matthew Prockup, Andreas F. Ehmann, Fabien Gouyon Erik M. Schmidt, Oscar Celma, and Youngmoo E. Kim."
van den Oord A.; Dieleman S.; Schrauwen B.,Transfer learning by supervised pre-training for audio-based music classification,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055025883&partnerID=40&md5=1cddf939824e402ab2094ea05ed49b5c,"van den Oord A., Electronics and Information Systems department, Ghent University, Belgium; Dieleman S., Electronics and Information Systems department, Ghent University, Belgium; Schrauwen B., Electronics and Information Systems department, Ghent University, Belgium","Very few large-scale music research datasets are publicly available. There is an increasing need for such datasets, because the shift from physical to digital distribution in the music industry has given the listener access to a large body of music, which needs to be cataloged efficiently and be easily browsable. Additionally, deep learning and feature learning techniques are becoming increasingly popular for music information retrieval applications, and they typically require large amounts of training data to work well. In this paper, we propose to exploit an available large-scale music dataset, the Million Song Dataset (MSD), for classification tasks on other datasets, by reusing models trained on the MSD for feature extraction. This transfer learning approach, which we refer to as supervised pre-training, was previously shown to be very effective for computer vision problems. We show that features learned from MSD audio fragments in a supervised manner, using tag labels and user listening data, consistently outperform features learned in an unsupervised manner in this setting, provided that the learned feature extractor is of limited complexity. We evaluate our approach on the GTZAN, 1517-Artists, Unique and Magnatagatune datasets. © Aäron van den Oord, Sander Dieleman, Benjamin Schrauwen."
Repetto R.C.; Serra X.,Creating a corpus of Jingju (Beijing opera) music and possibilities for melodic analysis,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054970902&partnerID=40&md5=8dc605e6fcd9f6fc3a299e530aecea02,"Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Jingju (Beijing opera) is a Chinese traditional performing art form in which theatrical and musical elements are intimately combined. As an oral tradition, its musical dimension is the result of the application of a series of predefined conventions and it offers unique concepts for musicological research. Computational analyses of jingju music are still scarce, and only a few studies have dealt with it from an MIR perspective. In this paper we present the creation of a corpus of jingju music in the framework of the CompMusic project that is formed by audio, editorial metadata, lyrics and scores. We discuss the criteria followed for the acquisition of the data, describe the content of the corpus, and evaluate its suitability for computational and musicological research. We also identify several research problems that can take advantage of this corpus in the context of computational musicology, especially for melodic analysis, and suggest approaches for future work. © Rafael Caro Repetto, Xavier Serra."
Molina E.; Barbancho A.M.; Tardón L.J.; Barbancho I.,Evaluation framework for automatic singing transcription,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038600955&partnerID=40&md5=1f50c7f63d1f5383d86e2565349328a9,"Molina E., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho A.M., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Tardón L.J., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho I., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain","In this paper, we analyse the evaluation strategies used in previous works on automatic singing transcription, and we present a novel, comprehensive and freely available evaluation framework for automatic singing transcription. This framework consists of a cross-annotated dataset and a set of extended evaluation measures, which are integrated in a Matlab toolbox. The presented evaluation measures are based on standard MIREX note-tracking measures, but they provide extra information about the type of errors made by the singing transcriber. Finally, a practical case of use is presented, in which the evaluation framework has been used to perform a comparison in detail of several state-of-the-art singing transcribers. © Emilio Molina, Ana M. Barbancho, Lorenzo J. Tardón, Isabel Barbancho."
Silva D.F.; Rossi R.G.; Rezende S.O.; Batista G.E.A.P.A.,Music classification by transductive learning using bipartite heterogeneous networks,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055483672&partnerID=40&md5=f58b5becf1836e07d357c9d6d2fb9dd5,"Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Rossi R.G., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Rezende S.O., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil","The popularization of music distribution in electronic format has increased the amount of music with incomplete metadata. The incompleteness of data can hamper some important tasks, such as music and artist recommendation. In this scenario, transductive classification can be used to classify the whole dataset considering just few labeled instances. Usually transductive classification is performed through label propagation, in which data are represented as networks and the examples propagate their labels through their connections. Similarity-based networks are usually applied to model data as network. However, this kind of representation requires the definition of parameters, which significantly affect the classification accuracy, and presents a high cost due to the computation of similarities among all dataset instances. In contrast, bipartite heterogeneous networks have appeared as an alternative to similarity-based networks in text mining applications. In these networks, the words are connected to the documents which they occur. Thus, there is no parameter or additional costs to generate such networks. In this paper, we propose the use of the bipartite network representation to perform transductive classification of music, using a bag-of-frames approach to describe music signals. We demonstrate that the proposed approach outperforms other music classification approaches when few labeled instances are available. © Diego F. Silva, Rafael G. Rossi, Solange O. Rezende, Gustavo E. A. P. A. Batista."
Khlif A.; Sethu V.,An iterative multi range non-negative matrix factorization algorithm for polyphonic music transcription,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010872946&partnerID=40&md5=479f3f1c46eb392b6f1ebbcfd25decb8,"Khlif A., École des Mines ParisTech, France; Sethu V., University of New South Wales, Australia","This article presents a novel iterative algorithm based on Non-negative Matrix Factorisation (NMF) that is particularly well suited to the task of automatic music transcription (AMT). Compared with previous NMF based techniques, this one does not aim at factorizing the time-frequency representation of the entire musical signal into a combination of the possible set of notes. Instead, the proposed algorithm proceeds iteratively by initially decomposing a part of the time-frequency representation into a combination of a small subset of all possible notes then reinvesting this information in the following step involving a large subset of notes. Specifically, starting with the lowest octave of notes that is of interest, each iteration increases the set of notes under consideration by an octave. The resolution of a lower dimensionality problem used to properly initialize matrices for a more complex problem, results in a gain of some percent in the transcription accuracy. © Anis Khlif, Vidhyasaharan Sethu."
Laaksonen A.,Automatic melody transcription based on chord transcription,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057932221&partnerID=40&md5=6a3c2fa9a77dc6bb8c040a52d5207b0e,"Laaksonen A., Department of Computer Science, University of Helsinki, Finland","This paper focuses on automatic melody transcription in a situation where a chord transcription is already available. Given an excerpt of music in audio form and a chord transcription in symbolic form, the task is to create a symbolic melody transcription that consists of note onset times and pitches. We present an algorithm that divides the audio into segments based on the chord transcription, and then matches potential melody patterns to each segment. The algorithm uses chord information to favor melody patterns that are probable in the given harmony context. To evaluate the algorithm, we present a new ground truth dataset that consists of 1,5 hours of audio excerpts together with hand-made melody and chord transcriptions. © Antti Laaksonen."
Gulati S.; Serrà J.; Serra X.,Improving melodic similarity in Indian art music using culture-specific melodic characteristics,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973306872&partnerID=40&md5=4ba8c9daad88a1f314ee406b2b2401d1,"Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Telefonica Research, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Detecting the occurrences of rāgs’ characteristic melodic phrases from polyphonic audio recordings is a fundamental task for the analysis and retrieval of Indian art music. We propose an abstraction process and a complexity weighting scheme which improve melodic similarity by exploiting specific melodic characteristics in this music. In addition, we propose a tetrachord normalization to handle transposed phrase occurrences. The melodic abstraction is based on the partial transcription of the steady regions in the melody, followed by a duration truncation step. The proposed complexity weighting accounts for the differences in the melodic complexities of the phrases, a crucial aspect known to distinguish phrases in Carnatic music. For evaluation we use over 5 hours of audio data comprising 625 annotated melodic phrases belonging to 10 different phrase categories. Results show that the proposed melodic abstraction and complexity weighting schemes significantly improve the phrase detection accuracy, and that tetrachord normalization is a successful strategy for dealing with transposed phrase occurrences in Carnatic music. In the future, it would be worthwhile to explore the applicability of the proposed approach to other melody dominant music traditions such as Flamenco, Beijing opera and Turkish Makam music. © Sankalp Gulati, Joan Serrà and Xavier Serra."
Huang P.-S.; Kim M.; Hasegawa-Johnson M.; Smaragdis P.,Singing-voice separation from monaural recordings using deep recurrent neural networks,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046988721&partnerID=40&md5=3286c4aa59dbb85dc35f4de680ef36d8,"Huang P.-S., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, United States; Kim M., Department of Computer Science, University of Illinois at Urbana-Champaign, United States; Hasegawa-Johnson M., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, United States; Smaragdis P., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, United States, Department of Computer Science, University of Illinois at Urbana-Champaign, United States, Adobe Research, United States","Monaural source separation is important for many real world applications. It is challenging since only single channel information is available. In this paper, we explore using deep recurrent neural networks for singing voice separation from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal connections are explored. We propose jointly optimizing the networks for multiple source signals by including the separation step as a nonlinear operation in the last layer. Different discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30~2.48 dB GNSDR gain and 4.32~5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset. © Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis."
Li B.; Duan Z.,Score following for piano performances with sustain-pedal effects,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994840537&partnerID=40&md5=e5f093b7fab1c17cb6857d149512a4a5,"Li B., Audio Information Research (AIR) Lab, University of Rochester, Department of Electrical and Computer Engineering, United States; Duan Z., Audio Information Research (AIR) Lab, University of Rochester, Department of Electrical and Computer Engineering, United States","One challenge in score following (i.e., mapping audio frames to score positions in real time) for piano performances is the mismatch between audio and score caused by the usage of the sustain pedal. When the pedal is pressed, notes played will continue to sound until the string vibration naturally ceases. This makes the notes longer than their notated lengths and overlap with later notes. In this paper, we propose an approach to address this problem. Given that the most competitive wrong score positions for each audio frame are the ones before the correct position due to the sustained sounds, we remove partials of sustained notes and only retain partials of “new notes” in the audio representation. This operation reduces sustain-pedal effects by weakening the match between the audio frame and previous wrong score positions, hence encourages the system to align to the correct score position. We implement this idea based on a state-of-the-art score following framework. Experiments on synthetic and real piano performances from the MAPS dataset show significant improvements on both alignment accuracy and robustness. © Bochen Li, Zhiyao Duan."
Zhang S.; Repetto R.C.; Serra X.,Study of the similarity between linguistic tones and melodic pitch contours in Beijing opera singing,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054973049&partnerID=40&md5=228c40edda7e083d8ffaa0ccdeb1f1a0,"Zhang S., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Features of linguistic tone contours are important factors that shape the distinct melodic characteristics of different genres of Chinese opera. In Beijing opera, the presence of a two-dialectal tone system makes the tone-melody relationship more complex. In this paper, we propose a novel data-driven approach to analyze syllable-sized tone-pitch contour similarity in a corpus of Beijing Opera (381 arias) with statistical modeling and machine learning methods. A total number of 1,993 pitch contour units and attributes were extracted from a selection of 20 arias. We then build Smoothing Spline ANOVA models to compute matrixes of average melodic contour curves by tone category and other attributes. A set of machine learning and statistical analysis methods are applied to 30-point pitch contour vectors as well as dimensionality-reduced representations using Symbolic Aggregate approXimation(SAX). The results indicate an even mixture of shapes within all tone categories, with the absence of evidence for a predominant dialectal tone system in Beijing opera. We discuss the key methodological issues in melody-tone analysis and future work on pair-wise contour unit analysis. © Shuo Zhang, Rafael Caro Repetto, Xavier Serra."
Duan Z.; Temperley D.,Note-level music transcription by maximum likelihood sampling,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028047705&partnerID=40&md5=840f7bf0ae4a4813c4ca9b68e9397e69,"Duan Z., University of Rochester, Dept. Electrical and Computer Engineering, United States; Temperley D., University of Rochester, Eastman School of Music, United States","Note-level music transcription, which aims to transcribe note events (often represented by pitch, onset and offset times) from music audio, is an important intermediate step towards complete music transcription. In this paper, we present a note-level music transcription system, which is built on a state-of-the-art frame-level multi-pitch estimation (MPE) system. Preliminary note-level transcription achieved by connecting pitch estimates into notes often lead to many spurious notes due to MPE errors. In this paper, we propose to address this problem by randomly sampling notes in the preliminary note-level transcription. Each sample is a subset of all notes and is viewed as a note-level transcription candidate. We evaluate the likelihood of each candidate using the MPE model, and select the one with the highest likelihood as the final transcription. The likelihood treats notes in a transcription as a whole and favors transcriptions with less spurious notes. Experiments conducted on 110 pieces of J.S. Bach chorales with polyphony from 2 to 4 show that the proposed sampling scheme significantly improves the transcription performance from the preliminary approach. The proposed system also significantly outperforms two other state-of-the-art systems in both frame-level and note-level transcriptions. © Zhiyao Duan, David Temperley."
Yadati K.; Larson M.; Liem C.C.S.; Hanjalic A.,Detecting drops in electronic dance music: Content based approaches to a socially significant music event,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040762482&partnerID=40&md5=e3cb4bc799c93725825a40cea266d950,"Yadati K., Delft University of Technology, Netherlands; Larson M., Delft University of Technology, Netherlands; Liem C.C.S., Delft University of Technology, Netherlands; Hanjalic A., Delft University of Technology, Netherlands","Electronic dance music (EDM) is a popular genre of music. In this paper, we propose a method to automatically detect the characteristic event in an EDM recording that is referred to as a drop. Its importance is reflected in the number of users who leave comments in the general neighborhood of drop events in music on online audio distribution platforms like SoundCloud. The variability that characterizes realizations of drop events in EDM makes automatic drop detection challenging. We propose a two-stage approach to drop detection that first models the sound characteristics during drop events and then incorporates temporal structure by zeroing in on a watershed moment. We also explore the possibility of using the drop-related social comments on the SoundCloud platform as weak reference labels to improve drop detection. The method is evaluated using data from SoundCloud. Performance is measured as the overlap between tolerance windows centered around the hypothesized and the actual drop. Initial experimental results are promising, revealing the potential of the proposed method for combining content analysis and social activity to detect events in music recordings. © Karthik Yadati, Martha Larson, Cynthia C. S. Liem, Alan Hanjalic."
Flexer A.,On inter-rater agreement in audio music similarity,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066081175&partnerID=40&md5=2a280609331eb39d08c77e862bae746d,"Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Freyung 6/6, Vienna, Austria",One of the central tasks in the annual MIREX evaluation campaign is the”Audio Music Similarity and Retrieval (AMS)” task. Songs which are ranked as being highly similar by algorithms are evaluated by human graders as to how similar they are according to their subjective judgment. By analyzing results from the AMS tasks of the years 2006 to 2013 we demonstrate that: (i) due to low inter-rater agreement there exists an upper bound of performance in terms of subjective gradings; (ii) this upper bound has already been achieved by participating algorithms in 2009 and not been surpassed since then. Based on this sobering result we discuss ways to improve future evaluations of audio music similarity. © Arthur Flexer.
Aljanaki A.; Wiering F.; Veltkamp R.C.,Computational modeling of induced emotion using GEMs,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061439478&partnerID=40&md5=42961bd554eb36c17bb947e82c45695d,"Aljanaki A., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands","Most researchers in the automatic music emotion recognition field focus on the two-dimensional valence and arousal model. This model though does not account for the whole diversity of emotions expressible through music. Moreover, in many cases it might be important to model induced (felt) emotion, rather than perceived emotion. In this paper we explore a multidimensional emotional space, the Geneva Emotional Music Scales (GEMS), which addresses these two issues. We collected the data for our study using a game with a purpose. We exploit a comprehensive set of features from several state-of-the-art toolboxes and propose a new set of harmonically motivated features. The performance of these feature sets is compared. Additionally, we use expert human annotations to explore the dependency between musicologically meaningful characteristics of music and emotional categories of GEMS, demonstrating the need for algorithms that can better approximate human perception. © Anna Aljanaki, Frans Wiering, Remco C. Veltkamp."
Van Balen J.; Bountouridis D.; Wiering F.; Veltkamp R.,Cognition-inspired descriptors for scalable cover song retrieval,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042128674&partnerID=40&md5=617dd7fe7bfd2979d24ba7388351d043,"Van Balen J., Utrecht University, Department of Information and Computing Sciences, Netherlands; Bountouridis D., Utrecht University, Department of Information and Computing Sciences, Netherlands; Wiering F., Utrecht University, Department of Information and Computing Sciences, Netherlands; Veltkamp R., Utrecht University, Department of Information and Computing Sciences, Netherlands","Inspired by representations used in music cognition studies and computational musicology, we propose three simple and interpretable descriptors for use in mid- to high-level computational analysis of musical audio and applications in content-based retrieval. We also argue that the task of scalable cover song retrieval is very suitable for the development of descriptors that effectively capture musical structures at the song level. The performance of the proposed descriptions in a cover song problem is presented. We further demonstrate that, due to the musically-informed nature of the descriptors, an independently established model of stability and variation in covers songs can be integrated to improve performance. © Jan Van Balen, Dimitrios Bountouridis, Frans Wiering, Remco Veltkamp."
Wu B.; Horner A.; Lee C.,Emotional predisposition of musical instrument timbres with static spectra,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059071611&partnerID=40&md5=fe360ea5d2d9349d0ed929defd8e400f,"Wu B., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Horner A., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Lee C., Information Systems Technology and Design Pillar Singapore University of Technology and Design, 20 Dover Drive, Singapore, 138682, Singapore","Music is one of the strongest triggers of emotions. Recent studies have shown strong emotional predispositions for musical instrument timbres. They have also shown significant correlations between spectral centroid and many emotions. Our recent study on spectral centroid-equalized tones further suggested that the even/odd harmonic ratio is a salient timbral feature after attack time and brightness. The emergence of the even/odd harmonic ratio motivated us to go a step further: to see whether the spectral shape of musical instruments alone can have a strong emotional predisposition. To address this issue, we conducted followup listening tests of static tones. The results showed that the even/odd harmonic ratio again significantly correlated with most emotions, consistent with the theory that static spectral shapes have a strong emotional predisposition. © Bin Wu, Andrew Horner, Chung Lee."
Raffel C.; Ellis D.P.W.,Large-scale content-based matching of MIDI and audio files,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973305560&partnerID=40&md5=47ef0b0e05129933690fac22a297835b,"Raffel C., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States","MIDI files, when paired with corresponding audio recordings, can be used as ground truth for many music information retrieval tasks. We present a system which can efficiently match and align MIDI files to entries in a large corpus of audio content based solely on content, i.e., without using any metadata. The core of our approach is a convolutional network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space. Once represented in this way, we can efficiently perform large-scale dynamic time warping searches to match MIDI data to audio recordings. We evaluate our approach on the task of matching a huge corpus of MIDI files to the Million Song Dataset. © Colin Raffel, Daniel P. W. Ellis."
Deruty E.; Pachet F.,The MIR perspective on the evolution of dynamics in mainstream music,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958070138&partnerID=40&md5=f9612257f816fc4e47aa305d771be059,"Deruty E., Sony Computer Science Laboratory, Akoustic Arts, Paris, France; Pachet F., Sony Computer Science Laboratory, Paris, France","Understanding the evolution of mainstream music is of high interest for the music production industry. In this context, we argue that a MIR perspective may be used to highlight, in particular, relations between dynamics and various properties of mainstream music. We illustrate this claim with two results obtained from a diachronic analysis performed on 7200 tracks released between 1967 and 2014. This analysis suggests that 1) the so-called “loudness war” has peaked in 2007, and 2) its influence has been important enough to override the impact of genre on dynamics. In other words, dynamics in mainstream music are primarily related to a track’s year of release, rather than to its genre. © Emmanuel Deruty François Pachet."
Böck S.; Krebs F.; Widmer G.,Accurate tempo estimation based on recurrent neural networks and resonating comb filters,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994647104&partnerID=40&md5=e8f4b49f80e46cc2d230aecd9eb2d267,"Böck S., Department of Computational Perception Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception Johannes Kepler University, Linz, Austria","In this paper we present a new tempo estimation algorithm which uses a bank of resonating comb filters to determine the dominant periodicity of a musical excerpt. Unlike existing (comb filter based) approaches, we do not use handcrafted features derived from the audio signal, but rather let a recurrent neural network learn an intermediate beat-level representation of the signal and use this information as input to the comb filter bank. While most approaches apply complex post-processing to the output of the comb filter bank like tracking multiple time scales, processing different accent bands, modelling metrical relations, categoris-ing the excerpts into slow / fast or any other advanced processing, we achieve state-of-the-art performance on nine of ten datasets by simply reporting the highest resonator’s histogram peak. © Sebastian Böck, Florian Krebs and Gerhard Widmer."
Singhi A.; Brown D.G.,Are poetry and lyrics all that different?,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015318084&partnerID=40&md5=1a08f3f257826cea72ba7ab74f102866,"Singhi A., University of Waterloo, Cheriton School of Computer Science, Canada; Brown D.G., University of Waterloo, Cheriton School of Computer Science, Canada","We hypothesize that different genres of writing use different adjectives for the same concept. We test our hypothesis on lyrics, articles and poetry. We use the English Wikipedia and over 13,000 news articles from four leading newspapers for the article data set. Our lyrics data set consists of lyrics of more than 10,000 songs by 56 popular English singers, and our poetry dataset is made up of more than 20,000 poems from 60 famous poets. We find the probability distribution of synonymous adjectives in all the three different categories and use it to predict if a document is an article, lyrics or poetry given its set of adjectives. We achieve an accuracy level of 67% for lyrics, 80% for articles and 57% for poetry. Using these probability distribution we show that adjectives more likely to be used in lyrics are more rhymable than those more likely to be used in poetry, but they do not differ significantly in their semantic orientations. Furthermore we show that our algorithm is successfully able to detect poetic lyricists like Bob Dylan from non-poetic ones like Bryan Adams, as their lyrics are more often misclassified as poetry. © Abhishek Singhi, Daniel G. Brown."
Krebs F.; Böck S.; Widmer G.,An efficient state-space model for joint tempo and meter tracking,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993966282&partnerID=40&md5=7325f53ec11ca914dd4090ea59834890,"Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Dynamic Bayesian networks (e.g., Hidden Markov Models) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhythmic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic parameters from a piece of music. While this allows the mutual dependencies between these parameters to be exploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We incorporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets. © Florian Krebs, Sebastian Böck, and Gerhard Widmer."
Martorell A.; Gómez E.,Systematic multi-scale set-class analysis,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069932760&partnerID=40&md5=14a5ec53f7660749ccc3cb887fe3763b,"Martorell A., Universitat Pompeu Fabra, Spain; Gómez E., Universitat Pompeu Fabra, Spain","This work reviews and elaborates a methodology for hierarchical multi-scale set-class analysis of music pieces. The method extends the systematic segmentation and representation of Sapp’s ‘keyscapes’ to the description stage, by introducing a set-class level of description. This provides a systematic, mid-level, and standard analytical lexicon, which allows the description of any notated music based on fixed temperaments. The method benefits from the representation completeness, the compromise between generalisation and discrimination of the set-class spaces, and the access to hierarchical inclusion relations over time. The proposed class-matrices are multidimensional time series encoding the pitch content of every possible music segment over time, regardless the involved time-scales, in terms of a given set-class space. They provide the simplest information mining methods with the ability of capturing sophisticated tonal relations. The proposed class-vectors, quantifying the presence of every possible set-class in a piece, are discussed for advanced explorations of corpora. The compromise between dimensionality and informativeness provided by the class-matrices and class-vectors, is discussed in relation with standard content-based tonal descriptors, and music information retrieval applications. © Agustín Martorell, Emilia Gómez."
Su L.; Yu L.-F.; Yang Y.-H.,Sparse cepstral and phase codes for guitar playing technique classification,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034833748&partnerID=40&md5=4f96ecf3c4fee9765023a51cc375f1b5,"Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yu L.-F., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","Automatic recognition of guitar playing techniques is challenging as it is concerned with subtle nuances of guitar timbres. In this paper, we investigate this research problem by a comparative study on the performance of features extracted from the magnitude spectrum, cepstrum and phase derivatives such as group-delay function (GDF) and instantaneous frequency deviation (IFD) for classifying the playing techniques of electric guitar recordings. We consider up to 7 distinct playing techniques of electric guitar and create a new individual-note dataset comprising of 7 types of guitar tones for each playing technique. The dataset contains 6,580 clips and 11,928 notes. Our evaluation shows that sparse coding is an effective means of mining useful patterns from the primitive time-frequency representations and that combining the sparse representations of logarithm cepstrum, GDF and IFD leads to the highest average F-score of 71.7%. Moreover, from analyzing the confusion matrices we find that cepstral and phase features are particularly important in discriminating highly similar techniques such as pull-off, hammer-on and bending. We also report a preliminary study that demonstrates the potential of the proposed methods in automatic transcription of real-world electric guitar solos. © Li Su, Li-Fan Yu and Yi-Hsuan Yang."
Venkataramani S.; Nayak N.; Rao P.; Velmurugan R.,Vocal separation using singer-vowel priors obtained from polyphonic audio,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028075336&partnerID=40&md5=cda8a5804e210621a9c075796e040d0f,"Venkataramani S., Department of Electrical Engineering, IIT Bombay, Mumbai, 400076, India; Nayak N., Sensibol Audio Technologies Pvt. Ltd, India; Rao P., Department of Electrical Engineering, IIT Bombay, Mumbai, 400076, India; Velmurugan R., Department of Electrical Engineering, IIT Bombay, Mumbai, 400076, India","Single-channel methods for the separation of the lead vocal from mixed audio have traditionally included harmonic-sinusoidal modeling and matrix decomposition methods, each with its own strengths and shortcomings. In this work we use a hybrid framework to incorporate prior knowledge about singer and phone identity to achieve the superior separation of the lead vocal from the instrumental background. Singer specific dictionaries learned from available polyphonic recordings provide the soft mask that effectively attenuates the bleeding-through of accompanying melodic instruments typical of purely harmonic-sinusoidal model based separation. The dictionary learning uses NMF optimization across a training set of mixed signal utterances while keeping the vocal signal bases constant across the utterances. A soft mask is determined for each test mixed utterance frame by imposing sparseness constraints in the NMF partial co-factorization. We demonstrate significant improvements in reconstructed signal quality arising from the more accurate estimation of singer-vowel spectral envelope. © Shrikant Venkataramani, Nagesh Nayak, Preeti Rao, Rajbabu Velmurugan."
Kong L.W.; Lee T.,Automatic key partition based on tonal organization information of classical music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069902172&partnerID=40&md5=2fbe87c813b48244b8ee7772acbbb010,"Kong L.W., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong SAR, China; Lee T., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong SAR, China","Key information is a useful information for tonal music analysis. It is related to chord progressions, which follows some specific structures and rules. In this paper, we describe a generative account of chord progression consisting of phrase-structure grammar rules proposed by Martin Rohrmeier. With some modifications, these rules can be used to partition a chord symbol sequence into different key areas, if modulation occurs. Exploiting tonal grammar rules, the most musically sensible key partition of chord sequence is derived. Some examples of classical music excerpts are evaluated. This rule-based system is compared against another system which is based on dynamic programming of harmonic-hierarchy information. Using Kostka-Payne corpus as testing data, the experimental result shows that our system is better in terms of key detection accuracy. © Lam Wang Kong, Tan Lee."
Nunes L.; Rocamora M.; Jure L.; Biscainho L.W.P.,Beat and downbeat tracking based on rhythmic patterns applied to the Uruguayan candombe drumming,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002676697&partnerID=40&md5=e56ce4af9cd9ce309df113a4ab2f97fb,"Nunes L., ATL-Brazil, Microsoft, United States; Rocamora M., Universidad de la República, Uruguay; Jure L., Universidad de la República, Uruguay; Biscainho L.W.P., Federal Univ. of Rio de Janeiro, Brazil","Computational analysis of the rhythmic/metrical structure of music from recorded audio is a hot research topic in music information retrieval. Recent research has explored the explicit modeling of characteristic rhythmic patterns as a way to improve upon existing beat-tracking algorithms, which typically fail on dealing with syncopated or polyrhythmic music. This work takes the Uruguayan Candombe drumming (an afro-rooted rhythm from Latin America) as a case study. After analyzing the aspects that make this music genre troublesome for usual algorithmic approaches and describing its basic rhythmic patterns, the paper proposes a supervised scheme for rhythmic pattern tracking that aims at finding the metric structure from a Candombe recording, including beat and downbeat phases. Then it evaluates and compares the performance of the method with those of general-purpose beat-tracking algorithms through a set of experiments involving a database of annotated recordings totaling over two hours of audio. The results of this work reinforce the advantages of tracking rhythmic patterns (possibly learned from annotated music) when it comes to automatically following complex rhythms. A software implementation of the proposal as well as the annotated database utilized are available to the research community with the publication of this paper. © Leonardo Nunes, Martín Rocamora, Luis Jure, Luiz W. P. Biscainho."
Hu X.; Lee J.H.; Wong L.K.Y.,Music information behaviors and system preferences of university students in Hong kong,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064493843&partnerID=40&md5=b260bb6975a173ac63d67be73d48687b,"Hu X., University of Hong Kong, Hong Kong; Lee J.H., University of Washington, United States; Wong L.K.Y., University of Hong Kong, Hong Kong","This paper presents a user study on music information needs and behaviors of university students in Hong Kong. A mix of quantitative and qualitative methods was used. A survey was completed by 101 participants and supplemental interviews were conducted in order to investigate users’ music information related activities. We found that university students in Hong Kong listened to music frequently and mainly for the purposes of entertainment, singing and playing instruments, and stress reduction. This user group often searches for music with multiple methods, but common access points like genre and time period were rarely used. Sharing music with people in their online social networks such as Facebook and Weibo was a common activity. Furthermore, the popularity of smartphones prompted the need for streaming music and mobile music applications. We also examined users’ preferences on music services available in Hong Kong such as YouTube and KKBox, as well as the characteristics liked and disliked by the users. The results not only offer insights into non-Western users’ music behaviors but also for designing online music services for young music listeners in Hong Kong. © Xiao Hu, Jin Ha Lee, Leanne Ka Yan Wong."
Glazyrin N.,Towards automatic content-based separation of DJ mixes into single tracks,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028080646&partnerID=40&md5=280ef71c6d671032fc33da09c7c2d458,"Glazyrin N., Ural Federal University, Russian Federation","DJ mixes and radio show recordings constitute an important and underexploited music and data source. In this paper we try to approach the problem of separation of a continuous DJ mix into single tracks or timestamping a mix. Sharing some aspects with the task of structural segmentation, this problem has a number of distinctive features that make difficulties for structural segmentation algorithms designed to work with a single track. We use the information derived from spectrum data to separate tracks from each other. We show that the metadata that usually comes with DJ mixes can be exploited to improve the separation. An iterative algorithm that can consider both content-based data and user provided metadata is proposed and evaluated on a collection of freely available timestamped DJ mix recordings of various styles. © Nikolay Glazyrin."
Morchid M.; Dufour R.; Linarès G.,A combined thematic and acoustic approach for a music recommendation service in TV commercials,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069922280&partnerID=40&md5=65c828cea61032ac69b1efcf372ea329,"Morchid M., LIA - University of Avignon, France; Dufour R., LIA - University of Avignon, France; Linarès G., LIA - University of Avignon, France","Most of modern advertisements contain a song to illustrate the commercial message. The success of a product, and its economic impact, can be directly linked to this choice. Finding the most appropriate song is usually made manually. Nonetheless, a single person is not able to listen and choose the best music among millions. The need for an automatic system for this particular task becomes increasingly critical. This paper describes the LIA music recommendation system for advertisements using both textual and acoustic features. This system aims at providing a song to a given commercial video and was evaluated in the context of the MediaEval 2013 Soundtrack task [14]. The goal of this task is to predict the most suitable soundtrack from a list of candidate songs, given a TV commercial. The organizers provide a development dataset including multimedia features. The initial assumption of the proposed system is that commercials which sell the same type of product, should also share the same music rhythm. A two-fold system is proposed: find commercials with close subjects in order to determine the mean rhythm of this subset, and then extract, from the candidate songs, the music which better corresponds to this mean rhythm. © Mohamed Morchid, Richard Dufour, Georges Linarès."
Thompson L.; Mauch M.; Dixon S.,Drum transcription via classification of bar-level rhythmic patterns,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019997798&partnerID=40&md5=1b07bda9bdf7489f1d4312e33f0a5f1f,"Thompson L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We propose a novel method for automatic drum transcription from audio that achieves the recognition of individual drums by classifying bar-level drum patterns. Automatic drum transcription has to date been tackled by recognising individual drums or drum combinations. In high-level tasks such as audio similarity, statistics of longer rhythmic patterns have been used, reflecting that musical rhythm emerges over time. We combine these two approaches by classifying bar-level drum patterns on sub-beat quantised timbre features using support vector machines. We train the classifier using synthesised audio and carry out a series of experiments to evaluate our approach. Using six different drum kits, we show that the classifier generalises to previously unseen drum kits when trained on the other five (80% accuracy). Measures of precision and recall show that even for incorrectly classified patterns many individual drum events are correctly transcribed. Tests on 14 acoustic performances from the ENST-Drums dataset indicate that the system generalises to real-world recordings. Limited by the set of learned patterns, performance is slightly below that of a comparable method. However, we show that for rock music, the proposed method performs as well as the other method and is substantially more robust to added polyphonic accompaniment. © Lucas Thompson, Matthias Mauch and Simon Dixon."
Stephen Downie J.; Hu X.; Lee J.H.; Choi K.; Cunningham S.J.; Hao Y.,"Ten years of MIREX: Reflections, challenges and opportunities",2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047950429&partnerID=40&md5=7b78b146f0a38913fc16ed09ed07bd77,"Stephen Downie J., University of Illinois, United States; Hu X., University of Hong Kong, Hong Kong; Lee J.H., University of Washington, United States; Choi K., University of Illinois, United States; Cunningham S.J., University of Waikato, New Zealand; Hao Y., University of Illinois, United States","The Music Information Retrieval Evaluation eXchange (MIREX) has been run annually since 2005, with the October 2014 plenary marking its tenth iteration. By 2013, MIREX has evaluated approximately 2000 individual music information retrieval (MIR) algorithms for a wide range of tasks over 37 different test collections. MIREX has involved researchers from over 29 different countries with a median of 109 individual participants per year. This paper summarizes the history of MIREX from its earliest planning meeting in 2001 to the present. It reflects upon the administrative, financial, and technological challenges MIREX has faced and describes how those challenges have been surmounted. We propose new funding models, a distributed evaluation framework, and more holistic user experience evaluation tasks—some evolutionary, some revolutionary—for the continued success of MIREX. We hope that this paper will inspire MIR community members to contribute their ideas so MIREX can have many more successful years to come. © J. Stephen Downie, Xiao Hu, Jin Ha Lee, Kahyun Choi, Sally Jo Cunningham, Yun Hao."
Driedger J.; Prätzlich T.; Müller M.,Let it bee – Towards NMF-inspired audio mosaicing,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984816257&partnerID=40&md5=d963ac73d91ff56dc9b27709bf2fafd6,"Driedger J., International Audio Laboratories, Erlangen, Germany; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","A swarm of bees buzzing “Let it be” by the Beatles or the wind gently howling the romantic “Gute Nacht” by Schubert – these are examples of audio mosaics as we want to create them. Given a target and a source recording, the goal of audio mosaicing is to generate a mosaic recording that conveys musical aspects (like melody and rhythm) of the target, using sound components taken from the source. In this work, we propose a novel approach for automatically generating audio mosaics with the objective to preserve the source’s timbre in the mosaic. Inspired by algorithms for non-negative matrix factorization (NMF), our idea is to use update rules to learn an activation matrix that, when multiplied with the spectrogram of the source recording, resembles the spectrogram of the target recording. However, when applying the original NMF procedure, the resulting mosaic does not adequately reflect the source’s timbre. As our main technical contribution, we propose an extended set of update rules for the iterative learning procedure that supports the development of sparse diagonal structures in the activation matrix. We show how these structures better retain the source’s timbral characteristics in the resulting mosaic. © Jonathan Driedger, Thomas Prätzlich, Meinard Müller."
Miron M.; Carabias-Orti J.J.; Janer J.,Audio-to-score alignment at note level for orchestral recordings,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013215748&partnerID=40&md5=839ebd2df3e20ed8f17ca8168962c425,"Miron M., Music Technology Group, Universitat Pompeu Fabra, Spain; Carabias-Orti J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Janer J., Music Technology Group, Universitat Pompeu Fabra, Spain","In this paper we propose an offline method for refining audio-to-score alignment at the note level in the context of orchestral recordings. State-of-the-art score alignment systems estimate note onsets with a low time resolution, and without detecting note offsets. For applications such as score-informed source separation we need a precise alignment at note level. Thus, we propose a novel method that refines alignment by determining the note onsets and offsets in complex orchestral mixtures by combining audio and image processing techniques. First, we introduce a note-wise pitch salience function that weighs the harmonic contribution according to the notes present in the score. Second, we perform image binarization and blob detection based on connectivity rules. Then, we pick the best combination of blobs, using dynamic programming. We finally obtain onset and offset times from the boundaries of the most salient blob. We evaluate our method on a dataset of Bach chorales, showing that the proposed approach can accurately estimate note onsets and offsets. © Marius Miron, Julio José Carabias-Orti, Jordi Janer."
Zhou X.; Lerch A.,Chord detection using deep learning,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978854736&partnerID=40&md5=9dc00781fcbe961953f8c3677d3d5eb0,"Zhou X., Center for Music Technology, Georgia Institute of Technology, Georgia; Lerch A., Center for Music Technology, Georgia Institute of Technology, Georgia","In this paper, we utilize deep learning to learn high-level features for audio chord detection. The learned features, obtained by a deep network in bottleneck architecture, give promising results and outperform state-of-the-art systems. We present and evaluate the results for various methods and configurations, including input pre-processing, a bottleneck architecture, and SVMs vs. HMMs for chord classification. © Xinquan Zhou, Alexander Lerch."
Arzt A.; Widmer G.,Real-time music tracking using multiple performances as a reference,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006887569&partnerID=40&md5=fe87ae4ac4b7adacb32503edb7ceacd8,"Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In general, algorithms for real-time music tracking directly use a symbolic representation of the score, or a synthesised version thereof, as a reference for the on-line alignment process. In this paper we present an alternative approach. First, different performances of the piece in question are collected and aligned (off-line) to the symbolic score. Then, multiple instances of the on-line tracking algorithm (each using a different performance as a reference) are used to follow the live performance, and their output is combined to come up with the current position in the score. As the evaluation shows, this strategy improves both the robustness and the precision, especially on pieces that are generally hard to track (e.g. pieces with extreme, abrupt tempo changes, or orchestral pieces with a high degree of polyphony). Finally, we describe a real-world application, where this music tracking algorithm was used to follow a world-famous orchestra in a concert hall in order to show synchronised visual content (the sheet music, explanatory text and videos) to members of the audience. © Andreas Arzt, Gerhard Widmer."
Duval E.; van Berchum M.; Jentzsch A.; Parra Chico G.A.; Drakos A.,Musicology of early music with europeana tools and services,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005994076&partnerID=40&md5=2083b0f5d422abc02e1062aa1dfe5f32,"Duval E., Dept. of Computer Science, KU Leuven, B, Belgium; van Berchum M., KNAW-DANS, Utrecht University, Netherlands; Jentzsch A., Open Knowledge Foundation, D, United Kingdom; Parra Chico G.A., Dept. of Computer Science, KU Leuven, B, Belgium; Drakos A., AgroKnow, Greece","The Europeana repository hosts large collections of digitized music manuscripts and prints. This paper investigates how tools and services for this repository can enable Early Music musicologists to carry out their research in a more effective or efficient way, or to carry out research that is impossible to do without such tools or services. We report on the methodology, user-centered development of a suite of tools that we have integrated loosely, in order to experiment with this specific target audience and an evaluation of the impact that such tools may have on how these musicologists carry out their research. Positive feedback relates to the automation of data sharing between the loosely coupled tools and support for an integrated workflow. Participants in this study wanted to have the ability to work not only with individual items, but also with collections of such items. The use of search facets to filter, and visualization around time and place were positively evaluated, as was the use of Optical Music Recognition and computer-supported analysis of music scores. The musicologists were not convinced of the value of activity streams. They also wanted a less strictly linear organization of their workflow and the ability to not only consume items from the repository, but to also push their research results back into the Europeana repository. © Erik Duval, Marnix van Berchum, Anja Jentzsch,."
Kaneshiro B.; Dmochowski J.P.,Neuroimaging methods for music information retrieval: Current findings and future prospects,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988033894&partnerID=40&md5=cbb74b49b792297b64ef608db72b973d,"Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, Stanford, CA, United States; Dmochowski J.P., Department of Psychology, Stanford University, Stanford, CA, United States","Over the past decade and a half, music information retrieval (MIR) has grown into a robust, cross-disciplinary field spanning a variety of research domains. Collaborations between MIR and neuroscience researchers, however, are still rare, and to date only a few studies using approaches from one domain have successfully reached an audience in the other. In this paper, we take an initial step toward bridging these two fields by reviewing studies from the music neuroscience literature, with an emphasis on imaging modalities and analysis techniques that might be of practical interest to the MIR community. We show that certain approaches currently used in a neuroscientific setting align with those used in MIR research, and discuss implications for potential areas of future research. We additionally consider the impact of disparate research objectives between the two fields, and how such a discrepancy may have hindered cross-discipline output thus far. It is hoped that a heightened awareness of this literature will foster interaction and collaboration between MIR and neuroscience researchers, leading to advances in both fields that would not have been achieved independently. © Blair Kaneshiro, Jacek P. Dmochowski."
Church M.; Cuthbert M.S.,Improving rhythmic transcriptions via probability models applied post-OMR,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056669908&partnerID=40&md5=c82ec08e4cfcfba1887746c10ff635b8,"Church M., Applied Math, Harvard University, Google Inc., United States; Cuthbert M.S., Music and Theater Arts, M.I.T, United States","Despite many improvements in the recognition of graphical elements, even the best implementations of Optical Music Recognition (OMR) introduce inaccuracies in the resultant score. These errors, particularly rhythmic errors, are time consuming to fix. Most musical compositions repeat rhythms between parts and at various places throughout the score. Information about rhythmic self-similarity, however, has not previously been used in OMR systems. This paper describes and implements methods for using the prior probabilities for rhythmic similarities in scores produced by a commercial OMR system to correct rhythmic errors which cause a contradiction between the notes of a measure and the underlying time signature. Comparing the OMR output and post-correction results to hand-encoded scores of 37 polyphonic pieces and movements (mostly drawn from the classical repertory), the system reduces incorrect rhythms by an average of 19% (min: 2%, max: 36%). The paper includes a public release of an implementation of the model in music21 and also suggests future refinements and applications to pitch correction that could further improve the accuracy of OMR systems. © Maura Church, Michael Scott Cuthbert."
Tang Z.; Black D.A.A.,Melody extraction from polyphonic audio of Western opera: A method based on detection of the singer’s formant,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020986484&partnerID=40&md5=80f85bb14fad117b82da6f96357f21b2,"Tang Z., University of Washington, Department of Electrical Engineering, United States; Black D.A.A., Queen Mary University of London, Electronic Engineering and Computer Science, United Kingdom","Current melody extraction approaches perform poorly on the genre of opera [1, 2]. The singer’s formant is defined as a prominent spectral-envelope peak around 3 kHz found in the singing of professional Western opera singers [3]. In this paper we introduce a novel melody extraction algorithm based on this feature for opera signals. At the front end, it automatically detects the singer’s formant according to the Long-Term Average Spectrum (LTAS). This detection function is also applied to the short-term spectrum in each frame to determine the melody. The Fan Chirp Transform (FChT) [4] is used to compute pitch salience as its high time-frequency resolution overcomes the difficulties introduced by vibrato. Subharmonic attenuation is adopted to handle octave errors which are common in opera vocals. We improve the FChT algorithm so that it is capable of correcting outliers in pitch detection. The performance of our method is compared to 5 state-of-the-art melody extraction algorithms on a newly created dataset and parts of the ADC2004 dataset. Our algorithm achieves an accuracy of 87.5% in singer’s formant detection. In the evaluation of melody extraction, it has the best performance in voicing detection (91.6%), voicing false alarm (5.3%) and overall accuracy (82.3%). © Zheng Tang, Dawn A. A. Black."
Schramm R.; de Souza Nunes H.; Jung C.R.,Automatic solfège assessment,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008164342&partnerID=40&md5=da36930e755400f57b9be45ab213c3fe,"Schramm R., Institute of Informatics, Federal University of Rio Grande do Sul, Brazil; de Souza Nunes H., Department of Music, Federal University of Rio Grande do Sul of Music, Brazil; Jung C.R., Institute of Informatics, Federal University of Rio Grande do Sul, Brazil","This paper presents a note-by-note approach for automatic solfège assessment. The proposed system uses melodic transcription techniques to extract the sung notes from the audio signal, and the sequence of melodic segments is subsequently processed by a two stage algorithm. On the first stage, an aggregation process is introduced to perform the temporal alignment between the transcribed melody and the music score (ground truth). This stage implicitly aggregates and links the best combination of the extracted melodic segments with the expected note in the ground truth. On the second stage, a statistical method is used to evaluate the accuracy of each detected sung note. The technique is implemented using a Bayesian classifier, which is trained using an audio dataset containing individual scores provided by a committee of expert listeners. These individual scores were measured at each musical note, regarding the pitch, onset, and offset accuracy. Experimental results indicate that the classification scheme is suitable to be used as an assessment tool, providing useful feedback to the student. © Rodrigo Schramm, Helena de Souza Nunes, Cláudio Rosito Jung."
Aljanaki A.; Wiering F.; Veltkamp R.C.,Emotion based segmentation of musical audio,2015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988327778&partnerID=40&md5=a46205f4017c59f08147f605a2a9aae7,"Aljanaki A., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands","The dominant approach to musical emotion variation detection tracks emotion over time continuously and usually deals with time resolutions of one second. In this paper we discuss the problems associated with this approach and propose to move to bigger time resolutions when tracking emotion over time. We argue that it is more natural from the listener’s point of view to regard emotional variation in music as a progression of emotionally stable segments. In order to enable such tracking of emotion over time it is necessary to segment music at the emotional boundaries. To address this problem we conduct a formal evaluation of different segmentation methods as applied to a task of emotional boundary detection. We collect emotional boundary annotations from three annotators for 52 musical pieces from the RWC music collection that already have structural annotations from the SALAMI dataset. We investigate how well structural segmentation explains emotional segmentation and find that there is a large overlap, though about a quarter of emotional boundaries do not coincide with structural ones. We also study inter-annotator agreement on emotional segmentation. Lastly, we evaluate different unsupervised segmentation methods when applied to emotional boundary detection and find that, in terms of F-measure, the Structural Features method performs best. © Anna Aljanaki, Frans Wiering, Remco C. Veltkamp."
Dutta S.; Murthy H.A.,Discovering typical motifs of a rāga from one-liners of songs in carnatic music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039957811&partnerID=40&md5=f181a03d444ef227585665d2da7e97d4,"Dutta S., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India; Murthy H.A., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India","Typical motifs of a rāga can be found in the various songs that are composed in the same rāga by different composers. The compositions in Carnatic music have a definite structure, the one commonly seen being pallavi, anupallavi and charanam. The tala is also fixed for every song. Taking lines corresponding to one or more cycles of the pallavi, anupallavi and charanam as one-liners, one-liners across different songs are compared using a dynamic programming based algorithm. The density of match between the one-liners and normalized cost along-with a new measure, which uses the stationary points in the pitch contour to reduce the false alarms, are used to determine and locate the matched pattern. The typical motifs of a rāga are then filtered using compositions of various rāgas. Motifs are considered typical if they are present in the compositions of the given rāga and are not found in compositions of other rāgas. © Shrey Dutta, Hema A. Murthy."
Przyjaciel-Zablocki M.; Hornung T.; Schätzle A.; Gauß S.; Taxidou I.; Lausen G.,MUSE: A music recommendation management system,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069902609&partnerID=40&md5=8dd189f177c9b2cb3923ee6c99618599,"Przyjaciel-Zablocki M., Department of Computer Science, University of Freiburg, Germany; Hornung T., Department of Computer Science, University of Freiburg, Germany; Schätzle A., Department of Computer Science, University of Freiburg, Germany; Gauß S., Department of Computer Science, University of Freiburg, Germany; Taxidou I., Department of Computer Science, University of Freiburg, Germany; Lausen G., Department of Computer Science, University of Freiburg, Germany","Evaluating music recommender systems is a highly repetitive, yet non-trivial, task. But it has the advantage over other domains that recommended songs can be evaluated immediately by just listening to them. In this paper, we present MUSE – a music recommendation management system – for solving the typical tasks of an in vivo evaluation. MUSE provides the typical off-the-shelf evaluation algorithms, offers an online evaluation system with automatic reporting, and by integrating online streaming services also a legal possibility to evaluate the quality of recommended songs in real time. Finally, it has a built-in user management system that conforms with state-of-the-art privacy standards. New recommender algorithms can be plugged in comfortably and evaluations can be configured and managed online. © Martin Przyjaciel-Zablocki, Thomas Hornung, Alexander Schätzle, Sven Gauß, Io Taxidou, Georg Lausen."
Wu M.-J.; Jang J.-S.R.; Lu C.-H.,Gender identification and age estimation of users based on music metadata,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057947447&partnerID=40&md5=bdd1f8f1735ad6095a347687d6a433b2,"Wu M.-J., Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Computer Science Department, National Taiwan University, Taipei, Taiwan; Lu C.-H., Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Institute for Information Industry, Taipei, Taiwan","Music recommendation is a crucial task in the field of music information retrieval. However, users frequently withhold their real-world identity, which creates a negative impact on music recommendation. Thus, the proposed method recognizes users’ real-world identities based on music metadata. The approach is based on using the tracks most frequently listened to by a user to predict their gender and age. Experimental results showed that the approach achieved an accuracy of 78.87% for gender identification and a mean absolute error of 3.69 years for the age estimation of 48403 users, demonstrating its effectiveness and feasibility, and paving the way for improving music recommendation based on such personal information. © Ming-Ju Wu, Jyh-Shing Roger Jang, Chun-Hung Lu."
Jiang N.; Müller M.,Automated methods for analyzing music recordings in sonata form,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063429818&partnerID=40&md5=7b46b9e6e5240e6d752a6b2cd9cc43a8,"Jiang N., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","The sonata form has been one of the most important large-scale musical structures used since the early Classical period. Typically, the first movements of symphonies and sonatas follow the sonata form, which (in its most basic form) starts with an exposition and a repetition thereof, continues with a development, and closes with a recapitulation. The recapitulation can be regarded as an altered repeat of the exposition, where certain substructures (first and second subject groups) appear in musically modified forms. In this paper, we introduce automated methods for analyzing music recordings in sonata form, where we proceed in two steps. In the first step, we derive the coarse structure by exploiting that the recapitulation is a kind of repetition of the exposition. This requires audio structure analysis tools that are invariant under local modulations. In the second step, we identify finer substructures by capturing relative modulations between the subject groups in exposition and recapitulation. We evaluate and discuss our results by means of the Beethoven piano sonatas. In particular, we introduce a novel visualization that not only indicates the benefits and limitations of our methods, but also yields some interesting musical insights into the data. © 2013 International Society for Music Information Retrieval."
Kirlin P.B.,A data set for computational studies of schenkerian analysis,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977905068&partnerID=40&md5=f8140277461862dd86b23740b9a19ba2,"Kirlin P.B., Department of Mathematics and Computer Science, Rhodes College, United States","Schenkerian analysis, a kind of hierarchical music analysis, is widely used by music theorists. Though it is part of the standard repertoire of analytical techniques, computational studies of Schenkerian analysis have been hindered by the lack of available data sets containing both musical compositions and ground-truth analyses of those compositions. Without such data sets, it is difficult to empirically study the patterns that arise in analyses or rigorously evaluate the performance of intelligent systems for this kind of analysis. To combat this, we introduce the first publicly available large-scale data set of computer-processable Schenkerian analyses. We discuss the choice of musical selections in the data set, the encoding of the music and the corresponding ground-truth analyses, and the possible uses of these data. As an example of the utility of the data set, we present an algorithm that transforms the Schenkerian analyses into hierarchically-organized data structures that are easily manipulated in software. © Phillip B. Kirlin."
Van Balen J.; Burgoyne J.A.; Wiering F.; Veltkamp R.C.,An analysis of chorus features in popular song,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066062329&partnerID=40&md5=edaa248f95c4d687cf5b19a09eebdf9a,"Van Balen J., Utrecht University, Department of Information and Computing Sciences, Netherlands; Burgoyne J.A., Universiteit van Amsterdam, Institute for Logic, Language and Computation, Netherlands; Wiering F., Utrecht University, Department of Information and Computing Sciences, Netherlands; Veltkamp R.C., Utrecht University, Department of Information and Computing Sciences, Netherlands","This paper presents a computational study of the perceptual and musicological audio features that correlate with the structural function of sections in pop songs, specifically the chorus. Choruses have been described as more prominent, more catchy and more memorable than other sections in a song, yet chorus detection applications have always been primarily based on identifying the most-repeated section in a song. Inspired by cognitive research rather than applied signal processing, this computational analysis compiles a list of robust and interpretable features and models their influence on the ‘chorusness’ of a collection of song sections from the Billboard dataset. This is done through the unsupervised learning of a probabilistic graphical model. We show that timbre and timbre variety are more strongly related to chorus qualities than harmony and absolute pitch height. A regression and a classification experiment are performed to quantify these relations. © 2013 International Society for Music Information Retrieval."
Laaksonen A.; Lemström K.,On finding symbolic themes directly from audio files using dynamic programming,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069838428&partnerID=40&md5=8e017af5f043c1b3c2030efb2e1133ae,"Laaksonen A., Department of Computer Science, University of Helsinki, Finland; Lemström K., Department of Computer Science, University of Helsinki, Finland, Laurea University of Applied Sciences, Finland","In this paper our goal is to find occurrences of a theme within a musical work. The theme is given in a symbolic form that is searched for directly in an audio file. We present a dynamic programming algorithm that is related to an existing time-warp invariant algorithm. However, the new algorithm is computationally more efficient than its predecessor, and it can also be used for approximate timescale invariant search. In the latter case the note durations in the query are taken into account, but some time jittering is allowed for. When dealing with audio, these are important properties because the number of possible note events is large and the note positions are not exact. We evaluate the algorithm using a collection of themes from Tchaikovsky’s symphonies. The new approximate time-scaled algorithm seems to be a good choice for this setting. © 2013 International Society for Music Information Retrieval."
Humphrey E.J.; Nieto O.; Bello J.P.,Data driven and discriminative projections for large-scale cover song identification,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006053298&partnerID=40&md5=34e299aed256d1866766aa6a34bec9b6,"Humphrey E.J., Music and Audio Research Laboratory, New York University, United States; Nieto O., Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","The predominant approach to computing document similarity in web scale applications proceeds by encoding task-specific invariance in a vectorized representation, such that the relationship between items can be computed efficiently by a simple scoring function, e.g. Euclidean distance. Here, we improve upon previous work in large-scale cover song identification by using data-driven projections at different time-scales to capture local features and embed summary vectors into a semantically organized space. We achieve this by projecting 2D-Fourier Magnitude Coefficients (2D-FMCs) of beat-chroma patches into a sparse, high dimensional representation which, due to the shift invariance properties of the Fourier Transform, is similar in principle to convolutional sparse coding. After aggregating these local beat-chroma projections, we apply supervised dimensionality reduction to recover an embedding where distance is useful for cover song retrieval. Evaluating on the Million Song Dataset, we find our method outperforms the current state of the art overall, but significantly so for top-k metrics, which indicate improved usability. © 2013 International Society for Music Information Retrieval."
Böck S.; Krebs F.; Widmer G.,A multi-model approach to beat tracking considering heterogeneous music styles,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973298434&partnerID=40&md5=3e1e03cdb993004b6d581d8306b9813c,"Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possible beat positions. It chooses the model with the most appropriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27% over existing state-of-the-art methods. Under certain conditions the system is able to match even human tapping performance. © Sebastian Böck, Florian Krebs and Gerhard Widmer."
Kosta K.; Song Y.; Fazekas G.; Sandler M.B.,A study of cultural dependence of perceived mood in greek music,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028332273&partnerID=40&md5=4b8d559d11cbb6d09096928df2645ffb,"Kosta K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Song Y., Centre for Digital Music, Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","Several algorithms have been developed in the music information retrieval community for predicting mood in music in order to facilitate organising and accessing large audio collections. Little attention has been paid however to how perceived emotion depends on cultural factors, such as listeners’ acculturation or familiarity with musical background or language. In this study, we examine this dependence in the context of Greek music. A large representative database of Greek songs has been created and sampled observing predefined criteria such as the balance between Eastern and Western influenced musical genres. Listeners were then asked to rate songs according to their perceived mood. We collected continuous ratings of arousal and valence for short song excerpts and also asked participants to select a mood tag from a controlled mood vocabulary that best described the music. We analysed the consistency of ratings between Greek and non-Greek listeners and the relationships between the categorical and dimensional representations of emotions. Our results show that there is a greater agreement in listener’s judgements with Greek background compared to the group with varying background. These findings suggest valuable implications on the future development of mood prediction systems. © 2013 International Society for Music Information Retrieval."
Burgoyne J.A.; de Haas W.B.; Pauwels J.,On comparative statistics for labelling tasks: What can we learn from MIREX ACE 2013?,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973329302&partnerID=40&md5=4da69c833a8138dad3d747818d11a112,"Burgoyne J.A., Universiteit van Amsterdam, Netherlands; de Haas W.B., Universiteit Utrecht, Netherlands; Pauwels J., STMS IRCAM, CNRS, UPMC, France","For MIREX 2013, the evaluation of audio chord estimation (ACE) followed a new scheme. Using chord vocabularies of differing complexity as well as segmentation measures, the new scheme provides more information than the ACE evaluations from previous years. With this new information, however, comes new interpretive challenges. What are the correlations among different songs and, more importantly, different submissions across the new measures? Performance falls off for all submissions as the vocabularies increase in complexity, but does it do so directly in proportion to the number of more complex chords, or are certain algorithms indeed more robust? What are the outliers, song-algorithm pairs where the performance was substantially higher or lower than would be predicted, and how can they be explained? Answering these questions requires moving beyond the Friedman tests that have most often been used to compare algorithms to a richer underlying model. We propose a logistic-regression approach for generating comparative statistics for MIREX ACE, supported with generalised estimating equations (GEES) to correct for repeated measures. We use the MIREX 2013 ACE results as a case study to illustrate our proposed method, including some of interesting aspects of the evaluation that might not apparent from the headline results alone. © John Ashley Burgoyne, W. Bas de Haas, Johan Pauwels."
Rodríguez-López M.; Volk A.; Bountouridis D.,Multi-strategy segmentation of melodies,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948953110&partnerID=40&md5=5ba8893218e3a712b2d6128a105f5861,"Rodríguez-López M., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands; Bountouridis D., Utrecht University, Netherlands","Melodic segmentation is a fundamental yet unsolved problem in automatic music processing. At present most melody segmentation models rely on a ‘single strategy’ (i.e. they model a single perceptual segmentation cue). However, cognitive studies suggest that multiple cues need to be considered. In this paper we thus propose and evaluate a ‘multi-strategy’ system to automatically segment symbolically encoded melodies. Our system combines the contribution of different single strategy boundary detection models. First, it assesses the perceptual relevance of a given boundary detection model for a given input melody; then it uses the boundaries predicted by relevant detection models to search for the most plausible segmentation of the melody. We use our system to automatically segment a corpus of instrumental and vocal folk melodies. We compare the predictions to human annotated segments, and to state of the art segmentation methods. Our results show that our system outperforms the state-of-the-art in the instrumental set. © Marcelo Rodríguez-López, Anja Volk, Dimitrios Bountouridis."
Wu B.; Wun S.; Lee C.; Horner A.,Spectral correlates in emotion labeling of sustained musical instrument tones,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055724292&partnerID=40&md5=8200fd8035c43599fe584a6f23442559,"Wu B., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Wun S., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Lee C., Information Systems Technology and Design Pillar, Singapore University of Technology and Design, 20 Dover Drive, 138682, Singapore; Horner A., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong","Music is one of the strongest inducers of emotion in humans. Melody, rhythm, and harmony provide the primary triggers, but what about timbre? Do the musical instruments have underlying emotional characters? For example, is the well-known melancholy sound of the English horn due to its timbre or to how composers use it? Though music emotion recognition has received a lot of attention, researchers have only recently begun considering the relationship between emotion and timbre. To this end, we devised a listening test to compare representative tones from eight different wind and string instruments. The goal was to determine if some tones were consistently perceived as being happier or sadder in pairwise comparisons. A total of eight emotions were tested in the study. The results showed strong underlying emotional characters for each instrument. The emotions Happy, Joyful, Heroic, and Comic were strongly correlated with one another. The violin, trumpet, and clarinet best represented these emotions. Sad and Depressed were also strongly correlated. These two emotions were best represented by the horn and flute. Scary was the emotional outlier of the group, while the oboe had the most emotionally neutral timbre. Also, we found that emotional judgment correlates significantly with average spectral centroid for the more distinctive emotions, including Happy, Joyful, Sad, Depressed, and Shy. These results can provide insights in orchestration, and lay the groundwork for future studies on emotion and timbre. © 2013 International Society for Music Information Retrieval."
Maezawa A.; Itoyama K.; Yoshii K.; Okuno H.G.,Bayesian audio alignment based on a unified generative model of music composition and performance,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977918138&partnerID=40&md5=de55ab115ee0fc785f8b50c9dc575233,"Maezawa A., Yamaha Corporation, Japan, Kyoto University, Japan; Itoyama K., Kyoto University, Japan; Yoshii K., Kyoto University, Japan; Okuno H.G., Waseda University, Japan","This paper presents a new probabilistic model that can align multiple performances of a particular piece of music. Conventionally, dynamic time warping (DTW) and left-to-right hidden Markov models (HMMs) have often been used for audio-to-audio alignment based on a shallow acoustic similarity between performances. Those methods, however, cannot distinguish latent musical structures common to all performances and temporal dynamics unique to each performance. To solve this problem, our model explicitly represents two state sequences: a top-level sequence that determines the common structure inherent in the music itself and a bottom-level sequence that determines the actual temporal fluctuation of each performance. These two sequences are fused into a hierarchical Bayesian HMM and can be learned at the same time from the given performances. Since the top-level sequence assigns the same state for note combinations that repeatedly appear within a piece of music, we can unveil the latent structure of the piece. Moreover, we can easily compare different performances of the same piece by analyzing the bottom-level sequences. Experimental evaluation showed that our method outperformed the conventional methods. © Akira Maezawa, Katsutoshi Itoyama, Kazuyoshi Yoshii, Hiroshi G. Okuno."
Bountouridis D.; Veltkamp R.C.; Van Balen J.,Placing music artists and songs in time using editorial metadata and web mining techniques,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069838031&partnerID=40&md5=3202a0fe39817ecfbc59e33da010f5ff,"Bountouridis D., Utrecht University, Department of Information and Computing Sciences, Netherlands; Veltkamp R.C., Utrecht University, Department of Information and Computing Sciences, Netherlands; Van Balen J., Utrecht University, Department of Information and Computing Sciences, Netherlands","This paper investigates the novel task of situating music artists and songs in time, thereby adding contextual information that typically correlates with an artist’s similarities, collaborations and influences. The proposed method makes use of editorial metadata in conjunction with web mining techniques, aiming to infer an artist’s productivity over time and estimate the original year of release of a song. Experimental evaluation over a set of Dutch and American music confirms the practicality and reliability of the proposed methods. As a consequence, large-scale correlational analyses between artist productivity and other musical characteristics (e.g. versatility, eminence) become possible. © 2013 International Society for Music Information Retrieval."
Sasaki S.; Yoshii K.; Nakano T.; Goto M.; Morishima S.,LyricSRadar: A lyrics retrieval system based on latent topics of lyrics,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951980403&partnerID=40&md5=3bf7ee27a185a20e0f5e6346599149e5,"Sasaki S., Waseda University, Japan; Yoshii K., Kyoto University, Japan; Nakano T., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Morishima S., Waseda University, Japan","This paper presents a lyrics retrieval system called LyricsRadar that enables users to interactively browse song lyrics by visualizing their topics. Since conventional lyrics retrieval systems are based on simple word search, those systems often fail to reflect user’s intention behind a query when a word given as a query can be used in different contexts. For example, the wordtearscan appear not only in sad songs (e.g., feel heartrending), but also in happy songs (e.g., weep for joy). To overcome this limitation, we propose to automatically analyze and visualize topics of lyrics by using a well-known text analysis method called latent Dirichlet allocation (LDA). This enables LyricsRadar to offer two types of topic visualization. One is the topic radar chart that visualizes the relative weights of five latent topics of each song on a pentagon-shaped chart. The other is radar-like arrangement of all songs in a two-dimensional space in which song lyrics having similar topics are arranged close to each other. The subjective experiments using 6,902 Japanese popular songs showed that our system can appropriately navigate users to lyrics of interests. © Shoto Sasaki, Kazuyoshi Yoshii, Tomoyasu Nakano, Masataka Goto, Shigeo Morishima."
Kell T.; Tzanetakis G.,Empirical analysis of track selection and ordering in electronic dance music using audio feature extraction,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003731489&partnerID=40&md5=acb565ec694e4f9e114374ecce5b0690,"Kell T., IDMIL, CIRMMT, McGill University, Canada; Tzanetakis G., University of Victoria, Canada","Disc jockeys are in some ways the ultimate experts at selecting and playing recorded music for an audience, especially in the context of dance music. In this work, we empirically investigate factors affecting track selection and ordering using DJ-created mixes of electronic dance music. We use automatic content-based analysis and discuss the implications of our findings to playlist generation and ordering. Timbre appears to be an important factor when selecting tracks and ordering tracks, and track order itself matters, as shown by statistically significant differences in the transitions between the original order and a shuffled version. We also apply this analysis to ordering heuristics and suggest that the standard playlist generation model of returning tracks in order of decreasing similarity to the initial track may not be optimal, at least in the context of track ordering for electronic dance music. © 2013 International Society for Music Information Retrieval."
Fourer D.; Rouas J.-L.; Hanna P.; Robine M.,Automatic timbre classification of ethnomusicological audio recordings,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985039648&partnerID=40&md5=2a0af71e1619908927ca1d619badab1d,"Fourer D., LaBRI, CNRS UMR 5800, University of Bordeaux, France; Rouas J.-L., LaBRI, CNRS UMR 5800, University of Bordeaux, France; Hanna P., LaBRI, CNRS UMR 5800, University of Bordeaux, France; Robine M., LaBRI, CNRS UMR 5800, University of Bordeaux, France","Automatic timbre characterization of audio signals can help to measure similarities between sounds and is of interest for automatic or semi-automatic databases indexing. The most effective methods use machine learning approaches which require qualitative and diversified training databases to obtain accurate results. In this paper, we introduce a diversified database composed of worldwide non-western instruments audio recordings on which is evaluated an effective timbre classification method. A comparative evaluation based on the well studied Iowa musical instruments database shows results comparable with those of state-of-the-art methods. Thus, the proposed method offers a practical solution for automatic ethnomusicological indexing of a database composed of diversified sounds with various quality. The relevance of audio features for the timbre characterization is also discussed in the context of non-western instruments analysis. © Dominique Fourer, Jean-Luc Rouas, Pierre Hanna, Matthias Robine."
McKay C.,JProductionCritic: An educational tool for detecting technical errors in audio mixes,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069875722&partnerID=40&md5=84655fe6122d871e57eda610575c14c4,"McKay C., Marianopolis College, CIRMMT, Canada","jProductionCritic is an open-source educational framework for automatically detecting technical recording, editing and mixing problems in audio files. It is intended to be used as a learning and proofreading tool by students and amateur producers, and can also assist teachers as a timesaving tool when grading recordings. A number of novel error detection algorithms are implemented by jProductionCritic. Problems detected include edit errors, clipping, noise infiltration, poor use of dynamics, poor track balancing, and many others. The error detection algorithms are highly configurable, in order to meet the varying aesthetics of different musical genres (e.g. Baroque vs. noise music). Effective general-purpose default settings were developed based on experiments with a variety of student pieces, and these settings were then validated using a reserved set of student pieces. jProductionCritic is also designed to serve as an extensible framework to which new detection modules can be easily plugged in. It is hoped that this will help to galvanize MIR research relating to audio production, an area that is currently underrepresented in the MIR literature, and that this work will also help to address the current general lack of educational production software. © 2013 International Society for Music Information Retrieval."
Farrahi K.; Schedl M.; Vall A.; Hauger D.; Tkalčič M.,Impact of listening behavior on music recommendation,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964729952&partnerID=40&md5=f7eb0cb5d24d2086f36dd36d536df8a4,"Farrahi K., Goldsmiths, University of London London, United Kingdom; Schedl M., Johannes Kepler University Linz, Austria; Vall A., Johannes Kepler University Linz, Austria; Hauger D., Johannes Kepler University Linz, Austria; Tkalčič M., Johannes Kepler University Linz, Austria","The next generation of music recommendation systems will be increasingly intelligent and likely take into account user behavior for more personalized recommendations. In this work we consider user behavior when making recommendations with features extracted from a user’s history of listening events. We investigate the impact of listener’s behavior by considering features such as play counts, “mainstreaminess”, and diversity in music taste on the performance of various music recommendation approaches. The underlying dataset has been collected by crawling social media (specifically Twitter) for listening events. Each user’s listening behavior is characterized into a three dimensional feature space consisting of play count, “mainstreaminess” (i.e. the degree to which the observed user listens to currently popular artists), and diversity (i.e. the diversity of genres the observed user listens to). Drawing subsets of the 28,000 users in our dataset, according to these three dimensions, we evaluate whether these dimensions influence figures of merit of various music recommendation approaches, in particular, collaborative filtering (CF) and CF enhanced by cultural information such as users located in the same city or country. © 2014 International Society for Music Information Retrieval."
Choudhury M.; Bhagwan R.; Bali K.,The use of melodic scales in bollywood music: An empirical study,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069854807&partnerID=40&md5=257cb203d1ca6e8943c0578dad9b8890,"Choudhury M., Microsoft Research Lab, India; Bhagwan R., Microsoft Research Lab, India; Bali K., Microsoft Research Lab, India","Hindi film music, which is commonly referred to as “Bollywood” music, is one of the most popular forms of music in the world today. One of the reasons for its popularity has been the willingness of Bollywood composers to adopt and be influenced by various musical forms including Western pop, jazz, rock, and classical music. However, till date, we are unaware of any systematic quantitative analysis of how this genre has changed and evolved over the years since its inception in the early 20th century. In this paper, we study the evolution of Bollywood music with respect to the use of melodic scales. We analyse songs composed over seven decades using a database of top-lists, which reveals many interesting patterns. We also analyze the scale usage patterns in the music of some of the most popular composers, which clearly brings out certain idiosyncrasies and preferences of each of them. © 2013 International Society for Music Information Retrieval."
Nakamura E.; Ono N.; Sagayama S.,Merged-output HMM for piano fingering of both hands,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961808605&partnerID=40&md5=94c7142c365c38438b71ab2a08388a0f,"Nakamura E., National Institute of Informatics, Tokyo, 101-8430, Japan; Ono N., National Institute of Informatics, Tokyo, 101-8430, Japan; Sagayama S., Meiji University, Tokyo, 164-8525, Japan","This paper discusses a piano fingering model for both hands and its applications. One of our motivations behind the study is automating piano reduction from ensemble scores. For this, quantifying the difficulty of piano performance is important where a fingering model of both hands should be relevant. Such a fingering model is proposed that is based on merged-output hidden Markov model and can be applied to scores in which the voice part for each hand is not indicated. The model is applied for decision of fingering for both hands and voice-part separation, automation of which is itself of great use and were previously difficult. A measure of difficulty of performance based on the fingering model is also proposed and yields reasonable results. © Eita Nakamura, Nobutaka Ono, Shigeki Sagayama."
Nakamura T.; Shikata K.; Takamune N.; Kameoka H.,Harmonic-temporal factor decomposition incorporating music prior information for informed monaural source separation,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973404822&partnerID=40&md5=e591d5269efe9cc90375685d9efec5c5,"Nakamura T., Graduate School of Information Science and Technology, University of Tokyo, Japan; Shikata K., Graduate School of Information Science and Technology, University of Tokyo, Japan; Takamune N., Graduate School of Information Science and Technology, University of Tokyo, Japan; Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Japan, NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, Japan","For monaural source separation two main approaches have thus far been adopted. One approach involves applying non-negative matrix factorization (NMF) to an observed magnitude spectrogram, interpreted as a non-negative matrix. The other approach is based on the concept of computational auditory scene analysis (CASA). A CASA-based approach called the “harmonic-temporal clustering (HTC)” aims to cluster the time-frequency components of an observed signal based on a constraint designed according to the local time-frequency structure common in many sound sources (such as harmonicity and the continuity of frequency and amplitude modulations). This paper proposes a new approach for monaural source separation called the “Harmonic-Temporal Factor Decomposition (HTFD)” by introducing a spectrogram model that combines the features of the models employed in the NMF and HTC approaches. We further describe some ideas how to design the prior distributions for the present model to incorporate musically relevant information into the separation scheme. © Tomohiko Nakamura, Kotaro Shikata, Norihiro Takamune, Hirokazu Kameoka."
Schoeffler M.; Stöter F.-R.; Bayerlein H.; Edler B.; Herre J.,An experiment about estimating the number of instruments in polyphonic music: A comparison between internet and laboratory results,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041551313&partnerID=40&md5=639bdd151f908f99b4643220086515f0,"Schoeffler M., International Audio Laboratories, Erlangen, Germany; Stöter F.-R., International Audio Laboratories, Erlangen, Germany; Bayerlein H., International Audio Laboratories, Erlangen, Germany; Edler B., International Audio Laboratories, Erlangen, Germany; Herre J., International Audio Laboratories, Erlangen, Germany","Internet experiments in the fields of music perception and music information retrieval are becoming more and more popular. However, not many Internet experiments are compared to laboratory experiments, the consequence being that the effect of the uncontrolled Internet environment on the results is unknown. In this paper the results of an Internet experiment with 1168 participants are compared to those of the same experiment with 62 participants but previously conducted in a controlled environment. The comparison of the Internet and laboratory results enabled us to make a point on whether the Internet can be used for our experiment procedure. The experiment aimed to investigate the listeners ability to correctly estimate the number of instruments being played back in a given excerpt of music. The participants listened to twelve short classical and pop music excerpts each composed using one to six instruments. For each music excerpt the participants were asked how many instruments they could hear and how certain they were about their estimation. © 2013 International Society for Music Information Retrieval."
Liang D.; Paisley J.; Ellis D.P.W.,Codebook-based scalable music tagging with poisson matrix factorization,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991009696&partnerID=40&md5=b883678d19619145fd29dbe1dee10dea,"Liang D., Department of Electrical Engineering, Columbia University, United States; Paisley J., Department of Electrical Engineering, Columbia University, United States; Ellis D.P.W., Department of Electrical Engineering, Columbia University, United States","Automatic music tagging is an important but challenging problem within MIR. In this paper, we treat music tagging as a matrix completion problem. We apply the Poisson matrix factorization model jointly on the vector-quantized audio features and a “bag-of-tags” representation. This approach exploits the shared latent structure between semantic tags and acoustic codewords. Leveraging the recently-developed technique of stochastic variational inference, the model can tractably analyze massive music collections. We present experimental results on the CAL500 dataset and the Million Song Dataset for both annotation and retrieval tasks, illustrating the steady improvement in performance as more data is used. © Dawen Liang, John Paisley, Daniel P. W. Ellis."
Srinivasamurthy A.; Repetto R.C.; Sundar H.; Serra X.,Transcription and recognition of syllable based percussion patterns: The case of beijing opera,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994535270&partnerID=40&md5=15630e0e139be078a59fc0aa0276354e,"Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sundar H., Speech and Audio Group, Indian Institute of Science, Bangalore, India; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In many cultures of the world, traditional percussion music uses mnemonic syllables that are representative of the timbres of instruments. These syllables are orally transmitted and often provide a language for percussion in those music cultures. Percussion patterns in these cultures thus have a well defined representation in the form of these syllables, which can be utilized in several computational percussion pattern analysis tasks. We explore a connected word speech recognition based framework that can effectively utilize the syllabic representation for automatic transcription and recognition of audio percussion patterns. In particular, we consider the case of Beijing opera and present a syllable level hidden markov model (HMM) based system for transcription and classification of percussion patterns. The encouraging classification results on a representative dataset of Beijing opera percussion patterns supports our approach and provides further insights on the utility of these syllables for computational description of percussion patterns. © Ajay Srinivasamurthy, Rafael Caro Repetto, Harshavardhan Sundar, Xavier Serra."
Schlüter J.,Learning binary codes for efficient large-scale music similarity search,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026602507&partnerID=40&md5=d2a37fed701a6f97f4603fc361524e0e,"Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Content-based music similarity estimation provides a way to find songs in the unpopular “long tail” of commercial catalogs. However, state-of-the-art music similarity measures are too slow to apply to large databases, as they are based on finding nearest neighbors among very high-dimensional or non-vector song representations that are difficult to index. In this work, we adopt recent machine learning methods to map such song representations to binary codes. A linear scan over the codes quickly finds a small set of likely neighbors for a query to be refined with the original expensive similarity measure. Although search costs grow linearly with the collection size, we show that for commercial-scale databases and two state-of-the-art similarity measures, this outperforms five previous attempts at approximate nearest neighbor search. When required to return 90% of true nearest neighbors, our method is expected to answer 4.2 1-NN queries or 1.3 50-NN queries per second on a collection of 30 million songs using a single CPU core; an up to 260 fold speedup over a full scan of 90% of the database. © 2013 International Society for Music Information Retrieval."
Wang S.; Ewert S.; Dixon S.,Robust joint alignment of multiple versions of a piece of music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973284602&partnerID=40&md5=9bc5a54b36ef1b6aec9e43d8eae5df6c,"Wang S., Queen Mary University of London, United Kingdom; Ewert S., Queen Mary University of London, United Kingdom; Dixon S., Queen Mary University of London, United Kingdom","Large music content libraries often comprise multiple versions of a piece of music. To establish a link between different versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Due to the leeway in interpreting a piece, any two versions can differ significantly, for example, in terms of local tempo, articulation, or playing style. For a given pair of versions, these differences can be significant such that even state-of-the-art methods fail to identify a correct alignment. In this paper, we present a novel method that increases the robustness for difficult to align cases. Instead of aligning only pairs of versions as done in previous methods, our method aligns multiple versions in a joint manner. This way, the alignment can be computed by comparing each version not only with one but with several versions, which stabilizes the comparison and leads to an increase in alignment robustness. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state-of-the-art method, with significantly less outliers (standard deviation 53% lower). © Siying Wang, Sebastian Ewert, Simon Dixon."
Liu I.-T.; Lin Y.-T.; Wu J.-L.,Music cut and paste: A personalized musical medley generating system,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044504696&partnerID=40&md5=43cbc8b8ca6e177d9fe370e11438be47,"Liu I.-T., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Lin Y.-T., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Wu J.-L., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","A musical medley is a piece of music that is composed of parts of existing pieces. Manually creating medley is time consuming because it is not easy to find out proper clips to put in succession and seamlessly connect them. In this work, we propose a framework for creating personalized music medleys from users’ music collection. Unlike existing similar works in which only low-level features are used to select candidate clips and locate possible transition points among clips, we take song structures and song phrasing into account during medley creation. Inspired by the musical dice game, we treat the medley generation process as an audio version of musical dice game. That is, once the analysis on the songs of user collection has been done, the system is able to generate various medleys with different probabilities. This flexibility brings us the ability to create medleys according to the user-specified conditions, such as the medley structure or some must-use clips. The preliminary subjective evaluations showed that the proposed system is effective in selecting connectable clips that preserved chord progression structure. Besides, connecting the clips at phrase boundaries acquired more user preference than previous works did. © 2013 International Society for Music Information Retrieval."
Pauwels J.; Kaiser F.; Peeters G.,Combining harmony-based and novelty-based approaches for structural segmentation,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052779227&partnerID=40&md5=bcccdb3e45849af7226e6b5b306e58e9,"Pauwels J., STMS IRCAM-CNRS-UPMC, France; Kaiser F., STMS IRCAM-CNRS-UPMC, France; Peeters G., STMS IRCAM-CNRS-UPMC, France","This paper describes a novel way to combine a well-proven method of structural segmentation through novelty detection with a recently introduced method based on harmonic analysis. The former system works by looking for peaks in novelty curves derived from self-similarity matrices. The latter relies on the detection of key changes and on the differences in prior probability of chord transitions according to their position in a structural segment. Both approaches are integrated into a probabilistic system that jointly estimates keys, chords and structural boundaries. The novelty curves are herein used as observations. In addition, chroma profiles are used as features for the harmony analysis. These observations are then subjected to a constrained transition model that is musically motivated. An information theoretic justification of this model is also given. Finally, an evaluation of the resulting system is performed. It is shown that the combined system improves the results of both constituting components in isolation. © 2013 International Society for Music Information Retrieval."
Cherla S.; Weyde T.; d’Avila Garcez A.,Multiple viewpoint melodic prediction with fixed-context neural networks,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975204842&partnerID=40&md5=f80e30ed4e93020115cc5c22bf4d6600,"Cherla S., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; d’Avila Garcez A., Machine Learning Group, Department of Computer Science, City University London, United Kingdom","The multiple viewpoints representation is an event-based representation of symbolic music data which offers a means for the analysis and generation of notated music. Previous work using this representation has predominantly relied on n-gram and variable order Markov models for music sequence modelling. Recently the efficacy of a class of distributed models, namely restricted Boltzmann machines, was demonstrated for this purpose. In this paper, we demonstrate the use of two neural network models which use fixed-length sequences of various viewpoint types as input to predict the pitch of the next note in the sequence. The predictive performance of each of these models is comparable to that of models previously evaluated on the same task. We then combine the predictions of individual models using an entropy-weighted combination scheme to improve the overall prediction performance, and compare this with the predictions of a single equivalent model which takes as input all the viewpoint types of each of the individual models in the combination. © Srikanth Cherla, Tillman Weyde and Artur d’Avila Garcez."
Lartillot O.,In-depth motivic analysis based on multiparametric closed pattern and cyclic sequence mining,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956821076&partnerID=40&md5=c1c46fe66f2eec73ed779f5173d97898,"Lartillot O., Aalborg University, Department of Architecture, Design and Media Technology, Denmark","The paper describes a computational system for exhaustive but compact description of repeated motivic patterns in symbolic representations of music. The approach follows a method based on closed heterogeneous pattern mining in multiparametrical space with control of pattern cyclicity. This paper presents a much simpler description and justification of this general strategy, as well as significant simplifications of the model, in particular concerning the management of pattern cyclicity. A new method for automated bundling of patterns belonging to same motivic or thematic classes is also presented. The good performance of the method is shown through the analysis of a piece from the JKUPDD database. Ground-truth motives are detected, while additional relevant information completes the ground-truth musicological analysis. The system, implemented in Matlab, is made publicly available as part of MiningSuite, a new open-source framework for audio and music analysis. © Olivier Lartillot."
Boulanger-Lewandowski N.; Bengio Y.; Vincent P.,Audio chord recognition with recurrent neural networks,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054275611&partnerID=40&md5=34ecf2cb2a67b5c045e4a0b8cdbd1857,"Boulanger-Lewandowski N., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada; Bengio Y., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada; Vincent P., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada","In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task. © 2013 International Society for Music Information Retrieval."
Yakar T.B.; Litman R.; Sprechmann P.; Bronstein A.; Sapiro G.,Bilevel sparse models for polyphonic music transcription,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017397082&partnerID=40&md5=98ca79f75c5ae249532b96e594fbb19d,"Yakar T.B., Tel Aviv University, Israel; Litman R., Tel Aviv University, Israel; Sprechmann P., Duke University, United States; Bronstein A., Tel Aviv University, Israel; Sapiro G., Duke University, United States","In this work, we propose a trainable sparse model for automatic polyphonic music transcription, which incorporates several successful approaches into a unified optimization framework. Our model combines unsupervised synthesis models similar to latent component analysis and nonnegative factorization with metric learning techniques that allow supervised discriminative learning. We develop efficient stochastic gradient training schemes allowing unsupervised, semi-, and fully supervised training of the model as well its adaptation to test data. We show efficient fixed complexity and latency approximation that can replace iterative minimization algorithms in time-critical applications. Experimental evaluation on synthetic and real data shows promising initial results. © 2013 International Society for Music Information Retrieval."
Ullrich K.; Schlüter J.; Grill T.,Boundary detection in music structure analysis using convolutional neural networks,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007420949&partnerID=40&md5=7dea69f019e885b872f394ff8b933dba,"Ullrich K., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Grill T., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","The recognition of boundaries, e.g., between chorus and verse, is an important task in music structure analysis. The goal is to automatically detect such boundaries in audio signals so that the results are close to human annotation. In this work, we apply Convolutional Neural Networks to the task, trained directly on mel-scaled magnitude spectrograms. On a representative subset of the SALAMI structural annotation dataset, our method outperforms current techniques in terms of boundary retrieval F-measure at different temporal tolerances: We advance the state-of-the-art from 0.33 to 0.46 for tolerances of ±0.5 seconds, and from 0.52 to 0.62 for tolerances of ±3 seconds. As the algorithm is trained on annotated audio data without the need of expert knowledge, we expect it to be easily adaptable to changed annotation guidelines and also to related tasks such as the detection of song transitions. © Karen Ullrich, Jan Schlüter, and Thomas Grill."
Sébastien V.; Sébastien D.; Conruyt N.,Annotating works for music education: Propositions for a musical forms and structures ontology and a musical performance ontology,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069854332&partnerID=40&md5=d24ae64f791e1e2da69d598a962e501f,"Sébastien V., IREMIA, Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Réunion, France; Sébastien D., IREMIA, Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Réunion, France; Conruyt N., IREMIA, Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Réunion, France","Web applications and mobile tablets are changing the way musicians practice their instrument. Now, they can access instantaneously thousands of musical scores online and play them while watching their tablet, put on their music stand. However musicians may have difficulties in getting appropriate tips and advice to play the chosen piece correctly. This is why we conceived a collaborative platform to annotate digital scores on tablets in previous work. However, we noticed that the current Music Ontology (MO) do not allow to tag these annotations appropriately. Thus, we present in this paper a proposition for a Musical Forms and Structures Ontology (MFSO) and a Musical Performance Ontology (MPO) based on music practice. A construction methodology and a model are first detailed. Then, a practical use case is presented. Lastly, inherent theoretical and practical difficulties encountered during the ontology framework’s conception are discussed. © 2013 International Society for Music Information Retrieval."
Kaiser F.; Peeters G.,A simple fusion method of state and sequence segmentation for music structure discovery,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009854728&partnerID=40&md5=15459ec8fb75cc5026a54891db9b875d,"Kaiser F., STMS IRCAM-CNRS-UPMC, 1 Place Igor Stravinsky, Paris, 75004, France; Peeters G., STMS IRCAM-CNRS-UPMC, 1 Place Igor Stravinsky, Paris, 75004, France","Methods for music structure segmentation are based on strong assumptions on the acoustical properties of structural segments. These assumptions relate to the novelty, homogeneity, repetition and/or regularity of the content. Each of these assumptions provide a different perspective on the music piece. These assumptions are however often considered separately in the methods. In this paper we propose a method for estimating the music structure segmentation based on the fusion of the novelty and repetition assumptions. This combination of different perspectives on the music pieces allows to generate more coherent acoustic segments and strongly improves the final music structure segmentation’s performance. © 2013 International Society for Music Information Retrieval."
Vigliensoni G.; Burlet G.; Fujinaga I.,Optical measure recognition in common music notation,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985933779&partnerID=40&md5=22e9d0513a85d05084d03b647689652d,"Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Burlet G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","This paper presents work on the automatic recognition of measures in common Western music notation scores using optical music recognition techniques. It is important to extract the bounding boxes of measures within a music score to facilitate some methods of multimodal navigation of music catalogues. We present an image processing algorithm that extracts the position of barlines on an input music score in order to deduce the number and position of measures on the page. An open-source implementation of this algorithm is made publicly available. In addition, we have created a ground-truth dataset of 100 images of music scores with manually annotated measures. We conducted several experiments using different combinations of values for two critical parameters to evaluate our measure recognition algorithm. Our algorithm obtained an f-score of 91 percent with the optimal set of parameters. Although our implementation obtained results similar to previous approaches, the scope and size of the evaluation dataset is significantly larger. © 2013 International Society for Music Information Retrieval."
Holzapfel A.; Krebs F.; Srinivasamurthy A.,Tracking the “odd”: Meter inference in a culturally diverse music corpus,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973295148&partnerID=40&md5=3db702b9eb0cfb87815917860577d175,"Holzapfel A., New York University Abu Dhabi, United Arab Emirates; Krebs F., Johannes Kepler University, Austria; Srinivasamurthy A., Universitat Pompeu Fabra, Spain","In this paper, we approach the tasks of beat tracking, downbeat recognition and rhythmic style classification in non-Western music. Our approach is based on a Bayesian model, which infers tempo, downbeats and rhythmic style, from an audio signal. The model can be automatically adapted to rhythmic styles and time signatures. For evaluation, we compiled and annotated a music corpus consisting of eight rhythmic styles from three cultures, containing a variety of meter types. We demonstrate that by adapting the model to specific styles, we can track beats and downbeats in odd meter types like 9/8 or 7/8 with an accuracy significantly improved over the state of the art. Even if the rhythmic style is not known in advance, a unified model is able to recognize the meter and track the beat with comparable results, providing a novel method for inferring the metrical structure in culturally diverse datasets. © Andre Holzapfel, Florian Krebs, Ajay Srinivasamurthy."
Saari P.; Eerola T.; Fazekas G.; Barthet M.; Lartillot O.; Sandler M.,The role of audio and tags in music mood prediction: A study using semantic layer projection,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044951126&partnerID=40&md5=458c170c3fdd0904534a8d8ce20942de,"Saari P., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Barthet M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","Semantic Layer Projection (SLP) is a method for automatically annotating music tracks according to expressed mood based on audio. We evaluate this method by comparing it to a system that infers the mood of a given track using associated tags only. SLP differs from conventional auto-tagging algorithms in that it maps audio features to a low-dimensional semantic layer congruent with the circumplex model of emotion, rather than training a model for each tag separately. We build the semantic layer using two large-scale data sets – crowd-sourced tags from Last.fm, and editorial annotations from the I Like Music (ILM) production music corpus – and use subsets of these corpora to train SLP for mapping audio features to the semantic layer. The performance of the system is assessed in predicting mood ratings on continuous scales in the two data sets mentioned above. The results show that audio is in general more efficient in predicting perceived mood than tags. Furthermore, we analytically demonstrate the benefit of using a combination of semantic tags and audio features in automatic mood annotation. © 2013 International Society for Music Information Retrieval."
Sarala P.; Murthy H.A.,Inter and intra item segmentation of continuous audio recordings of carnatic music for archival,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039931930&partnerID=40&md5=a7a54a295ecd1e48a2a286b100c950bc,"Sarala P., Computer Science and Engineering, Indian Institute of Technology, Madras, India; Murthy H.A., Computer Science and Engineering, Indian Institute of Technology, Madras, India","The purpose of this paper is to segment carnatic music recordings into individual items for archival purposes using applauses. A concert in carnatic music is replete with applauses. These applauses may be inter-item or intra-item applauses. A property of an item in carnatic music, is that within every item, a small portion of the audio corresponds to the rendering of a composition which is rendered by the entire ensemble of lead performer and accompanying instruments. A concert is divided into segments using applauses and the location of the ensemble in every item is first obtained using Cent Filterbank Cepstral Coefficients (CFCC) combined with Gaussian Mixture Models (GMMs). Since constituent parts of an item are rendered in a single raga, raga information is used to merge adjacent segments belonging to the same item. Inter-item applauses are used to locate the end of an item in a concert. The results are evaluated for fifty live recordings with 990 applauses in total. The classification accuracy for inter and intra item applauses is 93%. Given a song list and the audio, the song list is mapped to the segmented audio of items, which are then stored in the database. © 2013 International Society for Music Information Retrieval."
De Man B.; Leonard B.; King R.; Reiss J.D.,An analysis and evaluation of audio features for multitrack music mixtures,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983096463&partnerID=40&md5=0db1bb5ba861aad9bb7fea8ee2733607,"De Man B., Centre for Digital Music, Queen Mary University of London, United Kingdom; Leonard B., Graduate Program in Sound Recording, Schulich School of Music, McGill University, Canada, Centre for Interdisciplinary Research in Music Media and Technology, Canada; King R., Graduate Program in Sound Recording, Schulich School of Music, McGill University, Canada, Centre for Interdisciplinary Research in Music Media and Technology, Canada; Reiss J.D., Centre for Digital Music, Queen Mary University of London, United Kingdom","Mixing multitrack music is an expert task where characteristics of the individual elements and their sum are manipulated in terms of balance, timbre and positioning, to resolve technical issues and to meet the creative vision of the artist or engineer. In this paper we conduct a mixing experiment where eight songs are each mixed by eight different engineers. We consider a range of features describing the dynamic, spatial and spectral characteristics of each track, and perform a multidimensional analysis of variance to assess whether the instrument, song and/or engineer is the determining factor that explains the resulting variance, trend, or consistency in mixing methodology. A number of assumed mixing rules from literature are discussed in the light of this data, and implications regarding the automation of various mixing processes are explored. Part of the data used in this work is published in a new online multitrack dataset through which public domain recordings, mixes, and mix settings (DAW projects) can be shared. © Brecht De Man, Brett Leonard, Richard King and Joshua D. Reiss."
Burlet G.; Fujinaga I.,Robotaba guitar tablature transcription framework,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013648374&partnerID=40&md5=d25e431320f305c06e3a671626e10952,"Burlet G., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada","This paper presents Robotaba, a web-based guitar tablature transcription framework. The framework facilitates the creation of web applications in which polyphonic transcription and guitar tablature arrangement algorithms can be embedded. Such a web application is implemented, and consists of an existing polyphonic transcription algorithm and a new guitar tablature arrangement algorithm. The result is a unified system that is capable of transcribing guitar tablature from a digital audio recording and displaying the resulting tablature in the web browser. Additionally, two ground-truth datasets for polyphonic transcription and guitar tablature arrangement are compiled from manual transcriptions gathered from the tablature website ultimate-guitar.com. The implemented transcription web application is evaluated on the compiled ground-truth datasets using several metrics. © 2013 International Society for Music Information Retrieval."
van Herwaarden S.; Grachten M.; Bas de Haas W.,Predicting expressive dynamics in piano performances using neural networks,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989828176&partnerID=40&md5=ee92f1698110bbd3260abcefe4d1db4d,"van Herwaarden S., Austrian Research Institute for AI, Austria; Grachten M., Austrian Research Institute for AI, Austria; Bas de Haas W., Utrecht University, Netherlands","This paper presents a model for predicting expressive accentuation in piano performances with neural networks. Using Restricted Boltzmann Machines (RBMs), features are learned from performance data, after which these features are used to predict performed loudness. During feature learning, data describing more than 6000 musical pieces is used; when training for prediction, two datasets are used, both recorded on a Bösendorfer piano (accurately measuring note on- and offset times and velocity values), but describing different compositions performed by different pianists. The resulting model is tested by predicting note velocity for unseen performances. Our approach differs from earlier work in a number of ways: (1) an additional input representation based on a local history of velocity values is used, (2) the RBMs are trained to result in a network with sparse activations, (3) network connectivity is increased by adding skip-connections, and (4) more data is used for training. These modifications result in a network performing better than the state-of-the-art on the same data and more descriptive features, which can be used for rendering performances, or for gaining insight into which aspects of a musical piece influence its performance. © S. van Herwaarden, M. Grachten, W.B. de Haas."
Kroher N.; Gómez E.; Guastavino C.; Gómez F.; Bonada J.,Computational models for perceived melodic similarity in a cappella flamenco singing,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971242167&partnerID=40&md5=1ac8c98ac4b5b67b95d3528deb097c35,"Kroher N., Universitat Pompeu Fabra, Spain; Gómez E., Universitat Pompeu Fabra, Spain; Guastavino C., McGill University, CIRMMT, Canada; Gómez F., Technical University of Madrid, Spain; Bonada J., Universitat Pompeu Fabra, Spain","The present study investigates the mechanisms involved in the perception of melodic similarity in the context of a cappella flamenco singing performances. Flamenco songs belonging to the same style are characterized by a common melodic skeleton, which is subject to spontaneous improvisation containing strong prolongations and ornamen-tations. For our research we collected human similarity judgements from naïve and expert listeners who listened to audio recordings of a cappella flamenco performances as well as synthesized versions of the same songs. We furthermore calculated distances from manually extracted high-level descriptors defined by flamenco experts. The suitability of a set of computational melodic similarity measures was evaluated by analyzing the correlation between computed similarity and human ratings. We observed significant differences between listener groups and stimuli types. Furthermore, we observed a high correlation between human ratings and similarities computed from features from flamenco experts. We also observed that computational models based on temporal deviation, dynamics and ornamentation are better suited to model perceived similarity for this material than models based on chroma distance. © N. Kroher, E. Gómez, C. Guastavino, F. Gómez, J. Bonada."
Sigtia S.; Benetos E.; Cherla S.; Weyde T.; d’Avila Garcez A.S.; Dixon S.,An RNN-based music language model for improving automatic music transcription,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946061003&partnerID=40&md5=4fb8f97d2836888b7572230f0139058a,"Sigtia S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Department of Computer Science, City University, London, United Kingdom; Cherla S., Department of Computer Science, City University, London, United Kingdom; Weyde T., Department of Computer Science, City University, London, United Kingdom; d’Avila Garcez A.S., Department of Computer Science, City University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a significant 3% improvement in terms of F-measure, when compared to using an acoustic-only model. © S. Sigtia, E. Benetos, S. Cherla, T. Weyde, A. S. d’Avila Garcez, and S. Dixon."
Nieto O.; Farbood M.M.,Identifying polyphonic patterns from audio recordings using music segmentation techniques,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946037794&partnerID=40&md5=d497730b5959132238f190ed99af017a,"Nieto O., Music and Audio Research Lab, New York University, United States; Farbood M.M., Music and Audio Research Lab, New York University, United States","This paper presents a method for discovering patterns of note collections that repeatedly occur in a piece of music. We assume occurrences of these patterns must appear at least twice across a musical work and that they may contain slight differences in harmony, timbre, or rhythm. We describe an algorithm that makes use of techniques from the music information retrieval task of music segmentation, which exploits repetitive features in order to automatically identify polyphonic musical patterns from audio recordings. The novel algorithm is assessed using the recently published JKU Patterns Development Dataset, and we show how it obtains state-of-the-art results employing the standard evaluation metrics. © Oriol Nieto, Morwaread M. Farbood."
Khadkevich M.; Omologo M.,Large-scale cover song identification using chord profiles,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040692081&partnerID=40&md5=320134ee1642730f02b5bf184f44887e,"Khadkevich M., Fondazione Bruno Kessler-irst, via Sommarive 18, Povo, 38050, Italy; Omologo M., Fondazione Bruno Kessler-irst, via Sommarive 18, Povo, 38050, Italy","This paper focuses on cover song identification among datasets potentially containing millions of songs. A compact representation of music contents plays an important role in large-scale analysis and retrieval. The proposed approach is based on high-level summarization of musical songs using chord profiles. Search is performed in two steps. In the first step, the Locality Sensitive Hashing (LHS) method is used to retrieve songs with similar chord profiles. On the resulting list of songs a second processing step is applied to progressively refine the ranking. Experiments conducted on both the Million Song Dataset (MSD) and a subset of the Second Hand Songs (SHS) dataset showed the effectiveness of the proposed solution, which provides state-of-the-art results. © 2013 International Society for Music Information Retrieval."
Böck S.; Widmer G.,Local group delay based vibrato and tremolo suppression for onset detection,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069847676&partnerID=40&md5=dbe86df6074f82fc047bdc9f942e6484,"Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper we present a new vibrato and tremolo suppression technique for onset detection. It weights the differences of the magnitude spectrogram used for the calculation of the spectral flux onset detection function on the basis of the local group delay information. With this weighting technique applied, the onset detection function is able to reliably distinguish between genuine onsets and spectral energy peaks originating from vibrato or tremolo present in the signal and lowers the number of false positive detections considerably. Especially in cases of music with numerous vibratos and tremolos (e.g. opera singing or string performances) the number of false positive detections can be reduced by up to 50% without missing any additional events. Performance is evaluated and compared to current state-of-the-art algorithms using three different datasets comprising mixed audio material (25,927 onsets), violin recordings (7,677 onsets) and solo voice recordings of operas (1,448 onsets). © 2013 International Society for Music Information Retrieval."
Bonnin G.; Jannach D.,Evaluating the quality of playlists based on hand-crafted samples,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032556444&partnerID=40&md5=9217cead3b186d7543bfa64e65c8e956,"Bonnin G., TU Dortmund, Germany; Jannach D., TU Dortmund, Germany","The automated generation of playlists represents a particular type of the music recommendation problem with two special characteristics. First, the tracks of the list are usually consumed immediately at recommendation time; second, tracks are listened to mostly in consecutive order so that the sequence of the recommended tracks can be relevant. A number of different approaches for playlist generation have been proposed in the literature. In this paper, we review the existing core approaches to playlist generation, discuss aspects of appropriate offline evaluation designs and report the results of a comparative evaluation based on different data sets. Based on the insights from these experiments, we propose a comparably simple and computationally tractable new baseline algorithm for future comparisons, which is based on track popularity and artist information and is competitive with more sophisticated techniques in our evaluation settings. © 2013 International Society for Music Information Retrieval."
Song Y.; Dixon S.; Pearce M.; Halpern A.,Do online social tags predict perceived or induced emotional responses to music?,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015144404&partnerID=40&md5=09e8d083479720a21bf2b4ac2865da63,"Song Y., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Pearce M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Halpern A., Department of Psychology, Bucknell University, United States","Music provides a powerful means of communication and self-expression. A wealth of research has been performed on the study of music and emotion, including emotion modelling and emotion classification. The emergence of online social tags (OST) has provided highly relevant information for the study of mood, as well as an important impetus for using discrete emotion terms in the study of continuous models of affect. Yet, the extent to which human annotation reveals either perceived emotion or induced emotion remains unknown. 80 musical excerpts were randomly selected from a collection of 2904 songs labelled with the Last.fm tags “happy”, “sad”, “angry” and “relax”. Forty-seven participants provided emotion ratings on the two continuous dimensions of valence and arousal for both perceived and induced emotion. Analysis of variance did not reveal significant differences in ratings between perceived emotion and induced emotion. Moreover, the results indicated that, regardless of the discrete type of emotion experienced, listeners’ ratings of perceived and induced emotion were highly positively correlated. Finally, the emotion tags “happy”, “sad” and “angry” but not “relax” predicted the corresponding experimentally provided emotion categories. © 2013 International Society for Music Information Retrieval."
Pesek M.; Godec P.; Poredoš M.; Strle G.; Guna J.; Stojmenova E.; Pogačnik M.; Marolt M.,Introducing a dataset of emotional and color responses to music,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946808365&partnerID=40&md5=18a5515197955b226b8a4eccb7225022,"Pesek M., University of Ljubljana, Faculty of Computer and Information Science, Slovenia; Godec P., University of Ljubljana, Faculty of Computer and Information Science, Slovenia; Poredoš M., University of Ljubljana, Faculty of Computer and Information Science, Slovenia; Strle G., Scientific Research Centre of the Slovenian Academy of Sciences and Arts, Institute of Ethnomusicology, Slovenia; Guna J., University of Ljubljana, Faculty of Electrotechnics, Slovenia; Stojmenova E., University of Ljubljana, Faculty of Electrotechnics, Slovenia; Pogačnik M., University of Ljubljana, Faculty of Electrotechnics, Slovenia; Marolt M., University of Ljubljana, Faculty of Computer and Information Science, Slovenia","The paper presents a new dataset of mood-dependent and color responses to music. The methodology of gath-ering user responses is described along with two new inter-faces for capturing emotional states: the MoodGraph and MoodStripe. An evaluation study showed both inter-faces have significant advantage over more traditional methods in terms of intuitiveness, usability and time complexity. The preliminary analysis of current data (over 6.000 responses) gives an interesting insight into participants’ emotional states and color associations, as well as relationships between musically perceived and induced emotions. We believe the size of the dataset, in-terfaces and multi-modal approach (connecting emo-tional, visual and auditory aspects of human perception) give a valuable contribution to current research. © Matevž Pesek, Primož Godec, Mojca Poredoš, Gregor Strle, Jože Guna, Emilija Stojmenova, Matevž Pogačnik, Matija Marolt."
Hamel P.; Davies M.E.P.; Yoshii K.; Goto M.,Transfer learning in MIR: Sharing learned latent representations for music audio classification and similarity,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069815617&partnerID=40&md5=edaea987d5938d17da4a489c70ed81a8,"Hamel P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Davies M.E.P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper discusses the concept of transfer learning and its potential applications to MIR tasks such as music audio classification and similarity. In a traditional supervised machine learning setting, a system can only use labeled data from a single dataset to solve a given task. The labels associated with the dataset define the nature of the task to solve. A key advantage of transfer learning is in leveraging knowledge from related tasks to improve performance on a given target task. One way to transfer knowledge is to learn a shared latent representation across related tasks. This method has shown to be beneficial in many domains of machine learning, but has yet to be explored in MIR. Many MIR datasets for audio classification present a semantic overlap in their labels. Furthermore, these datasets often contain relatively few songs. Thus, there is a strong case for exploring methods to share knowledge between these datasets towards a more general and robust understanding of high level musical concepts such as genre and similarity. Our results show that shared representations can improve classification accuracy. We also show how transfer learning can improve performance for music similarity. © 2013 International Society for Music Information Retrieval."
Madsen J.; Sand Jensen B.; Larsen J.,Modeling temporal structure in music for emotion prediction using pairwise comparisons,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994018694&partnerID=40&md5=e7066f98c453cabfb03a41d88579d048,"Madsen J., Technical University of Denmark, Department of Applied Mathematics and Computer Science, Richard Petersens Plads, Building 321, Kongens Lyngby, 2800, Denmark; Sand Jensen B., Technical University of Denmark, Department of Applied Mathematics and Computer Science, Richard Petersens Plads, Building 321, Kongens Lyngby, 2800, Denmark; Larsen J., Technical University of Denmark, Department of Applied Mathematics and Computer Science, Richard Petersens Plads, Building 321, Kongens Lyngby, 2800, Denmark","The temporal structure of music is essential for the cognitive processes related to the emotions expressed in music. However, such temporal information is often disregarded in typical Music Information Retrieval modeling tasks of predicting higher-level cognitive or semantic aspects of music such as emotions, genre, and similarity. This paper addresses the specific hypothesis whether temporal information is essential for predicting expressed emotions in music, as a prototypical example of a cognitive aspect of music. We propose to test this hypothesis using a novel processing pipeline: 1) Extracting audio features for each track resulting in a multivariate” feature time series”. 2) Using generative models to represent these time series (acquiring a complete track representation). Specifically, we explore the Gaussian Mixture model, Vector Quantization, Autore-gressive model, Markov and Hidden Markov models. 3) Utilizing the generative models in a discriminative setting by selecting the Probability Product Kernel as the natural kernel for all considered track representations. We evaluate the representations using a kernel based model specifically extended to support the robust two-alternative forced choice self-report paradigm, used for eliciting expressed emotions in music. The methods are evaluated using two data sets and show increased predictive performance using temporal information, thus supporting the overall hypothesis. © Jens Madsen, Bjørn Sand Jensen, Jan Larsen."
Vigliensoni G.; Burgoyne J.A.; Fujinaga I.,MusicBrainz for the world: The Chilean experience,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014780043&partnerID=40&md5=ffcb821f670b670c53194a2a46de4360,"Vigliensoni G., CIRMMT, McGill University, Canada; Burgoyne J.A., ILLC, University of Amsterdam, Netherlands; Fujinaga I., CIRMMT, McGill University, Canada","In this paper we present our research in gathering data from several semi-structured collections of cultural heritage—Chilean music-related websites—and uploading the data into an open-source music database, where the data can be easily searched, discovered, and interlinked. This paper also reviews the characteristics of four user-contributed, music metadatabases (MusicBrainz, Discogs, MusicMoz, and FreeDB), and explains why we chose MusicBrainz as the repository for our data. We also explain how we collected data from the five most important sources of Chilean music-related data, and we give details about the context, design, and results of an experiment for artist name comparison to verify which of the artists that we have in our database exist in the MusicBrainz database already. Although it represents a single case study, we believe this information will be of great help to other MIR researchers who are trying to design their own studies of world music. © 2013 International Society for Music Information Retrieval."
de Valk R.; Weyde T.; Benetos E.,A machine learning approach to voice separation in lute tablature,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985018300&partnerID=40&md5=de3cb2b8d545f9a4ce0e9268d245a37d,"de Valk R., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Benetos E., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","In this paper, we propose a machine learning model for voice separation in lute tablature. Lute tablature is a practical notation that reveals only very limited information about polyphonic structure. This has complicated research into the large surviving corpus of lute music, notated exclusively in tablature. A solution may be found in automatic transcription, of which voice separation is a necessary step. During the last decade, several methods for separating voices in symbolic polyphonic music formats have been developed. However, all but two of these methods adopt a rule-based approach; moreover, none of them is designed for tablature. Our method differs on both these points. First, rather than using fixed rules, we use a model that learns from data: a neural network that predicts voice assignments for notes. Second, our method is specifically designed for tablature—tablature information is included in the features used as input for the models—but it can also be applied to other music corpora. We have experimented on a dataset containing tablature pieces of different polyphonic textures, and compare the results against those obtained from a baseline hidden Markov model (HMM) model. Additionally, we have performed a preliminary comparison of the neural network model with several existing methods for voice separation on a small dataset. We have found that the neural network model performs clearly better than the baseline model, and competitively with the existing methods. © 2013 International Society for Music Information Retrieval."
Lee J.H.; Choi K.; Hu X.; Stephen Downie J.,K-pop genres: A cross-cultural exploration,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053061979&partnerID=40&md5=e756ea6d947eedf6f973d9a218c12c22,"Lee J.H., University of Washington, United States; Choi K., University of Illinois, United States; Hu X., University of Hong Kong, Hong Kong; Stephen Downie J., University of Illinois, United States","Current music genre research tends to focus heavily on classical and popular music from Western cultures. Few studies discuss the particular challenges and issues related to non-Western music. The objective of this study is to improve our understanding of how genres are used and perceived in different cultures. In particular, this study attempts to fill gaps in our understanding by examining K-pop music genres used in Korea and comparing them with genres used in North America. We provide background information on K-pop genres by analyzing 602 genre-related labels collected from eight major music distribution websites in Korea. In addition, we report upon a user study in which American and Korean users annotated genre information for 1894 K-pop songs in order to understand how their perceptions might differ or agree. The results show higher consistency among Korean users than American users demonstrated by the difference in Fleiss’ Kappa values and proportion of agreed genre labels. Asymmetric disagreements between Americans and Koreans on specific genres reveal some interesting differences in the perception of genres. Our findings provide some insights into challenges developers may face in creating global music services. © 2013 International Society for Music Information Retrieval."
Agarwal P.; Karnick H.; Raj B.,A comparative study of indian and western music forms,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017535970&partnerID=40&md5=b255b833edf4f3f78222d9df5ca6a629,"Agarwal P., Indian Institute of Technology, Kanpur, India; Karnick H., Indian Institute of Technology, Kanpur, India; Raj B., Carnegie Mellon University, United States","Music in India has very ancient roots. Indian classical music is considered to be one of the oldest musical traditions in the world but compared to Western music very little work has been done in the areas of genre recognition, classification, automatic tagging, comparative studies etc. In this work, we investigate the structural differences between Indian and Western music forms and compare the two forms of music in terms of harmony, rhythm, micro-tones, timbre and other spectral features. To capture the temporal and static structure of the spectrogram, we form a set of global and local frame-wise features for 5- genres of each music form. We then apply Adaboost classification and GMM based Hidden Markov Models for four types of feature sets and observe that Indian Music performs better as compared to Western Music. We have achieved a best accuracy of 98.0% and 77.5% for Indian and Western musical genres respectively. Our comparative analysis indicates that features that work well with one form of music may not necessarily perform well with the other form. The results obtained on Indian Music Genres are better than the previous state-of-the-art. © 2013 International Society for Music Information Retrieval."
Porter A.; Sordo M.; Serra X.,Dunya: A system for browsing audio music collections exploiting cultural context,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008909252&partnerID=40&md5=52014f94d135c56a79a0ea17cce1f359,"Porter A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Music recommendation and discovery is an important MIR application with a strong impact in the music industry, but most music recommendation systems are still quite generic and without much musical knowledge. In this paper we present a web-based software application that lets users interact with an audio music collection through the use of musical concepts that are derived from a specific musical culture, in this case Carnatic music. The application includes a database containing information relevant to that music collection, such as audio recordings, editorial information, and metadata obtained from various sources. An analysis module extracts features from the audio recordings that are related to Carnatic music, which are then used to create musically meaningful relationships between all of the items in the database. The application displays the content of these items, allowing users to navigate through the collection by identifying and showing other information that is related to the currently viewed item, either by showing the relationships between them or by using culturally relevant similarity measures. The basic architecture and the design principles developed are reusable for other music collections with different characteristics. © 2013 International Society for Music Information Retrieval."
Burgoyne J.A.; Bountouridis D.; Van Balen J.; Honing H.,Hooked: A game for discovering what makes music catchy,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016305231&partnerID=40&md5=fad07bee794580b098704a67d673ed4b,"Burgoyne J.A., Music Cognition Group, University of Amsterdam, Netherlands; Bountouridis D., Department of Information and Computing Sciences, Utrecht University, Netherlands; Van Balen J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Honing H., Music Cognition Group, University of Amsterdam, Netherlands","Although there has been some empirical research on earworms, songs that become caught and replayed in one’s memory over and over again, there has been surprisingly little empirical research on the more general concept of the musical hook, the most salient moment in a piece of music, or the even more general concept of what may make music ‘catchy’. Almost by definition, people like catchy music, and thus this question is a natural candidate for approaching with ‘gamification’. We present the design of Hooked, a game we are using to study musical catchiness, as well as the theories underlying its design and the results of a pilot study we undertook to check its scientific validity. We found significant differences in time to recall pieces of music across different segments, identified parameters for making recall tasks more or less challenging, and found that players are not as reliable as one might expect at predicting their own recall performance. © 2013 International Society for Music Information Retrieval."
Prätzlich T.; Müller M.,Freischütz digital: A case study for reference-based audio segmentation of operas,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069822684&partnerID=40&md5=5a9fc9a3a69bb63172c4274d1f3a918e,"Prätzlich T., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Music information retrieval has started to become more and more important in the humanities by providing tools for computer-assisted processing and analysis of music data. However, when applied to real-world scenarios, even established techniques, which are often developed and tested under lab conditions, reach their limits. In this paper, we illustrate some of these challenges by presenting a study on automated audio segmentation in the context of the interdisciplinary project “Freischütz Digital”. One basic task arising in this project is to automatically segment different recordings of the opera “Der Freischütz” according to a reference segmentation specified by a domain expert (musicologist). As it turns out, the task is more complex as one may think at first glance due to significant acoustic and structural variations across the various recordings. As our main contribution, we reveal and discuss these variations by systematically adapting segmentation procedures based on synchronization and matching techniques. © 2013 International Society for Music Information Retrieval."
Gao B.; Dellandréa E.; Chen L.,Sparse music decomposition onto a MIDI dictionary driven by statistical music knowledge,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069881011&partnerID=40&md5=4528d45d2b09ad717c4175b3def7f65b,"Gao B., Université de Lyon, CNRS, Ecole centrale de Lyon, LIRIS, UMR5205, F-69134, France; Dellandréa E., Université de Lyon, CNRS, Ecole centrale de Lyon, LIRIS, UMR5205, F-69134, France; Chen L., Université de Lyon, CNRS, Ecole centrale de Lyon, LIRIS, UMR5205, F-69134, France","The general goal of music signal decomposition is to represent the music structure into a note level to provide valuable semantic features for further music analysis tasks. In this paper, we propose a new method to sparsely decompose the music signal onto a MIDI dictionary made of musical notes. Statistical music knowledge is further integrated into the whole sparse decomposition process. The proposed method is divided into a frame level sparse decomposition stage and a whole music level optimal note path searching. In the first stage note co-occurrence probabilities are embedded to generate a sparse multiple candidate graph while in the second stage note transition probabilities are incorporated into the optimal path searching. Experiments on real-world polyphonic music show that embedding music knowledge within the sparse decomposition achieves notable improvement in terms of note recognition precision and recall. © 2013 International Society for Music Information Retrieval."
Benetos E.; Holzapfel A.,Automatic transcription of Turkish makam music,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050217187&partnerID=40&md5=72585a0704ef22c129ebd539a333fd25,"Benetos E., City University London, United Kingdom; Holzapfel A., Boğaziçi University, Turkey","In this paper we propose an automatic system for transcribing makam music of Turkey. We document the specific traits of this music that deviate from properties that were targeted by transcription tools so far and we compile a dataset of makam recordings along with aligned microtonal ground-truth. An existing multi-pitch detection algorithm is adapted for transcribing music in 20 cent resolution, and the final transcription is centered around the tonic frequency of the recording. Evaluation metrics for transcribing microtonal music are utilized and results show that transcription of Turkish makam music in e.g. an interactive transcription software is feasible using the current state-of-the-art. © 2013 International Society for Music Information Retrieval."
Smith J.B.L.; Chew E.,A meta-analysis of the mirex structure segmentation task,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994518163&partnerID=40&md5=2dbec4d359876821d39d553f822a19c3,"Smith J.B.L., Queen Mary, University of London, United Kingdom; Chew E., Queen Mary, University of London, United Kingdom","The Music Information Retrieval Evaluation eXchange (MIREX) serves an essential function in the MIR community, but researchers have noted that the anonymity of its datasets, while useful, has made it difficult to interpret the successes and failures of the algorithms. We use the results of the 2012 MIREX Structural Segmentation task, which was accompanied by anonymous ground truth, to conduct a meta-evaluation of the algorithms. We hope this demonstrates the benefits, to both the participants and evaluators of MIREX, of releasing more data in evaluation tasks. Our aim is to learn more about the performance of the algorithms by studying how their success relates to properties of the annotations and recordings. We find that some evaluation metrics are redundant, and that several algorithms do not adequately model the true number of segments in typical annotations We also use publicly available ground truth to identify many of the recordings in the MIREX test sets, allowing us to identify specific pieces on which algorithms generally performed poorly and to discover where the most improvement is needed. © 2013 International Society for Music Information Retrieval."
Moreira J.; Roy P.; Pachet F.,VirtualBand: Interacting with stylistically consistent agents,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059163106&partnerID=40&md5=985db2aa4f73be6cb54bd33f281c2a37,"Moreira J., Sony CSL, Japan; Roy P., Sony CSL, Japan; Pachet F., Sony CSL, Japan","VirtualBand is a multi-agent system dedicated to live computer-enhanced music performances. VirtualBand enables one or several musicians to interact in real-time with stylistically plausible virtual agents. The problem addressed is the generation of virtual agents, each representing the style of a given musician, while reacting to human players. We propose a generation framework that relies on feature-based interaction. Virtual agents exploit a style database, which consists of audio signals from which a set of MIR features are extracted. Musical interactions are represented by directed connections between agents through these features. The connections are themselves specified as mappings and database filters. We claim that such a connection framework allows to implement meaningful musical interactions and to produce stylistically consistent musical output. We illustrate this concept through several examples in jazz improvisation, beatboxing and interactive mash-ups. © 2013 International Society for Music Information Retrieval."
Stober S.; Cameron D.J.; Grahn J.A.,Classifying EEG recordings of rhythm perception,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937702257&partnerID=40&md5=adbe973692c5eb4848fad5fa588a2b74,"Stober S., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Cameron D.J., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Grahn J.A., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada","Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. In this paper, we present first classification results using deep learning techniques on EEG data recorded within a rhythm perception study in Kigali, Rwanda. We tested 13 adults, mean age 21, who performed three behavioral tasks using rhythmic tone sequences derived from either East African or Western music. For the EEG testing, 24 rhythms – half East African and half Western with identical tempo and based on a 2-bar 12/8 scheme – were each repeated for 32 seconds. During presentation, the participants’ brain waves were recorded via 14 EEG channels. We applied stacked denoising autoencoders and convolutional neural networks on the collected data to distinguish African and Western rhythms on a group and individual participant level. Furthermore, we investigated how far these techniques can be used to recognize the individual rhythms. © Sebastian Stober, Daniel J. Cameron and Jessica A. Grahn."
Joren S.; Leman M.,Panako - A scalable acoustic fingerprinting system handling time-scale and pitch modification,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942370708&partnerID=40&md5=fc4426050f5b70be5e05bae64c4ac61a,"Joren S., Institute for Psychoacoustics and Electronic Music (IPEM), Department of Musicology, Ghent University, Ghent, Belgium; Leman M., Institute for Psychoacoustics and Electronic Music (IPEM), Department of Musicology, Ghent University, Ghent, Belgium","This paper presents a scalable granular acoustic fingerprinting system. An acoustic fingerprinting system uses condensed representation of audio signals, acoustic fingerprints, to identify short audio fragments in large audio databases. A robust fingerprinting system generates similar fingerprints for perceptually similar audio signals. The system presented here is designed to handle time-scale and pitch modifications. The open source implementation of the system is called Panako and is evaluated on commodity hardware using a freely available reference database with fingerprints of over 30,000 songs. The results show that the system responds quickly and reliably on queries, while handling time-scale and pitch modifications of up to ten percent. The system is also shown to handle GSM-compression, several audio effects and band-pass filtering. After a query, the system returns the start time in the reference audio and how much the query has been pitch-shifted or time-stretched with respect to the reference audio. The design of the system that offers this combination of features is the main contribution of this paper. © Six Joren, Marc Leman."
Rafii Z.; Germain F.G.; Sun D.L.; Mysore G.J.,Combining modeling of singing voice and background music for automatic separation of musical mixtures,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057712454&partnerID=40&md5=0aa4fad1e4cf8f7a7e9e917f95a6c4d5,"Rafii Z., Northwestern University, Department of Electrical Engineering and Computer Science, United States; Germain F.G., Stanford University, Center for Computer Research in Music and Acoustics, United States; Sun D.L., Stanford University, Center for Computer Research in Music and Acoustics, United States, Stanford University, Department of Statistics, United States; Mysore G.J., Adobe Research, United States","Musical mixtures can be modeled as being composed of two characteristic sources: singing voice and background music. Many music/voice separation techniques tend to focus on modeling one source; the residual is then used to explain the other source. In such cases, separation performance is often unsatisfactory for the source that has not been explicitly modeled. In this work, we propose to combine a method that explicitly models singing voice with a method that explicitly models background music, to address separation performance from the point of view of both sources. One method learns a singer-independent model of voice from singing examples using a Non-negative Matrix Factorization (NMF) based technique, while the other method derives a model of music by identifying and extracting repeating patterns using a similarity matrix and a median filter. Since the model of voice is singer-independent and the model of music does not require training data, the proposed method does not require training data from a user, once deployed. Evaluation on a data set of 1,000 song clips showed that combining modeling of both sources can improve separation performance, when compared with modeling only one of the sources, and also compared with two other state-of-the-art methods. © 2013 International Society for Music Information Retrieval."
Pesek M.; Leonardis A.; Marolt M.,A compositional hierarchical model for music information retrieval,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008439280&partnerID=40&md5=48db903c5b1548b8bca9937e20c1e98a,"Pesek M., University of Ljubljana, Faculty of computer and information science, Slovenia; Leonardis A., Centre for Computational Neuroscience and Cognitive Robotics, School of Computer Science, University of Birmingham, United Kingdom; Marolt M., Centre for Computational Neuroscience and Cognitive Robotics, School of Computer Science, University of Birmingham, United Kingdom","This paper presents a biologically-inspired compositional hierarchical model for MIR. The model can be treated as a deep learning model, and poses an alternative to deep architectures based on neural networks. Its main features are generativeness and transparency that allow clear insight into concepts learned from the input music signals. The model consists of multiple layers, each is composed of a number of parts. The hierarchical nature of the model corresponds well with the hierarchical structures in music. Parts in lower layers correspond to low-level concepts (e.g. tone partials), while parts in higher layers combine lower-level representations into more complex concepts (tones, chords). The layers are unsupervisedly learned one-by-one from music signals. Parts in each layer are compositions of parts from previous layers based on statistical co-occurrences as the driving force of the learning process. We present the model’s structure and compare it to other deep architectures. A preliminary evaluation of the model’s usefulness for automated chord estimation and multiple fundamental frequency estimation tasks is provided. Additionally, we show how the model can be extended to event-based music processing, which is our final goal. © Matevž Pesek, Aleš Leonardis, Matija Marolt."
Davies M.E.P.; Hamel P.; Yoshii K.; Goto M.,AutoMashUpper: An automatic multi-song mashup system,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057364410&partnerID=40&md5=e81e282a8cf209d3c8efd08c2d803373,"Davies M.E.P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Hamel P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper describes AutoMashUpper, an interactive system for creating music mashups by automatically selecting and mixing multiple songs together. Given a user-specified input song, the system first identifies the phrase-level structure and then estimates the “mashability” between each phrase section of the input and songs in the user’s music collection. Mashability is calculated based on the harmonic similarity between beat synchronous chromagrams over a user-definable range of allowable key shifts and tempi. Once a match in the collection for a given section of the input song has been found, a pitch-shifting and time-stretching algorithm is used to harmonically and temporally align the sections, after which the loudness of the transformed section is modified to ensure a balanced mix. AutoMashUpper has a user interface to allow visualisation and manipulation of mashups. When creating a mashup, users can specify a list of songs to choose from, modify the mashability parameters and change the granularity of the phrase segmentation. Once created, users can also switch, add, or remove sections from the mashup to suit their taste. In this way, AutoMashUpper can assist users to actively create new music content by enabling and encouraging them to explore the mashup space. © 2013 International Society for Music Information Retrieval."
Panteli M.; Purwins H.,A computational comparison of theory and practice of scale intonation in Byzantine chant,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069841913&partnerID=40&md5=4c7817643844767a5e5e9c317e91e40f,"Panteli M., Department of Computer Science, University of Cyprus, Cyprus; Purwins H., Neurotechnology Group, EE and CS, Berlin Institute of Technology, Germany, Sound and Music Computing Group, Aalborg University, Copenhagen, Denmark","Byzantine Chant performance practice is quantitatively compared to the Chrysanthine theory. The intonation of scale degrees is quantified, based on pitch class profiles. An analysis procedure is introduced that consists of the following steps: 1) Pitch class histograms are calculated via non-parametric kernel smoothing. 2) Histogram peaks are detected. 3) Phrase ending analysis aids the finding of the tonic to align histogram peaks. 4) The theoretical scale degrees are mapped to the practical ones. 5) A schema of statistical tests detects significant deviations of theoretical scale tuning from the estimated ones in performance practice. The analysis of 94 echoi shows a tendency of the singer to level theoretic particularities of the echos that stand out of the general norm in the octoechos: theoretically extremely large scale steps are diminished in performance. © 2013 International Society for Music Information Retrieval."
Kaneshiro B.; Kim H.-S.; Herrera J.; Oh J.; Berger J.; Slaney M.,QBT-extended: An annotated dataset of melodically contoured tapped queries,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069804344&partnerID=40&md5=6f93c149df156ce8792558a29d752a1c,"Kaneshiro B., CCRMA, Stanford University, United States; Kim H.-S., CCRMA, Stanford University, United States; Herrera J., CCRMA, Stanford University, United States; Oh J., CCRMA, Stanford University, United States; Berger J., CCRMA, Stanford University, United States; Slaney M., CCRMA, Stanford University, United States, Microsoft Research, CCRMA, United States","Query by tapping remains an intuitive yet underdeveloped form of content-based querying. Tapping databases suffer from small size and often lack useful annotations about users and query cues. More broadly, tapped representations of music are inherently lossy, as they lack pitch information. To address these issues, we publish QBT-Extended—an annotated dataset of over 3,300 tapped queries of pop song excerpts, along with a system for collecting them. The queries, collected from 60 users for 51 songs, contain both time stamps and pitch positions of tap events and are annotated with information about the user, such as musical training and familiarity with each excerpt. Queries were performed from both short-term and long-term memory, cued by lyrics alone or lyrics and audio. In the present paper, we characterize and evaluate the dataset and perform initial analyses, providing early insights into the added value of the novel information. While the current data were collected under controlled experimental conditions, the system is designed for large-scale, crowdsourced data collection, presenting an opportunity to expand upon this richer form of tapping data. © 2013 International Society for Music Information Retrieval."
Han Y.; Lee K.,Hierarchical approach to detect common mistakes of beginner flute players,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955100773&partnerID=40&md5=70621b77fa3666a4036dc2b2ed272e9e,"Han Y., Music and Audio Research Group, Seoul National University, Seoul, South Korea; Lee K., Music and Audio Research Group, Seoul National University, Seoul, South Korea","Music lessons are a repetitive process of giving feedback on a student’s performance techniques. The manner in which performance skills are improved depends on the particular instrument, and therefore, it is important to consider the unique characteristics of the target instrument. In this paper, we investigate the common mistakes of beginner flute players and propose a hierarchical approach to detect such mistakes. We first examine the structure and mechanism of the flute, and define several types of common mistakes that can be caused by incorrect assembly, poor blowing skills, or mis-fingering. We propose tailored algorithms for detecting each case by combining deterministic signal processing and deep learning, to quantify the quality of a flute sound. The system is structured hierarchically, as mis-fingering detection requires the input sound to be correctly assembled and blown to discriminate minor sound difference. Experimental results show that it is possible to identify different mistakes in flute performance using our proposed algorithms. © First author, Second author, Third author."
Yen F.; Luo Y.-J.; Chi T.-S.,Singing voice separation using spectro-temporal modulation features,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976221687&partnerID=40&md5=e27ca66b4355138c07c7e91339801ecd,"Yen F., Master Program of SMIT, National Chiao-Tung University, Taiwan; Luo Y.-J., Master Program of SMIT, National Chiao-Tung University, Taiwan; Chi T.-S., Dept. of Elec. and Comp. Engineering, National Chiao-Tung University, Taiwan","An auditory-perception inspired singing voice separation algorithm for monaural music recordings is proposed in this paper. Under the framework of computational auditory scene analysis (CASA), the music recordings are first transformed into auditory spectrograms. After extracting the spectral-temporal modulation contents of the time-frequency (T-F) units through a two-stage auditory model, we define modulation features pertaining to three categories in music audio signals: vocal, harmonic, and percussive. The T-F units are then clustered into three categories and the singing voice is synthesized from T-F units in the vocal category via time-frequency masking. The algorithm was tested using the MIR-1K dataset and demonstrated comparable results to other unsupervised masking approaches. Meanwhile, the set of novel features gives a possible explanation on how the auditory cortex analyzes and identifies singing voice in music audio mixtures. © Frederick Yen, Yin-Jyun Luo, Tai-Shih Chi."
Grant M.; Ekanayake A.; Turnbull D.,Meuse: Recommending internet radio stations,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030992562&partnerID=40&md5=7ca8cfab5f82370d9062775bff48e32d,"Grant M., Ithaca College, United States; Ekanayake A., Ithaca College, United States; Turnbull D., Ithaca College, United States","In this paper, we describe a novel Internet radio recommendation system called MeUse. We use the Shoutcast API to collect historical data about the artists that are played on a large set of Internet radio stations. This data is used to populate an artist-station index that is similar to the term-document matrix of a traditional text-based information retrieval system. When a user wants to find stations for a given seed artist, we check the index to determine a set of stations that are either currently playing or have recently played that artist. These stations are grouped into three clusters and one representative station is selected from each cluster. This promotes diversity among the stations that are returned to the user. In addition, we provide additional information such as relevant tags (e.g., genres, emotions) and similar artists to give the user more contextual information about the recommended stations. Finally, we describe a web-based user interface that provides an interactive experience that is more like a personalized Internet radio player (e.g., Pandora) and less like a search engine for Internet radio stations (e.g., Shoutcast). A small-scale user study suggests that the majority of users enjoyed using MeUse but that providing additional contextual information may be needed to help with recommendation transparency. © 2013 International Society for Music Information Retrieval."
Bogdanov D.; Wack N.; Gómez E.; Gulati S.; Herrera P.; Mayor O.; Roma G.; Salamon J.; Zapata J.; Serra X.,Essentia: An audio analysis library for music information retrieval,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019537868&partnerID=40&md5=b9ece58c08d191f5921f6b4fc48c3553,"Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Wack N., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Mayor O., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Roma G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Zapata J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications. © 2013 International Society for Music Information Retrieval."
Korzeniowski F.; Böck S.; Widmer G.,Probabilistic extraction of beat positions from a beat activation function,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003466090&partnerID=40&md5=7c66f23e97c75c972a879403b329b033,"Korzeniowski F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We present a probabilistic way to extract beat positions from the output (activations) of the neural network that is at the heart of an existing beat tracker. The method can serve as a replacement for the greedy search the beat tracker currently uses for this purpose. Our experiments show improvement upon the current method for a variety of data sets and quality measures, as well as better results compared to other state-of-the-art algorithms. © Filip Korzeniowski, Sebastian Böck, Gerhard Widmer."
McFee B.; Ellis D.P.W.,Analyzing song structure with spectral clustering,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946067288&partnerID=40&md5=eb5b8e66f2b6b15ba0db2242f4bf192e,"McFee B., Center for Jazz Studies, Columbia University, United States; Ellis D.P.W., LabROSA, Columbia University, United States","Many approaches to analyzing the structure of a musical recording involve detecting sequential patterns within a self-similarity matrix derived from time-series features. Such patterns ideally capture repeated sequences, which then form the building blocks of large-scale structure. In this work, techniques from spectral graph theory are applied to analyze repeated patterns in musical recordings. The proposed method produces a low-dimensional encoding of repetition structure, and exposes the hierarchical relationships among structural components at differing levels of granularity. Finally, we demonstrate how to apply the proposed method to the task of music segmentation. © Brian McFee, Daniel P.W. Ellis."
Koerich A.L.,Improving the reliability of music genre classification using rejection and verification,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032463965&partnerID=40&md5=2d96007906abbb5c71aa65ce599b48b5,"Koerich A.L., Pontifical Catholic University of Paraná (PUCPR), Federal University of Paraná (UFPR), Brazil","This paper presents a novel approach for post-processing the music genre hypotheses generated by a baseline classifier. Given a music piece, the baseline classifier produces a ranked list of the N best hypotheses consisting of music genre labels and recognition scores. A rejection strategy is then applied to either reject or accept the output of the baseline classifier. Some of the rejected instances are handled by a verification stage which extracts visual features from the spectrogram of the music signal and employs binary support vector machine classifiers to disambiguate between confusing classes. The rejection and verification approach has improved the reliability in classifying music genres. Our approach is described in detail and the experimental results on a benchmark dataset are presented. © 2013 International Society for Music Information Retrieval."
van Kranenburg P.; Karsdorp F.,Cadence detection in western traditional stanzaic songs using melodic and textual features,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956838999&partnerID=40&md5=b8ccb551c230dce322546397b6f735ac,"van Kranenburg P., Meertens Institute, Amsterdam, Netherlands; Karsdorp F., Meertens Institute, Amsterdam, Netherlands","Many Western songs are hierarchically structured in stanzas and phrases. The melody of the song is repeated for each stanza, while the lyrics vary. Each stanza is subdivided into phrases. It is to be expected that melodic and textual formulas at the end of the phrases offer intrinsic clues of closure to a listener or singer. In the current paper we aim at a method to detect such cadences in symbolically encoded folk songs. We take a trigram approach in which we classify trigrams of notes and pitches as cadential or as non-cadential. We use pitch, contour, rhythmic, textual, and contextual features, and a group of features based on the conditions of closure as stated by Narmour [11]. We employ a random forest classification algorithm. The precision of the classifier is considerably improved by taking the class labels of adjacent trigrams into account. An ablation study shows that none of the kinds of features is sufficient to account for good classification, while some of the groups perform moderately well on their own. © Peter van Kranenburg, Folgert Karsdorp."
Sidorov K.; Jones A.; Marshall D.,Music analysis as a smallest grammar problem,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956743603&partnerID=40&md5=1810a62254128d354d937917ced1c664,"Sidorov K., Cardiff University, United Kingdom; Jones A., Cardiff University, United Kingdom; Marshall D., Cardiff University, United Kingdom","In this paper we present a novel approach to music analysis, in which a grammar is automatically generated explaining a musical work’s structure. The proposed method is predicated on the hypothesis that the shortest possible grammar provides a model of the musical structure which is a good representation of the composer’s intent. The effectiveness of our approach is demonstrated by comparison of the results with previously-published expert analysis; our automated approach produces results comparable to human annotation. We also illustrate the power of our approach by showing that it is able to locate errors in scores, such as introduced by OMR or human transcription. Further, our approach provides a novel mechanism for intuitive high-level editing and creative transformation of music. A wide range of other possible applications exists, including automatic summarization and simplification; estimation of musical complexity and similarity, and plagiarism detection. © Kirill Sidorov, Andrew Jones, David Marshall."
Hamanaka M.; Hirata K.; Tojo S.,Musical structural analysis database based on GTTM,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956763221&partnerID=40&md5=3853cbed46c71bd912dbd62bc14d848f,"Hamanaka M., Kyoto University, Japan; Hirata K., Future University Hakodate, Japan; Tojo S., JAIST, Japan","This paper, we present the publication of our analysis data and analyzing tool based on the generative theory of tonal music (GTTM). Musical databases such as score databases, instrument sound databases, and musical pieces with standard MIDI files and annotated data are key to advancements in the field of music information technology. We started implementing the GTTM on a computer in 2004 and ever since have collected and publicized test data by musicologists in a step-by-step manner. In our efforts to further advance the research on musical structure analysis, we are now publicizing 300 pieces of analysis data as well as the analyzer. Experiments showed that for 267 of 300 pieces the analysis results obtained by a new musicologist were almost the same as the original results in the GTTM database and that the other 33 pieces had different interpretations. © Masatoshi Hamanaka, Keiji Hirata, Satoshi Tojo."
Hu Y.; Li D.; Mitsunori O.,Evaluation on feature importance for favorite song detection,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069879439&partnerID=40&md5=1eac4a3749d07a46aaaebb8ede1ad820,"Hu Y., Department of Computer Science, University of Miami, United States; Li D., Department of Computer Science, University of Miami, United States; Mitsunori O., Department of Computer Science, University of Miami, United States","Detecting whether a song is favorite for a user is an important but also challenging task in music recommendation. One of critical steps to do this task is to select important features for the detection. This paper presents two methods to evaluate feature importance, in which we compared nine available features based on a large user log in the real world. The set of features includes song metadata, acoustic feature, and user preference used by Collaborative Filtering techniques. The evaluation methods are designed from two views: i) the correlation between the estimated scores by song similarity in respect of a feature and the scores estimated by real play count, ii) feature selection methods over a binary classification problem, i.e., “like” or “dislike”. The experimental results show the user preference is the most important feature and artist similarity is of the second importance among these nine features. © 2013 International Society for Music Information Retrieval."
Cartwright M.; Pardo B.,Social-EQ: Crowdsourcing an equalization descriptor map,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054226848&partnerID=40&md5=66d11e5dcc51bf6a97ef6284c4718996,"Cartwright M., Northwestern University, EECS Department, United States; Pardo B., Northwestern University, EECS Department, United States","We seek to simplify audio production interfaces (such as those for equalization) by letting users communicate their audio production objectives with descriptive language (e.g. “Make the violin sound ‘warmer.’”). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin “warmer” with a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions need to be taken. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialEQ, a web-based project for learning a vocabulary of actionable audio equalization descriptors. Since deployment, SocialEQ has learned 324 distinct words in 731 learning sessions. Data on these terms is made available for download. We examine terms users have provided, exploring which ones map well to equalization, which ones have broadly-agreed upon meaning, which term have meanings specific small groups, and which terms are synonymous. © 2013 International Society for Music Information Retrieval."
"Polfreman R., Dr",Comparing onset detection & perceptual attack time,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052763440&partnerID=40&md5=29b8af59d9e40c12076ac79f046d5779,"Polfreman R., Dr, University of Southampton, United Kingdom","Accurate performance timing is associated with the perceptual attack time (PAT) of notes, rather than their physical or perceptual onsets (PhOT, POT). Since manual annotation of PAT for analysis is both time-consuming and impractical for real-time applications, automatic transcription is desirable. However, computational methods for onset detection in audio signals are conventionally measured against PhOT or POT data. This paper describes a comparison between PAT and onset detection data to assess whether in some circumstances they are similar enough to be equivalent, or whether additional models for PAT-PhOT difference are always necessary. Eight published onset algorithms, and one commercial system, were tested with five onset types in short monophonic sequences. Ground truth was established by multiple human transcription of the audio for PATs using rhythm adjustment with synchronous presentation, and parameters for each detection algorithm manually adjusted to produce the maximum agreement with the ground truth. Results indicate that for percussive attacks, a number of algorithms produce data close to or within the limits of human agreement and therefore may be substituted for PATs, while for non-percussive sounds corrective measures are necessary to match detector outputs to human estimates. © 2013 International Society for Music Information Retrieval."
Masuda T.; Yoshii K.; Goto M.; Morishima S.,Spotting a query phrase from polyphonic music audio signals based on semi-supervised nonnegative matrix factorization,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973290458&partnerID=40&md5=e3c601d4a57149411c017bd588b5f221,"Masuda T., Waseda University, Japan; Yoshii K., Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Morishima S., Waseda University, Japan","This paper proposes a query-by-audio system that aims to detect temporal locations where a musical phrase given as a query is played in musical pieces. The “phrase” in this paper means a short audio excerpt that is not limited to a main melody (singing part) and is usually played by a single musical instrument. A main problem of this task is that the query is often buried in mixture signals consisting of various instruments. To solve this problem, we propose a method that can appropriately calculate the distance between a query and partial components of a musical piece. More specifically, gamma process nonnegative matrix factorization (GaP-NMF) is used for decomposing the spectrogram of the query into an appropriate number of basis spectra and their activation patterns. Semi-supervised GaP-NMF is then used for estimating activation patterns of the learned basis spectra in the musical piece by presuming the piece to partially consist of those spectra. This enables distance calculation based on activation patterns. The experimental results showed that our method outperformed conventional matching methods. © Taro Masuda, Kazuyoshi Yoshii, Masataka Goto, Shigeo Morishima."
Xing Z.; Wang X.; Wang Y.,Enhancing collaborative filtering music recommendation by balancing exploration and exploitation,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011109065&partnerID=40&md5=5b788e2d5a4fa210ecedbe72c49e4cde,"Xing Z., School of Computing, National University of Singapore, Singapore; Wang X., School of Computing, National University of Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore","Collaborative filtering (CF) techniques have shown great success in music recommendation applications. However, traditional collaborative-filtering music recommendation algorithms work in a greedy way, invariably recommending songs with the highest predicted user ratings. Such a purely exploitative strategy may result in suboptimal performance over the long term. Using a novel reinforcement learning approach, we introduce exploration into CF and try to balance between exploration and exploitation. In order to learn users’ musical tastes, we use a Bayesian graphical model that takes account of both CF latent factors and recommendation novelty. Moreover, we designed a Bayesian inference algorithm to efficiently estimate the posterior rating distributions. In music recommendation, this is the first attempt to remedy the greedy nature of CF approaches. Results from both simulation experiments and user study show that our proposed approach significantly improves recommendation performance. © Zhe Xing, Xinxi Wang, Ye Wang."
Sturm B.L.; Collins N.,The Kiki-Bouba challenge: Algorithmic composition for content-based MIR research & development,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003624914&partnerID=40&md5=f46c12c7ea3015679acd20521874788e,"Sturm B.L., Audio Analysis Lab, Aalborg University, Denmark; Collins N., Dept. Music, Durham University, United Kingdom","We propose the “Kiki-Bouba Challenge” (KBC) for the research and development of content-based music information retrieval (MIR) systems. This challenge is unencumbered by several problems typically encountered in MIR research: insufficient data, restrictive copyrights, imperfect ground truth, a lack of specific criteria for classes (e.g., genre), a lack of explicit problem definition, and irrepro-ducibility. KBC provides a limitless amount of free data, a perfect ground truth, and well-specifiable and meaningful characteristics defining each class. These ideal conditions are made possible by open source algorithmic composition — a hitherto under-exploited resource for MIR. © Bob L. Sturm, Nick Collins."
Vera B.; Chew E.,Towards seamless network music performance: Predicting an ensemble’s expressive decisions for distributed performance,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009192202&partnerID=40&md5=68346923908b0b55a0985d6f5f15820a,"Vera B., Queen Mary University of London, Centre for Digital Music, United Kingdom; Chew E., Queen Mary University of London, Centre for Digital Music, United Kingdom","Internet performance faces the challenge of network latency. One proposed solution is music prediction, wherein musical events are predicted in advance and transmitted to distributed musicians ahead of the network delay. We present a context-aware music prediction system focusing on expressive timing: a Bayesian network that incorporates stylistic model selection and linear conditional gaussian distributions on variables representing proportional tempo change. The system can be trained using rehearsals of distributed or co-located ensembles. We evaluate the model by comparing its prediction accuracy to two others: one employing only linear conditional dependencies between expressive timing nodes but no stylistic clustering, and one using only independent distributions for timing changes. The three models are tested on performances of a custom-composed piece that is played ten times, each in one of two styles. The results are promising, with the proposed system outperforming the other two. In predictable parts of the performance, the system with conditional dependencies and stylistic clustering achieves errors of 15ms; in more difficult sections, the errors rise to 100ms; and, in unpredictable sections, the error is too great for seamless timing emulation. Finally, we discuss avenues for further research and propose the use of predictive timing cues using our system. © Bogdan Vera, Elaine Chew."
Yang P.-K.; Hsu C.-C.; Chien J.-T.,Bayesian singing-voice separation,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990833868&partnerID=40&md5=dc24ea0005559c48d0e8d16ccf43ba31,"Yang P.-K., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Hsu C.-C., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Chien J.-T., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","This paper presents a Bayesian nonnegative matrix factorization (NMF) approach to extract singing voice from background music accompaniment. Using this approach, the likelihood function based on NMF is represented by a Poisson distribution and the NMF parameters, consisting of basis and weight matrices, are characterized by the exponential priors. A variational Bayesian expectation-maximization algorithm is developed to learn variational parameters and model parameters for monaural source separation. A clustering algorithm is performed to establish two groups of bases: one is for singing voice and the other is for background music. Model complexity is controlled by adaptively selecting the number of bases for different mixed signals according to the variational lower bound. Model regularization is tackled through the uncertainty modeling via variational inference based on marginal likelihood. The experimental results on MIR-1K database show that the proposed method performs better than various unsupervised separation algorithms in terms of the global normalized source to distortion ratio. © Po-Kai Yang, Chung-Chien Hsu and Jen-Tzung Chien."
Davies M.E.P.; Böck S.,Evaluating the evaluation measures for beat tracking,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956832444&partnerID=40&md5=93f5601ec3d3eafd3e814ea9d1193cb6,"Davies M.E.P., Sound and Music Computing Group, INESC TEC, Porto, Portugal; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria","The evaluation of audio beat tracking systems is normally addressed in one of two ways. One approach is for human listeners to judge performance by listening to beat times mixed as clicks with music signals. The more common alternative is to compare beat times against ground truth annotations via one or more of the many objective evaluation measures. However, despite a large body of work in audio beat tracking, there is currently no consensus over which evaluation measure(s) to use, meaning multiple accuracy scores are typically reported. In this paper, we seek to evaluate the evaluation measures by examining the relationship between objective accuracy scores and human judgements of beat tracking performance. First, we present the raw correlation between objective scores and subjective ratings, and show that evaluation measures which allow alternative metrical levels appear more correlated than those which do not. Second, we explore the effect of parameterisation of objective evaluation measures, and demonstrate that correlation is maximised for smaller tolerance windows than those currently used. Our analysis suggests that true beat tracking performance is currently being overestimated via objective evaluation. © Mathew E. P. Davies, Sebastian Böck."
Lin Y.; Chen X.; Yang D.,Exploration of music emotion recognition based on MIDI,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046891653&partnerID=40&md5=15743094a75a3135347822bbca36731c,"Lin Y., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China","Audio and lyric features are commonly considered in the research of music emotion recognition, whereas MIDI features are rarely used. Some research revealed that among the features employed in music emotion recognition, lyric has the best performance on valence, MIDI takes the second place, and audio is the worst. However, lyric cannot be found in some music types, such as instrumental music. In this case, MIDI features can be considered as a choice for music emotion recognition on valence dimension. In this presented work, we systematically explored the effect and value of using MIDI features for music emotion recognition. Emotion recognition was treated as a regression problem in this paper. We also discussed the emotion regression performance of three aspects of music in terms of edited MIDI: chorus, melody, and accompaniment. We found that the MIDI features performed better than audio features on valence. And under the realistic conditions, converted MIDI performed better than edited MIDI on valence. We found that melody was more important to valence regression than accompaniment, which was in contrary to arousal. We also found that the chorus part of an edited MIDI might contain as sufficient information as the entire edited MIDI for valence regression. © 2013 International Society for Music Information Retrieval."
Vera B.; Chew E.; Healey P.G.T.,A study of ensemble synchronisation under restricted line of sight,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044856869&partnerID=40&md5=94a820b7cd0c541e9ae82cd4e00d38ff,"Vera B., Queen Mary University of London, Centre for Digital Music, United Kingdom; Chew E., Queen Mary University of London, Centre for Digital Music, United Kingdom; Healey P.G.T., Queen Mary University of London, Cognitive Science Research Group, United Kingdom","This paper presents a quantitative study of musician synchronisation in ensemble performance under restricted line of sight, an inherent condition in scenarios like distributed music performance. The study focuses on the relevance of gestural (e.g. visual, breath) cues in achieving note onset synchrony in a violin and cello duo, in which musicians must fulfill a mutual conducting role. The musicians performed two pieces – one with long notes separated by long pauses, another with long notes but no pauses – under direct, partial (silhouettes), and no line of sight. Analysis of the musicians’ note synchrony shows that visual contact significantly impacts synchronization in the first piece, but not significantly in the second piece, leading to the hypothesis that opportunities to shape notes may provide further cues for synchronization. The results also show that breath cues are important, and that the relative positions of these cues impact note asynchrony at the ends of pauses; thus, the advance timing information provided by breath cues could form a basis for generating virtual cues in distributed performance, where network latency delays sonic and visual cues. This study demonstrates the need to account for structure (e.g. pauses, long notes) and prosodic gestures in ensemble synchronisation. © 2013 International Society for Music Information Retrieval."
Prockup M.; Schmidt E.M.; Scott J.; Kim Y.E.,Toward understanding expressive percussion through content based analysis,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047961160&partnerID=40&md5=f1e4b7663d35340ba7f7b40ce7b2e45a,"Prockup M., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Schmidt E.M., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Scott J., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States","Musical expression is the creative nuance through which a musician conveys emotion and connects with a listener. In un-pitched percussion instruments, these nuances are a very important component of performance. In this work, we present a system that seeks to classify different expressive articulation techniques independent of percussion instrument. One use of this system is to enhance the organization of large percussion sample libraries, which can be cumbersome and daunting to navigate. This work is also a necessary first step towards understanding musical expression as it relates to percussion performance. The ability to classify expressive techniques can lead to the development of models that learn the the functionality of articulations in patterns, as well as how certain performers use them to communicate their ideas and define their musical style. Additionally, in working towards understanding expressive percussion, we introduce a publicly available dataset of articulations recorded from a standard four piece drum kit that captures the instrument’s expressive range. © 2013 International Society for Music Information Retrieval."
Peeters G.; Bisot V.,Improving music structure segmentation using lag-priors,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962476589&partnerID=40&md5=a17666ce67322dd78694020d762ed03a,"Peeters G., STMS IRCAM-CNRS-UPMC, France; Bisot V., STMS IRCAM-CNRS-UPMC, France","Methods for music structure discovery usually process a music track by first detecting segments and then labeling them. Depending on the assumptions made on the signal content (repetition, homogeneity or novelty), different methods are used for these two steps. In this paper, we deal with the segmentation in the case of repetitive content. In this field, segments are usually identified by looking for sub-diagonals in a Self-Similarity-Matrix (SSM). In order to make this identification more robust, Goto proposed in 2003 to cumulate the values of the SSM over constant-lag and search only for segments in the SSM when this sum is large. Since the various repetitions of a segment start simultaneously in a self-similarity-matrix, Serra et al. proposed in 2012 to cumulate these simultaneous values (using a so-called structure feature) to enhance the novelty of the starting and ending time of a segment. In this work, we propose to combine both approaches by using Goto method locally as a prior to the lag-dimensions of Serra et al. structure features used to compute the novelty curve. Through a large experiment on RWC and Isophonics test-sets and using MIREX segmentation evaluation measure, we show that this simple combination allows a large improvement of the segmentation results. © Geoffroy Peeters, Victor Bisot."
Kuuskankare M.; Sapp C.S.,Visual humdrum-library for PWGL,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069822629&partnerID=40&md5=d9fb419a5d2408893a09e49f15bdf5dc,"Kuuskankare M., DocMus, Sibelius Academy, United Kingdom; Sapp C.S., CCARH, Stanford University, United Kingdom","We introduce a PWGL Humdrum interface that integrates command-line unix tools for music analysis into a visual programming environment. This symbiosis allows users access to the strengths of each system—algorithmic composition and visual programming components of PWGL along with computational analysis and data processing features of Humdrum tools. Our novel interface for Humdrum graphical programming allows non-programmers better access to Humdrum analysis tools, particularly with the built-in music notation display capabilities of PWGL. ENP (Expressive Notation Package) data from PWGL can be exported as Humdrum data. Humdrum files in turn can be converted back into ENP data, allowing bi-directional communication between the two software systems. © 2013 International Society for Music Information Retrieval."
Marques A.; Andrade N.; Balby L.,Exploring the relation between novelty aspects and preferences in music listening,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044180467&partnerID=40&md5=c1f98ede71e923939c203e84744c4dce,"Marques A., Universidade Federal de Campina Grande, Brazil; Andrade N., Universidade Federal de Campina Grande, Brazil; Balby L., Universidade Federal de Campina Grande, Brazil","The discovery of new music, e.g. song tracks and artists, is a central aspect of music consumption. In order to assist users in this task, several mechanisms have been proposed to incorporate novelty awareness into music recommender systems. In this paper, we complement these efforts by investigating how the music preferences of users are affected by two different aspects of novel artists, namely familiarity and mainstreamness. We collected historical data from Last.fm users, a popular online music discovery service, to investigate how these aspects of novel artists relate to the preferences of music listeners for novel artists. The results of this analysis suggests that the users tend to cluster according to their novelty related preferences. We then conducted a comprehensive study on these groups, from where we derive implications and useful insights for developers of music retrieval services. © 2013 International Society for Music Information Retrieval."
Boland D.; Murray-Smith R.,Information-theoretic measures of music listening behaviour,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959414335&partnerID=40&md5=0a9753d0eab2957d567901d269851a42,"Boland D., School of Computing Science, University of Glasgow, United Kingdom; Murray-Smith R., School of Computing Science, University of Glasgow, United Kingdom","We present an information-theoretic approach to the measurement of users’ music listening behaviour and selection of music features. Existing ethnographic studies of music use have guided the design of music retrieval systems however are typically qualitative and exploratory in nature. We introduce the SPUD dataset, comprising 10, 000 handmade playlists, with user and audio stream metadata. With this, we illustrate the use of entropy for analysing music listening behaviour, e.g. identifying when a user changed music retrieval system. We then develop an approach to identifying music features that reflect users’ criteria for playlist curation, rejecting features that are independent of user behaviour. The dataset and the code used to produce it are made available. The techniques described support a quantitative yet user-centred approach to the evaluation of music features and retrieval systems, without assuming objective ground truth labels. © Daniel Boland, Roderick Murray-Smith."
Köküer M.; Jančovič P.; Ali-MacLachlan I.; Athwal C.,Automated detection of single- and multi-note ornaments in Irish traditional flute playing,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985028423&partnerID=40&md5=b973dc2008e9938a8cbc49de7266cacc,"Köküer M., DMT Lab, Birmingham City University, United Kingdom, School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom; Jančovič P., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom; Ali-MacLachlan I., DMT Lab, Birmingham City University, United Kingdom; Athwal C., DMT Lab, Birmingham City University, United Kingdom","This paper presents an automatic system for the detection of single- and multi-note ornaments in Irish traditional flute playing. This is a challenging problem because ornaments are notes of a very short duration. The presented ornament detection system is based on first detecting onsets and then exploiting the knowledge of musical ornamentation. We employed onset detection methods based on signal envelope and fundamental frequency and customised their parameters to the detection of soft onsets of possibly short duration. Single-note ornaments are detected based on the duration and pitch of segments, determined by adjacent onsets. Multi-note ornaments are detected based on analysing the sequence of segments. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD, which was manually annotated by an experienced flute player. The onset and single- and multi-note ornament detection performance is presented in terms of the precision, recall and F-measure. © Münevver Köküer, Peter Jančovič, Islah Ali-MacLachlan, Cham Athwal."
Benetos E.; Badeau R.; Weyde T.; Richard G.,Template adaptation for improving automatic music transcription,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978078861&partnerID=40&md5=f169fb17f913bb8edf031967397cb55f,"Benetos E., Department of Computer Science, City University, London, United Kingdom; Badeau R., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, France; Weyde T., Department of Computer Science, City University, London, United Kingdom; Richard G., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, France","In this work, we propose a system for automatic music transcription which adapts dictionary templates so that they closely match the spectral shape of the instrument sources present in each recording. Current dictionary-based automatic transcription systems keep the input dictionary fixed, thus the spectral shape of the dictionary components might not match the shape of the test instrument sources. By performing a conservative transcription pre-processing step, the spectral shape of detected notes can be extracted and utilized in order to adapt the template dictionary. We propose two variants for adaptive transcription, namely for single-instrument transcription and for multiple-instrument transcription. Experiments are carried out using the MAPS and Bach10 databases. Results in terms of multi-pitch detection and instrument assignment show that there is a clear and consistent improvement when adapting the dictionary in contrast with keeping the dictionary fixed. © E. Benetos, R. Badeau, T. Weyde, and G. Richard."
Gonzalez Thomas N.; Pasquier P.; Eigenfeldt A.; Maxwell J.B.,A methodology for the comparison of melodic generation models using meta-melo,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066842012&partnerID=40&md5=688e75985baa65169caa058411b46281,"Gonzalez Thomas N., MAMAS Lab, Simon Fraser University, Canada; Pasquier P., MAMAS Lab, Simon Fraser University, Canada; Eigenfeldt A., MAMAS Lab, Simon Fraser University, Canada; Maxwell J.B., MAMAS Lab, Simon Fraser University, Canada","We investigate Musical Metacreation algorithms by applying Music Information Retrieval techniques for comparing the output of three off-line, corpus-based style imitation models. The first is Variable Order Markov Chains, a statistical model; second is the Factor Oracle, a pattern matcher; and third, MusiCOG, a novel graphical model based on perceptual and cognitive processes. Our focus is on discovering which musical biases are introduced by the models, that is, the characteristics of the output which are shaped directly by the formalism of the models and not by the corpus itself. We describe META-MELO, a system that implements the three models, along with a methodology for the quantitative analysis of model output, when trained on a corpus of melodies in symbolic form. Results show that the models’ output are indeed different and suggest that the cognitive approach is more successful at the tasks, although none of them encompass the full creative space of the corpus. We conclude that this methodology is promising for aiding in the informed application and development of generative models for music composition problems. © 2013 International Society for Music Information Retrieval."
Tian M.; Fazekas G.; Black D.A.A.; Sandler M.,Design and evaluation of onset detectors using different fusion policies,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939862343&partnerID=40&md5=2bff3eef001242c15599066434a26df0,"Tian M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Black D.A.A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","Note onset detection is one of the most investigated tasks in Music Information Retrieval (MIR) and various detection methods have been proposed in previous research. The primary aim of this paper is to investigate different fusion policies to combine existing onset detectors, thus achieving better results. Existing algorithms are fused using three strategies, first by combining different algorithms, second, by using the linear combination of detection functions, and third, by using a late decision fusion approach. Large scale evaluation was carried out on two published datasets and a new percussion database composed of Chinese traditional instrument samples. An exhaustive search through the parameter space was used enabling a systematic analysis of the impact of each parameter, as well as reporting the most generally applicable parameter settings for the onset detectors and the fusion. We demonstrate improved results attributed to both fusion and the optimised parameter settings. © Mi Tian, György Fazekas, Dawn A. A. Black, Mark Sandler."
Driedger J.; Müller M.; Disch S.,Extending harmonic-percussive separation of audio signals,2014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946012233&partnerID=40&md5=bc7af7482188c47fcf588a68540dc1be,"Driedger J., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany; Disch S., Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany","In recent years, methods to decompose an audio signal into a harmonic and a percussive component have received a lot of interest and are frequently applied as a processing step in a variety of scenarios. One problem is that the computed components are often not of purely harmonic or percussive nature but also contain noise-like sounds that are neither clearly harmonic nor percussive. Furthermore, depending on the parameter settings, one often can observe a leakage of harmonic sounds into the percussive component and vice versa. In this paper we present two extensions to a state-of-the-art harmonic-percussive separation procedure to target these problems. First, we introduce a separation factor parameter into the decomposition process that allows for tightening separation results and for enforcing the components to be clearly harmonic or percussive. As second contribution, inspired by the classical sines+transients+noise (STN) audio model, this novel concept is exploited to add a third residual component to the decomposition which captures the sounds that lie in between the clearly harmonic and percussive sounds of the audio signal. © Jonathan Driedger, Meinard Müller, and Sascha Disch."
Schedl M.; Flexer A.,Putting the user in the center of music information retrieval,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476088&partnerID=40&md5=9e2be1471e00759ed3ade60118c08ef8,"Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Personalized and context-aware music retrieval and recommendation algorithms ideally provide music that perfectly fits the individual listener in each imaginable situation and for each of her information or entertainment need. Although first steps towards such systems have recently been presented at ISMIR and similar venues, this vision is still far away from being a reality. In this paper, we investigate and discuss literature on the topic of user-centric music retrieval and reflect on why the breakthrough in this field has not been achieved yet. Given the different expertises of the authors, we shed light on why this topic is a particularly challenging one, taking a psychological and a computer science view. Whereas the psychological point of view is mainly concerned with proper experimental design, the computer science aspect centers on modeling and machine learning problems. We further present our ideas on aspects vital to consider when elaborating user-aware music retrieval systems, and we also describe promising evaluation methodologies, since accurately evaluating personalized systems is a notably challenging task. © 2012 International Society for Music Information Retrieval."
Jure L.; Lopez E.; Rocamora M.; Cancela P.; Sponton H.; Irigaray I.,Pitch content visualization tools for music performance analysis,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448692&partnerID=40&md5=509c7f500bf2e4a965ec3854b17dad50,"Jure L., School of Music, Universidad de la República, Uruguay; Lopez E., Faculty of Engineering, Universidad de la República, Uruguay; Rocamora M., School of Music, Universidad de la República, Uruguay, Faculty of Engineering, Universidad de la República, Uruguay; Cancela P., Faculty of Engineering, Universidad de la República, Uruguay; Sponton H., Faculty of Engineering, Universidad de la República, Uruguay; Irigaray I., Faculty of Engineering, Universidad de la República, Uruguay","This work deals with pitch content visualization tools for the analysis of music performance from audio recordings. An existing computational method for the representation of pitch contours is briefly reviewed. Its application to music analysis is exemplified with two pieces of non-notated music: a field recording of a folkloric form of polyphonic singing and a commercial recording by a noted blues musician. Both examples have vocal parts exhibiting complex pitch evolution, difficult to analyze and notate with precision using Western common music notation. By using novel time-frequency analysis techniques that improve the location of the components of a harmonic sound, the melodic content representation implemented here allows a detailed study of aspects related to pitch intonation and tuning. This in turn permits an objective measurement of essential musical characteristics that are difficult or impossible to properly evaluate by subjective perception alone, and which are often not accounted for in traditional mu-sicological analysis. Two software tools are released that allow the practical use of the described methods. © 2012 International Society for Music Information Retrieval."
Coviello E.; Vaizman Y.; Chan A.B.; Lanckriet G.R.G.,Multivariate autoregressive mixture models for music auto-tagging,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447086&partnerID=40&md5=c4a79dc65f994e202191d26535eebf01,"Coviello E., University of California, San Diego, United States; Vaizman Y., University of California, San Diego, United States; Chan A.B., City University of Hong Kong, Hong Kong; Lanckriet G.R.G., University of California, San Diego, United States","We propose the multivariate autoregressive model for content based music auto-tagging. At the song level our approach leverages the multivariate autoregressive mixture (ARM) model, a generative time-series model for audio, which assumes each feature vector in an audio fragment is a linear function of previous feature vectors. To tackle tagmodel estimation, we propose an efficient hierarchical EM algorithm for ARMs (HEM-ARM), which summarizes the acoustic information common to the ARMs modeling the individual songs associated with a tag. We compare the ARM model with the recently proposed dynamic texture mixture (DTM) model. We hence investigate the relative merits of different modeling choices for music time-series: i) the flexibility of selecting higher memory order in ARM, ii) the capability of DTM to learn specific frequency basis for each particular tag and iii) the effect of the hidden layer of the DT versus the time efficiency of learning and inference with fully observable AR components. Finally, we experiment with a support vector machine (SVM) approach that classifies songs based on a kernel calculated on the frequency responses of the corresponding song ARMs. We show that the proposed approach outperforms SVMs trained on a different kernel function, based on a competing generative model. © 2012 International Society for Music Information Retrieval."
Schmidt E.M.; Kim Y.E.,Learning rhythm and melody features with deep belief networks,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905407267&partnerID=40&md5=287eb50d64e9d1181f749a0f06b1529e,"Schmidt E.M., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States","Deep learning techniques provide powerful methods for the development of deep structured projections connecting multiple domains of data. But the fine-tuning of such networks for supervised problems is challenging, and many current approaches are therefore heavily reliant on pre-training, which consists of unsupervised processing on the input observation data. In previous work, we have investigated using magnitude spectra as the network observations, finding reasonable improvements over standard acoustic representations. However, in necessarily supervised problems such as music emotion recognition, there is no guarantee that the starting points for optimization are anywhere near optimal, as emotion is unlikely to be the most dominant aspect of the data. In this new work, we develop input representations using harmonic/percussive source separation designed to inform rhythm and melodic contour. These representations are beat synchronous, providing an event-driven representation, and potentially the ability to learn emotion informative representations from pre-training alone. In order to provide a large dataset for our pre-training experiments, we select a subset of 50,000 songs from the Million Song Dataset, and employ their 30-60 second preview clips from 7digital to compute our custom feature representations. © 2013 International Society for Music Information Retrieval."
Collins N.,Influence in early electronic dance music: An audio content analysis investigation,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873469306&partnerID=40&md5=a3007a955dac11ac58729720438d412d,"Collins N., University of Sussex, United Kingdom","Audio content analysis can assist investigation of musical influence, given a corpus of date-annotated works. We study a number of techniques which illuminate musicological questions on genre and creative influence. By applying machine learning tests and statistical analysis to a database of early EDM tracks, we examine how distinct putatively different musical genres really are, the retrospectively labelled Detroit techno and Chicago house being the core case study. Further, by building predictive models based on works from earlier years, both by a priori assumed genre groups and by individual tracks, we examine questions of influence, and whether Detroit techno really is a sort of electronic future funk, and Chicago house an electronic extension of disco. We discuss the implications and prospects for modeling musical influence. © 2012 International Society for Music Information Retrieval."
Wülfing J.; Riedmiller M.,Unsupervised learning of local features for music classification,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465585&partnerID=40&md5=a1ab7025bce34f99d14bb270ebe83919,"Wülfing J., University of Freiburg, Germany; Riedmiller M., University of Freiburg, Germany","In this work we investigate the applicability of unsuper-vised feature learning methods to the task of automatic genre prediction of music pieces. More specifically we evaluate a framework that recently has been successfully used to recognize objects in images. We first extract local patches from the time-frequency transformed audio signal, which are then pre-processed and used for unsupervised learning of an overcomplete dictionary of local features. For learning we either use a bootstrapped k-means clustering approach or select features randomly. We further extract feature responses in a convolutional manner and train a linear SVM for classification. We extensively evaluate the approach on the GTZAN dataset, emphasizing the influence of important design choices such as dimensionality reduction, pooling and patch dimension on the classification accuracy. We show that convolutional extraction of local feature responses is crucial to reach high performance. Furthermore we find that using this approach, simple and fast learning techniques such as k-means or randomly selected features are competitive with previously published results which also learn features from audio signals. © 2012 International Society for Music Information Retrieval."
Joder C.; Schuller B.,Score-informed leading voice separation from monaural audio,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444645&partnerID=40&md5=f6c7123964c36026d51922c5a987f84a,"Joder C., Institute for Human-Machine Communication, Technische Universität, München, Germany; Schuller B., Institute for Human-Machine Communication, Technische Universität, München, Germany","Separating the leading voice from a musical recording seems to be natural to the human ear. Yet, it remains a difficult problem for automatic systems, in particular in the blind case, where no information is known about the signal. However, in the case where a musical score is available, one can take advantage of this additional information. In this paper, we present a novel application of this idea for leading voice separation exploiting a temporally-aligned MIDI Score. The model used is based on Nonnegative Matrix Factorization (NMF), whose solo part is represented by a source-filter model. We exploit the score information by constraining the source activations to conform to the aligned MIDI file. Experiments run on a database of real popular songs show that the use of these constraints can significantly improve the separation quality, in terms of both signal-based and perceptual evaluation metrics. © 2012 International Society for Music Information Retrieval."
Bryan N.J.; Mysore G.J.; Wang G.,Source separation of polyphonic music with interactive user-feedback on a piano roll display,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946069010&partnerID=40&md5=5a022d7853c9c71867b3aa698e26ab42,"Bryan N.J., CCRMA, Stanford University, United States; Mysore G.J., Adobe Research, United States; Wang G., CCRMA, Stanford University, United States","The task of separating a single recording of a polyphonic instrument (e.g. piano, guitar, etc.) into distinctive pitch tracks is challenging. One promising class of methods to accomplish this task is based on non-negative matrix factorization (NMF). Such methods, however, are still far from perfect. Distinct pitches from a single instrument have similar timbre, similar note attacks, and contain overlapping harmonics that all make separation difficult. In an attempt to overcome these issues, we use a database of synthesized piano and guitar recordings to learn the harmonic structure of distinct pitches, perform NMF-based separation, and then extend the method to allow an end-user to interactively correct for errors in the output separation estimates by drawing on a piano roll display of the separated tracks. The user-annotations are mapped to linear grouping regularization parameters within a modified NMF-based algorithm and are then used to refine the separation estimates in an iterative manner. For evaluation, a prototype user-interface was built and used to separate several polyphonic guitar and piano recordings. Initial results show that the method of interactive feedback can significantly increase the separation quality and produce high-quality separation results. © 2013 International Society for Music Information Retrieval."
Hu X.; Lee J.H.,A Cross-cultural study of music mood perception between American and Chinese listeners,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451903&partnerID=40&md5=70ce9dd3402bb05813077555a2d095e3,"Hu X., Faculty of Education, University of Hong Kong, Hong Kong; Lee J.H., Information School, University of Washington, United States","Music mood has been recognized as an important access point for music and many online music services support browsing by mood. However, how people judge music mood has not been well studied in the Music Information Retrieval (MIR) domain. In particular, people's cultural background is often assumed to be an important factor in music mood perception, but this assumption has not been verified by empirical studies. This paper reports on a study comparing mood judgments on a set of 30 songs by American and Chinese people. Results show that mood judgments do indeed differ between American and Chinese respondents. Furthermore, respondents' mood judgments tended to agree more with other respondents from the same culture than those from the other group. Both the song characteristics (e.g., genre, lyrical or instrumental) and the non-cultural background of the respondents (e.g., age, gender, familiarity with the songs) were analyzed to further examine the difference in mood judgments. Findings of this study help further our understanding on how cultural background affects mood perception. Also discussed in this paper are implications for designing MIR systems for cross-cultural music mood classification and recommendation. © 2012 International Society for Music Information Retrieval."
Mayer R.; Rauber A.,Towards time-resilient MIR processes,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873474989&partnerID=40&md5=347a2414cbe08ae2864ff94c07070b87,"Mayer R., Secure Business Austria, 1040 Vienna, Favoritenstrasse 16, Austria; Rauber A., Secure Business Austria, 1040 Vienna, Favoritenstrasse 16, Austria","In experimental sciences, under which we may likely subsume most research areas in MIR, repeatability is one of the key cornerstones of validating research and measuring progress. Yet, due to the complexity of typical MIR experiments, ensuring the capability of re-running any experiment, achieving exactly identical outputs is challenging at best. Performance differences observed may be attributed to incomplete documentation of the process, slight variations in data (preprocessing) or software libraries used, and others. Digital preservation aims at keeping digital objects authentically accessible and usable over long time spans. While traditionally focussed on individual objects, research is now moving towards the preservation of entire processes. In this paper we present the challenges of preserving a classical MIR process, i.e. music genre classifications, discuss the kinds of context information to be captured, as well as means to validate the re-execution of a preserved process. © 2012 International Society for Music Information Retrieval."
Proutskva P.; Rhodes C.; Wiggins G.; Crawford T.,Breathy or resonant - A controlled and curated dataset for phonation mode detection in singing,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448726&partnerID=40&md5=cfb6073ec7503e7fa00cbfe7f15f1b16,"Proutskva P., Goldsmiths, University of London, United Kingdom; Rhodes C., Goldsmiths, University of London, United Kingdom; Wiggins G., Queen Mary, University of London, United Kingdom; Crawford T., Goldsmiths, University of London, United Kingdom","This paper presents a new reference dataset of sustained, sung vowels with attached labels indicating the phonation mode. The dataset is intended for training computational models for automated phonation mode detection. Four phonation modes are distinguished by Johan Sundberg [15]: breathy, neutral, flow (or resonant) and pressed. The presented dataset consists of ca. 700 recordings of nine vowels from several languages, sung at various pitches in various phonation modes. The recorded sounds were produced by one female singer under controlled conditions, following recommendations by voice acoustics researchers. While datasets on phonation modes in speech exist, such resources for singing are not available. Our dataset closes this gap and offers researchers in various disciplines a reference and a training set. It will be made available online under Creative Commons license. Also, the format of the dataset is extensible. Further content additions and future support for the dataset are planned. © 2012 International Society for Music Information Retrieval."
Mauch M.; Dixon S.,A corpus-based study of rhythm patterns,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451988&partnerID=40&md5=c72946f851a5e906f3ce72f958257941,"Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We present a corpus-based study of musical rhythm, based on a collection of 4.8 million bar-length drum patterns extracted from 48, 176 pieces of symbolic music. Approaches to the analysis of rhythm in music information retrieval to date have focussed on low-level features for retrieval or on the detection of tempo, beats and drums in audio recordings. Musicological approaches are usually concerned with the description or implementation of man-made music theories. In this paper, we present a quantitative bottom-up approach to the study of rhythm that relies upon well-understood statistical methods from natural language processing. We adapt these methods to our corpus of music, based on the realisation that - unlike words - bar-length drum patterns can be systematically decomposed into sub-patterns both in time and by instrument. We show that, in some respects, our rhythm corpus behaves like natural language corpora, particularly in the sparsity of vocabulary. The same methods that detect word collocations allow us to quantify and rank idiomatic combinations of drum patterns. In other respects, our corpus has properties absent from language corpora, in particular, the high amount of repetition and strong mutual information rates between drum instruments. Our findings may be of direct interest to musicians and musicologists, and can inform the design of ground truth corpora and computational models of musical rhythm. © 2012 International Society for Music Information Retrieval."
Volk A.; Bas de Haas W.,A corpus-based study on ragtime syncopation,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956796322&partnerID=40&md5=cba038fe2208516e290c21527e495c4d,"Volk A., Utrecht University, Netherlands; Bas de Haas W., Utrecht University, Netherlands","This paper presents a corpus-based study on syncopation patterns in ragtime. We discuss open questions on the ragtime genre and the potential of computational tools in addressing these questions, contributing to the fields of Musicology and Music Information Retrieval (MIR), and giving back to the ragtime enthusiasts community. We introduce the RAG-collection of around 11000 ragtime MIDI files collected, organised, and distributed by many ragtime lovers around the world. The collection is accompanied by a compendium, providing useful metadata on ragtime compositions. Using this collection and the compendium, we investigate syncopation patterns in ragtime melodies, for which we tailored a melody extraction algorithm. We test and confirm musicological hypotheses about the occurrence of syncopation patterns that are considered typical for ragtime on the extracted melodies. Thus, the paper presents a first step towards modelling typical characteristics of the ragtime genre, which is an important means for enabling automatic genre classification. © 2013 International Society for Music Information Retrieval."
Yoshii K.; Tomioka R.; Mochihashi D.; Goto M.,Beyond NMF: Time-domain audio source separation without phase reconstruction,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937854467&partnerID=40&md5=8519f297cf20d3e0dcd6dbc1dfb6e37b,"Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Tomioka R., University of Tokyo, Japan; Mochihashi D., Institute of Statistical Mathematics (ISM), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a new fundamental technique for source separation of single-channel audio signals. Although nonnegative matrix factorization (NMF) has recently become very popular for music source separation, it deals only with the amplitude or power of the spectrogram of a given mixture signal and completely discards the phase. The component spectrograms are typically estimated using a Wiener filter that reuses the phase of the mixture spectrogram, but such rough phase reconstruction makes it hard to recover high-quality source signals because the estimated spectrograms are inconsistent, i.e., they do not correspond to any real time-domain signals. To avoid the frequency-domain phase reconstruction, we use positive semidefinite tensor factorization (PSDTF) for directly estimating source signals from the mixture signal in the time domain. Since PSDTF is a natural extension of NMF, an efficient multiplicative update algorithm for PSDTF can be derived. Experimental results show that PSDTF outperforms conventional NMF variants in terms of source separation quality. © 2013 International Society for Music Information Retrieval."
Bay M.; Ehmann A.F.; Beauchamp J.W.; Smaragdis P.; Stephen Downie J.,Second fiddle is important too: Pitch tracking individual voices in polyphonic music,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461145&partnerID=40&md5=b206d528253cd6c251a0b739564410be,"Bay M., Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Ehmann A.F., Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Beauchamp J.W., Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Smaragdis P., Department of Computer Science, University of Illinois, Urbana-Champaign, United States, Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Stephen Downie J., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Recently, there has been much interest in automatic pitch estimation and note tracking of polyphonic music. To date, however, most techniques produce a representation where pitch estimates are not associated with any particular instrument or voice. Therefore, the actual tracks for each instrument are not readily accessible. Access to individual tracks is needed for more complete music transcription and additionally will provide a window to the analysis of higher constructs such as counterpoint and instrument theme imitation during a composition. In this paper, we present a method for tracking the pitches (F0s) of individual instruments in polyphonic music. The system uses a pre-learned dictionary of spectral basis vectors for each note for a variety of musical instruments. The method then formulates the tracking of pitches of individual voices in a probabilistic manner by attempting to explain the input spectrum as the most likely combination of musical instruments and notes drawn from the dictionary. The method has been evaluated on a subset of the MIREX multiple-F0 estimation test dataset, showing promising results. © 2012 International Society for Music Information Retrieval."
Ross J.C.; Vinutha T.P.; Rao P.,Detecting melodic motifs from audio for Hindustani Classical Music,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455677&partnerID=40&md5=292d8fcf6125dee3ba202293ed1beacc,"Ross J.C., Department of Computer Science and Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India; Vinutha T.P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India","Melodic motifs form essential building blocks in Indian Classical music. The motifs, or key phrases, provide strong cues to the identity of the underlying raga in both Hindustani and Carnatic styles of Indian music. Thus the automatic detection of such recurring basic melodic shapes from audio is of relevance in music information retrieval. The extraction of melodic attributes from polyphonic audio and the variability inherent in the performance, which does not follow a predefined score, make the task particularly challenging. In this work, we consider the segmentation of selected melodic motifs from audio signals by computing similarity measures on time series of automatically detected pitch values. The methods are investigated in the context of detecting the signature phrase of Hindustani vocal music compositions (bandish) within and across performances. © 2012 International Society for Music Information Retrieval."
Wu D.,"Simultaneous unsupervised learning of flamenco metrical structure, hypermetrical structure, and multipart structural relations",2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973884765&partnerID=40&md5=0b5e640cfd87a618433f257ed6dc2717,"Wu D., HKUST, Human Language Technology Center, Department of CSE, Hong Kong","We show how a new unsupervised approach to learning musical relationships can exploit Bayesian MAP induction of stochastic transduction grammars to overcome the challenges of learning complex relationships between multiple rhythmic parts that previously lay outside the scope of general computational approaches to music structure learning. A good illustrative genre is flamenco, which employs not only regular but also irregular hypermetrical structures that rapidly switch between 3/4 and 6/8 mediocompas blocks. Moreover, typical flamenco idioms employ heavy syncopation and sudden, misleading off-beat accents and patterns, while often elliding the downbeat accents that humans as well as existing meter-finding algorithms rely on, thus creating a high degree of listener “surprise” that makes not only the structural relations, but even the metrical structure itself, ellusive to learn. Flamenco musicians rely on both complex regular hypermetrical knowledge as well as irregular real-time clues to recognize when to switch meters and patterns. Our new approach envisions this as an integrated problem of learning a bilingual transduction, i.e., a structural relation between two languages—where there are different musical languages of, say, flamenco percussion versus zapateado footwork or palmas hand clapping. We apply minimum description length criteria to induce transduction grammars that simultaneously learn (1) the multiple metrical structures, (2) the hypermetrical structure that stochastically governs meter switching, and (3) the probabilistic transduction relationship between patterns of different rhythmic languages that enables musicians to predict when to switch meters and how to select patterns depending on what fellow musicians are generating. © 2013 International Society for Music Information Retrieval."
Humphrey E.J.; Bello J.P.; LeCun Y.,Moving beyond feature design: Deep architectures and automatic feature learning in music informatics,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453413&partnerID=40&md5=600c912ce2d2d2e2fcaf37c5277b700f,"Humphrey E.J., Music and Audio Research Lab., NYU, United States; Bello J.P., Music and Audio Research Lab., NYU, United States; LeCun Y., Courant School of Computer Science, NYU, United States","The short history of content-based music informatics research is dominated by hand-crafted feature design, and our community has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding diminishing returns. This deceleration is largely due to the tandem of heuristic feature design and shallow processing architectures. We systematically discard hopefully irrelevant information while simultaneously calling upon creativity, intuition, or sheer luck to craft useful representations, gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of deep learning, it has only recently started to be explored in our field. By reviewing deep architectures and feature learning, we hope to raise awareness in our community about alternative approaches to solving MIR challenges, new and old alike. © 2012 International Society for Music Information Retrieval."
Sarroff A.M.; Casey M.,Groove kernels as rhythmic-acoustic motif descriptors,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977121228&partnerID=40&md5=5d4ce3cb60cead76990699c961c3fc61,"Sarroff A.M., Dartmouth College, Department of Computer Science, United States; Casey M., Dartmouth College, Department of Computer Science, United States","The “groove” of a song correlates with enjoyment and bodily movement. Recent work has shown that humans often agree whether a song does or does not have groove and how much groove a song has. It is therefore useful to develop algorithms that characterize the quality of groove across songs. We evaluate three unsupervised tempo-invariant models for measuring pairwise musical groove similarity: A temporal model, a timbre-temporal model, and a pitch-timbre-temporal model. The temporal model uses a rhythm similarity metric proposed by Holzapfel and Stylianou, while the timbre-inclusive models are built on shift invariant probabilistic latent component analysis. We evaluate the models using a dataset of over 8000 real-world musical recordings spanning approximately 10 genres, several decades, multiple meters, a large range of tempos, and Western and non-Western localities. A blind perceptual study is conducted: given a random music query, humans rate the groove similarity of the top three retrievals chosen by each of the models, as well as three random retrievals. © 2013 International Society for Music Information Retrieval."
Grachten M.; Gasser M.; Arzt A.; Widmer G.,Automatic alignment of music performances with structural differences,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973344156&partnerID=40&md5=b46fc8aff98534dbbb82d75d7155195c,"Grachten M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Arzt A., Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria","Both in interactive music listening, and in music performance research, there is a need for automatic alignment of different recordings of the same musical piece. This task is challenging, because musical pieces often contain parts that may or may not be repeated by the performer, possibly leading to structural differences between performances (or between performance and score). The most common alignment method, dynamic time warping (DTW), cannot handle structural differences adequately, and existing approaches to deal with structural differences explicitly rely on the annotation of “break points” in one of the sequences. We propose a simple extension of the Needleman-Wunsch algorithm to deal effectively with structural differences, without relying on annotations. We evaluate several audio features for alignment, and show how an optimal value can be found for the cost-parameter of the alignment algorithm. A single cost value is demonstrated to be valid across different types of music. We demonstrate that our approach yields roughly equal alignment accuracies compared to DTW in the absence of structural differences, and superior accuracies when structural differences occur. © 2013 International Society for Music Information Retrieval."
Cunningham S.J.; Lee J.H.,Influences of ISMIR and mirex research on technology patents,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907053036&partnerID=40&md5=02fd7217223e9c5188bedc4b68685821,"Cunningham S.J., Department of Computer Science, University of Waikato, New Zealand; Lee J.H., Information School, University of Washington, United States","Much of the current Music Information Retrieval (MIR) research aims to contribute to the field by creating practical music applications or algorithms that can be used as part of such applications. Understanding how academic research results influence and translate to commercial products can be useful for MIR researchers, especially when we try to measure the impact of our research. This study aims to improve our understanding of the commercial influence of academic MIR research by analyzing the patents citing publications from ISMIR (International Society for Music Information Retrieval) Conference proceedings and its associated MIREX (Music Information Retrieval Evaluation eXchange) MIR algorithm trials. In this paper, we provide our preliminary analyses of the relevant patents as well as the ISMIR publications that are referenced in those patents. © 2013 International Society for Music Information Retrieval."
Grosche P.; Serrà J.; Müller M.; Arcos J.L.,Structure-based audio fingerprinting for music retrieval,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451912&partnerID=40&md5=5ade038283f724198ad1a763140b7d7f,"Grosche P., Saarland University, Germany, MPI Informatik, Germany; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Spain; Müller M., Bonn University, Germany, MPI Informatik, Germany; Arcos J.L., Artificial Intelligence Research Institute (IIIA-CSIC), Spain","Content-based approaches to music retrieval are of great relevance as they do not require any kind of manually generated annotations. In this paper, we introduce the concept of structure fingerprints, which are compact descriptors of the musical structure of an audio recording. Given a recorded music performance, structure fingerprints facilitate the retrieval of other performances sharing the same underlying structure. Avoiding any explicit determination of musical structure, our fingerprints can be thought of as a probability density function derived from a self-similarity matrix. We show that the proposed fingerprints can be compared by using simple Euclidean distances without using any kind of complex warping operations required in previous approaches. Experiments on a collection of Chopin Mazurkas reveal that structure fingerprints facilitate robust and efficient content-based music retrieval. Furthermore, we give a musically informed discussion that also deepens the understanding of this popular Mazurka dataset. © 2012 International Society for Music Information Retrieval."
Liang D.; Hoffman M.D.; Ellis D.P.W.,Beta process sparse nonnegative matrix factorization for music,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973372047&partnerID=40&md5=7fc7fa4de889b2e7b90fd2c541579599,"Liang D., LabROSA, EE Dept., Columbia University, United States; Hoffman M.D., Adobe Research, Adobe Systems Incorporated, United States; Ellis D.P.W., Adobe Research, Adobe Systems Incorporated, United States","Nonnegative matrix factorization (NMF) has been widely used for discovering physically meaningful latent components in audio signals to facilitate source separation. Most of the existing NMF algorithms require that the number of latent components is provided a priori, which is not always possible. In this paper, we leverage developments from the Bayesian nonparametrics and compressive sensing literature to propose a probabilistic Beta Process Sparse NMF (BP-NMF) model, which can automatically infer the proper number of latent components based on the data. Unlike previous models, BP-NMF explicitly assumes that these latent components are often completely silent. We derive a novel mean-field variational inference algorithm for this nonconjugate model and evaluate it on both synthetic data and real recordings on various tasks. © 2013 International Society for Music Information Retrieval."
Barthet M.; Marston D.; Baume C.; Fazekas G.; Sandler M.,Design and evaluation of semantic mood models for music recommendation,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904287489&partnerID=40&md5=685f4546ab8913864c96431a0aa7bd5d,"Barthet M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Marston D., BBC R and D London, United Kingdom; Baume C., BBC R and D London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this paper we present and evaluate two semantic music mood models relying on metadata extracted from over 180,000 production music tracks sourced from I Like Music (ILM)’s collection. We performed non-metric multidimensional scaling (MDS) analyses of mood stem dissimilarity matrices (1 to 13 dimensions) and devised five different mood tag summarisation methods to map tracks in the dimensional mood spaces. We then conducted a listening test to assess the ability of the proposed models to match tracks by mood in a recommendation task. The models were compared against a classic audio content-based similarity model relying on Mel Frequency Cepstral Coefficients (MFCCs). The best performance (60% of correct match, on average) was yielded by coupling the five-dimensional MDS model with the term-frequency weighted tag centroid method to map tracks in the mood space. © 2013 International Society for Music Information Retrieval."
Scott J.; Kim Y.E.,Instrument identification informed multi-track mixing,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983139327&partnerID=40&md5=cc0b064a7582bb5527151467c0f84651,"Scott J., Music and Entertainment Technology Laboratory (MET-lab), Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab), Electrical and Computer Engineering, Drexel University, United States","Although digital music production technology has become more accessible over the years, the tools are complex and often difficult to navigate, resulting in a large learning curve for new users. This paper approaches the task of automated multi-track mixing from the perspective of applying common practices based on the instrument types present in a mixture. We apply basic principles to each track automatically, varying the parameters of gain, stereo panning, and coarse equalization. Assuming all instruments are known, a small listening evaluation is completed on the mixed tracks to validate the assumptions of the mixing model. This work represents an exploratory analysis into the efficacy of a hierarchical approach to multi-track mixing using instrument class as a guide to processing techniques. © 2013 International Society for Music Information Retrieval."
Gärtner D.,Tempo detection of urban music using tatum grid non-negative matrix factorization,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904331538&partnerID=40&md5=1719006c91a2a55b3db450063328f3b9,"Gärtner D., Fraunhofer Institute for Media Technology IDMT, Germany","High tempo detection accuracies have been reported for the analysis of percussive, constant-tempo, Western music audio signals. As a consequence, active research in the tempo detection domain has been shifted to yet open tasks like tempo analysis of non-percussive, expressive, or non-western music. Also, tempo detection is included in a large range of music-related software. In DJ software, features like beat-synching or tempo-synchronized sound effects are widely accepted in the DJ community, and their users rely on correct tempo hypothesis as their basis. In this paper, we are evaluating both academic and commercial tempo detection systems on a typical dataset of an urban club music DJ. Based on this evaluation, we identify octave errors as a problem that has not yet been solved. Further, an approach based on non-negative matrix factorization is presented. In its current state it can compete with the state of the art. It further provides a foundation to tackle the octave error issue in future research. © 2013 International Society for Music Information Retrieval."
Cabredo R.; Legaspi R.; Inventado P.S.; Numao M.,An emotion model for music using brain waves,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873452532&partnerID=40&md5=8bf8ba79afe59a8e512ce8aa75f58b86,"Cabredo R., Institute of Scientific and Industrial Research, Osaka University, Japan, Center for Empathic Human-Computer Interactions, De La Salle University, Philippines; Legaspi R., Institute of Scientific and Industrial Research, Osaka University, Japan; Inventado P.S., Institute of Scientific and Industrial Research, Osaka University, Japan, Center for Empathic Human-Computer Interactions, De La Salle University, Philippines; Numao M., Institute of Scientific and Industrial Research, Osaka University, Japan",Every person reacts differently to music. The task then is to identify a specific set of music features that have a significant effect on emotion for an individual. Previous research have used self-reported emotions or tags to annotate short segments of music using discrete labels. Our approach uses an electroencephalograph to record the subject's reaction to music. Emotion spectrum analysis method is used to analyse the electric potentials and provide continuous-valued annotations of four emotional states for different segments of the music. Music features are obtained by processing music information from the MIDI files which are separated into several segments using a windowing technique. The music features extracted are used in two separate supervised classification algorithms to build the emotion models. Classifiers have a minimum error rate of 5% predicting the emotion labels. © 2012 International Society for Music Information Retrieval.
Böck S.; Krebs F.; Schedl M.,Evaluating the online capabilities of onset detection methods,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873481777&partnerID=40&md5=519212661037d48cff2abb69c8f11de2,"Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper, we evaluate various onset detection algorithms in terms of their online capabilities. Most methods use some kind of normalization over time, which renders them unusable for online tasks. We modified existing methods to enable online application and evaluated their performance on a large dataset consisting of 27, 774 annotated onsets. We focus particularly on the incorporated preprocessing and peak detection methods. We show that, with the right choice of parameters, the maximum achievable performance is in the same range as that of offline algorithms, and that preprocessing can improve the results considerably. Furthermore, we propose a new onset detection method based on the common spectral flux and a new peak-picking method which outperforms traditional methods both online and offline and works with of various volume levels. © 2012 International Society for Music Information Retrieval."
Mauch M.; Ewert S.,The audio degradation toolbox and its application to robustness evaluation,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905267530&partnerID=40&md5=7fcc826576d3181014bdcc7133d80440,"Mauch M., Queen Mary University of London, Centre for Digital Music, United Kingdom; Ewert S., Queen Mary University of London, Centre for Digital Music, United Kingdom","We introduce the Audio Degradation Toolbox (ADT) for the controlled degradation of audio signals, and propose its usage as a means of evaluating and comparing the robustness of audio processing algorithms. Music recordings encountered in practical applications are subject to varied, sometimes unpredictable degradation. For example, audio is degraded by low-quality microphones, noisy recording environments, MP3 compression, dynamic compression in broadcasting or vinyl decay. In spite of this, no standard software for the degradation of audio exists, and music processing methods are usually evaluated against clean data. The ADT fills this gap by providing Matlab scripts that emulate a wide range of degradation types. We describe 14 degradation units, and how they can be chained to create more complex, ‘real-world’ degradations. The ADT also provides functionality to adjust existing ground-truth, correcting for temporal distortions introduced by degradation. Using four different music informatics tasks, we show that performance strongly depends on the combination of method and degradation applied. We demonstrate that specific degradations can reduce or even reverse the performance difference between two competing methods. ADT source code, sounds, impulse responses and definitions are freely available for download. © 2013 International Society for Music Information Retrieval."
Meredith D.,A geometric language for representing structure in polyphonic music,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873457544&partnerID=40&md5=80c16bd89ce283d7188c718d88ba1ffc,"Meredith D., Aalborg University, Denmark","In 1981, Deutsch and Feroe proposed a formal language for representing melodic pitch structure that employed the powerful concept of hierarchically-related pitch alphabets. However, neither rhythmic structure nor pitch structure in polyphonic music can be adequately represented using this language. A new language is proposed here that incorporates certain features of Deutsch and Feroe's model but extends and generalises it to allow for the representation of both rhythm and pitch structure in polyphonic music. The new language adopts a geometric approach in which a passage of polyphonic music is represented as a set of multidimensional points, generated by performing transformations on component patterns. The language introduces the concept of a periodic mask, a generalisation of Deutsch and Feroe's notion of a pitch alphabet, that can be applied to any dimension of a geometric representation, allowing for both rhythms and pitch collections to be represented parsimoniously in a uniform way. © 2012 International Society for Music Information Retrieval."
Terrell M.J.; Fazekas G.; Simpson A.J.R.; Smith J.; Dixon S.,Listening level changes music similarity,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873452097&partnerID=40&md5=13e64419ca525780eecc9dd319624722,"Terrell M.J., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Simpson A.J.R., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Smith J., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom","We examine the effect of listening level, i.e. the absolute sound pressure level at which sounds are reproduced, on music similarity, and in particular, on playlist generation. Current methods commonly use similarity metrics based on Mel-frequency cepstral coefficients (MFCCs), which are derived from the objective frequency spectrum of a sound. We follow this approach, but use the level-dependent auditory spectrum, evaluated using the loudness models of Glasberg and Moore, at three listening levels, to produce auditory spectrum cepstral coefficients (ASCCs). The AS-CCs are used to generate sets of playlists at each listening level, using a typical method, and these playlists were found to differ greatly. From this we conclude that music recommendation systems could be made more perceptually relevant if listening level information were included. We discuss the findings in relation to other fields within MIR where inclusion of listening level might also be of benefit. © 2012 International Society for Music Information Retrieval."
Müller M.; Prätzlich T.; Driedger J.,A Cross-version approach for stabilizing tempo-based novelty detection,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444305&partnerID=40&md5=97466c215cfed0af4c00e30068e2948c,"Müller M., Bonn University, Germany, MPI Informatik, Germany; Prätzlich T., Saarland University, Germany, MPI Informatik, Germany; Driedger J., Bonn University, Germany, Saarland University, Germany","The task of novelty detection with the objective of detecting changes regarding musical properties such as harmony, dynamics, timbre, or tempo is of fundamental importance when analyzing structural properties of music recordings. But for a specific audio version of a given piece of music, the novelty detection result may also crucially depend on the individual performance style of the musician. This particularly holds true for tempo-related properties, which may vary significantly across different performances of the same piece of music. In this paper, we show that tempo-based novelty detection can be stabilized and improved by simultaneously analyzing a set of different performances. We first warp the version-dependent novelty curves onto a common musical time axis, and then combine the individual curves to produce a single fusion curve. Our hypothesis is that musically relevant points of novelty tend to be consistent across different performances. This hypothesis is supported by our experiments in the context of music structure analysis, where the cross-version fusion curves yield, on average, better results than the novelty curves obtained from individual recordings. © 2012 International Society for Music Information Retrieval."
Sébastien V.; Ralambondrainy H.; Sébastien O.; Conruyt N.,Score Analyzer: Automatically determining scores difficulty level for instrumental e-learning,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462290&partnerID=40&md5=8ced487224c8f3fdcac2671ed2fd4ddf,"Sébastien V., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France; Ralambondrainy H., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France; Sébastien O., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France; Conruyt N., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France","Nowadays, huge sheet music collections exist on the Web, allowing people to access public domain scores for free. However, beginners may be lost in finding a score appropriate to their instrument level, and should often rely on themselves to start out on the chosen piece. In this instrumental e-Learning context, we propose a Score Analyzer prototype in order to automatically extract the difficulty level of a MusicXML piece and suggest advice thanks to a Musical Sign Base (MSB). To do so, we first review methods related to score performance information retrieval. We then identify seven criteria to characterize technical instrumental difficulties and propose methods to extract them from a MusicXML score. The relevance of these criteria is then evaluated through a Principal Components Analysis and compared to human estimations. Lastly we discuss the integration of this work to @-MUSE, a collaborative score annotation platform based on multimedia contents indexation. © 2012 International Society for Music Information Retrieval."
Koduri G.K.; Serrà J.; Serra X.,Characterization of intonation in Carnatic music by parametrizing pitch histograms,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449435&partnerID=40&md5=576af0e98824246419a65e423b0b6b15,"Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Intonation is an important concept in Carnatic music that is characteristic of a raaga, and intrinsic to the musical expression of a performer. In this paper we approach the description of intonation from a computational perspective, obtaining a compact representation of the pitch track of a recording. First, we extract pitch contours from automatically selected voice segments. Then, we obtain a a pitch histogram of its full pitch-range, normalized by the tonic frequency, from which each prominent peak is automatically labelled and parametrized. We validate such parame-trization by considering an explorative classification task: three raagas are disambiguated using the characterization of a single peak (a task that would seriously challenge a more naïve parametrization). Results show consistent improvements for this particular task. Furthermore, we perform a qualitative assessment on a larger collection of raa-gas, showing the discriminative power of the entire representation. The proposed generic parametrization of the intonation histogram should be useful for musically relevant tasks such as performer and instrument characterization. © 2012 International Society for Music Information Retrieval."
Rosão C.; Ribeiro R.; De Matos D.M.,Influence of Peak Selection methods on onset detection,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873458919&partnerID=40&md5=97920f6191a90e5706679868134cacbc,"Rosão C., ISCTE-IUL L2F, INESC-ID, Lisboa, Portugal; Ribeiro R., ISCTE-IUL L2F, INESC-ID, Lisboa, Portugal; De Matos D.M., IST/UTL, L2F/INESC-ID, Lisboa, Portugal","Finding the starting time of musical notes in an audio signal, that is, to perform onset detection, is an important task as this information can be used as the basis for high-level musical processing tasks. Many different methods exist to perform onset detection. However their results depend on a Peak Selection step that makes the decision whether an onset is present at some point in time. In this paper we review a number of different Peak Selection methods and compare their influence in the performance of different onset detection methods and on 4 distinct onset classes. Our results show that the post-processing method used deeply influences both positively and negatively the results obtained. © 2012 International Society for Music Information Retrieval."
Stober S.; Low T.; Gossen T.; Nürnberger A.,Incremental visualization of growing music collections,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911368782&partnerID=40&md5=684f9929b871dbabb22816d18bcd247a,"Stober S., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany; Low T., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany; Gossen T., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany; Nürnberger A., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany","Map-based visualizations – sometimes also called projections – are a popular means for exploring music collections. But how useful are they if the collection is not static but grows over time? Ideally, a map that a user is already familiar with should be altered as little as possible and only as much as necessary to reflect the changes of the underlying collection. This paper demonstrates to what extent existing approaches are able to incrementally integrate new songs into existing maps and discusses their technical limitations. To this end, Growing Self-Organizing Maps, (Landmark) Multidimensional Scaling, Stochastic Neighbor Embedding, and the Neighbor Retrieval Visualizer are considered. The different algorithms are experimentally compared based on objective quality measurements as well as in a user study with an interactive user interface. In the experiments, the well-known Beatles corpus comprising the 180 songs from the twelve official albums is used – adding one album at a time to the collection. © 2013 International Society for Music Information Retrieval."
Hockman J.A.; Davies M.E.P.; Fujinaga I.,"One in the jungle: Downbeat detection in hardcore, jungle, and drum and bass",2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873469004&partnerID=40&md5=0416b45c3cfed1bf35864061d88cd320,"Hockman J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, Canada, Distributed Digital Archives and Libraries (DDMAL), McGill University, Montreal, Canada; Davies M.E.P., Sound and Music Computing Group, INESC TEC, Porto, Portugal; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, Canada, Distributed Digital Archives and Libraries (DDMAL), McGill University, Montreal, Canada","Hardcore, jungle, and drum and bass (HJDB) are fast-paced electronic dance music genres that often employ resequenced breakbeats or drum samples from jazz and funk percussionist solos. We present a style-specific method for downbeat detection specifically designed for HJDB. The presented method combines three forms of metrical information in the prediction of downbeats: low-level onset event information; periodicity information from beat tracking; and high-level information from a regression model trained with classic breakbeats. In an evaluation using 206 HJDB pieces, we demonstrate superior accuracy of our style specific method over four general downbeat detection algorithms. We present this result to motivate the need for style-specific knowledge and techniques for improved downbeat detection. © 2012 International Society for Music Information Retrieval."
Benetos E.; Dixon S.; Giannoulis D.; Kirchhoff H.; Klapuri A.,Automatic music transcription: Breaking the glass ceiling,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447133&partnerID=40&md5=2933b8e9db00a73e86939a87bc176aa6,"Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Giannoulis D., Centre for Digital Music, Queen Mary University of London, United Kingdom; Kirchhoff H., Centre for Digital Music, Queen Mary University of London, United Kingdom; Klapuri A., Centre for Digital Music, Queen Mary University of London, United Kingdom","Automatic music transcription is considered by many to be the Holy Grail in the field of music signal analysis. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. In order to overcome the limited performance of transcription systems, algorithms have to be tailored to specific use-cases. Semiautomatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information across different methods and musical aspects. © 2012 International Society for Music Information Retrieval."
Orio N.; Piva R.,Combining timbric and rhythmic features for semantic music tagging,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951791111&partnerID=40&md5=6d83f696fd6899d3ab7732dffbd0b8d6,"Orio N., Department of Cultural Heritage, University of Padua, Italy; Piva R., Department of Information Engineering, University of Padua, Italy","In this paper we propose a novel approach to music tagging. The approach uses a statistical framework to model two acoustic features: timbre and rhythm. A collection of tagged music is thus represented as a graph where the states correspond to the songs and the models probabilities are related to the timbric and rhythmic similarity. Under the assumption that acoustically similar songs have similar tags, we infer the tags of a new song by adding it to the graph structure and observing the tags visited in acoustically meaningful random walks. The approach has been tested using the CAL500 dataset, with encouraging results in terms of precision. © 2013 International Society for Music Information Retrieval."
Şentürk S.; Gulati S.; Serra X.,Score informed tonic identification for makam music of Turkey,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926667499&partnerID=40&md5=6480ce699460454705a08dc657cb2ee8,"Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Tonic is a fundamental concept in many music traditions and its automatic identification should be relevant for establishing the reference pitch when we analyse the melodic content of the music. In this paper, we present two methodologies for the identification of the tonic in audio recordings of makam music of Turkey, both taking advantage of some score information. First, we compute a prominent pitch and a audio kernel-density pitch class distribution (KPCD) from the audio recording. The peaks in the KPCD are selected as tonic candidates. The first method computes a score KPCD from the monophonic melody extracted from the score. Then, the audio KPCD is circular-shifted with respect to each tonic candidate and compared with the score KPCD. The best matching shift indicates the estimated tonic. The second method extracts the monophonic melody of the most repetitive section of the score. Normalising the audio prominent pitch with respect to each tonic candidate, the method attempts to link the repetitive structural element given in the score with the respective time-intervals in the audio recording. The result producing the most confident links marks the estimated tonic. We have tested the methods on a dataset of makam music of Turkey, achieving a very high accuracy (94.9%) with the first method, and almost perfect identification (99.6%) with the second method. We conclude that score informed tonic identification can be a useful first step in the computational analysis (e.g. expressive analysis, intonation analysis, audio-score alignment) of music collections involving melody-dominant content. © 2013 International Society for Music Information Retrieval."
Kosta K.; Marchini M.; Purwins H.,Unsupervised chord-sequence generation from an audio example,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873468434&partnerID=40&md5=2c149440c7f57b8b6327d495bd7fbdd2,"Kosta K., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom, Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Spain; Marchini M., Neurotechnology Group, Berlin Institute of Technology, 10587 Berlin, Germany; Purwins H., Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Spain, Neurotechnology Group, Berlin Institute of Technology, 10587 Berlin, Germany","A system is presented that generates a sound sequence from an original audio chord sequence, having the following characteristics: The generation can be arbitrarily long, preserves certain musical characteristics of the original and has a reasonable degree of interestingness. The procedure comprises the following steps: 1) chord segmentation by onset detection, 2) representation as Constant Q Profiles, 3) multi-level clustering, 4) cluster level selection, 5) metrical analysis, 6) building of a suffix tree, 7) generation heuristics. The system can be seen as a computational model of the cognition of harmony consisting of an unsupervised formation of harmonic categories (via multilevel clustering) and a sequence learning module (via suffix trees) which in turn controls the harmonic categorization in a top-down manner (via a measure of regularity). In the final synthesis, the system recombines the audio material derived from the sample itself and it is able to learn various harmonic styles. The system is applied to various musical styles and is then evaluated subjectively by musicians and non-musicians, showing that it is capable of producing sequences that maintain certain musical characteristics of the original. © 2012 International Society for Music Information Retrieval."
Yang Y.-H.,Low-rank representation of both singing voice and music accompaniment via learned dictionaries,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946061883&partnerID=40&md5=5056c663295f7e43e37abc424e1b0aaa,"Yang Y.-H., Research Center for IT Innovation, Academia Sinica, Taiwan","Recent research work has shown that the magnitude spectrogram of a song can be considered as a superposition of a low-rank component and a sparse component, which appear to correspond to the instrumental part and the vocal part of the song, respectively. Based on this observation, one can separate singing voice from the background music. However, the quality of such separation might be limited, because the vocal part of a song can sometimes be low-rank as well. Therefore, we propose to learn the subspace structures of vocal and instrumental sounds from a collection of clean signals first, and then compute the low-rank representations of both the vocal and instrumental parts of a song based on the learned subspaces. Specifically, we use online dictionary learning to learn the subspaces, and propose a new algorithm called multiple low-rank representation (MLRR) to decompose a magnitude spectrogram into two low-rank matrices. Our approach is flexible in that the subspaces of singing voice and music accompaniment are both learned from data. Evaluation on the MIR-1K dataset shows that the approach improves the source-to-distortion ratio (SDR) and the source-to-interference ratio (SIR), but not the source-to-artifact ratio (SAR). © 2013 International Society for Music Information Retrieval."
Grohganz H.; Clausen M.; Jiang N.; Müller M.,Converting path structures into block structures using eigenvalue decompositions of self-similarity matrices,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905219825&partnerID=40&md5=a9b565743b0ffb5903098c75a61b8c76,"Grohganz H., Bonn University, Germany; Clausen M., Bonn University, Germany; Jiang N., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","In music structure analysis the two principles of repetition and homogeneity are fundamental for partitioning a given audio recording into musically meaningful structural elements. When converting the audio recording into a suitable self-similarity matrix (SSM), repetitions typically lead to path structures, whereas homogeneous regions yield block structures. In previous research, handling both structural elements at the same time has turned out to be a challenging task. In this paper, we introduce a novel procedure for converting path structures into block structures by applying an eigenvalue decomposition of the SSM in combination with suitable clustering techniques. We demonstrate the effectiveness of our conversion approach by showing that algorithms previously designed for homogeneity-based structure analysis can now be applied for repetition-based structure analysis. Thus, our conversion may open up novel ways for handling both principles within a unified structure analysis framework. © 2013 International Society for Music Information Retrieval."
Jin R.; Raphael C.,Interpreting rhythm in optical music recognition,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449670&partnerID=40&md5=784d578b0addf1624cd974e3765a1d53,"Jin R., School of Informatics and Computing, Indiana University, Bloomington, United States; Raphael C., School of Informatics and Computing, Indiana University, Bloomington, United States","We present a method for understanding the rhythmic content of a collection of identified symbols in optical music recognition, designed for polyphonic music. Our object of study is a measure of music symbols. Our model explains the symbols as a collection of voices, while the number of voices is variable throughout a measure. We introduce a dynamic programming framework that identifies the best-scoring interpretation subject to the constraint that each voice accounts for the musical time indicated by the known time signature. Our approach applies as well to the situation in which their are multiple possible hypotheses for each symbol, and thus combines interpretation with recognition in a top-down manner. We present experiments demonstrating a nearly 4-fold decrease in the number of false positive symbols with monophonic music, identify missing tuplets, and show preliminary results with polyphonic music. © 2012 International Society for Music Information Retrieval."
Bertin-Mahieux T.; Ellis D.P.W.,Large-scale cover song recognition using the 2D Fourier transform magnitude,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873468293&partnerID=40&md5=95fe816f8cfdf9cc0268281e8dce1137,"Bertin-Mahieux T., Columbia University, LabROSA, EE Dept., United States; Ellis D.P.W., Columbia University, LabROSA, EE Dept., United States","Large-scale cover song recognition involves calculating item-to-item similarities that can accommodate differences in timing and tempo, rendering simple Euclidean measures unsuitable. Expensive solutions such as dynamic time warping do not scale to million of instances, making them inappropriate for commercial-scale applications. In this work, we transform a beat-synchronous chroma matrix with a 2D Fourier transform and show that the resulting representation has properties that fit the cover song recognition task. We can also apply PCA to efficiently scale comparisons. We report the best results to date on the largest available dataset of around 18, 000 cover songs amid one million tracks, giving a mean average precision of 3.0%. © 2012 International Society for Music Information Retrieval."
Pugin L.; Crawford T.,Evaluating OMR on the early music online collection,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981550093&partnerID=40&md5=80e0528a43245e0e2bb4045395ddd667,"Pugin L., Swiss RISM Office, Switzerland; Crawford T., Goldsmiths College, University of London, United Kingdom","The Early Music Online (EMO) collection consists of about 300 printed music books of the sixteenth century held at the British Library. They were recently digitized from microfilms and made available online. In total, about 35,000 pages were digitized. This paper presents an optical music recognition (OMR) evaluation on the EMO collection. Firstly, the content of the collection is reviewed, looking at the type of music notation and the type of printing technique. Secondly, for the books for which it is possible (260 books), an OMR evaluation performed using the Aruspix OMR software application is presented. For each book, one randomly selected page of music was processed and the recognition rate was computed using a corrected transcription of the page. This evaluation shows very promising results for large-scale OMR on the EMO or similar collections. The paper also highlights critical points that should be taken into account in such an enterprise. © 2013 International Society for Music Information Retrieval."
Moore J.L.; Chen S.; Joachims T.; Turnbull D.,Taste over time: The temporal dynamics of user preferences,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956813216&partnerID=40&md5=bd93cd5864a558a51c41b881e09907c3,"Moore J.L., Cornell University, Dept. of Computer Science, United States; Chen S., Cornell University, Dept. of Computer Science, United States; Joachims T., Cornell University, Dept. of Computer Science, United States; Turnbull D., Ithaca College, Dept. of Computer Science, United States","We develop temporal embedding models for exploring how listening preferences of a population develop over time. In particular, we propose time-dynamic probabilistic embedding models that incorporate users and songs in a joint Eu-clidian space in which they gradually change position over time. Using large-scale Scrobbler data from Last.fm spanning a period of 8 years, our models generate trajectories of how user tastes changed over time, how artists developed, and how songs move in the embedded space. This ability to visualize and quantify listening preferences of a large population of people over a multi-year time period provides exciting opportunities for data-driven exploration of musicological trends and patterns. © 2013 International Society for Music Information Retrieval."
Dieleman S.; Schrauwen B.,Multiscale approaches to music audio feature learning,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947089220&partnerID=40&md5=a7b73e69140e46cefbd9032f1767f240,"Dieleman S., Electronics and Information Systems department, Ghent University, Belgium; Schrauwen B., Electronics and Information Systems department, Ghent University, Belgium","Content-based music information retrieval tasks are typically solved with a two-stage approach: features are extracted from music audio signals, and are then used as input to a regressor or classifier. These features can be engineered or learned from data. Although the former approach was dominant in the past, feature learning has started to receive more attention from the MIR community in recent years. Recent results in feature learning indicate that simple algorithms such as K-means can be very effective, sometimes surpassing more complicated approaches based on restricted Boltzmann machines, autoencoders or sparse coding. Furthermore, there has been increased interest in multiscale representations of music audio recently. Such representations are more versatile because music audio exhibits structure on multiple timescales, which are relevant for different MIR tasks to varying degrees. We develop and compare three approaches to multiscale audio feature learning using the spherical K-means algorithm. We evaluate them in an automatic tagging task and a similarity metric learning task on the Magnatagatune dataset. © 2013 International Society for Music Information Retrieval."
Dighe P.; Karnick H.; Raj B.,Swara histogram based structural analysis and identification of Indian classical ragas,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973313819&partnerID=40&md5=b7c57d2050d8f9464a2b92ae6e3201d4,"Dighe P., Indian Institute of Technology, Kanpur, India; Karnick H., Indian Institute of Technology, Kanpur, India; Raj B., Carnegie Mellon University, Pittsburgh, PA, United States","This work is an attempt towards robust automated analysis of Indian classical ragas through machine learning and signal processing tools and techniques. Indian classical music has a definite heirarchical structure where macro level concepts like thaats and raga are defined in terms of micro entities like swaras and shrutis. Swaras or notes in Indian music are defined only in terms of their relation to one another (akin to the movable do-re-mi-fa system), and an inference must be made from patterns of sounds, rather than their absolute frequency structure. We have developed methods to perform scale-independent raga identification using a random forest classifier on swara histograms and achieved state-of-the-art results for the same. The approach is robust as it directly works on partly noisy raga recordings from Youtube videos without knowledge of the scale used, whereas previous work in this direction often use audios generated in a controlled environment with the desired scale. The current work demonstrates the approach for 8 ragas namely Darbari, Khamaj, Malhar, Sohini, Bahar, Basant, Bhairavi and Yaman and we have achieved an average identification accuracy of 94.28% through the framework. © 2013 International Society for Music Information Retrieval."
Hillewaere R.; Manderick B.; Conklin D.,String methods for folk tune genre classification,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456470&partnerID=40&md5=b7c9c11c87b6f6fd66caf140e067321f,"Hillewaere R., Computational Modeling Lab., Department of Computing, Vrije Universiteit Brussel, Brussels, Belgium; Manderick B., Computational Modeling Lab., Department of Computing, Vrije Universiteit Brussel, Brussels, Belgium; Conklin D., Department of Computer Science and AI, Universidad del País Vasco UPV/EHU, San Sebastián, Spain, IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","In folk song research, string methods have been widely used to retrieve highly similar tunes or to perform tune family classification. In this study, we investigate how various string methods perform on a fundamentally different classification task, which is to classify folk tunes into genres, the genres being the dance types of the tunes. A new data set Dance-9 is therefore introduced. The different string method classification accuracies are compared with each other and also with n-gram models and global feature models which have been proven to be useful in previous folk song research. They are shown to yield similar results to the global feature models, but are outperformed by the n-gram models. © 2012 International Society for Music Information Retrieval."
Miryala S.S.; Bali K.; Bhagwan R.; Choudhury M.,Automatically identifying vocal expressions for music transcription,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905228891&partnerID=40&md5=ddd7b7142a6b0e04dc7f48d6a361a088,"Miryala S.S., Microsoft Research, India; Bali K., Microsoft Research, India; Bhagwan R., Microsoft Research, India; Choudhury M., Microsoft Research, India","Music transcription has many uses ranging from music information retrieval to better education tools. An important component of automated transcription is the identification and labeling of different kinds of vocal expressions such as vibrato, glides, and riffs. In Indian Classical Music such expressions are particularly important since a raga is often established and identified by the correct use of these expressions. It is not only important to classify what the expression is, but also when it starts and ends in a vocal rendition. Some examples of such expressions that are key to Indian music are Meend (vocal glides) and Andolan (very slow vibrato). In this paper, we present an algorithm for the automatic transcription and expression identification of vocal renditions with specific application to North Indian Classical Music. Using expert human annotation as the ground truth, we evaluate this algorithm and compare it with two machine-learning approaches. Our results show that we correctly identify the expressions and transcribe vocal music with 85% accuracy. As a part of this effort, we have created a corpus of 35 voice recordings, of which 12 recordings are annotated by experts. The corpus is available for download1. 1 © 2013 International Society for Music Information Retrieval."
Ranjani H.G.; Sreenivas T.V.,Hierarchical classification of carnatic music forms,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982275116&partnerID=40&md5=126572665cbb1fbe3faa68509f54312c,"Ranjani H.G., Dept. of Electrical Communication Engineering, Indian Institute of Science, Bangalore - 12, India; Sreenivas T.V., Dept. of Electrical Communication Engineering, Indian Institute of Science, Bangalore - 12, India","We address the problem of classifying a given piece of Carnatic art music into one of its several forms recognized pedagogically. We propose a hierarchical approach for classification of these forms as different combinations of rhythm, percussion and repetitive syllabic structures. The proposed 3-level hierarchy is based on various signal processing measures and classifiers. Features derived from short term energy contours, along with formant information are used to obtain discriminative features. The statistics of the features are used to design simple classifiers at each level of the hierarchy. The method is validated on a subset of IIT-M Carnatic concert music database, comprising of more than 20 hours of music. Using 10 s audio clips, we get an average f-ratio performance of 0.62 for the classification of the following six typesof Carnatic art music: /AlApana/, /viruttam/, /thillAna/, /krithi/, /thani-Avarthanam/ and /thAnam/. © 2013 International Society for Music Information Retrieval."
Ishwar V.; Dutta S.; Bellur A.; Murthy H.A.,Motif spotting in an alapana in carnatic music,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901439496&partnerID=40&md5=02c5746249d424d9c822de645fcf0c0e,"Ishwar V., Dept. of Computer Sci. and Engg, IIT, Madras, India; Dutta S., Dept. of Computer Sci. and Engg, IIT, Madras, India; Bellur A., Dept. of Electrical Engg, IIT, Madras, India; Murthy H.A., Dept. of Computer Sci. and Engg, IIT, Madras, India","This work addresses the problem of melodic motif spotting, given a query, in Carnatic music. Melody in Carnatic music is based on the concept of raga. Melodic motifs are signature phrases which give a raga its identity. They are also the fundamental units that enable extempore elaborations of a raga. In this paper, an attempt is made to spot typical melodic motifs of a raga queried in a musical piece using a two pass dynamic programming approach, with pitch as the basic feature. In the first pass, the rough longest common subsequence (RLCS) matching is performed between the saddle points of the pitch contours of the reference motif and the musical piece. These saddle points corresponding to quasi-stationary points of the motifs, are relevant entities of the raga. Multiple sequences are identified in this step, not all of which correspond to the the motif that is queried. To reduce the false alarms, in the second pass a fine search using RLCS is performed between the continuous pitch contours of the reference motif and the subsequences obtained in the first pass. The proposed methodology is validated by testing on Alapanas of 20 different musicians. © 2013 International Society for Music Information Retrieval."
Sakaue D.; Otsuka T.; Itoyama K.; Okuno H.G.,Bayesian nonnegative harmonic-temporal factorization and its application to multipitch analysis,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445984&partnerID=40&md5=e6d05932f27d0138984bd4e4e6aa086c,"Sakaue D., Graduate School of Informatics, Kyoto University, Japan; Otsuka T., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Okuno H.G., Graduate School of Informatics, Kyoto University, Japan","Since important musical features are mutually dependent, their relations should be analyzed simultaneously. Their Bayesian analysis is particularly important to reveal their statistical relation. As the first step for a unified music content analyzer, we focus on the harmonic and temporal structures of the wavelet spectrogram obtained from harmonic sounds. In this paper, we present a new Bayesian multipitch analyzer, called Bayesian non-negative harmonic-temporal factorization (BNHTF). BN-HTF models the harmonic and temporal structures separately based on Gaussian mixture model. The input signal is assumed to contain a finite number of harmonic sounds. Each harmonic sound is assumed to emit a large number of sound quanta over the time-log-frequency domain. The observation probability is expressed as the product of two Gaussian mixtures. The number of quanta is calculated in the e-neighborhood of each grid point on the spectrogram. BNHTF integrates latent harmonic allocation (LHA) and nonnegative matrix factorization (NMF) to estimate both the observation probability and the number of quanta. The model is optimized by newly designed deterministic procedures with several approximations for the variational Bayesian inference. Results of experiments on multipitch estimation with 40 musical pieces showed that BNHTF outperforms the conventional method by 0.018 in terms of F-measure on average. © 2012 International Society for Music Information Retrieval."
McFee B.; Lanckriet G.,Hypergraph models of playlist dialects,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873479796&partnerID=40&md5=a28c5d533027b0c80622f6fc1717e23c,"McFee B., Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","Playlist generation is an important task in music information retrieval. While previous work has treated a playlist collection as an undifferentiated whole, we propose to build playlist models which are tuned to specific categories or dialects of playlists. Toward this end, we develop a general class of flexible and scalable playlist models based upon hypergraph random walks. To evaluate the proposed models, we present a large corpus of categorically annotated, user-generated playlists. Experimental results indicate that category-specific models can provide substantial improvements in accuracy over global playlist models. © 2012 International Society for Music Information Retrieval."
Gu Y.; Raphael C.,Modeling piano interpretation using switching Kalman filter,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473337&partnerID=40&md5=4cc477f5fb150ca14ce1ef05de2fe6a7,"Gu Y., Indiana University, School of Informatics and Computing, United States; Raphael C., Indiana University, School of Informatics and Computing, United States","An approach of parsing piano music interpretation is presented. We focus mainly on quantifying expressive timing activities. A small number of different expressive timing behaviors (constant, slowing down, speeding up, accent) are defined in order to explain the tempo discretely. Given a MIDI performance of a piano music, we simultaneously estimate both discrete variables that corresponds to the behaviors and continuous variables that describe tempo. A graphical model is introduced to represent the evolution of the discrete behaviors and tempo progression. We demonstrate a computational method that acquires the approximate most likely configuration of the discrete behaviors and the hidden continuous variable tempo. This configuration represent a ""smoothed"" version of the performance which greatly reduces parametrization while retaining most of its musicality. Experiments are presented on several MIDI piano music performed on a digital piano. An user study is performed to evaluate our method. © 2012 International Society for Music Information Retrieval."
Chuan C.-H.; Chew E.,Creating ground truth for audio key finding: When the title key may not be the key,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455583&partnerID=40&md5=ca8487093a2d35f29f22c63c4e751bac,"Chuan C.-H., University of North Florida, School of Computing, United States; Chew E., Queen Mary, University of London, Centre for Digital Music, United Kingdom","In this paper, we present an effective and efficient way to create an accurately labeled dataset to advance audio key finding research. The MIREX audio key finding contest has been held twice using classical compositions for which the key is designated in the title. The problem with this accepted practice is that the title key may not be the perceived key in the audio excerpt. To reduce manual annotation, which is costly, we use a confusion index generated by existing audio key finding algorithms to determine if an audio excerpt requires manual annotation. We collected 3224 excerpts and identified 727 excerpts requiring manual annotation. We evaluate the algorithms' performance on these challenging cases using the title keys, and the re-labeled keys. The musicians who aurally identify the key also provide comments on the reasons for their choice. The relabeling process reveals the mismatch between title and perceived keys to be caused by tuning practices (in 471 of the 727 excerpts, 64.79%), and other factors (188 excerpts, 25.86%) including key modulation and intonation choices. The remaining 68 challenging cases provide useful information for algorithm design. © 2012 International Society for Music Information Retrieval."
Fenet S.; Grenier Y.; Richard G.,An extended audio-fingerprint method with capabilities for similar music detection,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971227413&partnerID=40&md5=68eb703969a6c0e37273db4212c8a0fc,"Fenet S., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37 rue Dareau, Paris, 75014, France; Grenier Y., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37 rue Dareau, Paris, 75014, France; Richard G., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37 rue Dareau, Paris, 75014, France","Content-based Audio Identification consists of retrieving the meta-data (i.e. title, artist, album) associated with an unknown audio excerpt. Audio fingerprint techniques are amongst the most efficient for this goal: following the extraction of a fingerprint from the unknown signal, the closest fingerprint in a reference database is sought in order to perform the identification. While being able to manage large scale databases, the recent developments in fingerprint methods have mostly focused on the improvement of robustness to post-processing distortions (equalization, amplitude compression, pitch-shifting,...). In this work, we describe a novel fingerprint model that is robust not only to the classical set of distortions handled by most methods but also to the variations that occur when a title is re-recorded (live vs studio version in particular). As a result our fingerprint method is able to identify any signal that is an excerpt of one of the references from the database or that is similar to one of the references. The issue that we cover thus lies at the intersection of audio fingerprint and cover song detection, meaning that the functional perimeter of our method is substantially larger than the classical audio fingerprint approaches. © 2013 International Society for Music Information Retrieval."
Urbano J.; McFee B.; Stephen Downie J.; Schedl M.,How significant is statistically significant? The case of audio music similarity and retrieval,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873460939&partnerID=40&md5=30dabb1804ac0f76a60f1ff7b18334b2,"Urbano J., University Carlos III of Madrid, Spain; McFee B., University of California, San Diego, United States; Stephen Downie J., University of Illinois, Urbana-Champaign, United States; Schedl M., Johannes Kepler University, Linz, Austria","The principal goal of the annual Music Information Retrieval Evaluation eXchange (MIREX) experiments is to determine which systems perform well and which systems perform poorly on a range of MIR tasks. However, there has been no systematic analysis regarding how well these evaluation results translate into real-world user satisfaction. For most researchers, reaching statistical significance in the evaluation results is usually the most important goal, but in this paper we show that indicators of statistical significance (i.e., small p-value) are eventually of secondary importance. Researchers who want to predict the real-world implications of formal evaluations should properly report upon practical significance (i.e., large effect-size). Using data from the 18 systems submitted to the MIREX 2011 Audio Music Similarity and Retrieval task, we ran an experiment with 100 real-world users that allows us to explicitly map system performance onto user satisfaction. Based upon 2, 200 judgments, the results show that absolute system performance needs to be quite large for users to be satisfied, and differences between systems have to be very large for users to actually prefer the supposedly better system. The results also suggest a practical upper bound of 80% on user satisfaction with the current definition of the task. Reflecting upon these findings, we make some recommendations for future evaluation experiments and the reporting and interpretation of results in peer-reviewing. © 2012 International Society for Music Information Retrieval."
Salamon J.; Gulati S.; Serra X.,A multipitch approach to tonic identification in Indian classical music,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475875&partnerID=40&md5=e7ef3879d89edb01efc9dadff1a5e776,"Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The tonic is a fundamental concept in Indian classical music since it constitutes the base pitch from which a lead performer constructs the melodies, and accompanying instruments use it for tuning. This makes tonic identification an essential first step for most automatic analyses of Indian classical music, such as intonation and melodic analysis, and raga recognition. In this paper we address the task of automatic tonic identification. Unlike approaches that identify the tonic from a single predominant pitch track, here we propose a method based on a multipitch analysis of the audio. We use a multipitch representation to construct a pitch histogram of the audio excerpt, out of which the tonic is identified. Rather than manually define a template, we employ a classification approach to automatically learn a set of rules for selecting the tonic. The proposed method returns not only the pitch class of the tonic but also the precise octave in which it is played. We evaluate the approach on a large collection of Carnatic and Hindustani music, obtaining an identification accuracy of 93%. We also discuss the types of errors made by our proposed method, as well as the challenges in generating ground truth annotations. © 2012 International Society for Music Information Retrieval."
Yang Y.-H.; Hu X.,Cross-cultural music mood classification: A comparison on English and Chinese songs,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451540&partnerID=40&md5=cb6d21b681218665007ae2d4a96b8600,"Yang Y.-H., Academia Sinica, Taiwan; Hu X., University of Denver, United States","Most existing studies on music mood classification have been focusing on Western music while little research has investigated whether mood categories, audio features, and classification models developed from Western music are applicable to non-Western music. This paper attempts to answer this question through a comparative study on English and Chinese songs. Specifically, a set of Chinese pop songs were annotated using an existing mood taxonomy developed for English songs. Six sets of audio features commonly used on Western music (e.g., timbre, rhythm) were extracted from both Chinese and English songs, and mood classification performances based on these feature sets were compared. In addition, experiments were conducted to test the generalizability of classification models across English and Chinese songs. Results of this study shed light on cross-cultural applicability of research results on music mood classification. © 2012 International Society for Music Information Retrieval."
Elowsson A.; Friberg A.; Madison G.; Paulin J.,Modelling the speed of music using features from harmonic/percussive separated audio,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907866137&partnerID=40&md5=48808b46c8f7f53462a4adf44cc322bb,"Elowsson A., KTH Royal Institute of Technology, CSC, Dept. of Speech, Music and Hearing, Sweden; Friberg A., KTH Royal Institute of Technology, CSC, Dept. of Speech, Music and Hearing, Sweden; Madison G., Department of Psychology, Umeå University, Sweden; Paulin J., Department of Psychology, Umeå University, Sweden","One of the major parameters in music is the overall speed of a musical performance. In this study, a computational model of speed in music audio has been developed using a custom set of rhythmic features. Speed is often associated with tempo, but as shown in this study, factors such as note density (onsets per second) and spectral flux are important as well. The original audio was first separated into a harmonic part and a percussive part and the features were extracted separately from the different layers. In previous studies, listeners had rated the speed of 136 songs, and the ratings were used in a regression to evaluate the validity of the model as well as to find appropriate features. The final models, consisting of 5 or 8 features, were able to explain about 90% of the variation in the training set, with little or no degradation for the test set. © 2013 International Society for Music Information Retrieval."
Nam J.; Herrera J.; Slaney M.; Smith J.,Learning sparse feature representations for music annotation and retrieval,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444848&partnerID=40&md5=dc09f3098ed46b842c9fb00f06e3a374,"Nam J., CCRMA, Stanford University, United States; Herrera J., CCRMA, Stanford University, United States; Slaney M., Yahoo, Research Stanford University, United States; Smith J., CCRMA, Stanford University, United States","We present a data-processing pipeline based on sparse feature learning and describe its applications to music annotation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are handcrafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features automatically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning algorithms to music data, in particular, focusing on a high-dimensional sparse-feature representation. Our experiments show that, using only a linear classifier, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and retrieval systems. © 2012 International Society for Music Information Retrieval."
Schindler A.; Mayer R.; Rauber A.,Facilitating comprehensive benchmarking experiments on the million song dataset,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459791&partnerID=40&md5=95d12c16e08094ac427f832fd2b506c1,"Schindler A., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Mayer R., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","The Million Song Dataset (MSD), a collection of one million music pieces, enables a new era of research of Music Information Retrieval methods for large-scale applications. It comes as a collection of meta-data such as the song names, artists and albums, together with a set of features extracted with the The Echo Nest services, such as loudness, tempo, and MFCC-like features. There is, however, no easily obtainable download for the audio files. Furthermore, labels for supervised machine learning tasks are missing. Researchers thus are currently restricted on working solely with these features provided, limiting the usefulness of MSD. We therefore present in this paper a more comprehensive set of data based on the MSD, allowing its broader use as benchmark collection. Specifically, we provide a wide and growing collection of other well-known features in the MIR domain, as well as ground truth data with a set of recommended training/test splits. We obtained these features from audio samples provided by 7digital.com, and metadata from the All Music Guide. While copyright prevents re-distribution of the audio snippets per se, the features as well as metadata are publicly available on our website for benchmarking evaluations. In this paper we describe the pre-processing and cleansing steps applied, as well as feature sets and tools made available, together with first baseline classification results. © 2012 International Society for Music Information Retrieval."
Fukayama S.; Yoshii K.; Goto M.,Chord-sequence-factory: A chord arrangement system modifying factorized chord sequence probabilities,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963746662&partnerID=40&md5=89f72489b947871146e997c483df3ca3,"Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a system named ChordSequenceFactory for automatically generating chord arrangements. A key element of musical composition is the arrangement of chord sequences because good chord arrangements have the potential to enrich the listening experience and create a pleasant feeling of surprise by borrowing elements from different musical styles in unexpected ways. While chord sequences have conventionally been modeled by using N-grams, generative grammars, or music theoretic rules, our system decomposes a matrix consisting of chord transition probabilities by using nonnegative matrix factorization. This enables us to not only generate chord sequences from scratch but also transfer characteristic transition patterns from one chord sequence to another. ChordSequenceFactory can assist users to edit chord sequences by modifying factorized chord transition probabilities and then automatically re-arranging them. By leveraging knowledge from chord sequences of over 2000 songs, our system can help users generate a wide range of musically interesting and entertaining chord arrangements. © 2013 International Society for Music Information Retrieval."
Dupont S.; Ravet T.,Improved audio classification using a novel non-linear dimensionality reduction ensemble approach,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935847385&partnerID=40&md5=f38beba8a0b68ca30640ba174773ac01,"Dupont S., University of Mons, Belgium; Ravet T., University of Mons, Belgium","Two important categories of machine learning methodologies have recently attracted much interest in classification research and its applications. On one side, unsupervised and semi-supervised learning allow to benefit from the availability of larger sets of training data, even if not fully annotated with class labels, and of larger sets of diverse feature representations, through novel dimensionality reduction schemes. On the other side, ensemble methods allow to benefit from more diversity in base learners though larger data and feature sets. In this paper, we propose a novel ensemble learning approach making use of recent non-linear dimensionality reduction methods. More precisely, we apply t-SNE (t-distributed Stochastic Neighbor Embedding) to a large feature set to come up with embeddings of various dimensionality. A k-NN classifier is then obtained for each embedding, leading to an ensemble whose estimates can then be combined, making use of various ensemble combination rules from the literature. The rationale of this approach resides in its potential capacity to better handle manifolds of different dimensionality in different regions of the feature space. We evaluate the approach on a transductive audio classification task, where only part of the whole data set is labeled. We confirm that dimensionality reduction by itself can improve performance (by 40% relative), and that creating an ensemble through the proposed approach further reduces classification error rate by about 10% relative. © 2013 International Society for Music Information Retrieval."
Bimbot F.; Deruty E.; Sargent G.; Vincent E.,"Semiotic structure labeling of music pieces: Concepts, methods and annotation conventions",2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873480534&partnerID=40&md5=08efa44e892832ef0c9a8ecf1204da8c,"Bimbot F., IRISA/METISS, CNRS - UMR 6074, France; Deruty E., INRIA/METISS, Rennes, France; Sargent G., IRISA/METISS, Université Rennes 1, France; Vincent E., INRIA/METISS, Rennes, France","Music structure description, i.e. the task of representing the high-level organization of music pieces in a concise, generic and reproducible way, is currently a scientific challenge both algorithmically and conceptually. In this paper, we focus on semiotic structure, i.e. the description of similarities and internal relationships within a music piece, as a low-rate stream of arbitrary symbols from a limited alphabet and we address methodological questions related to annotation. We formulate the labeling task as a blind demodulation problem, whose goal is to identify a minimal set of semiotic codewords, whose realizations within the music piece are subject to a number of connotative variations viewed as modulations. The determination of labels is achieved by combining morphological, paradigmatic and syntagmatic considerations relying respectively on (i) a morphological model of semiotic blocks in order to define their individual properties, (ii) the support of prototypical structural patterns to guide the comparison between blocks and (iii) a methodology for the determination of distinctive features across semiotic classes. Specific notations are introduced to account for unresolvable semiotic ambiguities, which are occasional but must be considered as inherent to the music matter itself. A set of 500 music pieces labeled in accordance with the proposed concepts and annotation conventions is being released with this article. © 2012 International Society for Music Information Retrieval."
Özaslan T.H.; Serra X.; Arcos J.L.,Characterization of embellishments in ney performances of makam music in Turkey,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453734&partnerID=40&md5=53d268add37ec389405990729063ca66,"Özaslan T.H., Artificial Intelligence Research Institute - CSIC, Bellaterra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Arcos J.L., Artificial Intelligence Research Institute - CSIC, Bellaterra, Spain","The embellishments of makam music in Turkey are an inherent characteristic of the music rather than a separate expressive resource, thus their understanding is essential to characterize this music. We do a computational study, in which we analyze audio recordings of 8 widely acknowledged Turkish ney players covering the period from the year 1920 to 2000. From the extracted fundamental frequency, we manually segment and identify 327 separate embellishments of the types vibrato and kaydirma. We analyze them and characterize the behavior of two features that help us differentiate performance styles, namely vibrato rate change and pitch bump. Also we compare these embellishments with the ones used in Western classical music. With our approach, we have an explicit and formalized way to understand ney embellishments, which is a step towards the automatic characterization of makam music in Turkey. © 2012 International Society for Music Information Retrieval."
Lefèvre A.; Bach F.; Févotte C.,Semi-supervised NMF with time-frequency annotations for single-channel source separation,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873466806&partnerID=40&md5=8577f5908b3c355836c17b0461c2775a,"Lefèvre A., INRIA Team SIERRA, France; Bach F., INRIA Team SIERRA, France; Févotte C., LTCI, Telecom ParisTech, France","We formulate a novel extension of nonnegative matrix factorization (NMF) to take into account partial information on source-specific activity in the spectrogram. This information comes in the form of masking coefficients, such as those found in an ideal binary mask. We show that state-of-the-art results in source separation may be achieved with only a limited amount of correct annotation, and furthermore our algorithm is robust to incorrect annotations. Since in practice ideal annotations are not observed, we propose several supervision scenarios to estimate the ideal masking coefficients. First, manual annotations by a trained user on a dedicated graphical user interface are shown to provide satisfactory performance although they are prone to errors. Second, we investigate simple learning strategies to predict the Wiener coefficients based on local information around a given time-frequency bin of the spectrogram. Results on single-channel source separation show that time-frequency annotations allow to disambiguate the source separation problem, and learned annotations open the way for a completely unsupervised learning procedure for source separation with no human intervention. © 2012 International Society for Music Information Retrieval."
Ni Y.; Mcvicar M.; Santos-Rodríguez R.; De Bie T.,Using hyper-genre training to explore genre information for automatic chord estimation,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453177&partnerID=40&md5=d197821b21140af95a86f8a5725273b4,"Ni Y., Intelligent Systems Laboratory, University of Bristol, United Kingdom; Mcvicar M., Intelligent Systems Laboratory, University of Bristol, United Kingdom; Santos-Rodríguez R., Intelligent Systems Laboratory, University of Bristol, United Kingdom; De Bie T., Intelligent Systems Laboratory, University of Bristol, United Kingdom","Recently a large amount of new chord annotations have been made available. This raises hopes for further development in automatic chord estimation. While more data seems to imply better performance, a major challenge however, is the wide variety of genres covered by these new data. As a result, the genre-independent training scheme as is common today is bound to fail. In this paper we investigate various options for exploring genre information for chord estimation, while also maximally exploiting the full dataset. More specifically, we propose a hyper-genre training scheme in which each genre cluster has its own parameters, tied together by hyper parameters as a Bayesian prior. The results are promising, showing significant improvements over other prevailing training schemes. © 2012 International Society for Music Information Retrieval."
Arora V.; Behera L.,Semi-supervised polyphonic source identification using PLCA based graph clustering,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901433498&partnerID=40&md5=ac71a95e49b572a6a79675407312addd,"Arora V., Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India; Behera L., Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India","For identifying instruments or singers in the polyphonic audio, supervised probabilistic latent component analysis (PLCA) is a popular tool. But in many cases individual source audio is not available for training. To address this problem, this paper proposes a novel scheme using semi-supervised PLCA with probabilistic graph clustering, which does not require individual sources for training. The PLCA is based on source-filter approach which models the spectral envelope as a weighted sum of elementary band-pass filters. The novel graph based approach, embedded in the PLCA framework, takes into account various perceptual cues for characterizing a source. These cues include temporal cues like the evolution of F0 contours as well as the acoustic cues like mel-frequency cepstral coefficients. The proposed scheme shows better results in identifying vocal sources than a state of the art unsupervised scheme. In addition, the proposed framework can be used to incorporate perceptual cues so as to enhance the performance of supervised schemes too. © 2013 International Society for Music Information Retrieval."
Scheeren F.M.; Pimenta M.S.; Keller D.; Lazzarini V.,Coupling social network services and support for online communities in codes environment,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969930477&partnerID=40&md5=388e96060b518497d4f14e424b57cd4d,"Scheeren F.M., Institute of Informatics UFRGS, Brazil; Pimenta M.S., Institute of Informatics UFRGS, Brazil; Keller D., Amazon Center for Music Research UFAC, Brazil; Lazzarini V., National Univ. of Ireland, Maynooth, Ireland","In recent years, our research group has been investigating the use of computing technology to support novice-oriented computer-based musical activities. CODES (Cooperative Music Prototyping Design) is a Web-based environment designed to allow novice users to create musical prototypes through combining basic sound patterns. This paper shows how CODES has been changed to provide support to some concepts originally from of Social Networks and also to Online Communities having Music Creation as intrinsic motivation. © 2013 International Society for Music Information Retrieval."
Lehner B.; Sonnleitner R.; Widmer G.,"Towards light-weight, real-time-capable singing voice detection",2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905250130&partnerID=40&md5=3098d60da8ced894d5fdd8f69a28ccc1,"Lehner B., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Sonnleitner R., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","We present a study that indicates that singing voice detection – the problem of identifying those parts of a polyphonic audio recording where one or several persons sing(s) – can be realised with substantially fewer (and less expensive) features than used in current state-of-the-art methods. Essentially, we show that MFCCs alone, if appropriately optimised and used with a suitable classifier, are sufficient to achieve detection results that seem on par with the state of the art – at least as far as this can be ascertained by direct, fair comparisons to existing systems. To make this comparison, we select three relevant publications from the literature where publicly accessible training/test data were used, and where the experimental setup is described in enough detail for us to perform fair comparison experiments. The result of the experiments is that with our simple, optimised MFCC-based classifier we achieve at least comparable identification results, but with (in some cases much) less computational effort, and without any need for extensive lookahead, thus paving the way to on-line, real-time voice detection applications. © 2013 International Society for Music Information Retrieval."
Mechtley B.; Cook P.; Spanias A.,Shortest path techniques for annotation and retrieval of environmental sounds,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461191&partnerID=40&md5=da02c4b6fa74315b58f626132f3d88b6,"Mechtley B., Arizona State University, Computer Science (SCIDSE), United States; Cook P., Princeton University, Computer Science and Music, United States; Spanias A., Arizona State University, Electrical Engineering (SECEE), United States","Many techniques for text-based retrieval and automatic annotation of music and sound effects rely on learning with explicit generalization, training individual classifiers for each tag. Non-parametric approaches, where queries are individually compared to training instances, can provide added flexibility, both in terms of robustness to shifts in database content and support for foreign queries, such as concepts not yet included in the database. In this paper, we build upon prior work in designing an ontological framework for annotation and retrieval of environmental sounds, where shortest paths are used to navigate a network containing edges that represent content-based similarity, semantic similarity, and user tagging data. We evaluate novel techniques for ordering query results using weights of both shortest paths and minimum cost paths of specified lengths, pruning outbound edges by nodes' K nearest neighbors, and adjusting edge weights depending on type (acoustic, semantic, or user tagging). We evaluate these methods both through traditional cross-validation and through simulation of live systems containing a complete collection of sounds and tags but incomplete tagging data. © 2012 International Society for Music Information Retrieval."
Wilmering T.; Fazekas G.; Sandler M.B.,The audio effects ontology,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896452212&partnerID=40&md5=7c4e297022b4350d2743a6178a3a0cda,"Wilmering T., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom","In this paper we present the Audio Effects Ontology for the ontological representation of audio effects in music production workflows. Designed as an extension to the Studio Ontology, its aim is to provide a framework for the detailed description and sharing of information about audio effects, their implementations, and how they are applied in real-world production scenarios. The ontology enables capturing and structuring data about the use of audio effects and thus facilitates reproducibility of audio effect application, as well as the detailed analysis of music production practices. Furthermore, the ontology may inform the creation of metadata standards for adaptive audio effects that map high-level semantic descriptors to control parameter values. The ontology is using Semantic Web technologies that enable knowledge representation and sharing, and is based on modular ontology design methodologies. It is evaluated by examining how it fulfils requirements in a number of production and retrieval use cases. © 2013 International Society for Music Information Retrieval."
Pikrakis A.; Báñez J.M.D.; Mora J.; Escobar F.; Gomez F.; Oramas S.; Gomez E.; Salamon J.,Tracking melodic patterns in flamenco singing by analyzing polyphonic music recordings,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465150&partnerID=40&md5=d5d9bde3f07988809f580cc2449d841e,"Pikrakis A., University of Piraeus, Greece; Báñez J.M.D., University of Sevilla, Spain; Mora J., University of Sevilla, Spain; Escobar F., University of Sevilla, Spain; Gomez F., Polytechnic University, Madrid, Spain; Oramas S., Polytechnic University, Madrid, Spain; Gomez E., Universitat Pompeu Fabra, Spain; Salamon J., Universitat Pompeu Fabra, Spain","The purpose of this paper is to present an algorithmic pipeline for melodic pattern detection in audio files. Our method follows a two-stage approach: first, vocal pitch sequences are extracted from the audio recordings by means of a predominant fundamental frequency estimation technique; second, instances of the patterns are detected directly in the pitch sequences by means of a dynamic programming algorithm which is robust to pitch estimation errors. In order to test the proposed method, an analysis of characteristic melodic patterns in the context of the flamenco fandango style was performed. To this end, a number of such patterns were defined in symbolic format by flamenco experts and were later detected in music corpora, which were composed of un-segmented audio recordings taken from two fandango styles, namely Valverde fandangos and Huelva capital fandangos. These two styles are representative of the fandango tradition and also differ with respect to their musical characteristics. Finally, the strategy in the evaluation of the algorithm performance was discussed by flamenco experts and their conclusions are presented in this paper. © 2012 International Society for Music Information Retrieval."
Gerber T.; Dutasta M.; Girin L.; Févotte C.,Professionally-produced music separation guided by covers,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448116&partnerID=40&md5=e41974a2585be448e6814b3d581d90b6,"Gerber T., Grenoble-INP, GIPSA-Lab., France; Dutasta M., Grenoble-INP, GIPSA-Lab., France; Girin L., Grenoble-INP, GIPSA-Lab., France; Févotte C., TELECOM ParisTech, CNRS LTCI, France","This paper addresses the problem of demixing professionally produced music, i.e., recovering the musical source signals that compose a (2-channel stereo) commercial mix signal. Inspired by previous studies using MIDI synthesized or hummed signals as external references, we propose to use the multitrack signals of a cover interpretation to guide the separation process with a relevant initialization. This process is carried out within the framework of the multichannel convolutive NMF model and associated EM/MU estimation algorithms. Although subject to the limitations of the convolutive assumption, our experiments confirm the potential of using multitrack cover signals for source separation of commercial music. © 2012 International Society for Music Information Retrieval."
Benetos E.; Weyde T.,Explicit duration hidden Markov models for multiple-instrument polyphonic music transcription,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963983592&partnerID=40&md5=067519ed92566ae66ac8f41f9efe0381,"Benetos E., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","In this paper, a method for multiple-instrument automatic music transcription is proposed that models the temporal evolution and duration of tones. The proposed model supports the use of spectral templates per pitch and instrument which correspond to sound states such as attack, sustain, and decay. Pitch-wise explicit duration hidden Markov models (EDHMMs) are integrated into a convolutive probabilistic framework for modelling the temporal evolution and duration of the sound states. A two-stage transcription procedure integrating note tracking information is performed in order to provide more robust pitch estimates. The proposed system is evaluated on multi-pitch detection and instrument assignment using various publicly available datasets. Results show that the proposed system outperforms a hidden Markov model-based transcription system using the same framework, as well as several state-of-the-art automatic music transcription systems. © 2013 International Society for Music Information Retrieval."
Aucouturier J.-J.; Bigand E.,Mel Cepstrum & Ann Ova: The difficult dialog between MIR and music cognition,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445003&partnerID=40&md5=b95b67a48ffcec99f045b9cc937f7c4c,"Aucouturier J.-J., LEAD/CNRS UMR 5022, University of Burgundy, Dijon, France; Bigand E., LEAD/CNRS UMR 5022, University of Burgundy, Dijon, France","Mel is a MIR researcher (the audio type) who's always been convinced that his field of research had something to contribute to the study of music cognition. His feeling, however, hasn't been much shared by the reviewers of the many psychology journals he tried submitting his views to. Their critics, rejecting his data as irrelevant, have frustrated him - the more he tried to rebut, the more defensive both sides of the debate became. He was close to give up his hopes of interdisciplinary dialog when, in one final and desperate rejection letter, he sensed an unusual touch of interest in the editor's response. She, a cognitive psychologist named Ann, was clearly open to discussion. This was the opportunity that Mel had always hoped for: clarifying what psychologists really think of audio MIR, correcting misconceptions that he himself made about cognition, and maybe, developing a vision of how both fields could work together. The following is the imaginary dialog that ensued. Meet Dr Mel Cepstrum, the MIR researcher, and Prof. Ann Ova, the psychologist. © 2012 International Society for Music Information Retrieval."
Devaney J.; Mandel M.; Fujinaga I.,A study of intonation in three-part singing using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT),2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476660&partnerID=40&md5=d5547773d66078baea308d3058d0550a,"Devaney J., CNMAT, UC Berkeley, School of Music, United States; Mandel M., Audience Inc., College of Engineering, Ohio State University, United States; Fujinaga I., CIRMMT, Schulich School of Music, McGill University, Canada","This paper introduces the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT), is a MATLAB toolkit for accurately aligning monophonic audio to MIDI scores as well as extracting and analyzing timing-, pitch-, and dynamics-related performance data from the aligned recordings. This paper also presents the results of an analysis performed with AMPACT on an experiment studying intonation in three-part singing. The experiment examines the interval size and drift in four ensembles' performances of a short exercise by Benedetti, which was designed to highlight the conflict between Just Intonation tuning and pitch drift. © 2012 International Society for Music Information Retrieval."
Font F.; Serrà J.; Serra X.,Folksonomy-based tag recommendation for online audio clip sharing,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873460515&partnerID=40&md5=b10ea431818a860622a7f614e2f23a88,"Font F., Music Technology Gorup, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain; Serra X., Music Technology Gorup, Universitat Pompeu Fabra, Barcelona, Spain","Collaborative tagging has emerged as an efficient way to semantically describe online resources shared by a community of users. However, tag descriptions present some drawbacks such as tag scarcity or concept inconsistencies. In these situations, tag recommendation strategies can help users in adding meaningful tags to the resources being described. Freesound is an online audio clip sharing site that uses collaborative tagging to describe a collection of more than 140, 000 sound samples. In this paper we propose four algorithm variants for tag recommendation based on tag co-occurrence in the Freesound folksonomy. On the basis of removing a number of tags that have to be later predicted by the algorithms, we find that using ranks instead of raw tag similarities produces statistically significant improvements. Moreover, we show how specific strategies for selecting the appropriate number of tags to be recommended can significantly improve algorithms' performance. These two aspects provide insight into some of the most basic components of tag recommendation systems, and we plan to exploit them in future real-world deployments. © 2012 International Society for Music Information Retrieval."
Page K.R.; Fields B.; De Roure D.; Crawford T.; Stephen Downie J.,"Reuse, remix, repeat: The workflows of MIR",2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472741&partnerID=40&md5=5f9c240f9d9e93fcd9db22dd7282acf0,"Page K.R., Oxford e-Research Centre, University of Oxford, United Kingdom; Fields B., Musicmetric (Semetric Ltd.), United Kingdom, Department of Computing, Goldsmiths, University of London, United Kingdom; De Roure D., Oxford e-Research Centre, University of Oxford, United Kingdom; Crawford T., Department of Computing, Goldsmiths, University of London, United Kingdom; Stephen Downie J., Graduate School of Library and Information Sciences, University of Illinois, United States","Many solutions for the reuse and remixing of MIR methods and the tools implementing them have been introduced over recent years. Proposals for achieving the necessary interoperability have ranged from shared software libraries and interfaces, through common frameworks and portals, to standardised file formats and metadata. Each proposal shares the desire to reuse and combine repurposable components into assemblies (or ""workflows"") that can be used in novel and possibly more ambitious ways. Reuse and remixing also have great implications for the process of MIR research. The encapsulation of any algorithm and its operation - including inputs, parameters, and outputs - is fundamental to the repeatability and reproducibility of any experiment. This is desirable both for the open and reliable evaluation of algorithms (e.g. in MIREX) and for the advancement of MIR by building more effectively upon prior research. At present there is no clear best practice widely adopted throughout the community. Should this be considered a failure? Are there limits to interoperability unique to MIR, and how might they be overcome? In this paper we assess contemporary MIR solutions to these issues, aligning them with the emerging notion of Research Objects for reproducible research in other domains, and propose their adoption as a route to reuse in MIR. © 2012 International Society for Music Information Retrieval."
Su L.; Yang Y.-H.,Sparse modeling for artist identification: Exploiting phase information and vocal separation,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963947798&partnerID=40&md5=4f007d08229736274ce6d2161696191b,"Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","As artist identification deals with the vocal part of music, techniques such as vocal sound separation and speech feature extraction has been found relevant. In this paper, we argue that the phase information, which is usually overlooked in the literature, is also informative in modeling the voice timbre of a singer, given the necessary processing techniques. Specifically, instead of directly using the raw phase spectrum as features, we show that significantly better performance can be obtained by learning sparse features from the negative derivative of phase with respect to frequency (i.e., group delay function) using unsupervised feature learning algorithms. Moreover, better performance is achieved by using singing voice separation as a pre-processing step, and then learning features from both the magnitude spectrum and the group delay function. The proposed system achieves 66% accuracy in identifying 20 artists from the artist20 dataset, which is better than a prior art by 7%. © 2013 International Society for Music Information Retrieval."
Hauger D.; Schedl M.; Košir A.; Tkalčič M.,The million musical tweets dataset: What can we learn from microblogs,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897638496&partnerID=40&md5=c6009bb0e03f5e921b984332ad7f73e4,"Hauger D., Johannes Kepler University Linz, Austria; Schedl M., Johannes Kepler University Linz, Austria; Košir A., University of Ljubljana, Slovenia; Tkalčič M., Johannes Kepler University Linz, Austria","Microblogs and Social Media applications are continuously growing in spread and importance. Users of Twitter, the currently most popular platform for microblogging, create more than a billion posts (called tweets) every week. Among all the different types of information being shared, some people post their music listening behavior, which is why Twitter became interesting for the Music Information Retrieval (MIR) community. Depending on the device and personal settings, some users provide geographic coordinates for their microposts. Having continuously crawled and analyzed tweets for more than 500 days (17 months) we can now present the “Million Musical Tweet Dataset” (MMTD) – the biggest publicly available source of microblog-based music listening histories that includes geographic, temporal, and other contextual information. These extended information makes the MMTD outstanding from other datasets providing music listening histories. We introduce the dataset, give basic statistics about its composition, and show how this dataset allows to detect new contextual music listening patterns by performing a comprehensive statistical investigation with respect to correlation between music taste and day of the week, hour of day, and country. © 2013 International Society for Music Information Retrieval."
Gkiokas A.; Katsouros V.; Carayannis G.,Reducing tempo octave errors by periodicity vector coding and SVM learning,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456653&partnerID=40&md5=ae239812cfed7fd8d32bd3c57c772bfe,"Gkiokas A., Institute for Language and Speech Processing, R.C. Athena, Greece, National Technical University of Athens, Greece; Katsouros V., Institute for Language and Speech Processing, R.C. Athena, Greece; Carayannis G., National Technical University of Athens, Greece","In this paper we present a method for learning tempo classes in order to reduce tempo octave errors. There are two main contributions of this paper in the rhythm analysis field. Firstly, a novel technique is proposed to code the rhythm periodicity functions of a music signal. Target tempi range is divided into overlapping ""tempo bands"" and the periodicity function is filtered by triangular masks aligned to those tempo bands, in order to calculate the respective saliencies, followed by the application of the DCT transform on band strengths. The second contribution is the adoption of Support Vector Machines to learn broad tempo classes from the coded periodicity vectors. Training instances are assigned a tempo class according to annotated tempo. The classes are assumed to correspond to ""music speed"". At classification phase, each target excerpt is assigned a tempo class label by the SVM. Target periodicity vector is masked by the predicted tempo class range, and tempo is estimated by peak picking in the reduced periodicity vector. The proposed method was evaluated on the benchmark ISMIR 2004 Tempo Induction Evaluation Exchange Dataset for both tempo class and tempo value estimation tasks. Results indicate that the proposed approach provides an efficient framework to tackle the tempo estimation task. © 2012 International Society for Music Information Retrieval."
Giraud M.; Groult R.; Levé F.,Detecting episodes with harmonic sequences for fugue analysis,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873454232&partnerID=40&md5=2a8e31c02f6ad830211aa6dce583905e,"Giraud M., LIFL, CNRS, Université Lille 1 INRIA, Lille, France; Groult R., MIS, Université Picardie Jules Verne, Amiens, France; Levé F., MIS, Université Picardie Jules Verne, Amiens, France","Fugues alternate between instances of the subject and of other patterns, such as the counter-subject, and modula-tory sections called episodes. The episodes play an important role in the overall design of a fugue: detecting them may help the analysis of the fugue, in complement to a subject and a counter-subject detection. We propose an algorithm to retrieve episodes in the fugues of the first book of Bach's Well-Tempered Clavier, starting from a symbolic score which is already track-separated. The algorithm does not use any information on subject or counter-subject occurrences, but tries to detect partial harmonic sequences, that is similar pitch contour in at least two voices. For this, it uses a substitution function considering ""quantized partially overlapping intervals"" © 2012 International Society for Music Information Retrieval. [14] and a strict length matching for all notes, except for the first and the last one. On half of the tested fugues, the algorithm has correct or good results, enabling to sketch the design of the fugue. © 2012 International Society for Music Information Retrieval."
Izmirli O.; Sharma G.,Bridging printed music and audio through alignment using a mid-level score representation,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456894&partnerID=40&md5=637f8b2bded314a607d157b437c11386,"Izmirli O., Center for Arts and Technology, Computer Science Department, Connecticut College, United States; Sharma G., Center for Arts and Technology, Computer Science Department, Connecticut College, United States","We present a system that utilizes a mid-level score representation for aligning printed music to its audio rendition. The mid-level representation is designed to capture an approximation to the musical events present in the printed score. It consists of a template based note detection frontend that seeks to detect notes without regard to musical duration, accidentals or the key signature. The presented method is designed for the commonly used grand staff and the approach is extendable to other types of scores. The image processing consists of page segmentation into lines followed by multiple stages that optimally orient the lines and establish a reference grid to be used in the note identification stage. Both the audio and the printed score are converted into compatible frequency representations. Alignment is performed using dynamic time warping with a specially designed distance measure. The insufficient pitch resolution due to the reductive nature of the mid-level representation is compensated by this pitch tolerant distance measure. Evaluation is carried out at the beat level using annotated scores and audio. The results demonstrate that the approach provides an efficient and practical alternative to methods that rely on symbolic MIDI-like information through OMR methods for alignment. © 2012 International Society for Music Information Retrieval."
Cai Z.; Ellis R.J.; Duan Z.; Lu H.; Wang Y.,Basic evaluation of auditory temporal stability (BEATS): A novel rationale and implementation,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916223613&partnerID=40&md5=bc9717679ac1ad9127fe85de478137d4,"Cai Z., School of Computing National University of Singapore, Singapore; Ellis R.J., School of Computing National University of Singapore, Singapore; Duan Z., School of Computing National University of Singapore, Singapore; Lu H., School of Computer Science, Fudan University, China; Wang Y., School of Computing National University of Singapore, Singapore","The accurate detection of pulse-level temporal stability has important practical applications; for example, the creation of fixed-tempo playlists for recreational exercise (e.g., jogging), rehabilitation therapy (e.g., rhythmic gait training), or disc jockeying (e.g., dance mixes). Although there are numerous software algorithms which return simple point estimate statistics of “overall” tempo, none has operationalized the beat-to-beat stability of an inter-beat interval series. We propose such a method here, along with several novel summary statistics. We illustrate this approach using a public data set (the 10,000-item subset of the Million Song Dataset) and outline a series of future steps for this project. © 2013 International Society for Music Information Retrieval."
Pachet F.; Suzda J.; Martín D.,A comprehensive online database of machine-readable leadsheets for jazz standards,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923211959&partnerID=40&md5=101af6554b24d87bff712c578631f8f2,"Pachet F., Sony CSL, Japan; Suzda J., Sony CSL, Japan; Martín D., Sony CSL, Japan","Jazz standards are songs representative of a body of musical knowledge shared by most professional jazz musicians. As such, the corpus of jazz standards constitutes a unique opportunity to study a musical genre with a “closed-world” approach, since most jazz composers are no longer in activity today. Although many scores for jazz standards can be found on the Internet, no effort, to our knowledge, has been dedicated so far to building a comprehensive database of machine-readable scores for jazz standards. This paper reports on the rationale, design and population of such a database, containing harmonic (chord progressions) as well as melodic and structural information. The database can be used to feed both analysis and generation systems. We report on preliminary results in this vein. We get around the tricky and often unclear copyright issues imposed by the publishing industry, by providing only statistical information about songs. The completeness of such a database should benefit many research experiments in MIR and opens up novel and exciting applications in music generation exploiting symbolic information, notably in style modeling. © 2013 International Society for Music Information Retrieval."
Krebs F.; Böck S.; Widmer G.,Rhythmic pattern modeling for beat and downbeat tracking in musical audio,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911929737&partnerID=40&md5=24ec924b8dff3b8ad441912b6a16f4cc,"Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Rhythmic patterns are an important structural element in music. This paper investigates the use of rhythmic pattern modeling to infer metrical structure in musical audio recordings. We present a Hidden Markov Model (HMM) based system that simultaneously extracts beats, downbeats, tempo, meter, and rhythmic patterns. Our model builds upon the basic structure proposed by Whiteley et. al [20], which we further modified by introducing a new observation model: rhythmic patterns are learned directly from data, which makes the model adaptable to the rhythmical structure of any kind of music. For learning rhythmic patterns and evaluating beat and downbeat tracking, 697 ballroom dance pieces were annotated with beat and measure information. The results showed that explicitly modeling rhythmic patterns of dance styles drastically reduces octave errors (detection of half or double tempo) and substantially improves downbeat tracking. © 2013 International Society for Music Information Retrieval."
Robertson A.,Decoding tempo and timing variations in music recordings from beat annotations,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873446242&partnerID=40&md5=df0b9a770b5ba9c6f2843ac58edfe77e,"Robertson A., School of Electronic Engineering and Computer Science, United Kingdom","This paper addresses the problem of determining tempo and timing data from a list of beat annotations. Whilst an approximation to the tempo can be calculated from the inter-beat interval, the annotations also include timing variations due to expressively timed events, phase shifts and errors in the annotation times. These deviations tend to propagate into the tempo graph and so tempo analysis methods tend to average over recent inter-beat intervals. However, whilst this minimises the effect such timing deviations have on the local tempo estimate, it also obscures the expressive timing devices used by the performer. Here we propose a more formal method for calculation of the optimal tempo path through use of an appropriate cost function that incorporates tempo change, phase shift and expressive timing. © 2012 International Society for Music Information Retrieval."
Fohl W.; Turkalj I.; Meisel A.,A feature relevance study for guitar tone classification,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475561&partnerID=40&md5=283371e23b410343dd68d9cc15df3028,"Fohl W., HAW Hamburg University of Applied Sciences, Germany; Turkalj I., HAW Hamburg University of Applied Sciences, Germany; Meisel A., HAW Hamburg University of Applied Sciences, Germany","A series of experiments on the automatic classification of classical guitar sounds with support vector machines has been carried out to investigate the relevance of the features and to minimise the feature set for successful classification. Features used for classification were the time series of the partial tone amplitudes, and of the MFCCs, and the energy distribution of the nontonal percussive sound that is produced in the attack phase of the tone. Furthermore the influence of sound parameters as timbre, player, fret position and string number on the recognition rate is investigated. Finally, several nonlinear kernels are compared in their classification performance. It turns out, that a selection of 505 features out of the full feature set of 1155 elements does only reduce the recognition rate of a linear SVM from 82% to 78%. With the use of a polynomial instead of a linear kernel the recognition rate with the reduced feature set can even be increased to 84%. © 2012 International Society for Music Information Retrieval."
Ünal E.; Bozkurt B.; Kemal Karaosmanoǧlu M.,N-gram based statistical makam detection on makam music in Turkey using symbolic data,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455699&partnerID=40&md5=5338980d1a745c021e80bea812b28552,"Ünal E., TÜB IT AK-B ILGEM, Turkey; Bozkurt B., Bahçeşehir University, Turkey; Kemal Karaosmanoǧlu M., Yildiz Technical University, Turkey","This work studies the effect of different score representations and the potential of n-grams in makam classification for traditional makam music in Turkey. While makams are defined with various characteristics including a distinct set of pitches, pitch hierarchy, melodic direction, typical phrases and typical makam transitions, such characteristics result in certain n-gram distributions which can be used for makam detection effectively. 13 popular makams, some of which are very similar to each other, are used in this study. Using the leave-one-out strategy, makam models are created statistically and tested against the left out music piece. Tests indicate that n-gram based statistical modeling and perplexity based similarity metric can be effectively used for makam detection. However the main dimension that cannot be captured is the overall progression which is the most unique feature for classification of close makams that uses the same scale notes as well as the same tonic. © 2012 International Society for Music Information Retrieval."
De Haas W.B.; Magalhães J.P.; Wiering F.,Improving audio chord transcription by exploiting harmonic and metric knowledge,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462428&partnerID=40&md5=a32daf37d2b61f54202459bf899d89bb,"De Haas W.B., Utrecht University, Netherlands; Magalhães J.P., University of Oxford, United Kingdom; Wiering F., Utrecht University, Netherlands","We present a new system for chord transcription from polyphonic musical audio that uses domain-specific knowledge about tonal harmony and metrical position to improve chord transcription performance. Low-level pulse and spectral features are extracted from an audio source using the Vamp plugin architecture. Subsequently, for each beat-synchronised chromagram we compute a list of chord candidates matching that chromagram, together with the confidence in each candidate. When one particular chord candidate matches the chromagram significantly better than all others, this chord is selected to represent the segment. However, when multiple chords match the chromagram similarly well, we use a formal music theoretical model of tonal harmony to select the chord candidate that best matches the sequence based on the surrounding chords. In an experiment we show that exploiting metrical and harmonic knowledge yields statistically significant chord transcription improvements on a corpus of 217 Beatles, Queen, and Zweieck songs. © 2012 International Society for Music Information Retrieval."
Collins T.; Arzt A.; Flossmann S.; Widmer G.,SIARCT-CFP: Improving precision and the discovery of inexact musical patterns in point-set representations,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904316629&partnerID=40&md5=774a380f3306aba20627bff73ba912cb,"Collins T., Department of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University Linz, Austria; Flossmann S., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","The geometric approach to intra-opus pattern discovery (in which notes are represented as points in pitch-time space in order to discover repeated patterns within a piece of music) shows promise particularly for polyphonic music, but has attracted some criticism because: (1) the approach extends to a limited number of inexact repetition types only; (2) typically geometric pattern discovery algorithms have poor precision, returning many false positives. This paper describes and evaluates a solution to the inexactness problem where algorithms for pattern discovery and inexact pattern matching are integrated for the first time. Two complementary solutions are proposed and assessed for the precision problem, one involving categorisation (hence reduction) of output patterns, and the second involving a new algorithm that calculates the difference between consecutive point pairs, rather than all point pairs. © 2013 International Society for Music Information Retrieval."
Arjannikov T.; Sanden C.; Zhang J.Z.,Verifying tag annotations through association analysis,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960885181&partnerID=40&md5=9a329dfbfaeabe1e7b26ef501d02ab50,"Arjannikov T., University of Lethbridge, Canada; Sanden C., University of Lethbridge, Canada; Zhang J.Z., University of Lethbridge, Canada","Music tags provide descriptive and rich information about a music piece, including its genre, artist, emotion, instrument, etc. While many work on automating it, at present, tag annotation is largely a manual process. It often involves judgements and opinions from people of different background and level of musical expertise. Therefore, the resulting tags are usually subjective, ambiguous, and error-prone. To deal with this situation, we seek automatic methods to verify and monitor this process. Furthermore, because multiple tags can annotate each music piece, our task lends itself to multi-label methods which capture the inherent associations among annotations in a given music repository. In this paper, we propose a novel approach to verify the quality of music tag annotations via association analysis. We demonstrate the effectiveness of our approach through a series of simulations using four publicly available music datasets. To our knowledge, our work is among the initial efforts in verifying music tag annotations. © 2013 International Society for Music Information Retrieval."
Cherla S.; Weyde T.; d’Avila Garcez A.; Pearce M.,A distributed model for multiple-viewpoint melodic prediction,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905383620&partnerID=40&md5=6a460d4d52683a9d29656b1b06fffea7,"Cherla S., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; d’Avila Garcez A., Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Pearce M., Centre for Digital Music, Queen Mary University of London, United Kingdom","The analysis of sequences is important for extracting information from music owing to its fundamentally temporal nature. In this paper, we present a distributed model based on the Restricted Boltzmann Machine (RBM) for melodic sequences. The model is similar to a previous successful neural network model for natural language [2]. It is first trained to predict the next pitch in a given pitch sequence, and then extended to also make use of information in sequences of note-durations in monophonic melodies on the same task. In doing so, we also propose an efficient way of representing this additional information that takes advantage of the RBM’s structure. In our evaluation, this RBM-based prediction model performs slightly better than previously evaluated n-gram models in most cases. Results on a corpus of chorale and folk melodies showed that it is able to make use of information present in longer contexts more effectively than n-gram models, while scaling linearly in the number of free parameters required. © 2013 International Society for Music Information Retrieval."
Cheng T.; Dixon S.; Mauch M.,A deterministic annealing EM algorithm for automatic music transcription,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963975554&partnerID=40&md5=a7754762153ed0befc952b48f83c962f,"Cheng T., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom","In the past decade, non-negative matrix factorisation (NMF) and probabilistic latent component analysis (PLCA) have been used widely in automatic music transcription. Despite their successes, these methods only guarantee that the decomposition converges to a local minimum in the cost function. In order to find better local minima, we propose to extend an existing PLCA-based transcription method with the deterministic annealing EM (DAEM) algorithm. The PLCA update rules are modified by introducing a “temperature” parameter. At higher temperatures, general areas of the search space containing good solutions are found. As the temperature is gradually decreased, distinctions in the data are sharpened, resulting in a more fine-grained optimisation at each successive temperature. This process reduces the dependence on the initialisation, which is otherwise a limitation of NMF and PLCA approaches. The method was tested on two standard multi-instrument transcription data sets (MIREX and Bach10). Experimental results show that the proposed method significantly outperforms a state-of-the-art reference method, according to both frame-based and note-based metrics. An additional analysis of instrument assignment results shows that instrument spectra are typically modelled as mixtures of templates from several instruments. © 2013 International Society for Music Information Retrieval."
Silva D.F.; Papadopoulos H.; Batista G.E.A.P.A.; Ellis D.P.W.,A video compression-based approach to measure music structure similarity,2013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928159114&partnerID=40&md5=828ede74b8f055a09392269202d4981a,"Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Papadopoulos H., Laboratoire des Signaux et Systèmes (L2S), CNRS UMR, 8506, France; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Ellis D.P.W., Department of Electrical Engineering, Columbia University, United States","The choice of the distance measure between time-series representations can be decisive to achieve good classification results in many content-based information retrieval applications. In the field of Music Information Retrieval, two-dimensional representations of the music signal are ubiquitous. Such representations are useful to display patterns of evidence that are not clearly revealed directly in the time domain. Among these representations, self-similarity matrices have become common representations for visualizing the time structure of an audio signal. In the context of organizing recordings, recent work has shown that, given a collection of recordings, it is possible to to group performances of the same musical work based on the pairwise similarity between structural representations of the audio signal. In this work, we introduce the use of the Campana-Keogh distance, a video compression-based measure, to compare musical items based on their structure. Through extensive experiments, we show that the use of this distance measure outperforms the results of previous work using similar approaches but other distance measures. Along with quantitative results, detailed examples are provided to to illustrate the benefits of using the newly proposed distance measure. © 2013 International Society for Music Information Retrieval."
Watson D.; Mandryk R.L.,Modeling musical mood from audio features and listening context on an in-situ data set,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449069&partnerID=40&md5=4860f350164f46c6b4b5f53856b40ddb,"Watson D., University of Saskatchewan, Canada; Mandryk R.L., University of Saskatchewan, Canada","Real-life listening experiences contain a wide range of music types and genres. We create the first model of musical mood using a data set gathered in-situ during a user's daily life. We show that while audio features, song lyrics and socially created tags can be used to successfully model musical mood with classification accuracies greater than chance, adding contextual information such as the listener's affective state or listening context can improve classification accuracy. We successfully classify musical arousal with a classification accuracy of 67% and musical valence with an accuracy of 75% when using both musical features and listening context. © 2012 International Society for Music Information Retrieval."
Martin B.; Hanna P.; Ta V.-T.; Ferraro P.; Desainte-Catherine M.,Exemplar-based assignment of large missing audio parts using string matching on tonal features,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591527&partnerID=40&md5=0a9d22c68a835cfd4e29ec6f1ed87afe,"Martin B., LaBRI, Universite de Bordeaux, France; Hanna P., LaBRI, Universite de Bordeaux, France; Ta V.-T., LaBRI, Universite de Bordeaux, France; Ferraro P., LaBRI, Universite de Bordeaux, France; Desainte-Catherine M., LaBRI, Universite de Bordeaux, France","We propose a new approach for assigning audio data in large missing audio parts (from 1 to 16 seconds). Inspired by image inpainting approaches, the proposed method uses the repetitive aspect of music pieces on musical features to recover missing segments via an exemplar-based reconstruction. Tonal features combined with a string matching technique allows locating repeated segments accurately. The evaluation consists in performing on both musician and nonmusician subjects listening tests of randomly reconstructed audio excerpts, and experiments highlight good results in assigning musically relevant parts. The contribution of this paper is twofold: bringing musical features to solve a signal processing problem in the case of large missing audio parts, and successfully applying exemplar-based techniques on musical signals while keeping a musical consistency on audio pieces. © 2011 International Society for Music Information Retrieval."
Abeßer J.; Lartillot O.,Modelling musical attributes to characterize two-track recordings with bass and drums,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604006&partnerID=40&md5=4c63af9d3e1242e5bc96c9809fd06a3e,"Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvä skylä, Finland","In this publication, we present a method to characterize twotrack audio recordings (bass and drum instruments) based on musical attributes. These attributes are modelled using different regression algorithms. All regression models are trained based on score-based audio features computed from given scores and human annotations of the attributes. We compare five regression model configurations that predict values of different attributes. The regression models are trained based on manual annotations from 11 participants for a data-set of 70 double-track recordings. The average estimation errors within a cross-validation scenario are computed as evaluation measure. Models based on Partial Least Squares Regression (PLSR) with preceding Principal Component Analysis (PCA) and on Support Vector Regression (SVR) performed best. © 2011 International Society for Music Information Retrieval."
Vaizman Y.; Granot R.Y.; Lanckriet G.,Modeling dynamic patterns for emotional content in music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598676&partnerID=40&md5=2b2f7ebfec03456c248442696b939a80,"Vaizman Y., Edmond and Lily Safra Center for Brain Sciences, ICNC, Hebrew University, Jerusalem, Israel; Granot R.Y., Musicology Dept., Hebrew University, Jerusalem, Israel; Lanckriet G., Electrical and Computer Engineering Dept., University of California, San Diego, United States","Emotional content is a major component in music. It has long been a research topic of interest to discover the acoustic patterns in the music that carry that emotional information, and enable performers to communicate emotional messages to listeners. Previous works looked in the audio signal for local cues, most of which assume monophonic music, and their statistics over time. Here, we used generic audio features, that can be calculated for any audio signal, and focused on the progression of these features through time, investigating how informative the dynamics of the audio is for emotional content. Our data is comprised of piano and vocal improvisations of musically trained performers, instructed to convey 4 categorical emotions. We applied Dynamic Texture Mixture (DTM), that models both the instantaneous sound qualities and their dynamics, and demonstrated the strength of the model. We further showed that once taking the dynamics into account even highly reduced versions of the generic audio features carry a substantial amount of information about the emotional content. Finally, we demonstrate how interpreting the parameters of the trained models can yield interesting cognitive suggestions. © 2011 International Society for Music Information Retrieval."
Chuan C.-H.,A comparison of statistical and rule-based models for style-specific harmonization,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592691&partnerID=40&md5=9fd73fd83d26a83fc03cd4b43559ba13,"Chuan C.-H., School of Computing, University of North Florida, Jacksonville, FL, United States","The process of generating chords for harmonizing a melody with the goal of mimicking an artist's style is investigated in this paper. We compared and tested three different approaches, including a rule-based model, a statistical model, and a hybrid system of the two, for such tasks. Experiments were conducted using songs from seven stylistically identifiable pop/rock bands, and the chords generated by the systems were compared to the ones in the artists' original work. Evaluations were performed on multiple aspects, including calculating the average percentage of chords that were the same and those that were related, studying the manner in which the size of the training set affects the output harmonization, and examining a system's behaviors in terms of the ability of generating unseen chords and the number of unique chords produced per song. We observed that the rule-based system performs comparably well while the result of the system with learning capability varies as the training set grows. © 2011 International Society for Music Information Retrieval."
Martin B.; Brown D.G.; Hanna P.; Ferraro P.,Blast for audio sequences alignment: A fast scalable cover identification tool,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873425329&partnerID=40&md5=d7ae6bbda38de4f904d07f3c09436ae8,"Martin B., Université de Bordeaux, CNRS, LaBRI, UMR 5800, France; Brown D.G., University of Waterloo, Cheriton School of Computer Science, Canada; Hanna P., Université de Bordeaux, CNRS, LaBRI, UMR 5800, France; Ferraro P., Université de Bordeaux, CNRS, LaBRI, UMR 5800, France","Searching for similarities in large musical databases is common for applications such as cover song identification. These methods typically use dynamic programming to align the shared musical motifs between subparts of two recordings. Such music local alignment methods are slow, as are the bioinformatics algorithms they are closely related to. We have adapted the ideas of the Basic Local Alignment Search Tool (BLAST) for biosequence alignment to the domain of aligning sequences of chroma features. Our tool allows local music sequence alignment in near-linear time. It identifies small regions of exact match between sequences, called seeds, and builds local alignments that include these seeds. Seed determination is a key issue for the accuracy of the method and closely depends on the database, the representation and the application. We introduce a particular seeding approach for cover detection, and evaluate it on both a 2000-piece training set and the million song dataset (MSD). We show that the heuristic alignment drastically improves time computation for cover song detection. Alignment sensitivity is still very high on the small database, but is dramatically weakened on the MSD, due to differences in chroma features. We discuss the impact of different choices of these features on alignment of musical pieces. © 2012 International Society for Music Information Retrieval."
Woelfer J.P.; Lee J.H.,The role of music in the lives of homeless young people: A preliminary report,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873422734&partnerID=40&md5=64fc3e9b3e2df60067c45713b8700766,"Woelfer J.P., Information School, University of Washington, Seattle, WA 98195, United States; Lee J.H., Information School, University of Washington, Seattle, WA 98195, United States","This paper is a preliminary report of findings in an ongoing study of the role of music in the lives of homeless young people which is taking place in Vancouver, British Columbia and Seattle, WA. One hundred homeless young people in Vancouver took part in online surveys, 20 of these young people participated in interviews and 64 completed design activities. Surveys included demographic and music questions. Interviews consisted of questions about music listening and preferences. In the design activities, participants envisioned a music device and provided a drawing and a scenario. Since the study is on-going, findings are limited to descriptive analysis of survey data supplemented with interview data. These findings provide initial insights into music listening behaviors, social aspects of shared music interests, and preferred music genres, bands and artists, and moods. © 2012 International Society for Music Information Retrieval."
Kamalzadeh M.; Baur D.; Möller T.,A survey on music listening and management behaviours,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873417428&partnerID=40&md5=c1c3000e855de5fc6edb61ac6dfcaa10,"Kamalzadeh M., Simon Fraser University, Canada; Baur D., University of Calgary, Canada; Möller T., Simon Fraser University, Canada","We report the results of a survey on music listening and management behaviours. The survey was conducted online with 222 participants with mostly technical backgrounds drawn from a college age population. The median size of offline music collections was found to be roughly 2540 songs (sum of physical media and digital files). The major findings of our survey show that elements such as familiarity of songs, how distracting they are, how much they match the listener's mood, and the desire of changing the mood within one listening session, are all affected by the activity during which music is listened to. While people want to have options for manipulating the above elements to control their experience, they prefer a minimal amount of interaction in general. Current music players lack such flexibility in their controls. Finally, online recommender systems have not gained much popularity thus far. © 2012 International Society for Music Information Retrieval."
Andén J.; Mallat S.,Multiscale scattering for audio classification,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597627&partnerID=40&md5=d8af0a041cf0cd29c70fa75a434f6be0,"Andén J., CMAP, Ecole Polytechnique, 91128 Palaiseau, France; Mallat S., CMAP, Ecole Polytechnique, 91128 Palaiseau, France","Mel-frequency cepstral coefficients (MFCCs) are efficient audio descriptors providing spectral energy measurements over short time windows of length 23 ms. These measurements, however, lose non-stationary spectral information such as transients or time-varying structures. It is shown that this information can be recovered as spectral co-occurrence coefficients. Scattering operators compute these coefficients with a cascade of wavelet filter banks and modulus rectifiers. The signal can be reconstructed from scattering coefficients by inverting these wavelet modulus operators. An application to genre classification shows that second-order cooccurrence coefficients improve results obtained by MFCC and Delta-MFCC descriptors. © 2011 International Society for Music Information Retrieval."
Ewert S.; Müller M.,Score-informed voice separation for piano recordings,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873590791&partnerID=40&md5=35c68943022e842dc689852b8bd03f91,"Ewert S., Computer Science III, University of Bonn, Germany; Müller M., MPI Informatik, Saarland University, Germany","The decomposition of a monaural audio recording into musicallymeaningful sound sources or voices constitutes a fundamental problem in music information retrieval. In this paper, we consider the task of separating a monaural piano recording into two sound sources (or voices) that correspond to the left hand and the right hand. Since in this scenario the two sources share many physical properties, sound separation approaches identifying sources based on their spectral envelope are hardly applicable. Instead, we propose a score-informed approach, where explicit note events specified by the score are used to parameterize the spectrogram of a given piano recording. This parameterization then allows for constructing two spectrograms considering only the notes of the left hand and the right hand, respectively. Finally, inversion of the two spectrograms yields the separation result. First experiments show that our approach, which involves high-resolution music synchronization and parametric modeling techniques, yields good results for realworld non-synthetic piano recordings. © 2011 International Society for Music Information Retrieval."
Gómez E.; Cañadas F.; Salamon J.; Bonada J.; Vera P.; Cabañas P.,Predominant fundamental frequency Estimation vs Singing voice separation for the automatic transcription of accompanied flamenco singing,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424095&partnerID=40&md5=b6ee60a7980a6912e1f74830f5ef3161,"Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Cañadas F., Telecommunication Engineering Department, University of Jaen, Spain; Salamon J., Music Technology Group, Universitat Pompeu Fabra, Spain; Bonada J., Music Technology Group, Universitat Pompeu Fabra, Spain; Vera P., Telecommunication Engineering Department, University of Jaen, Spain; Cabañas P., Telecommunication Engineering Department, University of Jaen, Spain","This work evaluates two strategies for predominant fundamental frequency (f0) estimation in the context of melodic transcription from flamenco singing with guitar accompaniment. The first strategy extracts the f 0 from salient pitch contours computed from the mixed spectrum; the second separates the voice from the guitar and then performs mono-phonic f 0 estimation. We integrate both approaches with an automatic transcription system, which first estimates the tuning frequency and then implements an iterative strategy for note segmentation and labeling. We evaluate them on a flamenco music collection, including a wide range of singers and recording conditions. Both strategies achieve satisfying results. The separation-based approach yields a good overall accuracy (76.81%), although instrumental segments have to be manually located. The predominant f0 estimator yields slightly higher accuracy (79.72%) but does not require any manual annotation. Furthermore, its accuracy increases (84.68%) if we adapt some algorithm parameters to each analyzed excerpt. Most transcription errors are due to incorrect f0 estimations (typically octave and voicing errors in strong presence of guitar) and incorrect note segmentation in highly ornamented sections. Our study confirms the difficulty of transcribing flamenco singing and the need for repertoire-specific and assisted algorithms for improving state-of-the-art methods. © 2012 International Society for Music Information Retrieval."
Chien Y.-R.; Wang H.-M.; Jeng S.-K.,An acoustic-phonetic approach to vocal melody extraction,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588865&partnerID=40&md5=5a7088884f2452f88276a795f64f84c0,"Chien Y.-R., Graduate Institute of Communication Engineering, National Taiwan University, Taiwan, Institute of Information Science, Academia Sinica, Taiwan; Wang H.-M., Institute of Information Science, Academia Sinica, Taiwan; Jeng S.-K., Graduate Institute of Communication Engineering, National Taiwan University, Taiwan, Department of Electrical Engineering, National Taiwan University, Taiwan","This paper addresses the problemof extracting vocalmelodies from polyphonic audio. In short-term processing, a timbral distance between each pitch contour and the space of human voice is measured, so as to isolate any vocal pitch contour. Computation of the timbral distance is based on an acousticphonetic parametrization of human voiced sound. Longterm processing organizes short-term procedures in such a manner that relatively reliable melody segments are determined first. Tested on vocal excerpts from the ADC 2004 dataset, the proposed system achieves an overall transcription accuracy of 77%. © 2011 International Society for Music Information Retrieval."
Marques G.; Domingues M.A.; Langlois T.; Gouyon F.,Three current issues in music autotagging,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607627&partnerID=40&md5=b25bd8015e7c4adfa7abbfebdfbd5499,"Marques G., DEETC-ISEL Lisboa, Portugal; Domingues M.A., INESC Porto, Portugal; Langlois T., DI-FCUL Lisboa, Portugal; Gouyon F., INESC Porto, Portugal","The purpose of this paper is to address several aspects of music autotagging. We start by presenting autotagging experiments conducted with two different systems and show performances on a par with a method representative of the state-of-the-art. Beyond that, we illustrate via systematic experiments the importance of a number of issues relevant to autotagging, yet seldom reported in the literature. First, we show that the evaluation of autotagging techniques is fragile in the sense that small alterations to the set of tags to be learned, or in the set of music pieces may lead to dramatically different results. Hence we stress a set of methodological recommendations regarding data and evaluation metrics. Second, we conduct experiments on the generality of autotagging models, showing that a number of different methods at a similar performance level to the state-of-the-art fail to learn tag models able to generalize to datasets from different origins. Third we show that current performance level of a direct mapping between audio features and tags still appears insufficient to enable the possibility of exploiting natural tag correlations as a second stage to improve performance. © 2011 International Society for Music Information Retrieval."
Koduri G.K.; Miron M.; Serrà J.; Serra X.,Computational approaches for the understanding of melody in carnatic music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604998&partnerID=40&md5=eb8e4e40631de142f7c591bcc43bde58,"Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Miron M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The classical music traditions of the Indian subcontinent, Hindustani and Carnatic, offer an excellent ground on which to test the limitations of current music information research approaches. At the same time, studies based on these music traditions can shed light on how to solve new and complex music modeling problems. Both traditions have very distinct characteristics, specially compared with western ones: they have developed unique instruments, musical forms, performance practices, social uses and context. In this article, we focus on the Carnatic music tradition of south India, especially on its melodic characteristics. We overview the theoretical aspects that are relevant for music information research and discuss the scarce computational approaches developed so far. We put emphasis on the limitations of the current methodologies and we present open issues that have not yet been addressed and that we believe are important to be worked on. © 2011 International Society for Music Information Retrieval."
Ness S.; Trail S.; Driessen P.; Schloss A.; Tzanetakis G.,Music information robotics: Coping strategies for musically challenged robots,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602299&partnerID=40&md5=587d1842b0f5df5802a200a7fd3af296,"Ness S., University of Victoria, Canada; Trail S., University of Victoria, Canada; Driessen P., University of Victoria, Canada; Schloss A., University of Victoria, Canada; Tzanetakis G., University of Victoria, Canada","In the past few years there has been a growing interest in music robotics. Robotic instruments that generate sound acoustically using actuators have been increasingly developed and used in performances and compositions over the past 10 years. Although such devices can be very sophisticated mechanically, in most cases they are passive devices that directly respond to control messages from a computer. In the few cases where more sophisticated control and feedback is employed it is in the form of simple mappings with little musical understanding. Several techniques for extracting musical information have been proposed in the field of music information retrieval. In most cases the focus has been the batch processing of large audio collections rather than real time performance understanding. In this paper we describe how such techniques can be adapted to deal with some of the practical problems we have experienced in our own work with music robotics. Of particular importance is the idea of self-awareness or proprioception in which the robot(s) adapt their behavior based on understanding the connection between their actions and sound generation through listening. More specifically we describe techniques for solving the following problems: 1) controller mapping 2) velocity calibration, and 3) gesture recognition. © 2011 International Society for Music Information Retrieval."
Hamel P.; Bengio Y.; Eck D.,Building musically-relevant audio features through multiple timescale representations,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419188&partnerID=40&md5=a2841e1b2c67870d49a0ebcb65c01a85,"Hamel P., DIRO, Université de Montréal, Montréal, QC, Canada; Bengio Y., DIRO, Université de Montréal, Montréal, QC, Canada; Eck D., Google Inc., Mountain View, CA, United States","Low-level aspects of music audio such as timbre, loud-ness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and require a better representation of time dynamics. For various music information retrieval tasks, one would benefit from modelling both low and high level aspects in a unified feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale features. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for automatic tag annotation. © 2012 International Society for Music Information Retrieval."
Daido R.; Hahm S.-J.; Ito M.; Makino S.; Ito A.,A system for evaluating singing enthusiasm for karaoke,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607349&partnerID=40&md5=f2a90a3ba8df1ae2fb0002f17702d7bd,"Daido R., Graduate School of Engineering, Tohoku University, Japan; Hahm S.-J., Graduate School of Engineering, Tohoku University, Japan; Ito M., Tohoku Institute of Technology, Japan; Makino S., Tohoku Bunka Gakuen University, Japan; Ito A., Graduate School of Engineering, Tohoku University, Japan","Evaluation of singing skill is a popular function of karaoke machines. Here, we introduce a different aspect of evaluating the singing voice of an amateur singer: ""enthusiasm"". First, we investigated whether human listeners can evaluate enthusiasm consistently and whether the listener's perception matches the singer's enthusiasm. We then identified three acoustic features relevant to the perception of enthusiasm: A-weighted power, ""fall-down"", and vibrato extent. Finally, we developed a system for evaluating singing enthusiasm using these features, and obtained a correlation coefficient of 0.65 between the system output and human evaluation. © 2011 International Society for Music Information Retrieval."
Fuhrmann F.; Herrera P.,Quantifying the relevance of locally extracted information for musical instrument recognition from entire pieces of music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598303&partnerID=40&md5=12496e40e298cd8af5f869414575b55a,"Fuhrmann F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this work we study the problem of automatic musical instrument recognition from entire pieces of music. In particular, we present and evaluate 4 different methods to select, from an unknown piece of music, relevant excerpts in terms of instrumentation, on top of which instrument recognition techniques are applied to infer the labels. Since the desired information is assumed to be redundant (we may extract just a few labels from a thousands of audio frames) we examine the recognition performance, the amount of data used for processing, and their possible correlation. Experimental results on a collection ofWestern music pieces reveal state-ofthe- art performance in instrument recognition together with a great reduction of the required input data. However, we also observe a performance ceiling with the currently applied instrument recognition method. © 2011 International Society for Music Information Retrieval."
Macrae R.; Dixon S.,Ranking lyrics for online search,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439450&partnerID=40&md5=93775d6bf117b043f2350151190675fc,"Macrae R., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","When someone wishes to find the lyrics for a song they typically go online and use a search engine. There are a large number of lyrics available on the internet as the effort required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the user with customary search engine page ranking formula deciding the ordering of these results based on links, views, clicks, etc. However the content, and specifically, the accuracy of the lyrics in question are not analysed or used in any way to determine the rank of the lyrics, despite this being of concern to the searcher. In this work, we show that online lyrics are often inaccurate and the ranking methods used by search engines do not distinguish the more accurate annotations. We present an alternative method for ranking lyrics based purely on the collection of lyrics themselves using the Lyrics Concurrence. © 2012 International Society for Music Information Retrieval."
Juhász Z.,Low dimensional visualisation of folk music systems using the self organising cloud,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604482&partnerID=40&md5=bb8e3abb90d025fe013a2c39e2e3b5ce,"Juhász Z., Res Inst. For Technical Physics and Materials Sciences, H-1525 Budapest, P.O.B. 49, Hungary","We describe a computational method derived from self organizing mapping and multidimensional scaling algorithms for automatic classification and visual clustering of large vector databases. Testing the method on a large corpus of folksongs we have found that the performance of the classification and topological clustering was significantly improved compared to current techniques. Applying the method to an analysis of the connections of 31 Eurasian and North-American folk music cultures, a clearly interpretable system of musical connections was revealed. The results show the relevance of the musical language groups in the oral tradition of the humanity. © 2011 International Society for Music Information Retrieval."
Hu Y.; Ogihara M.,Nextone player: A music recommendation system based on user behavior,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588899&partnerID=40&md5=36cef38f05794c7dfd3969ec2d263925,"Hu Y., Department of Computer Science, University of Miami, United States; Ogihara M., Department of Computer Science, University of Miami, United States","We present a new approach to recommend suitable tracks from a collection of songs to the user. The goal of the system is to recommend songs that are favored by the user, are fresh to the user's ear, and fit the user's listening pattern. We use ""Forgetting Curve"" to assess freshness of a song and evaluate ""favoredness"" using user log. We analyze user's listening pattern to estimate the level of interest of the user in the next song. Also, we treat user behavior on the song being played as feedback to adjust the recommendation strategy for the next one. We develop an application to evaluate our approach in the real world. The user logs of trial volunteers show good performance of the proposed method. © 2011 International Society for Music Information Retrieval. © 2011 International Society for Music Information Retrieval."
Davies S.; Allen P.; Mann M.; Cox T.,Musical moods: A mass participation experiment for affective classification of music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594935&partnerID=40&md5=ef18b68c768e9390f2797c065688bf2e,"Davies S., BBC Research and Development, United Kingdom; Allen P., BBC Research and Development, United Kingdom; Mann M., BBC Research and Development, United Kingdom; Cox T., University of Salford, United Kingdom","In this paper we present our mass participation experiment, Musical Moods. This experiment placed 144 theme tunes online, taken from TV and radio programmes from the last 60 years of the British Broadcasting Corporations (BBC) output. Members of the public were then invited to audition then rate these according to a set of semantic differentials based on the affective categories of evaluation, potency and activity. Participants were also asked to rate their familiarity of the theme tune and how much they liked the theme tune. A final question asked participants to identify the genre of the TV programme with which they associated the tune. The purpose of this is to aid in the affective classification of large-scale TV archives, such as those possessed by the BBC. We find correlations between evaluation and potency, potency and activity but none between activity and evaluation but no clear correlation between affect and genre. This paper presents our key findings from an analysis of the results along with our plans for further analysis. The initial results from this experiment are based on an analyses of over 51,000 answers from over 13,000 participants. © 2011 International Society for Music Information Retrieval."
Peeters G.; Cornu F.; Charbuillet Ch.; Tardieu D.; Burred J.J.; Vian M.; Botherel V.; Rault J.-B.; Cabanal J.-Ph.,"A Multimedia search and navigation prototype, including music and video-clips",2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427658&partnerID=40&md5=359eaf43c6c69d7012f8832c90678898,"Peeters G., STMS, IRCAM-CNRS-UPMC, France; Cornu F., STMS, IRCAM-CNRS-UPMC, France; Charbuillet Ch., STMS, IRCAM-CNRS-UPMC, France; Tardieu D., STMS, IRCAM-CNRS-UPMC, France; Burred J.J., STMS, IRCAM-CNRS-UPMC, France; Vian M., Bertin Technologies, France; Botherel V., Orange-Labs, France; Rault J.-B., Orange-Labs, France; Cabanal J.-Ph., Orange-Labs, France","Moving music indexing technologies developed in a research lab to their integration and use in the context of a third-party search and navigation engine that indexes music files, archives of TV music programs and videoclips, involves a set of choices and works that we relate here. First one has to choose technologies that perform well, which are scalable (in terms of computation time of extraction and item comparison for search-by-similarity), and which are not sensitive to media quality (being able to process equally music files or audio tracks from video archives). These technologies must be applied to estimate tags chosen to be understandable and useful for users (the specific genre and mood tags or other content-descriptions). For training the related technologies, relevant and reliable annotated corpus must be created. For using them, relevant user-scenarios must be created and friendly Graphical User-Interface designed. In this paper, we share the experience we had in a recent project on integrating six state-of-the-art music-indexing technologies in a multimedia search and navigation prototype. © 2012 International Society for Music Information Retrieval."
Boulanger-Lewandowski N.; Bengio Y.; Vincent P.,Discriminative non-negative matrix factorization for multiple pitch estimation,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873429839&partnerID=40&md5=cd0c49e5b38033674b1fd9e048c31a45,"Boulanger-Lewandowski N., Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada; Bengio Y., Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada; Vincent P., Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada","In this paper, we present a supervised method to improve the multiple pitch estimation accuracy of the non-negative matrix factorization (NMF) algorithm. The idea is to extend the sparse NMF framework by incorporating pitch information present in time-aligned musical scores in order to extract features that enforce the separability between pitch labels. We introduce two discriminative criteria that maximize inter-class scatter and quantify the predictive potential of a given decomposition using logistic regressors. Those criteria are applied to both the latent variable and the deterministic autoencoder views of NMF, and we devise efficient update rules for each. We evaluate our method on three polyphonic datasets of piano recordings and orchestral instrument mixes. Both models greatly enhance the quality of the basis spectra learned by NMF and the accuracy of multiple pitch estimation. © 2012 International Society for Music Information Retrieval."
Panagakis Y.; Kotropoulos C.,Music structure analysis by ridge regression of beat-synchronous audio features,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420616&partnerID=40&md5=955288cfe54c00575aadfcaf20dafe89,"Panagakis Y., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Kotropoulos C., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece","A novel unsupervised method for automatic music structure analysis is proposed. Three types of audio features, namely the mel-frequency cepstral coefficients, the chroma features, and the auditory temporal modulations are employed in order to form beat-synchronous feature sequences modeling the audio signal. Assume that the feature vectors from each segment lie in a subspace and the song as a whole occupies the union of several subspaces. Then any feature vector can be represented as a linear combination of the feature vectors stemming from the same subspace. The coefficients of such a linear combination are found by solving an appropriate ridge regression problem, resulting to the ridge representation (RR) of the audio features. The RR yields an affinity matrix with nonzero within-subspace affinities and zero between-subspace ones, revealing the structure of the music recording. The segmentation of the feature sequence into music segments is found by applying the normalized cuts algorithm to the RR-based affinity matrix. In the same context, the combination of multiple audio features is investigated as well. The proposed method is referred to as ridge regression-based music structure analysis (RRMSA). State-of-the-art performance is reported for the RRMSA by conducting experiments on the manually annotated Beatles benchmark dataset. © 2012 International Society for Music Information Retrieval."
O'Hara T.; Schüler N.; Lu Y.; Tamir D.E.,Inferring chord sequence meanings via lyrics: Process and evaluation,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873437186&partnerID=40&md5=1aab5fb8e6a92fc7505cecd77718d869,"O'Hara T., Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States; Schüler N., School of Music, Texas State University, San Marcos, TX 78666, United States; Lu Y., Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States; Tamir D.E., Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States","We improve upon our simple approach for learning the ""associational meaning"" of chord sequences from lyrics based on contingency statistics induced over a set of lyrics with chord annotations. Specifically, we refine this process by using word alignment tools developed for statistical machine translation, and we also use a much larger set of chord annotations. In addition, objective evaluation measures are included. Thus, this work validates a novel application of lexicon induction techniques over parallel corpora to a domain outside of natural language learning. To confirm the associations commonly attributed to major versus minor chords (i.e., happy and sad, respectively), we compare the inferred word associations against synonyms reflecting this dichotomy. To evaluate meanings associated with chord sequences, we check how often tagged chords occur in songs labeled with the same overall meaning. © 2012 International Society for Music Information Retrieval."
Liem C.C.S.; Hanjalic A.,Expressive timing from cross-performance and audio-based alignment patterns: An extended case study,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606376&partnerID=40&md5=d08feb0c87890c1327003042fd28df01,"Liem C.C.S., Multimedia Information Retrieval Lab, Delft University of Technology, Netherlands; Hanjalic A., Multimedia Information Retrieval Lab, Delft University of Technology, Netherlands","Audio recordings of classical music pieces reflect the artistic interpretation of the piece as seen by the recorded performing musician. With many recordings being typically available for the same music piece, multiple expressive rendition variations of this piece are obtained, many of which are induced by the underlying musical content. In earlier work, we focused on timing as a means of expressivity, and proposed a light-weight, unsupervised and audio-based method to study timing deviations among different performances through alignment patterns. By using the standard deviation of alignment patterns as a measure for the display of individuality in a recording, structural and interpretational aspects of a music piece turned out to be highlighted in a qualitative case study on five Chopin mazurkas. In this paper, we propose an entropy-based deviation measure as an alternative to the existing standard deviation measure. The obtained results for multiple short-time window resolutions, both from a quantitative and qualitative perspective, strengthen our earlier finding that the found patterns are musically informative and confirm that entropy is a good alternative measure for highlighting expressive timing deviations in recordings. © 2011 International Society for Music Information Retrieval."
Bragg J.; Chew E.; Shieber S.,Neo-riemannian cycle detection with weighted finite-state transducers,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607621&partnerID=40&md5=56d9a6b22e1989ca0835af25d85818fa,"Bragg J., Harvard University, United States; Chew E., Queen Mary University of London, United Kingdom; Shieber S., Harvard University, United States","This paper proposes a finite-state model for detecting harmonic cycles as described by neo-Riemannian theorists. Given a string of triads representing a harmonic analysis of a piece, the task is to identify and label all substrings corresponding to these cycles with high accuracy. The solution method uses a noisy channel model implemented with weighted finitestate transducers. On a dataset of four works by Franz Schubert, our model predicted cycles in the same regions as cycles in the ground truth with a precision of 0.18 and a recall of 1.0. The recalled cycles had an average edit distance of 3.2 insertions or deletions from the ground truth cycles, which average 6.4 labeled triads in length. We suggest ways in which our model could be used to contribute to current work in music theory, and be generalized to other music pattern-finding applications. © 2011 International Society for Music Information Retrieval."
Wang D.; Ogihara M.,Potential relationship discovery in tag-aware music style clustering and artist social networks,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598855&partnerID=40&md5=4bd8dd8031c496aa0caa08dae46b9495,"Wang D., Center for Computational Science, University of Miami, Coral Gables, FL, United States; Ogihara M., Center for Computational Science, University of Miami, Coral Gables, FL, United States","With the rapid growth of music information and data in today's ever changing world, exploring and analyzing music style has become more and more difficult. Traditional content-based methods for music style analysis and newly emerged tag-based methods usually assume music items are independent of each other. However, in real world applications, do there exist some relationships among them. In this paper, we construct the social relation graph among different music artists by extracting the friendship information from social media such as Twitter, and incorporate the generated social networking graph into tag-based music style clustering. Experiments on real data show the effectiveness of this novel integration of different information sources. © 2011 International Society for Music Information Retrieval."
Bennett C.; McNeer R.; Leider C.,Urgency analysis of audible alarms in the operating room,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873603265&partnerID=40&md5=26e0e2e0f8251dd01d265725d791880f,"Bennett C., Department of Anesthesiology, Miller School of Medicine, University of Miami, United States; McNeer R., Department of Anesthesiology, Miller School of Medicine, University of Miami, United States; Leider C., Department of Music Engineering, Frost School of Music, University of Miami, United States","Recent studies by researchers, governmental agencies, and safety organizations have recognized a deficiency in the performance of medically related audible alarms [1-4]. In the clinical setting, care providers can suffer from alarm fatigue, a condition in which audible alarms in an operating room are perceived as a nuisance. In this study, we explore the auditory features associated with current audible alarms using tools from the music information retrieval community, and then we examine how those auditory features correlate to listeners' perception of urgency. The results show that aperiodic changes in the auditory spectrum over time are the most salient contributor to the perception of urgency in sound. These results could inform the development of a novel standard regarding the composition of medical audible alarms. © 2011 International Society for Music Information Retrieval."
Hankinson A.; Porter A.; Burgoyne J.A.; Thompson J.; Vigliensoni G.; Liu W.; Chiu R.; Fujinaga I.,Digital document image retrieval using optical music recognition,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873418537&partnerID=40&md5=ba7e4951a47c55d4f1c125b8fe62b180,"Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Porter A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Thompson J., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Liu W., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Chiu R., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada","Optical music recognition (OMR) and optical character recognition (OCR) have traditionally been used for document transcription - that is, extracting text or symbolic music from page images for use in an editor while discarding all spatial relationships between the transcribed notation and the original image. In this paper we discuss how OCR has shifted fundamentally from a transcription tool to an indexing tool for document image collections resulting from large digitization efforts. OMR tools and procedures, in contrast, are still focused on small-scale modes of operation. We argue that a shift in OMR development towards document image indexing would present new opportunities for searching, browsing, and analyzing large musical document collections. We present a prototype system we built to evaluate the tools and to develop practices needed to process print and manuscript sources. © 2012 International Society for Music Information Retrieval."
Sordo M.; Serrà J.; Koduri G.K.; Serra X.,Extracting semantic information from an online Carnatic music forum,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426831&partnerID=40&md5=90603c0e3fda24bd524eb1e9d7ce8e2c,"Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain; Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","By mining user-generated text content we can obtain music-related information that could not otherwise be extracted from audio signals or symbolic score representations. In this paper we propose a methodology for extracting music-related semantic information from an online discussion forum, rasikas.org, dedicated to the Carnatic music tradition. We first define a dictionary of relevant terms within categories such as raagas, taalas, performers, composers, and instruments, and create a complex network representation by matching such dictionary against the forum posts. This network representation is used to identify popular terms within the forum, as well as relevant co-occurrences and semantic relationships. This way, for instance, we are able to learn the instrument played by a performer with 95% accuracy, to discover the confusion between two raagas with different naming conventions, or to infer semantic relationships regarding lineage or musical influence. This contribution is a first step towards the automatic creation of ontologies for specific musical cultures. © 2012 International Society for Music Information Retrieval."
Pugin L.; Kepper J.; Roland P.; Hartwig M.; Hankinson A.,Separating presentation and content in MEI,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424411&partnerID=40&md5=16d3667a0fe3ab6681967fd923fe93ac,"Pugin L., Swiss RISM, Fribourg University, Germany; Kepper J., Edirom, Germany; Roland P., University of Virginia, United States; Hartwig M., Edirom, Germany; Hankinson A., McGill University, Schulich School of Music, Canada","Common Western music notation is traditionally organized on staves that can be grouped into systems. When multiple systems appear on a page, they are arranged from the top to the bottom of the page, similar to lines of words in a text document. Encoding music notation documents for printing requires this arrangement to be captured. However, in the music notation model proposed by the Music Encoding Initiative (MEI), the hierarchy of the XML sub-tree representing the music emphasizes the content rather than the layout. Since systems and pages do not coincide with the musical content, they are encoded in a secondary hierarchy that contains very limited information. In this paper, we present a complementary solution for augmenting the level of detail of the layout of musical documents; that is, the layout information can be encoded in a separate sub-tree with cross-references to other elements holding the musical content. The major advantage of the proposed solution is that it enables multiple layout descriptions, each describing a different visual instantiation of the same musical content. © 2012 International Society for Music Information Retrieval."
Bugge E.P.; Juncher K.L.; Mathiasen B.S.; Simonsen J.G.,Using sequence alignment and voting to improve optical music recognition from multiple recognizers,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596832&partnerID=40&md5=e18e72776f4a84c958ff3ac102c0c757,"Bugge E.P., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark; Juncher K.L., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark; Mathiasen B.S., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark; Simonsen J.G., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark","Digitalizing sheet music using Optical Music Recognition (OMR) is error-prone, especially when using noisy images created from scanned prints. Inspired by DNA-sequence alignment, we devise a method to use multiple sequence alignment to automatically compare output from multiple third party OMR tools and perform automatic error-correction of pitch and duration of notes. We perform tests on a corpus of 49 one-page scores of varying quality. Our method on average reduces the amount of errors from an ensemble of 4 commercial OMR tools. The method achieves, on average, fewer errors than each recognizer by itself, but statistical tests show that it is significantly better than only 2 of the 4 commercial recognizers. The results suggest that recognizers may be improved somewhat by sequence alignment and voting, but that more elaborate methods may be needed to obtain substantial improvements. All software, scanned music data used for testing, and experiment protocols are open source and available at: http://code.google.com/p/omr-errorcorrection/. © 2011 International Society for Music Information Retrieval."
Song Y.; Dixon S.; Pearce M.,Evaluation of musical features for emotion classification,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423683&partnerID=40&md5=56a721c74268089d9533d3c221d192f0,"Song Y., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Pearce M., Centre for Digital Music, Queen Mary University of London, United Kingdom","Because music conveys and evokes feelings, a wealth of research has been performed on music emotion recognition. Previous research has shown that musical mood is linked to features based on rhythm, timbre, spectrum and lyrics. For example, sad music correlates with slow tempo, while happy music is generally faster. However, only limited success has been obtained in learning automatic classifiers of emotion in music. In this paper, we collect a ground truth data set of 2904 songs that have been tagged with one of the four words ""happy"", ""sad"", ""angry"" and ""relaxed"", on the Last. FM web site. An excerpt of the audio is then retrieved from 7Digital.com, and various sets of audio features are extracted using standard algorithms. Two classifiers are trained using support vector machines with the polynomial and radial basis function kernels, and these are tested with 10-fold cross validation. Our results show that spectral features outperform those based on rhythm, dynamics, and, to a lesser extent, harmony. We also find that the polynomial kernel gives better results than the radial basis function, and that the fusion of different feature sets does not always lead to improved classification. © 2012 International Society for Music Information Retrieval."
Arzt A.; Böck S.; Widmer G.,Fast identification of piece and score position via symbolic fingerprinting,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421780&partnerID=40&md5=ced21e465b08ee2429520423e6650e3d,"Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In this paper we present a novel algorithm that, given a short snippet of an audio performance (piano music, for the time being), identifies the piece and the score position. Instead of using audio matching methods we propose a combination of a state-of-the-art music transcription algorithm and a new symbolic fingerprinting method. The resulting system is usable in both on-line and off-line scenarios and thus may be of use in many application areas. As the evaluation shows the system operates with only minimal lag and achieves high precision even with very short queries. © 2012 International Society for Music Information Retrieval."
Lagrange M.; Ozerov A.; Vincent E.,Robust singer identification in polyphonic music using melody enhancement and uncertainty-based learning,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421020&partnerID=40&md5=1acde08fa51d82611bdc7ffceaa95732,"Lagrange M., STMS, IRCAM - CNRS, UPMC, France; Ozerov A., Technicolor Research and Innovation, France; Vincent E., INRIA, Centre de Rennes - Bretagne Atlantique, France","Enhancing specific parts of a polyphonic music signal is believed to be a promising way of breaking the glass ceiling that most Music Information Retrieval (MIR) systems are now facing. The use of signal enhancement as a pre-processing step has led to limited improvement though, because distortions inevitably remain in the enhanced signals that may propagate to the subsequent feature extraction and classification stages. Previous studies attempting to reduce the impact of these distortions have relied on the use of feature weighting or missing feature theory. Based on advances in the field of noise-robust speech recognition, we represent the uncertainty about the enhanced signals via a Gaussian distribution instead that is subsequently propagated to the features and to the classifier. We introduce new methods to estimate the uncertainty from the signal in a fully automatic manner and to learn the classifier directly from polyphonic data. We illustrate the results by considering the task of identifying, from a given set of singers, which one is singing at a given time in a given song. Experimental results demonstrate the relevance of our approach. © 2012 International Society for Music Information Retrieval."
Wang X.; Chen X.; Yang D.; Wu Y.,Music emotion classification of chinese songs based on lyrics using TF*IDF and rhyme,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873590604&partnerID=40&md5=845ff0a2133848682f54555c21c24bd3,"Wang X., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China; Wu Y., Institute of Computer Science and Technology, Peking University, China",This paper presents the outcomes of research into an automatic classification system based on the lingual part of music. Two novel kinds of short features are extracted from lyrics using tf*idf and rhyme. Meta-learning algorithm is adapted to combine these two sets of features. Results show that our features promote the accuracy of classification and meta-learning algorithm is effective in fusing the two features. © 2011 International Society for Music Information Retrieval.
Thoshkahna B.; Ramakrishnan K.R.,A postprocessing technique for improved harmonic / percussion separation for polyphonic music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600899&partnerID=40&md5=1b25bffc2070d114212d0583e03c3880,"Thoshkahna B., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India; Ramakrishnan K.R., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India","In this paper we propose a postprocessing technique for a spectrogram diffusion based harmonic/percussion decomposition algorithm. The proposed technique removes harmonic instrument leakages in the percussion enhanced outputs of the baseline algorithm. The technique uses median filtering and an adaptive detection of percussive segments in subbands followed by piecewise signal reconstruction using envelope properties to ensure that percussion is enhanced while harmonic leakages are suppressed. A new binarymask is created for the percussion signal which upon applying on the original signal improves harmonic versus percussion separation. We compare our algorithm with two recent techniques and show that on a database of polyphonic Indian music, the postprocessing algorithm improves the harmonic versus percussion decomposition significantly. © 2011 International Society for Music Information Retrieval."
Tryfou G.; Härmä A.; Mouchtaris A.,Tempo estimation based on linear prediction and perceptual modelling,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592508&partnerID=40&md5=00555eb3fa4cabbca25e2772b0dfa6ed,"Tryfou G., Foundation for Research and Technology - Hellas (FORTH-ICS), Institute of Computer Science, Heraklion, Crete, Greece, Department of Computer Science, University of Crete, Heraklion, Crete, Greece; Härmä A., Philips Research, Eindhoven, Netherlands; Mouchtaris A., Foundation for Research and Technology - Hellas (FORTH-ICS), Institute of Computer Science, Heraklion, Crete, Greece, Department of Computer Science, University of Crete, Heraklion, Crete, Greece","Many applications demand the automatic induction of the tempo of a musical excerpt. The tempo estimation systems follow a general scheme that consists of two main steps: the creation of a feature list and the detection of periodicities on this list. In this study, we propose a new method for the implementation of the first step, along with the addition of a final step that will enhance the tempo estimation procedure. The proposed method for the extraction of the feature list is based on Gammatone subspace analysis and Linear Prediction Error Filters (LPEFs). As a final step on the system, the application of a model that approximates the tempo perception by human listeners is proposed. The results of the evaluation indicate the proposed method compares favourably with other, state-of-the-art tempo estimation methods, using only one frame of the musical experts when most of the literature methods demand the processing of the whole piece. © 2011 International Society for Music Information Retrieval."
Ellis K.; Coviello E.; Lanckriet G.R.G.,Semantic annotation and retrieval of music using a bag of systems representation,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592866&partnerID=40&md5=087ed42489720dc227288ece892d7170,"Ellis K., University of California, San Diego, United States; Coviello E., University of California, San Diego, United States; Lanckriet G.R.G., University of California, San Diego, United States","We present a content-based auto-tagger that leverages a rich dictionary of musical codewords, where each codeword is a generative model that captures timbral and temporal characteristics of music. This leads to a higher-level, concise ""Bag of Systems"" (BoS) representation of the characteristics of a musical piece. Once songs are represented as a BoS histogram over codewords, traditional algorithms for text document retrieval can be leveraged for music autotagging. Compared to estimating a single generative model to directly capture the musical characteristics of songs associated with a tag, the BoS approach offers the flexibility to combine different classes of generative models at various time resolutions through the selection of the BoS codewords. Experiments show that this enriches the audio representation and leads to superior auto-tagging performance. © 2011 International Society for Music Information Retrieval."
Flexer A.; Schnitzer D.; Schlüter J.,A MIREX meta-analysis of hubness in audio music similarity,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435945&partnerID=40&md5=f7e501b8ad3ecf3f34cec34a31428116,"Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schlüter J., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","We use results from the 2011 MIREX ""Audio Music Similarity and Retrieval"" task for a meta analysis of the hub phenomenon. Hub songs appear similar to an undesirably high number of other songs due to a problem of measuring distances in high dimensional spaces. Comparing 17 algorithms we are able to confirm that different algorithms produce very different degrees of hubness. We also show that hub songs exhibit less perceptual similarity to the songs they are close to, according to an audio similarity function, than non-hub songs. Application of the recently introduced method of ""mutual proximity"" is able to decisively improve this situation. © 2012 International Society for Music Information Retrieval."
Bandera C.D.L.; Barbancho A.M.; Tardón L.J.; Sammartino S.; Barbancho I.,Humming method for content-based music information retrieval,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592626&partnerID=40&md5=02c76ddc2ed0fe9152323868ea5f9824,"Bandera C.D.L., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Barbancho A.M., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Tardón L.J., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Sammartino S., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Barbancho I., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain","In this paper a humming method for music information retrieval is presented. The system uses a database with real songs and does not need another type of symbolic representation of them. The system employs an original fingerprint based on chroma vectors to characterize the humming and the references songs. With this fingerprint, it is possible to get the hummed songs without needed of transcription of the notes of the humming or of the songs. The system showed a good performance on Pop/Rock and Spanish folk music. © 2011 International Society for Music Information Retrieval."
Kemal Karaosmanoǧlu M.,A Turkish makam music symbolic database for music information retrieval: SymbTr,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873432678&partnerID=40&md5=48117b1831fe09437c7d0594aea4574a,"Kemal Karaosmanoǧlu M., Yildiz Technical University, Turkey","Turkish makam music needs a comprehensive database for public consumption, to be used in MIR. This article introduces SymbTr, a Turkish Makam Music Symbolic Representation Database, aimed at filling this void. SymbTr consists of musical information in text, PDF, and MIDI formats. Raw data, drawn from reliable sources, and consisting of 1, 700 musical pieces in Turkish art and folk music was processed featuring distinct examples in 155 diverse makams, 100 usuls and 48 forms. Special care was devoted to selection of works that scatter across a broad historical time span and were among those still performed today. Total number of musical notes in these pieces was 630, 000, corresponding to a nominal playback time of 72 hours. Synthesized sounds particular to Turkish makam music were used in MIDI playback, and transcription/playback errors were corrected by input from experts. Symbolic representation data, open to the public, is output from a computer program developed exclusively for Turkish makam music. SymbTr was designed as a wholesome representation of aforementioned distinct auditory and visual features that distinguish Turkish makam music from other music genres. This article explains the database format in detail, and also provides, through examples, statistical information on pitch/interval allocation and distribution. © 2012 International Society for Music Information Retrieval."
Hu X.; Yu B.,Exploring the relationship between mood and creativity in rock lyrics,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604977&partnerID=40&md5=aaa50420072369c2a9809ce71f8f00af,"Hu X., Library and Information Science Program, Morgridge College of Education, University of Denver, United States; Yu B., School of Information Studies, Syracuse University, United States","The relationship between mood and creativity has been widely studied in psychology, however, no conclusion is reached in terms of which mood triggers high creativity, positive or negative. This paper provides new insights to this on-going argument by examining the relationship between lyrics creativity and music mood. We use three computational measures to gauge lyrics creativity: Type-to- Token Ratio, word norms fraction, and WordNet similarity. We then test three hypotheses regarding differences in lyrics creativity between music with different moods on 2715 U.S. rock songs. The three measures led to consistent findings that lyrics of negative and sad songs demonstrate higher linguistic creativity than those of positive and happy songs. Our findings support previous studies in psycholinguistics that people write more creatively when the text conveys sad or negative sentiment, and contradict previous research that positive mood triggers more unusual word associations. The result also indicates that different measures capture different aspects of lyrics creativity. © 2011 International Society for Music Information Retrieval."
García-Díez S.; Saerens M.; Senelle M.; Fouss F.,A simple-cycles weighted kernel based on harmony structure for similarity retrieval,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602397&partnerID=40&md5=a4ed57b0ad73d0e31ec8de05c370ac13,"García-Díez S., Université catholique de Louvain, Belgium; Saerens M., Université catholique de Louvain, Belgium; Senelle M., Université catholique de Louvain - Site de Mons, Belgium; Fouss F., Université catholique de Louvain - Site de Mons, Belgium","This paper introduces a novel methodology for music similarity retrieval based on chord progressions. From each chord progression, a directed labeled graph containing the interval transitions is extracted. This graph will be used as input for a graph comparison method based on simple cycles - cycles where the only repeated nodes are the first and the last one. In music, simple cycles represent the repetitive sub-structures of, e.g., modern pop/rock music. By means of a kernel function [10] whose feature space is spanned by these simple cycles, we obtain a kernel matrix (similarity matrix) which can then be used in music similarity retrieval tasks. The resulting algorithm has a time complexity of O(n+m(c+1)), where n is the number of vertices, m is the number of edges, and c is the number of simple cycles. The performance of our method is tested on both an idiom retrieval task, and a cover song retrieval task. Empirical results show the improved accuracy of our method in comparison with other string-matching, and graph-comparison methods used as baseline."
Lee J.H.; Waterman N.M.,Understanding user requirements for music information services,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415163&partnerID=40&md5=d2ab9ec869aef1b0d316592fee1702f8,"Lee J.H., Information School, University of Washington, United States; Waterman N.M., Information School, University of Washington, United States","User studies in the music information retrieval and music digital library fields have been gradually increasing in recent years, but large-scale studies that can help detect common user behaviors are still lacking. We have conducted a large-scale user survey in which we asked numerous questions related to users' music needs, uses, seeking, and management behaviors. In this paper, we present our preliminary findings, specifically focusing on the responses to questions of users' favorite music related websites/applications and the reasons why they like them. We provide a list of popular music services, as well as an analysis of how these services are used, and what qualities are valued. Our findings suggest several trends in the types of music services people like: an increase in the popularity of music streaming and mobile music consumption, the emergence of new functionality, such as music identification and cloud music services, an appreciation of music videos, serendipitous discovery of music, and customizability, as well as users' changing expectations of particular types of music information. © 2012 International Society for Music Information Retrieval."
Peeters G.; Fort K.,Towards a (better) definition of the description of annotated MIR corpora,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436630&partnerID=40&md5=9427252be8f0f4beeddc69bac02af262,"Peeters G., STMS, IRCAM-CNRS-UPMC, Paris, France; Fort K., INIST-CNRS and Université Paris 13, Sorbonne Paris Cité, LIPN, Nancy, France","Today, annotated MIR corpora are provided by various research labs or companies, each one using its own annotation methodology, concept definitions, and formats. This is not an issue as such. However, the lack of descriptions of the methodology used - how the corpus was actually annotated, and by whom - and of the annotated concepts, i.e. what is actually described, is a problem with respect to the sustainability, usability, and sharing of the corpora. Experience shows that it is essential to define precisely how annotations are supplied and described. We propose here a survey and consolidation report on the nature of the annotated corpora used and shared in MIR, with proposals for the axis against which corpora can be described so to enable effective comparison and the inherent influence this has on tasks performed using them. © 2012 International Society for Music Information Retrieval."
Kirchhoff H.; Dixon S.; Klapuri A.,Multi-template shift-variant non-negative matrix deconvolution for semi-automatic music transcription,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431582&partnerID=40&md5=8053bd835b7ab286740753e8f1ee964e,"Kirchhoff H., Queen Mary University of London, Centre for Digital Music, United Kingdom; Dixon S., Queen Mary University of London, Centre for Digital Music, United Kingdom; Klapuri A., Queen Mary University of London, Centre for Digital Music, United Kingdom","For the task of semi-automatic music transcription, we extended our framework for shift-variant non-negative matrix deconvolution (svNMD) to work with multiple templates per instrument and pitch. A k-means clustering based learning algorithm is proposed that infers the templates from the data based on the provided user information. We experimentally explored the maximum achievable transcription accuracy of the algorithm and evaluated the prospective performance in a realistic setting. The results showed a clear superiority of the Itakura-Saito divergence over the Kullback-Leibler divergence and a consistent improvement of the maximum achievable accuracy when each pitch is represented by more than one spectral template. © 2012 International Society for Music Information Retrieval."
Speck J.A.; Schmidt E.M.; Morton B.G.; Kim Y.E.,A comparative study of collaborative vs. Traditional musical mood annotation,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597750&partnerID=40&md5=64b8e8e590ba204bc79ab542b43f7328,"Speck J.A., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States; Schmidt E.M., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States; Morton B.G., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States; Kim Y.E., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States","Organizing music by emotional association is a natural process for humans, but the ambiguous nature of emotion makes it a difficult task for machines. Automatic systems for music emotion recognition rely on ground truth data collected from humans, and more effective methods for collecting such data are being continuously developed. In previous work, we developed MoodSwings, an online collaborative game for crowdsourcing dynamic (persecond) mood ratings from multiple players within the twodimensional arousal-valence (A-V) representation of emotion. MoodSwings has proven effective for data collection, but potential data effects caused by collaborative labeling have not yet been analyzed. In this work, we compare the effectiveness of MoodSwings to that of a more traditional data collection method, where annotation is performed by single, paid annotators. We implement a simplified labeling task to run on Amazon's crowdsourcing engine, Mechanical Turk (MTurk), and analyze the labels collected with each method. A statistical comparison shows consistencies between MoodSwings and MTurk data, and we produce similar results using each as training data for automatic emotion production via supervised machine learning. Furthermore the new dataset collected via MTurk has been made available to the Music Information Retrieval community. © 2011 International Society for Music Information Retrieval."
Moore J.L.; Chen S.; Joachims T.; Turnbull D.,Learning to embed songs and tags for playlist prediction,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416522&partnerID=40&md5=1e4d231997ed60838d7312317d483758,"Moore J.L., Cornell University, Dept. of Computer Science, United States; Chen S., Cornell University, Dept. of Computer Science, United States; Joachims T., Cornell University, Dept. of Computer Science, United States; Turnbull D., Ithaca College, Dept. of Computer Science, United States","Automatically generated playlists have become an important medium for accessing and exploring large collections of music. In this paper, we present a probabilistic model for generating coherent playlists by embedding songs and social tags in a unified metric space. We show how the embedding can be learned from example playlists, providing the metric space with a probabilistic meaning for song/song, song/tag, and tag/tag distances. This enables at least three types of inference. First, our models can generate new playlists, outperforming conventional n-gram models in terms of predictive likelihood by orders of magnitude. Second, the learned tag embeddings provide a generalizing representation for embedding new songs, allowing it to create playlists even for songs it has never observed in training. Third, we show that the embedding space provides an effective metric for matching songs to natural-language queries, even if tags for a large fraction of the songs are missing. © 2012 International Society for Music Information Retrieval."
Xie B.; Bian W.; Tao D.; Chordia P.,Music tagging with regularized logistic regression,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873608388&partnerID=40&md5=49b4845ba72f073a6655b966fe0be3e1,"Xie B., GTCMT, Georgia Tech, Atlanta, GA, United States; Bian W., QCIS, Univ of Tech Sydney, Sydney, NSW, Australia; Tao D., QCIS, Univ of Tech Sydney, Sydney, NSW, Australia; Chordia P., GTCMT, Georgia Tech, Atlanta, GA, United States","In this paper, we present a set of simple and efficient regularized logistic regression algorithms to predict tags of music. We first vector-quantize the delta MFCC features using k-means and construct ""bag-of-words"" representation for each song. We then learn the parameters of these logistic regression algorithms from the ""bag-of- words"" vectors and ground truth labels in the training set. At test time, the prediction confidence by the linear classifiers can be used to rank the songs for music annotation and retrieval tasks. Thanks to the convex property of the objective functions, we adopt an efficient and scalable generalized gradient method to learn the parameters, with global optimum guaranteed. And we show that these efficient algorithms achieve stateof- the-art performance in annotation and retrieval tasks evaluated on CAL-500. © 2011 International Society for Music Information Retrieval."
Papadopoulos H.; Kowalski M.,Sparse signal decomposition on hybrid dictionaries using musical priors,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607213&partnerID=40&md5=bc4a699377266649bda15bdc92847f76,"Papadopoulos H., UMR 8506, Laboratoire des Signaux et Systèmes, CNRS-SUPELEC-Univ Paris-Sud, 91172 Gif-sur-Yvette cedex, France; Kowalski M., UMR 8506, Laboratoire des Signaux et Systèmes, CNRS-SUPELEC-Univ Paris-Sud, 91172 Gif-sur-Yvette cedex, France","This paper investigates the use of musical priors for sparse expansion of audio signals of music on overcomplete dictionaries taken from the union of two orthonormal bases. More specifically, chord information is used to build a structured model that takes into account dependencies between coefficients of the decomposition. Evaluation on various music signals shows that our approach provides results whose quality measured by the signal-to-noise ratio corresponds to state-of-the-art approaches, and shows that our model is relevant to represent audio signals ofWestern tonal music and opens new perspectives. © 2011 International Society for Music Information Retrieval."
Rafii Z.; Pardo B.,Music/voice separation using the similarity matrix,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416007&partnerID=40&md5=82ba03a053dd97b16de6d5346e9dee30,"Rafii Z., Northwestern University, EECS Department, Evanston, IL, United States; Pardo B., Northwestern University, EECS Department, Evanston, IL, United States","Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient. © 2012 International Society for Music Information Retrieval."
Fenet S.; Richard G.; Grenier Y.,A scalable audio fingerprint method with robustness to pitch-shifting,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588095&partnerID=40&md5=1d81d343be5d25580d1ac6c513269a94,"Fenet S., Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France; Richard G., Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France; Grenier Y., Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France","Audio fingerprint techniques should be robust to a variety of distortions due to noisy transmission channels or specific sound processing. Although most of nowadays techniques are robust to the majority of them, the quasi-systematic use of a spectral representation makes them possibly sensitive to pitch-shifting. This distortion indeed induces a modification of the spectral content of the signal. In this paper, we propose a novel fingerprint technique, relying on a hashing technique coupled with a CQT-based fingerprint, with a strong robustness to pitch-shifting. Furthermore, we have associated this method with an efficient post-processing for the removal of false alarms. We also present the adaptation of a database pruning technique to our specific context. We have evaluated our approach on a real-life broadcast monitoring scenario. The analyzed data consisted of 120 hours of real radio broadcast (thus containing all the distortions that would be found in an industrial context). The reference database consisted of 30.000 songs. Our method, thanks to its increased robustness to pitch-shifting, shows an excellent detection score. © 2011 International Society for Music Information Retrieval."
Montecchio N.; Cont A.,Accelerating the mixing phase in studio recording productions by automatic audio alignment,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592861&partnerID=40&md5=b8df7e061c7fbf710708083ba38cd782,"Montecchio N., Department of Information Engineering, University of Padova, Italy; Cont A., Institut de Recherche et Coordination Acoustique/Musique (IRCAM), France","We propose a system for accelerating the mixing phase in a recording production, by making use of audio alignment techniques to automatically align multiple takes of excerpts of a music piece against a performance of the whole work. We extend the approach of our previous work, based on sequential Montecarlo inference techniques, that was targeted at real-time alignment for score/audio following. The proposed approach is capable of producing partial alignments as well as identifying relevant regions in the partial results with regards to the reference, for better integration within a studio mix workflow. The approach is evaluated using data obtained from two recording sessions of classical music pieces, and we discuss its effectiveness for reducing manual work in a production chain. © 2011 International Society for Music Information Retrieval."
Topel S.S.; Casey M.A.,Elementary sources: Latent component analysis for music composition,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594489&partnerID=40&md5=4c5f02b45d65a495c12e8a791d5779aa,"Topel S.S., Bregman Music Audio Research Studio, Dartmouth College, United States; Casey M.A., Bregman Music Audio Research Studio, Dartmouth College, United States","Complexity of music audio signals creates an access problemto specificmusical objects or structures within the source samples. Instead of employing more commonly used audio analysis or production techniques to access features, we describe extraction of sub-mixtures from real-world audio using a Probabilistic Latent Component Analysis-based decomposition tool for music composition. This is highlighted with the presentation of a prior relevant compositional approach named Spectral Music along with a discussion of five compositions extending these principles using methods more commonly associated with source separation research. © 2011 International Society for Music Information Retrieval."
Niedermayer B.; Böck S.; Widmer G.,"On the importance of ""Real"" audio data for mir algorithm evaluation at the note-level - A comparative study",2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599014&partnerID=40&md5=812b577c066fd27a8ecd0f11b048e61c,"Niedermayer B., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","A considerable number of MIR tasks requires annotations at the note-level for the purpose of in-depth evaluation. A common means of obtaining accurately annotated data corpora is to start with a symbolic representation of a piece and generate corresponding audio data. This study investigates the effect of audio quality and source on the performance of two representative MIR algorithms - Onset Detection and Audio Alignment. Three kinds of audio material are compared: piano pieces generated using a freely available software synthesizer with its default instrument patches; a commercial high-quality sample library; and audio recordings made on a real (computer-controlled) grand piano. Also, the effect of varying richness of artistic changes in tempo and dynamics or natural asynchronies is examined. We show that the algorithms' performance on the different datasets varies considerably, but synthesized audio, does not necessarily yield better results. © 2011 International Society for Music Information Retrieval."
Wang J.-C.; Lee H.-S.; Wang H.-M.; Jeng S.-K.,Learning the similarity of audio music in bag-offrames representation from tagged music data,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594763&partnerID=40&md5=e0029ac5c06eadd21740e4710a8fe51b,"Wang J.-C., Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan, Institute of Information Science, Academia Sinica, Taipei, Taiwan; Lee H.-S., Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan, Institute of Information Science, Academia Sinica, Taipei, Taiwan; Wang H.-M., Institute of Information Science, Academia Sinica, Taipei, Taiwan; Jeng S.-K., Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","Due to the cold-start problem, measuring the similarity between two pieces of audio music based on their low-level acoustic features is critical to many Music Information Retrieval (MIR) systems. In this paper, we apply the bag-offrames (BOF) approach to represent low-level acoustic features of a song and exploit music tags to help improve the performance of the audio-based music similarity computation. We first introduce a Gaussian mixture model (GMM) as the encoding reference for BOF modeling, then we propose a novel learning algorithm to minimize the similarity gap between low-level acoustic features and music tags with respect to the prior weights of the pre-trained GMM. The results of audio-based query-by-example MIR experiments on the MajorMiner and Magnatagatune datasets demonstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ BOF modeling. © 2011 International Society for Music Information Retrieval."
Ehmann A.F.; Bay M.; Downie J.S.; Fujinaga I.; De Roure D.,Music structure segmentation algorithm evaluation: Expanding on MIREX 2010 analyses and datasets,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605778&partnerID=40&md5=977ea797b9635142a92725d2011720f8,"Ehmann A.F., GSLIS, University of Illinois, Urbana-Champaign, United States; Bay M., GSLIS, University of Illinois, Urbana-Champaign, United States; Downie J.S., GSLIS, University of Illinois, Urbana-Champaign, United States; Fujinaga I., Schulich School of Music, McGill University, Canada; De Roure D., Oxford E-Research Centre, University of Oxford, United Kingdom","Music audio structure segmentation has been a task in the Music Information Retrieval Evaluation eXchange (MIREX) since 2009. In 2010, five algorithms were evaluated against two datasets (297 and 100 songs) with an almost exclusive focus on western popular music. A new annotated dataset significantly larger in size and with a more diverse range of musical styles became available in 2011. This new dataset comprises over 1,300 songs spanning pop, jazz, classical, and world music styles. The algorithms from the 2010 iteration of MIREX are re-evaluated against this new dataset. This paper presents a detailed analysis of these evaluation results in order to gain a better understanding of the current state-of-the-art in automatic structure segmentation. These expanded analyses focus on the interaction of algorithm performance and rankings with datasets, musical styles, and annotation level. Because the new dataset contains multiple annotations for each song, we also introduce a baseline for expected human performance for this task. © 2011 International Society for Music Information Retrieval."
Cunningham S.J.; Bainbridge D.; Stephen Downie J.,The impact of MIREX on scholarly research (2005 - 2010),2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428806&partnerID=40&md5=857cd892682fdf38ff263882c9c8e3e9,"Cunningham S.J., University of Waikato, Hamilton, New Zealand; Bainbridge D., University of Waikato, Hamilton, New Zealand; Stephen Downie J., University of Illinois, Urbana-Champaign, United States","This paper explores the impact of the MIREX (Music Information Retrieval Evaluation eXchange) evaluation initiative on scholarly research. Impact is assessed through a bibliometric evaluation of both the MIREX extended abstracts and the papers citing the MIREX results, the trial framework and methodology, or MIREX datasets. Impact is examined through number of publications and citation analysis. We further explore the primary publication venues for MIREX results, the geographic distribution of both MIREX contributors and researchers citing MIREX results, and the spread of MIREX-based research beyond the MIREX contributor teams. This analysis indicates that research in this area is highly collaborative, has achieved an international dissemination, and has grown to have a significant profile in the research literature. © 2012 International Society for Music Information Retrieval."
Kameoka H.; Ochiai K.; Nakano M.; Tsuchiya M.; Sagayama S.,Context-free 2D tree structure model of musical notes for Bayesian modeling of polyphonic spectrograms,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436781&partnerID=40&md5=f73ac291a2d1777adb0d6cc556fc425d,"Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan, NTT Communication Science Laboratories, NTT Corporation Morinosato, Atsugi, Kanagawa, 243-0198, Wakamiya 3-1, Japan; Ochiai K., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan; Nakano M., NTT Communication Science Laboratories, NTT Corporation Morinosato, Atsugi, Kanagawa, 243-0198, Wakamiya 3-1, Japan; Tsuchiya M., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan","This paper proposes a Bayesian model for automatic music transcription. Automatic music transcription involves several subproblems that are interdependent of each other: multiple fundamental frequency estimation, onset detection, and rhythm/tempo recognition. In general, simultaneous estimation is preferable when several estimation problems have chicken-and-egg relationships. This paper proposes modeling the generative process of an entire music spectrogram by combining the sub-process by which a musically natural tempo curve is generated, the sub-process by which a set of note onset positions is generated based on a 2-dimensional tree structure representation of music, and the sub-process by which a music spectrogram is generated according to the tempo curve and the note onset positions. Most conventional approaches to music transcription perform note extraction prior to structure analysis, but accurate note extraction has been a difficult task. By contrast, thanks to the combined generative model, the present method performs note extraction and structure estimation simultaneously and thus the optimal solution is obtained within a unified framework. We show some of the transcription results obtained with the present method. © 2012 International Society for Music Information Retrieval."
Neubarth K.; Bergeron M.; Conklin D.,Associations between musicology and music information retrieval,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599842&partnerID=40&md5=8dd899a4cfe612f41d4469cbd7ada2e9,"Neubarth K., Canterbury Christ Church University, Canterbury, United Kingdom; Bergeron M., CIRMMT, McGill University, Montreal, Canada; Conklin D., Universidad del País Vasco, San Sebastían, Spain, IKERBASQUE, Basque Foundation for Science, Spain","A higher level of interdisciplinary collaboration between music information retrieval (MIR) and musicology has been proposed both in terms of MIR tools for musicology, and musicological motivation and interpretation of MIR research. Applying association mining and content citation analysis methods to musicology references in ISMIR papers, this paper explores which musicological subject areas are of interest to MIR, whether references to specific musicology areas are significantly over-represented in specific MIR areas, and precisely why musicology is cited in MIR. © 2011 International Society for Music Information Retrieval."
Otsuka T.; Nakadai K.; Ogata T.; Okuno H.G.,Incremental bayesian audio-to-score alignment with flexible harmonic structure models,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592130&partnerID=40&md5=0b380a2ab9d0d55c35f9b8592dc6f7e9,"Otsuka T., School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Nakadai K., Honda Research Institute Japan, Co., Ltd., Wako, Saitama 351-0114, Japan; Ogata T., School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","Music information retrieval, especially the audio-to-score alignment problem, often involves a matching problem between the audio and symbolic representations. We must cope with uncertainty in the audio signal generated from the score in a symbolic representation such as the variation in the timbre or temporal fluctuations. Existing audio-to-score alignment methods are sometimes vulnerable to the uncertainty in which multiple notes are simultaneously played with a variety of timbres because these methods rely on static observation models. For example, a chroma vector or a fixed harmonic structure template is used under the assumption that musical notes in a chord are all in the same volume and timbre. This paper presents a particle filterbased audio-to-score alignment method with a flexible observation model based on latent harmonic allocation. Our method adapts to the harmonic structure for the audio-toscore matching based on the observation of the audio signal through Bayesian inference. Experimental results with 20 polyphonic songs reveal that our method is effective when more number of instruments are involved in the ensemble. © 2011 International Society for Music Information Retrieval."
Müller M.; Grosche P.; Jiang N.,A segment-based fitness measure for capturing repetitive structures of music recordings,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602790&partnerID=40&md5=640871dd654420d16cd809efb0250ff5,"Müller M., MPI Informatik, Saarland University, Germany; Grosche P., MPI Informatik, Saarland University, Germany; Jiang N., MPI Informatik, Saarland University, Germany","In this paper, we deal with the task of determining the audio segment that best represents a given music recording (similar to audio thumbnailing). Typically, such a segment has many (approximate) repetitions covering large parts of the music recording. As main contribution, we introduce a novel fitness measure that assigns to each segment a fitness value that expresses how much and how well the segment ""explains"" the repetitive structure of the recording. In combination with enhanced feature representations, we show that our fitness measure can cope even with strong variations in tempo, instrumentation, and modulations that may occur within and across related segments. We demonstrate the practicability of our approach by means of several challenging examples including field recordings of folk music and recordings of classical music. © 2011 International Society for Music Information Retrieval."
Thomas V.; Wagner C.; Clausen M.,OCR-Based post-processing of OMR for the recovery of transposing instruments in complex orchestral scores,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606472&partnerID=40&md5=32a3ac5c6bd68e625bfe4e1051cfc72b,"Thomas V., Computer Science III, University of Bonn, Germany; Wagner C., Computer Science III, University of Bonn, Germany; Clausen M., Computer Science III, University of Bonn, Germany","Given a scanned score page, Optical Music Recognition (OMR) attempts to reconstruct all contained music information. However, the available OMR systems lack the ability to recognize transposition information contained in complex orchestral scores. 1 An additional unsolved OMR problem is the handling of orchestral scores using compressed notation. 2 Here, the information of which instrument has to play which staff is crucial for a correct interpretation of the score. But this mapping is lost along the pages of the score during the OMR process. In this paper, we present a method for retrieving the instrumentation and transposition information of orchestral scores. In our approach, we combine the results of Optical Character Recognition (OCR) and OMR to regain the information available through text annotations of the score. In addition, a method to reconstruct the instrument and transposition information for staves where text annotations were omitted or not recognized is presented. In an evaluation we analyze the impact of transposition information on the quality of score-audio synchronizations of orchestral music. The results show that the knowledge of transposing instruments improves the synchronization accuracy and that our method helps in regaining this knowledge. © 2011 International Society for Music Information Retrieval."
Bogdanov D.; Herrera P.,Howmuch metadata do we need in music recommendation? A subjective evaluation using preference sets,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595124&partnerID=40&md5=ab4efc746d140ee9610744c374a82b4f,"Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this work we consider distance-based approaches to music recommendation, relying on an explicit set of music tracks provided by the user as evidence of his/her music preferences. Firstly, we propose a purely content-based approach, working on low-level (timbral, temporal, and tonal) and inferred high-level semantic descriptions of music. Secondly, we consider its simple refinement by adding a minimum amount of genre metadata. We compare the proposed approaches with one content-based and three metadata-based baselines. As such, we consider content-based approach working on inferred semantic descriptors, a tag-based recommender exploiting artist tags, a commercial black-box recommender partially employing collaborative filtering information, and a simple genre-based random recommender. We conduct a listening experiment with 19 participants. The obtained results reveal that although the low-level/semantic content-based approach does not achieve the performance of the baseline working exclusively on the inferred semantic descriptors, the proposed refinement provides significant improvement in the listeners' satisfaction comparable with metadata-based approaches, and surpasses these approaches by the number of novel relevant recommendations. We conclude that the proposed content-based approach refined by simple genre metadata is suited for music discovery not only in the long-tail but also within popular music items. © 2011 International Society for Music Information Retrieval."
Zapata J.R.; Holzapfel A.; Davies M.E.P.; Oliveira J.L.; Gouyon F.,Assigning a confidence threshold on automatic beat annotation in large datasets,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431245&partnerID=40&md5=6e7a67c86acb16ea47e75885c85bf2da,"Zapata J.R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Davies M.E.P., Sound and Music Computing Group, INESC TEC, Porto, Portugal; Oliveira J.L., Sound and Music Computing Group, INESC TEC, Porto, Portugal, Faculty of Engineering, University of Porto, Porto, Portugal; Gouyon F., Sound and Music Computing Group, INESC TEC, Porto, Portugal, Faculty of Engineering, University of Porto, Porto, Portugal","In this paper we establish a threshold for perceptually acceptable beat tracking based on the mutual agreement of a committee of beat trackers. In the first step we use an existing annotated dataset to show that mutual agreement can be used to select one committee member as the most reliable beat tracker for a song. Then we conduct a listening test using a subset of the Million Song Dataset to establish a threshold which results in acceptable quality of the chosen beat output. For both datasets, we obtain a percentage of trackable music of about 73%, and we investigate which data tags are related to acceptable and problematic beat tracking. The results indicate that current datasets are biased towards genres which tend to be easy for beat tracking. The proposed methods provide a means to automatically obtain a confidence value for beat tracking in non-annotated data and to choose between a number of beat tracker outputs. © 2012 International Society for Music Information Retrieval."
Battenberg E.; Wessel D.,Analyzing drum patterns using conditional deep belief networks,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426072&partnerID=40&md5=9ed762da3067406ac90cd0c79b8077cf,"Battenberg E., University of California, Berkeley, Dept. of Electrical Engineering and Computer Sciences, United States; Wessel D., University of California, Berkeley, Center for New Music and Audio Technologies, United States","We present a system for the high-level analysis of beat-synchronous drum patterns to be used as part of a comprehensive rhythmic understanding system. We use a multilayer neural network, which is greedily pre-trained layer-by-layer using restriced Boltzmann machines (RBMs), in order to model the contextual time-sequence information of a drum pattern. For the input layer of the network, we use a conditional RBM, which has been shown to be an effective generative model of multi-dimensional sequences. Subsequent layers of the neural network can be pre-trained as conditional or standard RBMs in order to learn higherlevel rhythmic features. We show that this model can be fine-tuned in a discriminative manner to make accurate predictions about beat-measure alignment. The model generalizes well to multiple rhythmic styles due to the distributed state-space of the multi-layer neural network. In addition, the outputs of the discriminative network can serve as posterior probabilities over beat-alignment labels. These posterior probabilities can be used for Viterbi decoding in a hidden Markov model in order to maintain temporal continuity of the predicted information. © 2012 International Society for Music Information Retrieval."
Müller M.; Jiang N.,A scape plot representation for visualizing repetitive structures of music recordings,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873434119&partnerID=40&md5=d961bc26d6357f4a324479f5b2e240cd,"Müller M., Bonn University, MPI Informatik, Germany; Jiang N., Saarland University, MPI Informatik, Germany","The development of automated methods for revealing the repetitive structure of a given music recording is of central importance in music information retrieval. In this paper, we present a novel scape plot representation that allows for visualizing repetitive structures of the entire music recording in a hierarchical, compact, and intuitive way. In a scape plot, each point corresponds to an audio segment identified by its center and length. As our main contribution, we assign to each point a color value so that two segment properties become apparent. Firstly, we use the lightness component of the color to indicate the repetitive-ness of the encoded segment, where we revert to a recently introduced fitness measure. Secondly, we use the hue component of the color to reveal the relations between different segments. To this end, we introduce a novel grouping procedure that automatically maps related segments to similar hue values. By discussing a number of popular and classical music examples, we illustrate the potential and visual appeal of our representation and also indicate limitations. © 2012 International Society for Music Information Retrieval."
Chen R.; Shen W.; Srinivasamurthy A.; Chordia P.,Chord recognition using duration-explicit Hidden Markov Models,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435049&partnerID=40&md5=7cd4b96a16c80b68e93cd2af11cabdeb,"Chen R., Georgia Tech Center for Music Technology, United States; Shen W., Georgia Tech Center for Music Technology, United States; Srinivasamurthy A., Georgia Tech Center for Music Technology, United States; Chordia P., Smule Inc., United States","We present an audio chord recognition system based on a generalization of the Hidden Markov Model (HMM) in which the duration of chords is explicitly considered - a type of HMM referred to as a hidden semi-Markov model, or duration-explicit HMM (DHMM). We find that such a system recognizes chords at a level consistent with the state-of-the-art systems - 84.23% on Uspop dataset at the major/minor level. The duration distribution is estimated from chord duration histograms on the training data. It is found that the state-of-the-art recognition result can be improved upon by using several duration distributions, which are found automatically by clustering song-level duration histograms. The paper further describes experiments which shed light on the extent to which context information, in the sense of transition matrices, is useful for the audio chord recognition task. We present evidence that the context provides surprisingly little improvement in performance, compared to isolated frame-wise recognition with simple smoothing. We discuss possible reasons for this, such as the inherent entropy of chord sequences in our training database. © 2012 International Society for Music Information Retrieval."
Neubarth K.; Goienetxea I.; Johnson C.G.; Conklin D.,Association mining of folk music genres and toponyms,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873438951&partnerID=40&md5=acf845aed4c4f24401d5982a4e8bb5ae,"Neubarth K., Canterbury Christ Church University, Canterbury, United Kingdom, School of Computing, University of Kent, Canterbury, United Kingdom; Goienetxea I., Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, San Sebastián, Spain; Johnson C.G., School of Computing, University of Kent, Canterbury, United Kingdom; Conklin D., Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, San Sebastián, Spain, IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","This paper demonstrates how association rule mining can be applied to discover relations between two ontologies of folk music: a genre and a region ontology. Genreregion associations have been widely studied in folk music research but have been neglected in music information retrieval. We present a method of association rule mining with constraints consisting of rule templates and rule evaluation measures to identify different, musicologically motivated, categories of genre-region associations. The method is applied to a corpus of 1902 Basque folk tunes, and several interesting rules and rule sets are discovered. © 2012 International Society for Music Information Retrieval."
Bosch J.J.; Janer J.; Fuhrmann F.; Herrera P.,A comparison of sound segregation techniques for predominant instrument recognition in musical audio signals,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436256&partnerID=40&md5=f44c7ae816a805e4e9151380788d8ca3,"Bosch J.J., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain; Janer J., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain; Fuhrmann F., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain; Herrera P., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain","The authors address the identification of predominant music instruments in polytimbral audio by previously dividing the original signal into several streams. Several strategies are evaluated, ranging from low to high complexity with respect to the segregation algorithm and models used for classification. The dataset of interest is built from professionally produced recordings, which typically pose problems to state-of-art source separation algorithms. The recognition results are improved a 19% with a simple sound segregation pre-step using only panning information, in comparison to the original algorithm. In order to further improve the results, we evaluated the use of a complex source separation as a pre-step. The results showed that the performance was only enhanced if the recognition models are trained with the features extracted from the separated audio streams. In this way, the typical errors of state-of-art separation algorithms are acknowledged, and the performance of the original instrument recognition algorithm is improved in up to 32%. © 2012 International Society for Music Information Retrieval."
Barthet M.; Dixon S.,Ethnographic observations of musicologists at the british library: Implications for music information retrieval,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601686&partnerID=40&md5=5a82b561d0848ad966c113d64cd752c9,"Barthet M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Without a rich understanding of user behaviours and needs, music information retrieval (MIR) systems might not be ideally suited to their potential users. In this study, we followed an ethnographic methodology to elicit some of the strategies used by musicologists to explore and document musical performances, in order to investigate if and how technologies could enhance such a process. Observations of musicologists studying historical recordings of classical music were conducted at the British Library. The observations show that the musicologists alternate between a closed listening practice, relying exclusively on aural observations, and a multimodal listening practice, where they interact with various music representations and information sources using different media (e.g. metadata about the recordings and performers, sound visualisations, scores, lyrics and performance videos). The spoken parts of broadcast recordings brought historical/extra-musical clues helping to understand music performance practices. Sound visualisation and computational methods fostered the analysis of specific musical expression patterns. We suggest that software designed for musicologists should facilitate switching between closed and multimodal listening modes, interaction with scores and lyrics, and analysis and annotation of speech and music performance using content-based MIR techniques. © 2011 International Society for Music Information Retrieval."
Mauch M.; Fujihara H.; Yoshii K.; Goto M.,Timbre and melody features for the recognition of vocal activity and instrumental solos in polyphonic music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599953&partnerID=40&md5=6354b13167e1bac04fc65bd5cd48b400,"Mauch M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We propose the task of detecting instrumental solos in polyphonic music recordings, and the usage of a set of four audio features for vocal and instrumental activity detection. Three of the features are based on the prior extraction of the predominant melody line, and have not been used in the context of vocal/instrumental activity detection. Using a support vector machine hidden Markov model we conduct 14 experiments to validate several combinations of our proposed features. Our results clearly demonstrate the benefit of combining the features: the best performance was always achieved by combining all four features. The top accuracy for vocal activity detection is 87.2%. The more difficult task of detecting instrumental solos equally benefits from the combination of all features and achieves an accuracy of 89.8% and a satisfactory precision of 61.1%. With this paper we also release to the public the 102 annotations we used for training and testing. The annotations offer not only vocal/nonvocal labels, but also distinguish between female and male singers, and different solo instruments. © 2011 International Society for Music Information Retrieval."
Bertin-Mahieux T.; Ellis D.P.W.; Whitman B.; Lamere P.,The million song dataset,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597375&partnerID=40&md5=86d221873f2c64e30ffb6186a09ce10c,"Bertin-Mahieux T., LabROSA, EE Dept., Columbia University, United States; Ellis D.P.W., LabROSA, EE Dept., Columbia University, United States; Whitman B., Echo Nest, Somerville, MA, United States; Lamere P., Echo Nest, Somerville, MA, United States","We introduce the Million Song Dataset, a freely-available collection of audio features and metadata for a million contemporary popular music tracks. We describe its creation process, its content, and its possible uses. Attractive features of the Million Song Database include the range of existing resources to which it is linked, and the fact that it is the largest current research dataset in our field. As an illustration, we present year prediction as an example application, a task that has, until now, been difficult to study owing to the absence of a large set of suitable data. We show positive results on year prediction, and discuss more generally the future development of the dataset. © 2011 International Society for Music Information Retrieval."
Burlet G.; Porter A.; Hankinson A.; Fujinaga I.,NEON.Js: Neume editor online,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415214&partnerID=40&md5=9b694d3addbebfc1b240344f987587a7,"Burlet G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Porter A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","This paper introduces Neon.js, a browser-based music notation editor written in JavaScript. The editor can be used to manipulate digitally encoded musical scores in square-note notation. This type of notation presents certain challenges to a music notation editor, since many neumes (groups of pitches) are ligatures - continuous graphical symbols that represent multiple notes. Neon.js will serve as a component within an online optical music recognition framework. The primary purpose of the editor is to provide a readily accessible interface to easily correct errors made in the process of optical music recognition. In this context, we envision an environment that promotes crowdsourcing to further the creation of editable and searchable online symbolic music collections and for generating and editing ground-truth data to train optical music recognition algorithms. © 2012 International Society for Music Information Retrieval."
Nieto O.; Humphrey E.J.; Bello J.P.,Compressing music recordings into audio summaries,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423876&partnerID=40&md5=e884f45e7c15ac9fa5a272090e9b7fd7,"Nieto O., New York University, United States; Humphrey E.J., New York University, United States; Bello J.P., New York University, United States","We present a criterion to generate audible summaries of music recordings that optimally explain a given track with mutually disjoint segments of itself. We represent audio as sequences of beat-synchronous harmonic features and use an exhaustive search to identify the best summary. To demonstrate the merit of this approach, we evaluate the criterion and show consistency across a collection of multiple recordings of different works. Finally, we present a fast algorithm that approximates the exhaustive search and allows us to automatically learn the hyperparameters of the algorithm for a given track. © 2012 International Society for Music Information Retrieval."
Sprechmann P.; Bronstein A.; Sapiro G.,Real-time online singing voice separation from monaural recordings using robust low-rank modeling,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423755&partnerID=40&md5=28847d3a7d519d59ba2bb71aa51ef967,"Sprechmann P., University of Minnesota, United States; Bronstein A., Tel Aviv University, Israel; Sapiro G., University of Minnesota, United States","Separating the leading vocals from the musical accompaniment is a challenging task that appears naturally in several music processing applications. Robust principal component analysis (RPCA) has been recently employed to this problem producing very successful results. The method decomposes the signal into a low-rank component corresponding to the accompaniment with its repetitive structure, and a sparse component corresponding to the voice with its quasi-harmonic structure. In this paper we first introduce a non-negative variant of RPCA, termed as robust low-rank non-negative matrix factorization (RNMF). This new framework better suits audio applications. We then propose two efficient feed-forward architectures that approximate the RPCA and RNMF with low latency and a fraction of the complexity of the original optimization method. These approximants allow incorporating elements of unsupervised, semi- and fully-supervised learning into the RPCA and RNMF frameworks. Our basic implementation shows several orders of magnitude speedup compared to the exact solvers with no performance degradation, and allows online and faster-than-real-time processing. Evaluation on the MIR-1K dataset demonstrates state-of-the-art performance. © 2012 International Society for Music Information Retrieval."
Schreiber H.; Grosche P.; Müller M.,A re-ordering strategy for accelerating index-based audio fingerprinting,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873608121&partnerID=40&md5=7161738ebad52bc6c283105fdf073664,"Schreiber H., Tagtraum Industries Incorporated, United States; Grosche P., MPI Informatik, Saarland University, Germany; Müller M., MPI Informatik, Saarland University, Germany","The Haitsma/Kalker audio fingerprinting system [4] has been in use for years, but its search algorithm's scalability has not been researched very well. In this paper we show that by simple re-ordering of the query fingerprint's subprints in the index-based retrieval step, the overall search performance can be increased significantly. Furthermore, we show that combining longer fingerprints with re-ordering can lead to even higher performance gains, up to a factor of 9.8. The proposed re-ordering scheme is based on the observation that sub-prints, which are elements of n-runs of identical consecutive sub-prints, have a higher survival rate in distorted copies of a signal (e.g. after mp3 compression) than other sub-prints. © 2011 International Society for Music Information Retrieval."
Hu X.; Kando N.,User-centered Measures Vs. System effectiveness in finding similar songs,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873422267&partnerID=40&md5=2ffc6325846771416558dc7d4adbefd3,"Hu X., Faculty of Education, University of Hong Kong, Hong Kong; Kando N., National Institute of Informatics, Japan","User evaluation in the domain of Music Information Retrieval (MIR) has been very scarce, while algorithms and systems in MIR have been improving rapidly. With the maturity of system-centered evaluation in MIR, time is ripe for MIR evaluation to involve users. In this study, we compare user-centered measures to a system effectiveness measure on the task of retrieving similar songs. To collect user-centered measures, we conducted a user experiment with 50 participants using a set of music retrieval systems that have been evaluated by a system-centered approach in the Music Information Retrieval Evaluation eXchange (MIREX). The results reveal weak correlation between user-centered measures and system effectiveness. It is also found that user-centered measures can disclose difference between systems when there was no difference on system-effectiveness. © 2012 International Society for Music Information Retrieval."
Schmidt E.M.; Scott J.; Kim Y.E.,Feature learning in dynamic environments: Modeling the acoustic structure of musical emotion,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420121&partnerID=40&md5=17c484752499451352316d4166a625cb,"Schmidt E.M., Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States; Scott J., Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States","While emotion-based music organization is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant feature representation for music emotion recognition has yet emerged. Much of the difficulty in developing emotion-based features is the ambiguity of the ground-truth. Even using the smallest time window, opinions about emotion are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled human response labels to music in the arousal-valence (A-V) emotion space with time-varying stochastic distributions. Current methods for automatic detection of emotion in music seek performance increases by combining several feature domains (e.g. loudness, timbre, harmony, rhythm). Such work has focused largely in dimensionality reduction for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this work, we seek to employ regression-based deep belief networks to learn features directly from magnitude spectra. Taking into account the dynamic nature of music, we investigate combining multiple timescales of aggregated magnitude spectra as a basis for feature learning. © 2012 International Society for Music Information Retrieval."
Mauch M.; Levy M.,Structural change on multiple time scales as a correlate of musical complexity,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596725&partnerID=40&md5=98a4c15c900c57e5a15d342306c116d1,"Mauch M., Karen House, Last.fm, London, N1 6DL, 1-11 Bache's Street, United Kingdom; Levy M., Karen House, Last.fm, London, N1 6DL, 1-11 Bache's Street, United Kingdom","We propose the novel audio feature structural change for the analysis and visualisation of recorded music, and argue that it is related to a particular notion of musical complexity. Structural change is a meta feature that can be calculated from an arbitrary frame-wise basis feature, with each element in the structural change feature vector representing the change of the basis feature at a different time scale. We describe an efficient implementation of the feature and discuss its properties based on three basis features pertaining to harmony, rhythm and timbre. We present a novel flowerlike visualisation that allows us to illustrate the overall structural change characteristics of a piece of audio in a compact way. Several examples of real-world music and synthesised audio exemplify the characteristics of the structural change feature. We present the results of a web-based listening experiment with 197 participants to show the validity of the proposed feature. © 2011 International Society for Music Information Retrieval."
Gulluni S.; Buisson O.; Essid S.; Richard G.,An interactive system for electro-acoustic music analysis,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597139&partnerID=40&md5=b9d53692b2abce4b9babcf5700e3fc0e,"Gulluni S., Institut National de L'Audiovisuel, Bry-sur-marne, France; Buisson O., Institut National de L'Audiovisuel, Bry-sur-marne, France; Essid S., Institut Telecom, Telecom ParisTech, Paris, France; Richard G., Institut Telecom, Telecom ParisTech, Paris, France","This paper, presents an interactive approach for the analysis of electro-acoustic music. An original classification scheme is devised using relevance feedback and active-learning segment selection in an interactive loop. Validation and correction information given by the user is injected in the learning process at each iteration to achieve more accurate classification. An experimental study is conducted to evaluate and compare the different classification and relevance feedback approaches that are envisaged, using a database of polyphonic pieces (with a varying degree of polyphony). The results show that the different approaches are adapted to different applications and they achieve satisfying performance in a reasonable number of iterations. © 2011 International Society for Music Information Retrieval."
Dieleman S.; Brakel P.; Schrauwen B.,Audio-based music classification with a pretrained convolutional network,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602768&partnerID=40&md5=d7898fbba31372e9d54480dc8e638c5d,"Dieleman S., Electronics and Information Systems Department, Ghent University, Belgium; Brakel P., Electronics and Information Systems Department, Ghent University, Belgium; Schrauwen B., Electronics and Information Systems Department, Ghent University, Belgium","Recently the 'Million Song Dataset', containing audio features and metadata for one million songs, was made available. In this paper, we build a convolutional network that is then trained to perform artist recognition, genre recognition and key detection. The network is tailored to summarize the audio features over musically significant timescales. It is infeasible to train the network on all available data in a supervised fashion, so we use unsupervised pretraining to be able to harness the entire dataset: we train a convolutional deep belief network on all data, and then use the learnt parameters to initialize a convolutional multilayer perceptron with the same architecture. The MLP is then trained on a labeled subset of the data for each task. We also train the same MLP with randomly initialized weights. We find that our convolutional approach improves accuracy for the genre recognition and artist recognition tasks. Unsupervised pretraining improves convergence speed in all cases. For artist recognition it improves accuracy as well. © 2011 International Society for Music Information Retrieval."
Bohak C.; Marolt M.,Finding repeating stanzas in folk songs,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431799&partnerID=40&md5=416b509198c5e3bb1c5dffaf7e182771,"Bohak C., University of Ljubljana, Slovenia; Marolt M., University of Ljubljana, Slovenia","Folk songs are typically composed of repeating parts - stanzas. To find such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of specific issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several methods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method consists of several steps. In the first step breathing (vocal) pauses are detected, which represent the candidate beginnings of individual stanzas. Next, a similarity measure is calculated between the first and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate beginnings represent the actual boundaries between stanzas, a scoring function is defined based on the calculated similarities between stanzas. A peak picking method is used in combination with global thresholding for the final selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive. © 2012 International Society for Music Information Retrieval."
Velasco M.J.; Large E.W.,Pulse detection in syncopated rhythms using neural oscillators,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604700&partnerID=40&md5=0f132aa18bd198d1509d365fd8ff83c3,"Velasco M.J., Center for Complex Systems and Brain Sciences, Florida Atlantic University, United States; Large E.W., Center for Complex Systems and Brain Sciences, Florida Atlantic University, United States","Pulse and meter are remarkable in part because these perceived periodicities can arise from rhythmic stimuli that are not periodic. This phenomenon is most striking in syncopated rhythms, found in many genres of music, including music of non-Western cultures. In general, syncopated rhythms may have energy at frequencies that do not correspond to perceived pulse or meter, and perceived metrical frequencies that are weak or absent in the objective rhythmic stimulus. In this paper, we consider syncopated rhythms that contain little or no energy at the pulse frequency. We used 16 rhythms (3 simple, 13 syncopated) to test a model of pulse/meter perception based on nonlinear resonance, comparing the nonlinear resonance model with a linear analysis. Both models displayed the ability to differentiate between duple and triple meters, however, only the nonlinear model exhibited resonance at the pulse frequency for the most challenging syncopated rhythms. This result suggests that nonlinear resonance may provide a viable approach to pulse detection in syncopated rhythms. © 2011 International Society for Music Information Retrieval."
Yoshii K.; Goto M.,A vocabulary-free infinity-gram model for nonparametric bayesian chord progression analysis,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597170&partnerID=40&md5=9d51b1a072c192a21ba77e9c41452759,"Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents probabilistic n-gram models for symbolic chord sequences. To overcome the fundamental limitations in conventional models-that the model optimality is not guaranteed, that the value of n is fixed uniquely, and that a vocabulary of chord types (e.g., major, minor, · · · ) is defined in an arbitrary way-we propose a vocabulary-free infinity-gram model based on Bayesian nonparametrics. It accepts any combinations of notes as chord types and allows each chord appearing in a sequence to have an unbounded and variable-length context. All possibilities of n are taken into account when calculating the predictive probability of a next chord given a particular context, and when an unseen chord type emerges we can avoid out-of-vocabulary error by adaptively evaluating the 0-gram probability, i.e., the combinatorial probability of note components. Our experiments using Beatles songs showed that the predictive performance of the proposed model is better than that of the state-of-theart models and that we could find stochastically-coherent chord patterns by sorting variable-length n-grams in a line according to their generative probabilities. © 2011 International Society for Music Information Retrieval."
Corrêa D.C.; Costa L.D.F.; Levada A.L.M.,Finding community structure in music genres networks,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588177&partnerID=40&md5=95e4979068b6fa24f43a6026bc4c7dc3,"Corrêa D.C., Instituto de Física de São Carlos, Universidade de São Paulo, Brazil; Costa L.D.F., Instituto de Física de São Carlos, Universidade de São Paulo, Brazil; Levada A.L.M., Departamento de Computação, Universidade Federal de São Carlos, Brazil","Complex networks have shown to be promising mechanisms to represent several aspects of nature, since their topological and structural features help in the understanding of relations, properties and intrinsic characteristics of the data. In this context, we propose to build music networks in order to find community structures of music genres. Our main contributions are twofold: 1) Define a totally unsupervised approach for music genres discrimination; 2) Incorporate topological features in music data analysis. We compared different distance metrics and clustering algorithms. Each song is represented by a vector of conditional probabilities for the note values in its percussion track. Initial results indicate the effectiveness of the proposed methodology. © 2011 International Society for Music Information Retrieval."
Niitsuma M.; Tomita Y.,Classifying bach's handwritten c-clefs,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873589025&partnerID=40&md5=ed44ac026578a3d5994947acce307474,"Niitsuma M., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom; Tomita Y., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom","The aim of this study is to explore how we could use computational technology to help determination of the chronology of music manuscripts. Applying a battery of techniques to Bach's manuscripts reveals the limitation in current image processing techniques, thereby clarifying future tasks. Analysis of C-clefs, the chosen musical symbol for this study, extracted from Bach's manuscripts dating from 1708-1748, is also carried out. Random forest using 15 features produces significant accuracy for chronological classification. © 2011 International Society for Music Information Retrieval."
Xia G.; Liang D.; Dannenberg R.B.; Harvilla M.J.,"Segmentation, clustering, and display in a personal audio database for musicians",2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596073&partnerID=40&md5=2bfc516193318f0d02191f3a5643c345,"Xia G., Carnegie Mellon University, United States; Liang D., Carnegie Mellon University, United States; Dannenberg R.B., Carnegie Mellon University, United States; Harvilla M.J., Carnegie Mellon University, United States","Managing music audio databases for practicing musicians presents new and interesting challenges. We describe a systematic investigation to provide useful capabilities to musicians both in rehearsal and when practicing alone. Our goal is to allow musicians to automatically record, organize, and retrieve rehearsal (and other) audio to facilitate review and practice (for example, playing along with difficult passages). We introduce a novel music classification system based on Eigenmusic and Adaboost to separate rehearsal recordings into segments, an unsupervised clustering and alignment process to organize segments, and a digital music display interface that provides both graphical input and output in terms of conventional music notation. © 2011 International Society for Music Information Retrieval."
Salamon J.; Peeters G.; Robel A.,Statistical characterisation of melodic pitch contours and its application for melody extraction,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420493&partnerID=40&md5=4f6e734462e094d45d03d7e12b50bfcf,"Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Peeters G., Sound Analysis-Synthesis Team, IRCAM, CNRS STMS, 75004 Paris, France; Robel A., Sound Analysis-Synthesis Team, IRCAM, CNRS STMS, 75004 Paris, France","In this paper we present a method for the statistical characterisation of melodic pitch contours, and apply it to automatic melody extraction from polyphonic music signals. Within the context of melody extraction, pitch contours represent time and frequency continuous sequences of pitch candidates out of which the melody must be selected. In previous studies we presented a melody extraction algorithm in which contour features are used in a heuristic manner to filter out non-melodic contours. In our current work, we present a method for the statistical modelling of these features, and propose an algorithm for melody extraction based on the obtained model. The algorithm exploits the learned model to compute a ""melodiness"" index for each pitch contour, which is then used to select the melody out of all pitch contours generated for an excerpt of polyphonic music. The proposed approach has the advantage that new contour features can be easily incorporated into the model without the need to manually devise rules to address each feature individually. The method is evaluated in the context of melody extraction and obtains promising results, performing comparably to a state-of-the-art heuristic-based algorithm. © 2012 International Society for Music Information Retrieval."
Yoshii K.; Goto M.,Infinite composite autoregressive models for music signal analysis,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873433845&partnerID=40&md5=02a8b4fc55e2d500ed31d48770810f4b,"Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents novel probabilistic models that can be used to estimate multiple fundamental frequencies (F0s) from polyphonic audio signals. These models are nonpara-metric Bayesian extensions of nonnegative matrix factorization (NMF) based on the source-filter paradigm, and in them an amplitude or power spectrogram is decomposed as the product of two kinds of spectral atoms (sources and filters) and time-varying gains of source-filter pairs. In this study we model musical instruments as autoregressive systems that combine two types of sources - periodic signals (comb-shaped densities) and white noise (flat density) - with all-pole filters representing resonance characteristics. One of the main problems with such composite autoregressive models (CARMs) is that the numbers of sources and filters should be given in advance. To solve this problem, we propose nonparametric Bayesian models based on gamma processes and efficient variational and multiplicative learning algorithms. These infinite CARMs (iCARMs) can discover appropriate numbers of sources and filters in a data-driven manner. We report the experimental results of multipitch analysis on the MAPS piano database. © 2012 International Society for Music Information Retrieval."
Lee J.H.; Cunningham S.J.,The impact (or non-impact) of user studies in Music Information Retrieval,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435748&partnerID=40&md5=da5f1c1029a68923afb76674c35c707f,"Lee J.H., Information School, University of Washington, United States; Cunningham S.J., Department of Computer Science, University of Waikato, New Zealand","Most Music Information Retrieval (MIR) researchers will agree that understanding users' needs and behaviors is critical for developing a good MIR system. The number of user studies in the MIR domain has been gradually increasing since the early 2000s reflecting the need for empirical studies of users. However, despite the growing number of user studies and the wide recognition of their importance, it is unclear how large their impact has been in the field; on how systems are developed, evaluation tasks are created, and how we understand critical concepts such as music similarity or music mood. In this paper, we present our analysis on the growth, publication and citation patterns, and design of 155 user studies. This is followed by a discussion of a number of issues/challenges in conducting MIR user studies and distributing the research results. We conclude by making recommendations to increase the visibility and impact of user studies in the field. © 2012 International Society for Music Information Retrieval."
Sioros G.; Holzapfel A.; Guedes C.,On measuring syncopation to drive an interactive music system,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426250&partnerID=40&md5=62c01ed7ecf234aba7af39182319768e,"Sioros G., Faculdade de Engenharia, Universidade do Porto, INESC Porto, Portugal; Holzapfel A., Music Technology Group, Universitat Pompeu Fabra, Spain; Guedes C., Faculdade de Engenharia, Universidade do Porto, INESC Porto, Portugal","In this paper we address the problem of measuring syncopation in order to mediate a musically meaningful interaction between a live music performance and an automatically generated rhythm. To this end we present a simple, yet effective interactive music system we developed. We shed some light on the complex nature of syncopation by looking into MIDI data from drum loops and whole songs. We conclude that segregation into individual rhythmic layers is necessary in order to measure the syncopation of a music ensemble. This implies that measuring syncopation on polyphonic audio signals is not yet tractable using the current state-of-the-art in audio analysis. © 2012 International Society for Music Information Retrieval."
Salamon J.; Urbano J.,Current challenges in the evaluation of predominant Melody Extraction algorithms,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873432474&partnerID=40&md5=f3c6465db186d1528e4c7801915e025d,"Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Urbano J., Department of Computer Science, University Carlos III of Madrid, Leganés, Spain","In this paper we analyze the reliability of the evaluation of Audio Melody Extraction algorithms. We focus on the procedures and collections currently used as part of the annual Music Information Retrieval Evaluation eXchange (MIREX), which has become the de-facto benchmark for evaluating and comparing melody extraction algorithms. We study several factors: the duration of the audio clips, time offsets in the ground truth annotations, and the size and musical content of the collection. The results show that the clips currently used are too short to predict performance on full songs, highlighting the paramount need to use complete musical pieces. Concerning the ground truth, we show how a minor error, specifically a time offset between the annotation and the audio, can have a dramatic effect on the results, emphasizing the importance of establishing a common protocol for ground truth annotation and system output. We also show that results based on the small ADC04, MIREX05 and INDIAN08 collections are unreliable, while the MIREX09 collections are larger than necessary. This evidences the need for new and larger collections containing realistic music material, for reliable and meaningful evaluation of Audio Melody Extraction. © 2012 International Society for Music Information Retrieval."
Şentürk S.; Chordia P.,Modeling melodic improvisation in turkish folk music using variable-length markov models,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598006&partnerID=40&md5=feefe30eed296b3a95fa93b65cfd9a1a,"Şentürk S., Georgia Tech Center for Music Technology, Atlanta, GA, United States; Chordia P., Georgia Tech Center for Music Technology, Atlanta, GA, United States","The paper describes a new database, which currently consists of 64 songs encompassing approximately 6600 notes, and a system, which uses Variable-Length Markov Models (VLMM) to predict the melodies in the uzun hava (long tune) form, a melodic structure in Turkish folk music. The work shows VLMMs are highly predictive. This suggests that variable-length Markov models (VLMMs) may be applied to makam-based and non-metered musical forms, in addition to Western musical traditions. To the best of our knowledge, the work presents the first symbolic, machine readable database of uzun havas and the first application of predictive modeling in Turkish folk music. © 2011 International Society for Music Information Retrieval."
Wolff D.; Weyde T.; Stober S.; Nürnberger A.,A systematic comparison of music similarity adaptation approaches,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436680&partnerID=40&md5=61b26b28bc09e2315e8e372a30a72776,"Wolff D., MIRG, School of Informatics, City University London, United Kingdom; Weyde T., MIRG, School of Informatics, City University London, United Kingdom; Stober S., Data and Knowledge Engineering Group, Otto-von-Guericke-Universität, Magdeburg, Germany; Nürnberger A., Data and Knowledge Engineering Group, Otto-von-Guericke-Universität, Magdeburg, Germany","In order to support individual user perspectives and different retrieval tasks, music similarity can no longer be considered as a static element of Music Information Retrieval (MIR) systems. Various approaches have been proposed recently that allow dynamic adaptation of music similarity measures. This paper provides a systematic comparison of algorithms for metric learning and higher-level facet distance weighting on the MagnaTagATune dataset. A cross-validation variant taking into account clip availability is presented. Applied on user generated similarity data, its effect on adaptation performance is analyzed. Special attention is paid to the amount of training data necessary for making similarity predictions on unknown data, the number of model parameters and the amount of information available about the music itself. © 2012 International Society for Music Information Retrieval."
Anan Y.; Hatano K.; Bannai H.; Takeda M.; Satoh K.,Polyphonic music classification on symbolic data using dissimilarity functions,2012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415756&partnerID=40&md5=692617e8e75f6f2c0c6f842cdac99d5b,"Anan Y., Department of Informatics, Kyushu University, Japan; Hatano K., Department of Informatics, Kyushu University, Japan; Bannai H., Department of Informatics, Kyushu University, Japan; Takeda M., Department of Informatics, Kyushu University, Japan; Satoh K., National Institute of Informatics, Japan","This paper addresses the polyphonic music classification problem on symbolic data. A new method is proposed which converts music pieces into binary chroma vector sequences and then classifies them by applying the dissimilarity-based classification method TWIST proposed in our previous work. One advantage of using TWIST is that it works with any dissimilarity measure. Computational experiments show that the proposed method drastically outperforms SVM and k-NN, the state-of-the-art classification methods. © 2012 International Society for Music Information Retrieval."
Foster P.; Klapuri A.; Plumbley M.D.,Causal prediction of continuous-valued music features,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606282&partnerID=40&md5=a9ba8481e2d9b1365691bf260e5ea985,"Foster P., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Klapuri A., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Plumbley M.D., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom","This paper investigates techniques for predicting sequences of continuous-valued feature vectors extracted from musical audio. In particular, we consider prediction of beatsynchronous Mel-frequency cepstral coefficients and chroma features in a causal setting, where features are predicted as they unfold in time. The methods studied comprise autoregressive models, N-gram models incorporating a smoothing scheme, and a novel technique based on repetition detection using a self-distance matrix. Furthermore, we propose a method for combining predictors, which relies on a running estimate of the error variance of the predictors to inform a linear weighting of the predictor outputs. Results indicate that incorporating information on long-term structure improves the prediction performance for continuous-valued, sequential musical data. For the Beatles data set, combining the proposed self-distance based predictor with both N-gram and autoregressive methods results in an average of 13% improvement compared to a linear predictive baseline. © 2011 International Society for Music Information Retrieval."
Gunaratna C.; Stoner E.; Menezes R.,Using network sciences to rank musicians and composers in brazilian popular music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592265&partnerID=40&md5=1d6fa35c7d0e26ee940c9090ebb054bf,"Gunaratna C., Computer Sciences, Florida Tech, United States; Stoner E., Computer Sciences, Florida Tech, United States; Menezes R., Computer Sciences, Florida Tech, United States",Music fascinates and touches most people. This fascination leads to opinions about the music pieces that reflects people's exposure and personal experience. This inherent bias of people towards music indicates that personal opinion is inappropriate for defining the quality of music and musicians. This paper takes a holistic view of the problem and delves into the understanding of the structure of Brazilian music rooted in Network Sciences. In this paper we work with a large database of albums of Brazilian music and study the structure of collaborations between all the musicians and composers. The collaboration is modelled as a social network of musicians and then analyzed from different perspectives with the goal of describing what we call the structure of that musical genre as well as provide a ranking of musicians and composers. © 2011 International Society for Music Information Retrieval.
Stowell D.; Dixon S.,Mir in school? Lessons from ethnographic observation of secondary school music classes,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585522&partnerID=40&md5=e3a7f71b6dbef4a160c582cc753eea1c,"Stowell D., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","To help maximise the usefulness of MIR technologies in the wider community, we conducted an ethnographic study of music lessons in secondary schools in London, UK. The purpose is to understand better how musical concepts are negotiated with and without technology, so we can understand when and how MIR tools might be useful. We report on some of the themes uncovered, both about the range of technologies deployed in schools and about the ways different musical concepts are discussed. Importantly, this rich observation elicits some of the nuances between various highand low-technologies. In particular, we discuss issues of multimodality and the role of technologies such as Youtube, as well as specific issues around musical concepts such as genre and rhythm. © 2011 International Society for Music Information Retrieval."
Weigl D.M.; Guastavino C.,User studies in the music information retrieval literature,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873433514&partnerID=40&md5=7db322a247f520c389d3d5ede988ff62,"Weigl D.M., School of Information Studies, McGill University, Montreal QC, Canada; Guastavino C., School of Information Studies, McGill University, Montreal QC, Canada","This paper presents an overview of user studies in the Music Information Retrieval (MIR) literature. A focus on the user has repeatedly been identified as a key requirement for future MIR research; yet empirical user studies have been relatively sparse in the literature, the overwhelming research attention in MIR remaining systems-focused. We present research topics, methodologies, and design implications covered in the user studies conducted thus far. © 2011 International Society for Music Information Retrieval."
Burgoyne J.A.; Wild J.; Fujinaga I.,An expert ground-truth set for audio chord recognition and music analysis,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572845&partnerID=40&md5=f33417e99e623642b55ece9762284ed3,"Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada; Wild J., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada","Audio chord recognition has attracted much interest in recent years, but a severe lack of reliable training data-both in terms of quantity and range of sampling-has hindered progress. Working with a team of trained jazz musicians, we have collected time-aligned transcriptions of the harmony in more than a thousand songs selected randomly from the Billboard ""Hot 100"" chart in the United States between 1958 and 1991. These transcriptions contain complete information about upper extensions and alterations as well as information about meter, phrase, and larger musical structure. We expect that these transcriptions will enable significant advances in the quality of training for audio-chord-recognition algorithms, and furthermore, because of an innovative sampling methodology, the data are usable as they stand for computational musicology. The paper includes some summary figures and statistics to help readers understand the scope of the data as well as information for obtaining the transcriptions for their own research. © 2011 International Society for Music Information Retrieval."
Six J.; Cornelis O.,Tarsos - A platform to explore pitch scales in non-western and western music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871334099&partnerID=40&md5=d3d11dbbff6589476bd5d05ac897589e,"Six J., Royal Academy of Fine Arts and Royal Conservatory, University College Ghent, Belgium; Cornelis O., Royal Academy of Fine Arts and Royal Conservatory, University College Ghent, Belgium","This paper presents Tarsos 1 , a modular software platform to extract and analyze pitch and scale organization in music, especially geared towards the analysis of non-Western music. Tarsos aims to be a user-friendly, graphical tool to explore tone scales and pitch organization in music of the world. With Tarsos pitch annotations are extracted from an audio signal that are then processed to form musicologically meaningful representations. These representations cover more than the typicalWestern 12 pitch classes, since a fine-grained resolution of 1200 cents is used. Both scales with and without octave equivalence can be displayed graphically. The Tarsos API 2 creates opportunities to analyse large sets of - ethnic - music automatically. The graphical user interface can be used for detailed, manually adjusted analysis of specific songs. Several output modalities make Tarsos an interesting tool for musicological analysis, educational purposes and even for artistic productions. © 2011 International Society for Music Information Retrieval."
Duggan B.; O'Shea B.,Tunepal - Disseminating a music information retrieval system to the traditional irish music community,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601771&partnerID=40&md5=89bba0c20694990a62e33a80518310aa,"Duggan B., Dublin Institute of Technology, School of Computing, Dublin 8, Kevin St, Ireland; O'Shea B., Dublin Institute of Technology, School of Computing, Dublin 8, Kevin St, Ireland","In this paper we present two new query-by-playing (QBP) music information retrieval (MIR) systems aimed at musicians playing traditional Irish dance music. Firstly, a browser hosted system - tunepal.org is presented. Secondly, we present Tunepal for iPhone/iPod touch devices - a QBP system that can be used in situ in traditional music sessions. Both of these systems use a backend corpus of 13,290 tunes drawn from community sources and ""standard"" references. These systems have evolved from academic research to become popular tools used by musicians around the world. 16,064 queries have been logged since the systems were launched on 31 July, 2009 and 11 February, 2010 respectively to 18 May 2010. As we log data on every query made, including geocoding queries made on the iPhone, we propose that these tools may be used to follow trends in the playing of traditional music. We also present an analysis of the data we have collected on the usage of these systems. © 2010 International Society for Music Information Retrieval."
Tjoa S.K.; Ray Liu K.J.,Factorization of overlapping harmonic sounds using approximate matching pursuit,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583863&partnerID=40&md5=713dced090628387d39512aaa6a4b42b,"Tjoa S.K., Imagine Research, San Francisco, CA 94114, United States; Ray Liu K.J., University of Maryland, College Park, MD 20742, United States","Factorization of polyphonic musical signals remains a difficult problem due to the presence of overlapping harmonics. Existing dictionary learning methods cannot guarantee that the learned dictionary atoms are semantically meaningful. In this paper, we explore the factorization of harmonic musical signals when a fixed dictionary of harmonic sounds is already present. We propose a method called approximate matching pursuit (AMP) that can efficiently decompose harmonic sounds by using a known predetermined dictionary. We illustrate the effectiveness of AMP by decomposing polyphonic musical spectra with respect to a large dictionary of instrumental sounds. AMP executes faster than orthogonal matching pursuit yet performs comparably based upon recall and precision. © 2011 International Society for Music Information Retrieval."
Orio N.; Rizo D.; Miotto R.; Montecchio N.; Schedl M.; Lartillot O.,Musiclef: A benchmark activity in multimodal music information retrieval,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861082718&partnerID=40&md5=f5553ab5ec3dead921042387c0f4ec7b,"Orio N., University of Padova, Italy; Rizo D., University of Alicante, Spain; Miotto R., University of Padova, Italy; Montecchio N., University of Padova, Italy; Schedl M., Johannes Kepler University, Austria; Lartillot O., Academy of Finland, Finland","This work presents the rationale, tasks and procedures of MusiCLEF, a novel benchmarking activity that has been developed along with the Cross-Language Evaluation Forum (CLEF). The main goal of MusiCLEF is to promote the development of new methodologies for music access and retrieval on real public music collections, which can combine content-based information, automatically extracted from music files, with contextual information, provided by users via tags, comments, or reviews. Moreover, MusiCLEF aims at maintaining a tight connection with real application scenarios, focusing on issues on music access and retrieval that are faced by professional users. To this end, this year's evaluation campaign focused on two main tasks: automatic categorization of music to be used as soundtrack of TV shows and automatic identification of the digitized material of a music digital library. © 2011 International Society for Music Information Retrieval."
Coviello E.; Miotto R.; Lanckriet G.R.G.,Combining content-based auto-taggers with decision-fusion,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873571639&partnerID=40&md5=388a564e5ea0c188f18aa0b9af3830af,"Coviello E., University of California, San Diego, United States; Miotto R., University of Padova, Italy; Lanckriet G.R.G., University of California, San Diego, United States","To automatically annotate songs with descriptive keywords, a variety of content-based auto-tagging strategies have been proposed in recent years. Different approaches may capture different aspects of a song's musical content, such as timbre, temporal dynamics, rhythmic qualities, etc. As a result, some auto-taggers may be better suited to model the acoustic characteristics commonly associated with one set of tags, while being less predictive for other tags. This paper proposes decision-fusion, a principled approach to combining the predictions of a diverse collection of content-based autotaggers that focus on various aspects of the musical signal. By modeling the correlations between tag predictions of different auto-taggers, decision-fusion leverages the benefits of each of the original auto-taggers, and achieves superior annotation and retrieval performance. © 2011 International Society for Music Information Retrieval."
Humphrey E.,Automatic characterization of digital music for rhythmic auditory stimulation,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607237&partnerID=40&md5=d794aeb094c8d6bec2c1d0be78e27215,"Humphrey E., Music Engineering Technology Group, University of Miami, Coral Gables, FL 33124, United States","A computational rhythm analysis system is proposed to characterize the suitability of musical recordings for rhythmic auditory stimulation, a neurologic music therapy technique that uses rhythm to entrain periodic physical motion. Current applications of RAS are limited by the general inability to take advantage of the enormous amount of digital music that exists today. The system aims to identify motor-rhythmic music for the entrainment of neuromuscular activity for rehabilitation and exercise, motivating the concept of musical ""use-genres."" This work builds upon prior research in meter and tempo analysis to establish a representation of rhythm chroma and alternatively describe beat spectra. © 2010 International Society for Music Information Retrieval."
Bertin-Mahieux T.; J.weiss R.; Ellis D.P.W.,Clustering beat-chroma patterns in a large music database,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602984&partnerID=40&md5=e82252b92a519dc1fd3c20b6f375b5c4,"Bertin-Mahieux T., Columbia University, United States; J.weiss R., New York University, United States; Ellis D.P.W., Columbia University, United States","A musical style or genre implies a set of common conventions and patterns combined and deployed in different ways to make individual musical pieces; for instance, most would agree that contemporary pop music is assembled from a relatively small palette of harmonic and melodic patterns. The purpose of this paper is to use a database of tens of thousands of songs in combination with a compact representation of melodic-harmonic content (the beatsynchronous chromagram) and data-mining tools (clustering) to attempt to explicitly catalog this palette - at least within the limitations of the beat-chroma representation. We use online k-means clustering to summarize 3.7 million 4-beat bars in a codebook of a few hundred prototypes. By measuring how accurately such a quantized codebook can reconstruct the original data, we can quantify the degree of diversity (distortion as a function of codebook size) and temporal structure (i.e. the advantage gained by joint quantizing multiple frames) in this music. The most popular codewords themselves reveal the common chords used in the music. Finally, the quantized representation of music can be used for music retrieval tasks such as artist and genre classification, and identifying songs that are similar in terms of their melodic-harmonic content. © 2010 International Society for Music Information Retrieval."
Marques C.; Guilherme I.R.; Nakamura R.Y.M.; Papa J.P.,New trends in musical genre classification using optimum-path forest,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575554&partnerID=40&md5=b793293111d753a0ee5373beeee78bda,"Marques C., Dep. of Statistics, Applied Math. and Computation, UNESP - Univ Estadual Paulista, Rio Claro, SP, Brazil; Guilherme I.R., Dep. of Statistics, Applied Math. and Computation, UNESP - Univ Estadual Paulista, Rio Claro, SP, Brazil; Nakamura R.Y.M., Department of Computing, UNESP - Univ Estadual Paulista, Bauru, SP, Brazil; Papa J.P., Department of Computing, UNESP - Univ Estadual Paulista, Bauru, SP, Brazil","Musical genre classification has been paramount in the last years, mainly in large multimedia datasets, in which new songs and genres can be added at every moment by anyone. In this context, we have seen the growing of musical recommendation systems, which can improve the benefits for several applications, such as social networks and collective musical libraries. In this work, we have introduced a recent machine learning technique named Optimum-Path Forest (OPF) for musical genre classification, which has been demonstrated to be similar to the state-of-the-art pattern recognition techniques, but much faster for some applications. Experiments in two public datasets were conducted against Support Vector Machines and a Bayesian classifier to show the validity of our work. In addition, we have executed an experiment using very recent hybrid feature selection techniques based on OPF to speed up feature extraction process. © 2011 International Society for Music Information Retrieval."
Haas W.B.D.; Magalhães J.P.; Veltkamp R.C.; Wiering F.,Harmtrace: Improving harmonic similarity estimation using functional harmony analysis,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578029&partnerID=40&md5=07ff5e20aa783580e94127c89a10d7d2,"Haas W.B.D., Utrecht University, Netherlands; Magalhães J.P., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands","Harmony theory has been essential in composing, analysing, and performing music for centuries. Since Western tonal harmony exhibits a considerable amount of structure and regularity, it lends itself to formalisation. In this paper we present HARMTRACE, a system that, given a sequence of symbolic chord labels, automatically derives the harmonic function of a chord in its tonal context. Among other applications, these functional annotations can be used to improve the estimation of harmonic similarity in a local alignment of two annotated chord sequences. We evaluate HARMTRACE and three other harmonic similarity measures on a corpus of 5,028 chord sequences that contains harmonically related pieces. The results show that HARMTRACE outperforms all three other similarity measures, and that information about the harmonic function of a chord improves the estimation of harmonic similarity between two chord sequences. © 2011 International Society for Music Information Retrieval."
Vigliensoni G.; Burgoyne J.A.; Hankinson A.; Fujinaga I.,Automatic pitch recognition in printed square-note notation,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861085383&partnerID=40&md5=1ae6f2b3c37a2a91a122cc7f331c8ee8,"Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","In this paper we present our research in the development of a pitch-finding system to extract the pitches of neumes-some of the oldest representations of pitch in Western music- from the Liber Usualis, a well-known compendium of plainchant as used in the Roman Catholic church. Considerations regarding the staff position, staff removal, space- and linezones, as well as how we treat specific neume classes and modifiers are covered. This type of notation presents a challenge for traditional optical music recognition (OMR) systems because individual note pitches are indivisible from the larger ligature group that forms the neume. We have created a dataset of correctly-notated transcribed chant for comparing the performance of different variants of our pitch-finding system. The best result showed a recognition rate of 97% tested with more than 2000 neumes. © 2011 International Society for Music Information Retrieval."
Nam J.; Ngiam J.; Lee H.; Slaney M.,A classification-based polyphonic piano transcription approach using learned feature representations,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578548&partnerID=40&md5=9fa52e630c645bf95285b818120ebd1d,"Nam J., Stanford University, United States; Ngiam J., Stanford University, United States; Lee H., Univ. of Michigan, Ann Arbor, United States; Slaney M., Yahoo Research, United States","Recently unsupervised feature learning methods have shown great promise as a way of extracting features from high dimensional data, such as image or audio. In this paper, we apply deep belief networks to musical data and evaluate the learned feature representations on classification-based polyphonic piano transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three public piano datasets. The results show that the learned features outperform the baseline features, and also our method gives significantly better frame-level accuracy than other state-of-the-art music transcription methods. © 2011 International Society for Music Information Retrieval."
Schmidt E.M.; Kim Y.E.,Modeling musical emotion dynamics with conditional random fields,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872700353&partnerID=40&md5=2206507b9b94d9731f6b7a3e74103d18,"Schmidt E.M., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States","Human emotion responses to music are dynamic processes that evolve naturally over time in synchrony with the music. It is because of this dynamic nature that systems which seek to predict emotion in music must necessarily analyze such processes on short-time intervals, modeling not just the relationships between acoustic data and emotion parameters, but how those relationships evolve over time. In this work we seek to model such relationships using a conditional random field (CRF), a powerful graphical model which is trained to predict the conditional probability p(y|x) for a sequence of labels y given a sequence of features x. Treating our features as deterministic, we retain the rich local subtleties present in the data, which is especially applicable to contentbased audio analysis, given the abundance of data in these problems. We train our graphical model on the emotional responses of individual annotators in an 11×11 quantized representation of the arousal-valence (A-V) space. Our model is fully connected, and can produce estimates of the conditional probability for each A-V bin, allowing us to easily model complex emotion-space distributions (e.g. multimodal) as an A-V heatmap. © 2011 International Society for Music Information Retrieval."
Schedl M.; Pohle T.; Koenigstein N.; Knees P.,What's hot ? Estimating country-specific artist popularity,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604093&partnerID=40&md5=50490c3e076a16d96a07d8ef836c4b2a,"Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Koenigstein N., Faculty of Engineering, Tel Aviv University, Tel Aviv, Israel; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Predicting artists that are popular in certain regions of the world is a well desired task, especially for the music industry. Also the cosmopolitan and cultural-aware music aficionado is likely be interested in which music is currently ""hot"" in other parts of the world. We therefore propose four approaches to determine artist popularity rankings on the country-level. To this end, we mine the following data sources: page counts from Web search engines, user posts on Twitter, shared folders on the Gnutella file sharing network, and playcount data from last.fm. We propose methods to derive artist rankings based on these four sources and perform cross-comparison of the resulting rankings via overlap scores. We further elaborate on the advantages and disadvantages of all approaches as they yield interestingly diverse results. © 2010 International Society for Music Information Retrieval."
Draman N.A.; Wilson C.; Ling S.,Modified AIS-based classifier for music genre classification,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599760&partnerID=40&md5=c489a9800ca45fcb9478997805eb8b63,"Draman N.A., Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia; Wilson C., Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia; Ling S., Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia","Automating human capabilities for classifying different genre of songs is a difficult task. This has led to various studies that focused on finding solutions to solve this problem. Analyzing music contents (often referred as content- based analysis) is one of many ways to identify and group similar songs together. Various music contents, for example beat, pitch, timbral and many others were used and analyzed to represent the music. To be able to manipulate these content representations for recognition: feature extraction and classification are two major focuses of investigation in this area. Though various classification techniques proposed so far, we are introducing yet another one. The objective of this paper is to introduce a possible new technique in the Artificial Immune System (AIS) domain called a modified immune classifier (MIC) for music genre classification. MIC is the newest version of Negative Selection Algorithm (NSA) where it stresses the self and non-self cells recognition and a complementary process for generating detectors. The discussion will detail out the MIC procedures applied and the modified part in solving the classification problem. At the end, the results of proposed framework will be presented, discussed and directions for future work are given. © 2010 International Society for Music Information Retrieval."
Anan Y.; Hatano K.; Bannai H.; Takeda M.,Music genre classification using similarity functions,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873330630&partnerID=40&md5=f210348e9ee0ea33c937ac65fe4ee8cb,"Anan Y., Department of Informatics, Kyushu University, Japan; Hatano K., Department of Informatics, Kyushu University, Japan; Bannai H., Department of Informatics, Kyushu University, Japan; Takeda M., Department of Informatics, Kyushu University, Japan","We consider music classification problems. A typical machine learning approach is to use support vector machines with some kernels. This approach, however, does not seem to be successful enough for classifying music data in our experiments. In this paper, we follow an alternative approach. We employ a (dis)similarity-based learning framework proposed byWang et al. This (dis)similarity-based approach has a theoretical guarantee that one can obtain accurate classifiers using (dis)similarity measures under a natural assumption. We demonstrate the effectiveness of our approach in computational experiments using Japanese MIDI data. © 2011 International Society for Music Information Retrieval."
Chandrasekhar V.; Sharifi M.; Ross D.A.,Survey and evaluation of audio fingerprinting schemes for mobile query-by-example applications,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586601&partnerID=40&md5=4160a6508c991a66293b808c96d05034,,"We survey and evaluate popular audio fingerprinting schemes in a common framework with short query probes captured from cell phones. We report and discuss results important for mobile applications: Receiver Operating Characteristic (ROC) performance, size of fingerprints generated compared to size of audio probe, and transmission delay if the fingerprint data were to be transmitted over a wireless link. We hope that the evaluation in this work will guide work towards reducing latency in practical mobile audio retrieval applications. © 2011 International Society for Music Information Retrieval."
Schedl M.; Knees P.; Böck S.,Investigating the similarity space of music artists on the micro-blogosphere,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591875&partnerID=40&md5=8fa5f656fb0e2147a14ae2a4c7d7197d,"Schedl M., Department of Computational Perception, Johannes Kepler University Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University Linz, Austria","Microblogging services such as Twitter have become an important means to share information. In this paper, we thoroughly analyze their potential for a key challenge in the field of MIR, namely the elaboration of perceptually meaningful similarity measures. To this end, comprehensive evaluation experiments were conducted using Twitter posts gathered during a period of several months. We investigated 23,100 combinations of different term weighting strategies, normalization methods, index term sets, Twitter query schemes, and similarity measurement techniques, aiming at determining in which way they influence the similarity estimates' quality. Evaluation was performed on the task of similar artist retrieval. Two data sets were used: one of 224 well-known artists with a uniform genre distribution, the other constituting a collection of 3,000 artists extracted from last.fm and allmusic.com. © 2011 International Society for Music Information Retrieval."
Sioros G.; Guedes C.,Complexity driven recombination of midi loops,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453416&partnerID=40&md5=20052222236653961615f8b13a1ae09e,"Sioros G., University of Porto, FEUP, INESC, 4200-465 Porto, Rua Dr. Roberto Frias, s/n, Portugal; Guedes C., University of Porto, FEUP, INESC, 4200-465 Porto, Rua Dr. Roberto Frias, s/n, Portugal","An algorithm and a software application for recombining in real time MIDI drum loops that makes use of a novel analysis of rhythmic patterns that sorts them in order of their complexity is presented. We measure rhythmic complexity by comparing each rhythmic pattern found in the loops to a metrical template characteristic of its time signature. The complexity measure is used to sort the MIDI loops prior to utilizing them in the recombination algorithm. This way, the user can effectively control the complexity and variation in the generated rhythm during performance. © 2011 International Society for Music Information Retrieval."
Weninger F.; Wöllmer M.; Schuller B.,"Automatic assessment of singer traits in popular music: Gender, age, height and race",2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870515394&partnerID=40&md5=8eea548623ada988e28fbcd8f660f833,"Weninger F., Institute for Human-Machine Communication, Technische Universität München, Germany; Wöllmer M., Institute for Human-Machine Communication, Technische Universität München, Germany; Schuller B., Institute for Human-Machine Communication, Technische Universität München, Germany","We investigate fully automatic recognition of singer traits, i. e., gender, age, height and 'race' of the main performing artist(s) in recorded popular music. Monaural source separation techniques are combined to simultaneously enhance harmonic parts and extract the leading voice. For evaluation the UltraStar database of 581 pop music songs with 516 distinct singers is chosen. Extensive test runs with Long Short-Term Memory sequence classification reveal that binary classification of gender, height, race and age reaches up to 89.6, 72.1, 63.3 and 57.6% unweighted accuracy on beat level in unseen test data. © 2011 International Society for Music Information Retrieval."
Scott J.; Migneco R.; Morton B.; Hahn C.M.; Diefenbach P.; Kim Y.E.,An audio processing library for MIR application development in flash,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602337&partnerID=40&md5=45ecf7ed75aef6f43f89f26154585711,"Scott J., Department of Electrical and Computer Engineering, Drexel University, United States; Migneco R., Department of Electrical and Computer Engineering, Drexel University, United States; Morton B., Department of Electrical and Computer Engineering, Drexel University, United States; Hahn C.M., Department of Electrical and Computer Engineering, Drexel University, United States; Diefenbach P., Department of Media Arts and Design, Drexel University, United States; Kim Y.E., Department of Electrical and Computer Engineering, Drexel University, United States","In recent years, the Adobe Flash platform has risen as a credible and universal platform for rapid development and deployment of interactive web-based applications. It is also the accepted standard for delivery of streaming media, and many web applications related to music information retrieval, such as Pandora, Last.fm and Musicovery, are built using Flash. The limitations of Flash, however, have made it difficult for music-IR researchers and developers to utilize complex sound and music signal processing within their web applications. Furthermore, the real-time audio processing and synchronization required for some music-IR-related activities demands significant computational power and specialized audio algorithms, far beyond what is possible to implement using Flash scripting. By taking advantage of features recently added to the platform, including dynamic audio control and C crosscompilation for near-native performance, we have developed the Audio-processing Library for Flash (ALF), providing developers with a library of common audio processing routines and affording Flash developers a degree of sound interaction previously unavailable through webbased platforms. We present several music-IR-driven applications that incorporate ALF to demonstrate its utility. © 2010 International Society for Music Information Retrieval."
Zacharakis A.; Pastiadis K.; Papadelis G.; Reiss J.D.,An investigation of musical timbre: Uncovering salient semantic descriptors and perceptual dimensions,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572972&partnerID=40&md5=f92350742c17bc1bd4222153c839aee3,"Zacharakis A., Centre for Digital Music, Queen Mary University of London, London, UK, United Kingdom; Pastiadis K., School of Music Studies, Aristotle University of Thessaloniki, Thessaloniki, Greece; Papadelis G., School of Music Studies, Aristotle University of Thessaloniki, Thessaloniki, Greece; Reiss J.D., Centre for Digital Music, Queen Mary University of London, London, UK, United Kingdom",A study on the verbal attributes of musical timbre was conducted in an effort to identify the most significant semantic descriptors and to quantify the association between prominent timbral aspects and several categorical properties of environmental entities. A verbal attribute magnitude estimation (VAME) type of listening test in which participants were asked to describe 23 musical sounds using 30 Greek adjectives together with verbal terms of their own choice was designed and conducted for this purpose. Factor and Cluster Analysis were performed on the subjective evaluation data in order to shed some light on the relationships between the adjectives that were proposed and to conclude to the number and quality of the salient perceptual dimensions required for the description of this set of sounds. © 2011 International Society for Music Information Retrieval.
Henaff M.; Jarrett K.; Kavukcuoglu K.; Lecun Y.,Unsupervised learning of sparse features for scalable audio classification,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864122549&partnerID=40&md5=ba42d8fc371b714c28ee450421cb8eed,"Henaff M., Courant Institute of Mathematical Sciences, New York University, United States; Jarrett K., Courant Institute of Mathematical Sciences, New York University, United States; Kavukcuoglu K., Courant Institute of Mathematical Sciences, New York University, United States; Lecun Y., Courant Institute of Mathematical Sciences, New York University, United States","In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4% accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets. © 2011 International Society for Music Information Retrieval."
Schnitzer D.; Flexer A.; Schedl M.; Widmer G.,Using mutual proximity to improve content-based audio similarity,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421776&partnerID=40&md5=2f3d52564e946f714889d0ae3979f13f,"Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","This work introduces Mutual Proximity, an unsupervised method which transforms arbitrary distances to similarities computed from the shared neighborhood of two data points. This reinterpretation aims to correct inconsistencies in the original distance space, like the hub phenomenon. Hubs are objects which appear unwontedly often as nearest neighbors in predominantly high-dimensional spaces. We apply Mutual Proximity to a widely used and standard content-based audio similarity algorithm. The algorithm is known to be negatively affected by the high number of hubs it produces. We show that without a modification of the audio similarity features or inclusion of additional knowledge about the datasets, applying Mutual Proximity leads to a significant increase of retrieval quality: (1) hubs decrease and (2) the k-nearest-neighbor classification rates increase significantly. The results of this paper show that taking the mutual neighborhood of objects into account is an important aspect which should be considered for this class of content-based audio similarity algorithms. © 2011 International Society for Music Information Retrieval."
Mcvicar M.; Ni Y.; De Bie T.; Santos-Rodriguez R.,Leveraging noisy online databases for use in chord recognition,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873577341&partnerID=40&md5=18c6426f97240ba0bbc6426f9402c899,"Mcvicar M., B University of Bristol, Intelligent Systems La, United Kingdom; Ni Y., B University of Bristol, Intelligent Systems La, United Kingdom; De Bie T., B University of Bristol, Intelligent Systems La, United Kingdom; Santos-Rodriguez R., III of Madrid, University Carlos, Spain","The most significant problem faced by Machine Learningbased chord recognition systems is arguably the lack of highquality training examples. In this paper, we address this problem by leveraging the availability of chord annotations from guitarist websites. We show that such annotations can be used as partial supervision of a semi-supervised chord recognition method-partial since accurate timing information is lacking. A particular challenge in the exploitation of these data is their low quality, potentially even leading to a performance degradation if used directly. We demonstrate however that a curriculum learning strategy can be used to automatically rank annotations according to their potential for improving the performance. Using this strategy, our experiments show a modest improvement for a simple major/ minor chord alphabet, but a highly significant improvement for a much larger chord alphabet. © 2011 International Society for Music Information Retrieval."
Baur D.; Steinmayr B.; Butz A.,Songwords: Exploring music collections through lyrics,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593138&partnerID=40&md5=f418e5f9742a437410df498aa3d926c3,"Baur D., Media Informatics Group, University of Munich (LMU), Munich, Germany; Steinmayr B., Media Informatics Group, University of Munich (LMU), Munich, Germany; Butz A., Media Informatics Group, University of Munich (LMU), Munich, Germany","The lyrics of a song are an interesting, yet underused type of symbolic music data. We present SongWords, an application for tabletop computers that allows browsing and exploring a music collection based on its lyrics. Song- Words can present the collection in a self-organizing map or sorted along different dimensions. Songs can be ordered by lyrics, user-generated tags or alphabetically by name, which allows exploring simple correlations, e.g., between genres (such as gospel) and words (such as lord). In this paper, we discuss the design rationale and implementation of SongWords as well as a user study with personal music collections. We found that lyrics indeed enable a different access to music collections and identified some challenges for future lyrics-based interfaces. © 2010 International Society for Music Information Retrieval."
Scott J.; Kim Y.E.,Analysis of acoustic features for automated multi-track mixing,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575046&partnerID=40&md5=6fe3491c47a6b1d74249b18d01c8a116,"Scott J., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States","The capability of the average person to generate digital music content has rapidly expanded over the past several decades. While the mechanics of creating a multi-track recording are relatively straightforward, using the available tools to create professional quality work requires substantial training and experience. We address one of the most fundamental processes to creating a finished product, namely determining the relative gain levels of each track to produce a final, mixed song. By modeling the time-varying mixing coefficients with a linear dynamical system, we train models that predict a weight vector for a given instrument using features extracted from the audio content of all of the tracks. © 2011 International Society for Music Information Retrieval."
Urbano J.,Information retrieval meta-evaluation: Challenges and opportunities in the music domain,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861015604&partnerID=40&md5=5812f0d6e5e3054aeca1f917071def04,"Urbano J., Department of Computer Science, University Carlos III of Madrid, Spain","The Music Information Retrieval field has acknowledged the need for rigorous scientific evaluations for some time now. Several efforts were set out to develop and provide the necessary infrastructure, technology and methodologies to carry out these evaluations, out of which the annual Music Information Retrieval Evaluation eXchange emerged. The community as a whole has enormously gained from this evaluation forum, but very little attention has been paid to reliability and correctness issues. From the standpoint of the analysis of experimental validity, this paper presents a survey of past meta-evaluation work in the context of Text Information Retrieval, arguing that the music community still needs to address various issues concerning the evaluation of music systems and the IR cycle, pointing out directions for further research and proposals in this line. © 2011 International Society for Music Information Retrieval."
Van Kranenburg P.; Biró D.P.; Ness S.; Tzanetakis G.,A computational investigation of melodic contour stability in jewish torah trope performance traditions,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871324695&partnerID=40&md5=796acfd76a0f4bc5d50b404586275784,"Van Kranenburg P., Meertens Institute, Canada; Biró D.P., University of Victoria, Canada; Ness S., University of Victoria, Canada; Tzanetakis G., University of Victoria, Canada","The cantillation signs of the Jewish Torah trope are of particular interest to chant scholars interested in the gradual transformation of oral music performance into notation. Each sign, placed above or below the text, acts as a ""melodic idea"" which either connects or divides words in order to clarify the syntax, punctuation and, in some cases, meaning of the text. Unlike standard music notation, the interpretations of each sign are flexible and influenced by regional traditions, practices of given Jewish communities, larger musical influences beyond Jewish communities, and improvisatory elements incorporated by a given reader. In this paper we describe our collaborative work in developing and using computational tools to assess the stability of melodic formulas of cantillation signs based on two different performance traditions. We also show that a musically motivated alignment algorithm obtains better results than the more commonly used dynamic time warping method for calculating similarity between pitch contours. Using a participatory design process our team, which includes a domain expert, has developed an interactive web-based interface that enables researches to explore aurally and visually chant recordings and explore the relations between signs, gestures and musical representations. © 2011 International Society for Music Information Retrieval."
Foucard R.; Essid S.; Lagrange M.; Richard G.,Multi-scale temporal fusion by boosting for music classification,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587720&partnerID=40&md5=61b18357a0ac869b5afa7f953bfd2f87,"Foucard R., CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France; Essid S., CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France; Lagrange M., CNRS-STMS, Ircam, 75004 Paris, 1, place Igor Stravinsky, France; Richard G., CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France","Short-term and long-term descriptors constitute complementary pieces of information in the analysis of audio signals. However, because they are extracted over different time horizons, it is difficult to exploit them concurrently in a fully effective manner. In this paper we propose a novel temporal fusion method that leverages the effectiveness of a given set of features by efficiently combining multi-scale versions of them. This fusion is achieved using a boosting technique exploiting trees as weak classifiers, which has the advantage of performing an embedded feature selection. We apply our algorithm to two standard classification tasks, namely musical instrument recognition and multi-tag classification. Our experiments indicate that the multi-scale approach is able to select different features at different scales and significantly outperforms the mono-scale systems in terms of classification performance. © 2011 International Society for Music Information Retrieval."
Fazekas G.; Sandler M.B.,The studio ontology framework,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583402&partnerID=40&md5=aa7169f0bf40491165b627685b76c5a0,"Fazekas G., Centre for Digital Music, Queen Mary University, London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University, London, United Kingdom","This paper introduces the Studio Ontology Framework for describing and sharing detailed information about music production. The primary aim of this ontology is to capture the nuances of record production by providing an explicit, application and situation independent conceptualisation of the studio environment. We may use the ontology to describe real-world recording scenarios involving physical hardware, or (post) production on a personal computer. It builds on Semantic Web technologies and previously published ontologies for knowledge representation and knowledge sharing. © 2011 International Society for Music Information Retrieval."
Dessein A.; Cont A.; Lemaitre G.,Real-time polyphonic music transcription with non-negative matrix factorization and beta-divergence,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602321&partnerID=40&md5=c3f46f7bf3d4410d91380632844ba88d,"Dessein A., IRCAM - CNRS UMR 9912, Paris, France; Cont A., IRCAM - CNRS UMR 9912, Paris, France; Lemaitre G., IRCAM - CNRS UMR 9912, Paris, France","In this paper, we investigate the problem of real-time polyphonic music transcription by employing non-negative matrix factorization techniques and the β-divergence as a cost function. We consider real-world setups where the music signal arrives incrementally to the system and is transcribed as it unfolds in time. The proposed transcription system is addressed with a modified non-negative matrix factorization scheme, called non-negative decomposition, where the incoming signal is projected onto a fixed basis of templates learned off-line prior to the decomposition. We discuss the use of non-negative matrix factorization with the β-divergence to achieve the real-time decomposition. The proposed system is evaluated on the specific task of piano music transcription and the results show that it can outperform several state-of-the-art off-line approaches. © 2010 International Society for Music Information Retrieval."
Mann M.; Cox T.J.; Li F.F.,Music mood classification of television theme tunes,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456455&partnerID=40&md5=f70fc79abd877cc8c54fafded54a0def,"Mann M., BBC R and D, London, United Kingdom; Cox T.J., University of Salford, United Kingdom; Li F.F., University of Salford, United Kingdom","This paper introduces methods used for Music Mood Classification to assist in the automated tagging of television programme theme tunes for the first time. The methods employed use a knowledge driven approach with tailored parameters extractable from the Matlab MIR Toolbox [1]. Four new features were developed, three based on tonality and one on tempo, to enable a degree of quantified tagging, using support vector machines, employing various kernels, optimised along six mood axes. Using a ""nearest neighbour"" method of optimisation, a success rate in the range of 80-94% was achieved in being able to classify musical audio on a five point mood scale. © 2011 International Society for Music Information Retrieval."
Funasawa S.; Ishizaki H.; Hoashi K.; Takishima Y.; Katto J.,Automated music slideshow generation using web images based on lyrics,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598613&partnerID=40&md5=38062ecdecdc980bf1c74593a418ae89,"Funasawa S., Waseda University, Japan; Ishizaki H., KDDI R and D Laboratories Inc., Japan; Hoashi K., KDDI R and D Laboratories Inc., Japan; Takishima Y., KDDI R and D Laboratories Inc., Japan; Katto J., Waseda University, Japan","In this paper, we propose a system which automatically generates slideshows for music, by utilizing images retrieved from photo sharing web sites, based on query words extracted from song lyrics. The proposed system consists of two major steps: (1) query extraction from song lyrics, (2) image selection from web image search results. Moreover, in order to improve the display duration of each image in the slideshow, we adjust image transition timing by analyzing the duration of each lyric line in the input song. We have conducted subjective evaluation experiments, which prove that the proposal can generate impressive music slideshows for any input song. © 2010 International Society for Music Information Retrieval."
Duan Z.; Pardo B.,Aligning semi-improvised music audio with its lead sheet,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574923&partnerID=40&md5=a4d0b2e4a4dfb1919b48b3b6b8a1642c,"Duan Z., Department of Electrical Engineering and Computer Science, Northwestern University, United States; Pardo B., Department of Electrical Engineering and Computer Science, Northwestern University, United States","Existing audio-score alignment methods assume that the audio performance is faithful to a fully-notated MIDI score. For semi-improvised music (e.g. jazz), this assumption is strongly violated. In this paper, we address the problem of aligning semi-improvised music audio with a lead sheet. Our approach does not require prior training on performances of the lead sheet to be aligned. We start by analyzing the problem and propose to represent the lead sheet as a MIDI file together with a structural information file. Then we propose a dynamic-programming-based system to align the chromagram representations of the audio performance and the MIDI score. Techniques are proposed to address the chromagram scaling, key transposition and structural change (e.g. a performer unexpectedly repeats a section) problems. We test our system on 3 jazz lead sheets. For each sheet we align a set of solo piano performances and a set of fullband commercial recordings with different instrumentation and styles. Results show that our system achieves promising results on some highly improvised music. © 2011 International Society for Music Information Retrieval."
Laitinen M.; Lemström K.,Dynamic programming in transposition and time-warp invariant polyphonic content-based music retrieval,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573437&partnerID=40&md5=84175f909aef369607a0f930a52dd378,"Laitinen M., Department of Computer Science, University of Helsinki, Finland; Lemström K., Department of Computer Science, University of Helsinki, Finland","We consider the problem of transposition and time-warp invariant (TTWI) polyphonic content-based music retrieval (CBMR) in symbolically encoded music. For this setting, we introduce two new algorithms based on dynamic programming. Given a query point set, of sizem, to be searched for in a database point set, of size n, and applying a search window of width w, our algorithms run in time O(mnw) for finding exact TTWI occurrences, and O(mnw2) for partial occurrences. Our new algorithms are computationally more efficient as their counterparts in the worst case scenario. More importantly, the elegance of our algorithms lies in their simplicity: they are much easier to implement and to understand than the rivalling sweepline-based algorithms. Our solution bears also theoretical interest. Dynamic programming has been used in very basic content-based retrieval problems, but generalizing them to more complex cases has proven to be challenging. In this special, seemingly more complex case, however, dynamic programming seems to be a viable option. © 2011 International Society for Music Information Retrieval."
Müller M.; Ewert S.,Chroma toolbox: Matlab implementations for extracting variants of chroma-based audio features,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870562580&partnerID=40&md5=bfc257295369eb972f9755a950a6b772,"Müller M., MPI Informatik, Saarland University, Germany; Ewert S., Computer Science III, University of Bonn, Germany","Chroma-based audio features, which closely correlate to the aspect of harmony, are a well-established tool in processing and analyzing music data. There are many ways of computing and enhancing chroma features, which results in a large number of chroma variants with different properties. In this paper, we present a chroma toolbox [13], which contains MATLAB implementations for extracting various types of recently proposed pitch-based and chroma-based audio features. Providing the MATLAB implementations on a welldocumented website under a GNU-GPL license, our aim is to foster research in music information retrieval. As another goal, we want to raise awareness that there is no single chroma variant that works best in all applications. To this end, we discuss two example applications showing that the final music analysis result may crucially depend on the initial feature design step. © 2011 International Society for Music Information Retrieval."
Chen R.; Li M.,Music structural segmentation by combining harmonic and timbral information,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869803250&partnerID=40&md5=a02f04b54f66891a08308def71f0d9c5,"Chen R., Georgia Tech Center for Music Technology, Georgia Institute of Technology, United States; Li M., Chinese Academy of Sciences, Institute of Acoustics, China","We propose a novel model for music structural segmentation aiming at combining harmonic and timbral information. We use two-level clustering with splitting initialization and random turbulence to produce segment labels using chroma and MFCC separately as feature. We construct a score matrix to combine segment labels from both aspects. Finally Nonnegative Matrix Factorization and Maximum Likelihood are applied to extract the final segment labels. By comparing sparseness, our method is capable of automatically determining the number of segment types in a given song. The pairwise F-measure of our algorithm can reach 0.63 without rules of music knowledge, running on 180 Beatles songs. We show our model can be easily associated with more sophisticated structural segmentation algorithms and extended to probabilistic models. © 2011 International Society for Music Information Retrieval."
Wu F.-H.F.; Lee T.-C.; Jang J.-S.R.; Chang K.K.; Lu C.H.; Wang W.N.,A two-fold dynamic programming approach to beat tracking for audio music with time-varying tempo,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574002&partnerID=40&md5=ccad15499d8d2988808a611ac6d01011,"Wu F.-H.F., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Lee T.-C., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Chang K.K., Department of Computer Science, King's College London, London, United Kingdom; Lu C.H., Institute for Information Industry, IDEAS, Taipei, Taiwan; Wang W.N., Institute for Information Industry, IDEAS, Taipei, Taiwan","Automatic beat tracking and tempo estimation are challenging tasks, especially for audio music with timevarying tempo. This paper proposes a two-fold dynamic programming (DP) approach to deal with beat tracking with time-varying tempo. In particular, the first DP computes the tempo curve from the tempogram. The second DP identifies the optimum beat positions from the novelty and tempo curves. Experimental results demonstrate satisfactory performance for music with significant tempo variations. The proposed approach was submitted to the task of audio beat tracking in MIREX 2010 and was ranked no. 1 for 6 performance indices out of 10, for the dataset with variable tempo. © 2011 International Society for Music Information Retrieval."
Lee J.H.,How similar is too similar?: Exploring users perceptions of similarity in playlist evaluation,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867180950&partnerID=40&md5=91277f3b081402deb9800505714728df,"Lee J.H., University of Washington, United States","The Audio Music Similarity and Retrieval (AMS) task in the annual Music Information Retrieval eXchange relies on human-evaluation. One limitation of the current design of AMS is that evaluators are provided with scarce contextual information as to why they are evaluating the similarity of the songs and how this information will be used. This study explores the potential use of AMS results for generating playlists based on similarity. We asked participants to listen to a subset of results from the 2010 AMS task and evaluate the set of candidates generated by the algorithms as a playlist generated from a seed song (the query). We found that while similarity does affect how people feel about the candidate set as a playlist, other factors such as variety, metadata, personal preference, familiarity, mix of familiar and new music, etc. also strongly affect users' perceptions of playlist quality as well. We discuss six user behaviors in detail and the implications for the AMS evaluation task. © 2011 International Society for Music Information Retrieval."
Urbano J.; Martín D.; Marrero M.; Morato J.,Audio music similarity and retrieval: Evaluation power and stability,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861089565&partnerID=40&md5=5a1690f2488c105cd6840688c971cacd,"Urbano J., Department of Computer Science, University Carlos III of Madrid, Spain; Martín D., Department of Computer Science, University Carlos III of Madrid, Spain; Marrero M., Department of Computer Science, University Carlos III of Madrid, Spain; Morato J., Department of Computer Science, University Carlos III of Madrid, Spain","In this paper we analyze the reliability of the results in the evaluation of Audio Music Similarity and Retrieval systems. We focus on the power and stability of the evaluation, that is, how often a significant difference is found between systems and how often these significant differences are incorrect. We study the effect of using different effectiveness measures with different sets of relevance judgments, for varying number of queries and alternative statistical procedures. Different measures are shown to behave similarly overall, though some are much more sensitive and stable than others. The use of different statistical procedures does improve the reliability of the results, and it allows using as little as half the number of queries currently used in MIREX evaluations while still offering very similar reliability levels. We also conclude that experimenters can be very confident that if a significant difference is found between two systems, the difference is indeed real. © 2011 International Society for Music Information Retrieval."
Li J.; Li T.; Ogihara M.,Hierarchical co-clustering of music artists and tags,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597367&partnerID=40&md5=8226bf67640f247ef3e59a84c63dfc3b,"Li J., School of Computer Science, Florida International University, Miami, FL, United States; Li T., School of Computer Science, Florida International University, Miami, FL, United States; Ogihara M., Department of Computer Science, University of Miami, Coral Gables, FL, United States","The user-assigned tag is a growingly important research topic in MIR. Noticing that some tags are more specific versions of others, this paper studies the problem of organizing tags into a hierarchical structure by taking into ac- count the fact that the corresponding artists are organized into a hierarchy based on genre and style. A novel clustering algorithm, Hierarchical Co-clustering Algorithm (HCC), is proposed as a solution. Unlike traditional hierarchical clustering algorithms that deal with homogeneous data only, the proposed algorithm simultaneously organizes two distinct data types into hierarchies. HCC is additionally able to receive constraints that state certain objects ""must-be-together"" or ""should-be-together"" and build clusters so as to satisfying the constraints. HCC may lead to better and deeper understandings of relationship between artists and tags assigned to them. An experiment finds that by trying to hierarchically cluster the two types of data better clusters are obtained for both. It is also shown that HCC is able to incorporate instance-level constraints on artists and/or tags to improve the clustering process. © 2010 International Society for Music Information Retrieval."
Xiao Q.; Suzuki M.; Kita K.,Fast hamming space search for audio fingerprinting systems,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578580&partnerID=40&md5=7c75d42a1b862f8ecaa9ebaf3619de30,"Xiao Q., Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan; Suzuki M., Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan; Kita K., Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan","In music information retrieval, a huge search space has to be explored because a query audio clip can start at any position of any music in the database, and also a query is often corrupted by significant noise and distortion. Audio fingerprints have recently attracted much attention in music information retrieval, for they provide a compact representation of the perceptually relevant parts of audio signals. In this paper, we propose an extremely fast method of exploring a huge Hamming space for audio fingerprinting systems. The effectiveness of the proposed method has been evaluated by experiments using a database of 8,740 songs. © 2011 International Society for Music Information Retrieval."
Sanden C.; Zhang J.Z.,An empirical study of multi-label classifiers for music tag annotation,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584325&partnerID=40&md5=822e0d8da41951b08c742205a916e125,"Sanden C., Department of Mathematics and Computer Science, University of Lethbridge, Lethbridge, AB, Canada; Zhang J.Z., Department of Mathematics and Computer Science, University of Lethbridge, Lethbridge, AB, Canada","In this paper we study the problem of automatic music tag annotation. Treating tag annotation as a computational classification process, we attempt to explore the relationship between acoustic features and music tags. Toward this end, we conduct a series of empirical experiments to evaluate a set of multi-label classifiers and demonstrate which ones are more suitable for music tag annotation. Furthermore, we discuss various factors in the classification process, such as feature sets, frame sizes, etc. Experiments on two publicly available datasets show that the Calibrated Label Ranking (CLR) algorithm outperforms the other classifiers for a selection of evaluation measures. © 2011 International Society for Music Information Retrieval."
Ahonen T.E.; Lemstr̈om K.; Linkola S.,"Compression-based similarity measures in symbolic, polyphonic music",2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873441308&partnerID=40&md5=fdec2b19718cab65f153c224b72bb9e4,"Ahonen T.E., Department of Computer, Science University of Helsinki, Finland; Lemstr̈om K., Department of Computer, Science University of Helsinki, Finland; Linkola S., Department of Computer, Science University of Helsinki, Finland","We present a novel compression-based method for measuring similarity between sequences of symbolic, polyphonic music. The method is based on mapping the values of binary chromagrams extracted from MIDI files to tonal centroids, then quantizing the tonal centroid representation values to sequences, and finally measuring the similarity between the quantized sequences using Normalized Compression Distance (NCD). The method is comprehensively evaluated with a test set of classical music variations, and the highest achieved precision and recall values suggest that the proposed method can be applied for similarity measuring. Also, we analyze the performance of the method and discuss what should be taken into consideration when applying the method for measurement tasks. © 2011 International Society for Music Information Retrieval."
McFee B.; Lanckriet G.,The natural language of playlists,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581058&partnerID=40&md5=47525511cb6c5c9da4ad3c14b72f66d5,"McFee B., Department of Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Department of Electrical and Computer Engineering, University of California, San Diego, United States","We propose a simple, scalable, and objective evaluation procedure for playlist generation algorithms. Drawing on standard techniques for statistical natural language processing, we characterize playlist algorithms as generative models of strings of songs belonging to some unknown language. To demonstrate the procedure, we compare several playlist algorithms derived from content, semantics, and meta-data. We then develop an efficient algorithm to learn an optimal combination of simple playlist algorithms. Experiments on a large collection of naturally occurring playlists demonstrate the efficacy of the evaluation procedure and learning algorithm. © 2011 International Society for Music Information Retrieval."
Nakashika T.; Takiguchi T.; Ariki Y.,Constrained spectrum generation using a probabilistic spectrum envelope for mixed music analysis,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585493&partnerID=40&md5=eb02ef3bfcd04673e1d9d06d39795520,"Nakashika T., Department of Computer Science and Systems Engineering, Kobe University, Japan; Takiguchi T., Department of Computer Science and Systems Engineering, Kobe University, Japan; Ariki Y., Department of Computer Science and Systems Engineering, Kobe University, Japan","NMF (Non-negative Matrix Factorization) has been one of the most widely-used techniques for musical signal analysis in recent years. In particular, the supervised type of NMF is garnering much attention in source separation with respect to the analysis accuracy and speed. In this approach, a large number of spectral samples is used for analyzing a signal. If the system has a minimal number of samples, the accuracy deteriorates. Because such methods require all the possible samples for the analysis, it is hard to build a practical analysis system. To analyze signals properly even when short of samples, we propose a novel method that combines a supervised NMF and probabilistic search algorithms. In this approach, it is assumed that each instrumental category has a model-invariant feature called a probabilistic spectrum envelope (PSE). The algorithm starts with learning the PSEs of each category using a technique based on Gaussian Process Regression. Using the PSEs for spectrum generation, an observed spectrum is analyzed under the framework of a supervised NMF. The optimum spectrum can be searched by Genetic Algorithm using sparseness and density constraints. © 2011 International Society for Music Information Retrieval."
Gang R.; Bocko G.; Lundberg J.; Roessner S.; Headla D.; Bocko M.F.,A real-time signal processing framework of musical expressive feature extraction using matlab,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475009&partnerID=40&md5=d999bf0253c8046467868445f25036f9,"Gang R., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; Bocko G., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; Lundberg J., Dept. of Music Theory, Eastman School of Music, University of Rochester, United States; Roessner S., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; Headla D., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States, Dept. of Music Theory, Eastman School of Music, University of Rochester, United States; Bocko M.F., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States, Dept. of Music Theory, Eastman School of Music, University of Rochester, United States","In this paper we propose a real-time signal processing framework for musical audio that 1) aligns the audio with an existing music score or creates a musical score by automated music transcription algorithms; and 2) obtains the expressive feature descriptors of music performance by comparing the score with the audio. Real-time audio segmentation algorithms are implemented to identify the onset points of music notes in the incoming audio stream. The score related features and musical expressive features are extracted based on these segmentation results. In a realtime setting, these audio segmentation and feature extraction operations have to be accomplished at (or shortly after) the note onset points, when an incomplete length of audio signal is captured. To satisfy real-time processing requirements while maintaining feature accuracy, our proposed framework combines the processing stages of prediction, estimation, and updating in both audio segmentation and feature extraction algorithms in an integrated refinement process. The proposed framework is implemented in a MATLAB real-time signal processing framework. © 2011 International Society for Music Information Retrieval."
Koenigstein N.; Shavitt Y.; Weinsberg E.; Weinsberg U.,On the applicability of peer-to-peer data in music information retrieval research,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606759&partnerID=40&md5=b9239696e5583b9b759c2eee5bd79af8,"Koenigstein N., School of Electrical Engineering, Tel-Aviv University, Israel; Shavitt Y., School of Electrical Engineering, Tel-Aviv University, Israel; Weinsberg E., Dept. of Industrial Engineering, Tel-Aviv University, Israel; Weinsberg U., School of Electrical Engineering, Tel-Aviv University, Israel","Peer-to-Peer (P2P) networks are being increasingly adopted as an invaluable resource for variousmusic information retrieval (MIR) tasks, including music similarity, recommendation and trend prediction. However, these networks are usually extremely large and noisy, which raises doubts regarding the ability to actually extract sufficiently accurate information. This paper evaluates the applicability of using data originating from p2p networks for MIR research, focusing on partial crawling, inherent noise and localization of songs and search queries. These aspects are quantified using songs collected from the Gnutella p2p network. We show that the power-law nature of the network makes it relatively easy to capture an accurate view of the main-streams using relatively little effort. However, some applications, like trend prediction, mandate collection of the data from the ""long tail"", hence a much more exhaustive crawl is needed. Furthermore, we present techniques for overcoming noise originating from user generated content and for filtering non informative data, while minimizing information loss. © 2010 International Society for Music Information Retrieval."
Wolff D.; Weyde T.,Adapting metrics for music similarity using comparative ratings,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578459&partnerID=40&md5=f98ac4f3744bd035945e3f94f49639dc,"Wolff D., Department of Computing, City University London, United Kingdom; Weyde T., Department of Computing, City University London, United Kingdom","Understanding how we relate and compare pieces of music has been a topic of great interest in musicology as well as for business applications, such as music recommender systems. The way music is compared seems to vary among both individuals and cultures. Adapting a generic model to user ratings is useful for personalisation and can help to better understand such differences. This paper presents an approach to use machine learning techniques for analysing user data that specifies song similarity. We explore the potential for learning generalisable similarity measures with two stateof- the-art algorithms for learning metrics. We use the audio clips and user ratings in the MagnaTagATune dataset, enriched with genre annotations from the Magnatune label. © 2011 International Society for Music Information Retrieval."
Panagakis Y.; Kotropoulos C.; Arce G.R.,ℓ1-Graph based music structure analysis,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869827694&partnerID=40&md5=de17a721d0e15a011c4e49f0c812bf2a,"Panagakis Y., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Kotropoulos C., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Arce G.R., Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE 19716-3130, United States","An unsupervised approach for automatic music structure analysis is proposed resorting to the following assumption: If the feature vectors extracted from a specific music segment are drawn from a single subspace, then the sequence of feature vectors extracted from a music recording will lie in a union of as many subspaces as the music segments in this recording are. It is well known that each feature vector stemming from a union of independent linear subspaces admits a sparse representation with respect to a dictionary formed by all other feature vectors with nonzero coefficients associated only to feature vectors that stem from its own subspace. Such sparse representation reveals the relationships among the feature vectors and it is used to construct a similarity graph, the so-called ℓ1-graph. Accordingly, the segmentation of audio features is obtained by applying spectral clustering to the ℓ1-graph. The performance of the just described approach is assessed by conducting experiments on the Pop- Music and the UPF Beatles benchmark datasets. Promising results are reported. © 2011 International Society for Music Information Retrieval."
Constantin C.; Faget Z.; Mouza C.D.; Rigaux P.,The melodic signature index for fast content-based retrieval of symbolic scores,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865823620&partnerID=40&md5=af0543c3259118c06e5deb2fb38e90ab,"Constantin C., LIP6, Univ. Paris 6, Paris, France; Faget Z., Armadillo and Univ. Paris-Dauphine, France; Mouza C.D., CEDRIC, CNAM, France; Rigaux P., Armadillo and Univ. Paris-Dauphine, France","NEUMA is an on-line library that stores collections of symbolic scores and proposes a public interface to search for melodic pieces based on several kinds of patterns: pitchesbased, with or without rhythms, transposed or not. In addition, searches can be either exact or approximate. We describe an index structure apt at supporting all these searches in a consistent setting. Its distinctive feature is an encoding of the various information that might be involved in the pattern-matching process with algebraic signatures. The properties of these signatures are suitable to represent in a compact and expressive way the sequences of complex features that constitute a melodic description. © 2011 International Society for Music Information Retrieval."
Abdoli S.,Iranian traditional music dastgah classification,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869173020&partnerID=40&md5=35eff515d5430453c8ab68d698f7e6c0,"Abdoli S., Computer Department, Central Tehran Branch, Islamic Azad University, Tehran, Iran","In this study, a system for Iranian traditional music Dastgah classification is presented. Persian music is based upon a set of seven major Dastgahs. The Dastgah in Persian music is similar to western musical scales and also Maqams in Turkish and Arabic music. Fuzzy logic type 2 as the basic part of our system has been used for modeling the uncertainty of tuning the scale steps of each Dastgah. The method assumes each performed note as a Fuzzy Set (FS), so each musical piece is a set of FSs. The maximum similarity between this set and theoretical data indicates the desirable Dastgah. In this study, a collection of small-sized dataset for Persian music is also given. The results indicate that the system works accurately on the dataset. © 2011 International Society for Music Information Retrieval."
Kirlin P.B.; Jensen D.D.,Probabilistic modeling of hierarchical music analysis,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867605584&partnerID=40&md5=1e9500f030275c8944ac3e4f6d3cacac,"Kirlin P.B., Department of Computer Science, University of Massachusetts, Amherst, United States; Jensen D.D., Department of Computer Science, University of Massachusetts, Amherst, United States","Hierarchical music analysis, as exemplified by Schenkerian analysis, describes the structure of a musical composition by a hierarchy among its notes. Each analysis defines a set of prolongations, where musical objects persist in time even though others are present. We present a formal model for representing hierarchical music analysis, probabilistic interpretations of that model, and an efficient algorithm for computing the most probable analysis under these interpretations. We represent Schenkerian analyses as maximal outerplanar graphs (MOPs). We use this representation to encode the largest known data set of computer-processable Schenkerian analyses, and we use these data to identify statistical regularities in the human-generated analyses. We show that a dynamic programming algorithm can be applied to these regularities to identify the maximum likelihood analysis for a given piece of music. © 2011 International Society for Music Information Retrieval."
Grosche P.; Müller M.; Sapp C.S.,What makes beat tracking difficult? A case study on chopin mazurkas,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595480&partnerID=40&md5=2675eb793e97702ba70a208b7ba407cd,"Grosche P., MPI Informatik, Saarland University, Germany; Müller M., MPI Informatik, Saarland University, Germany; Sapp C.S., CCRMA / CCARH, Standford University, United States","The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat tracking approaches still have significant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for detecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musical properties of certain beats that frequently evoke tracking errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimental results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material. © 2010 International Society for Music Information Retrieval."
Knees P.; Schedl M.; Pohle T.; Seyerlehner K.; Widmer G.,Supervised and unsupervised web document filtering techniques to improve text-based music retrieval,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594554&partnerID=40&md5=906d104a99f8318679d8cf48c16c23ac,"Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We aim at improving a text-based music search engine by applying different techniques to exclude misleading information from the indexing process. The idea of the original approach is to index music pieces by ""contextual"" information, more precisely, by all texts to be found on Web pages retrieved via a commonWeb search engine. This representation allows for issuing arbitrary textual queries to retrieve relevant music pieces. The goal of this work is to improve precision of the retrieved set of music pieces by filtering outWeb pages that lead to irrelevant tracks. To this end we present two unsupervised and two supervised filtering approaches. Evaluation is carried out on two collections previously used in the literature. The obtained results suggest that the proposed filtering techniques can improve results significantly but are only effective when applied to large and diverse music collections with millions of Web pages associated. © 2010 International Society for Music Information Retrieval."
Lee J.H.,Crowdsourcing music similarity judgments using mechanical turk,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598993&partnerID=40&md5=bc2baf2215eda3547385c2e9fbd5cc9b,"Lee J.H., University of Washington, United States","Collecting human judgments for music similarity evaluation has always been a difficult and time consuming task. This paper explores the viability of Amazon Mechanical Turk (MTurk) for collecting human judgments for audio music similarity evaluation tasks. We compared the similarity judgments collected from Evalutron6000 (E6K) and MTurk using the Music Information Retrieval Evaluation eXchange 2009 Audio Music Similarity and Retrieval task dataset. Our data show that the results are highly comparable, and MTurk may be a useful method for collecting subjective ground truth data. Furthermore, there are several benefits to using MTurk over the traditional E6K infrastructure. We conclude that using MTurk is a practical alternative of music similarity when it is used with some precautions. © 2010 International Society for Music Information Retrieval."
Serrá J.; Koduri G.K.; Miron M.; Serra X.,Assessing the tuning of sung indian classical music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449697&partnerID=40&md5=9e0803372c5679e34a1c9cfab73634b4,"Serrá J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Miron M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The issue of tuning in Indian classical music has been, historically, a matter of theoretical debate. In this paper, we study its contemporary practice in sung performances of Carnatic and Hindustani music following an empiric and quantitative approach. To do so, we select stable fundamental frequencies, estimated via a standard algorithm, and construct interval histograms from a pool of recordings. We then compare such histograms against the ones obtained for different music sources and against the theoretical values derived from 12-note just intonation and equal temperament. Our results evidence that the tunings in Carnatic and Hindustani music differ, the former tending to a just intonation system and the latter having much equal-tempered influences. Carnatic music also presents signs of a more continuous distribution of pitches. Further subdivisions of the octave are partially investigated, finding no strong evidence of them. © 2011 International Society for Music Information Retrieval."
Mcvicar M.; Freeman T.; Bie T.D.,Mining the correlation between lyrical and audio features and the emergence of mood,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872703021&partnerID=40&md5=0a881fdf7e5d7d21b068a102272044e0,"Mcvicar M., Intelligent Systems Lab, University of Bristol, United Kingdom; Freeman T., University of Bristol, Engineering Mathematics, United Kingdom; Bie T.D., Intelligent Systems Lab, University of Bristol, United Kingdom","Understanding the mood of music holds great potential for recommendation and genre identification problems. Unfortunately, hand-annotating music with mood tags is usually an expensive, time-consuming and subjective process, to such an extent that automatic mood recognition methods are required. In this paper we present a new unsupervised learning approach for mood recognition, based on the lyrics and the audio of a song. Our system thus eliminates the need for ground truth mood annotations, even for training the system. We hypothesize that lyrics and audio are both partially determined by the mood, and that there are no other strong common effects affecting these aspects of music. Based on this assumption, mood can be detected by performing a multi-modal analysis, identifying what lyrics and audio have in common. We demonstrate the effectiveness of this using Canonical Correlation Analysis, and confirm our hypothesis in a subsequent analysis of the results. © 2011 International Society for Music Information Retrieval."
Kolozali S.; Barthet M.; Fazekas G.; Sandler M.,Knowledge representation issues in musical instrument ontology design,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587735&partnerID=40&md5=f1665e6d5f4664435305f43c916feb85,"Kolozali S., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Barthet M., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London, United Kingdom","This paper presents preliminary work on musical instruments ontology design, and investigates heterogeneity and limitations in existing instrument classification schemes. Numerous research to date aims at representing information about musical instruments. The works we examined are based on the well known Hornbostel and Sach's classification scheme. We developed representations using the Ontology Web Language (OWL), and compared terminological and conceptual heterogeneity using SPARQL queries. We found evidence to support that traditional designs based on taxonomy trees lead to ill-defined knowledge representation, especially in the context of an ontology for the Semantic Web. In order to overcome this issue, it is desirable to have an instrument ontology that exhibits a semantically rich structure. © 2011 International Society for Music Information Retrieval."
Hankinson A.; Roland P.; Fujinaga I.,The music encoding initiative as a document-encoding framework,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581512&partnerID=40&md5=f3c5f7a4eab15ce467dca2b0640e20eb,"Hankinson A., CIRMMT, Schulich School of Music, McGill University, Canada; Roland P., University of Virginia, United States; Fujinaga I., CIRMMT, Schulich School of Music, McGill University, Canada","Recent changes in the Music Encoding Initiative (MEI) have transformed it into an extensible platform from which new notation encoding schemes can be produced. This paper introduces MEI as a document-encoding framework, and illustrates how it can be extended to encode new types of notation, eliminating the need for creating specialized and potentially incompatible notation encoding standards. © 2011 International Society for Music Information Retrieval."
Viro V.,Peachnote: Music score search and analysis platform,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572779&partnerID=40&md5=218b4cf371b48255fe35838cc56644e2,"Viro V., Ludwig-Maximilians-University, Munich, Germany","Hundreds of thousands of music scores are being digitized by libraries all over the world. In contrast to books, they generally remain inaccessible for content-based retrieval and algorithmic analysis. There is no analogue to Google Books for music scores, and there exist no large corpora of symbolic music data that would empower musicology in the way large text corpora are empowering computational linguistics, sociology, history, and other humanities that have printed word as their major source of evidence about their research subjects. We want to help change that. In this paper we present the first result of our work in this direction - the Music Ngram Viewer and search engine, an analog of Google Books Ngram Viewer and Google Books search for music scores. © 2011 International Society for Music Information Retrieval."
Levé F.; Groult R.; Arnaud G.; Séguin C.; Gaymay R.; Giraud M.,Rhythm extraction from polyphonic symbolic music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572935&partnerID=40&md5=4e4089d1e01a7a508e2481f184a733eb,"Levé F., MIS, Université de Picardie Jules Verne, Amiens, France; Groult R., MIS, Université de Picardie Jules Verne, Amiens, France; Arnaud G., MIS, Université de Picardie Jules Verne, Amiens, France; Séguin C., MIS, Université de Picardie Jules Verne, Amiens, France; Gaymay R., LIFL, Université Lille 1, CNRS, France; Giraud M., LIFL, Université Lille 1, CNRS, France","In this paper, we focus on the rhythmic component of symbolic music similarity, proposing several ways to extract a monophonic rhythmic signature from a symbolic polyphonic score. To go beyond the simple extraction of all time intervals between onsets (noteson extraction), we select notes according to their length (short and long extractions) or their intensities (intensity+/- extractions). Once the rhythm is extracted, we use dynamic programming to compare several sequences. We report results of analysis on the size of rhythm patterns that are specific to a unique piece, as well as experiments on similarity queries (ragtime music and Bach chorale variations). These results show that long and intensity+ extractions are often good choices for rhythm extraction. Our conclusions are that, even from polyphonic symbolic music, rhythm alone can be enough to identify a piece or to perform pertinent music similarity queries, especially when using wise rhythm extractions. © 2011 International Society for Music Information Retrieval."
Smith J.B.L.; Burgoyne J.A.; Fujinaga I.; De Roure D.; Downie J.S.,Design and creation of a large-scale database of structural annotations,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863546487&partnerID=40&md5=3bf7cd5c9c7b6b412cea664e9d93fb0b,"Smith J.B.L., University of Southern California, United States; Burgoyne J.A., McGill University, Canada; Fujinaga I., McGill University, Canada; De Roure D., University of Oxford, United Kingdom; Downie J.S., University of Illinois, Urbana-Champaign, United States","This paper describes the design and creation of an unprecedentedly large database of over 2400 structural annotations of nearly 1400 musical recordings. The database is intended to be a test set for algorithms that will be used to analyze a much larger corpus of hundreds of thousands of recordings, as part of the Structural Analysis of Large Amounts of Musical Information (SALAMI) project. This paper describes the design goals of the database and the practical issues that were encountered during its creation. In particular, we discuss the selection of the recordings, the development of an annotation format and procedure that adapts work by Peeters and Deruty [10], and the management and execution of the project. We also summarize some of the properties of the resulting corpus of annotations, including average inter-annotator agreement. © 2011 International Society for Music Information Retrieval."
Sargent G.; Bimbot F.; Vincent E.,A regularity-constrained viterbi algorithm and its application to the structural segmentation of songs,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868284061&partnerID=40&md5=7aedeadb2c1f552dba3c76fe85358010,"Sargent G., IRISA (UMR 6074), Universit́e de Rennes 1, France; Bimbot F., CNRS, IRISA (UMR 6074), France; Vincent E., INRIA Rennes, Bretagne Atlantique, France","This paper presents a general approach for the structural segmentation of songs. It is formalized as a cost optimization problem that combines properties of the musical content and prior regularity assumption on the segment length. A versatile implementation of this approach is proposed by means of a Viterbi algorithm, and the design of the costs are discussed. We then present two systems derived from this approach, based on acoustic and symbolic features respectively. The advantages of the regularity constraint are evaluated on a database of 100 popular songs by showing a significant improvement of the segmentation performance in terms of F-measure. © 2011 International Society for Music Information Retrieval."
Miller S.; Reimer P.; Ness S.; Tzanetakis G.,"Geoshuffle: Location-aware, content-based music browsing using self-organizing tag clouds",2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602385&partnerID=40&md5=6f77dc4b0813111d97894bbbf8e6ce57,"Miller S., Department of Electrical and Computer Engineering, University of Victoria, Canada; Reimer P., Department of Electrical and Computer Engineering, University of Victoria, Canada; Ness S., Department of Computer Science, University of Victoria, Canada; Tzanetakis G., Department of Computer Science, University of Victoria, Canada","In the past few years the computational capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShuffle - a prototype system for content-based music browsing and exploration that targets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning capabilities based on GPS. GeoShuffle adds location-based and time-based context to a user's listening preferences. Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of interaction is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of textual information that can be displayed. We propose selforganizing tag clouds, a 2D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evalute the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can improve the quality of music recommendation and that selforganizing tag clouds provide faster browsing and are more engaging than text-based tag clouds. © 2010 International Society for Music Information Retrieval."
Cuthbert M.S.; Ariza C.; Friedland L.,Feature extraction and machine learning on symbolic music using the music21 toolkit,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582967&partnerID=40&md5=7ab85de81198c56f1250611a0be38698,"Cuthbert M.S., Music and Theater Arts, M.I.T., United States; Ariza C., Music and Theater Arts, M.I.T., United States; Friedland L., Department of Computer Science, University of Massachusetts, Amherst, United States","Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the ""feature"" capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system's built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper's demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music. © 2011 International Society for Music Information Retrieval."
Cho T.; Bello J.P.,A feature smoothing method for chord recognition using recurrence plots,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873375453&partnerID=40&md5=e21be71c47db97444688a0a728b55455,"Cho T., Music and Audio Research Laboratory (MARL), New York University, New York, United States; Bello J.P., Music and Audio Research Laboratory (MARL), New York University, New York, United States","In this paper, we propose a feature smoothing technique for chord recognition tasks based on repeated patterns within a song. By only considering repeated segments of a song, our method can smooth the features without losing chord boundary information and fine details of the original feature. While a similar existing technique requires several hard decisions such as beat quantization and segmentation, our method uses a simple pragmatic approach based on recurrence plot to decide which repeated parts to include in the smoothing process. This approach uses a more formal definition of the repetition search and allows shorter (""chordsize"") repeated segments to contribute to the feature improvement process. In our experiments, our method outperforms conventional and popular smoothing techniques (a moving average filter and a median filter). In particular, it shows a synergistic effect when used with the Viterbi decoder. © 2011 International Society for Music Information Retrieval."
Bryan N.J.; Wang G.,Musical influence network analysis and rank of sample-based music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586928&partnerID=40&md5=b554f672f0271a92af1bc094f2db32fd,"Bryan N.J., Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States; Wang G., Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States","Computational analysis of musical influence networks and rank of sample-based music is presented with a unique outside examination of the WhoSampled.com dataset. The exemplary dataset maintains a large collection of artist-to-artist relationships of sample-based music, specifying the origins of borrowed or sampled material on a song-by-song basis. Directed song, artist, and musical genre networks are created from the data, allowing the application of social network metrics to quantify various trends and characteristics. In addition, a method of influence rank is proposed, unifying song-level networks to higher-level artist and genre networks via a collapse-and-sum approach. Such metrics are used to help interpret and describe interesting patterns of musical influence in sample-based music suitable for musicological analysis. Empirical results and visualizations are also presented, suggesting that sampled-based influence networks follow a power-law degree distribution; heavy influence of funk, soul, and disco music on modern hip-hop, R&B, and electronic music; and other musicological results. © 2011 International Society for Music Information Retrieval."
Goto M.; Yoshii K.; Fujihara H.; Mauch M.; Nakano T.,Songle: A web service for active music listening improved by user contributions,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860458729&partnerID=40&md5=5568ad3c03035e2cad1ef200d41a9f31,"Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Mauch M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Nakano T., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper describes a public web service for active music listening, Songle, that enriches music listening experiences by using music-understanding technologies based on signal processing. Although various research-level interfaces and technologies have been developed, it has not been easy to get people to use them in everyday life. Songle serves as a showcase to demonstrate how people can benefit from music-understanding technologies by enabling people to experience active music listening interfaces on the web. Songle facilitates deeper understanding of music by visualizing music scene descriptions estimated automatically, such as music structure, hierarchical beat structure, melody line, and chords. When using music-understanding technologies, however, estimation errors are inevitable. Songle therefore features an efficient error correction interface that encourages people to contribute by correcting those errors to improve the web service. We also propose a mechanism of collaborative training for music-understanding technologies, in which corrected errors will be used to improve the music-understanding performance through machine learning techniques. We hope Songle will serve as a research platform where other researchers can exhibit results of their music-understanding technologies to jointly promote the popularization of the field of music information research. © 2011 International Society for Music Information Retrieval."
Dixon S.; Tidhar D.; Benetos E.,"The temperament police: The truth, the ground truth, and nothing but the truth",2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864241911&partnerID=40&md5=468e9e8c598cd9aed876657ecf2ce3fa,"Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Tidhar D., AHRC Research Centre for Musical Performance as Creative Practice, King's College London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","The tuning system of a keyboard instrument is chosen so that frequently used musical intervals sound as consonant as possible. Temperament refers to the compromise arising from the fact that not all intervals can be maximally consonant simultaneously. Recent work showed that it is possible to estimate temperament from audio recordings with no prior knowledge of the musical score, using a conservative (high precision, low recall) automatic transcription algorithm followed by frequency estimation using quadratic interpolation and bias correction from the log magnitude spectrum. In this paper we develop a harpsichord-specific transcription system to analyse over 500 recordings of solo harpsichord music for which the temperament is specified on the CD sleeve notes. We compare the measured temperaments with the annotations and discuss the differences between temperament as a theoretical construct and as a practical issue for professional performers and tuners. The implications are that ground truth is not always scientific truth, and that content-based analysis has an important role in the study of historical performance practice. © 2011 International Society for Music Information Retrieval."
Hamel P.; Lemieux S.; Bengio Y.; Eck D.,Temporal pooling and multiscale learning for automatic annotation and ranking of music audio,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864146684&partnerID=40&md5=f7033f92ad7b42b55c540ec948974d4c,"Hamel P., DIRO, Université de Montréal, Montréal, QC, Canada; Lemieux S., DIRO, Université de Montréal, Montréal, QC, Canada; Bengio Y., DIRO, Université de Montréal, Montréal, QC, Canada; Eck D., Google Inc., Mountain View, CA, United States","This paper analyzes some of the challenges in performing automatic annotation and ranking of music audio, and proposes a few improvements. First, we motivate the use of principal component analysis on the mel-scaled spectrum. Secondly, we present an analysis of the impact of the selection of pooling functions for summarization of the features over time. We show that combining several pooling functions improves the performance of the system. Finally, we introduce the idea of multiscale learning. By incorporating these ideas in our model, we obtained state-of-the-art performance on the Magnatagatune dataset. © 2011 International Society for Music Information Retrieval."
Mattek A.; Casey M.,Crossmodal aesthetics from a feature extraction perspective: A pilot study,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572708&partnerID=40&md5=b9e84ad3d6825a1b3ae29ef6c355a496,"Mattek A., Dartmouth College, United States; Casey M., Dartmouth College, United States","This paper investigates perceptual relationships between art in the auditory and visual domains. First, we conducted a behavioral experiment asking subjects to assess similarity between 10 musical recordings and 10 works of abstract art. We found a significant degree of agreement across subjects as to which images correspond to which audio, even though neither the audio nor the images possessed semantic content. Secondly, we sought to find the relationship between audio and images within a defined feature space that correlated with the subjective similarity judgments. We trained two regression models using leave- one-subject-out and leave-one-audio-out crossvalidation respectively, and exhaustively evaluated each model's ability to predict features of subject-ranked similar images using only a given audio clip's features. A retrieval task used the predicted image features to retrieve likely related images from the data set. The task was evaluated using the ground truth of subjects' actual similarity judgments. Our results show a mean cross-validated prediction accuracy of 0.61 with p<0.0001 for the first model, and a mean prediction accuracy of 0.51 with p<0.03 for the second model. © 2011 International Society for Music Information Retrieval."
Chang K.K.; Jang J.-S.R.; Iliopoulos C.S.,Music genre classification via compressive sampling,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607146&partnerID=40&md5=fab3176c47a2b71eaeb6957f1fd374c1,"Chang K.K., Department of Computer Science, King's College London, London, United Kingdom; Jang J.-S.R., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Iliopoulos C.S., Department of Computer Science, King's College London, London, United Kingdom","Compressive sampling (CS) is a new research topic in signal processing that has piqued the interest of a wide range of researchers in different fields recently. In this paper, we present a CS-based classifier for music genre classification, with two sets of features, including short-time and long-time features of audio music. The proposed classifier generates a compact signature to achieve a significant reduction in the dimensionality of the audio music signals. The experimental results demonstrate that the computation time of the CS-based classifier is only about 20% of SVM on GTZAN dataset, with an accuracy of 92.7%. Several experiments were conducted in this study to illustrate the feasibility and robustness of the proposed methods as compared to other approaches. © 2010 International Society for Music Information Retrieval."
Thoshkahna B.; Nsabimana F.X.; Ramakrishnan K.R.,A transient detection algorithm for audio using iterative analysis of STFT,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575018&partnerID=40&md5=6ec10853d53ce3a561f3906f91ae3728,"Thoshkahna B., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India; Nsabimana F.X., Project Group Hearing, Speech and Audio Technology, Fraunhofer Institute of Digital Media Technology, Oldenberg, Germany; Ramakrishnan K.R., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India","We propose an iterative algorithm to detect transient segments in audio signals. Short time Fourier transform(STFT) is used to detect rapid local changes in the audio signal. The algorithm has two steps that iteratively - (a) calculate a function of the STFT and (b) build a transient signal. A dynamic thresholding scheme is used to locate the potential positions of transients in the signal. The iterative procedure ensures that genuine transients are built up while the localised spectral noise are suppressed by using an energy criterion. The extracted transient signal is later compared to a ground truth dataset. The algorithm performed well on two databases. On the EBU-SQAM database of monophonic sounds, the algorithmachieved an F-measure of 90% while on our database of polyphonic audio an F-measure of 91% was achieved. This technique is being used as a preprocessing step for a tempo analysis algorithm and a TSR (Transients + Sines + Residue) decomposition scheme. © 2011 International Society for Music Information Retrieval."
Unal E.; Chew E.; Georgiou P.; Narayanan S.S.,A perplexity based cover song matching system for short length queries,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576743&partnerID=40&md5=cc96b20b15a61b33635addeaa6ec7d99,"Unal E., TÜBITAK BILGEM, Turkey; Chew E., Queen Mary University of London, United Kingdom; Georgiou P., University of Southern, CA, United States; Narayanan S.S., University of Southern, CA, United States","A music retrieval system that matches a short length music query with its variations in a database is proposed. In order to avoid the negative effects of different orchestration and performance style a nd t empo o n transcription and matching, a mid-level representation schema and a tonal modeling approach is used. The mid-level representation approach transcribes the music pieces into a sequence of music tags corresponding to major and minor triad labels. From the transcribed sequence, n-gram models are built to statistically represent the harmonic progression. For retrieval, a perplexity based similarity score is calculated between each n-gram in the database and that for the query. The retrieval performance of the system is presented for a dataset of 2000 classical music pieces modeled using ngrams of sizes 2 through 6. We observe improvements in retrieval performance with increasing query length and ngram order. The improvement converges to a little over one for all query lengths tested when n reaches 6. © 2011 International Society for Music Information Retrieval."
Dressler K.,An auditory streaming approach for melody extraction from polyphonic music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419027&partnerID=40&md5=f5718286b82d079ef71e42fc5a47b4ff,"Dressler K., Fraunhofer Institute for Digital Media Technology IDMT, Ilmenau, Germany","This paper proposes an efficient approach for the identification of the predominant voice from polyphonic musical audio. The algorithm implements an auditory streaming model which builds upon tone objects and salient pitches. The formation of voices is based on the regular update of the frequency and the magnitude of so called streaming agents, which aim at salient tones or pitches close to their preferred frequency range. Streaming agents which succeed to assemble a big magnitude start new voice objects, which in turn add adequate tones. The algorithm was evaluated as part of a melody extraction system during the MIREX audio melody extraction evaluation, where it gained very good results in the voicing detection and overall accuracy. © 2011 International Society for Music Information Retrieval."
Wang C.-C.; Jang J.-S.R.; Wang W.,An improved query by singing/humming system using melody and lyrics information,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594172&partnerID=40&md5=3314de9636bdc44d725dfc7ace05f5d0,"Wang C.-C., Dept. of CS, MIR Lab, Tsing Hua Univ., Taiwan; Jang J.-S.R., Dept. of CS, MIR Lab, Tsing Hua Univ., Taiwan; Wang W., Institute for Information Industry, Taiwan","This paper proposes an improved query by singing/ humming (QBSH) system using both melody and lyrics information for achieving better performance. Singing/ humming discrimination (SHD) is first performed to distinguish singing from humming queries. For a humming query, we apply a pitch-only melody recognition method that has been used for QBSH task at MIREX with rank-1 performance. For a singing query, we combine the scores from melody recognition and lyrics recognition to take advantage of the extra lyrics information. Lyrics recognition is based on a modified tree lexicon that is commonly used in speech recognition. The performance of the overall QBSH system achieves 39.01% and 23.53% error reduction rates, respectively, for top-20 recognition under two experimental settings, indicating the feasibility of the proposed method. © 2010 International Society for Music Information Retrieval."
Okumura K.; Sako S.; Kitamura T.,Stochastic modeling of a musical performance with expressive representations from the musical score,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586837&partnerID=40&md5=f1198f674902f4c277b5a6619aefd614,"Okumura K., Nagoya Institute of Technology, Japan; Sako S., Nagoya Institute of Technology, Japan; Kitamura T., Nagoya Institute of Technology, Japan","This paper presents a method for describing the characteristics of human musical performance. We consider the problem of building models that express the ways in which deviations from a strict interpretations of the score occurs in the performance, and that cluster these deviations automatically. The clustering process is performed using expressive representations unambiguously notated on the musical score, without any arbitrariness by the human observer. The result of clustering is obtained as hierarchical tree structures for each deviational factor that occurred during the operation of the instrument. This structure represents an approximation of the performer's interpretation with information notated on the score they used during the performance. This model represents the conditions that generate the difference in the fluctuation of performance expression and the amounts of deviational factors directly from the data of real performance. Through validations of applying the method to the data measured from real performances, we show that the use of information regarding expressive representation on the musical score enables the efficient estimation of generative-model for the musical performance. © 2011 International Society for Music Information Retrieval."
Mauch M.; Dixon S.,Approximate note transcription for the improved identification of difficult chords,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595823&partnerID=40&md5=933a0157821aba12bb3c81b0d467469a,"Mauch M., Centre for Digital Music, Queen Mary University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University, London, United Kingdom","The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord profiles and higher-level time-series modelling have received a lot of attention, resulting in methods with an overall performance of more than 70% in the MIREX Chord Detection task 2009. Research on the front end of chord transcription algorithms has often concentrated on finding good chord templates to fit the chroma features. In this paper we reverse this approach and seek to find chroma features that are more suitable for usage in a musically-motivated model. We do so by performing a prior approximate transcription using an existing technique to solve non-negative least squares problems (NNLS). The resulting NNLS chroma features are tested by using them as an input to an existing state-of-the-art high-level model for chord transcription. We achieve very good results of 80% accuracy using the song collection and metric of the 2009 MIREX Chord Detection tasks. This is a significant increase over the top result (74%) in MIREX 2009. The nature of some chords makes their identification particularly susceptible to confusion between fundamental frequency and partials. We show that the recognition of these diffcult chords in particular is substantially improved by the prior approximate transcription using NNLS. © 2010 International Society for Music Information Retrieval."
McFee B.; Barrington L.; Lanckriet G.,Learning similarity from collaborative filters,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596877&partnerID=40&md5=d773286b2bebab8b5206d0bfc60489fc,"McFee B., Department of Computer Science and Engineering, University of California, San Diego, United States; Barrington L., Department of Electrical and Computer Engineering, University of California, San Diego, United States; Lanckriet G., Department of Electrical and Computer Engineering, University of California, San Diego, United States","Collaborative filtering methods (CF) exploit the wisdom of crowds to capture deeply structured similarities in musical objects, such as songs, artists or albums. When CF is available, it frequently outperforms content-based methods in recommendation tasks. However, songs in the so-called ""long tail"" cannot reap the benefits of collaborative filtering, and practitioners must rely on content-based methods. We propose a method for improving contentbased recommendation in the long tail by learning an optimized similarity function from a sample of collaborative filtering data. Our experimental results demonstrate substantial improvements in accuracy by learning optimal similarity functions. © 2010 International Society for Music Information Retrieval."
Joo S.; Park S.; Jo S.; Yoo C.D.,Melody extraction based on harmonic coded structure,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579451&partnerID=40&md5=78f659b1159c7f6b875f37d2940a3516,"Joo S., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Park S., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Jo S., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Yoo C.D., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea","This paper considers a melody extraction algorithm that estimates the melody in polyphonic audio using the harmonic coded structure (HCS) to model melody in the minimum mean-square-error (MMSE) sense. The HCS is harmonically modulated sinusoids with the amplitudes defined by a set of codewords. The considered algorithm performs melody extraction in two steps: i) pitch-candidate estimation and ii) pitch-sequence identification. In the estimation step, pitch candidates are estimated such that the HCS best represents the polyphonic audio in the MMSE sense. In the identification step, a melody line is selected from many possible pitch sequences based on the properties of melody line. Posterior to the melody line selection, a smoothing process is applied to refine spurious pitches and octave errors. The performance of the algorithm is evaluated and compared using the ADC04 and the MIREX05 dataset. The results show that the performance of the proposed algorithm is better than or comparable to other algorithms submitted to MIREX2009. © 2011 International Society for Music Information Retrieval."
Knight T.; Upham F.; Fujinaga I.,The potential for automatic assessment of trumpet tone quality,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572020&partnerID=40&md5=0beafc0d0123f241d895ae288cd02e30,"Knight T., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Upham F., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada","The goal of this study was to examine the possibility of training machine learning algorithms to differentiate between the performance of good notes and bad notes. Four trumpet players recorded a total of 239 notes from which audio features were extracted. The notes were subjectively graded by five brass players. The resulting dataset was used to train support vector machines with different groupings of ratings. Splitting the data set into two classes (good and bad) at the median rating, the classifier showed an average success rate of 72% when training and testing using cross-validation. Splitting the data into three roughly-equal classes (good, medium, and bad), the classifier correctly identified the class an average of 54% of the time. Even using seven classes, the classifier identified the correct class 46% of the time, which is better than the result expected from chance or from the strategy of picking the most populous class (36%). © 2011 International Society for Music Information Retrieval."
Levy M.,Improving perceptual tempo estimation with crowd-sourced annotations,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873422152&partnerID=40&md5=449b1a1b43851cbf030b686cec8380c5,"Levy M., Last.fm Ltd., Karen House, London N1 6DL, 1-11 Baches Street, United Kingdom","We report the design and results of a web-based experiment intended to support the development and evaluation of tempo estimation algorithms, in which users tap to music and select descriptive labels. Analysis of the tapping data and labels chosen shows that, while different listeners frequently entrain to different metrical levels for some pieces, they rarely disagree about which pieces are fast and which are slow. We show how this result can be used to improve both the evaluation metrics used for automatic tempo estimation and the estimation algorithms themselves. We also report the relative performance of two recent tempo estimation methods according to a further controlled experiment that does not depend on groundtruth values of any kind. © 2011 International Society for Music Information Retrieval."
Mak C.-M.; Lee T.; Senapati S.; Yeung Y.-T.; Lam W.-K.,Similarity measures for chinese pop music based on low-level audio signal attributes,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593366&partnerID=40&md5=8323d1c8b19f16c5300042769784e18e,"Mak C.-M., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Lee T., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Senapati S., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Yeung Y.-T., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Lam W.-K., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong","In this article a method of computing similarity of two Chinese pop songs is presented. It is based on five attributes extracted from the audio signal. They include music instrument, singing voice style, singer gender, tempo, and degree of noisiness. We compare the computed similarity measures with similarity scores obtained with subjective listening by over 200 human subjects. The results show that rhythm and mood related attributes like tempo and degree of noisiness are most correlated to human perception of Chinese pop songs. Instrument and singing style are relatively less relevant. The results of subjective evaluation also indicate that the proposed method of similarity computation is fairly correlated with human perception. © 2010 International Society for Music Information Retrieval."
Bergstra J.; Mandel M.; Eck D.,Scalable genre and tag prediction with spectral covariance,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599467&partnerID=40&md5=e5d2f1ec0f45525ac46293869ad58bb5,"Bergstra J., University of Montreal, Canada; Mandel M., University of Montreal, Canada; Eck D., University of Montreal, Canada","Cepstral analysis is effective in separating source from filter in vocal and monophonic [pitched] recordings, but is it a good general-purpose framework for working with music audio? We evaluate covariance in spectral features as an alternative to means and variances in cepstral features (particularly MFCCs) as summaries of frame-level features. We find that spectral covariance is more effective than mean, variance, and covariance statistics of MFCCs for genre and social tag prediction. Support for our model comes from strong and state-of-the-art performance on the GTZAN genre dataset, MajorMiner, and MagnaTagatune. Our classification strategy based on linear classifiers is easy to implement, exhibits very little sensitivity to hyper-parameters, trains quickly (even for web-scale datasets), is fast to apply, and offers competitive performance in genre and tag prediction. © 2010 International Society for Music Information Retrieval."
Cabredo R.; Legaspi R.; Numao M.,Identifying emotion segments in music by discovering motifs in physiological data,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587633&partnerID=40&md5=d0103e447fb7905df7c440294efb0036,"Cabredo R., Institute of Scientific and Industrial Research, Osaka University, Japan; Legaspi R., Institute of Scientific and Industrial Research, Osaka University, Japan; Numao M., Institute of Scientific and Industrial Research, Osaka University, Japan","Music can induce different emotions in people. We propose a system that can identifymusic segmentswhich induce specific emotions from the listener. The work involves building a knowledge base with mappings between affective states (happiness, sadness, etc.) and music features (rhythm, chord progression, etc.). Building this knowledge base requires background knowledge from music and emotions psychology. Psychophysiological responses of a user, particularly, the blood volume pulse, are taken while he listens to music. These signals are analyzed and mapped to various musical features of the songs he listened to. A motif discovery algorithm used in data mining is adapted to analyze signals of physiological data. Motif discovery finds patterns in the data that indicate points of interest in the music. The differentmotifs are stored in a library of patterns and used to identify other songs that have similar musical content. Results show that motifs selected have similar chord progressions. Some of which include frequently used chords in western pop music. © 2011 International Society for Music Information Retrieval."
Serra X.,A multicultural approach in music information research,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436679&partnerID=40&md5=7b4219f4fe82686f8c342791c301bcc0,"Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Our information technologies do not respond to the world's multicultural reality; in fact, we are imposing the paradigms of our market-driven western culture also on IT, thus facilitating the access of a small part of the world's information to a small part of the world's population. The current IT research efforts may even make it worse, and future IT will accentuate this information bias. Most IT research is being carried out with a western centered approach and as a result, most of our data models, cognition models, user models, interaction models, ontologies, etc., are culturally biased. This fact is quite evident in music information research, since, despite the world's richness in terms of musical culture, most research is centered on CDs and metadata of western commercial music. This is the motivation behind a large and ambitious project funded by the European Research Council entitled ""CompMusic: Computational Models for the discovery of the world's music."" In this paper we present the ideas supporting this project, the challenges that we want to work on, and the proposed approaches to tackle these challenges. © 2011 International Society for Music Information Retrieval."
Mcfee B.; Lanckriet G.,Large-scale music similarity search with spatial trees,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572318&partnerID=40&md5=682de5945f371af2c6235be17d8e5d5d,"Mcfee B., Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","T Many music information retrieval tasks require finding the nearest neighbors of a query item in a high-dimensional space. However, the complexity of computing nearest neighbors grows linearly with size of the database, making exact retrieval impractical for large databases. We investigate modern variants of the classical KD-tree algorithm, which efficiently index high-dimensional data by recursive spatial partitioning. Experiments on the Million Song Dataset demonstrate that content-based similarity search can be significantly accelerated by the use of spatial partitioning structures. © 2011 International Society for Music Information Retrieval."
Mayer R.; Rauber A.,Musical genre classification by ensembles of audio and lyrics features,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862061913&partnerID=40&md5=c6d72228cf990562fd0158b90f71a509,"Mayer R., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","Algorithms that can understand and interpret characteristics of music, and organise them for and recommend them to their users can be of great assistance in handling the ever growing size of both private and commercial collections. Music is an inherently multi-modal type of data, and the lyrics associated with the music are as essential to the reception and the message of a song as is the audio. In this paper, we present advanced methods on how the lyrics domain of music can be combined with the acoustic domain. We evaluate our approach by means of a common task in music information retrieval, musical genre classification. Advancing over previous work that showed improvements with simple feature fusion, we apply the more sophisticated approach of result (or late) fusion. We achieve results superior to the best choice of a single algorithm on a single feature set. © 2011 International Society for Music Information Retrieval."
Laplante A.,Social capital and music discovery: An examination of the ties through which late adolescents discover new music,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870573555&partnerID=40&md5=e370b5cb2f87bc38636074d2637dd0e5,"Laplante A., ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Canada","Research on everyday life information seeking has demonstrated that people often relied on other people to obtain the information they need. Weak ties (i.e., acquaintances) were found to be particularly instrumental to get new information. This study employed social network analysis to examine the characteristics of the ties through which late adolescents (15-17 years old) discover new music. In-depth interviews with 19 adolescents were conducted, which generated a sample of 334 ties. A statistical analysis of the ties showed that these adolescents relied mostly on strong ties to expand their music repertoire, that is, on people to which they felt very close and with whom they had frequent contacts. These ties were predominantly homophilous in terms of age, gender and musical taste. It was also found that parents were more likely than friends or other types of kins to be instrumental for music discovery. These findings suggest that a better knowledge of the characteristics of the ties through which people discover new music could provide useful insights for the design of recommender systems that include social networking features. © 2011 International Society for Music Information Retrieval."
Raphael C.; Wang J.,Newapproaches to optical music recognition,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873442036&partnerID=40&md5=fd0b3c17bf03b0469a8e9722a04546e0,"Raphael C., Indiana University, School of Informatics and Computing, United States; Wang J., Indiana University, School of Informatics and Computing, United States","We present the beginnings of a new system for optical music recognition (OMR), aimed toward the score images of the InternationalMusic Score Library Project (IMSLP). Our system focuses on measures as the basic unit of recognition. We identify candidate composite symbols (chords and beamed groups) using grammatically-formulated top-down model-based methods, while employing template matching to find isolated rigid symbols. We reconcile these overlapping symbols by seeking non-overlapping variants of the composite symbols that best account for the pixel data. We present results on a representative score from the IMSLP. © 2011 International Society for Music Information Retrieval."
Macrae R.; Dixon S.,"Guitar tab mining, analysis and ranking",2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420306&partnerID=40&md5=bb35dab58df2806784801b4c470474d9,"Macrae R., Centre for Digital Music, Queen Mary University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University, London, United Kingdom","With over 4.5 million tablatures and chord sequences (collectively known as tabs), the web holds vast quantities of hand annotated scores in non-standardised text files. These scores are typically error-prone and incomplete, and tab collections contain many duplicates, making retrieval of high quality tabs difficult. Despite this, tabs are by far the most popular means of sharing musical instructions on the internet. We have developed tools that use text analysis and alignment for the automatic retrieval, interpretation and analysis of such tabs in order to filter and estimate the most accurate tabs from the multitude available. We show that the standard means of ranking tabs, such as search engine ranks or user ratings, have little correlation with the accuracy of a tab and that a better ranking method is to use features such as the concurrency between tabs of the same song. We also compare the quality of top-ranked tabs with state-of-the-art chord transcription output and find that the latter provides a more reliable source of chord symbols with an accuracy rate 10% higher than the ranked hand annotations. © 2011 International Society for Music Information Retrieval."
Schuller B.; Weninger F.; Dorfner J.,Multi-modal non-prototypical music mood analysis in continuous space: Reliability and performances,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871398919&partnerID=40&md5=deea7b314eea3ec4b9cdd4879861efac,"Schuller B., Institute for Human-Machine Communication, Technische Universitä München, Germany; Weninger F., Institute for Human-Machine Communication, Technische Universitä München, Germany; Dorfner J., Institute for Energy Economy and Application Technology, Technische Universität München, Germany","Music Mood Classification is frequently turned into 'Music Mood Regression' by using a continuous dimensional model rather than discrete mood classes. In this paper we report on automatic analysis of performances in a mood space spanned by arousal and valence on the 2.6 k songs NTWICM corpus of popular UK chart music in full realism, i. e., by automatic web-based retrieval of lyrics and diverse acoustic features without pre-selection of prototypical cases. We discuss optimal modeling of the gold standard by introducing the evaluator weighted estimator principle, group-wise feature relevance, 'tuning' of the regressor, and compare early and late fusion strategies. In the result, correlation coefficients of .736 (valence) and .601 (arousal) are reached on previously unseen test data. © 2011 International Society for Music Information Retrieval."
Mathieu B.; Essid S.; Fillon T.; Prado J.; Richard G.,"Yaafe, an easy to use and efficient audio feature extraction software",2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597271&partnerID=40&md5=4a81ef44e84ea15c56ed6935e84afc2e,"Mathieu B., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Essid S., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Fillon T., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Prado J., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Richard G., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France","Music Information Retrieval systems are commonly built on a feature extraction stage. For applications involving automatic classification (e.g. speech/music discrimination, music genre or mood recognition, ...), traditional approaches will consider a large set of audio features to be extracted on a large dataset. In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efficient feature extraction. In this paper, a new audio feature extraction software, YAAFE 1 , is presented and compared to widely used libraries. The main advantage of YAAFE is a significantly lower complexity due to the appropriate exploitation of redundancy in the feature calculation. YAAFE remains easy to configure and each feature can be parameterized independently. Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License. © 2010 International Society for Music Information Retrieval."
McKay C.; Bainbridge D.,A musical web mining and audio feature extraction extension to the greenstone digital library software,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584527&partnerID=40&md5=174c2a40fec76b9972db77a9829af3de,"McKay C., CIRMMT, Marianopolis College, Montréal, Canada; Bainbridge D., University of Waikato, Hamilton, New Zealand","This paper describes updates to the Greenstone open source digital library software that significantly expand its functionality with respect to music. The first of the two major improvements now allows Greenstone to extract and store classification-oriented features from audio files using a newly updated version of the jAudio software. The second major improvement involves the implementation and integration of the new jSongMiner software, which provides Greenstone with a framework for automatically identifying audio recordings using audio fingerprinting and then extracting extensive metadata about them from a variety of resources available on the Internet. Several illustrative use cases and case studies are discussed. © 2011 International Society for Music Information Retrieval."
Mora J.; Gómez F.; Gómez E.; Escobar-Borrego F.; Díaz-Báñez J.M.,Characterization and melodic similarity of a cappella flamenco cantes,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592148&partnerID=40&md5=821f7f2ccd7cf03d7990b72a77036e5a,"Mora J., Department of Evolutive and Educational Psychology, University of Seville, Spain; Gómez F., Applied Mathematics Department, School of Computer Science, Polytechnic University of Madrid, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Escobar-Borrego F., Department of Audiovisual Communication, Publicity and Literature, University of Seville, Spain; Díaz-Báñez J.M., Department of Applied Mathematics II, University of Seville, Spain","This paper intends to research on the link between musical similarity and style and sub-style (variant) classification in the context of flamenco a cappella singing styles. Given the limitation of standard computational models for melodic characterization and similarity computation in this particular context, we have proposed a specific set of melodic features adapted to flamenco singing styles. In order to evaluate them, we have gathered a collection of music recordings from the most representative singers and have manually extracted those proposed features. Based on those features, we have defined a similarity measure between two performances and have validated their usefulness in differentiating several styles and variants. The main conclusion of this work is the need to incorporate specific musical features to the design of similarity measures for flamenco music so that flamencoadapted MIR systems can be developed. © 2010 International Society for Music Information Retrieval."
Bimbot F.; Deruty E.; Sargent G.; Vincent E.,Methodology and resources for the structural segmentation of music pieces into autonomous and comparable blocks,2011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576872&partnerID=40&md5=6d4e342413c9e1d915ab2c4aafebefb3,"Bimbot F., METISS - Speech and Audio Research Group, IRISA, CNRS - UMR 6074, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; Deruty E., METISS - Speech and Audio Research Group, INRIA, Rennes Bretagne Atlantique, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; Sargent G., METISS - Speech and Audio Research Group, Université de Rennes 1, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; Vincent E., METISS - Speech and Audio Research Group, INRIA, Rennes Bretagne Atlantique, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France","The approach called decomposition into autonomous and comparable blocks specifies a methodology for producing music structure annotation by human listeners based on a set of criteria relying on the listening experience of the human annotator [12]. The present article develops further a number of fundamental notions and practical issues, so as to facilitate the usability and the reproducibility of the approach. We formalize the general methodology as an iterative process which aims at estimating both a structural metric pattern and its realization, by searching empirically for an optimal compromise describing the organization of the content of the music piece in the most economical way, around a typical timescale. Based on experimental observations, we detail some practical considerations and we illustrate the method by an extensive case study. We introduce a set of 500 songs for which we are releasing freely the structural annotations to the research community, for examination, discussion and utilization. © 2011 International Society for Music Information Retrieval."
Schmidt E.M.; West K.; Kim Y.E.,Efficient acoustic feature extraction for music information retrieval using programmable gate arrays,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873673427&partnerID=40&md5=37c89946217f584c0a20c381ac1c346e,"Schmidt E.M., MET-lab, Drexel University, United States; West K., IMIRSEL, University of Illinois, United States; Kim Y.E., MET-lab, Drexel University, United States","Many of the recent advances in music information retrieval from audio signals have been data-driven, i.e., resulting from the analysis of very large data sets. Widespread performance evaluations on common data sets, such as the annual MIREX events, have also been instrumental in advancing the field. These endeavors incur a large computational cost, and could potentially benefit greatly from more rapid calculation of acoustic features. Traditional, clusterbased solutions for large-scale feature extraction are expensive and space- and power-inefficient. Using the massively parallel architecture of the field programmable gate array (FPGA), it is possible to design an application specific chip rivaling the speed of a cluster for large-scale acoustic feature computation at lower cost. Recent advances in development tools, such as the Xilinx Blockset in Simulink, allow rapid prototyping, simulation, and implementation on actual hardware. Such devices also show potential for the implementation of MIR systems on embedded devices such as cell phones and PDAs where hardware acceleration would be an absolute necessity. We present a prototype library for acoustic feature calculation for implementation on Xilinx FPGA hardware. Furthermore, using a genre classification task we compare the performance of simulated hardware features to those computed using standard methods, demonstrating a nearly negligible drop in classification performance with the potential for large reductions in computation time. © 2009 International Society for Music Information Retrieval."
Jeon W.; Ma C.; Cheng Y.M.,An efficient signal-matching approach to melody indexing and search using continuous pitch contours andwavelets,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873692182&partnerID=40&md5=5bb9e85912dfe93bacf66081304caedb,"Jeon W., Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States; Ma C., Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States; Cheng Y.M., Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States","We describe a method of indexing and efficiently searching music melodies based on their continuous dominant fundamental frequency (f0) contours without obtaining notelevel transcriptions. Each f0 contour is encoded by a redundant set of wavelet coefficients that represent its shape in level-normalized form at various locations and time scales. This allows a query melody to be exhaustively compared with variable-length portions of a target melody at arbitrary locations while accounting for differences in key and tempo. Themethod is applied in a Query-by-Humming (QBH) systemwhere usersmay search a database of recorded pop songs by humming or singing an arbitrary part of the melody of an intended song. The system has fast retrieval times because the wavelet coefficients can be effectively indexed in a binary tree and a vector distance measure instead of dynamic programming is used for comparisons. Using automatic pitch extraction to obtain all f0 contours from acoustic data, the method demonstrates practical performance in an experiment with an existing monophonic data set and in a preliminary experiment with real-world polyphonic music. © 2009 International Society for Music Information Retrieval."
Lidy T.; Mayer R.; Rauber A.; Ponce De León P.J.; Pertusa A.; Iñesta J.M.,A cartesian ensemble of feature subspace classifiers for music categorization,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586694&partnerID=40&md5=822dcd6825fe1e8a78eae3f5c6d48d9a,"Lidy T., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Mayer R., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Ponce De León P.J., Departamento de Lenguajes, Sistemas Informaticos, University of Alicante, Spain; Pertusa A., Departamento de Lenguajes, Sistemas Informaticos, University of Alicante, Spain; Iñesta J.M., Departamento de Lenguajes, Sistemas Informaticos, University of Alicante, Spain","We present a cartesian ensemble classification system that is based on the principle of late fusion and feature subspaces. These feature subspaces describe different aspects of the same data set. The framework is built on the Weka machine learning toolkit and able to combine arbitrary feature sets and learning schemes. In our scenario, we use it for the ensemble classification of multiple feature sets from the audio and symbolic domains. We present an extensive set of experiments in the context of music genre classification, based on numerous Music IR benchmark datasets, and evaluate a set of combination/voting rules. The results show that the approach is superior to the best choice of a single algorithm on a single feature set. Moreover, it also releases the user from making this choice explicitly. © 2010 International Society for Music Information Retrieval."
McKay C.; Burgoyne J.A.; Hockman J.; Smith J.B.L.; Vigliensoni G.; Fujinaga I.,"Evaluating the genre classification performance of lyrical features relative to audio, symbolic and cultural features",2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866621013&partnerID=40&md5=f6a76e110f560060ef97a6d77afcd175,"McKay C., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Hockman J., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Smith J.B.L., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","This paper describes experimental research investigating the genre classification utility of combining features extracted from lyrical, audio, symbolic and cultural sources of musical information. It was found that cultural features consisting of information extracted from both web searches and mined listener tags were particularly effective, with the result that classification accuracies were achieved that compare favorably with the current state of the art of musical genre classification. It was also found that features extracted from lyrics were less effective than the other feature types. Finally, it was found that, with some exceptions, combining feature types does improve classification performance. The new lyricFetcher and jLyrics software are also presented as tools that can be used as a framework for developing more effective classification methodologies based on lyrics in the future. © 2010 International Society for Music Information Retrieval."
Joder C.; Essid S.; Richard G.,An improved hierarchical approach for music-to-symbolic score alignment,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051648102&partnerID=40&md5=be6ed89cf85e07f477e74261bb1dcf77,"Joder C., CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France; Essid S., CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France; Richard G., CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France","We present an efficient approach for an off-line alignment of a symbolic score to a recording of the same piece, using a statistical model. A hidden state model is built from the score, which allows for the use of two different kinds of features, namely chroma vectors and an onset detection function (spectral flux) with specific production models, in a simple manner. We propose a hierarchical pruning method for an approximate decoding of this statistical model. This strategy reduces the search space in an adaptive way, yielding a better overall efficiency than the tested state-of-the art method. Experiments run on a large database of 94 pop songs show that the resulting system obtains higher recognition rates than the dynamic programming algorithm (DTW), with a significantly lower complexity, even though the rhythmic information is not used for the alignment. © 2010 International Society for Music Information Retrieval."
Marsden A.,Recognition of variations using automatic schenkerian reduction,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578828&partnerID=40&md5=01d04ec49c14a8bf6289e6a7f9b1642e,"Marsden A., Lancaster Institute for Contemporary Arts, Lancaster University, United Kingdom","Experiments on techniques to automatically recognise whether or not an extract of music is a variation of a given theme are reported, using a test corpus derived from ten of Mozart's sets of variations for piano. Methods which examine the notes of the surface are compared with methods which make use of an automatically derived quasi-Schenkerian reduction of the theme and the extract in question. The maximum average F-measure achieved was 0.87. Unexpectedly, this was for a method of matching based on the surface alone, and in general the results for matches based on the surface were marginally better than those based on reduction, though the small number of possible test queries means that this result cannot be regarded as conclusive. Other inferences on which factors seem to be important in recognising variations are discussed. Possibilities for improved recognition of matching using reduction are outlined. © 2010 International Society for Music Information Retrieval."
Angeles B.; McKay C.; Fujinaga I.,Discovering metadata inconsistencies,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574983&partnerID=40&md5=27c24566ce5a2817677015a1fb36ae97,"Angeles B., CIRMMT, Schulich School of Music, McGill University, Canada; McKay C., CIRMMT, Schulich School of Music, McGill University, Canada; Fujinaga I., CIRMMT, Schulich School of Music, McGill University, Canada","This paper describes the use of fingerprinting-based querying in identifying metadata inconsistencies in music libraries, as well as the updates to the jMusicMeta- Manager software in order to perform the analysis. Test results are presented for both the Codaich database and a generic library of unprocessed metadata. Statistics were computed in order to evaluate the differences between a manually-maintained library and an unprocessed collection when comparing metadata with values on a MusicBrainz server queried by fingerprinting. © 2010 International Society for Music Information Retrieval."
Grindlay G.; Ellis D.P.W.,A probabilistic subspace model for multi-instrument polyphonic transcription,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053048831&partnerID=40&md5=67346c8dee814844d37d871af5e08382,"Grindlay G., Dept. of Electrical Engineering, LabROSA, Columbia University, United States; Ellis D.P.W., Dept. of Electrical Engineering, LabROSA, Columbia University, United States","In this paper we present a general probabilistic model suitable for transcribing single-channel audio recordings containing multiple polyphonic sources. Our system requires no prior knowledge of the instruments in the mixture, although it can benefit from this information if available. In contrast to many existing polyphonic transcription systems, our approach explicitly models the individual instruments and is thereby able to assign detected notes to their respective sources. We use a set of training instruments to learn a model space which is then used during transcription to constrain the properties of models fit to the target mixture. In addition, we encourage model sparsity using a simple approach related to tempering. We evaluate our method on both recorded and synthesized two-instrument mixtures, obtaining average framelevel F-measures of up to 0.60 for synthesized audio and 0.53 for recorded audio. If knowledge of the instrument types in the mixture is available, we can increase these measures to 0.68 and 0.58, respectively, by initializing the model with parameters from similar instruments. © 2010 International Society for Music Information Retrieval."
Gkiokas A.; Katsouros V.; Carayannis G.,Tempo induction using filterbank analysis and tonal features,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585927&partnerID=40&md5=3058b38982dc942fe287b8407c6f2fb4,"Gkiokas A., Institute for Language and Speech Processing, Greece, National Technical University of Athens, Greece; Katsouros V., Institute for Language and Speech Processing, Greece; Carayannis G., National Technical University of Athens, Greece","This paper presents an algorithm that extracts the tempo of a musical excerpt. The proposed system assumes a constant tempo and deals directly with the audio signal. A sliding window is applied to the signal and two feature classes are extracted. The first class is the log-energy of each band of a mel-scale triangular filterbank, a common feature vector used in various MIR applications. For the second class, a novel feature for the tempo induction task is presented; the strengths of the twelve western musical tones at all octaves are calculated for each audio frame, in a similar fashion with Pitch Class Profile. The timeevolving feature vectors are convolved with a bank of resonators, each resonator corresponding to a target tempo. Then the results of each feature class are combined to give the final output. The algorithm was evaluated on the popular ISMIR 2004 Tempo Induction Evaluation Exchange Dataset. Results demonstrate that the superposition of the different types of features enhance the performance of the algorithm, which is in the current state-of-the-art algorithms of the tempo induction task. © 2010 International Society for Music Information Retrieval."
Bimbot F.; Le Blouch O.; Sargent G.; Vincent E.,Decomposition into autonomous and comparable blocks : A structural description of music pieces,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591001&partnerID=40&md5=e00978b5b29ee918532a9e78ad7875e8,"Bimbot F., IRISA, CNRS - UMR 6074, Campus Universitaire de Beaulieu, 35042 Rennes cedex, France; Le Blouch O., INRIA, Campus Universitaire de Beaulieu, 35042 Rennes cedex, Rennes Bretagne Atlantique, France; Sargent G., Campus Universitaire de Beaulieu, Université de Rennes 1, 35042 Rennes cedex, France; Vincent E., INRIA, Campus Universitaire de Beaulieu, 35042 Rennes cedex, Rennes Bretagne Atlantique, France","The structure of a music piece is a concept which is often referred to in various areas of music sciences and technologies, but for which there is no commonly agreed definition. This raises a methodological issue in MIR, when designing and evaluating automatic structure inference algorithms. It also strongly limits the possibility to produce consistent large-scale annotation datasets in a cooperative manner. This article proposes an approach called decomposition into autonomous and comparable blocks, based on principles inspired from structuralism and generativism. It specifies a methodology for producing music structure annotation by human listeners based on simple criteria and resorting solely to the listening experience of the annotator. We show on a development set that the proposed approach can provide a reasonable level of concordance across annotators and we introduce a set of annotations on the RWC database, intended to be released to the MIR community. © 2010 International Society for Music Information Retrieval."
Oliveira J.L.; Gouyon F.; Martins L.G.; Reis L.P.,IBT: A real-time tempo and beat tracking system,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864672836&partnerID=40&md5=cd83552150c924270d44225c22c4f512,"Oliveira J.L., Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal, Artificial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal; Gouyon F., Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal; Martins L.G., Research Center for Science and Technology in Art (CITAR), UCP, Porto, Portugal; Reis L.P., Artificial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal","This paper describes a tempo induction and beat tracking system based on the efficient strategy (initially introduced in the BeatRoot system [Dixon S., ""Automatic extraction of tempo and beat from expressive performances."" Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main reasons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consideration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform). The system is implemented in C++, permitting faster than real-time processing of audio data. It is integrated in the MARSYAS framework, and is therefore available under GPL for users and/or researchers. Detailed evaluation of the causal and non-causal versions of the system on common benchmark datasets show performances reaching those of state-of-the-art beat trackers. We propose a series of lines for future work based on careful analysis of the results. © 2010 International Society for Music Information Retrieval."
Hamel P.; Eck D.,Learning features from music audio with deep belief networks,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584268&partnerID=40&md5=ff934d48d288a327dcd225176cf7e171,"Hamel P., DIRO, CIRMMT, Université de Montréal, Canada; Eck D., DIRO, CIRMMT, Université de Montréal, Canada","Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically extract relevant features from audio for a given task. The feature extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the audio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. In particular, we learned the features to solve the task of genre recognition. The learned features perform significantly better than MFCCs. Moreover, we obtain a classification accuracy of 84.3% on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classifiers using frame-based features. We also applied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features. © 2010 International Society for Music Information Retrieval."
Crawford T.; Mauch M.; Rhodes C.,Recognizing classical works in historical recordings,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955383518&partnerID=40&md5=02f31eee9cdfcee462b511bf12b3736c,"Crawford T., Centre for Cognition, Computation and Culture, Goldsmiths, University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Rhodes C., Department of Computing, Goldsmiths, University of London, United Kingdom","In collections of recordings of classical music, it is normal to find multiple performances, usually by different artists, of the same pieces of music. While there may be differences in many dimensions of musical similarity, such as timbre, pitch or structural detail, the underlying musical content is essentially and recognizably the same. The degree of divergence is generally less than that found between 'cover songs' in the domain of popular music, and much less than in typical performances of jazz standards. MIR methods, based around variants of the chroma representation, can be useful in tasks such as work identification especially where disco/bibliographical metadata is absent or incomplete as well as for access, curation and management of collections. We describe some initial experiments in work-recognition on a test-collection comprising c. 2000 digital transfers of historical recordings, and show that the use of NNLS chroma, a new, musically-informed chroma feature, dramatically improves recognition. © 2010 International Society for Music Information Retrieval."
Hoashi K.; Hamawaki S.; Ishizaki H.; Takishima Y.; Katto J.,Usability evaluation of visualization interfaces for content-based music retrieval systems,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873672344&partnerID=40&md5=6c69bba389a9514c114667a0d077041b,"Hoashi K., KDDI R and D Laboratories Inc., Japan; Hamawaki S., Graduate School of Science and Engineering, Waseda University, Japan; Ishizaki H., KDDI R and D Laboratories Inc., Japan; Takishima Y., KDDI R and D Laboratories Inc., Japan; Katto J., Graduate School of Science and Engineering, Waseda University, Japan","This research presents a formal user evaluation of a typical visualization method for content-based music information retrieval (MIR) systems, and also proposes a novel interface to improve MIR usability. Numerous interfaces to visualize content-based MIR systems have been proposed, but reports on user evaluations of such proposed GUIs are scarce. This research aims to evaluate the effectiveness of a typical 2-D visualization method for content-based MIR systems, by conducting comparative user evaluations against the traditional list-based format to present MIR results to the user. Based on the observations of the experimental results, we next propose a 3-D visualization system, which features a function to specify sub-regions of the feature space based on genre classification results, and a function which allows users to select features that are assigned to the axes of the 3-D space. Evaluation of this GUI conclude that the functions of the 3-D system can significantly improve both the efficiency and usability of MIR systems. © 2009 International Society for Music Information Retrieval."
Ferrer R.; Eerola T.,Timbral qualities of semantic structures of music,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583216&partnerID=40&md5=4b3f5131a053ac6eed730f8300c0c2fe,"Ferrer R., Finnish Centre of Excellence in Interdisciplinary Music Research, Finland; Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, Finland","The rapid expansion of social media in music has provided the field with impressive datasets that offer insights into the semantic structures underlying everyday uses and classification of music. We hypothesize that the organization of these structures are rather directly linked with the ""qualia"" of the music as sound. To explore the ways in which these structures are connected with the qualities of sounds, a semantic space was extracted from a large collection of musical tags with latent semantic and cluster analysis. The perceptual and musical properties of 19 clusters were investigated by a similarity rating task that used spliced musical excerpts representing each cluster. The resulting perceptual space denoting the clusters correlated high with selected acoustical features extracted from the stimuli. The first dimension related to the high-frequency energy content, the second to the regularity of the spectrum, and the third to the fluctuations within the spectrum. These findings imply that meaningful organization of music may be derived from low-level descriptions of the excerpts. Novel links with the functions of music embedded into the tagging information included within the social media are proposed. © 2010 International Society for Music Information Retrieval."
Paulus J.,Improving markov model-based music piece structure labelling with acoustic information,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583694&partnerID=40&md5=bd2e218276e70ecba6403abcbdfe905f,"Paulus J., Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany","This paper proposes using acoustic information in the labelling of music piece structure descriptions. Here, music piece structure means the sectional form of the piece: temporal segmentation and grouping to parts such as chorus or verse. The structure analysis methods rarely provide the parts with musically meaningful names. The proposed method labels the parts in a description. The baseline method models the sequential dependencies between musical parts with N-grams and uses themfor the labelling. The acoustic model proposed in this paper is based on the assumption that the parts with the same label even in different pieces share some acoustic properties compared to other parts in the same pieces. The proposed method uses mean and standard deviation of relative loudness in a part as the feature which is then modelled with a single multivariate Gaussian distribution. The method is evaluated on three data sets of popular music pieces, and in all of them the inclusion of the acoustic model improves the labelling accuracy over the baseline method. © 2010 International Society for Music Information Retrieval."
Miotto R.; Orio N.,A probabilistic approach to merge context and content information for music retrieval,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857180763&partnerID=40&md5=fc69ea25948f0ad9bdde99529722f052,"Miotto R., University of Padova, Italy; Orio N., University of Padova, Italy","An interesting problem in music information retrieval is how to combine the information from different sources in order to improve retrieval effectiveness. This paper introduces an approach to represent a collection of tagged songs through an hidden Markov model with the purpose to develop a system that merges in the same framework both acoustic similarity and semantic descriptions. The former provides content-based information on song similarity, the latter provides context-aware information about individual songs. Experimental results show how the proposed model leads to better performances than approaches that rank songs using both a single information source and a their linear combination. © 2010 International Society for Music Information Retrieval."
Hirjee H.; Brown D.G.,Solving misheard lyric search queries using a probabilistic model of speech sounds,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872866272&partnerID=40&md5=75d954cfb2f31fbc185ba1cedf0409b3,"Hirjee H., Cheriton School of Computer Science, University of Waterloo, Canada; Brown D.G., Cheriton School of Computer Science, University of Waterloo, Canada","Music listeners often mishear the lyrics to unfamiliar songs heard from public sources, such as the radio. Since standard text search engines will find few relevant results when they are entered as a query, these misheard lyrics require phonetic pattern matching techniques to identify the song. We introduce a probabilistic model of mishear- ing trained on examples of actual misheard lyrics, and develop a phoneme similarity scoring matrix based on this model. We compare this scoring method to simpler pattern matching algorithms on the task of finding the correct lyric from a collection given a misheard query. The probabilistic method significantly outperforms all other methods, finding 5-8% more correct lyrics within the first five hits than the previous best method. © 2010 International Society for Music Information Retrieval."
Rump H.; Miyabe S.; Tsunoo E.; Ono N.; Sagama S.,Autoregressive MFCC models for genre classification improved by harmonic-percussion separation,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053014937&partnerID=40&md5=ad75c3ea1bdfec126bae1b0e8cae16c7,"Rump H., School of Information Science and Technology, University of Tokyo, Japan; Miyabe S., School of Information Science and Technology, University of Tokyo, Japan; Tsunoo E., School of Information Science and Technology, University of Tokyo, Japan; Ono N., School of Information Science and Technology, University of Tokyo, Japan; Sagama S., School of Information Science and Technology, University of Tokyo, Japan","In this work we improve accuracy of MFCC-based genre classification by using the Harmonic-Percussion Signal Separation (HPSS) algorithm on the music signal, and then calculate the MFCCs on the separated signals. The choice of the HPSS algorithm was mainly based on the observation that the presence of harmonics causes the high MFCCs to be noisy. A multivariate autoregressive (MAR) model was trained on the improved MFCCs, and performance in the task of genre classification was evaluated. By combining features calculated on the separated signals, relative error rate reductions of 20% and 16.2% were obtained when an SVM classifier was trained on the MFCCs and MAR features respectively. Next, by analyzing the MAR features calculated on the separated signals, it was concluded that the original signal contained some information which the MAR model was capable of handling, and that the best performance was obtained when all three signals were used. Finally, by choosing the number of MFCCs from each signal type to be used in the autoregressive modelling, it was verified that the best performance was reached when the high MFCCs calculated on the harmonic signal were discarded. © 2010 International Society for Music Information Retrieval."
Laplante A.,Users' relevance criteria in music retrieval in everyday life: An exploratory study,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447381&partnerID=40&md5=3487fec29409d25660f773db9c591a0b,"Laplante A., ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Montréal, QC, Canada","The paper presents the findings of a qualitative study on the way young adults make relevance inferences about music items when searching for music for recreational purposes. Data were collected through in-depth interviews and analyzed following the constant comparative method. Content analysis revealed that participants used four types of clues to make relevance inferences: bibliographic metadata (e.g., names of contributors, labels), relational metadata (e.g., genres, similar artists), associative metadata (e.g., cover arts), and recommendations/reviews. Relevance judgments were also found to be influenced by the external context (i.e., the functions music plays in one's life) and the internal context (i.e., individual tastes and beliefs, state of mind). © 2010 International Society for Music Information Retrieval."
Yoshii K.; Goto M.,Infinite latent harmonic allocation: A nonparametric bayesian approach to multipitch analysis,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586598&partnerID=40&md5=c43105f4270d0f03ba115f359f27170f,"Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a statistical method called Infinite Latent Harmonic Allocation (iLHA) for detecting multiple fundamental frequencies in polyphonic audio signals. Conventional methods face a crucial problem known as model selection because they assume that the observed spectra are superpositions of a certain fixed number of bases (sound sources and/or finer parts). iLHA avoids this problem by assuming that the observed spectra are superpositions of a stochastically-distributed unbounded (theoretically infinite) number of bases. Such uncertainty can be treated in a principled way by leveraging the state-of-the-art paradigm of machine-learning called Bayesian nonparametrics. To represent a set of time-sliced spectral strips, we formulated nested infinite Gaussian mixture models (GMMs) based on hierarchical and generalized Dirichlet processes. Each strip is allowed to contain an unbounded number of sound sources (GMMs), each of which is allowed to contain an unbounded number of harmonic partials (Gaussians). To train the nested infinite GMMs efficiently, we used a modern inference technique called collapsed variational Bayes (CVB). Our experiments using audio recordings of real piano and guitar performances showed that fully automated iLHA based on noninformative priors performed as well as optimally tuned conventional methods. © 2010 International Society for Music Information Retrieval."
Hsu C.-L.; Jang J.-S.R.,Singing pitch extraction by voice vibrato/tremolo estimation and instrument partial deletion,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051657795&partnerID=40&md5=b596c7118468c2598983c786163ec30f,"Hsu C.-L., Computer Science Department, Multimedia Information Retrieval Laboratory, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Computer Science Department, Multimedia Information Retrieval Laboratory, National Tsing Hua University, Hsinchu, Taiwan","This paper proposes a novel and effective approach to extract the pitches of the singing voice from monaural polyphonic songs. The sinusoidal partials of the musical audio signals are first extracted. The Fourier transform is then applied to extract the vibrato/tremolo information of each partial. Some criteria based on this vibrato/tremolo information are employed to discriminate the vocal partials from the music accompaniment partials. Besides, a singing pitch trend estimation algorithm which is able to find the global singing progressing tunnel is also proposed. The singing pitches can then be extracted more robustly via these two processes. Quantitative evaluation shows that the proposed algorithms significantly improve the raw pitch accuracy of our previous approach and are comparable with other state of the art approaches submitted to MIREX. © 2010 International Society for Music Information Retrieval."
Maezawa A.; Goto M.; Okuno H.G.,Query-by-conducting: An interface to retrieve classical-music interpretations by real-time tempo input,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051631241&partnerID=40&md5=a81b6f4163f6ca956967cc921ceb9a89,"Maezawa A., Dept. of Intelligence Science and Technology, School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper presents an interface for finding interpretations of a user-specified music, Query-by-Conducting. In classical music, there are many interpretations to a particular piece, and finding ""the"" interpretation that matches the listener's taste allows a listener to further enjoy the piece. The critical issue in finding such an interpretation is the way or interface to allow the listener to listen through different interpretations. Our interface allows a user, by swinging a conducting hardware interface, to conduct the desired global tempo along the playback of a piece, at any time in the piece. The real-time conducting input by the user dynamically switches the interpretation being played back to the one closest to how the user is currently conducting. At the end of the piece, our interface ranks each interpretation according to how close the tempo of each interpretation was to the user input. At the core of our interface is an automated tempo estimation method based on audio-score alignment. We improve tempo estimation by requiring the audio-score alignment of different interpretations to be consistent with each other. We evaluate the tempo estimation method using a solo, chamber, and orchestral repertoire. The proposed tempo estimation decreases the error by as much as 0.94 times the original error. © 2010 International Society for Music Information Retrieval."
Bosteels K.; Pampalk E.; Kerre E.E.,Evaluating and analysing dynamic playlist generation heuristics using radio logs and fuzzy set theory,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873695311&partnerID=40&md5=0294ae969febee49afaa94d073db7f2d,"Bosteels K., Ghent University, Gent, Belgium; Pampalk E., Last.fm Ltd., London, United Kingdom; Kerre E.E., Ghent University, Gent, Belgium","In this paper, we analyse and evaluate several heuristics for adding songs to a dynamically generated playlist. We explain how radio logs can be used for evaluating such heuristics, and show that formalizing the heuristics using fuzzy set theory simplifies the analysis. More concretely, we verify previous results by means of a large scale evaluation based on 1.26 million listening patterns extracted from radio logs, and explain why some heuristics perform better than others by analysing their formal definitions and conducting additional evaluations. © 2009 International Society for Music Information Retrieval."
Izmirli Ö.; Dannenberg R.B.,Understanding features and distance functions for music sequence alignment,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573319&partnerID=40&md5=a026c7434a3ea02674582a9d71cc8a12,"Izmirli Ö., Computer Science Department, Center for Arts and Technology, Connecticut College, United States; Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States","We investigate the problem of matching symbolic representations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representations that optimize the classification of ""matching"" vs. ""non-matching"" frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representation but they also reveal interesting projection structures that differ distinctly from the traditional chromagram. © 2010 International Society for Music Information Retrieval."
Honingh A.; Bod R.,Pitch class set categories as analysis tools for degrees of tonality,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573958&partnerID=40&md5=5ca7d6af3998920fe156c7054557843e,"Honingh A., Institute for Logic, Language and Computation, University of Amsterdam, Netherlands; Bod R., Institute for Logic, Language and Computation, University of Amsterdam, Netherlands","This is an explorative paper in which we present a new method for music analysis based on pitch class set categories. It has been shown before that pitch class sets can be divided into six different categories. Each category inherits a typical character which can ""tell"" something about the music in which it appears. In this paper we explore the possibilities of using pitch class set categories for 1) classification in major/minor mode, 2) classification in tonal/atonal music, 3) determination of a degree of tonality, and 4) determination of a composer's period. © 2010 International Society for Music Information Retrieval."
Collins T.; Thurlow J.; Laney R.; Willis A.; Garthwaite P.H.,A comparative evaluation of algorithms for discovering translational patterns in baroque keyboard works,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954502326&partnerID=40&md5=7be4977588e0d5e67751f0963c686fca,"Collins T., Open University, United Kingdom; Thurlow J., University of Cambridge, United Kingdom; Laney R., Open University, United Kingdom; Willis A., Open University, United Kingdom; Garthwaite P.H., Open University, United Kingdom","We consider the problem of intra-opus pattern discovery, that is, the task of discovering patterns of a specified type within a piece of music. A music analyst undertook this task for works by Domenico Scarlattti and Johann Sebastian Bach, forming a benchmark of 'target' patterns. The performance of two existing algorithms and one of our own creation, called SIACT, is evaluated by comparison with this benchmark. SIACT out-performs the existing algorithms with regard to recall and, more often than not, precision. It is demonstrated that in all but the most carefully selected excerpts of music, the two existing algorithms can be affected by what is termed the 'problem of isolated membership'. Central to the relative success of SIACT is our intention that it should address this particular problem. The paper contrasts string-based and geometric approaches to pattern discovery, with an introduction to the latter. Suggestions for future work are given. © 2010 International Society for Music Information Retrieval."
Kaiser F.; Sikora T.,Music structure discovery in popular music using non-negative matrix factorization,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581905&partnerID=40&md5=94391ce2dd7e936a7b516cbcda12ee8b,"Kaiser F., Communication Systems Group, Technische Universiẗ, Berlin, Germany; Sikora T., Communication Systems Group, Technische Universiẗ, Berlin, Germany","We introduce a method for the automatic extraction of musical structures in popular music. The proposed algorithm uses non-negative matrix factorization to segment regions of acoustically similar frames in a self-similarity matrix of the audio data. We show that over the dimensions of the NMF decomposition, structural parts can easily be modeled. Based on that observation, we introduce a clustering algorithm that can explain the structure of the whole music piece. The preliminary evaluation we report in the the paper shows very encouraging results. © 2010 International Society for Music Information Retrieval."
Weiss R.J.; Bello J.P.,Identifying repeated patterns in music using sparse convolutive non-negative matrix factorization,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573953&partnerID=40&md5=48e8f7d80e4b7269b96d38cd6eac325d,"Weiss R.J., Music and Audio Research Lab (MARL), New York University, United States; Bello J.P., Music and Audio Research Lab (MARL), New York University, United States","We describe an unsupervised, data-driven, method for automatically identifying repeated patterns in music by analyzing a feature matrix using a variant of sparse convolutive non-negative matrix factorization. We utilize sparsity constraints to automatically identify the number of patterns and their lengths, parameters that would normally need to be fixed in advance. The proposed analysis is applied to beatsynchronous chromagrams in order to concurrently extract repeated harmonic motifs and their locations within a song. Finally, we show how this analysis can be used for longterm structure segmentation, resulting in an algorithm that is competitive with other state-of-the-art segmentation algorithms based on hidden Markov models and self similarity matrices. © 2010 International Society for Music Information Retrieval."
Murao K.; Nakano M.; Kitano Y.; Ono N.; Sagayama S.,Monophonic instrument sound segregation by clustering NMF components based on basis similarity and gain disjointness,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052993673&partnerID=40&md5=77bbf30bf4c2a03d3e8a2b73d87d6ec9,"Murao K., School of Information Science and Technology, University of Tokyo, Japan; Nakano M., School of Information Science and Technology, University of Tokyo, Japan; Kitano Y., School of Information Science and Technology, University of Tokyo, Japan; Ono N., School of Information Science and Technology, University of Tokyo, Japan; Sagayama S., School of Information Science and Technology, University of Tokyo, Japan","This paper discusses a method for monophonic instrument sound separation based on nonnegative matrix factorization (NMF). In general, it is not easy to classify NMF components into each instrument. By contrast, monophonic instrument sound gives us an important clue to classify them, because no more than one sound would be activated simultaneously. Our approach is to classify NMF components into each instrument based on basis spectrum vector similarity and temporal activity disjointness. Our clustering employs a hierarchical clustering algorithm: group average method (GAM). The efficiency of our approach is evaluated by some experiments. © 2010 International Society for Music Information Retrieval."
Fremerey C.; Müller M.; Clausen M.,Handling repeats and jumps in score-performance synchronization,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960505828&partnerID=40&md5=fdc8c63fe77e80158d73a1f9cceb4a6d,"Fremerey C., Department of Computer Science, Bonn University, Bonn, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Clausen M., Department of Computer Science, Bonn University, Bonn, Germany","Given a score representation and a recorded performance of the same piece of music, the task of score-performance synchronization is to temporally align musical sections such as bars specified by the score to temporal sections in the performance. Most of the previous approaches assume that the score and the performance to be synchronized globally agree with regard to the overall musical structure. In practice, however, this assumption is often violated. For example, a performer may deviate from the score by ignoring a repeat or introducing an additional repeat that is not written in the score. In this paper, we introduce a synchronization approach that can cope with such structural differences. As main technical contribution, we describe a novel variant of dynamic time warping (DTW), referred to as JumpDTW, which allows for handling jumps and repeats in the alignment. Our approach is evaluated for the practically relevant case of synchronizing score data obtained from scanned sheet music via optical music recognition to corresponding audio recordings. Our experiments based on Beethoven piano sonatas show that JumpDTW can robustly identify and handle most of the occurring jumps and repeats leading to an overall alignment accuracy of over 99% on the bar-level. © 2010 International Society for Music Information Retrieval."
Kim Y.E.; Schmidt E.M.; Migneco R.; Morton B.G.; Richardson P.; Scott J.; Speck J.A.; Turnbull D.,Music emotion recognition: A state of the art review,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591302&partnerID=40&md5=4cb6e1ed89144c53f9c1ae8400db638f,"Kim Y.E., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Schmidt E.M., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Migneco R., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Morton B.G., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Richardson P., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Scott J., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Speck J.A., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Turnbull D., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States","This paper surveys the state of the art in automatic emotion recognition in music. Music is oftentimes referred to as a ""language of emotion"" [1], and it is natural for us to categorize music in terms of its emotional associations. Myriad features, such as harmony, timbre, interpretation, and lyrics affect emotion, and the mood of a piece may also change over its duration. But in developing automated systems to organize music in terms of emotional content, we are faced with a problem that oftentimes lacks a welldefined answer; there may be considerable disagreement regarding the perception and interpretation of the emotions of a song or ambiguity within the piece itself. When compared to other music information retrieval tasks (e.g., genre identification), the identification of musical mood is still in its early stages, though it has received increasing attention in recent years. In this paper we explore a wide range of research in music emotion recognition, particularly focusing on methods that use contextual text information (e.g., websites, tags, and lyrics) and content-based approaches, as well as systems combining multiple feature domains. © 2010 International Society for Music Information Retrieval."
Burgoyne J.A.; Devaney J.; Ouyang Y.; Pugin L.; Himmelman T.; Fujinaga I.,Lyric extraction and recognition on digital images of early music sources,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873686672&partnerID=40&md5=5ed4ee38eadb54da684a3853b5855c53,"Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Devaney J., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Ouyang Y., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Himmelman T., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada","Optical music recognition (OMR) is one of the most promising tools for generating large-scale, distributable libraries of musical data. Much OMR work has focussed on instrumental music, avoiding a special challenge vocal music poses for OMR: lyric recognition. Lyrics complicate the page layout, making it more difficult to identify the regions of the page that carry musical notation. Furthermore, users expect a complete OMR process for vocal music to include recognition of the lyrics, reunification of syllables when they have been separated, and alignment of these lyrics with the recognised music. Unusual layouts and inconsistent practises for syllabification, however, make lyric recognition more challenging than traditional optical character recognition (OCR). This paper surveys historical approaches to lyric recognition, outlines open challenges, and presents a new approach to extracting text lines in medieval manuscripts, one of the frontiers of OMR research today. © 2009 International Society for Music Information Retrieval."
Panagakis Y.; Kotropoulos C.; Arce G.R.,Sparse multi-label linear embedding within nonnegative tensor factorization applied to music tagging,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578446&partnerID=40&md5=5be9d56d821d5970668bfe8992c500ea,"Panagakis Y., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Kotropoulos C., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Arce G.R., Dept. of Electrical and Computer Engineering, University of Delaware, Newark, DE 19716-3130, United States","A novel framework for music tagging is proposed. First, each music recording is represented by bio-inspired auditory temporal modulations. Then, a multilinear subspace learning algorithm based on sparse label coding is developed to effectively harness the multi-label information for dimensionality reduction. The proposed algorithm is referred to as Sparse Multi-label Linear Embedding Non- negative Tensor Factorization, whose convergence to a stationary point is guaranteed. Finally, a recently proposed method is employed to propagate the multiple labels of training auditory temporal modulations to auditory temporal modulations extracted from a test music recording by means of the sparse ℓ1 reconstruction coefficients. The overall framework, that is described here, outperforms both humans and state-of-the-art computer audition systems in the music tagging task, when applied to the CAL500 dataset. © 2010 International Society for Music Information Retrieval."
Tjoa S.K.; Ray Liu K.J.,Musical instrument recognition using biologically inspired filtering of temporal dictionary atoms,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588006&partnerID=40&md5=840bcb81b55d2e1df21b7b75c791b48b,"Tjoa S.K., Department of Electrical and Computer Engineering, University of Maryland, College Park, MD 20742, United States; Ray Liu K.J., Department of Electrical and Computer Engineering, University of Maryland, College Park, MD 20742, United States","Most musical instrument recognition systems rely entirely upon spectral information instead of temporal information. In this paper, we test the hypothesis that temporal information can improve upon the accuracy achievable by the state of the art in instrument recognition. Unlike existing temporal classification methods which use traditional features such as temporal moments, we extract novel features from temporal atoms generated by nonnegative matrix factorization by using a multiresolution gamma filterbank. Among isolated sounds taken from twenty-four instrument classes, the proposed system can achieve 92.3% accuracy, thus improving upon the state of the art. © 2010 International Society for Music Information Retrieval."
Hockman J.A.; Fujinaga I.,Fast vs slow: Learning tempo octaves from user data,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871379653&partnerID=40&md5=b6ba5a1b58213f6e39f8e5510c940d1a,"Hockman J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada","The widespread use of beat- and tempo-tracking methods in music information retrieval tasks has been marginalized due to undesirable sporadic results from these algorithms. While sensorimotor and listening studies have demonstrated the subjectivity and variability inherent to human performance of this task, MIR applications such as recommendation require more reliable output than available from present tempo estimation models. In this paper, we present a initial investigation of tempo assessment based on the simple classification of whether the music is fast or slow. Through three experiments, we provide performance results of our method across two datasets, and demonstrate its usefulness in the pursuit of a reliable global tempo estimation. © 2010 International Society for Music Information Retrieval."
Eyben F.; Böck S.; Schuller B.; Graves A.,Universal onset detection with bidirectional long short-term memory neural networks,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863771261&partnerID=40&md5=07fb54d73b1e6a588270ae8c99956535,"Eyben F., Institute for Human-Machine Communication, Technische Universität München, Germany; Böck S., Institute for Human-Machine Communication, Technische Universität München, Germany; Schuller B., Institute for Human-Machine Communication, Technische Universität München, Germany; Graves A., Institute for Computer Science VI, Technische Universität München, Germany","Many different onset detection methods have been proposed in recent years. However those that perform well tend to be highly specialised for certain types of music, while those that are more widely applicable give only moderate performance. In this paper we present a new onset detector with superior performance and temporal precision for all kinds of music, including complex music mixes. It is based on auditory spectral features and relative spectral differences processed by a bidirectional Long Short-Term Memory recurrent neural network, which acts as reduction function. The network is trained with a large database of onset data covering various genres and onset types. Due to the data driven nature, our approach does not require the onset detection method and its parameters to be tuned to a particular type of music. We compare results on the Bello onset data set and can conclude that our approach is on par with related results on the same set and outperforms them in most cases in terms of F1-measure. For complex music with mixed onset types, an absolute improvement of 3.6% is reported. © 2010 International Society for Music Information Retrieval."
Smith L.M.,Beat critic: Beat tracking octave error identification by metrical profile analysis,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585095&partnerID=40&md5=7893a35264431d96e4dec4512bcc85cc,"Smith L.M., IRCAM, France","Computational models of beat tracking of musical audio have been well explored, however, such systems often make ""octave errors"", identifying the beat period at double or half the beat rate than that actually recorded in the music. A method is described to detect if octave errors have occurred in beat tracking. Following an initial beat tracking estimation, a feature vector of metrical profile separated by spectral subbands is computed. A measure of subbeat quaver (1/8th note) alternation is used to compare half time and double time measures against the initial beat track estimation and indicate a likely octave error. This error estimate can then be used to re-estimate the beat rate. The performance of the approach is evaluated against the RWC database, showing successful identification of octave errors for an existing beat tracker. Using the octave error detector together with the existing beat tracking model improved beat tracking by reducing octave errors to 43% of the previous error rate. © 2010 International Society for Music Information Retrieval."
Karydis I.; Radovanović M.; Nanopoulos A.; Ivanović M.,"Looking through the ""Glass ceiling"": A conceptual framework for the problems of spectral similarity",2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649416079&partnerID=40&md5=0c3aae790be541df9424c9106bdb2a36,"Karydis I., Dept. of Informatics, Ionian University, Greece; Radovanović M., Faculty of Science, University of Novi Sad, Serbia; Nanopoulos A., Inst. of Computer Science, University of Hildesheim, Germany; Ivanović M., Faculty of Science, University of Novi Sad, Serbia","Spectral similarity measures have been shown to exhibit good performance in several Music Information Retrieval (MIR) applications. They are also known, however, to possess several undesirable properties, namely allowing the existence of hub songs (songs which frequently appear in nearest neighbor lists of other songs), ""orphans"" (songs which practically never appear), and difficulties in distinguishing the farthest from the nearest neighbor due to the concentration effect caused by high dimensionality of data space. In this paper we develop a conceptual framework that allows connecting all three undesired properties. We show that hubs and ""orphans"" are expected to appear in high-dimensional data spaces, and relate the cause of their appearance with the concentration property of distance / similarity measures. We verify our conclusions on realmusic data, examining groups of frames generated by Gaussian Mixture Models (GMMs), considering two similarity measures: Earth Mover's Distance (EMD) in combination with Kullback-Leibler (KL) divergence, and Monte Carlo (MC) sampling. The proposed framework can be useful to MIR researchers to address problems of spectral similarity, understand their fundamental origins, and thus be able to develop more robust methods for their remedy. © 2010 International Society for Music Information Retrieval."
Hu Y.; Chen X.; Yang D.,Lyric-based song emotion detection with affective lexicon and fuzzy clustering method,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873686728&partnerID=40&md5=f27a7a9d51a1be05c45a540a8244c489,"Hu Y., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China","A method is proposed for detecting the emotions of Chinese song lyrics based on an affective lexicon. The lexicon is composed of words translated from ANEW and words selected by other means. For each lyric sentence, emotion units, each based on an emotion word in the lexicon, are found out, and the influences of modifiers and tenses on emotion units are taken into consideration. The emotion of a sentence is calculated from its emotion units. To figure out the prominent emotions of a lyric, a fuzzy clustering method is used to group the lyric's sentences according to their emotions. The emotion of a cluster is worked out from that of its sentences considering the individual weight of each sentence. Clusters are weighted according to the weights and confidences of their sentences and singing speeds of sentences are considered as the adjustment of the weights of clusters. Finally, the emotion of the cluster with the highest weight is selected from the prominent emotions as the main emotion of the lyric. The performance of our approach is evaluated through an experiment of emotion classification of 500 Chinese song lyrics. © 2009 International Society for Music Information Retrieval."
Chordia P.; Sastry A.; Malikarjuna T.; Albin A.,Multiple viewpoints modeling of tabla sequences,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960496938&partnerID=40&md5=41743e0f2482ce6801bb6ea58ca67f13,"Chordia P., Center for Music Technology, Georgia Tech, Atlanta, GA, United States; Sastry A., Center for Music Technology, Georgia Tech, Atlanta, GA, United States; Malikarjuna T., Center for Music Technology, Georgia Tech, Atlanta, GA, United States; Albin A., Center for Music Technology, Georgia Tech, Atlanta, GA, United States","We describe a system that attempts to predict the continuation of a symbolically encoded tabla composition at each time step using a variable-length n-gram model. Using cross-entropy as a measure of model fit, the best model attained an entropy rate of 0.780 in a cross-validation experiment, showing that symbolic tabla compositions can be effectively encoded using such a model. The choice of smoothing algorithm, which determines how information from different-order models is combined, is found to be an important factor in the models performance. We extend the basic n-gram model by adding viewpoints, other streams of information that can be used to improve predictive performance. First, we show that adding a short-term model, built on the current composition and not the entire corpus, leads to substantial improvements. Additional experiments were conducted with derived types, representations derived from the basic data type (stroke names), and cross-types, which model dependencies between parameters, such as duration and stroke name. For this database, such extensions improved performance only marginally, although this may have been due to the low entropy rate attained by the basic model. © 2010 International Society for Music Information Retrieval."
Vincent E.; Raczynski S.A.; Ono N.; Sagayama S.,A roadmap towards versatile MIR,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582945&partnerID=40&md5=a3fc4e8ca6409b25557289e34a19394e,"Vincent E., INRIA, France; Raczynski S.A., University of Tokyo, Japan; Ono N., University of Tokyo, Japan; Sagayama S., University of Tokyo, Japan","Most MIR systems are specifically designed for one application and one cultural context and suffer from the semantic gap between the data and the application. Advances in the theory of Bayesian language and information processing enable the vision of a versatile, meaningful and accurate MIR system integrating all levels of information. We propose a roadmap to collectively achieve this vision. © 2010 International Society for Music Information Retrieval."
Marolt M.; Lefeber M.,It's time for a song - Transcribing recordings of bell-playing clocks,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856146718&partnerID=40&md5=d66e904a10763c7726d2ace9fc7a5228,"Marolt M., Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Lefeber M., Meertens Instituut and Museum Speelklok, Netherlands","The paper presents an algorithm for automatic transcription of recordings of bell-playing clocks. Bell-playing clocks are clocks containing a hidden bell-playing mechanism that is periodically activated to play a melody. Clocks from the eighteenth century give us unique insight into the musical taste of their owners, so we are interested in studying their repertoire and performances - thus the need for automatic transcription. In the paper, we first present an analysis of acoustical properties of bells found in bell-playing clocks. We propose a model that describes positions of bell partials and an algorithm that discovers the number of bells and positions of their partials in a given recording. To transcribe a recording, we developed a probabilistic method that maximizes the joint probability of a note sequence given the recording and positions of bell partials. Finally, we evaluate our algorithms on a set of recordings of bell-playing clocks. © 2010 International Society for Music Information Retrieval."
Ahonen T.E.,Combining chroma features for cover version identification,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864189055&partnerID=40&md5=d9a6bce3427d2862794a63a040f78bd3,"Ahonen T.E., Department of Computer Science, University of Helsinki, Finland","We present an approach for cover version identification which is based on combining different discretized features derived from the chromagram vectors extracted from the audio data. For measuring similarity between features, we use a parameter-free quasi-universal similarity metric which utilizes data compression. Evaluation proves that combined feature distances increase the accuracy in cover version identification. © 2010 International Society for Music Information Retrieval."
Hoffman M.D.; Blei D.M.; Cook P.R.,Easy as CBA: A simple probabilistic model for tagging music,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873687590&partnerID=40&md5=779bb731f413d1d72461bba87cf488dc,"Hoffman M.D., Dept. of Computer Science, Princeton University, United States; Blei D.M., Dept. of Computer Science, Princeton University, United States; Cook P.R., Dept. of Computer Science, Princeton University, United States","Many songs in large music databases are not labeled with semantic tags that could help users sort out the songs they want to listen to from those they do not. If the words that apply to a song can be predicted from audio, then those predictions can be used both to automatically annotate a song with tags, allowing users to get a sense of what qualities characterize a song at a glance. Automatic tag prediction can also drive retrieval by allowing users to search for the songs most strongly characterized by a particular word. We present a probabilistic model that learns to predict the probability that a word applies to a song from audio. Our model is simple to implement, fast to train, predicts tags for new songs quickly, and achieves state-of-the-art performance on annotation and retrieval tasks. © 2009 International Society for Music Information Retrieval."
Schuller B.; Kozielski C.; Weninger F.; Eyben F.; Rigoll G.,Vocalist gender recognition in recorded popular music,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585840&partnerID=40&md5=7caae6cc732a62b0aed92ca845fbcdca,"Schuller B., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Kozielski C., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Weninger F., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Eyben F., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Rigoll G., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany","We introduce the task of vocalist gender recognition in popular music and evaluate the benefit of Non-Negative Matrix Factorization based enhancement of melodic components to this aim. The underlying automatic separation of drum beats is described in detail, and the obtained significant gain by its use is verified in extensive test-runs on a novel database of 1.5 days of MP3 coded popular songs based on transcriptions of the Karaoke-game UltraStar. As classifiers serve Support Vector Machines and Hidden Naive Bayes. Overall, the suggested methods lead to fully automatic recognition of the pre-dominant vocalist gender at 87.31% accuracy on song level for artists unkown to the system in originally recorded music. © 2010 International Society for Music Information Retrieval."
Koenigstein N.; Lanckriet G.; McFee B.; Shavitt Y.,Collaborative filtering based on P2P networks,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588717&partnerID=40&md5=d30f716fb1c91aff41c78716d4e50db7,"Koenigstein N., School of Electrical Engineering, Tel Aviv University, Israel; Lanckriet G., Department of Electrical and Computer Engineering, University of California, San Diego, United States; McFee B., Department of Computer Science and Engineering, University of California, San Diego, United States; Shavitt Y., School of Electrical Engineering, Tel Aviv University, Israel","Peer-to-Peer (P2P) networks are used by millions of people for sharing music files. As these networks become ever more popular, they also serve as an excellent source for Music Information Retrieval (MIR) tasks. This paper reviews the latest MIR studies based on P2P data-sets, and presents a new file sharing data collection system over the Gnutella. We discuss several advantages of P2P based data-sets over some of the more ""traditional"" data sources, and evaluate the information quality of our data-set in comparison to other data sources (Last.fm, social tags, biography data, and MFCCs). The evaluation is based on an artists similarity task using Partial Order Embedding (POE). We show that a P2P based Collaborative Filtering dataset performs at least as well as ""traditional"" data-sets, yet maintains some inherent advantages such as scale, availability and additional information features such as ID3 tags and geographical location. © 2010 International Society for Music Information Retrieval."
Lu Q.; Chen X.; Yang D.; Wang J.,Boosting for multi-modal music emotion classification,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871292332&partnerID=40&md5=9d2064fd7d757fbdd60b1edf3d8fb4d4,"Lu Q., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China; Wang J., Institute of Computer Science and Technology, Peking University, China","With the explosive growth of music recordings, automatic classification of music emotion becomes one of the hot spots on research and engineering. Typical music emotion classification (MEC) approaches apply machine learning methods to train a classifier based on audio features. In addition to audio features, the MIDI and lyrics features of music also contain useful semantic information for predicting the emotion of music. In this paper we apply AdaBoost algorithm to integrate MIDI, audio and lyrics information and propose a two-layer classifying strategy called Fusion by Subtask Merging for 4-class music emotion classification. We evaluate each modality respectively using SVM, and then combine any two of the three modalities, using AdaBoost algorithm (MIDI+audio, MIDI+lyrics, audio+lyrics). Moreover, integrating this in a multimodal system (MIDI+audio+lyrics) allows an improvement in the overall performance. The experimental results show that MIDI, audio and lyrics information are complementary, and can be combined to improve a classification system. © 2010 International Society for Music Information Retrieval."
Chuan C.-H.; Chew E.,Quantifying the benefits of using an interactive decision support tool for creating musical accompaniment in a particular style,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856418678&partnerID=40&md5=d060598af74b7851056b8d4f074a07f6,"Chuan C.-H., School of Computing, University of North Florida, Jacksonville, FL, United States; Chew E., Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States","We present a human-centered experiment designed to measure the degree of support for creating musical accompaniment provided by an interactive composition decision- support system. We create an interactive system with visual and audio cues to assist users in the choosing of chords to craft an accompaniment in a desired style. We propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quantitative measures of musical distance - percentage correct and closely related chords, and average neo-Riemannian distance - compare the user-created accompaniment with the original, with and without decision support. Numbers of backward edits, unique chords explored, and repeated chord choices during composition help quantify composition behavior. We present experimental data from musicians and non-musicians. We observe that decision support reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as repeated chord choices, and the gap between musicians' and non-musicians' work, without significantly limiting the range of users' choices. © 2010 International Society for Music Information Retrieval."
Niedermayer B.; Widmer G.,A multi-pass algorithm for accurate audio-to-score alignment,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051657095&partnerID=40&md5=28db7a4271388884469a185116f2cfc4,"Niedermayer B., Department for Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Most current audio-to-score alignment algorithms work on the level of score time frames; i.e., they cannot differentiate between several notes occurring at the same discrete time within the score. This level of accuracy is sufficient for a variety of applications. However, for those that deal with, for example, musical expression analysis such microtimings might also be of interest. Therefore, we propose a method that estimates the onset times of individual notes in a post-processing step. Based on the initial alignment and a feature obtained by matrix factorization, those notes for which the confidence in the alignment is high are chosen as anchor notes. The remaining notes in between are revised, taking into account the additional information about these anchors and the temporal relations given by the score. We show that this method clearly outperforms a reference method that uses the same features but does not differentiate between anchor and non-anchor notes. © 2010 International Society for Music Information Retrieval."
Flexer A.; Schnitzer D.; Gasser M.; Pohle T.,Combining features reduces hubness in audio similarity,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960546359&partnerID=40&md5=5f2e668cb465d1f455beb234b11ea5a2,"Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In audio based music similarity, a well known effect is the existence of hubs, i.e. songs which appear similar to many other songs without showing any meaningful perceptual similarity. We verify that this effect also exists in very large databases (> 250000 songs) and that it even gets worse with growing size of databases. By combining different aspects of audio similarity we are able to reduce the hub problem while at the same time maintaining a high overall quality of audio similarity. © 2010 International Society for Music Information Retrieval."
Schnitzer D.; Flexer A.; Widmer G.; Gasser M.,Islands of gaussians: The self organizing map and gaussian music similarity features,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155154171&partnerID=40&md5=30aff0ff52751e7f7eadfd7a35366f09,"Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Multivariate Gaussians are of special interest in the MIR field of automatic music recommendation. They are used as the de facto standard representation of music timbre to compute music similarity. However, standard algorithms for clustering and visualization are usually not designed to handle Gaussian distributions and their attached metrics (e.g. the Kullback-Leibler divergence). Hence to use these features the algorithms generally handle them indirectly by first mapping them to a vector space, for example by deriving a feature vector representation from a similarity matrix. This paper uses the symmetrized Kullback-Leibler centroid of Gaussians to show how to avoid the vectorization detour for the Self Organizing Maps (SOM) data visualization algorithm. We propose an approach so that the algorithm can directly and naturally work on Gaussian music similarity features to compute maps of music collections. We show that by using our approach we can create SOMs which (1) better preserve the original similarity topology and (2) are far less complex to compute, as the often costly vectorization step is eliminated. © 2010 International Society for Music Information Retrieval."
Mandel M.I.; Eck D.; Bengio Y.,Learning tags that vary within a song,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053156439&partnerID=40&md5=36b11421038c43f7342798b5caf267ea,"Mandel M.I., LISA Lab, Université de Montréal, Canada; Eck D., LISA Lab, Université de Montréal, Canada; Bengio Y., LISA Lab, Université de Montréal, Canada","This paper examines the relationship between human generated tags describing different parts of the same song. These tags were collected using Amazon's Mechanical Turk service. We find that the agreement between different people's tags decreases as the distance between the parts of a song that they heard increases. To model these tags and these relationships, we describe a conditional restricted Boltzmann machine. Using this model to fill in tags that should probably be present given a context of other tags, we train automatic tag classifiers (autotaggers) that outperform those trained on the original data. © 2010 International Society for Music Information Retrieval."
Konz V.; Müller M.; Ewert S.,A multi-perspective evaluation framework for chord recognition,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053003979&partnerID=40&md5=14247cca58c735e69a2a4eb6a7b5cf56,"Konz V., MPI Informatik, Saarland University, Germany; Müller M., MPI Informatik, Saarland University, Germany; Ewert S., Department of Computer Science III, University of Bonn, Germany","The automated extraction of chord labels from audio recordings constitutes a major task in music information retrieval. To evaluate computer-based chord labeling procedures, one requires ground truth annotations for the underlying audio material. However, the manual generation of such annotations on the basis of audio recordings is tedious and time-consuming. On the other hand, trained musicians can easily derive chord labels from symbolic score data. In this paper, we bridge this gap by describing a procedure that allows for transferring annotations and chord labels from the score domain to the audio domain and vice versa. Using music synchronization techniques, the general idea is to locally warp the annotations of all given data streams onto a common time axis, which then allows for a cross-domain evaluation of the various types of chord labels. As a further contribution of this paper, we extend this principle by introducing amulti-perspective evaluation framework for simultaneously comparing chord recognition results over multiple performances of the same piece of music. The revealed inconsistencies in the results do not only indicate limitations of the employed chord labeling strategies but also deepen the understanding of the underlying music material. © 2010 International Society for Music Information Retrieval."
Le Coz M.; Lachambre H.; Koenig L.; Andre-Obrecht R.,A segmentation-based tempo induction method,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866673349&partnerID=40&md5=3b916e6e8d5c87447475a8882853b962,"Le Coz M., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France; Lachambre H., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France; Koenig L., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France; Andre-Obrecht R., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France","The automatized beat detection and localization have been the subject of multiple research in the field of music information retrieval. Most of the methods are based on onset detection. We propose an alternative approach: Our method is based on the ""Forward-Backward segmentation"": the segments may be interpreted as attacks, decays, sustains and releases of notes. We process the segment boundaries as a weighted Dirac signal. Three methods devived from its spectral analysis are proposed to find a periodicity which corresponds to the tempo. The experiments are carried out on a corpus of 100 songs of the RWC database. The performances of our system on this base demonstrate a potential in the use of a "" Forward- Backward Segmentation"" for temporal information retrieval in musical signals. © 2010 International Society for Music Information Retrieval."
Ramirez C.; Ohya J.,Symbol classification approach for OMR of square notation manuscripts,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955723424&partnerID=40&md5=f53882bd3148408e8641415072b43c70,"Ramirez C., Waseda University, Japan; Ohya J., Waseda University, Japan","Researchers in the field of OMR (Optical Music Recognition) have acknowledged that the automatic transcription of medieval musical manuscripts is still an open problem [2, 3], mainly due to lack of standards in notation and the physical quality of the documents. Nonetheless, the amount of medieval musical manuscripts is so vast that the consensus seems to be that OMR can be a vital tool to help in the preserving and sharing of this information in digital format. In this paper we report our results on a preliminary approach to OMR of medieval plainchant manuscripts in square notation, at the symbol classification level, which produced good results in the recognition of eight basic symbols. Our preliminary approach consists of the preprocessing, segmentation, and classification stages. © 2010 International Society for Music Information Retrieval."
Barrington L.; Oda R.; Lanckriet G.,Smarter than genius? Human evaluation of music recommender systems,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873683421&partnerID=40&md5=4aced3b02495612b8e0c8d87d5dcdb00,"Barrington L., Electrical and Computer Engineering, University of California, San Diego, United States; Oda R., Cognitive Science, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","Genius is a popular commercial music recommender system that is based on collaborative filtering of huge amounts of user data. To understand the aspects of music similarity that collaborative filtering can capture, we compare Genius to two canonical music recommender systems: one based purely on artist similarity, the other purely on similarity of acoustic content. We evaluate this comparison with a user study of 185 subjects. Overall, Genius produces the best recommendations. We demonstrate that collaborative filtering can actually capture similarities between the acoustic content of songs. However, when evaluators can see the names of the recommended songs and artists, we find that artist similarity can account for the performance of Genius. A system that combines these musical cues could generate music recommendations that are as good as Genius, even when collaborative filtering data is unavailable. © 2009 International Society for Music Information Retrieval."
Vigliensoni G.; McKay C.; Fujinaga I.,Using jWEBminer 2.0 to improve music classification performance by combining different types of features mined from the web,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871992019&partnerID=40&md5=294f910557cdd760d33efdfd400e784b,"Vigliensoni G., CIRMMT, McGill University, Canada; McKay C., CIRMMT, McGill University, Canada; Fujinaga I., CIRMMT, McGill University, Canada","This paper presents the jWebMiner 2.0 cultural feature extraction software and describes the results of several musical genre classification experiments performed with it. jWebMiner 2.0 is an easy-to-use and open-source tool that allows users to mine the Internet in order to extract features based on both Last.fm social tags and general web search string co-occurrences extracted using the Yahoo! API. The experiments performed found that the features based on social tags were more effective at classifying music into a small (5-genre) genre ontology, but the features based on general web co-occurrences were more effective at classifying a moderate (10-genre) ontology. It was also found that combining the two types of features resulted in improved performance overall. © 2010 International Society for Music Information Retrieval."
Pinto A.,Eigenvector-based relational motif discovery,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576440&partnerID=40&md5=1aab043e6e702544f9269efa5b359bd3,"Pinto A., Dipartimento di Informatica E Comunicazione, Universitá degli Studi di Milano, I-20135 Milano, Via Comelico 39/41, Italy","The development of novel analytical tools to investigate the structure of music works is central in current music information retrieval research. In particular, music summarization aims at finding the most representative parts of a music piece (motifs) that can be exploited for an efficient music database indexing system. Here we present a novel approach for motif discovery in music pieces based on an eigenvector method. Scores are segmented into a network of bars and then ranked depending on their centrality. Bars with higher centrality are more likely to be relevant for music summarization. Results on the corpus of J.S.Bach's 2-part Inventions demonstrate the effectiveness of the method and suggest that different musical metrics might be more suitable than others for different applications. © 2010 International Society for Music Information Retrieval."
Van Zaanen M.; Kanters P.,Automatic mood classification using TF*IDF based on lyrics,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862936491&partnerID=40&md5=2b417c4854be9f4d41c1d88ce12a118a,"Van Zaanen M., Tilburg Center for Cognition and Communication, Tilburg University, Tilburg, Netherlands; Kanters P., Tilburg Center for Cognition and Communication, Tilburg University, Tilburg, Netherlands","This paper presents the outcomes of research into using lingual parts of music in an automatic mood classification system. Using a collection of lyrics and corresponding user-tagged moods, we build classifiers that classify lyrics of songs into moods. By comparing the performance of different mood frameworks (or dimensions), we examine to what extent the linguistic part of music reveals adequate information for assigning a mood category and which aspects of mood can be classified best. Our results show that word oriented metrics provide a valuable source of information for automatic mood classification of music, based on lyrics only. Metrics such as term frequencies and tf*idf values are used to measure relevance of words to the different mood classes. These metrics are incorporated in a machine learning classifier setup. Different partitions of the mood plane are investigated and we show that there is no large difference in mood prediction based on the mood division. Predictions on the valence, tension and combinations of aspects lead to similar performance. © 2010 International Society for Music Information Retrieval."
Paulus J.; Müller M.; Klapuri A.,Audio-based music structure analysis,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863552197&partnerID=40&md5=ea67d8da0ed5d8e42bd4f5a30d4c7440,"Paulus J., Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Klapuri A., Centre for Digital Music, Queen Mary Univ. of London, London, United Kingdom","Humans tend to organize perceived information into hierarchies and structures, a principle that also applies to music. Even musically untrained listeners unconsciously analyze and segment music with regard to various musical aspects, for example, identifying recurrent themes or detecting temporal boundaries between contrasting musical parts. This paper gives an overview of state-of-theart methods for computational music structure analysis, where the general goal is to divide an audio recording into temporal segments corresponding to musical parts and to group these segments into musically meaningful categories. There are many different criteria for segmenting and structuring music audio. In particular, one can identify three conceptually different approaches, which we refer to as repetition-based, novelty-based, and homogeneitybased approaches. Furthermore, one has to account for different musical dimensions such as melody, harmony, rhythm, and timbre. In our state-of-the-art report, we address these different issues in the context of music structure analysis, while discussing and categorizing the most relevant and recent articles in this field. © 2010 International Society for Music Information Retrieval."
Schmidt E.M.; Kim Y.E.,Prediction of time-varying musical mood distributions from audio,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555174290&partnerID=40&md5=34157cfffc550abc1377b1b806ad577e,"Schmidt E.M., Department of Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Department of Electrical and Computer Engineering, Drexel University, United States","The appeal of music lies in its ability to express emotions, and it is natural for us to organize music in terms of emotional associations. But the ambiguities of emotions make the determination of a single, unequivocal response label for the mood of a piece of music unrealistic. We address this lack of specificity by modeling human response labels to music in the arousal-valence (A-V) representation of affect as a stochastic distribution. Based upon our collected data, we present and evaluate methods using multiple sets of acoustic features to estimate these mood distributions parametrically using multivariate regression. Furthermore, since the emotional content of music often varies within a song, we explore the estimation of these A-V distributions in a time-varying context, demonstrating the ability of our system to track changes on a short-time basis. © 2010 International Society for Music Information Retrieval."
Ganseman J.; Scheunders P.; Mysore G.J.; Abel J.S.,Evaluation of a score-informed source separation system,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053006437&partnerID=40&md5=0990aa343e1f9ee9ed6b3b5abc23b54c,"Ganseman J., Department of Physics, IBBT - Visielab, University of Antwerp, 2000 Antwerp, Belgium; Scheunders P., Department of Physics, IBBT - Visielab, University of Antwerp, 2000 Antwerp, Belgium; Mysore G.J., Department of Music, CCRMA, Stanford University, Stanford, CA 94305, United States; Abel J.S., Department of Music, CCRMA, Stanford University, Stanford, CA 94305, United States","In this work, we investigate a method for score-informed source separation using Probabilistic Latent Component Analysis (PLCA). We present extensive test results that give an indication of the performance of the method, its strengths and weaknesses. For this purpose, we created a test database that has been made available to the public, in order to encourage comparisons with alternative methods. © 2010 International Society for Music Information Retrieval."
Vatolkin I.; Theimer W.; Botteck M.,Amuse (Advanced music explorer) - A multitool framework for music data analysis,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959404927&partnerID=40&md5=bcf2454b5e388ae6f2c20c32be71d42a,"Vatolkin I., Department of Algorithm Engineering, TU Dortmund, Germany; Theimer W., Research in Motion, Bochum, Germany; Botteck M.","A large variety of research tools is available now for music information retrieval tasks. In this paper we present a further framework which aims to facilitate the interaction between these applications. Since the available tools are very different in target domain, range of available methods, learning efforts, installation and runtime characteristics etc., it is not easy to find software which is optimal for certain research goals. Another problematic issue is that many incompatible data formats exist, so it is not always possible to use output from one tool just as input for another one. At first we describe some of the available projects and outline our motivation starting the development of AMUSE framework for audio data analysis. Requirements and application purposes are given. The structure of our framework is introduced in detail and the information for efficient application is provided. Finally we discuss several ideas for further work. © 2010 International Society for Music Information Retrieval."
Collins N.,Computational analysis of musical influence: A musicological case study using MIR tools,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955393344&partnerID=40&md5=dc421e173250ea53b129f2c22fac26f9,"Collins N., Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QJ, United Kingdom","Are there new insights through computational methods to the thorny problem of plotting the flow of musical influence? This project, motivated by a musicological study of early synth pop, applies MIR tools as an aid to the investigator. Web scraping and web services provide one angle, sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz. Charts of influence are constructed in GraphViz combining artist similarity and dates. Content based music similarity is the second approach, based around a core collection of synth pop albums. The prospect for new musical analyses are discussed with respect to these techniques. © 2010 International Society for Music Information Retrieval."
Wang D.; Li T.; Ogihara M.,Are tags better than audio features? The effect of joint use of tags and audio content features for artistic style clustering,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586245&partnerID=40&md5=0d4001bd77be32b8d9fbe2754299d83a,"Wang D., School of Computer Science, Florida International University, Miami, FL, United States; Li T., School of Computer Science, Florida International University, Miami, FL, United States; Ogihara M., Department of Computer Science, University of Miami, Coral Gables, FL, United States","Social tags are receiving growing interests in information retrieval. In music information retrieval previous research has demonstrated that tags can assist in music classification and clustering. This paper studies the problem of combining tags and audio contents for artistic style clustering. After studying the effectiveness of using tags and audio contents separately for clustering, this paper proposes a novel language model that makes use of both data sources. Experiments with various methods for combining feature sets demonstrate that tag features are more useful than audio content features for style clustering and that the proposed model can marginally improve clustering performance by combing tags and audio contents. © 2010 International Society for Music Information Retrieval."
Jewell M.O.; Rhodes C.; D'Inverno M.,Querying improvised music: Do you sound like yourself?,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955412346&partnerID=40&md5=2647d066b855ee2beeef255a90eb88ff,"Jewell M.O., Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom; Rhodes C., Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom; D'Inverno M., Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom","Improvisers are often keen to assess how their performance practice stands up to an ideal: whether that ideal is of technical accuracy or instant composition of material meeting complex harmonic constraints at speed. This paper reports on the development of an interface for querying and navigating a collection of recorded material for the purpose of presenting information on musical similarity, and the application of this interface to the investigation of a set of recordings by jazz performers. We investigate the retrieval performance of our tool, and in analysing the 'hits' and particularly the 'misses', provide information suggesting a change in one of the authors' improvisation style. © 2010 International Society for Music Information Retrieval."
Lemstrom K.,Towards more robust geometric content-based music retrieval,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053097401&partnerID=40&md5=00f4a4bfabfd7686d69dcac881bc6ee3,"Lemstrom K., Department of Computer Science, University of Helsinki, Finland","This paper studies the problem of transposition and time-scale invariant (ttsi) polyphonic music retrieval in symbolically encoded music. In the setting, music is represented by sets of points in plane. We give two new algorithms. Applying a search window of size w and given a query point set, of size m, to be searched for in a database point set, of size n, our algorithm for exact ttsi occurrences runs in O(mwn log n) time; for partial occurrences we have an O(mnw2 log n) algorithm. The framework used is flexible allowing development towards even more robust geometric retrieval. © 2010 International Society for Music Information Retrieval."
Macrae R.; Dixon S.,Accurate real-time windowed time warping,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578359&partnerID=40&md5=d494982adb0b3e6d4a76ad3c17454996,"Macrae R., Centre for Digital Music, Queen Mary University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University, London, United Kingdom","Dynamic Time Warping (DTW) is used to find alignments between two related streams of information and can be used to link data, recognise patterns or find similarities. Typically, DTW requires the complete series of both input streams in advance and has quadratic time and space requirements. As such DTW is unsuitable for real-time applications and is inefficient for aligning long sequences. We present Windowed Time Warping (WTW), a variation on DTW that, by dividing the path into a series of DTW windows and making use of path cost estimation, achieves alignments with an accuracy and efficiency superior to other leading modifications and with the capability of synchronising in real-time. We demonstrate this method in a score following application. Evaluation of the WTW score following system found 97.0% of audio note onsets were correctly aligned within 2000 ms of the known time. Results also show reductions in execution times over state-of-theart efficient DTW modifications. © 2010 International Society for Music Information Retrieval."
Molina-Solana M.; Grachten M.; Widmer G.,Evidence for pianist-specific rubato style in chopin nocturnes,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051604329&partnerID=40&md5=a68cb17c78d24252341cdd2aad0b3677,"Molina-Solana M., Dpt. Computer Science and AI, University of Granada, Spain; Grachten M., IPEM - Dept. of Musicology, Ghent University, Belgium; Widmer G., Dpt. of Computational Perception, Johannes Kepler Univ., Austria","The performance of music usually involves a great deal of interpretation by the musician. In classical music, the final ritardando is a good example of the expressive aspect of music performance. Even though expressive timing data is expected to have a strong component that is determined by the piece itself, in this paper we investigate to what degree individual performance style has an effect on the timing of final ritardandi. The particular approach taken here uses Friberg and Sundberg's kinematic rubato model in order to characterize performed ritardandi. Using a machine- learning classifier, we carry out a pianist identification task to assess the suitability of the data for characterizing the in- dividual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, when cancelling the piece-specific aspects, pianists can often be identified with accuracy above baseline. This fact suggests the existence of a performer-specific style of playing ritardandi. © 2010 International Society for Music Information Retrieval."
Conklin D.; Bergeron M.,Discovery of contrapuntal patterns,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959609428&partnerID=40&md5=e866145476371110c1981fbef3b19969,"Conklin D., Department of Computer Science and AI, Basque Foundation for Science, Universidad del Páis Vasco, Bilbao, San Sebastían, Spain; Bergeron M., CIRMMT, McGill University, Montreal, Canada","This paper develops and applies a new method for the discovery of polyphonic patterns. The method supports the representation of abstract relations that are formed between notes that overlap in time without being simultaneous. Such relations are central to understanding species counterpoint. The method consists of an application of the vertical viewpoint technique, which relies on a vertical slicing of the musical score. It is applied to two-voice contrapuntal textures extracted from the Bach chorale harmonizations. Results show that the new method is powerful enough to represent and discover distinctive modules of species counterpoint, including remarkably the suspension principle of fourth species counterpoint. In addition, by focusing on two voices in particular and setting them against all other possible voice pairs, the method can elicit patterns that illustrate well the unique treatment of the voices under investigation, e.g. the inner and outer voices. The results are promising and indicate that the method is suitable for computational musicology research. © 2010 International Society for Music Information Retrieval."
Gärtner D.,Singing / RAP classification of isolated vocal tracks,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053108452&partnerID=40&md5=f21a6d7483de5c2798dc4b143389cb31,"Gärtner D., Fraunhofer Institute for Digital Media Technology IDMT, Germany","In this paper, a system for the classification of the vocal characteristics in HipHop / R&B music is presented. Isolated vocal track segments, taken from acapella versions of commercial recordings, are classified into classes singing and rap. A feature-set motivated by work from song / speech classification, speech emotion recognition, and from differences that humans perceive and utilize, is presented. An SVM is used as classifier, accuracies of about 90% are achieved. In addition, the features are analyzed according to their contribution, using the IRMFSP feature selection algorithm. In another experiment, it is shown that the features are robust against utterance-specific characteristics. © 2010 International Society for Music Information Retrieval."
Jo S.; Yoo C.D.,Melody extraction from polyphonic audio based on particle filter,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572703&partnerID=40&md5=415da956a5b89847bfe56ab0bb7e7456,"Jo S., Department of Electrical Engineering, Korea Advanced Institute of Science Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Yoo C.D., Department of Electrical Engineering, Korea Advanced Institute of Science Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea","This paper considers a particle filter based algorithm to ex- tract melody from a polyphonic audio in the short-time Fourier transforms (STFT) domain. The extraction is focused on overcoming the difficulties due to harmonic / percussive sound interferences, possibility of octave mismatch, and dynamic variation in melody. The main idea of the algorithm is to consider probabilistic relations between melody and polyphonic audio. Melody is assumed to follow a Markov process, and the framed segments of polyphonic audio are assumed to be conditionally independent given the parameters that represent the melody. The melody parameters are estimated using sequential importance sampling (SIS) which is a conventional particle filter method. In this paper, the likelihood and state transition are defined to overcome the aforementioned difficulties. The SIS algorithm relies on sequential importance density, and this density is designed using multiple pitches which are estimated by a simple multi-pitch extraction algorithm. Experimental results show that the considered algorithm outperforms other famous melody extraction algorithms in terms of the raw pitch accuracy (RPA) and the raw chroma accuracy (RCA). © 2010 International Society for Music Information Retrieval."
Rocher T.; Robine M.; Hanna P.; Oudre L.,Concurrent estimation of chords and keys from audio,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572944&partnerID=40&md5=8a8a61cf198f717fe905af9ae0169e55,"Rocher T., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; Robine M., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; Hanna P., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; Oudre L., TELECOM ParisTech, Institut TELECOM, 75014 Paris, 37-39 rue Dareau, France","This paper proposes a new method for local key and chord estimation from audio signals. A harmonic content of the musical piece is first extracted by computing a set of chroma vectors. Correlation with fixed chord and key templates then selects a set of key/chord pairs for every frame. A weighted acyclic harmonic graph is then built with these pairs as vertices, and the use of a musical distance to weigh its edges. Finally, the output sequences of chords and keys are obtained by finding the best path in the graph. The proposed system allows a mutual and beneficial chord and key estimation. It is evaluated on a corpus composed of Beatles songs for both the local key estimation and chord recognition tasks. Results show that it performs better than state-of-the art chord analysis algorithms while providing a more complete harmonic analysis. © 2010 International Society for Music Information Retrieval."
Schepens W.,Chronicle: Representation of complex time structures,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873681901&partnerID=40&md5=256faf9baac430445da448b1f56fd1a6,"Schepens W., University College Ghent, Belgium","Chronicle is a novel open source system for representing structured data involving time, such as music. It offers an XML-based file format, object models for internal representation in various programming languages, and software libraries and tools for reading and writing XML and for data transformations. Chronicle defines basic blocks for representing timebased information using events, a hierarchy of groups and instantiable templates. It supports two modes of timing: local timing within a group and association with other elements. The built-in mechanism for resolving time references can be used to implement both timescale mappings and tagging of information. Chronicle aims to be a powerful and flexible foundation on which new file formats and software can be built. Chronicle focuses on structure and timing, but leaves the actual content free to choose. Thus format- or softwaredevelopers can specify their own domain-model. This makes it possible to make representations for different types of musical information (scores, performance data, ...) in different styles or cultures (CMN, non-western, contemporary, ...), but also for other domains like choreography, scheduling, task management, and so on. It is also ideal for structured tagging of audio and multimedia (movie subtitles, karaoke, synchronisation, ...) and for representing ""internal"" data used in music algorithms. The system is organized in four levels of increasing complexity. Software developed for a specific level and domain will also accept lower level data, while users can choose to represent data in a higher level and use Chronicle tools to reduce the level. © 2009 International Society for Music Information Retrieval."
Cuthbert M.S.; Ariza C.,Music21: A toolkit for computer-aided musicology and symbolic music data,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873479813&partnerID=40&md5=3a16428b90cf2f918794462d048ca682,"Cuthbert M.S., Department of Music and Theater Arts, Massachusetts Institute of Technology, United States; Ariza C., Department of Music and Theater Arts, Massachusetts Institute of Technology, United States","Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (scorebased) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demonstrating how to use it and the types of problems it is wellsuited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores. © 2010 International Society for Music Information Retrieval."
Barbieri G.; Pachet F.; Esposti M.D.; Roy P.,Is there a relation between the syntax and the fitness of an audio feature?,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600345&partnerID=40&md5=2e67b57ce4be860f8e965541e176e9d4,"Barbieri G., Dip. di Matematica, Università di Bologna, Italy; Pachet F., Sony CSL, Paris, France; Esposti M.D., Dip. di Matematica, Università di Bologna, Italy; Roy P., Sony CSL, Paris, France","Feature generation has been proposed recently to generate feature sets automatically, as opposed to human-designed feature sets. This technique has shown promising results in many areas of supervised classification, in particular in the audio domain. However, feature generation is usually performed blindly, with genetic algorithms. As a result search performance is poor, thereby limiting its practical use. We propose a method to increase the search performance of feature generation systems. We focus on analytical features, i.e. features determined by their syntax. Our method consists in first extracting statistical proper- ties of the feature space called spin patterns, by analogy with statistical physics. We show that spin patterns carry information about the topology of the feature space. We exploit these spin patterns to guide a simulated annealing algorithm specifically designed for feature generation. We evaluate our approach on three audio classification problems, and show that it increases performance by an order of magnitude. More generally this work is a first step in using tools from statistical physics for the supervised classification of complex audio signals. © 2010 International Society for Music Information Retrieval."
Inskip C.; MacFarlane A.; Rafferty P.,"Upbeat and quirky, with a bit of a build: Interpretive repertoires in creative music search",2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581828&partnerID=40&md5=1f5b0e4ea84c71b0b743680ad8fc56a0,"Inskip C., Dept of Info Science, City University London, United Kingdom; MacFarlane A., Dept of Info Science, City University London, United Kingdom; Rafferty P., Dept of Info Studies, University of Aberystwyth, United Kingdom","Pre-existing commercial music is widely used to accompany moving images in films, TV commercials and computer games. This process is known as music synchronisation. Professionals are employed by rights holders and film makers to perform creative music searches on large catalogues to find appropriate pieces of music for synchronisation. This paper discusses a Discourse Analysis of thirty interview texts related to the process. Coded examples are presented and discussed. Four interpretive repertoires are identified: the Musical Repertoire, the Soundtrack Repertoire, the Business Repertoire and the Cultural Repertoire. These ways of talking about music are adopted by all of the community regardless of their interest as Music Owner or Music User. Music is shown to have multi-variate and sometimes conflicting meanings within this community which are dynamic and negotiated. This is related to a theoretical feedback model of communication and meaning making which proposes that Owners and Users employ their own and shared ways of talking and thinking about music and its context to determine musical meaning. The value to the music information retrieval community is to inform system design from a user information needs perspective. © 2010 International Society for Music Information Retrieval."
Coviello E.; Barrington L.; Chan A.B.; Lanckriet Gert.R.G.,Automatic music tagging with time series models,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579339&partnerID=40&md5=74095ba7f158918218ce21a96c9c5b22,"Coviello E., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Barrington L., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Chan A.B., Dept. of Computer Science, University of Hong Kong, Hong Kong; Lanckriet Gert.R.G., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","State-of-the-art systems for automatic music tagging model music based on bag-of-feature representations which give little or no account of temporal dynamics, a key characteristic of the audio signal. We describe a novel approach to automatic music annotation and retrieval that captures temporal (e.g., rhythmical) aspects as well as timbral content. The proposed approach leverages a recently proposed song model that is based on a generative time series model of the musical content - the dynamic texture mixture (DTM) model - that treats fragments of audio as the output of a linear dynamical system. To model characteristic temporal dynamics and timbral content at the tag level, a novel, efficient hierarchical EM algorithm for DTM (HEM-DTM) is used to summarize the common information shared by DTMs modeling individual songs associated with a tag. Experiments show learning the semantics of music benefits from modeling temporal dynamics. © 2010 International Society for Music Information Retrieval."
Wang J.; Chen X.; Hu Y.; Feng T.,Predicting high-level music semantics using social tags via ontology-based reasoning,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955369613&partnerID=40&md5=b59fb4ee534e358e33afb004c16a3720,"Wang J., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Hu Y., Institute of Computer Science and Technology, Peking University, China; Feng T., Institute of Computer Science and Technology, Peking University, China","High-level semantics such as ""mood"" and ""usage"" are very useful in music retrieval and recommendation but they are normally hard to acquire. Can we predict them from a cloud of social tags? We propose a semantic identification and reasoning method: Given a music taxonomy system, we map it to an ontology's terminology, map its finite set of terms to the ontology's assertional axioms, and then map tags to the closest conceptual level of the referenced terms in WordNet to enrich the knowledge base, then we predict richer high-level semantic information with a set of reasoning rules. We find this method predicts mood annotations for music with higher accuracy, as well as giving richer semantic association information, than alternative SVM-based methods do. © 2010 International Society for Music Information Retrieval."
Holzapfel A.; Stylianou Y.,Parataxis: Morphological similarity in traditional music,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865698922&partnerID=40&md5=52ff09c47f0de2cb88264fb56557507c,"Holzapfel A., FORTH, Institute of Computer Science, University of Crete, Greece; Stylianou Y., FORTH, Institute of Computer Science, University of Crete, Greece","In this paper an automatic system for the detection of similar phrases in music of the Eastern Mediterranean is proposed. This music follows a specific structure, which is referred to as parataxis. The proposed system can be applied to audio signals of complex mixtures that contain the lead melody together with instrumental accompaniment. It is shown that including a lead melody estimation into a stateof- the-art system for cover song detection leads to promising results on a dataset of transcribed traditional dances from the island of Crete in Greece. Furthermore, a general framework that includes also rhythmic aspects is proposed. The proposed method represents a simple framework for the support of ethnomusicological studies on related forms of traditional music. © 2010 International Society for Music Information Retrieval."
Hillewaere R.; Manderick B.; Conklin D.,String quartet classification with monophonic models,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579972&partnerID=40&md5=79fb3913a6e04bd4920f1f8ca3f57f13,"Hillewaere R., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Manderick B., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Conklin D., Department of Computer Science and AI, Basque Foundation or Science, Universidad del Páis Vasco, Bilbao, San Sebastían, Spain","Polyphonic music classification remains a very challenging area in the field of music information retrieval. In this study, we explore the performance of monophonic models on single parts that are extracted from the polyphony. The presented method is specifically designed for the case of voiced polyphony, but can be extended to any type of music with multiple parts. On a dataset of 207 Haydn and Mozart string quartet movements, global feature models with standard machine learning classifiers are compared with a monophonic n-gram model for the task of composer recognition. Global features emerging from feature selection are presented, and future guidelines for the research of polyphonic music are outlined. © 2010 International Society for Music Information Retrieval."
Han Y.; Raphael C.,Informed source separation of orchestra and soloist,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862112409&partnerID=40&md5=20c1979e776154ba690cac4724a5bf5a,"Han Y., School of Informatics and Computing, Indiana University, Bloomington, United States; Raphael C., School of Informatics and Computing, Indiana University, Bloomington, United States","A novel technique of unmasking to repair the degradation in sources separated by spectrogram masking is proposed. Our approach is based on explicit knowledge of the musical audio at note level from a score-audio alignment, which we termed Informed Source Separation (ISS). Such knowledge allows the spectrogram energy to be decomposed into note-based models. We assume that a spectrogram mask for the solo is obtained and focus on the problem of repairing audio resulting from applying the mask. We evaluate the spectrogram as well as the harmonic structure of the music. We either search for unmasked (orchestra) partials of the orchestra to be transposed onto a masked (solo) region or reshape a solo partial with phase and amplitude imputed from unmasked regions. We describe a Kalman smoothing technique to decouple the phase and amplitude of a musical partial that enables the modification to the spectrogram. Audio examples from a piano concerto are available for evaluation. © 2010 International Society for Music Information Retrieval."
Moelants D.; Cornelis O.; Leman M.,Exploring African tone scales,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873687321&partnerID=40&md5=ce13827b29d7dbca3ed049b5290cfc61,"Moelants D., Ghent University, Belgium; Cornelis O., University College, Ghent, Belgium; Leman M., Ghent University, Belgium","Key-finding is a central topic in Western music analysis and development of MIR tools. However, most approaches rely on the Western 12-tone scale, which is not universally used. African music does not follow a fixed tone scale. In order to classify and study African tone scales, we developed a system in which the pitch is first analyzed on a continuous scale. Peak analysis is then applied on these data to extract the actual scale used. This system has been applied to a selection of African music, it allows us to look for similarities using cross-correlation. Thus it provides an interesting tool for query-by-example and database management in collections of ethnic music which can not be simply classified according to keys. Next to this the data can be used for ethnomusicological research. The study of the intervals used in this collection, e.g., gives us evidence for Western influence, with recent recordings having a tendency to use more regular intervals. © 2009 International Society for Music Information Retrieval."
Hu X.; Stephen Downie J.,When lyrics outperform audio for music mood classification: A feature analysis,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555199091&partnerID=40&md5=db4f91a47d3c2b8f5b16a5f915e18286,"Hu X., School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Stephen Downie J., School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","This paper builds upon and extends previous work on multi-modal mood classification (i.e., combining audio and lyrics) by analyzing in-depth those feature types that have shown to provide statistically significant improvements in the classification of individual mood categories. The dataset used in this study comprises 5,296 songs (with lyrics and audio for each) divided into 18 mood categories derived from user-generated tags taken from last.fm. These 18 categories show remarkable consistency with the popular Russell's mood model. In seven categories, lyric features significantly outperformed audio spectral features. In one category only, audio outperformed all lyric features types. A fine grained analysis of the significant lyric feature types indicates a strong and obvious semantic association between extracted terms and the categories. No such obvious semantic linkages were evident in the case where audio spectral features proved superior. © 2010 International Society for Music Information Retrieval."
Raczynski S.A.; Vincent E.; Bimbot F.; Sagayama S.,Multiple pitch transcription using DBN-based musicological models,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573353&partnerID=40&md5=6bdae665cc64fef40510a3a50420cee5,"Raczynski S.A., University of Tokyo, Bunkyo-ku, Tokyo 133-8656, 7-3-1 Hongo, Japan; Vincent E., INRIA Rennes, 35042 Rennes Cedex, Britagne Atlantique, France; Bimbot F., INRIA Rennes, 35042 Rennes Cedex, Britagne Atlantique, France; Sagayama S., University of Tokyo, Bunkyo-ku, Tokyo 133-8656, 7-3-1 Hongo, Japan","We propose a novel approach to solve the problem of estimating pitches of notes present in an audio signal. We have developed a probabilistically rigorous model that takes into account temporal dependencies between musical notes and between the underlying chords, as well as the instantaneous dependencies between chords, notes and the observed note saliences. We investigated its modeling ability by measuring the cross-entropy with symbolic (MIDI) data and then proceed to observe the model's performance in multiple pitch estimation of audio data. © 2010 International Society for Music Information Retrieval."
Abeßer J.; Bräuer P.; Lukashevich H.; Schuller G.,Bass playing style detection based on high-level features and pattern similarity,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053125108&partnerID=40&md5=db3777dde03905527c8967922f8889be,"Abeßer J., Fraunhofer IDMT, Ilmenau, Germany; Bräuer P., Piranha Musik and IT, Berlin, Germany; Lukashevich H., Fraunhofer IDMT, Ilmenau, Germany; Schuller G., Fraunhofer IDMT, Ilmenau, Germany","In this paper, we compare two approaches for automatic classification of bass playing styles, one based on highlevel features and another one based on similarity measures between bass patterns. For both approaches,we compare two different strategies: classification of patterns as a whole and classification of all measures of a pattern with a subsequent accumulation of the classification results. Furthermore, we investigate the influence of potential transcription errors on the classification accuracy, which tend to occur when real audio data is analyzed. We achieve best classification accuracy values of 60.8% for the feature-based classification and 68.5% for the classification based on pattern similarity based on a taxonomy consisting of 8 different bass playing styles. © 2010 International Society for Music Information Retrieval."
Chen Y.-X.; Klüber R.,ThumbnailDJ: Visual thumbnails of music content,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586667&partnerID=40&md5=438a2b9bb8e723bfdb318f3bc7286936,"Chen Y.-X., Media Informatics, University of Munich, 80333 Munich, Amalienstr. 17, Germany; Klüber R., Media Informatics, University of Munich, 80333 Munich, Amalienstr. 17, Germany","Musical perception is non-visual and people cannot describe what a song sounds like without listening to it. To facilitate music browsing and searching, we explore the automatic generation of visual thumbnails for music. Targeting an expert user groups, DJs, we developed a concept named ThumbnailDJ: Based on a metaphor of music notation, a visual thumbnail can be automatically generated for an audio file, including information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, and our concept was preferred most. Based on the results of this interview, we refined ThumbnailDJ and conducted an evaluation with DJs. The results confirmed that ThumbnailDJ can facilitate expert users browsing and searching within their music collection. © 2010 International Society for Music Information Retrieval."
Urbano J.; Marrero M.; Martín D.; Lloréns J.,Improving the generation of ground truths based on partially ordered lists,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053112542&partnerID=40&md5=c9416e55aaa2b06b6fefc06b42498bc4,"Urbano J., Department of Computer Science, University Carlos III, Madrid, Spain; Marrero M., Department of Computer Science, University Carlos III, Madrid, Spain; Martín D., Department of Computer Science, University Carlos III, Madrid, Spain; Lloréns J., Department of Computer Science, University Carlos III, Madrid, Spain","Ground truths based on partially ordered lists have been used for some years now to evaluate the effectiveness of Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity. However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations. In this paper we revise the methodology used to generate these ground truths and disclose some issues that need to be addressed. In particular, we focus on the arrangement and aggregation of the relevant results, and show that it is not possible to ensure lists completely consistent. We develop a measure of consistency based on Average Dynamic Recall and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method. The results of the MIREX 2005 evaluation are revisited using these alternative ground truths. © 2010 International Society for Music Information Retrieval."
Grachten M.; Widmer G.,Who is who in the end? Recognizing pianists by their final ritardandi,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873673455&partnerID=40&md5=25335c5623244b201fa3e083ee434131,"Grachten M., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","The performance of music usually involves a great deal of interpretation by the musician. In classical music, final ritardandi are emblematic for the expressive aspect of music performance. In this paper we investigate to what degree individual performance style has an effect on the form of final ritardandi. To this end we look at interonset-interval deviations from a performance norm. We define a criterion for filtering out deviations that are likely to be due to measurement error. Using a machine-learning classifier, we evaluate an automatic pairwise pianist identification task as an initial assessment of the suitability of the filtered data for characterizing the individual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, pianists can often be identified with accuracy significantly above baseline. © 2009 International Society for Music Information Retrieval."
Cunningham S.J.; Nichols D.M.,Exploring social music behavior: An investigation of music selection at parties,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873674146&partnerID=40&md5=9537100c310ccf9aef8a2534ee95c60d,"Cunningham S.J., Department of Computer Science, University of Waikato, Hamilton, New Zealand; Nichols D.M., Department of Computer Science, University of Waikato, Hamilton, New Zealand","This paper builds an understanding how music is currently listened to by small (fewer than 10 individuals) to medium-sized (10 to 40 individuals) gatherings of people- how songs are chosen for playing, how the music fits in with other activities of group members, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of participant observations and interviews focusing on the selection of songs to play at social gatherings. We suggest features for software to support music playing at parties. © 2009 International Society for Music Information Retrieval."
Han B.-J.; Rho S.; Dannenberg R.B.; Hwang E.,SMERS: Music emotion recognition using support vector regression,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873695101&partnerID=40&md5=5870e81c1ec255c9e49da837f162a777,"Han B.-J., School of Electrical Engineering, Korea University, South Korea; Rho S., School of Electrical Engineering, Korea University, South Korea; Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States; Hwang E., School of Electrical Engineering, Korea University, South Korea","Music emotion plays an important role in music retrieval, mood detection and other music-related applications. Many issues for music emotion recognition have been addressed by different disciplines such as physiology, psychology, cognitive science and musicology. We present a support vector regression (SVR) based music emotion recognition system. The recognition process consists of three steps: (i) seven distinct features are extracted from music; (ii) those features are mapped into eleven emotion categories on Thayer's two-dimensional emotion model; (iii) two regression functions are trained using SVR and then arousal and valence values are predicted. We have tested our SVR-based emotion classifier in both Cartesian and polar coordinate system empirically. The result indicates the SVR classifier in the polar representation produces satisfactory result which reaches 94.55% accuracy superior to the SVR (in Cartesian) and other machine learning classification algorithms such as SVM and GMM. © 2009 International Society for Music Information Retrieval."
Miotto R.; Barrington L.; Lanckriet G.,Improving auto-tagging by modeling semantic co-occurrences,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584760&partnerID=40&md5=c84e004205bb2ed7e04587657f5a6ede,"Miotto R., University of Padova, Italy; Barrington L., UC San Diego, United States; Lanckriet G., UC San Diego, United States",Automatic taggers describe music in terms of a multinomial distribution over relevant semantic concepts. This paper presents a framework for improving automatic tagging of music content by modeling contextual relationships between these semantic concepts. The framework extends existing auto-tagging methods by adding a Dirichlet mixture to model the contextual co-occurrences between semantic multinomials. Experimental results show that adding context improves automatic annotation and retrieval of music and demonstrate that the Dirichlet mixture is an appropriate model for capturing co-occurrences between semantics. © 2010 International Society for Music Information Retrieval.
Wołkowicz J.; Kešelj V.,Predicting development of research in music based on parallels with natural language processing,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952011219&partnerID=40&md5=a6ca65de8993f2e7a0003503f13acabe,"Wołkowicz J., Faculty of Computer Science, Dalhousie University, Canada; Kešelj V., Faculty of Computer Science, Dalhousie University, Canada","The hypothesis of the paper is that the domain of Natural Languages Processing (NLP) resembles current research in music so one could benefit from this by employing NLP techniques to music. In this paper the similarity between both domains is described. The levels of NLP are listed with pointers to respective tasks within the research of computational music. A brief introduction to history of NLP enables locating music research in this history. Possible directions of research in music, assuming its affinity to NLP, are introduced. Current research in generational and statistical music modeling is compared to similar NLP theories. The paper is concluded with guidelines for music research and information retrieval. © 2010 International Society for Music Information Retrieval."
Schedl M.,On the use of microblogging posts for similarity estimation and artist labeling,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869153307&partnerID=40&md5=e58d3fcae55a7c25fd40452b3020d867,"Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Microblogging services, such as Twitter, have risen enormously in popularity during the past years. Despite their popularity, such services have never been analyzed for MIR purposes, to the best of our knowledge. We hence present first investigations of the usability of music artist-related microblogging posts to perform artist labeling and similarity estimation tasks. To this end, we look into different text-based indexing models and term weighting measures. Two artist collections are used for evaluation, and the different methods are evaluated against data from last.fm. We show that microblogging posts are a valuable source for musical meta-data. © 2010 International Society for Music Information Retrieval."
Hankinson A.; Pugin L.; Fujinaga I.,An interchange format for optical music recognition applications,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591058&partnerID=40&md5=475d20ea40c63c2b2b1818c1511e0783,"Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada; Pugin L., RISM Switzerland, Geneva University, Switzerland; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada","Page appearance and layout for music notation is a critical component of the overall musical information contained in a document. To capture and transfer this information, we outline an interchange format for OMR applications, the OMR Interchange Package (OIP) format, which is designed to allow layout information and page images to be preserved and transferred along with semantic musical content. We identify a number of uses for this format that can enhance digital representations of music, and introduce a novel idea for distributed optical music recognition system based on this format. © 2010 International Society for Music Information Retrieval."
Sammartino S.; Tardón L.J.; De La Bandera C.; Barbancho I.; Barbancho A.M.,The standardized variogram as a novel tool for audio similarity measure,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865508167&partnerID=40&md5=a3641cab68489aa9155423763e91d0a4,"Sammartino S., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; Tardón L.J., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; De La Bandera C., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; Barbancho I., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; Barbancho A.M., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain","Most of methods for audio similarity evaluation are based on the Mel frequency cepstral coefficients, employed as main tool for the characterization of audio contents. Such approach needs some way of data compression aimed to optimize the information retrieval task and to reduce the computational costs derived from the usage of cluster analysis tools and probabilistic models. A novel approach is presented in this paper, based on the standardized variogram. This tool, inherited from Geostatistics, is applied to MFCCs matrices to reduce their size and compute compact representations of the audio contents (song signatures), aimed to evaluate audio similarity. The performance of the proposed approach is analyzed in comparison with other alternative methods and on the base of human responses. © 2010 International Society for Music Information Retrieval."
Lagrange M.; Serrà J.,Unsupervised accuracy improvement for cover song detection using spectral connectivity network,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859883644&partnerID=40&md5=2031e2fc302f57035943de13f2b8ac7c,"Lagrange M., IRCAM-CNRS UMR 9912, 75004 Paris, 1 place Igor Stravinsky, France; Serrà J., Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Roc Boronat 138, Spain","This paper introduces a new method for improving the accuracy in medium scale music similarity problems. Recently, it has been shown that the raw accuracy of query by example systems can be enhanced by considering priors about the distribution of its output or the structure of the music collection being considered. The proposed approach focuses on reducing the dependency to those priors by considering an eigenvalue decomposition of the aforementioned system's output. Experiments carried out in the framework of cover song detection show that the proposed approach has good performance for enhancing a high accuracy system. Furthermore, it maintains the accuracy level for lower performing systems. © 2010 International Society for Music Information Retrieval."
Hrybyk A.; Kim Y.,Combined audio and video analysis for guitar chord identification,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574298&partnerID=40&md5=ac5f5888add16d3866d6e75c1bbdde36,"Hrybyk A., Department of Electrical and Computer Engineering, Drexel University, United States; Kim Y., Department of Electrical and Computer Engineering, Drexel University, United States","This paper presents a multi-modal approach to automatically identifying guitar chords using audio and video of the performer. Chord identification is typically performed by analyzing the audio, using a chroma based feature to extract pitch class information, then identifying the chord with the appropriate label. Even if this method proves perfectly accurate, stringed instruments add extra ambiguity as a single chord or melody may be played in different positions on the fretboard. Preserving this information is important, because it signifies the original fingering, and implied ""easiest"" way to perform the selection. This chord identification system combines analysis of audio to determine the general chord scale (i.e. A major, G minor), and video of the guitarist to determine chord voicing (i.e. open, barred, inversion), to accurately identify the guitar chord. © 2010 International Society for Music Information Retrieval."
Kelly C.; Gainza M.; Dorran D.; Coyle E.,Locating tune changes and providing a semantic labelling of sets of irish traditional tunes,2010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576596&partnerID=40&md5=276b1d4b885633ff27991518e4e4cd81,"Kelly C., Audio Research Group, Dublin 8, DIT Kevin St., Ireland; Gainza M., Audio Research Group, Dublin 8, DIT Kevin St., Ireland; Dorran D., Audio Research Group, Dublin 8, DIT Kevin St., Ireland; Coyle E., Audio Research Group, Dublin 8, DIT Kevin St., Ireland","An approach is presented which provides the tune change locations within a set of Irish Traditional tunes. Also provided are semantic labels for each part of each tune within the set. A set in Irish Traditional music is a number of individual tunes played segue. Each of the tunes in the set are made up of structural segments called parts. Musical variation is a prominent characteristic of this genre. However, a certain set of notes known as 'set accented tones' are considered impervious to musical variation. Chroma information is extracted at 'set accented tone' locations within the music. The resulting chroma vectors are grouped to represent the parts of the music. The parts are then compared with one another to form a part similarity matrix. Unit kernels which represent the possible structures of an Irish Traditional tune are matched with the part similarity matrix to determine the tune change locations and semantic part labels. © 2010 International Society for Music Information Retrieval."
Degara-Quintela N.; Pena A.; Torres-Guijarro S.,A comparison of score-level fusion rules for onset detection in music signals,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591406&partnerID=40&md5=c586e95ca5889dd9e991dbcdf7155681,"Degara-Quintela N., Department of Signal Theory and Communications, University of Vigo, Spain; Pena A., Department of Signal Theory and Communications, University of Vigo, Spain; Torres-Guijarro S., Laboratorio Oficial de Metroloxía de Galicia (LOMG), Spain","Finding automatically the starting time of audio events is a difficult process. A promising approach for onset detection lies in the combination of multiple algorithms. The goal of this paper is to compare score-level fusion rules that combine signal processing algorithms in a problem of automatic detection of onsets. Previous approaches usually combine detection functions by adding these functions in the time domain. The combination methods explored in this work fuse, at score-level, the peak score information (peak time and onset probability) in order to obtain a better estimate of the probability of having an onset given the probability estimates of multiple experts. Three state-ofthe- art spectral-based onset detection functions are used: a spectral flux detection function, a weighted phase deviation function, and a complex domain detection function. Both untrained and trained fusion rules will be compared using a standard data set of music excerpts. © 2009 International Society for Music Information Retrieval."
Oudre L.; Grenier Y.; Févotte C.,Template-based chord recognition: Influence of the chord types,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873639352&partnerID=40&md5=6c09874c43fb4c99464dc869fd947dbf,"Oudre L., Institut TELECOM, TELECOM ParisTech, CNRS LTCI, 75014 Paris, 37-39 rue Dareau, France; Grenier Y., Institut TELECOM, TELECOM ParisTech, CNRS LTCI, 75014 Paris, 37-39 rue Dareau, France; Févotte C., CNRS LTCI, TELECOM ParisTech, 75014 Paris, 37-39 rue Dareau, France","This paper describes a fast and efficient template-based chord recognition method. We introduce three chord models taking into account one or more harmonics for the notes of the chord. The use of pre-determined chord models enables to consider several types of chords (major, minor, dominant seventh, minor seventh, augmented, diminished...). After extracting a chromagram from the signal, the detected chord over a frame is the one minimizing a measure of fit between the chromagramframe and the chord templates. Several popular measures in the probability and signal processing field are considered for our task. In order to take into account the time persistence, we perform a post-processing filtering over the recognition criteria. The transcription tool is evaluated on the 13 Beatles albums with different chord types and compared to state-of-theart chord recognition methods. We particularly focus on the influence of the chord types considered over the performances of the system. Experimental results show that our method outperforms the state-of-the-art and more importantly is less computationally demanding than the other evaluated systems. © 2009 International Society for Music Information Retrieval."
Gross-Amblard D.; Rigaux P.; Abrouk L.; Cullot N.,Fingering watermarking in symbolic digital scores,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873649830&partnerID=40&md5=0f5e731991ea91d41ca21614d4ce6691,"Gross-Amblard D., Le2i-CNRS Lab, Universit́e de Bourgogne, France; Rigaux P., Lamsade-CNRS Lab, Universit́e de Dauphine, Paris IX, France; Abrouk L., Le2i-CNRS Lab, Universit́e de Bourgogne, France; Cullot N., Le2i-CNRS Lab, Universit́e de Bourgogne, France","We propose a new watermarking method that hides the writer's identity into symbolic musical scores featuring fingering annotations. These annotations constitute a valuable part of the symbolic representation, yet they can be slightly modified without altering the quality of the musical information. The method applies a controlled distortion of the existing fingerings so that unauthorized copies can be identified. The proposed watermarkingmethod is robust against attacks like random fingering alterations and score cropping, and its detection does not require the original fingering, but only the suspect one. The method is general and applies to various fingering contexts and instruments. Keywords. Watermarking, fingering. © 2009 International Society for Music Information Retrieval."
Heittola T.; Klapuri A.; Virtanen T.,Musical instrument recognition in polyphonic audio using source-filter model for sound separation,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873616077&partnerID=40&md5=02395740c38008640c5418a9ce499ca6,"Heittola T., Deparment of Signal Processing, Tampere University of Technology, Finland; Klapuri A., Deparment of Signal Processing, Tampere University of Technology, Finland; Virtanen T., Deparment of Signal Processing, Tampere University of Technology, Finland","This paper proposes a novel approach to musical instrument recognition in polyphonic audio signals by using a source-filter model and an augmented non-negative matrix factorization algorithm for sound separation. The mixture signal is decomposed into a sum of spectral bases modeled as a product of excitations and filters. The excitations are restricted to harmonic spectra and their fundamental frequencies are estimated in advance using a multipitch estimator, whereas the filters are restricted to have smooth frequency responses by modeling them as a sum of elementary functions on the Mel-frequency scale. The pitch and timbre information are used in organizing individual notes into sound sources. In the recognition, Mel-frequency cepstral coefficients are used to represent the coarse shape of the power spectrum of sound sources and Gaussian mixture models are used to model instrument-conditional densities of the extracted features. The method is evaluated with polyphonic signals, randomly generated from 19 instrument classes. The recognition rate for signals having six note polyphony reaches 59%. © 2009 International Society for Music Information Retrieval."
Grosche P.; Müller M.,A mid-level representation for capturing dominant tempo and pulse information in music recordings,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873626279&partnerID=40&md5=4ebef0063268a64ec91b2b732432890b,"Grosche P., MPI Informatik, Saarland University, Saarbrücken, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany","Automated beat tracking and tempo estimation from music recordings become challenging tasks in the case of nonpercussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation which captures predominant local pulse information. To this end, we first derive a tempogram by performing a local spectral analysis on a previously extracted, possibly very noisy onset representation. From this, we derive for each time position the predominant tempo as well as a sinusoidal kernel that best explains the local periodic nature of the onset representation. Then, our main idea is to accumulate the local kernels over time yielding a single function that reveals the predominant local pulse (PLP). We show that this function constitutes a robust mid-level representation from which one can derive musically meaningful tempo and beat information for non-percussive music even in the presence of significant tempo fluctuations. Furthermore, our representation allows for incorporating prior knowledge on the expected tempo range to exhibit information on different pulse levels. © 2009 International Society for Music Information Retrieval."
Duan Z.; Han J.; Pardo B.,Harmonically informed multi-pitch tracking,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052983176&partnerID=40&md5=d24843e3f1518db9d8a0153a9db6d5f4,"Duan Z., Northwestern University, Evanston IL, United States; Han J., Northwestern University, Evanston IL, United States; Pardo B., Northwestern University, Evanston IL, United States","This paper presents a novel system for multi-pitch tracking, i.e. estimate the pitch trajectory of each monophonic source in a mixture of harmonic sounds. The system consists of two stages: multi-pitch estimation and pitch trajectory formation. In the first stage, we propose a new approach based on modeling spectral peaks and non-peak regions to estimate pitches and polyphony in each single frame. In the second stage, we view the pitch trajectory formation problem as a constrained clustering problem of pitch estimates in all the frames. Constraints are imposed on some pairs of pitch estimates, according to time and frequency proximity. In clustering, harmonic structure is employed as the feature. The proposed system is tested on 10 recorded four-part J. S. Bach chorales. Both multi-pitch estimation and tracking results are very promising. In addition, for multi-pitch estimation, the proposed system is shown to outperform a state-of-the-art multi-pitch estimation approach. © 2009 International Society for Music Information Retrieval."
Battenberg E.; Wessel D.,Accelerating non-negative matrix factorization for audio source separation on multi-core and many-core architectures,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955589746&partnerID=40&md5=6bd1e50274b66d2961d781f2aa7a97d2,"Battenberg E., Parallel Computing Laboratory, University of California, Berkeley, United States; Wessel D., Center for New Music and Audio Technologies, University of California, Berkeley, United States","Non-negative matrix factorization (NMF) has been successfully used in audio source separation and parts-based analysis; however, iterative NMF algorithms are computationally intensive, and therefore, time to convergence is very slow on typical personal computers. In this paper, we describe high performance parallel implementations of NMF developed using OpenMP for shared-memory multicore systems and CUDA for many-core graphics processors. For 20 seconds of audio, we decrease running time from 18.5 seconds to 2.6 seconds using OpenMP and 0.6 seconds using CUDA. These performance increases allow source separation to be carried out on entire songs in a number of seconds, a process which was previously impractical with respect to time. We give insight into how such significant speed gains were made and encourage the development and use of parallel music information retrieval software. © 2009 International Society for Music Information Retrieval."
Hamanaka M.; Tojo S.,Interactive GTTM analyzer,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867621903&partnerID=40&md5=8ed3453e1840a1d0571196287ab7e7fe,"Hamanaka M., University of Tsukuba, Japan; Tojo S., Japan Advanced Institute of Science and Technology, Japan","We describe an interactive analyzer for the generative theory of tonal music (GTTM). Generally, a piece of music has more than one interpretation, and dealing with such ambiguity is one of the major problems when constructing a music analysis system. To solve this problem, we propose an interactive GTTM analyzer, called an automatic time-span tree analyzer (ATTA), with a GTTM manual editor. The ATTA has adjustable parameters that enable the analyzer to generate multiple analysis results. As the ATTA cannot output all the analysis results that correspond to all the interpretations of a piece of music, we designed a GTTM manual editor, which generates all the analysis results. Experimental results showed that our interactive GTTM analyzer outperformed the GTTM manual editor without an ATTA. Since we hope to contribute to the research of music analysis, we publicize our interactive GTTM analyzer and a dataset of three hundred pairs of a score and analysis results by musicologist on our website http://music.iit.tsukuba.ac.jp/hamanaka/gttm.htm, which is the largest database of analyzed results from the GTTM to date. © 2009 International Society for Music Information Retrieval."
Stolzenburg F.,A periodicity-based theory for harmony perception and scales,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871313430&partnerID=40&md5=a72b1868b2a0a825ce21f53f68d1d2d9,"Stolzenburg F., Automation and Computer Sciences Department, Hochschule Harz, 38855 Wernigerode, Germany","Empirical results demonstrate, that human subjects rate harmonies, e.g. major and minor triads, differently with respect to their sonority. These judgements of listeners have a strong psychophysical basis. Therefore, harmony perception often is explained by the notions of dissonance and tension, computing the consonance of one or two intervals. In this paper, a theory on harmony perception based on the notion of periodicity is introduced. Mathematically, periodicity is derivable from the frequency ratios of the tones in the chord with respect to its lowest tone. The used ratios can be computed by continued fraction expansion and are psychophysically motivated by the just noticeable differences in pitch perception. The theoretical results presented here correlate well to experimental results and also explain the origin of complex chords and common musical scales. © 2009 International Society for Music Information Retrieval."
Izmirli Ö.,Tonal-atonal classification of music audio using diffusion maps,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873650382&partnerID=40&md5=9bf4a0b1a913ff5f5431291dce18c153,"Izmirli Ö., Computer Science Department, Center for Arts and Technology, Connecticut College, United States","In this paper we look at the problem of classifying music audio as tonal or atonal by learning a low-dimensional structure representing tonal relationships among keys. We use a training set composed of tonal pieces which includes all major and minor keys. A kernel eigenmap based method is used for structure learning and discovery. Specifically, a Diffusion Maps (DM) framework is used and its parameter tuning is discussed. Since these methods do not scale well with increasing data size, it becomes infeasible to use these methods in online applications. In order to facilitate on-line classification an outof- sample extension to the DM framework is given. The learned structure of tonal relationships is presented and a simple scheme for classification of tonal-atonal pieces is proposed. Evaluation results show that the method is able to perform at an accuracy above 90% with the current data set. © 2009 International Society for Music Information Retrieval."
Bay M.; Ehmann A.F.; Stephen Downie J.,Evaluation of multiple-F0 estimation and tracking systems,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632675&partnerID=40&md5=0a869fe3508cd3cf9bae1e612949b34c,"Bay M., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Stephen Downie J., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music information retrieval systems. This paper presents the systematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music. The evaluations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008. The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed. © 2009 International Society for Music Information Retrieval."
Serrà J.; Zanin M.; Laurier C.; Sordo M.,Unsupervised detection of cover song sets: Accuracy improvement and original identification,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859911465&partnerID=40&md5=699e77b7b47758aca0de0637e7c017e7,"Serrà J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Zanin M., Universidad Autó noma de Madrid, Madrid, Spain; Laurier C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The task of identifying cover songs has formerly been studied in terms of a prototypical query retrieval framework. However, this framework is not the only one the task allows. In this article, we revise the task of identifying cover songs to include the notion of sets (or groups) of covers. In particular, we study the application of unsupervised clustering and community detection algorithms to detect cover sets. We consider current state-of-the-art algorithms and propose new methods to achieve this goal. Our experiments show that the detection of cover sets is feasible, that it can be performed in a reasonable amount of time, that it does not require extensive parameter tuning, and that it presents certain robustness to inaccurate measurements. Furthermore, we highlight two direct outcomes that naturally arise from the proposed framework revision: increasing the accuracy of query retrieval-based systems and detecting the original song within a set of covers. © 2009 International Society for Music Information Retrieval."
Niitsuma M.; Fujinami T.; Tomita Y.,The intersection of computational analysis and music manuscripts: A newmodel for bach source studies of the 21st century,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873635025&partnerID=40&md5=3560a36d1f356a888e957c86631ecb48,"Niitsuma M., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom; Fujinami T., Japan Advanced Institute of Science and Technology (JAIST), School of Knowledge Science, Japan; Tomita Y., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom","This paper addresses the intersection of computational analysis and musicological source studies. In musicology, scholars often find themselves in the situation where their methodologies are inadequate to achieve their goals. Their problems appear to be twofold: (1) the lack of scientific objectivity and (2) the over-reliance on new source discoveries. We propose three stages to resolve these problems, a preliminary result of which is shown. The successful outcome of this work will have a huge impact not only on musicology but also on a wide range of subjects. © 2009 International Society for Music Information Retrieval."
Bohak C.; Marolt M.,Calculating similarity of folk song variants with melody-based features,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865816822&partnerID=40&md5=5e84d2fe1bcb00c66a6e07b8c9c467a8,"Bohak C., Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Marolt M., Faculty of Computer and Information Science, University of Ljubljana, Slovenia","As folk songs live largely through oral transmission, there usually is no standard form of a song - each performance of a folk song may be unique. Different interpretations of the same song are called song variants, all variants of a song belong to the same variant type. In the paper, we explore how various melody-based features relate to folk song variants. Specifically, we explore whether we can derive a melodic similarity measure that would correlate to variant types in the sense that it would measure songs belonging to the same variant type as more similar, in contrast to songs from different variant types. The measure would be useful for folk song retrieval based on variant types, classification of unknown tunes, as well as a measure of similarity between variant types. We experimented with a number of melodic features calculated from symbolic representations of folk song melodies and combined them into a melodybased folk song similarity measure. We evaluated the measure on the task of classifying an unknown melody into a set of existing variant types. We show that the proposed measure gives the correct variant type in the top 10 list for 68% of queries in our data set. © 2009 International Society for Music Information Retrieval."
Knees P.; Pohle T.; Schedl M.; Schnitzer D.; Seyerlehner K.; Widmer G.,Augmenting text-based music retrieval with audio similarity,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863768356&partnerID=40&md5=1550062b1be15496e265251d48a324b3,"Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schnitzer D., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Vienna, Austria; Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Vienna, Austria","We investigate an approach to a music search engine that indexesmusic pieces based on relatedWeb documents. This allows for searching for relevant music pieces by issuing descriptive textual queries. In this paper, we examine the effects of incorporating audio-based similarity into the text-based ranking process - either by directly modifying the retrieval process or by performing post-hoc audiobased re-ranking of the search results. The aimof this combination is to improve ranking quality by including relevant tracks that are left out by text-based retrieval approaches. Our evaluations show overall improvements but also expose limitations of these unsupervised approaches to combining sources. Evaluations are carried out on two collections, one large real-world collection containing about 35,000 tracks and on the CAL500 set. © 2009 International Society for Music Information Retrieval."
Benetos E.; Holzapfel A.; Stylianou Y.,Pitched instrument onset detection based on auditory spectra,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873668480&partnerID=40&md5=680d7df1d9104c32b8649d59220e213c,"Benetos E., Institute of Computer Science, FORTH, Greece, Computer Science Department, Multimedia Informatics Lab, University of Crete, Greece; Holzapfel A., Institute of Computer Science, FORTH, Greece, Computer Science Department, Multimedia Informatics Lab, University of Crete, Greece; Stylianou Y., Institute of Computer Science, FORTH, Greece, Computer Science Department, Multimedia Informatics Lab, University of Crete, Greece","In this paper, a novel method for onset detection of music signals using auditory spectra is proposed. The auditory spectrogram provides a time-frequency representation that employs a sound processing model resembling the human auditory system. Recent work on onset detection employs DFT-based features, such as the spectral flux and group delay function. The spectral flux and group delay are introduced in the auditory framework and an onset detection algorithm is proposed. Experiments are conducted on a dataset covering 11 pitched instrument types, consisting of 1829 onsets in total. Results indicate the superiority of the auditory representations over the DFT-based ones, with the auditory spectral flux exhibiting an onset detection improvement by 2% in terms of F-measure when compared to the DFT-based feature. © 2009 International Society for Music Information Retrieval."
Koenigstein N.; Shavitt Y.,Song ranking based on piracy in peer-to-peer networks,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873618528&partnerID=40&md5=a70ca4551815245d194bdba24e7ba712,"Koenigstein N., School of Electrical Engineering, Tel Aviv University, Israel; Shavitt Y., School of Electrical Engineering, Tel Aviv University, Israel","Music sales are loosing their role as a means for music dissemination but are still used by the music industry for ranking artist success, e.g., in the Billboard Magazine chart. Thus, it was suggested recently to use social networks as an alternative ranking system; a suggestion which is problematic due to the ease of manipulating the list and the difficulty of implementation. In this work we suggest to use logs of queries from peer-to-peer file-sharing systems for ranking song success. We show that the trend and fluctuations of the popularity of a song in the Billboard list have strong correlation (0.89) to the ones in a list built from the P2P network, and that the P2P list has a week advantage over the Billboard list. Namely, music sales are strongly correlated with music piracy. © 2009 International Society for Music Information Retrieval."
Hamel P.; Wood S.; Eck D.,Automatic identification of instrument classes in polyphonic and poly-instrument audio,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873649320&partnerID=40&md5=2addf1dc18c3c9b37570b16ff636f8a8,"Hamel P., Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada; Wood S., Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada; Eck D., Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada","We present and compare several models for automatic identification of instrument classes in polyphonic and poly-instrument audio. The goal is to be able to identify which categories of instrument (Strings, Woodwind, Guitar, Piano, etc.) are present in a given audio example. We use a machine learning approach to solve this task. We constructed a system to generate a large database of musically relevant poly-instrument audio. Our database is generated from hundreds of instruments classified in 7 categories. Musical audio examples are generated by mixing multi-track MIDI files with thousands of instrument combinations. We compare three different classifiers : a Support Vector Machine (SVM), a Multilayer Perceptron (MLP) and a Deep Belief Network (DBN).We show that the DBN tends to outperform both the SVM and the MLP in most cases. © 2009 International Society for Music Information Retrieval."
Hirjee H.; Brown D.G.,Automatic detection of internal and imperfect rhymes in rap lyrics,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576245&partnerID=40&md5=1a37da783c8ce33e0d11e4d969e850ef,"Hirjee H., Cheriton School of Computer Science, University of Waterloo, Canada; Brown D.G., Cheriton School of Computer Science, University of Waterloo, Canada","Imperfect and internal rhymes are two important features in rap music often ignored in the music information retrieval community. We develop a method of scoring potential rhymes using a probabilistic model based on phoneme frequencies in rap lyrics. We use this scoring scheme to automatically identify internal and line-final rhymes in song lyrics and demonstrate the performance of this method compared to rules-based models. Higher level rhyme features are produced and used to compare rhyming styles in song lyrics from different genres, and for different rap artists. © 2009 International Society for Music Information Retrieval."
Bas De Haas W.; Rohrmeier M.; Veltkamp R.C.; Wiering F.,Modeling harmonic similarity using a generative grammar of tonal harmony,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957957955&partnerID=40&md5=b033a39cf7b9009bd18bb3f3b899e628,"Bas De Haas W., Utrecht University, Netherlands; Rohrmeier M., University of Cambridge, United Kingdom; Veltkamp R.C., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands","In this paper we investigate a new approach to the similarity of tonal harmony. We create a fully functional remodeling of an earlier version of Rohrmeier's grammar of harmony. With this grammar an automatic harmonic analysis of a sequence of symbolic chord labels is obtained in the form of a parse tree. The harmonic similarity is determined by finding and examining the largest labeled common embeddable subtree (LLCES) of two parse trees. For the calculation of the LLCES a new O(min(n,m)nm) time algorithm is presented, where n and m are the sizes of the trees. For the analysis of the LLCES we propose six distance measures that exploit several structural characteristics of the Combined LLCES. We demonstrate in a retrieval experiment that at least one of these new methods significantly outperforms a baseline string matching approach and thereby show that using additional musical knowledge from music cognitive and music theoreticmodels actually helps improving retrieval performance. © 2009 International Society for Music Information Retrieval."
Fremerey C.; Clausen M.; Ewert S.; Müller M.,Sheet music-audio identification,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865827312&partnerID=40&md5=e20a0b1c87d86e2969fe247f163797a6,"Fremerey C., Computer Science III, Bonn University, Bonn, Germany; Clausen M., Computer Science III, Bonn University, Bonn, Germany; Ewert S., Computer Science III, Bonn University, Bonn, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany","In this paper, we introduce and discuss the task of sheet music-audio identification. Given a query consisting of a sequence of bars from a sheet music representation, the task is to find corresponding sections within an audio interpretation of the same piece. Two approaches are proposed: a semi-automatic approach using synchronization and a fully automatic approach using matching techniques. A workflow is described that allows for evaluating the matching approach using the results of the more reliable synchronization approach. This workflow makes it possible to handle even complex queries from orchestral scores. Furthermore, we present an evaluation procedure, where we investigate several matching parameters and tempo estimation strategies. Our experiments have been conducted on a dataset comprising pieces of various instrumentations and complexity. © 2009 International Society for Music Information Retrieval."
Bade K.; Nürnberger A.; Stober S.; Garbers J.; Wiering F.,Supporting folk-song research by automatic metric learning and ranking,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582087&partnerID=40&md5=5576478a2ef7526140250b4be0201cd0,"Bade K., Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; Nürnberger A., Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; Stober S., Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; Garbers J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Department of Information and Computing Sciences, Utrecht University, Netherlands","In folk song research, appropriate similarity measures can be of great help, e.g. for classification of new tunes. Several measures have been developed so far. However, a particular musicological way of classifying songs is usually not directly reflected by just a single one of these measures. We show how a weighted linear combination of different basic similarity measures can be automatically adapted to a specific retrieval task by learning this metric based on a special type of constraints. Further, we describe how these constraints are derived from information provided by experts. In experiments on a folk song database, we show that the proposed approach outperforms the underlying basic similarity measures and study the effect of different levels of adaptation on the performance of the retrieval system. © 2009 International Society for Music Information Retrieval."
Liem C.C.S.; Hanjalic A.,Cover song retrieval: A comparative study of system component choices,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873639481&partnerID=40&md5=79b9a2a2b5842895698d1d28eea55326,"Liem C.C.S., Department of Mediamatics, Delft University of Technology, Netherlands; Hanjalic A., Department of Mediamatics, Delft University of Technology, Netherlands","The Cover Song Retrieval (CSR) problem has received considerable attention in the MIREX 2006-2008 evaluation sessions. While the reported performance figures provide a general idea about the strengths of the submitted systems, it is not clear what actually causes the reported performance of a certain system. In other words, the question arises whether some system component design choices are more critical for a system's performance results than others. In order to obtain a better understanding of the performance of current CSR approaches and to give recommendations for future research in the field of CSR, we designed and performed a comparative study involving system component design approaches from the best-performing systems in MIREX 2006 and 2007. The datasets used for evaluation were carefully chosen to cover the broad spectrum of the cover song domain, while still providing designated test cases. While the choice of the dissimilarity assessment method was found to cause the largest CSR performance boost and very good retrieval results were obtained on classical opus retrieval cases, results obtained on a new test case, involving recordings originating from different microphone sets, point out new challenges in optimizing the feature representation step. © 2009 International Society for Music Information Retrieval."
Hsu C.-L.; Chen L.-Y.; Jang J.-S.R.; Li H.-J.,Singing pitch extraction from monaural polyphonic songs by contextual audio modeling and singing harmonic enhancement,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867604169&partnerID=40&md5=3a1f99e5e62e7ab5246f256ff0109044,"Hsu C.-L., Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; Chen L.-Y., Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; Jang J.-S.R., Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; Li H.-J., Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Taiwan","This paper proposes a novel approach to extract the pitches of singing voices from monaural polyphonic songs. The hidden Markov model (HMM) is adopted to model the transition between adjacent singing pitches in time, and the relationships between melody and its chord, which is implicitly represented by features extracted from the spectrum. Moreover, another set of features which represents the energy distribution of the enhanced singing harmonic structure is proposed by applying a normalized sub-harmonic summation technique. By using these two feature sets with complementary characteristics, a 2- stream HMM is constructed for singing pitch extraction. Quantitative evaluation shows that the proposed system outperforms the compared approaches for singing pitch extraction from polyphonic songs. © 2009 International Society for Music Information Retrieval."
Diakopoulos D.; Vallis O.; Hochenbaum J.; Murphy J.; Kapur A.,21ST century electronica: MIR techniques for classification and performance,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053113331&partnerID=40&md5=47909180c69f5c63e227e302b74aa067,"Diakopoulos D., California Institute of Arts, Valencia, CA, United States; Vallis O., New Zealand School of Music, Wellington, New Zealand; Hochenbaum J., New Zealand School of Music, Wellington, New Zealand; Murphy J., California Institute of Arts, Valencia, CA, United States; Kapur A., New Zealand School of Music, Wellington, New Zealand","The performance of electronica by Disc Jockys (DJs) presents a unique opportunity to develop interactions between performer and music. Through recent research in the MIR field, new tools for expanding DJ performance are emerging. The use of spectral, loudness, and temporal descriptors for the classification of electronica is explored. Our research also introduces the use of a multitouch interface to drive a performance-oriented DJ application utilizing the feature set. Furthermore, we present that a multi-touch surface provides an extensible and collaborative interface for browsing and manipulating MIRrelated data in real time. © 2009 International Society for Music Information Retrieval."
Gruhne M.; Dittmar C.; Gaertner D.,Improving rhythmic similarity computation by beat histogram transformations,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866006511&partnerID=40&md5=d844bc60d3042f4a41b71ab9385723c6,"Gruhne M., Bach Technology AS, Germany; Dittmar C., Fraunhofer IDMT, Germany; Gaertner D., Fraunhofer IDMT, Germany","Rhythmic descriptors are often utilized for semantic music classification, such as genre recognition or tempo detection. Several algorithms dealing with the extraction of rhythmic information from music signals were proposed in literature. Most of them derive a so-called beat histogram by auto-correlating a representation of the temporal envelope of the music signal. To circumvent the problem of tempo dependency, post-processing via higher-order statistics has been reported. Tests concluded, that these statistics are still tempo dependent to a certain extent. This paper describes a method, which transforms the original auto-correlated envelope into a tempo-independent rhythmic feature vector by multiplying the lag-axis with a stretch factor. This factor is computed with a new correlation technique which works in the logarithmic domain. The proposed method is evaluated for rhythmic similarity, consisting of two tasks: One test with manually created rhythms as proof of concept and another test using a large realworld music archive. © 2009 International Society for Music Information Retrieval."
Schnitzer D.; Flexer A.; Widmer G.,A filter-and-refine indexing method for fast similarity search in millions of music tracks,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869799881&partnerID=40&md5=847ebe1fbd6f6a862f84a1c1a0028202,"Schnitzer D., Austrian Research Institute for Artificial Intelligence, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence, Austria; Widmer G., Johannes Kepler University, Linz, Austria",We present a filter-and-refine method to speed up acoustic audio similarity queries which use the Kullback-Leibler divergence as similarity measure. The proposed method rescales the divergence and uses a modified FastMap [1] implementation to accelerate nearest-neighbor queries. The search for similar music pieces is accelerated by a factor of 10-30 compared to a linear scan but still offers high recall values (relative to a linear scan) of 95 - 99%. We show how the proposed method can be used to query several million songs for their acoustic neighbors very fast while producing almost the same results that a linear scan over the whole database would return. We present a working prototype implementation which is able to process similarity queries on a 2.5 million songs collection in about half a second on a standard CPU. © 2009 International Society for Music Information Retrieval.
Orio N.; Rodà A.,A measure of melodic similarity based on a graph representation of the music structure,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866726570&partnerID=40&md5=39bec8958ef6c588a27bcfe9cb62075a,"Orio N., Department of Information Engineering, University of Padova, Italy; Rodà A., Lab. AVIRES, University of Udine, Italy","Content-based music retrieval requires to define a similarity measure between music documents. In this paper, we propose a novel similarity measure between melodic content, as represented in symbolic notation, that takes into account musicological aspects on the structural function of the melodic elements. The approach is based on the representation of a collection of music scores with a graph structure, where terminal nodes directly describe the music content, internal nodes represent its incremental generalization, and arcs denote the relationships among them. The similarity between two melodies can be computed by analyzing the graph structure and finding the shortest path between the corresponding nodes inside the graph. Preliminary results in terms of music similarity are presented using a small test collection. © 2009 International Society for Music Information Retrieval."
Montecchio N.; Orio N.,A discrete filter bank approach to audio to score matching for polyphonic music,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873661147&partnerID=40&md5=974659cbbfe05b94463d2df938f63eff,"Montecchio N., Department of Information Engineering, University of Padova, Italy; Orio N., Department of Information Engineering, University of Padova, Italy","This paper presents a system for tracking the position of a polyphonic music performance in a symbolic score, possibly in real time. The system, based on Hidden Markov Models, is briefly presented, focusing on specific aspects such as observation modeling based on discrete filterbanks, in contrast with traditional FFT-based approaches, and describing the approaches to decoding. Experimental results are provided to assess the validity of the presented model. Proof-of-concept applications are shown, which effectively employ the described approach beyond the traditional automatic accompaniment system. © 2009 International Society for Music Information Retrieval."
Chordia P.; Rae A.,Using source separation to improve tempo detection,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873443702&partnerID=40&md5=204c65f189b13b6e90cf1c0985c880a6,"Chordia P., Dept. of Music, Georgia Tech Center for Music Technology, GTCMT, United States; Rae A., Dept. of Music, Georgia Tech Center for Music Technology, GTCMT, United States","We describe a novel tempo estimation method based on decomposing musical audio into sources using principal latent component analysis (PLCA). The approach is motivated by the observation that in rhythmically complex music, some layers may be more rhythmically regular than the overall mix, thus facilitating tempo detection. Each excerpt was analyzed using PLCA and the resulting components were each tempo tracked using a standard autocorrelationbased algorithm. We describe several techniques for aggregating or choosing among the multiple estimates that result from this process to extract a global tempo estimate. The system was evaluated on the MIREX 2006 training database as well as a newly constructed database of rhythmically complex electronic music consisting of 27 examples (IDM DB). For these databases the algorithms improved accuracy by 10% (60% vs 50%) and 22.3% (48.2% vs. 25.9%) respectively. These preliminary results suggest that for some types of music, source-separation may lead to better tempo detection. © 2009 International Society for Music Information Retrieval."
Robine M.; Hanna P.; Lagrange M.,Meter class profiles for music similarity and retrieval,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873647892&partnerID=40&md5=527a42b3f4a69d39dfcd44ca282fd2b3,"Robine M., LaBRI, University of Bordeaux, 33405 Talence cedex, 351, cours de la Libération, France; Hanna P., LaBRI, University of Bordeaux, 33405 Talence cedex, 351, cours de la Libération, France; Lagrange M., Telecom ParisTech, 75634 Paris Cedex 13, 46, rue Barrault, France","Rhythm is one of the main properties of Western tonal music. Existing content-based retrieval systems generally deal with melody or style. A few existing ones based on meter or rhythm characteristics have been recently proposed but they require a precise analysis, or they rely on a low-level descriptor. In this paper, we propose a midlevel descriptor: the Meter Class Profile (MCP). The MCP is centered on the tempo and represents the strength of beat multiples, including the measure rate, and the beat subdivisions. TheMCP coefficients are estimated by means of the autocorrelation and the Fourier transform of the onset detection curve. Experiments on synthetic and real databases are presented, and the results demonstrate the efficacy of the MCP descriptor in clustering and retrieval of songs according to their metric properties. © 2009 International Society for Music Information Retrieval."
Santoro C.A.; Cheng C.I.,Multiple F0 estimation in the transform domain,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632329&partnerID=40&md5=9ba21f5159c27500274c22afd7864228,"Santoro C.A., LSB Audio, Tampa, FL 33610, United States, Frost School of Music, Music Engineering Technology, University of Miami, Coral Gables, FL 33124, United States; Cheng C.I., Frost School of Music, Music Engineering Technology, University of Miami, Coral Gables, FL 33124, United States, Department of Electrical and Computer Engineering, University of Miami, United States","A novel algorithm is proposed to estimate the fundamental frequencies present in polyphonic acoustic mixtures expressed in a transform domain. As an example, the algorithm operates on Modified Discrete Cosine Transform (MDCT) coefficients in order to demonstrate the utility of the method in commercially available perceptual audio codecs which use the MDCT. An auditory model is developed along with several optimizations that deal with the constraints of processing in the transform-domain, including an interpolation method, a transform-domain half-wave rectification model, tonal component estimation, and sparse convolution. Test results are separated by instrument and analyzed in detail. The proposed algorithm is shown to perform comparably to state of the art time-domain methods. © 2009 International Society for Music Information Retrieval."
Kobayashi Y.,Automatic generation of musical instrument detector by using evolutionary learning method,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868666230&partnerID=40&md5=e311cc697ccfd9fb8178176ac40e0ed6,"Kobayashi Y., SONY Corporation, Japan","This paper presents a novel way of generating information extractors that obtain high-level information from recorded music such as the presence of a certain musical instrument. Our information extractor is comprised of a feature set and a discrimination or regression formula. We introduce a scheme to generate the entire information extractor given only a large amount of labeled dataset. For example, data could be waveform, and label could be the presence of musical instruments in them. We propose a very flexible description of features that allows various kinds of data other than waveform. Our proposal also includes a modified evolutionary learning method to optimize the feature set. We applied our scheme to automatically generate musical instrument detectors for mixed-down music in stereo. The experiment showed that our scheme could find a suitable set of features for the objective and could generate good detectors. © 2009 International Society for Music Information Retrieval."
Martin B.; Robine M.; Hanna P.,Musical structure retrieval by aligning self-similarity matrices,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960695452&partnerID=40&md5=ca823a4f4d46270d9d36cef7c3de84c3,"Martin B., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France; Robine M., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France; Hanna P., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France","We propose a new retrieval system based on musical structure using symbolic structural queries. The aim is to compare musical form in audio files without extracting explicitly the underlying audio structure. From a given or arbitrary segmentation, an audio file is segmented. Irrespective of the audio feature choice, we then compute a selfsimilarity matrix whose coefficients correspond to the estimation of the similarity between entire parts, obtained by local alignment. Finally, we compute a binary matrix from the symbolic structural query and compare it to the audio segmented matrix, which provides a structural similarity score. We perform experiments using large databases of audio files, and prove robustness to possible imprecisions in the structural query. © 2009 International Society for Music Information Retrieval."
Xu X.; Naito M.; Kato T.; Kawai H.,Robust and fast lyric search based on phonetic confusion matrix,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873622094&partnerID=40&md5=d24c4bd3657c922034f82068b20b6f37,"Xu X., KDDI R and D Laboratories, Inc., Japan; Naito M., KDDI R and D Laboratories, Inc., Japan; Kato T., KDDI R and D Laboratories, Inc., Japan; Kawai H., National Institute of Information and Communications Technology, Japan","This paper proposes a robust and fast lyric search method for music information retrieval. Current lyric search systems by normal text retrieval techniques are severely deteriorated in the case that the queries of lyric phrases contain incorrect parts due to mishearing and misremembering. To solve this problem, the authors apply acoustic distance, which is computed based on a confusion matrix of an ASR experiment, into DP-based phonetic string matching. The experimental results show that the search accuracy is increased by more than 40% compared with the normal text retrieval method; and by 2% ∼4% compared with the conventional phonetic string matching method. Considering the high computation complexity of DP matching, the authors propose a novel two-pass search strategy to shorten the processing time. By pre-selecting the probable candidates by a rapid index-based search for the first pass and executing a DP-based search among these candidates during the second pass, the proposed method reduces processing time by 85.8% and keeps search accuracy at the same level as that of a complete search by DP matching with all lyrics. © 2009 International Society for Music Information Retrieval."
Abeßer J.; Lukashevich H.; Dittmar C.; Schuller G.,Genre classification using bass-related high-level features and playing styles,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053107868&partnerID=40&md5=101444f3498cf5f129e943130b3dd2f6,"Abeßer J., Fraunhofer IDMT, Germany; Lukashevich H., Fraunhofer IDMT, Germany; Dittmar C., Fraunhofer IDMT, Germany; Schuller G., Fraunhofer IDMT, Germany","Considering itsmediation role between the poles of rhythm, harmony, and melody, the bass plays a crucial role in most music genres. This paper introduces a novel set of transcription-based high-level features that characterize the bass and its interactionwith other participating instruments. Furthermore, a new method to model and automatically retrieve different genre-specific bass playing styles is presented. A genre classification task is used as benchmark to compare common machine learning algorithms based on the presented high-level features with a classification algorithm solely based on detected bass playing styles. © 2009 International Society for Music Information Retrieval."
Seyerlehner K.; Knees P.; Schnitzer D.; Widmer G.,Browsing music recommendation networks,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873649217&partnerID=40&md5=8a41a7e1f5bd372b7c96c5abc9c0e73a,"Seyerlehner K., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Schnitzer D., Austrian Research Institute for AI, Vienna, Austria; Widmer G., Austrian Research Institute for AI, Vienna, Austria",Many music portals offer the possibility to explore music collections via browsing automatically generated music recommendations. In this paper we argue that such music recommender systems can be transformed into an equivalent recommendation graph. We then analyze the recommendation graph of a real-world content-based music recommender systems to find out if users can really explore the underlying song database by following those recommendations. We find that some songs are not recommended at all and are consequently not reachable via browsing. We then take a first attempt to modify a recommendation network in such a way that the resulting network is better suited to explore the respective music space. © 2009 International Society for Music Information Retrieval.
Rafii Z.; Pardo B.,Learning to control a reverberator using subjective perceptual descriptors,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873664134&partnerID=40&md5=08322b3fdd56acf093cf204da62c8552,"Rafii Z., EECS Department, Northwestern University, Evanston IL, United States; Pardo B., EECS Department, Northwestern University, Evanston IL, United States","The complexity of existing tools for mastering audio can be daunting. Moreover, many people think about sound in individualistic terms (such as ""boomy"") that may not have clear mappings onto the controls of existing audio tools. We propose learning to map subjective audio descriptors, such as ""boomy"", onto measures of signal properties in order to build a simple controller that manipulates an audio reverberator in terms of a chosen descriptor. For example, ""make the sound less boomy"". In the learning process, a user is presented with a series of sounds altered in different ways by a reverberator and asked to rate how well each sound represents the audio concept. The system correlates these ratings with reverberator parameters to build a controller that manipulates reverberation in the user's terms. In this paper, we focus on developing the mapping between reverberator controls, measures of qualities of reverberation and user ratings. Results on 22 subjects show the system learns quickly (under 3 minutes of training per concept), predicts users responses well (mean correlation coefficient of system predictiveness 0.75) and meets users' expectations (average human rating of 7.4 out of 10). © 2009 International Society for Music Information Retrieval."
Bischoff K.; Firan C.S.; Paiu R.; Nejdl W.; Laurier C.; Sordo M.,Music mood and theme classification - A hybrid approach,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862927717&partnerID=40&md5=20eb93dbd217ac921da106e24d0ad4d1,"Bischoff K., L3S Research Center, Hannover, Appelstr. 4, Germany; Firan C.S., L3S Research Center, Hannover, Appelstr. 4, Germany; Paiu R., L3S Research Center, Hannover, Appelstr. 4, Germany; Nejdl W., L3S Research Center, Hannover, Appelstr. 4, Germany; Laurier C., Music Technology Group, Universitat Pompeu Fabra, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Spain","Music perception is highly intertwined with both emotions and context. Not surprisingly, many of the users' information seeking actions aim at retrieving music songs based on these perceptual dimensions - moods and themes, expressing how people feel about music or which situations they associate it with. In order to successfully support music retrieval along these dimensions, powerful methods are needed. Still, most existing approaches aiming at inferring some of the songs' latent characteristics focus on identifying musical genres. In this paper we aim at bridging this gap between users' information needs and indexed music features by developing algorithms for classifying music songs by moods and themes. We extend existing approaches by also considering the songs' thematic dimensions and by using social data from the Last.fm music portal, as support for the classification tasks. Our methods exploit both audio features and collaborative user annotations, fusing them to improve overall performance. Evaluation performed against the AllMusic.com ground truth shows that both kinds of information are complementary and should be merged for enhanced classification accuracy. © 2009 International Society for Music Information Retrieval."
Hankinson A.; Pugin L.; Fujinaga I.,Interfaces for document representation in digital music libraries,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856807509&partnerID=40&md5=b2506d09669413f65c7a2066816d2663,"Hankinson A., Schulich School of Music, McGill University, Montréal, QC, Canada; Pugin L., RISM, Bern, Switzerland; Fujinaga I., Schulich School of Music, McGill University, Montréal, QC, Canada","Musical documents, that is, documents whose primary content is printed music, introduce interesting design challenges for presentation in an online environment. Considerations for the unique properties of printed msic, as well as users' expected levels of comfort with these materials, present opportunities for developing a viewer specifically tailored to displaying musical documents. This paper outlines five design considerations for a music document viewer, drawing examples from existing digital music libraries. We then present our work towards incorporating these considerations in a new digital music library system currently under development. © 2009 International Society for Music Information Retrieval."
Reed J.T.; Ueda Y.; Siniscalchi S.; Uchiyama Y.; Sagayama S.; Lee C.-H.,Minimum classification error training to improve isolated chord recognition,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445119&partnerID=40&md5=c1083245d0f2b017fb683f8a4acea026,"Reed J.T., Georgia Institute of Technology, School of Electrical and Computer Engineering, Atlanta, GA 30332, United States; Ueda Y., Graduate School of Information Science and Technology, University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; Siniscalchi S., Department of Electronics and Telecommunications, Norwegian University of Science and Technology, Trondheim, Norway; Uchiyama Y., Graduate School of Information Science and Technology, University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; Lee C.-H., Georgia Institute of Technology, School of Electrical and Computer Engineering, Atlanta, GA 30332, United States","Audio chord detection is the combination of two separate tasks: recognizing what chords are played and determining when chords are played. Most current audio chord detection algorithms use hidden Markov model (HMM) classifiers because of the task similarity with automatic speech recognition. For most speech recognition algorithms, the performance is measured by word error rate; i.e., only the identity of recognized segments is considered because word boundaries in continuous speech are often ambiguous. In contrast, audio chord detection performance is typically measured in terms of frame error rate, which considers both timing and classification. This paper treats these two tasks separately and focuses on the first problem; i.e., classifying the correct chords given boundary information. The best performing chroma/HMM chord detection algorithm, as measured in the 2008 MIREX Audio Chord Detection Contest, is used as the baseline in this paper. Further improvements are made to reduce feature correlation, account for differences in tuning, and incorporate minimum classification error (MCE) training in obtaining chord HMMs. Experiments demonstrate that classification rates can be improved with tuning compensation and MCE discriminative training. © 2009 International Society for Music Information Retrieval."
Khadkevich M.; Omologo M.,Use of hidden markov models and factored language models for automatic chord recognition,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873631003&partnerID=40&md5=119b0d25c20b74c5c188fdd4860aaa4a,"Khadkevich M., FBK-irst, Universitá degli studi di Trento, Povo - 38050, Trento, Via Sommarive, 14, Italy; Omologo M., Fondazione Bruno Kessler-irst, Povo - 38050 Trento, Via Sommarive, 18, Italy","This paper focuses on automatic extraction of acoustic chord sequences from a musical piece. Standard and factored language models are analyzed in terms of applicability to the chord recognition task. Pitch class profile vectors that represent harmonic information are extracted from the given audio signal. The resulting chord sequence is obtained by running a Viterbi decoder on trained hidden Markov models and subsequent lattice rescoring, applying the language model weight. We performed several experiments using the proposed technique. Results obtained on 175 manually-labeled songs provided an increase in accuracy of about 2%. © 2009 International Society for Music Information Retrieval."
Baur D.; Langer T.; Butz A.,Shades of music: Letting users discover sub-song similarities,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866716141&partnerID=40&md5=395c1682ef265506ce608d82d06ff35e,"Baur D., Media Informatics Group, University of Munich (LMU), Munich, Germany; Langer T., Media Informatics Group, University of Munich (LMU), Munich, Germany; Butz A., Media Informatics Group, University of Munich (LMU), Munich, Germany","Many interesting pieces of music violate established structures or rules of their genre on purpose. These songs can be very atypical in their interior structure and their different parts might actually allude to entirely different other songs or genres. We present a query-by-example-based user interface that shows songs related to the one currently playing. This relation is not based on overall similarity, but on the similarity between the part currently playing and parts of other songs in the collection along different dimensions (pitch, timbre, bars, beats, loudness). The similarity is initially computed automatically, but can be corrected by the user. Once a sufficient number of corrections has been made, we expect the similarity measure to reach an even higher precision. Our system thereby allows users to discover hidden similarities on the level of song sections instead of whole songs. © 2009 International Society for Music Information Retrieval."
Hu X.; Stephen Downie J.; Ehmann A.F.,Lyric text mining in music mood classification,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873668220&partnerID=40&md5=db47a6f039518a8e7be1e2960c85286d,"Hu X., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Stephen Downie J., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","This research examines the role lyric text can play in improving audio music mood classification. A new method is proposed to build a large ground truth set of 5,585 songs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relatively complete set of lyric features and representation models were investigated. The best performing lyric feature set was also compared to a leading audio-based system. In combining lyric and audio sources, hybrid feature sets built with three different feature selection methods were also examined. The results show patterns at odds with findings in previous studies: audio features do not always outperform lyrics features, and combining lyrics and audio features can improve performance in many mood categories, but not all of them. © 2009 International Society for Music Information Retrieval."
McFee B.; Lanckriet G.,Heterogeneous embedding for subjective artist similarity,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051504689&partnerID=40&md5=21c96b3001a83b00c213cf753bde891d,"McFee B., Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","We describe an artist recommendation system which integrates several heterogeneous data sources to form a holistic similarity space. Using social, semantic, and acoustic features, we learn a low-dimensional feature transformation which is optimized to reproduce human-derived measurements of subjective similarity between artists. By producing low-dimensional representations of artists, our system is suitable for visualization and recommendation tasks. © 2009 International Society for Music Information Retrieval."
Lemström K.; Wiggins G.A.,Formalizing invariances for content-based music retrieval,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873620003&partnerID=40&md5=f4e73de3c11852ea9c7301ca0ea5c8e2,"Lemström K., Department of Computer Science, University of Helsinki, Finland; Wiggins G.A., Department of Computing Goldsmiths, University of London, United Kingdom","Invariances are central concepts in content-based music retrieval. Musical representations and similarity measures are designed to capture musically relevant invariances, such as transposition invariance. Though regularly used, their explicit definition is usually omitted because of the heavy formalism required. The lack of explicit definition, however, can result in misuse or misunderstanding of the terms. We discuss the musical relevance of various musical invariances and develop a set-theoretic formalism, for defining and classifying them. Using it, we define the most common invariances, and give a taxonomy which they inhabit. The taxonomy serves as a useful tool for idetinfying where work is needed to address real world problems in content-based music retrieval. © 2009 International Society for Music Information Retrieval."
Ferguson S.; Cabrera D.,Auditory spectral summarisation for audio signals with musical applications,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873622300&partnerID=40&md5=84f1ff9926197332a3f16ee15f09178b,"Ferguson S., Faculty of Design, Architecture and Building, University of Technology, Sydney, Australia; Cabrera D., Faculty of Design, Architecture and Building, University of Technology, Sydney, Australia","Methods for spectral analysis of audio signals and their graphical display are widespread. However, assessing music and audio in the visual domain involves a number of challenges in the translation between auditory images into mental or symbolically represented concepts. This paper presents a spectral analysis method that exists entirely in the auditory domain, and results in an auditory presentation of a spectrum. It aims to strip a segment of audio signal of its temporal content, resulting in a quasi-stationary signal that possesses a similar spectrum to the original signal. The method is extended and applied for the purpose of music summarisation. © 2009 International Society for Music Information Retrieval."
Jacobson K.; Raimond Y.; Sandler M.,An ecosystem for transparent music similarity in an openworld,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955376846&partnerID=40&md5=ce4faca724f8d41251c0d7df61fd5708,"Jacobson K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Raimond Y., BBC, London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","There exist many methods for deriving music similarity associations and additional variations are likely to be seen in the future. In this work we introduce the Similarity Ontology for describing associations between items. Using a combination of RDF/OWL and N3, our ontology allows for transparency and provenance tracking in a distributed and open system. We describe a similarity ecosystem where agents assert and aggregate similarity statements on the Web of Data allowing a client application to make queries for recommendation, playlisting, or other tasks. In this ecosystem any number of similarity derivation methods can exist side-by-side, specifying similarity relationships as well as the processes used to derive these statements. The data consumer can then select which similarity statements to trust based on knowledge of the similarity derivation processes or a list of trusted assertion agents. © 2009 International Society for Music Information Retrieval."
Panagakis Y.; Kotropoulos C.; Arce G.R.,Music genre classification using locality preserving non-negative tensor factorization and sparse representations,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873668627&partnerID=40&md5=753c869741b8966271b2eb7e27cc907d,"Panagakis Y., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki GR-54124, Box 451, Greece; Kotropoulos C., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki GR-54124, Box 451, Greece, Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE, United States; Arce G.R., Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE, United States","A robust music genre classification framework is proposed that combines the rich, psycho-physiologically grounded properties of auditory cortical representations of music recordings and the power of sparse representation-based classifiers. A novel multilinear subspace analysis method that incorporates the underlying geometrical structure of the cortical representations space into non-negative tensor factorization is proposed for dimensionality reduction compatible to the working principle of sparse representationbased classification. The proposed method is referred to as Locality Preserving Non-Negative Tensor Factorization (LPNTF). Dimensionality reduction is shown to play a crucial role within the classification framework under study. Music genre classification accuracy of 92.4% and 94.38% on the GTZAN and the ISMIR2004 Genre datasets is reported, respectively. Both accuracies outperform any accuracy ever reported for state of the art music genre classification algorithms applied to the aforementioned datasets. © 2009 International Society for Music Information Retrieval."
Yang Y.-H.; Lin Y.-C.; Lee A.; Chen H.,Improving musical concept detection by ordinal regression and context fusion,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873672285&partnerID=40&md5=6cab6dddd1607d25af789559be343e1c,"Yang Y.-H., National Taiwan University, Taiwan; Lin Y.-C., National Taiwan University, Taiwan; Lee A., National Taiwan University, Taiwan; Chen H., National Taiwan University, Taiwan","To facilitate information retrieval of large-scale music databases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept detection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label classification methods. To further improve the detection accuracy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selection to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924. © 2009 International Society for Music Information Retrieval."
Lübbers D.; Jarke M.,Adaptive multimodal exploration of music collections,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053125498&partnerID=40&md5=9c770d751e5918fb8623e54b496f646f,"Lübbers D., Informatik 5, RWTH Aachen University, Aachen, Germany, Dept. Applied Information Technology, German University of Technology, Muscat, Oman; Jarke M., Informatik 5, RWTH Aachen University, Aachen, Germany","Discovering music that we like rarely happens as a result of a directed search. Except for the case where we have exact meta data at hand it is hard to articulate what song is attractive to us. Therefore it is essential to develop and evaluate systems that support guided exploratory browsing of the music space. While a number of algorithms for organizing music collections according to a given similarity measure have been applied successfully, the generated structure is usually only presented visually and listening requires cumbersome skipping through the individual pieces. To close this media gap we describe an immersive multimodal exploration environment which extends the presentation of a song collection in a video-game-like virtual 3-D landscape by carefully adjusted spatialized plackback of songs. The user can freely navigate through the virtual world guided by the acoustic clues surrounding him. Observing his interaction with the environment the system furthermore learns the user's way of structuring his collection by adapting a weighted combination of a wide range of integrated content-based, meta-data-based and collaborative similarity measures. Our evaluation proves the importance of auditory feedback for music exploration and shows that our system is capable of adjusting to different notions of similarity. © 2009 International Society for Music Information Retrieval."
Anglade A.; Ramirez R.; Dixon S.,Genre classification using harmony rules induced from automatic chord transcriptions,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955414033&partnerID=40&md5=f09702a371892bf6b4f11eef4a66e41c,"Anglade A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Ramirez R., Music Technology Group, Universitat Pompeu Fabra, Spain; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We present an automatic genre classification technique making use of frequent chord sequences that can be applied on symbolic as well as audio data. We adopt a first-order logic representation of harmony and musical genres: pieces of music are represented as lists of chords and musical genres are seen as context-free definite clause grammars using subsequences of these chord lists. To induce the contextfree definite clause grammars characterising the genres we use a first-order logic decision tree induction algorithm. We report on the adaptation of this classification framework to audio data using an automatic chord transcription algorithm. We also introduce a high-level harmony representation scheme which describes the chords in term of both their degrees and chord categories. When compared to another high-level harmony representation scheme used in a previous study, it obtains better classification accuracies and shorter run times. We test this framework on 856 audio files synthesized from Band in a Box files and covering 3 main genres, and 9 subgenres. We perform 3-way and 2-way classification tasks on these audio files and obtain good classification results: between 67% and 79% accuracy for the 2-way classification tasks and between 58% and 72% accuracy for the 3-way classification tasks. © 2009 International Society for Music Information Retrieval."
Pohle T.; Schnitzer D.; Schedl M.; Knees P.; Widmer G.,On rhythm and general music similarity,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869800834&partnerID=40&md5=1a0ddd86005f5098010899ec6f38ca48,"Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schnitzer D., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Wien, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Wien, Austria","The contribution of this paper is threefold: First, we propose modifications to Fluctuation Patterns [14]. The resulting descriptors are evaluated in the task of rhythm similarity computation on the ""Ballroom Dancers"" collection. Second, we show that by combining these rhythmic descriptors with a timbral component, results for rhythm similarity computation are improved beyond the level obtained when using the rhythm descriptor component alone. Third, we present one ""unified"" algorithm with fixed parameter set. This algorithm is evaluated on three different music collections. We conclude from these evaluations that the computed similarities reflect relevant aspects both of rhythm similarity and of general music similarity. The performance can be improved by tuning parameters of the ""unified"" algorithm to the specific task (rhythm similarity / general music similarity) and the specific collection, respectively. © 2009 International Society for Music Information Retrieval."
Hillewaere R.; Manderick B.; Conklin D.,Global feature versus event models for folk song classification,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873630448&partnerID=40&md5=44637cb229b9a1c81c53e07505a5e420,"Hillewaere R., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Manderick B., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Conklin D., Music Informatics Research Group, Department of Computing, City University London, United Kingdom","Music classification has been widely investigated in the past few years using a variety ofmachine learning approaches. In this study, a corpus of 3367 folk songs, divided into six geographic regions, has been created and is used to evaluate two popular yet contrasting methods for symbolic melody classification. For the task of folk song classification, a global feature approach, which summarizes a melody as a feature vector, is outperformed by an event model of abstract event features. The best accuracy obtained on the folk song corpus was achieved with an ensemble of event models. These results indicate that the event model should be the default model of choice for folk song classification. © 2009 International Society for Music Information Retrieval."
MacRitchie J.; Buck B.; Bailey N.J.,Visualising musical structure through performance gesture,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555199062&partnerID=40&md5=e2290e95e4f89edf366c456e423f0fb6,"MacRitchie J., Centre for Music Technology, University of Glasgow, United Kingdom; Buck B., Centre for Music Technology, University of Glasgow, United Kingdom; Bailey N.J., Centre for Music Technology, University of Glasgow, United Kingdom","A musical performance is seen as the performer's interpretation of a musical score, illuminating the interaction between the musical structure and implied emotive character [1]. It has been demonstrated that performers' physical gestures correlate with structural and emotional aspects of the piece they are performing and that this information can be decoded by an audience when presented with a visualonly performance [2]. This paper investigates the relationship between direction of physical movement and underlying musical structures. The Vicon motion capture system is used to record 3D movements made by nine university-level pianists performing Chopin preludes op.28 Nos 6 and 7. The examination of several pianists provides insight into the similarity and differences in gestures between performers and how these relate to structure. Principal Component Analysis (PCA) of these performances and consequent analysis of variance reveals a relationship between extrema of the first six significant components and timing of phrasing structure in Prelude 7 where motion troughs consistently lag behind the occurence of phrase boundaries in the audio. This relationship is then examined for Prelude 6 which encompasses longer, expanded phrases and changes in rhythm. These expanded phrases are associated with elongated or split gestures, and variations of the motif with changes in movement. © 2009 International Society for Music Information Retrieval."
Law E.; West K.; Mandel M.; Bay M.; Stephen Downie J.,Evaluation of algorithms using games: The case of music tagging,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873647698&partnerID=40&md5=dc297e6645a24a4c9e092eddfa3277f2,"Law E., CMU, United States; West K., IMIRSEL/UIUC, United States; Mandel M., Columbia University, United States; Bay M., IMIRSEL/UIUC, United States; Stephen Downie J., IMIRSEL/UIUC, United States","Search by keyword is an extremely popular method for retrieving music. To support this, novel algorithms that automatically tag music are being developed. The conventional way to evaluate audio tagging algorithms is to compute measures of agreement between the output and the ground truth set. In this work, we introduce a new method for evaluating audio tagging algorithms on a large scale by collecting set-level judgments from players of a human computation game called TagATune. We present the design and preliminary results of an experiment comparing five algorithms using this new evaluation metric, and contrast the results with those obtained by applying several conventional agreement-based evaluation metrics. © 2009 International Society for Music Information Retrieval."
Park T.H.; Li Z.; Wu W.,Easy does it: The electro-acoustic music analysis toolbox,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865489147&partnerID=40&md5=ed1db073f50a0321a7b5125e58ac816d,"Park T.H., Tulane University, United States; Li Z., Tulane University, United States; Wu W., Tulane University, United States","In this paper we present the EASY (Electro-Acoustic muSic analYsis) Toolbox software system for assisting electro-acoustic music analysis. The primary aims of the system are to present perceptually relevant features and audio descriptors via visual designs to gain more insight into electro-acoustic music works and provide easy-touse ""click-and-go"" software interface paradigms for practical use of the system by non-experts and experts alike. The development of the EASY system exploits MIR techniques with particular emphasis on the electroacoustic music repertoire - musical pieces that concentrate on timbral dimensions rather than traditional elements such as pitch, melody, harmony, and rhythm. The project was mainly inspired by the lack of software tools available for aiding electro-acoustic music analysis. The system's frameworks, feature analysis algorithms, along with the initial analyses of pieces are presented here. © 2009 International Society for Music Information Retrieval."
Hu D.J.; Saul L.K.,A probabilistic topic model for unsupervised learning of musical key-profiles,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864117534&partnerID=40&md5=004bce46bbd2e2a6e6c7446ed2e5cbf6,"Hu D.J., Department of Computer Science and Engineering, University of California, San Diego, United States; Saul L.K., Department of Computer Science and Engineering, University of California, San Diego, United States","We describe a probabilisticmodel for learning musical keyprofiles from symbolic files of polyphonic, classical music. Our model is based on Latent Dirichlet Allocation (LDA), a statistical approach for discovering hidden topics in large corpora of text. In our adaptation of LDA, symbolic music files play the role of text documents, groups of musical notes play the role of words, and musical keyprofiles play the role of topics. The topics are discovered as significant, recurring distributions over twelve neutral pitch-classes. Though discovered automatically, these distributions closely resemble the traditional key-profiles used to indicate the stability and importance of neutral pitchclasses in the major and minor keys of western music. Unlike earlier approaches based on human judgement, our model learns key-profiles in an unsupervised manner, inferring them automatically from a large musical corpus that contains no key annotations. We show how these learned key-profiles can be used to determine the key of a musical piece and track its harmonic modulations. We also show how the model's inferences can be used to compare musical pieces based on their harmonic structure. © 2009 International Society for Music Information Retrieval."
Stephen Downie J.; Byrd D.; Crawford T.,Ten years of ismir: Reflections on challenges and opportunities,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873625272&partnerID=40&md5=2a7bc2124c71b3d5ddf34325522acf76,"Stephen Downie J., University of Illinois, Urbana-Champaign, United States; Byrd D., Indiana University, Bloomington, United States; Crawford T., Goldsmiths College University, London, United Kingdom","The International Symposium on Music Information Retrieval (ISMIR) was born on 13 August 1999. This paper expresses the opinions of three of ISMIR's founders as they reflect upon what has happened during its first decade. The paper provides the background context for the events that led to the establishment of ISMIR. We highlight the first ISMIR, held in Plymouth, MA in October of 2000, and use it to elucidate key trends that have influenced subsequent ISMIRs. Indicators of growth and success drawn from ISMIR publication data are presented. The role that the Music Information Retrieval Evaluation eXchange (MIREX) has played at ISMIR is examined. The factors contributing to ISMIR's growth and success are also enumerated. The paper concludes with a set of challenges and opportunities that the newly formed International Society for Music Information Retrieval should embrace to ensure the future vitality of the conference series and the ISMIR community. © 2009 International Society for Music Information Retrieval."
Yoshii K.; Goto M.,Continuous PLSI and smoothing techniques for hybrid music recommendation,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867157558&partnerID=40&md5=d272d58716ac709902a554d824546ecd,"Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents an extended probabilistic latent semantic indexing (pLSI) for hybrid music recommendation that deals with rating data provided by users and with contentbased data extracted from audio signals. The original pLSI can be applied to collaborative filtering by treating users and items as discrete random variables that follow multinomial distributions. In hybrid recommendation, it is necessary to deal with musical contents that are usually represented as continuous vectorial values. To do this, we propose a continuous pLSI that incorporates Gaussian mixture models. This extension, however, causes a severe local optima problem because it increases the number of parameters drastically. This is considered to be a major factor generating ""hubs,"" which are items that are inappropriately recommended to almost all users. To solve this problem, we tested three smoothing techniques: multinomial smoothing,Gaussian parameter tying, and artist-based item clustering. The experimental results revealed that although the first method improved nothing, the others significantly improved the recommendation accuracy and reduced the hubness. This indicates that it is important to appropriately limit the model complexity to use the pLSI in practical. © 2009 International Society for Music Information Retrieval."
Maillet F.; Eck D.; Desjardins G.; Lamere P.,Steerable playlist generation by learning song similarity from radio station playlists,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867145161&partnerID=40&md5=503f67e180b68991ffb89e16f336f020,"Maillet F., DIRO, CIRMMT, Université de Montréal, Montreal, Canada; Eck D., DIRO, CIRMMT, Université de Montréal, Montreal, Canada; Desjardins G., DIRO, CIRMMT, Université de Montréal, Montreal, Canada; Lamere P., Echo Nest, Somerville, United States","This paper presents an approach to generating steerable playlists. We first demonstrate a method for learning song transition probabilities from audio features extracted from songs played in professional radio station playlists. We then show that by using this learnt similarity function as a prior, we are able to generate steerable playlists by choosing the next song to play not simply based on that prior, but on a tag cloud that the user is able to manipulate to express the high-level characteristics of the music he wishes to listen to. © 2009 International Society for Music Information Retrieval."
Grachten M.; Schedl M.; Pohle T.; Widmer G.,The ISMIR cloud: A decade of ISMIR conferences at your fingertips,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572127&partnerID=40&md5=f9f8f11e0d0d2be11161aa136c560875,"Grachten M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper, we analyze the proceedings of the past International Symposia on Music Information Retrieval (ISMIR). We extract meaningful term sets from the accepted submissions and apply term weighting and Web-based filtering techniques to distill information about the topics covered by the papers. This enables us to visualize and interpret the change of hot ISMIR topics in the course of time. Furthermore, the performed analysis allows for assessing the cumulative ISMIR proceedings by semantic content (rather than by literal text search). To illustrate this, we introduce two prototype applications that are publicly accessible online 1 . The first allows the user to search for ISMIR publications by selecting subsets of ISMIR topics. The second provides interactive visual access to the joint content of ISMIR publications in the form of a tag cloud - the ISMIR Cloud. © 2009 International Society for Music Information Retrieval."
Salamon J.; Rohrmeier M.,A quantitative evaluation of a two stage retrieval approach for a melodic query by example system,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956293795&partnerID=40&md5=286d7d277027cd0c323535c321b75041,"Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Rohrmeier M., Centre for Music and Science, Faculty of Music, University of Cambridge, United Kingdom","We present a two-stage approach for retrieval in a melodic Query by Example system inspired by the BLAST algorithm used in bioinformatics for DNA matching. The first stage involves an indexing method using n-grams and reduces the number of targets to consider in the second stage. In the second stage we use a matching algorithm based on local alignment with modified cost functions which take into account musical considerations. We evaluate our system using queries made by real users utilising both short-term and long-term memory, and present a detailed study of the system's parameters and how they affect retrieval performance and efficiency. We show that whilst similar approaches were shown to be unsuccessful for Query by Humming (where singing and transcription errors result in queries with higher error rates), in the case of our system the approach is successful in reducing the database size without decreasing retrieval performance. © 2009 International Society for Music Information Retrieval."
Li B.; Smith J.B.L.; Fujinaga I.,Optical audio reconstruction for stereo phonograph records using white light interferometry,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866309161&partnerID=40&md5=3397902173dc999ebea1d40577292b48,"Li B., Schulich School of Music, Music Technology Area, McGill University, Canada; Smith J.B.L., Schulich School of Music, Music Technology Area, McGill University, Canada; Fujinaga I., Schulich School of Music, Music Technology Area, McGill University, Canada","Our work focuses on optically reconstructing the stereo audio signal of a 33 rpm long-playing (LP) record using a white-light interferometry-based approach. Previously, a theoretical framework was presented, alongside the primitive reconstruction result from a few cycles of a stereo sinusoidal test signal. To reconstruct an audible duration of a longer stereo signal requires tackling new problems, such as disc warping, image alignment, and eliminating the effects of noise and broken grooves. This paper proposes solutions to these problems, and presents the complete workflow of our Optical Audio Reconstruction (OAR) system. © 2009 International Society for Music Information Retrieval."
Karydis I.; Nanopoulos A.; Gabriel H.-H.; Spiliopoulou M.,Tag-aware spectral clustering of music items,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866696691&partnerID=40&md5=192164a76d06e27c1b8cb8907726bc43,"Karydis I., Department of Informatics, Ionian University, Greece; Nanopoulos A., Institute of Informatics, Hildesheim University, Germany; Gabriel H.-H., Faculty of Computer Science, Otto-von-Guericke-University, Magdeburg, Germany; Spiliopoulou M., Faculty of Computer Science, Otto-von-Guericke-University, Magdeburg, Germany","Social tagging is an increasingly popular phenomenon with substantial impact on Music Information Retrieval (MIR). Tags express the personal perspectives of the user on the music items (such as songs, artists, or albums) they tagged. These personal perspectives should be taken into account in MIR tasks that assess the similarity between music items. In this paper, we propose an novel approach for clustering music items represented in social tagging systems. Its characteristic is that it determines similarity between items by preserving the 3-way relationships among the inherent dimensions of the data, i.e., users, items, and tags. Conversely to existing approaches that use reductions to 2- way relationships (between items-users or items-tags), this characteristic allows the proposed algorithm to consider the personal perspectives of tags and to improve the clustering quality. Due to the complexity of social tagging data, we focus on spectral clustering that has been proven effective in addressing complex data. However, existing spectral clustering algorithms work with 2-way relationships. To overcome this problem, we develop a novel data-modeling scheme and a tag-aware spectral clustering procedure that uses tensors (high-dimensional arrays) to store the multigraph structures that capture the personalised aspects of similarity. Experimental results with data from Last.fm indicate the superiority of the proposed method in terms of clustering quality over conventional spectral clustering approaches that consider only 2-way relationships. © 2009 International Society for Music Information Retrieval."
Raphael C.,Symbolic and structrual representation of melodic expression,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873630337&partnerID=40&md5=f1a50d59c1df5952856d6b6f92fa22aa,"Raphael C., School of Informatics and Computing, Indiana Univ., Bloomington, United States","A method for expressive melody synthesis is presented seeking to capture the structural and prosodic (stress, direction, and grouping) elements of musical interpretation. The interpretation of melody is represented through a hierarchical structural decomposition and a note-level prosodic annotation. An audio performance of the melody is constructed using the time-evolving frequency and intensity functions. A method is presented that transforms the expressive annotation into the frequency and intensity functions, thus giving the audio performance. In this framework, the problem of expressive rendering is cast as estimation of structural decomposition and the prosodic annotation. Examples are presented on a dataset of around 50 folk-like melodies, realized both from hand-marked and estimated annotations. © 2009 International Society for Music Information Retrieval."
Proutskova P.; Casey M.,You call that singing? Ensemble classification for multi-cultural collections of music recordings,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955425305&partnerID=40&md5=b30c475c6dd89e01dfb0eed5b153bfd7,"Proutskova P., Department of Computing, Goldsmiths, London, United Kingdom; Casey M., Bregman Music Research Laboratory, Dartmouth College, United States","The wide range of vocal styles, musical textures and recording techniques found in ethnomusicological field recordings leads us to consider the problem of automatically labeling the content to know whether a recording is a song or instrumental work. Furthermore, if it is a song, we are interested in labeling aspects of the vocal texture: e.g. solo, choral, acapella or singing with instruments. We present evidence to suggest that automatic annotation is feasible for recorded collections exhibiting a wide range of recording techniques and representing musical cultures from around the world. Our experiments used the Alan Lomax Cantometrics training tapes data set, to encourage future comparative evaluations. Experiments were conducted with a labeled subset consisting of several hundred tracks, annotated at the track and frame levels, as acapella singing, singing plus instruments or instruments only. We trained frame-by-frame SVM classifiers using MFCC features on positive and negative exemplars for two tasks: per-frame labeling of singing and acapella singing. In a further experiment, the frame-by-frame classifier outputs were integrated to estimate the predominant content of whole tracks. Our results show that frame-byframe classifiers achieved 71% frame accuracy and whole track classifier integration achieved 88% accuracy. We conclude with an analysis of classifier errors suggesting avenues for developing more robust features and classifier strategies for large ethnographically diverse collections. © 2009 International Society for Music Information Retrieval."
Ferraro P.; Hanna P.; Imbert L.; Izard T.,Accelerating query-by-humming on GPU,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053117737&partnerID=40&md5=56488dd608e55c7f173719b2aa6807f7,"Ferraro P., LaBRI - U. Bordeaux 1, France, PIMS/CNRS - U. Calgary, Canada; Hanna P., LaBRI - U. Bordeaux 1, France; Imbert L., PIMS/CNRS - U. Calgary, Canada, Lirmm - CNRS, France; Izard T., Lirmm - U. Montpellier 2, France","Searching for similarities in large musical databases has become a common procedure. Local alignment methods, based on dynamic programming, explore all the possible matchings between two musical pieces; and as a result return the optimal local alignment. Unfortunately these very powerful methods have a very high computational cost. The exponential growth of musical databases makes exact alignment algorithm unrealistic for searching similarities. Alternatives have been proposed in bioinformatics either by using heuristics or by developing faster implementation of exact algorithm. The main motivation of this work is to exploit the huge computational power of commonly available graphic cards to develop high performance solutions for Query-by-Humming applications. In this paper, we present a fast implementation of a local alignment method, which allows to retrieve a hummed query in a database of MIDI files, with good accuracy, in a time up to 160 times faster than other comparable systems. © 2009 International Society for Music Information Retrieval."
Ganseman J.; Scheunders P.; D'Haes W.,Using XML-formatted scores in real-time applications,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873664005&partnerID=40&md5=43f16451858e4e233b93cecb8c43dee8,"Ganseman J., Dept. of Physics, IBBT - Visionlab, University of Antwerp, B-2610 Wilrijk (Antwerp), building N, Belgium; Scheunders P., Dept. of Physics, IBBT - Visionlab, University of Antwerp, B-2610 Wilrijk (Antwerp), building N, Belgium; D'Haes W., Mu Technologies NV, B-3500 Hasselt, Singelbeekstraat 121, Belgium","In this paper we present fast and scalable methods to access relevant data from music scores stored in an XML based notation format, with the explicit goal of using scores in real-time audio processing frameworks. Quick and easy access is important when accessing or traversing a score, for instance for real-time playback. Any time complexity improvement in these contexts is valuable, while memory constraints are usually less important. We show that with some well chosen design choices and precomputation of the necessary data, runtime time complexity of several key score manipulation operations can be reduced to a level that allows use in a real-time context. © 2009 International Society for Music Information Retrieval."
Fuhrmann F.; Haro M.; Herrera P.,"Scalability, generality and temporal aspects in automatic recognition of predominant musical instruments in polyphonic music",2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605129&partnerID=40&md5=3c7a876a11383cc45c13a6e7d5a8db71,"Fuhrmann F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Haro M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we present an approach towards the classification of pitched and unpitched instruments in polyphonic audio. In particular, the presented study accounts for three aspects currently lacking in literature: model scalability to polyphonic data, model generalisation in respect to the number of instruments, and incorporation of perceptual information. Therefore, our goal is a unifying recognition framework which enables the extraction of the main instruments' information. The applied methodology consists of training classifiers with audio descriptors, using extensive datasets to model the instruments sufficiently. All data consist of real world music, including categories of 11 pitched and 3 percussive instruments. We designed our descriptors by temporal integration of the raw feature values, which are directly extracted from the polyphonic data. Moreover, to evaluate the applicability of modelling temporal aspects in polyphonic audio, we studied the performance of different encodings of the temporal information. Along with accuracies of 63% and 78% for the pitched and percussive classification task, results show both the importance of temporal encoding as well as strong limitations of modelling it accurately. © 2009 International Society for Music Information Retrieval."
Kim J.H.; Tomasik B.; Turnbull D.,Using artist similarity to propagate semantic information,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873643407&partnerID=40&md5=9c0900a57f751f6d73f5d20c86d1389b,"Kim J.H., Department of Computer Science, Swarthmore College, United States; Tomasik B., Department of Computer Science, Swarthmore College, United States; Turnbull D., Department of Computer Science, Swarthmore College, United States","Tags are useful text-based labels that encode semantic information about music (instrumentation, genres, emotions, geographic origins). While there are a number of ways to collect and generate tags, there is generally a data sparsity problem in which very few songs and artists have been accurately annotated with a sufficiently large set of relevant tags. We explore the idea of tag propagation to help alleviate the data sparsity problem. Tag propagation, originally proposed by Sordo et al., involves annotating a novel artist with tags that have been frequently associated with other similar artists. In this paper, we explore four approaches for computing artists similarity based on different sources of music information (user preference data, social tags, web documents, and audio content). We compare these approaches in terms of their ability to accurately propagate three different types of tags (genres, acoustic descriptors, social tags). We find that the approach based on collaborative filtering performs best. This is somewhat surprising considering that it is the only approach that is not explicitly based on notions of semantic similarity. We also find that tag propagation based on content-based music analysis results in relatively poor performance. © 2009 International Society for Music Information Retrieval."
Bretherton D.; Smith D.A.; Schraefel M.; Polfreman R.; Everist M.; Brooks J.; Lambert J.,Integrating musicology's heterogeneous data sources for better exploration,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873620826&partnerID=40&md5=c5f1464aa8bad26f825fa1e2905e7d0b,"Bretherton D., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Smith D.A., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Schraefel M., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Polfreman R., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Everist M., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Brooks J., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Lambert J., University of Southampton, Southampton, SO17 1BJ, United Kingdom","Musicologists have to consult an extraordinarily heterogeneous body of primary and secondary sources during all stages of their research. Many of these sources are now available online, but the historical dispersal of material across libraries and archives has now been replaced by segregation of data and metadata into a plethora of online repositories. This segregation hinders the intelligent manipulation of metadata, and means that extracting large tranches of basic factual information or running multi-part search queries is still enormously and needlessly time consuming. To counter this barrier to research, the ""musicSpace"" project is experimenting with integrating access to many of musicology's leading data sources via a modern faceted browsing interface that utilises Semantic Web and Web2.0 technologies such as RDF and AJAX. This will make previously intractable search queries tractable, enable musicologists to use their time more efficiently, and aid the discovery of potentially significant information that users did not think to look for. This paper outlines our work to date. © 2009 International Society for Music Information Retrieval."
Gó mez E.; Haro M.; Herrera P.,Music and geography: Content description of musical audio from different parts of theworld,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873621358&partnerID=40&md5=4a8edbfed4ec04ae53c0da9abdf19f26,"Gó mez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Haro M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper analyses how audio features related to different musical facets can be useful for the comparative analysis and classification of music from diverse parts of the world. The music collection under study gathers around 6,000 pieces, including traditional music from different geographical zones and countries, as well as a varied set of Western musical styles. We achieve promising results when trying to automatically distinguish music fromWestern and non-Western traditions. A 86.68% of accuracy is obtained using only 23 audio features, which are representative of distinct musical facets (timbre, tonality, rhythm), indicating their complementarity for music description. We also analyze the relative performance of the different facets and the capability of various descriptors to identify certain types of music. We finally present some results on the relationship between geographical location and musical features in terms of extracted descriptors. All the reported outcomes demonstrate that automatic description of audio signals together with data mining techniques provide means to characterize huge music collections from different traditions, complementing ethnomusicological manual analysis and providing a link between music and geography. © 2009 International Society for Music Information Retrieval."
Tidhar D.; Fazekas G.; Kolozali S.; Sandler M.,Publishing music similarity features on the semantic web,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955430512&partnerID=40&md5=bdd8a74ed81af4e283f302ec04496c47,"Tidhar D., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Kolozali S., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom","We describe the process of collecting, organising and publishing a large set of music similarity features produced by the SoundBite [10] playlist generator tool. These data can be a valuable asset in the development and evaluation of new Music Information Retrieval algorithms. They can also be used in Web-based music search and retrieval applications. For this reason, we make a database of features available on the Semantic Web via a SPARQL end-point, which can be used in Linked Data services. We provide examples of using the data in a research tool, as well as in a simple web application which responds to audio queries and finds a set of similar tracks in our database. © 2009 International Society for Music Information Retrieval."
Maxwell J.B.; Pasquier P.; Eigenfeldt A.,Hierarchical sequential memory for music: A cognitive model,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857236743&partnerID=40&md5=52608dd4ee319b018dfb25fb0cca1c1a,"Maxwell J.B., Simon Fraser University, Canada; Pasquier P., Simon Fraser University, Canada; Eigenfeldt A., Simon Fraser University, Canada","We propose a new machine-learning framework called the Hierarchical Sequential Memory for Music, or HSMM. The HSMM is an adaptation of the Hierarchical Temporal Memory (HTM) framework, designed to make it better suited to musical applications. The HSMM is an online learner, capable of recognition, generation, continuation, and completion of musical structures. © 2009 International Society for Music Information Retrieval."
Laurier C.; Sordo M.; Serr̀a J.; Herrera P.,Music mood representations from social tags,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632528&partnerID=40&md5=3178d5d675e12fcdaffd93ae259397a8,"Laurier C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serr̀a J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper presents findings about mood representations. We aim to analyze how do people tag music by mood, to create representations based on this data and to study the agreement between experts and a large community. For this purpose, we create a semantic mood space from last.fm tags using Latent Semantic Analysis. With an unsupervised clustering approach, we derive from this space an ideal categorical representation. We compare our community based semantic space with expert representations from Hevner and the clusters from the MIREX Audio Mood Classification task. Using dimensional reduction with a Self-Organizing Map, we obtain a 2D representation that we compare with the dimensional model from Russell. We present as well a tree diagram of the mood tags obtained with a hierarchical clustering approach. All these results show a consistency between the community and the experts as well as some limitations of current expert models. This study demonstrates a particular relevancy of the basic emotions model with four mood clusters that can be summarized as: happy, sad, angry and tender. This outcome can help to create better ground truth and to provide more realistic mood classification algorithms. Furthermore, this method can be applied to other types of representations to build better computational models. © 2009 International Society for Music Information Retrieval."
Klapuri A.,A method for visualizing the pitch content of polyphonic music signals,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873652876&partnerID=40&md5=92f9a7dc466914ac93cae0a10ab9c66d,"Klapuri A., Department of Signal Processing, Tampere University of Technology, Finland","This paper proposes a method for visualizing the pitch content of polyphonicmusic signals. More specifically, amodel is proposed for calculating the salience of pitch candidates within a given pitch range, and an optimization technique is proposed to find the parameters of the model. The aim is to produce a continuous function which shows peaks at the positions of true pitches and where spurious peaks at multiples and submultiples of the true pitches are suppressed. The proposed method was evaluated using synthesized MIDI signals, for which it outperformed a baseline method in terms of precision and recall. A straightforward visualization technique is proposed to render the pitch salience function on the traditional staves when the musical key and barline information is available. © 2009 International Society for Music Information Retrieval."
Lee J.H.; Cameron Jones M.; Stephen Downie J.,"An analysis of ismir proceedings: Patterns of authorship, topic, and citation",2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955445724&partnerID=40&md5=a464fd0043db394f1ea13785cac25f7a,"Lee J.H., Information School, University of Washington, United States; Cameron Jones M., University of Illinois, Urbana-Champaign, United States; Stephen Downie J., University of Illinois, Urbana-Champaign, United States","This paper presents analyses of peer-reviewed papers and posters published in the past nine years of ISMIR proceedings: examining publication and authorship practices, topics and titles of research, as well as the citation patterns among the ISMIR proceedings. The main objective is to provide an overview of the progress made over the past nine years in the ISMIR community and to obtain some insights into where the community should be heading in the coming years. Overall, the ISMIR community has grown considerably over the past nine years, both in the number of papers and posters published each year, as well as the number of authors contributing. Furthermore, the amount of collaboration among authors, as reflected in co-authorship, has increased. Main areas of research are revealed by an analysis of most commonly used title terms. Also, major authors and research groups are identified by analyzing the co-authorship and citation patterns in ISMIR proceedings. © 2009 International Society for Music Information Retrieval."
Haro M.; Herrera P.,From low-level to song-level percussion descriptors of polyphonic music,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954441720&partnerID=40&md5=b3d06b26d114ee1be0c634cb4e6a2862,"Haro M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We address here the automatic description of percussive events in real-world polyphonic music. By taking a pattern recognition approach we evaluate more than 2,450 objectlevel features. Three binary instrument-wise support vector machines (SVM) are built from a training set of more that 100 songs and 10 genres. Then, we use these binary models to build a drum transcription system achieving comparable results with state of the art algorithms. Finally, we present 17 song-level percussion descriptors computed from the imperfect output of the transcription algorithm. We evaluate the usefulness of the proposed descriptors in music information retrieval (MIR) tasks like genre classification, danceability estimation and Western vs. non- Western music discrimination. We conclude that the presented song-level percussion descriptors provide complementary information to ""classic"" descriptors, that can help in the previously mentioned MIR tasks. © 2009 International Society for Music Information Retrieval."
Lukashevich H.; Abeßer J.; Dittmar C.; Grossmann H.,From multi-labeling to multi-domain-labeling: A novel two-dimensional approach to music genre classification,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052114328&partnerID=40&md5=0cfa399f84fffbc91508c72f4bbf174f,"Lukashevich H., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany; Abeßer J., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany; Dittmar C., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany; Grossmann H., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany","In this publication we describe a novel two-dimensional approach for automatic music genre classification. Although the subject poses a well studied task in Music Information Retrieval, some fundamental issues of genre classification have not been covered so far. Especiallymany modern genres are influenced by manifold musical styles. Most of all, this holds true for the broad category ""World Music"", which comprises many different regional styles and a mutual mix up thereof. A common approach to tackle this issue in manual categorization is to assign multiple genre labels to a single recording. However, for commonly used automatic classification algorithms, multilabeling poses a problem due to its ambiguities. Thus, we propose to break down multi-label genre annotations into single-label annotations within given time segments and musical domains. A corresponding multi-stage evaluation based on a representative set of items from a global music taxonomy is performed and discussed accordingly. Therefore, we conduct 3 different experiments that cover multi-labeling, multi-labeling with time segmentation and the proposed multi-domain labeling. © 2009 International Society for Music Information Retrieval."
Tomasik B.; Kim J.H.; Ladlow M.; Augat M.; Tingle D.; Wicentowski R.; Turnbull D.,Using regression to combine data sources for semantic music discovery,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863731098&partnerID=40&md5=e0be98655a2ef2b4de6845eb38e78250,"Tomasik B., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Kim J.H., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Ladlow M., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Augat M., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Tingle D., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Wicentowski R., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Turnbull D., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States","In the process of automatically annotating songs with descriptive labels, multiple types of input information can be used. These include keyword appearances in web documents, acoustic features of the song's audio content, and similarity with other tagged songs. Given these individual data sources, we explore the question of how to aggregate them. We find that fixed-combination approaches like sum and max perform well but that trained linear regression models work better. Retrieval performance improves with more data sources. On the other hand, for large numbers of training songs, Bayesian hierarchical models that aim to share information across individual tag regressions offer no advantage. © 2009 International Society for Music Information Retrieval."
Thomas V.; Fremerey C.; Damm D.; Clausen M.,Slave: A score-lyrics-audio-video-explorer,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873659612&partnerID=40&md5=34a969da8bb1181dadbeefc2ef8cd933,"Thomas V., Department of Computer Science III, University of Bonn, Germany; Fremerey C., Department of Computer Science III, University of Bonn, Germany; Damm D., Department of Computer Science III, University of Bonn, Germany; Clausen M., Department of Computer Science III, University of Bonn, Germany","We introduce the music exploration system SLAVE, which is based upon previous developments of our group. SLAVE manages multimedia music collections and allows for multimodal navigation, playback, and visualization in an efficient and user-friendly manner. 1 While previously the focus of our system development has been the simultaneous exploration of digitized sheet music and audio, with SLAVE we enhance the functionalities by video and lyrics to achieve a more comprehensive music interaction. In this paper, we concentrate on two aspects. Firstly, we integrate video documents into our framework. Secondly, we introduce a graphical user interface for semi-automatic feature extraction, indexing, and synchronization of heterogeneous music collections. The output of this GUI is used by SLAVE to offer both high quality audio and video playback with time-synchronous display of digitized sheet music and content-based search. © 2009 International Society for Music Information Retrieval."
Langlois T.; Marques G.,A music classification method based on timbral features,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873657155&partnerID=40&md5=88384ee6c4b6c17c129134e48bae63bd,"Langlois T., Faculdade de Ciências, Universidade de Lisboa, Portugal; Marques G., Instituto Superior de Engenharia de Lisboa, Portugal","This paper describes amethod formusic classification based solely on the audio contents of the music signal. More specifically, the audio signal is converted into a compact symbolic representation that retains timbral characteristics and accounts for the temporal structure of a music piece. Models that capture the temporal dependencies observed in the symbolic sequences of a set of music pieces are built using a statistical language modeling approach. The proposed method is evaluated on two classification tasks (Music Genre classification and Artist Identification) using publicly available datasets. Finally, a distance measure between music pieces is derived from the method and examples of playlists generated using this distance are given. The proposed method is compared with two alternative approaches which include the use of Hidden Markov Models and a classification scheme that ignores the temporal structure of the sequences of symbols. In both cases the proposed approach outperforms the alternatives. © 2009 International Society for Music Information Retrieval."
Juhász Z.,Motive identification in 22 folksong corpora using dynamic time warping and self organizing maps,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861482921&partnerID=40&md5=cc82dd5036f740db231e1c6c2979bb27,"Juhász Z., Research Institute for Technical Physics and Materials Science, Budapest H-1525, P.O.B 49, Hungary","A system for automatic motive identification of large folksong corpora is described in this article. The method is based on a dynamic time warping algorithm determining inherent repeating elements of the melodies and a self-organizing map that learns the most typical motive contours. Using this system, the typical motive collections of 22 cultures in Eurasia have been determined, and another great common self organising map has been trained by the unified collection of the national/areal motive collections. The analysis of the overlaps of the national-areal excitations on the common map allowed us to draw a graph of connections, which shows two main distinct groups, according to the geographical distribution. © 2009 International Society for Music Information Retrieval."
Govaerts S.; Duval E.,A web-based approach to determine the origin of an artist,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051518421&partnerID=40&md5=8d131e140be1e667484072466fa5eade,"Govaerts S., Department of Computer Science, K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; Duval E., Department of Computer Science, K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium","One can define the origin of an artist as the geographical location where he started his career. The origin is an important metadata element, because it can help to specify subgenres, be an indicator of regional popularity and improve recommendations. In this paper, we present six methods to determine the origin, based on Web data sources: one extracts data from Last.fm, two query Freebase and three analyze biographies. We evaluate the different methods with 11275 artists. Circa 55% of the artists can be classified using biographies. The best Freebase method can classify 26% and the Last.fm based method 7%. When comparing on accuracy, the Last.fm and Freebase methods perform similarly with around 90% accuracy. For the biography-based methods we achieve 71%. To improve coverage, a final, hybrid method achieves 77% accuracy and 60% coverage. The accuracy of the continent classification is 87%. As a showcase for our classifier, we developed a mashup application that displays, among others, information about the origin of artists from radio station playlists on a map. © 2009 International Society for Music Information Retrieval."
Kato M.P.,Rhythmixearch: Searching for unknown music by mixing known music,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873616645&partnerID=40&md5=467b99f2a2c667299859448653bfa47c,"Kato M.P., Department of Social Informatics, Graduate School of Informatics, Kyoto University, Kyoto, Japan","We present a novel method for searching for unknown music. RhythMiXearch is a music search system we developed that can accept two music inputs and mix those inputs to search for music that could reasonably be a result of the mixture. This approach expands the ability of Query-by-Example and allows greater flexibility for users in finding unknown music. Each music piece stored by our system is characterized by text data written by users, i.e., review data. We used Latent Dirichlet Allocation (LDA) to capture semantics from the reviews that were then used to characterize the music by Hevner's eight impression categories. RhythMiXearch mixes two music inputs in accordance with a probabilistic mixture model and finds music that is the most likely product of the mixture. Our experimental results indicate that the proposed method is comparable to human in searching for music by multiple examples. © 2009 International Society for Music Information Retrieval."
Bello J.P.,Grouping recorded music by structural similarity,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873629090&partnerID=40&md5=891335b58820a65bb95fedea31b72e5e,"Bello J.P., Music and Audio Research Lab (MARL), New York University, United States","This paper introduces a method for the organization of recorded music according to structural similarity. It uses the Normalized Compression Distance (NCD) to measure the pairwise similarity between songs, represented using beat-synchronous self-similarity matrices. The approach is evaluated on its ability to cluster a collection into groups of performances of the same musical work. Tests are aimed at finding the combination of system parameters that improve clustering, and at highlighting the benefits and shortcomings of the proposed method. Results show that structural similarities can be well characterized by this approach, given consistency in beat tracking and overall song structure. © 2009 International Society for Music Information Retrieval."
Eerola T.; Lartillot O.; Toiviainen P.,Prediction of multidimensional emotional ratings in music from audio using multivariate regression models,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873646150&partnerID=40&md5=14ed0004388ee1ab623b2ec10743b3a2,"Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Toiviainen P., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland","Content-based prediction of musical emotions and moods has a large number of exciting applications in Music Information Retrieval. However, what should be predicted, and precisely how, remain a challenge in the field. We provide an empirical comparison of two common paradigms of emotion representation in music, opposing a multidimensional space to a set of basic emotions. New groundtruth data consisting of film soundtracks was used to assess the compatibility of these models. The findings suggest that the two are highly compatible and a quantitative mapping between the two is provided. Next we propose a model predicting perceived emotions based on a set of features extracted from the audio. The feature selection and transformation is given special emphasis and three separate data reduction techniques are compared (stepwise regression, principal component analysis, and partial least squares regression). Best linear models consisting of 2- 5 predictors from the data reduction process were able to account for between 58 and 85% of the variance. In general, partial least squares models performed the best and the data transformation has a significant role in building linear models. © 2009 International Society for Music Information Retrieval."
Julià C.F.; Jordà S.,Songexplorer: A tabletop application for exploring large collections of songs,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053102744&partnerID=40&md5=5f725e5917a3d8ca5395a107810fc143,"Julià C.F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Jordà S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper presents SongExplorer, a system for the exploration of large music collections on tabletop interfaces. SongExplorer addresses the problem of finding new interesting songs on large music databases, from an interaction design perspective. Using high level descriptors of musical songs, SongExplorer creates a coherent 2D map based on similarity, in which neighboring songs tend to be more similar. All songs are represented as throbbing circles that highlight their more relevant high-level properties, and the resulting music map is browseable and zoomable by the users who can use their fingers as well as specially designed tangible pucks, for helping them to find interesting music, independently of their previous knowledge of the collection. SongExplorer also offers basic player capabilities, allowing the users to organize the songs they have just discovered into playlists which can be manipulated as well as played and displayed. In this paper, the system hardware, software and interaction design are explained, and the usability tests carried are presented. Finally, conclusions and future work are discussed. © 2009 International Society for Music Information Retrieval."
Su M.-Y.; Yang Y.-H.; Lin Y.-C.; Chen H.,An integrated approach to music boundary detection,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856929813&partnerID=40&md5=2a902758a658f39d950fe0a5c1d4187c,"Su M.-Y., National Taiwan University, Taiwan; Yang Y.-H., National Taiwan University, Taiwan; Lin Y.-C., National Taiwan University, Taiwan; Chen H., National Taiwan University, Taiwan","Music boundary detection is a fundamental step of music analysis and summarization. Existing works use either unsupervised or supervised methodologies to detect boundary. In this paper, we propose an integrated approach that takes advantage of both methodologies. In particular, a graph-theoretic approach is proposed to fuse the results of an unsupervised model and a supervised one by the knowledge of the typical length of a music section. To further improve accuracy, a number of novel mid-level features are developed and incorporated to the boundary detection framework. Evaluation result on the RWC dataset shows the effectiveness of the proposed approach. © 2009 International Society for Music Information Retrieval."
Marolt M.,Probabilistic segmentation and labeling of ethnomusicological field recordings,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426490&partnerID=40&md5=4f07d3cf11abe8ecbbc60f7a32824e33,"Marolt M., Faculty of Computer and Information Science, University of Ljubljana, Slovenia","The paper presents a method for segmentation and labeling of ethnomusicological field recordings. Field recordings are integral documents of folk music performances and typically contain interviews with performers intertwined with actual performances. As these are live recordings of amateur folk musicians, they may contain interruptions, false starts, environmental noises or other interfering factors. Our goal was to design a robust algorithm that would approximate manual segmentation of field recordings. First, short audio fragments are classified into one of the following categories: speech, solo singing, choir singing, instrumental or bell chiming performance. Then, a set of candidate segment boundaries is obtained by observing how the energy of the signal and its content change, and finally the recording is segmented with a probabilistic model that maximizes the posterior probability of segments given a set of candidate segment boundaries with their probabilities and prior knowledge of lengths of segments belonging to different categories. Evaluation of the algorithm on a set of field recordings from the Ehtnomuse archive is presented. © 2009 International Society for Music Information Retrieval."
Weil J.; Sikora T.; Durrieu J.-L.; Richard G.,Automatic generation of lead sheets from polyphonic music signals,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052979880&partnerID=40&md5=32787e30b7a614d796532e65180b16a0,"Weil J., Communication Systems Group, Technische Universität Berlin, Germany; Sikora T., Communication Systems Group, Technische Universität Berlin, Germany; Durrieu J.-L., Institut Telecom, Telecom ParisTech, CNRS LTCI, France; Richard G., Institut Telecom, Telecom ParisTech, CNRS LTCI, France","A lead sheet is a type of music notation which summarizes the content of a song. The usual elements that are reproduced are the melody, chords, tempo, time signature, style and the lyrics, if any. In this paper we propose a system that aims at transcribing both the melody and the associated chords in a beat-synchronous framework. A beat tracker identifies the pulse positions and thus defines a beat grid on which the chord sequence and the melody notes are mapped. The harmonic changes are used to estimate the time signature and the down beats as well as the key of the piece. The different modules perform very well on each of the different tasks, and the lead sheets that were rendered show the potential of the approaches adopted in this paper. © 2009 International Society for Music Information Retrieval."
Godøy R.I.; Jensenius A.R.,Body movement in music information retrieval,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871795195&partnerID=40&md5=de36be4659a31179aefb43773e3d02fb,"Godøy R.I., Department of Musicology, FourMs, University of Oslo, Norway; Jensenius A.R., Department of Musicology, FourMs, University of Oslo, Norway","We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations. There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions. The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR. © 2009 International Society for Music Information Retrieval."
Mauch M.; Noland K.; Dixon S.,Using musical structure to enhance automatic chord transcription,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873653194&partnerID=40&md5=4bbde6022f17cd018ba4f7c30bd43f07,"Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Noland K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Chord extraction from audio is a well-established music computing task, and many valid approaches have been presented in recent years that use different chord templates, smoothing techniques and musical context models. The present work shows that additional exploitation of the repetitive structure of songs can enhance chord extraction, by combining chroma information from multiple occurrences of the same segment type. To justify this claim we modify an existing chord labelling method, providing it with manual or automatic segment labels, and compare chord extraction results on a collection of 125 songs to baseline methods without segmentation information. Our method results in consistent and more readily readable chord labels and provides a statistically significant boost in label accuracy. © 2009 International Society for Music Information Retrieval."
Niedermayer B.,Improving accuracy of polyphonic music-to-score alignment,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472299&partnerID=40&md5=c1a6e45b4522f88f57e5162e260b0bc9,"Niedermayer B., Department for Computational Perception, Johannes Kepler University, Linz, Austria","This paper presents a new method to refine music-to-score alignments. The proposed system works offline in two passes, where in the first step a state-of-the art alignment based on chroma vectors and dynamic time warping is performed. In the second step a non-negative matrix factorization is calculated within a small search window around each predicted note onset, using pretrained tone models of only those pitches which are expected to be played within that window. Note onsets are then reset according to the pitch activation patterns yielded by the matrix factorization. In doing so, we are able to resolve individual notes within a chord. We show that this method is feasible of increasing the accuracy of aligned note's onsets which are already aligned relatively near to the real note attack. However it is so far not suitable for the detection and correction of outliers which are displaced by a large timespan. We also compared our system to a reference method showing that it outperforms bandpass filtering based onset detection in the refinement step. © 2009 International Society for Music Information Retrieval."
Ishizaki H.; Hoashi K.; Takishima Y.,Full-automatic DJ mixing system with optimal tempo adjustment based on measurement function of user discomfort,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632142&partnerID=40&md5=c213acb800c3bb6d77f96d3cd698bc74,"Ishizaki H., KDDI R and D Laboratories Inc., Japan; Hoashi K., KDDI R and D Laboratories Inc., Japan; Takishima Y., KDDI R and D Laboratories Inc., Japan","This paper proposes an automatic DJ mixing method that can automate the processes of real world DJs and describes a prototype for a fully automatic DJ mix-like playing system. Our goal is to achieve a fully automatic DJ mixing system that can preserve overall user comfort level during DJ mixing. In this paper, we assume that the difference between the original and adjusted songs is the main cause of user discomfort in the mixed song. In order to preserve user comfort, we define the measurement function of user discomfort based on the results of a subjective experiment. Furthermore, this paper proposes a unique tempo adjustment technique called ""optimal tempo adjustment"", which is robust for any combination of tempi of songs to be mixed. In the subjective experiment, the proposed method obtained higher averages of user ratings on three evaluation items compared to the conventional method. These results indicate that our system is able to preserve user comfort. © 2009 International Society for Music Information Retrieval."
Lin H.-Y.; Lin Y.-T.; Tien M.-C.; Wu J.-L.,Music paste: Concatenating music clips based on chroma and rhythm features,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873661485&partnerID=40&md5=025d4d5ff848f3b1366303e050902c70,"Lin H.-Y., Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; Lin Y.-T., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Tien M.-C., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Wu J.-L., Department of Computer Science and Information Engineering, National Taiwan University, Taiwan, Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","In this paper, we provide a tool for automatically choosing appropriate music clips from a given audio collection and properly combining the chosen clips. To seamlessly concatenate two different music clips without causing any audible defect is really a hard nut to crack. Borrowing the idea from the musical dice game and the DJ's strategy and considering psychoacoustics, we employ the currently available audio analysis and editing techniques to paste music sounded as pleasant as possible. Besides, we conduct subjective evaluations on the correlation between pasting methods and the auditory quality of combined clips. The experimental results show that the automatically generated music pastes are acceptable to most of the evaluators. The proposed system can be used to generate lengthened or shortened background music and dancing suite, which is useful for some audio-assisted multimedia applications. © 2009 International Society for Music Information Retrieval."
Kuuskankare M.; Laurson M.,MIR in ENP - Rule-based music information retrieval from symbolic music notation,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873661087&partnerID=40&md5=f5c4977c67becd0beb8e271f0abf4e0e,"Kuuskankare M., Centre for Music and Technology, Sibelius Academy, Finland; Laurson M., Centre for Music and Technology, Sibelius Academy, Finland","Symbolic music information retrieval is one of the most underrepresented areas in the field of MIR. Here, symbolic music means common practice music notation-the musician readable format. In this paper we introduce a novel rule-based symbolic music retrieval mechanism. The Scripting system-ENP-Script-is augmentedwithMIR functionality. It allows us to performsophisticated retrieval operations on symbolic musical scores prepared with the help of the music notation system ENP. We will also give a special attention to visualization of the query results. All the statistical queries, such as histograms, are visualized with the help of common music notation where appropriate. N-grams and more complex queries-the ones dealing with voice leading, for example- are visualized directly in the score. Our aim is to demonstrate the power and expressivity of the combination of common music notation and a rulebased scripting language through several challenging examples. © 2009 International Society for Music Information Retrieval."
Müller M.; Grosche P.; Wiering F.,Robust segmentation and annotation of folk song recordings,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864694464&partnerID=40&md5=3d421ff5b639440dddfcb7ede3375f59,"Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Grosche P., MPI Informatik, Saarland University, Saarbrücken, Germany; Wiering F., Department of Information and Computing Sciences, Utrecht University, Utrecht, Netherlands","Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of score-based transcriptions. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer studied in the actual folk song research though they still may contain valuable information. In this paper, we introduce an automated approach for segmenting folk song recordings into its constituent stanzas, which can then be made accessible to folk song researchers by means of suitable visualization, searching, and navigation interfaces. Performed by elderly non-professional singers, the main challenge with the recordings is that most singers have serious problems with the intonation, fluctuating with their voices even over several semitones throughout a song. Using a combination of robust audio features along with various cleaning and audio matching strategies, our approach yields accurate segmentations even in the presence of strong deviations. © 2009 International Society for Music Information Retrieval."
Nichols E.; Morris D.; Basu S.; Raphael C.,Relationships between lyrics and melody in popular music,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873670905&partnerID=40&md5=65246405fb3883543a8d2b12b3351fcc,"Nichols E., Indiana University, Bloomington, IN, United States; Morris D., Microsoft Research, Redmond, WA, United States; Basu S., Microsoft Research, Redmond, WA, United States; Raphael C., Indiana University, Bloomington, IN, United States","Composers of popular music weave lyrics, melody, and instrumentation together to create a consistent and compelling emotional scene. The relationships among these elements are critical to musical communication, and understanding the statistics behind these relationships can contribute to numerous problems in music information retrieval and creativity support. In this paper, we present the results of an observational study on a large symbolic database of popular music; our results identify several patterns in the relationship between lyrics and melody. © 2009 International Society for Music Information Retrieval."
Tsuchihashi Y.; Kitahara T.; Katayose H.,Using bass-line features for content-based mir,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76949085512&partnerID=40&md5=cee0d97f42b74f7f9ae8c43af5e5defc,"Tsuchihashi Y., Kwansei Gakuin University, Japan; Kitahara T., Kwansei Gakuin University, Japan, CrestMuse Project, CREST, JST, Japan; Katayose H., Kwansei Gakuin University, Japan, CrestMuse Project, CREST, JST, Japan","We propose new audio features that can be extracted from bass lines. Most previous studies on content-based music information retrieval (MIR) used low-level features such as the mel-frequency cepstral coefficients and spectral centroid. Musical similarity based on these features works well to some extent but has a limit to capture fine musical characteristics. Because bass lines play important roles in both harmonic and rhythmic aspects and have a different style for each music genre, our bass-line features are expected to improve the similarity measure and classification accuracy. Furthermore, it is possible to achieve a similarity measure that enhances the bass-line characteristics by weighting the bass-line and other features. Results for applying our features to automatic genre classification and music collection visualization showed that our features improved genre classification accuracy and did achieve a similarity measure that enhances bass-line characteristics."
Ganseman J.; Scheunders P.; D'haes W.,Using xquery on musicxml databases for musicological analysis,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472565&partnerID=40&md5=a41c2f41b5aaeb0f2a04f4b8197828ec,"Ganseman J., IBBT - Visionlab, Dept. of Physics, University of Antwerp, B-2610 Wilrijk (Antwerp), Universiteitsplein 1, Belgium; Scheunders P., IBBT - Visionlab, Dept. of Physics, University of Antwerp, B-2610 Wilrijk (Antwerp), Universiteitsplein 1, Belgium; D'haes W., Mu Technologies NV, B-3500 Hasselt, Singelbeekstraat 121, Belgium","MusicXML is a fairly recent XML-based file format for music scores, now supported by many score and audio editing software applications. Several online score library projects exist or are emerging, some of them using MusicXML as main format. When storing a large set of XML-encoded scores in an XML database, XQuery can be used to retrieve information from this database. We present some small practical examples of such large scale analysis, using the Wikifonia lead sheet database and the eXist XQuery engine. This shows the feasibility of automated musicological analysis on digital score libraries using the latest software tools. Bottom line: it's easy."
Wang F.; Wang X.; Shao B.; Li T.; Ogihara M.,Tag integrated multi-label music style classification with hypergraph,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954634120&partnerID=40&md5=596d8e5c1c730b3b38f1626435b9dc01,"Wang F., Florida International University, United States; Wang X., Florida International University, United States; Shao B., Florida International University, United States; Li T., Florida International University, United States; Ogihara M., University of Miami, United States","Automatic music style classification is an important, but challenging problem in music information retrieval. It has a number of applications, such as indexing of and searching in musical databases. Traditional music style classification approaches usually assume that each piece of music has a unique style and they make use of the music contents to construct a classifier for classifying each piece into its unique style. However, in reality, a piece may match more than one, even several different styles. Also, in this modern Web 2.0 era, it is easy to get a hold of additional, indirect information (e.g., music tags) about music. This paper proposes a multi-label music style classification approach, called Hypergraph integrated Support Vector Machine (HiSVM), which can integrate both music contents and music tags for automatic music style classification. Experimental results based on a real world data set are presented to demonstrate the effectiveness of the method. © 2009 International Society for Music Information Retrieval."
Thompson J.; McKay C.; Burgoyne J.A.; Fujinaga I.,Additions and improvements to the ace 2.0 music classifier,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952330591&partnerID=40&md5=f2986d9d94a2f1c2d2c2959f1c6fa5cb,"Thompson J., Music Technology, McGill University, Canada; McKay C., CIRMMT, McGill University, Canada; Burgoyne J.A., CIRMMT, McGill University, Canada; Fujinaga I., CIRMMT, McGill University, Canada","This paper presents additions and improvements to the Autonomous Classification Engine (ACE), a framework for using and optimizing classifiers. Given a set of feature values, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality- reduction techniques in order to arrive at a configuration that is well-suited to a given problem. Changes and additions have been made to ACE in order to increase its functionality as well as to make it easier to use and incorporate into other software frameworks. Details are provided on ACE's remodeled class structure and associated API, the improved command line and graphical user interfaces, a new ACE XML 2.0 ZIP file format and expanded statistical reporting associated with cross validation. The resulting improved processing and methods of operation are also discussed. © 2009 International Society for Music Information Retrieval."
Miotto R.; Orio N.,A music identification system based on chroma indexing and statistical modeling,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873442069&partnerID=40&md5=6c1427f52b888a9485526eaf4afefe39,"Miotto R., Department of Information Engineering, University of Padova, Italy; Orio N., Department of Information Engineering, University of Padova, Italy","A methodology is described for the automatic identification of classical music works. It can be considered an extension of fingerprinting techniques because the identification is carried out also when the query is a different performance of the work stored in the database, possibly played by different instruments and with background noise. The proposed methodology integrates an already existing approach based on hidden Markov models with an additional component that aims at improving scalability. The general idea is to carry out a clustering of the collection to highlight a limited number of candidates to be used for the HMM-based identification. Clustering is computed using the chroma features of the music works, hashed in a single value and retrieved using a bag of terms approach. Evaluation results are provided to show the validity of the combined approaches."
Winget M.A.,"The liner notes digitization project: Providing users with cultural, historical, and critical music information",2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873467889&partnerID=40&md5=e3ebc0b5c0081aa27671888254b333cf,"Winget M.A., School of Information, University of Texas at Austin, Austin, TX 78712-0390, 1 University Station, D7000, United States","Digitizing cultural information is a complex endeavor. Not only do users expect to have access to primary information like digital music files; it is also becoming more important for digital systems to provide contextual information for the primary artifacts contained within. The Liner Notes Markup Language (LNML) was developed to provide an XML vocabulary for encoding complex contextual documents that include an album's packaging, informational notes and inserts, liners, and album labels. This paper describes the development of the LNML framework, its major structural elements and functions, and some of the more pressing problems related to usability and purpose. The current LNML model is based on the examination and encoding of fifty albums from the 80s Rock genre. We are currently encoding fifty additional Jazz albums, which will provide data to augment and strengthen the model. Development of the LNML is ongoing, with plans to examine Classical and World Music examples to further augment the model."
JoséBurred J.; Cella C.E.; Peeters G.; Röbel A.; Schwarz D.,Using the SDIF sound description interchange format for audio features,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873429140&partnerID=40&md5=02215fc1f101ce1bc0169110efa9a286,"JoséBurred J., IRCAM, CNRS STMS, France; Cella C.E., IRCAM, CNRS STMS, France; Peeters G., IRCAM, CNRS STMS, France; Röbel A., IRCAM, CNRS STMS, France; Schwarz D., IRCAM, CNRS STMS, France","We present a set of extensions to the Sound Description Interchange Format (SDIF) for the purpose of storage and/or transmission of general audio descriptors. The aim is to allow portability and interoperability between the feature extraction module of an audio information retrieval application and the remaining modules, such as training, classification or clustering. A set of techniques addressing the needs of short-time features and temporal modeling over longer windows are proposed, together with the mechanisms that allow further extensions or adaptations by the user. The paper is completed by an overview of the general aspects of SDIF and its practical use by means of a set of existing programming interfaces for, among others, C, C++ and Matlab."
Yoshii K.; Goto M.,Music thumbnailer: Visualizing musical pieces in thumbnail images based on acoustic features,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448962&partnerID=40&md5=407800fc32be644ccc79a573cffcfa88,"Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a principled method called MusicThumbnailer to transform musical pieces into visual thumbnail images based on acoustic features extracted from their audio signals. These thumbnails can help users immediately guess the musical contents of audio signals without trial listening. This method is consistent in ways that optimize thumbnails according to the characteristics of a target music collection. This means the appropriateness of transformation should be defined to eliminate ad hoc transformation rules. In this paper, we introduce three top-down criteria to improve memorability of thumbnails (generate gradations), deliver information more completely, and distinguish thumbnails more clearly. These criteria are mathematically implemented as minimization of brightness differences of adjacent pixels and maximization of brightness variances within and between thumbnails. The optimized parameters of a modified linear mapping model we assumed are obtained by minimizing a unified cost function based on the three criteria with a steepest descent method. Experimental results indicate that generated thumbnails can provide users with useful hints as to the musical contents of musical pieces."
Hamanaka M.; Hirata K.; Tojo S.,Melody expectation method based on GTTM and TPS,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873482483&partnerID=40&md5=c89b1259e11c883d8d461620a059d02e,"Hamanaka M., University of Tsukuba, Tsukuba, Ibaraki, 1-1-1, Tenodai, Japan; Hirata K., NTT Communication Science Laboratories, Kyoto, 2-4, Hikaridai, Seikacho, Kei-hanna Science City, Japan; Tojo S., Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, 1-1, Asahidai, Japan","A method that predicts the next notes is described for assisting musical novices to play improvisations. Melody prediction is one of the most difficult problems in musical information retrieval because composers and players may or may not create melodies that conform to our expectation. The development of a melody expectation method is thus important for building a system that supports musical novices because melody expectation is one of the most basic skills for a musician. Unlike most previous prediction methods, which use statistical learning, our method evaluates the appropriateness of each candidate note from the view point of musical theory. In particular, it uses the concept of melody stability based on the generative theory of tonal music (GTTM) and the tonal pitch space (TPS) to evaluate the appropriateness of the melody. It can thus predict the candidate next notes not only from the surface structure of the melody but also from the deeper structure of the melody acquired by GTTM and TPS analysis. Experimental results showed that the method can evaluate the appropriateness of the melody sufficiently well."
Fields B.; Rhodes C.; Casey M.; Jacobson K.,Social playlists and bottleneck measurements: Exploiting musician social graphs using content-based dissimilarity and pairwise maximum flow values,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415927&partnerID=40&md5=481195a8158891b1f816601ceabc4cbd,"Fields B., Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; Rhodes C., Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; Casey M., Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; Jacobson K., Centre for Digital Music, Queen Mary University of London, United Kingdom","We have sampled the artist social network of Myspace and to it applied the pairwise relational connectivity measure Minimum cut/Maximum flow. These values are then compared to a pairwise acoustic Earth Mover's Distance measure and the relationship is discussed. Further, a means of constructing playlists using the maximum flow value to exploit both the social and acoustic distances is realized."
Raimond Y.; Sandler M.,A Web of musical information,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052082822&partnerID=40&md5=3c91b1662ba1f91c2f069dd669cd7276,"Raimond Y., Centre for Digital Music, Queen Mary, University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, United Kingdom","We describe our recent achievements in interlinking several music-related data sources on the Semantic Web. In particular, we describe interlinked datasets dealing with Creative Commons content, editorial, encyclopedic, geographic and statistical data, along with queries they can answer and tools using their data. We describe our web services, providing an on-demand access to content-based features linked with such data sources and information pertaining to their creation (including processing steps, applied algorithms, inputs, parameters or associated developers). We also provide a tool allowing such music analysis services to be set up and scripted in a simple way."
Kleedorfer F.; Knees P.; Pohle T.,OH OH OH WHOAH! Towards automatic topic detection in song lyrics,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426396&partnerID=40&md5=804b7f846333daec32e343ec47054bae,"Kleedorfer F., Studio Smart Agent Technologies, Research Studios, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria",We present an algorithm that allows for indexing music by topic. The application scenario is an information retrieval system into which any song with known lyrics can be inserted and indexed so as to make a music collection browse-able by topic. We use text mining techniques for creating a vector space model of our lyrics collection and non-negative matrix factorization (NMF) to identify topic clusters which are then labeled manually. We include a discussion of the decisions regarding the parametrization of the applied methods. The suitability of our approach is assessed by measuring the agreement of test subjects who provide the labels for the topic clusters.
Baccigalupo C.; Plaza E.; Donaldson J.,Uncovering affinity of artists to multiple genres from social behaviour data,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427741&partnerID=40&md5=c5bcdbbd5a94bed5ebd9fad9f486d52b,"Baccigalupo C., IIIA, Artificial Intelligence Research Institute, CSIC, Spanish Council for Scientific Research, Spain; Plaza E., IIIA, Artificial Intelligence Research Institute, CSIC, Spanish Council for Scientific Research, Spain; Donaldson J., Indiana University, School of Informatics, United States","In organisation schemes, musical artists are commonly identified with a unique 'genre' label attached, even when they have affinity to multiple genres. To uncover this hidden cultural awareness about multi-genre affinity, we present a new model based on the analysis of the way in which a community of users organise artists and genres in playlists. Our work is based on a novel dataset that we have elaborated identifying the co-occurrences of artists in the playlists shared by the members of a popular Web-based community, and that is made publicly available. The analysis defines an automatic social-based method to uncover relationships between artists and genres, and introduces a series of novel concepts that characterises artists and genres in a richer way than a unique 'genre' label would do."
Ravelli E.; Richard G.; Daudet L.,Fast MIR in a sparse transform domain,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435245&partnerID=40&md5=65a76c523c7a4dad9b4c81380610818f,"Ravelli E., Université Paris 6, France; Richard G., TELECOM ParisTech., France; Daudet L., Université Paris 6, France","We consider in this paper sparse audio coding as an alternative to transform audio coding for efficient MIR in the transform domain. We use an existing audio coder based on a sparse representation in a union of MDCT bases, and propose a fast algorithm to compute mid-level representations for beat tracking and chord recognition, respectively an onset detection function and a chromagram. The resulting transform domain system is significantly faster than a comparable state-of-the-art system while obtaining close performance above 8 kbps."
Toh C.C.; Zhang B.; Wang Y.,Multiple-feature fusion based onset detection for solo singing voice,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472314&partnerID=40&md5=47bdca71e522d802fb65056dba97c2cd,"Toh C.C., School of Computing, National University of Singapore, Singapore, Singapore; Zhang B., School of Computing, National University of Singapore, Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore","Onset detection is a challenging problem in automatic singing transcription. In this paper, we address singing onset detection with three main contributions. First, we outline the nature of a singing voice and present a new singing onset detection approach based on supervised machine learning. In this approach, two Gaussian Mixture Models (GMMs) are used to classify audio features of onset frames and non-onset frames. Second, existing audio features are thoroughly evaluated for this approach to singing onset detection. Third, feature-level and decision-level fusion are employed to fuse different features for a higher level of performance. Evaluated on a recorded singing database, the proposed approach outperforms state-of-the-art onset detection algorithms significantly."
Dopler M.; Schedl M.; Pohle T.; Knees P.,Accessing music collections via representative cluster prototypes in a hierarchical organization scheme,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419184&partnerID=40&md5=3693e64dac787bf03cb735a5de284143,"Dopler M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria","This paper addresses the issue of automatically organizing a possibly large music collection for intuitive access. We present an approach to cluster tracks in a hierarchical manner and to automatically find representative pieces of music for each cluster on each hierarchy level. To this end, audio signal-based features are complemented with features derived via Web content mining in a novel way. Automatic hierarchical clustering is performed using a variant of the Self-Organizing Map, which we further modified in order to create playlists containing similar tracks. The proposed approaches for playlist generation on a hierarchically structured music collection and finding prototypical tracks for each cluster are then integrated into the Traveller's Sound Player, a mobile audio player application that organizes music in a playlist such that the distances between consecutive tracks are minimal. We extended this player to deal with the hierarchical nature of the playlists generated by the proposed structuring approach. As for evaluation, we first assess the quality of the clustering method using the measure of entropy on a genre-annotated test set. Second, the goodness of the method to find prototypical tracks for each cluster is investigated in a user study."
Tsunoo E.; Ono N.; Sagayama S.,Musical bass-line pattern clustering and its application to audio genre classification,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953292148&partnerID=40&md5=501772da2c1a131fcfb93adac749747a,"Tsunoo E., Graduate School of Information Science and Technology, University of Tokyo, Japan; Ono N., Graduate School of Information Science and Technology, University of Tokyo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Japan","This paper discusses a new approach for clustering musical bass-line patterns representing particular genres and its application to audio genre classification. Many musical genres are characterized not only by timbral information but also by distinct representative bass-line patterns. So far this kind of temporal features have not so effectively been utilized. In particular, modern music songs mostly have certain fixed bar-long bass-line patterns per genre. For instance, while frequently bass-lines in rock music have constant pitch and a uniform rhythm, in jazz music there are many characteristic movements such as walking bass. We propose a representative bass-line pattern template extraction method based on k-means clustering handling a pitchshift problem. After extracting the fundamental bass-line pattern templates for each genre, distances from each template are calculated and used as a feature vector for supervised learning. Experimental result shows that the automatically calculated bass-line pattern information can be used for genre classification effectively and improve upon current approaches based on timbral features. © 2009 International Society for Music Information Retrieval."
Tsai W.-H.; Liao S.-J.; Lai C.,Automatic identification of simultaneous singers in duet recordings,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866640041&partnerID=40&md5=6e4988fa663def9f461fbd2da24368ee,"Tsai W.-H., Graduate Institute of Computer and Communication Engineering, National Taipei University of Technology, Taipei, Taiwan; Liao S.-J., Graduate Institute of Computer and Communication Engineering, National Taipei University of Technology, Taipei, Taiwan; Lai C., Open Text Corporation, Ottawa, ON, Canada","The problem of identifying singers in music recordings has received considerable attention with the explosive growth of the Internet and digital media. Although a number of studies on automatic singer identification from acoustic features have been reported, most systems to date, however, reliably establish the identity of singers in solo recordings only. The research presented in this paper attempts to automatically identify singers in music recordings that contain overlapping singing voices. Two approaches to overlapping singer identification are proposed and evaluated. Results obtained demonstrate the feasibility of the systems."
Moh Y.; Buhmann J.M.,Kernel expansion for online preference tracking,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951967520&partnerID=40&md5=cee49eb4c1ec724b000ec67b0eb37476,"Moh Y., Institute of Computational Science, Swiss Federal Institute of Technology (ETH) Zurich, Switzerland; Buhmann J.M., Institute of Computational Science, Swiss Federal Institute of Technology (ETH) Zurich, Switzerland","User preferences of music genres can significantly changes over time depending on fashions and the personal situation of music consumers. We propose a model to learn user preferences and their changes in an adaptive way. Our approach refines a model for user preferences by explicitly considering two plausible constraints of computational costs and limited storage space. The model is required to adapt itself to changing data distributions, and yet be able to compress ""historical"" data. We exploit the success of kernel SVM, and we consider an online expansion of the induced space as a preprocessing step to a simple linear online learner that updates with maximal agreement to previously seen data."
Van Kranenburg P.; Volk A.; Wiering F.; Veltkamp R.C.,Musical models for folk-song melody alignment,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952347597&partnerID=40&md5=20d969a00bef9f148eda6ba56b8c3ec4,"Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Netherlands; Volk A., Department of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Department of Information and Computing Sciences, Utrecht University, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Netherlands","In this paper we show that the modeling of musical knowledge within alignment algorithms results in a successful similarity approach to melodies. The score of the alignment of two melodies is taken as a measure of similarity. We introduce a number of scoring functions that model the influence of different musical parameters. The evaluation of their retrieval performance on a well-annotated set of 360 folk-song melodies with various kinds of melodic variation, shows that a combination of pitch, rhythm and segmentation-based scoring functions performs best, with a mean average precision of 0.83. © 2009 International Society for Music Information Retrieval."
Riley J.,Application of the Functional Requirements for Bibliographic Records (FRBR) to music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052540367&partnerID=40&md5=696dad3f6f7019b09b88abff02b32dc6,"Riley J., Indiana University, Digital Library Program, United States","This paper describes work applying the Functional Requirements for Bibliographic Records (FRBR) model to music, as the basis for implementing a fully FRBR-compliant music digital library system. A detailed analysis of the FRBR and Functional Requirements for Authority Data (FRAD) entities and attributes is presented. The paper closes with a discussion of the ways in which FRBR is gaining adoption outside of the library environment in which it was born. This work benefits the MIR community by demonstrating a model that can be used in MIR systems for the storage of descriptive information in support of metadata-based searching, and by positioning the Variations system to be a source of robust descriptive information for use by third-party MIR systems."
Typke R.; Walczak-Typke A.C.,A tunneling-vantage indexing method for non-metrics,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949634823&partnerID=40&md5=683e21f91f3c86033493dd738c1aebd4,"Typke R., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Walczak-Typke A.C., Kurt Gödel Research Center for Mathematical Logic, University of Vienna, Austria","We consider an instance of the Earth Mover's Distance (EMD) useful for comparing rhythmical patterns. To make searches for r-near neighbours efficient, we decompose our search space into disjoint metric subspaces, in each of which the EMD reduces to the l1 norm. We then use a combined approach of two methods, one for searching within the subspaces, the other for searching between them. For the former, we show how one can use vantage indexing without false positives nor false negatives for solving the exact r-near neighbour problem, and find an optimum number and placement of vantage objects for this result. For searching between subspaces, where the EMD is not a metric, we show how one can guarantee that still no false negatives occur, and the percentage of false positives is reduced as the search radius is increased."
McKay C.; Fujinaga I.,"Combining features extracted from audio, symbolic and cultural sources",2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462486&partnerID=40&md5=4e8a1212a09e70f30c0fe73c5ff52015,"McKay C., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada","This paper experimentally investigates the classification utility of combining features extracted from separate audio, symbolic and cultural sources of musical information. This was done via a series of genre classification experiments performed using all seven possible combinations and subsets of the three corresponding types of features. These experiments were performed using jMIR, a software suite designed for use both as a toolset for performing MIR research and as a platform for developing and sharing new algorithms. The experimental results indicate that combining feature types can indeed substantively improve classification accuracy. Accuracies of 96.8% and 78.8% were attained respectively on 5 and 10-class genre taxonomies when all three feature types were combined, compared to average respective accuracies of 85.5% and 65.1% when features extracted from only one of the three sources of data were used. It was also found that combining feature types decreased the seriousness of those misclassifications that were made, on average, particularly when cultural features were included."
Daniel A.; Emiya V.; David B.,Perceptually-based evaluation of the errors usually made when automatically transcribing music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873454679&partnerID=40&md5=4f061999eb77650db8ce132aa88db26f,"Daniel A., TELECOM ParisTech (ENST), CNRS LTCI, 75634 Paris Cedex 13, 46, rue Barrault, France; Emiya V., TELECOM ParisTech (ENST), CNRS LTCI, 75634 Paris Cedex 13, 46, rue Barrault, France; David B., TELECOM ParisTech (ENST), CNRS LTCI, 75634 Paris Cedex 13, 46, rue Barrault, France","This paper investigates the perceptual importance of typical errors occurring when transcribing polyphonic music excerpts into a symbolic form. The case of the automatic transcription of piano music is taken as the target application and two subjective tests are designed. The main test aims at understanding how human subjects rank typical transcription errors such as note insertion, deletion or replacement, note doubling, incorrect note onset or duration, and so forth. The Bradley-Terry-Luce (BTL) analysis framework is used and the results show that pitch errors are more clearly perceived than incorrect loudness estimations or temporal deviations from the original recording. A second test presents a first attempt to include this information in more perceptually motivated measures for evaluating transcription systems."
Anglade A.; Dixon S.,Characterisation of harmony with inductive logic programming,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053098601&partnerID=40&md5=1c76ca682bf73267e0f3a5662541437f,"Anglade A., Queen Mary University of London, Centre for Digital Music, United Kingdom; Dixon S., Queen Mary University of London, Centre for Digital Music, United Kingdom","We present an approach for the automatic characterisation of the harmony of song sets making use of relational induction of logical rules. We analyse manually annotated chord data available in RDF and interlinked with web identifiers for chords which themselves give access to the root, bass, component intervals of the chords. We pre-process these data to obtain high-level information such as chord category, degree and intervals between chords before passing them to an Inductive Logic Programming software which extracts the harmony rules underlying them. This framework is tested over the Beatles songs and the Real Book songs. It generates a total over several experiments of 12,450 harmony rules characterising and differentiating the Real Book (jazz) songs and the Beatles' (pop) music. Encouragingly, a preliminary analysis of the most common rules reveals a list of well-known pop and jazz patterns that could be completed by a more in depth analysis of the other rules."
Lartillot O.; Eerola T.; Toiviainen P.; Fornari J.,"Multi-feature modeling of pulse clarity: Design, validation and optimization",2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473239&partnerID=40&md5=53d438a5b0b17a7fb76b95a37e6646af,"Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Toiviainen P., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Fornari J., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland","Pulse clarity is considered as a high-level musical dimension that conveys how easily in a given musical piece, or a particular moment during that piece, listeners can perceive the underlying rhythmic or metrical pulsation. The objective of this study is to establish a composite model explaining pulse clarity judgments from the analysis of audio recordings. A dozen of descriptors have been designed, some of them dedicated to low-level characterizations of the onset detection curve, whereas the major part concentrates on descriptions of the periodicities developed throughout the temporal evolution of music. A high number of variants have been derived from the systematic exploration of alternative methods proposed in the literature on onset detection curve estimation. To evaluate the pulse clarity model and select the best predictors, 25 participants have rated the pulse clarity of one hundred excerpts from movie soundtracks. The mapping between the model predictions and the ratings was carried out via regressions. Nearly a half of listeners' rating variance can be explained via a combination of periodicity-based factors."
Flanagan P.,Quantifying metrical ambiguity,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870940787&partnerID=40&md5=b55b2e0a31602661226887ef9215a3d2,,"This paper explores how data generated by meter induction models may be recycled to quantify metrical ambiguity, which is calculated by measuring the dispersion of metrical induction strengths across a population of possible meters. A measure of dispersion commonly used in economics to measure income inequality, the Gini coefficient, is introduced for this purpose. The value of this metric as a rhythmic descriptor is explored by quantifying the ambiguity of several common clave patterns and comparing the results to other metrics of rhythmic complexity and syncopation."
Müller M.; Konz V.; Scharfstein A.; Ewert S.; Clausen M.,Towards automated extraction of tempo parameters from expressive music recordings,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952339105&partnerID=40&md5=b28560ba4f0ef58c6f84449d908e85bf,"Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Konz V., MPI Informatik, Saarland University, Saarbrücken, Germany; Scharfstein A., MPI Informatik, Saarland University, Saarbrücken, Germany; Ewert S., Computer Science, Bonn University, Bonn, Germany; Clausen M., Computer Science, Bonn University, Bonn, Germany","A performance of a piece of music heavily depends on the musician's or conductor's individual vision and personal interpretation of the given musical score. As basis for the analysis of artistic idiosyncrasies, one requires accurate annotations that reveal the exact timing and intensity of the various note events occurring in the performances. In the case of audio recordings, this annotation is often done manually, which is prohibitive in view of large music collections. In this paper, we present a fully automatic approach for extracting temporal information from a music recording using score-audio synchronization techniques. This information is given in the form of a tempo curve that reveals the relative tempo difference between an actual performance and some reference representation of the underlying musical piece. As shown by our experiments on harmony-based Western music, our approach allows for capturing the overall tempo flow and for certain classes of music even finer expressive tempo nuances. © 2009 International Society for Music Information Retrieval."
Downie J.S.; Bay M.; Ehmann A.F.; Jones M.C.,Audio cover song identification: Mirex 2006-2007 results and analyses,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475412&partnerID=40&md5=fa899ccf953819b734dad31b7e9c5cd3,"Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Bay M., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Jones M.C., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","This paper presents analyses of the 2006 and 2007 results of the Music Information Retrieval Evaluation eXchange (MIREX) Audio Cover Song Identification (ACS) tasks. The Music Information Retrieval Evaluation eXchange (MIREX) is a community-based endeavor to scientifically evaluate music information retrieval (MIR) algorithms and techniques. The ACS task was created to motivate MIR researchers to expand their notions of similarity beyond acoustic similarity to include the important idea that musical works retain their identity notwithstanding variations in style, genre, orchestration, rhythm or melodic ornamentation, etc. A series of statistical analyses were performed that indicate significant improvements in this domain have been made over the course of 2006-2007. Post-hoc analyses reveal distinct differences between individual systems and the effects of certain classes of queries on performance. This paper discusses some of the techniques that show promise in this research domain."
Masahiro N.; Takaesu H.; Demachi H.; Oono M.; Saito H.,Development of an automatic music selection system based on runner's step frequency,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415395&partnerID=40&md5=6ebadee91dddfa17d3f5e14b8bdbdc56,"Masahiro N., Department of Computer Science, Keio University, Yokohama, Japan; Takaesu H., Department of Computer Science, Keio University, Yokohama, Japan; Demachi H., Department of Computer Science, Keio University, Yokohama, Japan; Oono M., Department of Computer Science, Keio University, Yokohama, Japan; Saito H., Department of Computer Science, Keio University, Yokohama, Japan","This paper presents an automatic music selection system based on runner's step frequency. Recent development of portable music players like iPod has increased the number of those who listen to music while exercising. However, few systems which connect exercises with music selection have been developed. We propose a system that automatically selects music suitable for user's running exercises. Although many parameters can be taken into account, as a first step we focus on runner's step frequency. This system selects music with tempo suitable for runner's step frequency and when runner's step frequency changes, it executes another music selection. The system consists of three modules: step frequency estimation, music selection, and music playing. In the first module, runner's step frequency is estimated from data derived from an acceleration sensor. In the second module, appropriate music is selected based on the estimated step frequency. In the third module, the selected music is played until runner's step frequency changes. In the experiment, subjects ran on a running machine at different paces listening to the music selected by the proposed system. Experimental results show that the system can estimate runner's SPM accurately and on the basis of the estimated SPM it can select music appropriate for users' exercises with more than 85.0% accuracy, and makes running exercises more pleasing."
Hoffman M.; Blei D.; Cook P.,Content-based musical similarity computation using the hierarchical dirichlet process,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420337&partnerID=40&md5=221726171f59818ff41bb91bcb69114e,"Hoffman M., Princeton University, Dept. of Computer Science, United States; Blei D., Princeton University, Dept. of Computer Science, United States; Cook P., Princeton University, Dept. of Computer Science, United States, Dept. of Music, Princeton University, United States","We develop a method for discovering the latent structure in MFCC feature data using the Hierarchical Dirichlet Process (HDP). Based on this structure, we compute timbral similarity between recorded songs. The HDP is a nonparametric Bayesian model. Like the Gaussian Mixture Model (GMM), it represents each song as a mixture of some number of multivariate Gaussian distributions However, the number of mixture components is not fixed in the HDP, but is determined as part of the posterior inference process. Moreover, in the HDP the same set of Gaussians is used to model all songs, with only the mixture weights varying from song to song. We compute the similarity of songs based on these weights, which is faster than previous approaches that compare single Gaussian distributions directly. Experimental results on a genre-based retrieval task illustrate that our HDP-based method is both faster and produces better retrieval quality than such previous approaches."
Kirlin P.B.; Utgoff P.E.,A framework for automated Schenkerian analysis,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249244118&partnerID=40&md5=cf3632e4fc71c8eb5179839216ec1330,"Kirlin P.B., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States; Utgoff P.E., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States","In Schenkerian analysis, one seeks to find structural dependences among the notes of a composition and organize these dependences into a coherent hierarchy that illustrates the function of every note. This type of analysis reveals multiple levels of structure in a composition by constructing a series of simplifications of a piece showing various elaborations and prolongations. We present a framework for solving this problem, called IVI, that uses a state-space search formalism. IVI includes multiple interacting components, including modules for various preliminary analyses (harmonic, melodic, rhythmic, and cadential), identifying and performing reductions, and locating pieces of the Ursatz. We describe a number of the algorithms by which IVI forms, stores, and updates its hierarchy of notes, along with details of the Ursatz-finding algorithm. We illustrate IVI's functionality on an excerpt from a Schubert piano composition, and also discuss the issues of subproblem interactions and the multiple parsings problem."
Murata K.; Nakadai K.; Yoshii K.; Takeda R.; Torii T.; Okuno H.G.; Hasegawa Y.; Tsujino H.,A robot singer with music recognition based on real-time beat tracking,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455176629&partnerID=40&md5=69ec73c922ebf93f21e7e546dc023b27,"Murata K., Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Japan; Nakadai K., Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Japan, Honda Research Institute Japan Co., Ltd., Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan; Takeda R., Graduate School of Informatics, Kyoto University, Japan; Torii T., Honda Research Institute Japan Co., Ltd., Japan; Okuno H.G., Graduate School of Informatics, Kyoto University, Japan; Hasegawa Y., Honda Research Institute Japan Co., Ltd., Japan; Tsujino H., Honda Research Institute Japan Co., Ltd., Japan","A robot that can provide an active and enjoyable user interface is one of the most challenging applications for music information processing, because the robot should cope with high-power noises including self voices and motor noises. This paper proposes noise-robust musical beat tracking by using a robot-embedded microphone, and describes its application to a robot singer with music recognition. The proposed beat tracking introduces two key techniques, that is, spectro-temporal pattern matching and echo cancellation. The former realizes robust tempo estimation with a shorter window length, thus, it can quickly adapt to tempo changes. The latter is effective to cancel self periodic noises such as stepping, scatting, and singing. We constructed a robot singer based on the proposed beat tracking for Honda ASIMO. The robot detects a musical beat with its own microphone in a noisy environment. It tries to recognize music based on the detected musical beat. When it successfully recognizes music, it sings while stepping according to the beat. Otherwise, it performs scatting instead of singing because the lyrics are unavailable. Experimental results showed fast adaptation to tempo changes and high robustness in beat tracking even when stepping, scatting and singing."
Pearce M.T.; Müllensiefen D.; Wiggins G.A.,A comparison of statistical and rule-based models of melodic segmentation,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053111706&partnerID=40&md5=654ad136fd432762dbd152c5919173b0,"Pearce M.T., Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom; Müllensiefen D., Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom; Wiggins G.A., Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom",We introduce a new model for melodic segmentation based on information-dynamic analysis of melodic structure. The performance of the model is compared to several existing algorithms in predicting the annotated phrase boundaries in a large corpus of folk music.
Mauch M.; Dixon S.,A discrete mixture model for chord labelling,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955840442&partnerID=40&md5=5d1df9525e3999d2acaa889a9ab1c1d2,"Mauch M., Queen Mary, University of London, Centre for Digital Music, United Kingdom; Dixon S., Queen Mary, University of London, Centre for Digital Music, United Kingdom","Chord labels for recorded audio are in high demand both as an end product used by musicologists and hobby musicians and as an input feature for music similarity applications. Many past algorithms for chord labelling are based on chromagrams, but distribution of energy in chroma frames is not well understood. Furthermore, non-chord notes complicate chord estimation. We present a new approach which uses as a basis a relatively simple chroma model to represent short-time sonorities derived from melody range and bass range chromagrams. A chord is then modelled as a mixture of these sonorities, or subchords. We prove the practicability of the model by implementing a hidden Markov model (HMM) for chord labelling, in which we use the discrete subchord features as observations. We model gamma-distributed chord durations by duplicate states in the HMM, a technique that had not been applied to chord labelling. We test the algorithm by five-fold cross-validation on a set of 175 hand-labelled songs performed by the Beatles. Accuracy figures compare very well with other state of the art approaches. We include accuracy specified by chord type as well as a measure of temporal coherence."
Sumi K.; Itoyama K.; Yoshii K.; Komatani K.; Ogata T.; Okuno H.G.,Automatic chord recognition based on probabilistic integration of chord transition and bass pitch estimation,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052068532&partnerID=40&md5=65c7c1cc5bd6fa5b9c81122add5c2c20,"Sumi K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Itoyama K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper presents a method that identifies musical chords in polyphonic musical signals. As musical chords mainly represent the harmony of music and are related to other musical elements such as melody and rhythm, the performance of chord recognition should improve if this interrelationship is taken into consideration. Nevertheless, this interrelationship has not been utilized in the literature as far as the authors are aware. In this paper, bass lines are utilized as clues for improving chord recognition because they can be regarded as an element of the melody. A probabilistic framework is devised to uniformly integrate bass lines extracted by using bass pitch estimation into a hypothesis-search-based chord recognition. To prune the hypothesis space of the search, the hypothesis reliability is defined as the weighted sum of three reliabilities: the likelihood of Gaussian Mixture Models for the observed features, the joint probability of chord and bass pitch, and the chord transition N-gram probability. Experimental results show that our method recognized the chord sequences of 150 songs in twelve Beatles albums; the average frame-rate accuracy of the results was 73.4%."
Trohidis K.; Tsoumakas G.; Kalliris G.; Vlahavas I.,Multi-label classification of music into emotions,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447495&partnerID=40&md5=a28dd212d81d2cebb943bdc60eab3a5c,"Trohidis K., Dept. of Journalism and Mass Communication, Aristotle University of Thessaloniki, Greece; Tsoumakas G., Dept. of Informatics, Aristotle University of Thessaloniki, Greece; Kalliris G., Dept. of Journalism and Mass Communication, Aristotle University of Thessaloniki, Greece; Vlahavas I., Dept. of Informatics, Aristotle University of Thessaloniki, Greece","In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features."
Niedermayer B.,Non-negative matrix division for the automatic transcription of polyphonic music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856171852&partnerID=40&md5=88db8781758c6e910581924f34497b18,"Niedermayer B., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper we present a new method in the style of non-negative matrix factorization for automatic transcription of polyphonic music played by a single instrument (e.g., a piano). We suggest using a fixed repository of base vectors corresponding to tone models of single pitches played on a certain instrument. This assumption turns the blind factorization into a kind of non-negative matrix division for which an algorithm is presented. The same algorithm can be applied for learning the model dictionary from sample tones as well. This method is biased towards the instrument used during the training phase. But this is admissible in applications like performance analysis of solo music. The proposed approach is tested on a Mozart sonata where a symbolic representation is available as well as the recording on a computer controlled grand piano."
Xiao L.; Tian A.; Li W.; Zhou J.,Using a statistic model tocapture the association between timbre and perceived tempo,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461657&partnerID=40&md5=06ff45bdf07073a50efc0bfa9a6a0f9f,"Xiao L., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China; Tian A., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China; Li W., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China; Zhou J., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China","The estimation of the perceived tempo is required in many MIR applications. However, automatic tempo estimation itself is still an open problem due to the insufficient understanding of the inherent mechanisms of the tempo perception. Published methods only use the information of rhythm pattern, so they may meet the half/double tempo error problem. To solve this problem, We propose to use statistic model to investigate the association between timbre and tempo and use timbre information to improve the performance of tempo estimation. Experiment results show that this approach performs at least comparably to existing tempo extraction algorithms."
Riley M.; Heinen E.; Ghosh J.,A text retrieval approach to content-based audio retrieval,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949578539&partnerID=40&md5=ade085468c8aa9e36f468007fa83313c,"Riley M., University of Texas, Austin, United States; Heinen E., University of Texas, Austin, United States; Ghosh J., University of Texas, Austin, United States","This paper presents a novel approach to robust, content-based retrieval of digital music. We formulate the hashing and retrieval problems analogously to that of text retrieval and leverage established results for this unique application. Accordingly, songs are represented as a ""Bag-of-Audio- Words"" and similarity calculations follow directly from the well-known Vector Space model [12]. We evaluate our system on a 4000 song data set to demonstrate its practical applicability, and evaluation shows our technique to be robust to a variety of signal distortions. Most interestingly, the system is capable of matching studio recordings to live recordings of the same song with high accuracy."
De Lima E.T.; Ramalho G.,On rhythmic pattern extraction in bossa nova music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428739&partnerID=40&md5=0a582253ff9686ea62fa6731e4783305,"De Lima E.T., Centro de Informätica (CIn), Univ. Federal de Pernambuco, 50732-970 - Recife - PE, Caixa Postal 7851, Brazil; Ramalho G., Centro de Informätica (CIn), Univ. Federal de Pernambuco, 50732-970 - Recife - PE, Caixa Postal 7851, Brazil","The analysis of expressive performance, an important research topic in Computer Music, is almost exclusively devoted to the study of Western Classical piano music. Instruments like the acoustic guitar and styles like Bossa Nova and Samba have been little studied, despite their harmonic and rhythmic richness. This paper describes some experimental results obtained with the extraction of rhythmic patterns from the guitar accompaniment of Bossa Nova songs. The songs, played by two different performers and recorded with the help of a MIDI guitar, were represented as strings and processed by FlExPat, a string matching algorithm. The results obtained were then compared to a previously acquired catalogue of ""good"" patterns."
Jacobson K.; Sandler M.; Fields B.,Using audio analysis and network structure to identify communities in on-line social networks of artists,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427408&partnerID=40&md5=91fc7a4ea8a5f93f3d66c827de23436a,"Jacobson K., Centre for Digital Music, Queen Mary, University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, United Kingdom; Fields B., Goldsmiths Digital Studios, Goldsmiths, University of London, United Kingdom","Community detection methods from complex network theory are applied to a subset of the Myspace artist network to identify groups of similar artists. Methods based on the greedy optimization of modularity and random walks are used. In a second iteration, inter-artist audio-based similarity scores are used as input to enhance these community detection methods. The resulting community structures are evaluated using a collection of artist-assigned genre tags. Evidence suggesting the Myspace artist network structure is closely related to musical genre is presented and a Semantic Web service for accessing this structure is described."
Dannenberg R.B.; Wasserman L.,Estimating the error distribution of a tap sequence without ground truth,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751665767&partnerID=40&md5=baa2f2d4424e7853724be02b5bae8fba,"Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States; Wasserman L., School of Computer Science, Carnegie Mellon University, United States","Detecting beats, estimating tempo, aligning scores to audio, and detecting onsets are all interesting problems in the field of music information retrieval. In much of this research, it is convenient to think of beats as occuring at precise time points. However, anyone who has attempted to label beats by hand soon realizes that precise annotation of music audio is not possible. A common method of beat annotation is simply to tap along with audio and record the tap times. This raises the question: How accurate are the taps? It may seem that an answer to this question would require knowledge of ""true"" beat times. However, tap times can be characterized as a random distribution around true beat times. Multiple independent taps can be used to estimate not only the location of the true beat time, but also the statistical distribution of measured tap times around the true beat time. Thus, without knowledge of true beat times, and without even requiring the existence of precise beat times, we can estimate the uncertainty of tap times. This characterization of tapping can be useful for estimating tempo variation and evaluating alternative annotation methods. © 2009 International Society for Music Information Retrieval."
Turnbull D.; Barrington L.; Lanckriet G.,Five approaches to collecting tags for music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873467755&partnerID=40&md5=c8425a97fbc9470b91f6d73efc08b5ad,"Turnbull D., UC, San Diego, United States; Barrington L., UC, San Diego, United States; Lanckriet G., UC, San Diego, United States","We compare five approaches to collecting tags for music: conducting a survey, harvesting social tags, deploying annotation games, mining web documents, and autotagging audio content. The comparison includes a discussion of both scalability (financial cost, human involvement, and computational resources) and quality (the cold start problem & popularity bias, strong vs. weak labeling, vocabulary structure & size, and annotation accuracy). We then describe one state-of-the-art system for each approach. The performance of each system is evaluated using a tag-based music information retrieval task. Using this task, we are able to quantify the effect of popularity bias on each approach by making use of a subset of more popular (short-head) songs and a set of less popular (long-tail) songs. Lastly, we propose a simple hybrid context-content system that combines our individual approaches and produces superior retrieval results."
Camacho A.,Detection of pitched/unpitched sound using pitch strength clustering,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955789679&partnerID=40&md5=a0f6ab2342c6a5b6e158370a7b3b0b33,"Camacho A., Computer and Information Science and Engineering Department, University of Florida, Gainesville, FL 32611, United States","A method for detecting pitched/unpitched sound is presented. The method tracks the pitch strength trace of the signal, determining clusters of pitch and unpitched sound. The criterion used to determine the clusters is the local maximization of the distance between the centroids. The method makes no assumption about the data except that the pitched and unpitched clusters have different centroids. This allows the method to dispense with free parameters. The method is shown to be more reliable than using fixed thresholds when the SNR is unknown."
Barrington L.; Yazdani M.; Turnbull D.; Lanckriet G.,Combining feature kernels for semantic music retrieval,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421439&partnerID=40&md5=6b0e2ff377d9383f7f16a70fbac86448,"Barrington L., Electrical and Computer Engineering, University of California, San Diego, United States; Yazdani M., Electrical and Computer Engineering, University of California, San Diego, United States; Turnbull D., Electrical and Computer Engineering, University of California, San Diego, United States, Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","We apply a new machine learning tool, kernel combination, to the task of semantic music retrieval. We use 4 different types of acoustic content and social context feature sets to describe a large music corpus and derive 4 individual kernel matrices from these feature sets. Each kernel is used to train a support vector machine (SVM) classifier for each semantic tag (e.g., 'aggressive', 'classic rock', 'distorted electric guitar') in a large tag vocabulary. We examine the individual performance of each feature kernel and then show how to learn an optimal linear combination of these kernels using convex optimization. We find that the retrieval performance of the SVMs trained using the combined kernel is superior to SVMs trained using the best individual kernel for a large number of tags. In addition, the weights placed on individual kernels in the linear combination reflect the relative importance of each feature set when predicting a tag."
Scaringella N.,Timbre and rhythmic TRAP-TANDEM features for music information retrieval,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863762281&partnerID=40&md5=0458b163436ae1afd595bf8251b81da8,"Scaringella N., Idiap Research Institute, Martigny, Switzerland, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland","The enormous growth of digital music databases has led to a comparable growth in the need for methods that help users organize and access such information. One area in particular that has seen much recent research activity is the use of automated techniques to describe audio content and to allow for its identification, browsing and retrieval. Conventional approaches to music content description rely on features characterizing the shape of the signal spectrum in relatively short-term frames. In the context of Automatic Speech Recognition (ASR), Hermansky [7] described an interesting alternative to short-term spectrum features, the TRAP-TANDEM approach which uses long-term band-limited features trained in a supervised fashion. We adapt this idea to the specific case of music signals and propose a generic system for the description of temporal patterns. The same system with different settings is able to extract features describing either timbre or rhythmic content. The quality of the generated features is demonstrated in a set of music retrieval experiments and compared to other state-of-the-art models."
Manaris B.; Krehbiel D.; Roos P.; Zalonis T.,Armonique: Experiments in content-based similarity retrieval using power-law melodic and timbre metrics,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052432239&partnerID=40&md5=be9edabda185abe59dd437e481bcbb24,"Manaris B., Computer Science Department, College of Charleston, Charleston, SC 29424, 66 George Street, United States; Krehbiel D., Psychology Department, Bethel College, North Newton, KS 67117, 300 E. 27th Street, United States; Roos P., Computer Science Department, College of Charleston, Charleston, SC 29424, 66 George Street, United States; Zalonis T., Computer Science Department, College of Charleston, Charleston, SC 29424, 66 George Street, United States","This paper presents results from an on-going MIR study utilizing hundreds of melodic and timbre features based on power laws for content-based similarity retrieval. These metrics are incorporated into a music search engine prototype, called Armonique. This prototype is used with a corpus of 9153 songs encoded in both MIDI and MP3 to identify pieces similar to and dissimilar from selected songs. The MIDI format is used to extract various power-law features measuring proportions of music-theoretic and other attributes, such as pitch, duration, melodic intervals, and chords. The MP3 format is used to extract power-law features measuring proportions within FFT power spectra related to timbre. Several assessment experiments have been conducted to evaluate the effectiveness of the similarity model. The results suggest that power-law metrics are very promising for content-based music querying and retrieval, as they seem to correlate with aspects of human emotion and aesthetics."
Lee K.; Cremer M.,Segmentation-based lyrics-audio alignment using dynamic programming,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423338&partnerID=40&md5=33e315cbfdf989df664cd0c166df2d07,"Lee K., Media Technology Lab., Gracenote, Emeryville, CA 94608, United States; Cremer M., Media Technology Lab., Gracenote, Emeryville, CA 94608, United States","In this paper, we present a system for automatic alignment of textual lyrics with musical audio. Given an input audio signal, structural segmentation is first performed and similar segments are assigned a label by computing the distance between the segment pairs. Using the results of segmentation and hand-labeled paragraphs in lyrics as a pair of input strings, we apply a dynamic programming (DP) algorithm to find the best alignment path between the two strings, achieving segment-to-paragraph synchronization. We demonstrate that the proposed algorithm performs well for various kinds of musical audio."
Sapp C.S.,Hybrid numeric/rank similarity metrics for musical performance analysis,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861214286&partnerID=40&md5=3e5f39268eeda8b9da26df36c2bfd367,"Sapp C.S., CHARM, Royal Holloway, University of London, United Kingdom","This paper describes a numerical method for examining similarities among tempo and loudness features extracted from recordings of the same musical work and evaluates its effectiveness compared to Pearson correlation. Starting with correlation at multiple timescales, other concepts such as a performance ""noise-floor"" are used to generate measurements which are more refined than correlation alone. The measurements are evaluated and compared to plain correlation in their ability to identify performances of the same Chopin mazurka played by the same pianist out of a collection of recordings by various pianists."
Magno T.; Sable C.,"A comparison of signal-based music recommendation to genre labels, collaborative filtering, musicological analysis, human recommendation, and random baseline",2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958014369&partnerID=40&md5=906fb0d7d311b7cc5457274b2921f6ad,"Magno T., Cooper Union, United States; Sable C., Cooper Union, United States","The emergence of the Internet as today's primary medium of music distribution has brought about demands for fast and reliable ways to organize, access, and discover music online. To date, many applications designed to perform such tasks have risen to popularity; each relies on a specific form of music metadata to help consumers discover songs and artists that appeal to their tastes. Very few of these applications, however, analyze the signal waveforms of songs directly. This low-level representation can provide dimensions of information that are inaccessible by metadata alone. To address this issue, we have implemented signalbased measures of musical similarity that have been optimized based on their correlations with human judgments. Furthermore, multiple recommendation engines relying on these measures have been implemented. These systems recommend songs to volunteers based on other songs they find appealing. Blind experiments have been conducted in which volunteers rate the systems' recommendations along with recommendations of leading online music discovery tools (Allmusic which uses genre labels, Pandora which uses musicological analysis, and Last.fm which uses collaborative filtering), random baseline recommendations, and personal recommendations by the first author. This paper shows that the signal-based engines perform about as well as popular, commercial, state-of-the-art systems."
Inskip C.; Macfarlane A.; Rafferty P.,"Music, movies and meaning: Communication in film-makers' Search for pre-existing music, and the implications for music information retrieval",2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951178915&partnerID=40&md5=aa26efc2ea9132c97fecc638c2d8e281,"Inskip C., Dept. of Information Science, City University London, United Kingdom; Macfarlane A., Dept. of Information Science, City University London, United Kingdom; Rafferty P., Dept. of Information Studies, University of Aberystwyth, United Kingdom","While the use of music to accompany moving images is widespread, the information behaviour, communicative practice and decision making by creative professionals within this area of the music industry is an under-researched area. This investigation discusses the use of music in films and advertising focusing on communication and meaning of the music and introduces a reflexive communication model. The model is discussed in relation to interviews with a sample of music professionals who search for and use music for their work. Key factors in this process include stakeholders, briefs, product knowledge and relevance. Searching by both content and context is important, although the final decision when matching music to picture is partly intuitive and determined by a range of stakeholders."
Knopke I.,The perlhumdrum and perllilypond toolkits for symbolic music information retrieval,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955182398&partnerID=40&md5=900aa7411dbeaad2759e765cf1f4570f,"Knopke I., Goldsmiths Digital Studios, United Kingdom","PerlHumdrum is an alternative toolkit for working with large numbers of Humdrum scores. While based on the original Humdrum toolkit, it is a completely new, self-contained implementation that can serve as a replacement, and may be a better choice for some computing systems. PerlHumdrum is fully object-oriented, is designed to easily facilitate analysis and processing of multiple humdrum files, and to answer common musicological questions across entire sets, collections of music, or even the entire output of single or multiple composers. Several extended capabilities that are not available in the original toolkit are also provided, such as translation of MIDI scores to Humdrum, provisions for constructing graphs, a graphical user interface for non-programmers, and the ability to generate complete scores or partial musical examples as standard musical notation using PerlLilypond. These tools are intended primarily for use by music theorists, computational musicologists, and Music Information Retrieval (MIR) researchers."
Müller M.; Ewert S.,Joint structure analysis with applications to music annotation and synchronization,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427712&partnerID=40&md5=06abadd6ca4443eee6a35211f93e7a0a,"Müller M., Saarland University, MPI Informatik, 66123 Saarbrücken, Campus E1 4, Germany; Ewert S., Bonn University, Computer Science III, 53117 Bonn, Römerstr. 164, Germany","The general goal of music synchronization is to automatically align different versions and interpretations related to a given musical work. In computing such alignments, recent approaches assume that the versions to be aligned correspond to each other with respect to their overall global structure. However, in real-world scenarios, this assumption is often violated. For example, for a popular song there often exist various structurally different album, radio, or extended versions. Or, in classical music, different recordings of the same piece may exhibit omissions of repetitions or significant differences in parts such as solo cadenzas. In this paper, we introduce a novel approach for automatically detecting structural similarities and differences between two given versions of the same piece. The key idea is to perform a single structural analysis for both versions simultaneously instead of performing two separate analyses for each of the two versions. Such a joint structure analysis reveals the repetitions within and across the two versions. As a further contribution, we show how this information can be used for deriving musically meaningful partial alignments and annotations in the presence of structural variations."
Maxwell J.B.; Eigenfeldt A.,A music database and query system for recombinant composition,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855833240&partnerID=40&md5=5564d277ed5b20be5a0d701d0f425ee8,"Maxwell J.B., School for the Contemporary Arts, Simon Fraser University, Burnaby, BC, Canada; Eigenfeldt A., School for the Contemporary Arts, Simon Fraser University, Burnaby, BC, Canada","We propose a design and implementation for a music information database and query system, the MusicDB, which can be used for Music Information Retrieval (MIR). The MusicDB is implemented as a Java package, and is loaded in MaxMSP using the mxj external. The MusicDB contains a music analysis module, capable of extracting musical information from standard MIDI files, and a search engine. The search engine accepts queries in the form of a simple six-part syntax, and can return a variety of different types of musical information, drawing on the encoded knowledge of musical form stored in the database."
Itoyama K.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.,Instrument equalizer for query-by-example retrieval: Improving sound source separation based on integrated harmonic and inharmonic models,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052051446&partnerID=40&md5=f8db8deaa01a05f21fd994656cc5bd5e,"Itoyama K., Graduate School of Infomatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Komatani K., Graduate School of Infomatics, Kyoto University, Japan; Ogata T., Graduate School of Infomatics, Kyoto University, Japan; Okuno H.G., Graduate School of Infomatics, Kyoto University, Japan","This paper describes a music remixing interface, called Instrument Equalizer, that allows users to control the volume of each instrument part within existing audio recordings in real time. Although query-by-example retrieval systems need a user to prepare favorite examples (songs) in general, our interface gives a user to generate examples from existing ones by cutting or boosting some instrument/vocal parts, resulting in a variety of retrieved results. To change the volume, all instrument parts are separated from the input sound mixture using the corresponding standard MIDI file. For the separation, we used an integrated tone (timbre) model consisting of harmonic and inharmonic models that are initialized with template sounds recorded from a MIDI sound generator. The remaining but critical problem here is to deal with various performance styles and instrument bodies that are not given in the template sounds. To solve this problem, we train probabilistic distributions of timbre features by using various sounds. By adding a new constraint of maximizing the likelihood of timbre features extracted from each tone model, we succeeded in estimating model parameters that better express actual timbre."
McKay C.; Burgoyne J.A.; Thompson J.; Fujinaga I.,"Using ace XML 2.0 to store and share feature, instance and class data for musical classification",2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952354248&partnerID=40&md5=55df1362eee45399f21a89bfd0c57bc1,"McKay C., CIRMMT, McGill University, Canada; Burgoyne J.A., CIRMMT, McGill University, Canada; Thompson J., Music Technology, McGill University, Canada; Fujinaga I., CIRMMT, McGill University, Canada","This paper introduces ACE XML 2.0, a set of file formats that are designed to meet the special representational needs of research in automatic music classification. Such standardized formats are needed to facilitate the sharing and long-term storage of valuable research data. ACE XML 2.0 is designed to represent a broad range of musical information clearly using a flexible, extensible, selfcontained and formally structured framework. An emphasis is placed on representing extracted feature values, feature descriptions, instance annotations, class ontologies and related metadata. © 2009 International Society for Music Information Retrieval."
Lukashevich H.,Towards quantitative measures of evaluating song segmentation,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873479799&partnerID=40&md5=085ef7e1a4a4e8d4347de460901312b9,"Lukashevich H., Fraunhofer IDMT, Ilmenau, Germany","Automatic music structure analysis or song segmentation has immediate applications in the field of music information retrieval. Among these applications is active music navigation, automatic generation of audio summaries, automatic music analysis, etc. One of the important aspects of a song segmentation task is its evaluation. Commonly, that implies comparing the automatically estimated segmentation with a ground-truth, annotated by human experts. The automatic evaluation of segmentation algorithms provides the quantitative measure that reflects how well the estimated segmentation matches the annotated ground-truth. In this paper we present a novel evaluation measure based on information-theoretic conditional entropy. The principal advantage of the proposed approach lies in the applied normalization, which enables the comparison of the automatic evaluation results, obtained for songs with a different amount of states. We discuss and compare the evaluation scores commonly used for evaluating song segmentation at present. We provide several examples illustrating the behavior of different evaluation measures and weigh the benefits of the presented metric against the others."
Rabbat P.; Pachet F.,Direct and inverse inference in music databases: How to make a song funk?,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873468536&partnerID=40&md5=238a05edb1a276605c5fb864f2933470,"Rabbat P., Sophis, France; Pachet F., Sony CSL, France","We propose an algorithm for exploiting statistical properties of large-scale metadata databases about music titles to answer musicological queries. We introduce two inference schemes called ""direct"" and ""inverse"" inference, based on an efficient implementation of a kernel regression approach. We describe an evaluation experiment conducted on a large-scale database of finegrained musical metadata. We use this database to train the direct inference algorithm, test it, and also to identify the optimal parameters of the algorithm. The inverse inference algorithm is based on the direct inference algorithm. We illustrate it with some examples."
Rafailidis D.; Nanopoulos A.; Cambouropoulos E.; Manolopoulos Y.,Detection of stream segments in symbolic musical data,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415206&partnerID=40&md5=7ca6b676d972d1762b3b71d6c5a06b6c,"Rafailidis D., Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece; Nanopoulos A., Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece; Cambouropoulos E., Dept. of Music Studies, Aristotle University of Thessaloniki, Greece; Manolopoulos Y., Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece","A listener is thought to be able to organise musical notes into groups within musical streams/voices. A stream segment is a relatively short coherent sequence of tones that is separated horizontally from co-sounding streams and, vertically from neighbouring musical sequences. This paper presents a novel algorithm that discovers musical stream segments in symbolic musical data. The proposed algorithm makes use of a single set of fundamental auditory principles for the concurrent horizontal and vertical segregation of a given musical texture into stream segments. The algorithm is tested against a small manually-annotated dataset of musical excerpts, and results are analysed; it is shown that the technique is promising."
Izumitani T.; Kashino K.,A robust musical audio search method based on diagonal dynamic programming matching of self-similarity matrices,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960674681&partnerID=40&md5=6af5fc90342b72a89172d10894e276eb,"Izumitani T., NTT Communication Science Laboratories, Japan; Kashino K., NTT Communication Science Laboratories, Japan","We propose a new musical audio search method based on audio signal matching that can cope with key and tempo variations. The method employs the self-similarity matrix of an audio signal to represent a key-invariant structure of musical audio. And, we use dynamic programming (DP) matching of self-similarity matrices to deal with time variations. However, conventional DP-based sequence matching methods cannot be directly applied for self-similarity matrices because they cannot treat gaps independently of other time frames. We resolve this problem by introducing ""matched element indices,"" which reflect the history of matching, to a DP-based sequence matching method. We performed experiments using musical audio signals. The results indicate that the proposed method improves the detection accuracy in comparison to that that obtained by two conventional methods, namely, DP matching with chroma-based vector rotations and a simple matching of self-similarity feature vectors."
Liu Y.; Wang Y.; Shenoy A.; Tsai W.-H.; Cai L.,Clustering music recordings by their keys,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73249150711&partnerID=40&md5=24051f3c4d850955d99a220c834ac133,"Liu Y., Department of Computer Science and Technology, Tsinghua University, Beijing, China, School of Computing, National University of Singapore, Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore; Shenoy A.; Tsai W.-H., Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan; Cai L., Department of Computer Science and Technology, Tsinghua University, Beijing, China","Music key, a high level feature of musical audio, is an effective tool for structural analysis of musical works. This paper presents a novel unsupervised approach for clustering music recordings by their keys. Based on chroma-based features extracted from acoustic signals, an inter-recording distance metric which characterizes diversity of pitch distribution together with harmonic center of music pieces, is introduced to measure dissimilarities among musical features. Then, recordings are divided into categories via unsupervised clustering, where the best number of clusters can be determined automatically by minimizing estimated Rand Index. Any existing technique for key detection can then be employed to identify key assignment for each cluster. Empirical evaluation on a dataset of 91 pop songs illustrates an average cluster purity of 57.3% and a Rand Index of close to 50%, thus highlighting the possibility of integration with existing key identification techniques to improve accuracy, based on strong cross-correlation data available from this framework for input dataset."
Kim Y.E.; Schmidt E.; Emelle L.,MoodSwings: A collaborative game for music mood label collection,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445412&partnerID=40&md5=afb938c6b23796dd9e345229a207e96a,"Kim Y.E., Electrical and Computer Engineering, Drexel University, United States; Schmidt E., Electrical and Computer Engineering, Drexel University, United States; Emelle L., Electrical and Computer Engineering, Drexel University, United States","There are many problems in the field of music information retrieval that are not only difficult for machines to solve, but that do not have well-defined answers. In labeling and detecting emotions within music, this lack of specificity makes it difficult to train systems that rely on quantified labels for supervised machine learning. The collection of such ""ground truth"" data for these subjectively perceived features necessarily requires human subjects. Traditional methods of data collection, such as the hiring of subjects, can be flawed, since labeling tasks are time-consuming, tedious, and expensive. Recently, there have been many initiatives to use customized online games to harness so-called ""Human Computation"" for the collection of label data, and several such games have been proposed to collect labels spanning an excerpt of music. We present a new game, MoodSwings (http://schubert.ece. drexel.edu/moodswings), which differs in that it records dynamic (per-second) labels of players' mood ratings of music, in keeping with the unique time-varying quality of musical mood. As in prior collaborative game approaches, players are partnered to verify each others' results, and the game is designed to maximize consensus-building between users. We present preliminary results from an initial set of game play data."
Meintanis K.A.; Shipman F.M.,Creating and evaluating multi-phrase music summaries,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873480268&partnerID=40&md5=fca813bcf946ae3e9f316e4580364fe0,"Meintanis K.A., Center for the Study of Digital Libraries, Department of Computer Science, Texas A and M University, United States; Shipman F.M., Center for the Study of Digital Libraries, Department of Computer Science, Texas A and M University, United States","Music summarization involves the process of identifying and presenting melody snippets carrying sufficient information for highlighting and remembering a song. In many commercial applications, the problem of finding those snippets is addressed by having humans select the most salient parts of the song or by extracting a few seconds from the song's introduction. Research in the automatic creation of music summaries has focused mainly on the extraction of one or more highly repetitive phrases to represent the whole song. This paper explores whether the composition of multiple ""characteristic"" phrases that are selected to be highly dissimilar to one another will increase the summary's effectiveness. This paper presents three variations of this multi-phrase music summarization approach and a human-centered evaluation comparing these algorithms. Results showed that the resulting multi-phrase summaries performed well in describing the songs. People preferred the multi-phrase summaries over presentations of the introductions of the songs."
Gasser M.; Flexer A.; Widmer G.,Streamcatcher: Integrated visualization of music clips and online audio streams,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873474373&partnerID=40&md5=406d013b1e75498683161d68bc6ea695,"Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We propose a content-based approach to explorative visualization of online audio streams (e.g., web radio streams). The visualization space is defined by prototypical instances of musical concepts taken from personal music collections. Our system shows the relation of prototypes to each other and generates an animated visualization that places representations of audio streams in the vicinity of their most similar prototypes. Both computation of music similarity and visualization are formulated for online real time performance. A software implementation of these ideas is presented and evaluated."
Cancela P.; Rocamora M.; López E.,An efficient multi-resolution spectral transform for music analysis,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349251543&partnerID=40&md5=f7a0602dcfa1d0e313fbe5edd3ddce5c,"Cancela P., Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay; Rocamora M., Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay; López E., Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay","In this paper we focus on multi-resolution spectral analysis algorithms for music signals based on the FFT. Two previously devised efficient algorithms (efficient constant- Q transform [1] and multiresolution FFT [2]) are reviewed and compared with a new proposal based on the IIR filtering of the FFT. Apart from its simplicity, the proposed method shows to be a good compromise between design flexibility and reduced computational effort. Additionally, it was used as a part of an effective melody extraction algorithm. © 2009 International Society for Music Information Retrieval."
Deliège F.; Chua B.Y.; Pedersen T.B.,High-level audio features: Distributed extraction and similarity search,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872864607&partnerID=40&md5=01a742ec7f05330b6a0833a04f47ef62,"Deliège F., Department of Computer Science, Aalborg University, Denmark; Chua B.Y., Department of Computer Science, Aalborg University, Denmark; Pedersen T.B., Department of Computer Science, Aalborg University, Denmark","Today, automatic extraction of high-level audio features suffers from two main scalability issues. First, the extraction algorithms are very demanding in terms of memory and computation resources. Second, copyright laws prevent the audio files to be shared among computers, limiting the use of existing distributed computation frameworks and reducing the transparency of the methods evaluation process. The iSound Music Warehouse (iSoundMW), presented in this paper, is a framework to collect and query high-level audio features. It performs the feature extraction in a two-step process that allows distributed computations while respecting copyright laws. Using public computers, the extraction can be performed on large scale music collections. However, to be truly valuable, data management tools to search among the extracted features are needed. The iSoundMW enables similarity search among the collected high-level features and demonstrates its flexibility and efficiency by using a weighted combination of high-level features and constraints while showing good search performance results."
Woodruff J.; Li Y.; Wang D.,Resolving overlapping harmonics for monaural musical sound separation using pitch and common amplitude modulation,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415211&partnerID=40&md5=98b7115f8c1433fbe86a66c32ded76dd,"Woodruff J., Dept. of Computer Science and Engineering, Ohio State University, United States; Li Y., Dept. of Computer Science and Engineering, Ohio State University, United States; Wang D., Dept. of Computer Science and Engineering, Center for Cognitive Science, Ohio State University, United States","In mixtures of pitched sounds, the problem of overlapping harmonics poses a significant challenge to monaural musical sound separation systems. In this paper we present a new algorithm for sinusoidal parameter estimation of overlapping harmonics for pitched instruments. Our algorithm is based on the assumptions that harmonics of the same source have correlated amplitude envelopes and the phase change of harmonics can be accurately predicted from an instrument's pitch. We exploit these two assumptions in a leastsquares estimation framework to resolve overlapping harmonics. This new algorithm is incorporated into a separation system and quantitative evaluation shows that the resulting system performs significantly better than an existing monaural music separation system for mixtures of harmonic instruments."
Zhang X.; Gerhard D.,Chord recognition using instrument voicing constraints,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445921&partnerID=40&md5=dcf2a154353a7f7ca7f5e3697e665217,"Zhang X., Dept. of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada; Gerhard D., Dept. of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada","This paper presents a technique of disambiguation for chord recognition based on a-priori knowledge of probabilities of chord voicings in the specific musical medium. The main motivating example is guitar chord recognition, where the physical layout and structure of the instrument, along with human physical and temporal constraints, make certain chord voicings and chord sequences more likely than others. Pitch classes are first extracted using the Pitch Class Profile (PCP) technique, and chords are then recognized using Artificial Neural Networks. The chord information is then analyzed using an array of voicing vectors (VV) indicating likelihood for chord voicings based on constraints of the instrument. Chord sequence analysis is used to reinforce accuracy of individual chord estimations. The specific notes of the chord are then inferred by combining the chord information and the best estimated voicing of the chord."
Scholz R.; Ramalho G.,COCHONUT: Recognizing complex chords from MIDI guitar sequences,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949575165&partnerID=40&md5=6eb0aa68c6e0eff5c7d923508d45c8b0,"Scholz R., Federal University of Pernambuco, Brazil; Ramalho G., Federal University of Pernambuco, Brazil","Chord recognition from symbolic data is a complex task, due to its strong context dependency and the large number of possible combinations of the intervals which the chords are made of, specially when dealing with dissonances, such as 7ths, 9ths, 13ths and suspended chords. None of the current approaches deal with such complexity. Most of them consider only simple chord patterns, in the best cases, including sevenths. In addition, when considering symbolic data captured from a MIDI guitar, we need to deal with non quantized and noisy data, which increases the difficulty of the task. The current symbolic approaches deal only with quantized data, with no automatic technique to reduce noise. This paper proposes a new approach to recognize chords, from symbolic MIDI guitar data, called COCHONUT (Complex Chords Nutting). The system uses contextual harmonic information to solve ambiguous cases, integrated with other techniques, such as decision theory, optimization, pattern matching and rule-based recognition. The results are encouraging and provide strong indications that the use of harmonic contextual information, integrated with other techniques, can actually improve the results currently found in literature."
Manzagol P.-A.; Bertin-Mahieux T.; Eck D.,On the use of sparse time-relative auditory codes for music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465396&partnerID=40&md5=c7bc98091843d2c7522dd9d56a33d31f,"Manzagol P.-A., Universitéde Montréal, Department of Computer Science, Montreal, Canada; Bertin-Mahieux T., Universitéde Montréal, Department of Computer Science, Montreal, Canada; Eck D., Universitéde Montréal, Department of Computer Science, Montreal, Canada","Many if not most audio features used in MIR research are inspired by work done in speech recognition and are variations on the spectrogram. Recently, much attention has been given to new representations of audio that are sparse and time-relative. These representations are efficient and able to avoid the time-frequency trade-off of a spectrogram. Yet little work with music streams has been conducted and these features remain mostly unused in the MIR community. In this paper we further explore the use of these features for musical signals. In particular, we investigate their use on realistic music examples (i.e. released commercial music) and their use as input features for supervised learning. Furthermore, we identify three specific issues related to these features which will need to be further addressed in order to obtain the full benefit for MIR applications."
Lemström K.; Mikkilä N.; Mäkinen V.,Fast index based filters for music retrieval,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873418049&partnerID=40&md5=d7be1fc7bfd980e07fa8cca98bd8e539,"Lemström K., University of Helsinki, Department of Computer Science, Finland; Mikkilä N., University of Helsinki, Department of Computer Science, Finland; Mäkinen V., University of Helsinki, Department of Computer Science, Finland","We consider two content-based music retrieval problems where the music is modeled as sets of points in the Euclidean plane, formed by the (on-set time, pitch) pairs. We introduce fast filtering methods based on indexing the underlying database. The filters run in a sublinear time in the length of the database, and they are lossless if a quadratic space may be used. By taking into account the application, the search space can be narrowed down, obtaining practically lossless filters using linear size index structures. For the checking phase, which dominates the overall running time, we exploit previously designed algorithms suitable for local checking. In our experiments on a music database, our best filter-based methods performed several orders of a magnitude faster than previous solutions."
Godfrey M.T.; Chordia P.,Hubs and homogeneity: Improving content-based music modeling,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416546&partnerID=40&md5=bec4a2bb5715b9315ea55ab1d24f6482,"Godfrey M.T., Georgia Institute of Technology, Music Technology Group, United States; Chordia P., Georgia Institute of Technology, Music Technology Group, United States","We explore the origins of hubs in timbre-based song modeling in the context of content-based music recommendation and propose several remedies. Specifically, we find that a process of model homogenization, in which certain components of a mixture model are systematically removed, improves performance as measured against several ground-truth similarity metrics. Extending the work of Aucouturier, we introduce several new methods of homogenization. On a subset of the uspop data set, model homogenization improves artist R-precision by a maximum of 3.5% and agreement to user collection co-occurrence data by 7.4%. We also explore differences in the effectiveness of the various homogenization methods for hub reduction. Further, we extend the modeling of frame-based MFCC features by using a kernel density estimation approach to non-parametric modeling. We find that such an approach significantly reduces the number of hubs (by 2.6% of the dataset) while improving agreement to ground-truth by 5% and slightly improving artist R-precision as compared with the standard parametric model."
Holzapfel A.; Stylianou Y.,Rhythmic similarity in traditional turkish music,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952350713&partnerID=40&md5=f7624fc8fdd495518707c67710907488,"Holzapfel A., Institute of Computer Science, FORTH, University of Crete, Greece; Stylianou Y., Institute of Computer Science, FORTH, University of Crete, Greece","In this paper, the problemof automatically assigning a piece of traditional Turkish music into a class of rhythm referred to as usul is addressed. For this, an approach for rhythmic similarity measurement based on scale transforms has been evaluated on a set of MIDI data. Because this task is related to time signature estimation, the accuracy of the proposed method is evaluated and compared with a state of the art time signature estimation approach. The results indicate that the proposed method can be successfully applied to audio signals of Turkish music and that it captures relevant properties of the individual usul. © 2009 International Society for Music Information Retrieval."
Duan Z.; Lu L.; Zhang C.,Collective annotation of music from multiple semantic categories,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431927&partnerID=40&md5=9f1959de6ebfbffa5975e4ddbe4625b1,"Duan Z., Microsoft Research Asia (MSRA), Sigma Center, Beijing 100080, Haidian District, China, State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China; Lu L., Microsoft Research Asia (MSRA), Sigma Center, Beijing 100080, Haidian District, China; Zhang C., State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China","Music semantic annotation aims to automatically annotate a music signal with a set of semantic labels (words or tags). Existing methods on music semantic annotation usually take it as a multi-label binary classification problem, and model each semantic label individually while ignoring their relationships. However, there are usually strong correlations between some labels. Intuitively, investigating this correlation can be helpful to improve the overall annotation performance. In this paper, we report our attempts to collective music semantic annotation, which not only builds a model for each semantic label, but also builds models for the pairs of labels that have significant correlations. Two methods are exploited in this paper, one based on a generative model (Gaussian Mixture Model), and another based on a discriminative model (Conditional Random Field). Experiments show slight but consistent improvement in terms of precision and recall, compared with the individual-label modeling methods."
Pachet F.; Roy P.,Hit song science is not yet a science,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445246&partnerID=40&md5=d47055227f115b5af5ce670a5c899745,"Pachet F., Sony CSL, France; Roy P., Sony CSL, France","We describe a large-scale experiment aiming at validating the hypothesis that the popularity of music titles can be predicted from global acoustic or human features. We use a 32.000 title database with 632 manually-entered labels per title including 3 related to the popularity of the title. Our experiment uses two audio feature sets, as well as the set of all the manually-entered labels but the popularity ones. The experiment shows that some subjective labels may indeed be reasonably well-learned by these techniques, but not popularity. This contradicts recent and sustained claims made in the MIR community and in the media about the existence of ""Hit Song Science""."
Kako T.; Ohishi Y.; Kameoka H.; Kashino K.; Takeda K.,Automatic identification for singing style based on sung melodic contour characterized in phase plane,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952429496&partnerID=40&md5=fa57cddf30f516cf893b581c7e8f89de,"Kako T., Graduate School of Information Science, Nagoya University, Japan; Ohishi Y., Communication Science Laboratories, NTT Corporation, Japan; Kameoka H., Communication Science Laboratories, NTT Corporation, Japan; Kashino K., Communication Science Laboratories, NTT Corporation, Japan; Takeda K., Graduate School of Information Science, Nagoya University, Japan","A stochastic representation of singing styles is proposed. The dynamic property of melodic contour, i.e., fundamental frequency (F0) sequence, is assumed to be the main cue for singing styles because it can characterize such typical ornamentations as vibrato . F0 signal trajectories in the phase plane are used as the basic representation. By fitting Gaussian mixture models to the observed F0 trajectories in the phase plane, a parametric representation is obtained by a set of GMM parameters. The effectiveness of our proposed method is confirmed through experimental evaluation where 94.1% accuracy for singer-class discrimination was obtained. © 2009 International Society for Music Information Retrieval."
Peeters G.; Fenech D.; Rodet X.,MCIPA: A music content information player and annotator for discovering music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873432129&partnerID=40&md5=42919ea220cefe0eeb1342c72f742f59,"Peeters G., Ircam, CNRS STMS, France; Fenech D., Ircam, CNRS STMS, France; Rodet X., Ircam, CNRS STMS, France","In this paper, we present a new tool for intra-document browsing of musical pieces. This tool is a multimedia player which represents the content of a musical piece visually. Each type of musical content (structure, chords, downbeats/beats, notes, events) is associated which a distinct visual representation. The user sees what he/she is listening too. He can also browse inside the music according to the visual content. For this, each type of visual object has a dedicated feedback, either as an audio-feedback or as a playhead feedback. Content information can be extracted automatically from audio (using signal processing algorithms) or annotated by hand by the user. This multimedia player can also be used as an annotator tool guided by the content."
Corthaut N.; Govaerts S.; Verbert K.; Duval E.,"Connecting the dots: Music metadata generation, schemas and applications",2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955914360&partnerID=40&md5=22f909b970f23ce7b6d7c3db56b0777c,"Corthaut N., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium; Govaerts S., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium; Verbert K., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium; Duval E., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium","With the ever-increasing amount of digitized music becoming available, metadata is a key driver for different music related application domains. A service that combines different metadata sources should be aware of the existence of different schemas to store and exchange music metadata. The user of a metadata provider could benefit from knowledge about the metadata needs for different music application domains. In this paper, we present how we can compare the expressiveness and richness of a metadata schema for an application. To cope with different levels of granularity in metadata fields we defined clusters of semantically related metadata fields. Similarly, application domains were defined to tackle the fine-grained functionality space in music applications. Next is shown to what extent music application domains and metadata schemas make use of the metadata field clusters. Finally, we link the metadata schemas with the application domains. A decision table is presented that assists the user of a metadata provider in choosing the right metadata schema for his application."
Kirlin P.B.,Using harmonic and melodic analyses to automate the initial stages of schenkerian analysis,2009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249234110&partnerID=40&md5=80142499fb46734bd029c6cbde1e936a,"Kirlin P.B., Department of Computer Science, University of Massachusetts, Amherst, United States","Structural music analysis is used to reveal the inner workings of a musical composition by recursively applying reductions to the music, resulting in a series of successively more abstract views of the composition. Schenkerian analysis is the most well-developed type of structural analysis, and while there is a wide body of research on the theory, there is no well-defined algorithm to perform such an analysis. A automated algorithm for Schenkerian analysis would be extremely useful to music scholars and researchers studying music from a computational standpoint. The first major step in producing a Schenkerian analysis involves selecting notes from the composition in question for the primary soprano and bass parts of the analysis. We present an algorithmfor this that uses harmonic andmelodic analyses to accomplish this task. © 2009 International Society for Music Information Retrieval."
Fujihara H.; Goto M.; Ogata J.,Hyperlinking lyrics: A method for creating hyperlinks between phrases in song lyrics,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049411004&partnerID=40&md5=9f59a906f0c7f03dc08bda3f1abc44c2,"Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Ogata J., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We describe a novel method for creating a hyperlink from a phrase in the lyrics of a song to the same phrase in the lyrics of another song. This method can be applied to various applications, such as song clustering based on the meaning of the lyrics and a music playback interface that will enable a user to browse and discover songs on the basis of lyrics. Given a song database consisting of songs with their text lyrics and songs without their text lyrics, our method first extracts appropriate keywords (phrases) from the text lyrics without using audio signals. It then finds these keywords in audio signals by estimating the keywords' start and end times. Although the performance obtained in our experiments has room for improvement, the potential of this new approach is shown."
Molina-Solana M.; Arcos J.L.; Gomez E.,Using expressive trends for identifying violin performers,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956613822&partnerID=40&md5=ad541bee7334785e310a64f7b97b6579,"Molina-Solana M., Comp. Science and AI Dep., University of Granada, 18071 Granada, Spain; Arcos J.L., IIIA, AI Research Institute, CSIC, Spanish National Res. Council, 08193 Bellaterra, Spain; Gomez E., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Spain","This paper presents a new approach for identifying professional performers in commercial recordings. We propose a Trend-based model that, analyzing the way Narmour's Implication-Realization patterns are played, is able to characterize performers. Concretely, starting from automatically extracted descriptors provided by state-of-the-art extraction tools, the system performs a mapping to a set of qualitative behavior shapes and constructs a collection of frequency distributions for each descriptor. Experiments were conducted in a data-set of violin recordings from 23 different performers. Reported results show that our approach is able to achieve high identification rates."
Doll T.M.; Migneco R.V.; Kim Y.E.,Online activities for music information and acoustics education and psychoacoustic data collection,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873442840&partnerID=40&md5=f60e90a8604b51ce6d3b16a0c35e54a2,"Doll T.M., Drexel University, Electrical and Computer Engineering, United States; Migneco R.V., Drexel University, Electrical and Computer Engineering, United States; Kim Y.E., Drexel University, Electrical and Computer Engineering, United States","Online collaborative activities provide a powerful platform for the collection of psychoacoustic data on the perception of audio and music from a very large numbers of subjects. Furthermore, these activities can be designed to simultaneously educate users about aspects of music information and acoustics, particularly for younger students in grades K-12. We have created prototype interactive activities illustrating aspects of two different sound and acoustics concepts: musical instrument timbre and the cocktail party problem (sound source isolation within mixtures) that also provide a method of collecting perceptual data related to these problems with a range of parameter variation that is difficult to achieve for large subject populations using traditional psychoacoustic evaluation. We present preliminary data from a pilot study where middle school students were engaged with the two activities to demonstrate the potential benefits as an education and data collection platform."
Flexer A.; Schnitzer D.; Gasser M.; Widmer G.,Playlist generation using start and end songs,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873443934&partnerID=40&md5=19685a322ed42c8f337e27f7d8fdb88c,"Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","A new algorithm for automatic generation of playlists with an inherent sequential order is presented. Based on a start and end song it creates a smooth transition allowing users to discover new songs in a music collection. The approach is based on audio similarity and does not require any kind of meta data. It is evaluated using both objective genre labels and subjective listening tests. Our approach allows users of the website of a public radio station to create their own digital ""mixtapes"" online."
Ono N.; Miyamoto K.; Kameoka H.; Sagayama S.,A real-time equalizer of harmonic and percussive components in music signals,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465086&partnerID=40&md5=7d7a0359d4b42a01e3e4690ba5bea8a3,"Ono N., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Miyamoto K., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Kameoka H., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Sagayama S., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan","In this paper, we present a real-time equalizer to control a volume balance of harmonic and percussive components in music signals without a priori knowledge of scores or included instruments. The harmonic and percussive components of music signals have much different structures in the power spectrogram domain, the former is horizontal, while the latter is vertical. Exploiting the anisotropy, our methods separate input music signals into them based on the MAP estimation framework. We derive two kind of algorithm based on a I-divergence-based mixing model and a hard mixing model. Although they include iterative update equations, we realized the real-time processing by a sliding analysis technique. The separated harmonic and percussive components are finally remixed in an arbitrary volume balance and played. We show the prototype system implemented on Windows environment."
Mayer R.; Neumayer R.; Rauber A.,Rhyme and style features for musical genre classification by song lyrics,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873470329&partnerID=40&md5=417df1262fcdc8abbedee76ac13fafac,"Mayer R., Department of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria; Neumayer R., Department of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria, Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway; Rauber A., Department of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria","How individuals perceive music is influenced by many different factors. The audible part of a piece of music, its sound, does for sure contribute, but is only one aspect to be taken into account. Cultural information influences how we experience music, as does the songs' text and its sound. Next to symbolic and audio based music information retrieval, which focus on the sound of music, song lyrics, may thus be used to improve classification or similarity ranking of music. Song lyrics exhibit specific properties different from traditional text documents - many lyrics are for example composed in rhyming verses, and may have different frequencies for certain parts-of-speech when compared to other text documents. Further, lyrics may use 'slang' language or differ greatly in the length and complexity of the language used, which can be measured by some statistical features such as word / verse length, and the amount of repetative text. In this paper, we present a novel set of features developed for textual analysis of song lyrics, and combine them with and compare them to classical bag-of-words indexing approaches. We present results for musical genre classification on a test collection in order to demonstrate our analysis."
Fiebrink R.; Wang G.; Cook P.,Support for MIR prototyping and real-time applications in the chuck programming language,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421464&partnerID=40&md5=a74d79a605ba99b0bd2f3c8f21123690,"Fiebrink R., Princeton University, United States; Wang G., Stanford University, United States; Cook P., Princeton University, United States","In this paper, we discuss our recent additions of audio analysis and machine learning infrastructure to the ChucK music programming language, wherein we provide a complementary system prototyping framework for MIR researchers and lower the barriers to applying many MIR algorithms in live music performance. The new language capabilities preserve ChucK's breadth of control - from high-level control using building block components to sample-level manipulation - and on-the-fly reprogrammability, allowing the programmer to experiment with new features, signal processing techniques, and learning algorithms with ease and flexibility. Furthermore, our additions integrate tightly with ChucK's synthesis system, allowing the programmer to apply the results of analysis and learning to drive real-time music creation and interaction within a single framework. In this paper, we motivate and describe our recent additions to the language, outline a ChucK-based approach to rapid MIR prototyping, present three case studies in which we have applied ChucK to audio analysis and MIR tasks, and introduce our new toolkit to facilitate experimentation with analysis and learning in the language."
Chuan C.-H.; Chew E.,Evaluating and visualizing effectiveness of style emulation in musical accompaniment,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856497995&partnerID=40&md5=237e57a7e42de526e723484fa8169f62,"Chuan C.-H., Department of Computer Science, University of Southern California, Viterbi School of Engineering, United States; Chew E., Epstein Department of Industrial and Systems Engineering, University of Southern California, Viterbi School of Engineering, United States, Radcliffe Institute for Advanced Study, Harvard University, United States","We propose general quantitative methods for evaluating and visualizing the results of machine-generated style-specific accompaniment. The evaluation of automated accompaniment systems, and the degree to which they emulate a style, has been based primarily on subjective opinion. To quantify style similarity between machine-generated and original accompaniments, we propose two types of measures: one based on transformations in the neo-Riemannian chord space, and another based on the distribution of melody-chord intervals. The first set of experiments demonstrate the methods on an automatic style-specific accompaniment (ASSA) system. They test the effect of training data choice on style emulation effectiveness, and challenge the assumption that more data is better. The second set of experiments compare the output of the ASSA system with those of a rule-based system, and random chord generator. While the examples focus primarily on machine emulation of Pop/Rock accompaniment, the methods generalize to music of other genres."
Slaney M.; Weinberger K.; White W.,Learning a metric for music similarity,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426251&partnerID=40&md5=dbb9f2c8110745f954929ae37eb7f071,"Slaney M., Yahoo Research, Santa Clara, CA 95054, 2821 Mission College Blvd., United States; Weinberger K., Yahoo Research, Santa Clara, CA 95054, 2821 Mission College Blvd., United States; White W., Yahoo Media Innovation, Berkeley, CA 94704, 1950 University Ave., United States","This paper describe five different principled ways to embed songs into a Euclidean metric space. In particular, we learn embeddings so that the pairwise Euclidean distance between two songs reflects semantic dissimilarity. This allows distance-based analysis, such as for example straightforward nearest-neighbor classification, to detect and potentially suggest similar songs within a collection. Each of the six approaches (baseline, whitening, LDA, NCA, LMNN and RCA) rotate and scale the raw feature space with a linear transform. We tune the parameters of these models using a song-classification task with content-based features."
Fremerey C.; Müller M.; Kurth F.; Clausen M.,Automatic mapping of scanned sheet music to audio recordings,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76949101999&partnerID=40&md5=4db3e86a737d4fb1e4187a029442b9a2,"Fremerey C., Computer Science III, University of Bonn, Bonn, Germany; Müller M., Max-Planck-Institut (MPI) for Informatics, Saarbrücken, Germany; Kurth F., Research Establishment for Applied Science (FGAN), Wachtberg, Germany; Clausen M., Computer Science III, University of Bonn, Bonn, Germany","Significant digitization efforts have resulted in large multimodal music collections comprising visual (scanned sheet music) as well as acoustic material (audio recordings). In this paper, we present a novel procedure for mapping scanned pages of sheet music to a given collection of audio recordings by identifying musically corresponding audio clips. To this end, both the scanned images as well as the audio recordings are first transformed into a common feature representation using optical music recognition (OMR) and methods from digital signal processing, respectively. Based on this common representation, a direct comparison of the two different types of data is facilitated. This allows for a search of scan-based queries in the audio collection. We report on systematic experiments conducted on the corpus of Beethoven's piano sonatas showing that our mapping procedure works with high precision across the two types of music data in the case that there are no severe OMR errors. The proposed mapping procedure is relevant in a real-world application scenario at the Bavarian State Library for automatically identifying and annotating scanned sheet music by means of already available annotated audio material."
Sordo M.; Celma O.; Blech M.; Guaus E.,The quest for musical genres: Do the experts and the wisdom of crowds agree?,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155212713&partnerID=40&md5=685feb84a6d47c196a79b1945189ee3b,"Sordo M., Music Technology Group, Universitat Pompeu Fabra, Spain; Celma O., Music Technology Group, Universitat Pompeu Fabra, Spain; Blech M., Music Technology Group, Universitat Pompeu Fabra, Spain; Guaus E., Music Technology Group, Universitat Pompeu Fabra, Spain","This paper presents some findings around musical genres. The main goal is to analyse whether there is any agreement between a group of experts and a community, when defining a set of genres and their relationships. For this purpose, three different experiments are conducted using two datasets: the MP3.com expert taxonomy, and last.fm tags at artist level. The experimental results show a clear agreement for some components of the taxonomy (Blues, HipHop), whilst in other cases (e.g. Rock) there is no correlations. Interestingly enough, the same results are found in the MIREX2007 results for audio genre classification task. Therefore, a multi-faceted approach for musical genre using expert based classifications, dynamic associations derived from the wisdom of crowds, and content-based analysis can improve genre classification, as well as other relevant MIR tasks such as music similarity or music recommendation."
Little D.; Pardo B.,Learning musical instruments from mixtures of audio with weak labels,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428417&partnerID=40&md5=1d6fb0c99ed5f06dab5106796fbb1d61,"Little D., EECS Department, Northwestern University, Evanston, IL 60208, United States; Pardo B., EECS Department, Northwestern University, Evanston, IL 60208, United States","We are interested in developing a system that learns to recognize individual sound sources in an auditory scene where multiple sources may be occurring simultaneously. We focus here on sound source recognition in music audio mixtures. Many researchers have made progress by using isolated training examples or very strongly labeled training data. We consider an alternative approach: the learner is presented with a variety of weaky-labeled mixtures. Positive examples include the target instrument at some point in a mixture of sounds, and negative examples are mixtures that do not contain the target. We show that it not only possible to learn from weakly-labeled mixtures of instruments, but that it works significantly better (78% correct labeling compared to 55%) than learning from isolated examples when the task is identification of an instrument in novel mixtures."
Pugin L.; Hockman J.; Burgoyne J.A.; Fujinaga I.,Gamera versus aruspix two optical music recognition approaches,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954257763&partnerID=40&md5=998ed84568831a09e3736950c46d6a68,"Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Hockman J., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada","Optical music recognition (OMR) applications are predominantly designed for common music notation and as such, are inherently incapable of adapting to specialized notation forms within early music. Two OMR systems, namely Gamut (a Gamera application) and Aruspix, have been proposed for early music. In this paper, we present a novel comparison of the two systems, which use markedly different approaches to solve the same problem, and pay close attention to the performance and learning rates of both applications. In order to obtain a complete comparison of Gamut and Aruspix, we evaluated the core recognition systems and the pitch determination processes separately. With our experiments, we were able to highlight the advantages of both approaches as well as causes of problems and possibilities for future improvements."
Hu X.; Downie J.S.; Laurier C.; Bay M.; Ehmann A.F.,The 2007 mirex audio mood classification task: Lessons learned,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873433681&partnerID=40&md5=91b5862fb8cf0e2cd524eb281f0f0cd8,"Hu X., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Laurier C., Music Technology Group, Universitat Pompeu Fabra, Spain; Bay M., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","Recent music information retrieval (MIR) research pays increasing attention to music classification based on moods expressed by music pieces. The first Audio Mood Classification (AMC) evaluation task was held in the 2007 running of the Music Information Retrieval Evaluation eXchange (MIREX). This paper describes important issues in setting up the task, including dataset construction and ground-truth labeling, and analyzes human assessments on the audio dataset, as well as system performances from various angles. Interesting findings include system performance differences with regard to mood clusters and the levels of agreement amongst human judgments regarding mood labeling. Based on these analyses, we summarize experiences learned from the first community scale evaluation of the AMC task and propose recommendations for future AMC and similar evaluation tasks."
Skalak M.; Han J.; Pardo B.,Speeding melody search with vantage point trees,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958197457&partnerID=40&md5=297de878f9f4ad8b47853a2e31e5296e,"Skalak M., Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States; Han J., Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States; Pardo B., Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States","Melodic search engines let people find music in online collections by specifying the desired melody. Comparing the query melody to every item in a large database is prohibitively slow. If melodies can be placed in a metric space, search can be sped by comparing the query to a limited number of vantage melodies, rather than the entire database. We describe a simple melody metric that is customizable using a small number of example queries. This metric allows use of a generalized vantage point tree to organize the database. We show on a standard melodic database that the general vantage tree approach achieves superior search results for query-by-humming compared to an existing vantage point tree method. We then show this method can be used as a preprocessor to speed search for non-metric melodic comparison."
Cunningham S.J.; Zhang Y.,Development of a music organizer for children,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873469614&partnerID=40&md5=deb42e772d1000ca1ee5e8bedf123c4b,"Cunningham S.J., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Zhang Y., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","Software development for children is challenging; children have their own needs, which often are not met by 'grown up' software. We focus on software for playing songs and managing a music collection - tasks that children take great interest in, but for which they have few or inappropriate tools. We address this situation with the design of a new music management system, created with children as design partners: the Kids Music Box."
Ogihara M.; Li T.,N-gram chord profiles for composer style representation,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416168&partnerID=40&md5=a9a14642a8f4356d684131099872d14e,"Ogihara M., Department of Computer Science, University of Miami, United States; Li T., School of Computer Science, Florida International University, United States","This paper studies the problem of using weighted N-grams of chord sequences to construct the profile of a composer. The N-gram profile of a chord sequence is the collection of all N-grams appearing in a sequence where each N-gram is given a weight proportional to its beat count. The N-gram profile of a collection of chord sequences is the simple average of the N-gram profile of all the chord sequences in the collection. Similarity of two composers is measured by the cosine of their respective profiles, which has a value in the range [0, 1]. Using the cosine-based similarity, a group of composers is clustered into a hierarchy, which appears to be explicable. Also, the composition style can be identified using N-gram signatures."
Hashida M.; Matsui T.; Katayose H.,A new music database describing deviation information of performance expressions,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455062&partnerID=40&md5=90b597877cd25d78ae00ad383e67339f,"Hashida M., Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan; Matsui T., Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan; Katayose H., Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan","We introduce the CrestMuse Performance Expression Database (CrestMusePEDB), a music database that describes music performance expression and is available for academic research. While music databases are being provided as MIR technologies continue to progress, few databases deal with performance expression. We constructed a music expression database, CrestMusePEDB. It may be utilized in the research fields of music informatics, music perception and cognition, and musicology. It will contain music expression information on virtuosis' expressive performances, including those of 3 to 10 players at a time, on about 100 pieces of classical Western music. The latest version of the database, CrestMusePEDB Ver. 2.0, is available. The paper gives an overview of CrestMusePEDB."
Mandel M.I.; Ellis D.P.W.,Multiple-instance learning for music information retrieval,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473825&partnerID=40&md5=c12cb3c41f747ef556bbee2b5fe37a74,"Mandel M.I., LabROSA, Dept. Elec. Eng., Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Dept. Elec. Eng., Columbia University, New York, NY, United States","Multiple-instance learning algorithms train classifiers from lightly supervised data, i.e. labeled collections of items, rather than labeled items. We compare the multiple-instance learners mi-SVM and MILES on the task of classifying 10-second song clips. These classifiers are trained on tags at the track, album, and artist levels, or granularities, that have been derived from tags at the clip granularity, allowing us to test the effectiveness of the learners at recovering the clip labeling in the training set and predicting the clip labeling for a held-out test set. We find that mi-SVM is better than a control at the recovery task on training clips, with an average classification accuracy as high as 87% over 43 tags; on test clips, it is comparable to the control with an average classification accuracy of up to 68%. MILES performed adequately on the recovery task, but poorly on the test clips."
Symeonidis P.; Ruxanda M.; Nanopoulos A.; Manolopoulos Y.,Ternary semantic analysis of social tags for personalized music recommendation,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956776436&partnerID=40&md5=10a8a979e95a953ed77e2796f9dd53ce,"Symeonidis P., Department of Informatics, Aristotle Univ. of Thessaloniki, Greece; Ruxanda M., Department of Computer Science, Aalborg University, Denmark; Nanopoulos A., Department of Informatics, Aristotle Univ. of Thessaloniki, Greece; Manolopoulos Y., Department of Informatics, Aristotle Univ. of Thessaloniki, Greece","Social tagging is the process by which many users add metadata in the form of keywords, to annotate information items. In case of music, the annotated items can be songs, artists, albums. Current music recommenders which employ social tagging to improve the music recommendation, fail to always provide appropriate item recommendations, because: (i) users may have different interests for a musical item, and (ii) musical items may have multiple facets. In this paper, we propose an approach that tackles the problem of the multimodal use of music. We develop a unified framework, represented by a 3-order tensor, to model altogether users, tags, and items. Then, we recommend musical items according to users multimodal perception of music, by performing latent semantic analysis and dimensionality reduction using the Higher Order Singular Value Decomposition technique. We experimentally evaluate the proposed method against two state-of-the-art recommendations algorithms using real Last.fm data. Our results show significant improvements in terms of effectiveness measured through recall/precision."
Burgoyne J.A.; Devaney J.; Pugin L.; Fujinaga I.,Enhanced bleedthrough correction for early music documents with recto-verso registration,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71249093284&partnerID=40&md5=27ef79d92140b68fec21a56dd3303bd7,"Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Devaney J., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada","Ink bleedthrough is common problem in early music documents. Even when such bleedthrough does not pose problems for human perception, it can inhibit the performance of optical music recognition (OMR). One way to reduce the amount of bleedthrough is to take into account what is printed on the reverse of the page. In order to do so, the reverse of the page must be registered to match the front of the page on a pixel-by-pixel basis. This paper describes our approach to registering scanned early music scores as well as our modifications to two robust binarization approaches to take into account bleedthrough and the information available from the registration process. We determined that although the information from registration itself often makes little difference in recognition performance, other modifications to binarization algorithms for correcting bleedthrough can yield dramatic increases in OMR results."
Tindale A.R.; Sprague D.; Tzanetakis G.,Strike-a-tune: Fuzzy music navigation using a drum interface,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601019&partnerID=40&md5=eb6553a79e79fcb4e0ce57b1af81a6a1,"Tindale A.R., Department of Computer Science, University of Victoria, Canada; Sprague D., Department of Computer Science, University of Victoria, Canada; Tzanetakis G., Department of Computer Science, University of Victoria, Canada","A traditional music library system controlled by a mouse and keyboard is precise, allowing users to select their desired song. Alternatively, randomized playlist or shuffles are used when users have no particular music in mind. We present a new interface and visualization system called Strike- A-Tune for fuzzy music navigation. Fuzzy navigation is an imprecise navigation approach allowing users to choose preference related items. We believe this will help users to play music they want to hear and re-discover infrequently played songs in their music library, thus combining the best aspects of precision navigation and shuffles. We have designed an interface using an electronic drum to communicate with a visualization and playback system. ©2007 Austrian Computer Society (OCG)."
Burred J.J.; Sikora T.,Monaural source separation from musical mixtures based on time-frequency timbre models,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957855900&partnerID=40&md5=8c6bb8d3d1802617d4dfb24093071fbe,"Burred J.J., Communication Systems Group, Technical University of Berlin, Germany; Sikora T., Communication Systems Group, Technical University of Berlin, Germany","We present a system for source separation from monaural musical mixtures based on sinusoidal modeling and on a library of timbre models trained a priori. The models, which rely on Principal Component Analysis, serve as time-frequency probabilistic templates of the spectral envelope. They are used to match groups of sinusoidal tracks and assign them to a source, as well as to reconstruct overlapping partials. The proposed method does not make any assumptions on the harmonicity of the sources, and does not require a previous multipitch estimation stage. Since the timbre matching stage detects the instruments present on the mixture, the system can also be used for classification and segmentation. ©2007 Austrian Computer Society (OCG)."
Mardirossian A.; Chew E.,Visualizing music: Tonal progressions and distributions,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867912258&partnerID=40&md5=73507de2963ef3960f6f22508baf5809,"Mardirossian A., Epstein Department of Industrial and Systems Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089, United States; Chew E., Epstein Department of Industrial and Systems Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089, United States","This paper presents a music visualization tool that shows the tonal progression in, and tonal distribution of, a piece of music on Lerdahl's two-dimensional tonal pitch space. The method segments a piece into uniform time slices, and determines the most likely key in each slice. It then generates the visualization by dynamically showing the sequence of keys as translucent, growing discs on the twodimensional plane. The frequency of a key is indicated by the size of its colored disc. Each color and position corresponds to a key, and related keys are shown in proximity with related colors. The visual result effectively presents the changing distribution of the keys employed. The proposed visualization is an improvement over more basic charting methods, such as histograms, and it maintains standards of information design in the form of added dimensionality, color, and animation. We show that the visualization is invariant under music transformations that preserve the piece's identity. We conclude by illustrating how this method may be used to visually distinguish between tonal progression and distribution patterns in western classical versus Armenian folk music. ©2007 Austrian Computer Society (OCG)."
Bello J.P.,"Audio-based cover song retrieval using approximate chord sequences: Testing shifts, gaps, swaps and beats",2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605856&partnerID=40&md5=e8b7dd8c88a88d64eb8ed1622399d57f,"Bello J.P., New York University, Music Technology, United States","This paper presents a variation on the theme of using string alignment for MIR in the context of cover song identification in audio collections. Here, the strings are derived from audio by means of HMM-based chord estimation. The characteristics of the cover-song ID problem and the nature of common chord estimation errors are carefully considered. As a result strategies are proposed and systematically evaluated for key shifting, the cost of gap insertions and character swaps in string alignment, and the use of a beat-synchronous feature set. Results support the view that string alignment, as a mechanism for audiobased retrieval, cannot be oblivious to the problems of robustly estimating musically-meaningful data from audio. ©2007 Austrian Computer Society (OCG)."
Mesaros A.; Virtanen T.; Klapuri A.,Singer identification in polyphonic music using vocal separation and pattern recognition methods,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582138&partnerID=40&md5=bac17055638184a80b9a5bd4ebbdaa8c,"Mesaros A., Institute of Signal Processing, Tampere University Technology, Finland; Virtanen T., Institute of Signal Processing, Tampere University Technology, Finland; Klapuri A., Institute of Signal Processing, Tampere University Technology, Finland","This paper evaluates methods for singer identification in polyphonic music, based on pattern classification together with an algorithm for vocal separation. Classification strategies include the discriminant functions, Gaussian mixture model (GMM)-based maximum likelihood classifier and nearest neighbour classifiers using Kullback-Leibler divergence between the GMMs. A novel method of estimating the symmetric Kullback-Leibler distance between two GMMs is proposed. Two different approaches to singer identification were studied: one where the acoustic features were extracted directly from the polyphonic signal and one where the vocal line was first separated from the mixture using a predominant melody transcription system. The methods are evaluated using a database of songs where the level difference between the singing and the accompaniment varies. It was found that vocal line separation enables robust singer identification down to 0dB and -5dB singer-to- accompaniment ratios. ©2007 Austrian Computer Society (OCG)."
Izmirli Ö.,Localized key finding from audio using non-negative matrix factorization for segmentation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857473577&partnerID=40&md5=7dc2f055c761cd498a45f593a5b971f1,"Izmirli Ö., Center for Arts and Technology, Computer Science Connecticut College, United States","A model for localized key finding from audio is proposed. Besides being able to estimate the key in which a piece starts, the model can also identify points of modulation and label multiple sections with their key names throughout a single piece. The front-end employs an adaptive tuning stage prior to spectral analysis and calculation of chroma features. The segmentation stage uses groups of contiguous chroma vectors as input and identifies sections that are candidates for unique local keys in relation to their neighboring key centers. Non-negative matrix factorization with additional sparsity constraints and additive updates is used for segmentation. The use of segmentation is demonstrated for single and multiple key estimation problems. A correlational model of key finding is applied to the candidate segments to estimate the local keys. Evaluation is given on three different data sets and a range of analysis parameters. ©2007 Austrian Computer Society (OCG)."
Diet J.; Kurth F.,The probado music repository at the bavarian state library,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873603281&partnerID=40&md5=68f0cf3a9ff13771008e857978db389f,"Diet J., Bavarian State Library, Ludwigstraße 16 80539 Munich, Germany; Kurth F., University of Bonn, Römerstraße 164 53117 Bonn, Germany","In this paper, we describe the Probado music repository which is currently set up at the Bavarian State Library, Munich, as part of the larger German Probado digital library initiative. Based on the FRBR approach, we propose a novel work-centric metadata model for organizing the document collection. The primary data contained in the repository currently consists of scanned sheet music and digitized audio recordings. The repository can be searched using both classical and content-based retrieval mechanisms. To this end, we propose a workflow for automated content-based document analysis and indexing. ©2007 Austrian Computer Society (OCG)."
Turnbull D.; Lanckriet G.; Pampalk E.; Goto M.,A supervised approach for detecting boundaries in music using difference features and boosting,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575037&partnerID=40&md5=a1f8f81c34e88f42ca8c7ea9c229529f,"Turnbull D., Computer Science and Engineering, University of California, San Diego, CA 92093, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, CA 92093, United States; Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba, Ibaraki 305-8568, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba, Ibaraki 305-8568, Japan","A musical boundary is a transition between two musical segments such as a verse and a chorus. Our goal is to automatically detect musical boundaries using temporallylocal audio features. We develop a set of difference features that indicate when there are changes in perceptual aspects (e.g., timbre, harmony, melody, rhythm) of the music. We show that many individual difference features are useful for detecting boundaries. By combining these features and formulating the problem as a supervised learning problem, we can further improve performance. This is an alternative to previous work on music segmentation which has focused on unsupervised approaches based on notions of self-similarity computed over an entire song. We evaluate performance using a publicly available data set of 100 copyright-cleared pop/rock songs, each of which has been segmented by a human expert. ©2007 Austrian Computer Society (OCG)."
Davis E.,Finding music in scholarly sets and series: The index to printed music (IPM),2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602231&partnerID=40&md5=9ab649de2b1959612cce2996913f6450,"Davis E., Head of Library, Wiener Music and Arts Library, Columbia University, United States","The Index to Printed Music (IPM) provides access to sets and series of music published beginning in the 19th century. Prepared by scholars and researchers, these titles vary considerably in length (single to multiple volumes), types (topical, pedagogical, historical, etc.), format (treatises, dissertations, editions, etc.), and geographic origin (chiefly Europe and North America). Bibliographical access to their contents is not readily available through library cataloging or reference works. IPM provides title, format, genre, instrumentation, and other metadata access to these publications in three inter-connected databases: Bibliography, Index, and Names, available by subscription through NISC International, Inc. The Index Database now contains over 255,000 entries, the Bibliography Database over 10,000 entries, and the Names Database over 15,000 authority records. Having built this groundwork, future steps involve linking to full-text score images where available through non-commercial projects, and partnerships with publishers and commercial vendors. ©2007 Austrian Computer Society (OCG)."
Gatzsche G.; Mehnert M.; Gatzsche D.; Brandenburg K.,A symmetry based approach for musical tonality analysis,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592290&partnerID=40&md5=e4ef60d7756a886006254db815c3f6c6,"Gatzsche G., Fraunhofer IDMT, Ilmenau, Germany; Mehnert M., Technische Universität, Ilmenau, Germany; Gatzsche D., Hochschule für Musik, Franz Liszt Weimar, Germany; Brandenburg K., Technische Universität, Ilmenau, Germany","We present a geometric approach for tonality analysis called symmetry model. To derive the symmetry model, Carol L.Krumhansl and E.J. Kessler's toroidal Multi Dimensional Scaling (MDS) solution is separated into a key spanning and a key related component. While the key spanning component represents relationships between different keys, the key related component is suitable for the analysis of inner relationships of diatonic keys, for example tension or resolution tendencies, or functional relationships. These features are directly related to the symmetric organisation of tones around the tonal center, which is particularly visualized by the key related component. ©2007 Austrian Computer Society (OCG)."
Silla Jr. C.N.; Koerich A.L.; Kaestner C.A.A.,The latin music database,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949101023&partnerID=40&md5=063fd01af0aae0a1e743789bb549967b,"Silla Jr. C.N., University of Kent, Computing Laboratory, United Kingdom; Koerich A.L., Pontifical Catholic, University of Paranä, Brazil; Kaestner C.A.A., Federal University of Technology of Paranä, Brazil","In this paper we present the Latin Music Database, a novel database of Latin musical recordings which has been developed for automatic music genre classification, but can also be used in other music information retrieval tasks. The method for assigning genres to the musical recordings is based on human expert perception and therefore capture their tacit knowledge in the genre labeling process. We also present the ethnomusicology of the genres available in the database as it might provide important information for the analysis of the results of any experiment that employs the database."
Geleijnse G.; Korst J.,Tool play live: Dealing with ambiguity in artist similarity mining from the web,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596773&partnerID=40&md5=c2503c3054f8c830ccd4b73fa87c7610,"Geleijnse G., Philips Research, Eindhoven, Netherlands; Korst J., Philips Research, Eindhoven, Netherlands","As methods in artist similarity identification using Web Music Information Retrieval perform well on known evaluation sets, we investigate the application of such a method to a more realistic data set. We notice that ambiguous artist names lead to unsatisfying results. We present a simple, efficient and unsupervised method to deal with ambiguous artist names. ©2007 Austrian Computer Society (OCG)."
Miotto R.; Orio N.,A methodology for the segmentation and identification of music works,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580062&partnerID=40&md5=7a968e56a6a12e51d3263ef7afc4be83,"Miotto R., Department of Information Engineering, University of Padova, Italy; Orio N., Department of Information Engineering, University of Padova, Italy","The identification of unknown recordings is a challenging problem that has several applications. In this paper, we focus on the identification of alternative releases of a given music work. To this end, a statistical model of the possible performances of a given score is built from the recording of a single performance. The methodology is based on the automatic segmentation of audio recordings, exploiting a technique that has been proposed for text segmentation. The segmentation is followed by the automatic extraction of a set of relevant audio features from each segment. Identification is then carried out using an application of hidden Markov models. The approach has been tested with a collection of orchestral music, showing good results in the identification of acoustic performances. ©2007 Austrian Computer Society (OCG)."
Han Y.; Raphael C.,Desoloing monaural audio using mixture models,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588067&partnerID=40&md5=547d994ea623ec5546270ae3a205fb90,"Han Y., School of Informaitcs, Indiana Univ., United States; Raphael C., School of Informaitcs, Indiana Univ., United States","We describe a new approach to the ""desoloing"" problem, in which one tries to isolate the accompanying instruments from a monaural recording of a soloist with accompaniment. Our approach is based on explicit knowledge of the audio in the form of a score match - A correspondence between a symbolic score and the music audio, giving the times of all musical events. We employ the familiar idea of masking the short time Fourier transform to eliminate the solo part. The ideal mask is estimated by fitting a model to the data, whose note-based components are derived from the score match. The parameters for our probabilistic model are estimated using the EM algorithm. ©2007 Austrian Computer Society (OCG)."
Jensen J.H.; Ellis D.P.W.; Christensen M.G.; Jensen S.H.,Evaluation of distance measures between gaussian mixture models of MFCCS,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606815&partnerID=40&md5=9798d4ffeefb2e10465732a57fcf8935,"Jensen J.H., Dept. Electron. Syst., Aalborg University, Denmark; Ellis D.P.W., LabROSA, Columbia University, United States; Christensen M.G., Dept. Electron. Syst., Aalborg University, Denmark; Jensen S.H., Dept. Electron. Syst., Aalborg University, Denmark","In music similarity and in the related task of genre classification, a distance measure between Gaussian mixture models is frequently needed. We present a comparison of the Kullback-Leibler distance, the earth movers distance and the normalized L2 distance for this application. Although the normalized L2 distance was slightly inferior to the Kullback-Leibler distance with respect to classification performance, it has the advantage of obeying the triangle inequality, which allows for efficient searching. ©2007 Austrian Computer Society (OCG)."
Leveau P.; Sodoyer D.; Daudet L.,Automatic instrument recognition in a polyphonic mixture using sparse representations,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605208&partnerID=40&md5=7d1b9b2ded599c0a5bee55f4872bac96,"Leveau P., GET-ENST (Télécom Paris), 75014 Paris, 46, rue Dareau, France, Institut Jean Le Rond D'Alembert, University Pierre et Marie Curie, 75015 Paris, 11, rue de Lourmel, France; Sodoyer D., Institut Jean Le Rond D'Alembert, University Pierre et Marie Curie, 75015 Paris, 11, rue de Lourmel, France; Daudet L., Institut Jean Le Rond D'Alembert, University Pierre et Marie Curie, 75015 Paris, 11, rue de Lourmel, France","In this paper, we introduce a method to address automatic instrument recognition in polyphonic music. It is based on the decomposition of the music signal with instrumentspecific harmonic atoms, yielding an approximate object representation of the signal. A post-processing is then applied to exhibit ensemble saliences that give clues about the number of instruments and their labels. The whole algorithm is then applied on artificial mixes of solo performances. The identification of the number of instrument reaches 73 % on 10-s segments and the fully blind problem of identification of the ensemble label without prior knowledge on the number of instruments is 17 %. ©2007 Austrian Computer Society (OCG)."
Allan H.; Ullensiefen D.M̈.; Geraintwiggins,Methodological considerations in studies of musical similarity,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873603426&partnerID=40&md5=108f814ec820182b87e44135d6e3e231,"Allan H., University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom; Ullensiefen D.M̈., University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom; Geraintwiggins, University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom","There are many different aspects of musical similarity [7]. Some relate to acoustic properties, such as melodic [5], rhythmic [10], harmonic [9] and timbral [2]. Others are bound up in cultural aspects: artists involved in creation, year of first release, subject matter of lyrics, demographics of listeners, etc. In judgments about musical similarity, the relative importance of each of these aspects will change, not only for different listeners, but also for the same listener in different contexts [11]. Extra care must therefore be taken when designing studies in musical similarity to ensure that the context is an explicit variable. This paper describes the methodology behind our work in context-based musical similarity; introduces a novel system through which users can specify by example the context and focus of their retrieval needs; and details the design of a study to find parameters for our system which can also be adapted to test the system as a whole. ©2007 Austrian Computer Society (OCG)."
Weyde T.; Wissmann J.; Neubarth K.,An experiment on the role of pitch intervals in melodic segmentation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585075&partnerID=40&md5=ea597e4afbf613a52fb1735ef98310eb,"Weyde T., Department of Computing, City University London, United Kingdom; Wissmann J., Department of Computing, City University London, United Kingdom; Neubarth K., Department of Computing, City University London, United Kingdom","This paper presents the results of an experiment to test the influence of IOI, dynamics, pitch change, and pitch direction change on melodic segmentation, extending an an earlier experiment [4]. The new results show little to no significant influence of pitch, when evaluated by a linear or log-linear statistical model with regression. This supports the earlier findings, which are in contrast to the commonly made assumption that greater pitch intervals lead to melodic segmentation. ©2007 Austrian Computer Society (OCG)."
Nichols E.; Raphael C.,Automatic transcription of music audio through continuous parameter tracking,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605686&partnerID=40&md5=88f7b0c467e5eae6cf4de6bc526ce8e9,"Nichols E., Dept. of Computer Science, Indiana Univ., United States; Raphael C., School of Informatics, Indiana Univ., United States","We present a method for transcribing arbitrary pitched music into a piano-roll-like representation that also tracks the amplitudes of the notes over time. We develop a probabilistic model that gives the likelihood of a frame of audio data given a vector of amplitudes for the possible notes. Using an approximation of the log likelihood function, we develop an objective function that is quadratic in the timevarying amplitude variables, while also depending on the discrete piano-roll variables. We optimize this function using a variant of dynamic programming, by repeatedly growing and pruning our histories. We present results on a variety of different examples using several measures of performance including an edit-distance measure as well as a frame-by-frame measure. ©2007 Austrian Computer Society (OCG)."
Pugin L.; Burgoyne J.A.; Fujinaga I.,Map adaptation to improve optical music recognition of early music documents using hidden markov models,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861078564&partnerID=40&md5=c45c6bc6c17133f6d01521825277a083,"Pugin L., Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Despite steady improvement in optical music recognition (OMR), early documents remain challenging because of the high variability in their contents. In this paper, we present an original approach using maximum a posteriori (MAP) adaptation to improve an OMR tool for early typographic prints dynamically based on hidden Markov models. Taking advantage of the fact that during the normal usage of any OMR tool, errors will be corrected, and thus ground-truth produced, the system can be adapted in real-time. We experimented with five 16th-century music prints using 250 pages of music and two procedures in applying MAP adaptation. With only a handful of pages, both recall and precision rates improved even when the baseline was above 95 percent. ©2007 Austrian Computer Society (OCG)."
Knees P.,Search & select - Intuitively retrieving music from large collections,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873571882&partnerID=40&md5=53eaca6291fbfb48c58553bdb29e9577,"Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria","A retrieval system for large-scale collections that allows users to search for music using natural language queries and relevance feedback is presented. In contrast to existing music search engines that are either restricted to manually annotated meta-data or based on a query-by-example variant, the presented approach describes audio pieces via a traditional term vector model and allows therefore to retrieve relevant music pieces by issuing simple free-form text queries. Term vector descriptors for music pieces are derived by applying Web-based and audio-based similarity measures. Additionally, as the user selects music pieces that he/she likes, the subsequent results are adapted to accommodate to the user's preferences. Real-world performance of the system is indicated by a small user study. ©2007 Austrian Computer Society (OCG)."
Ahmaniemi T.,Influence of tempo and subjective rating of music in step frequency of running,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578675&partnerID=40&md5=9cac8006122267585224a9ff673bf765,"Ahmaniemi T., Nokia Research Center, 00180 Helsinki, Itamerenkatu 11-13, Finland","The objective of this work was to study how the tempo and subjective motivational rating of personal music influence in step frequency during an exercise session. The participants (n=8) were requested to bring their own music to the test and rate it according to the motivational effect of each song. The test was conducted on a sports field where the participants were asked to perform a 30 minute exercise without paying attention to the test setup. Significant correlation was found between the subjective motivational rating of music and step frequency, while tempo did not have any influence. ©2007 Austrian Computer Society (OCG)."
Teodoru G.; Raphael C.,Pitch spelling with conditionally independent voices,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956595929&partnerID=40&md5=8ef788890efced984503821f1570e799,"Teodoru G., School of Informatics, Indiana University, United States; Raphael C., School of Informatics, Indiana University, United States","We introduce a new approach for pitch spelling from MIDI data based on a probabilistic model. The model uses a hidden sequence of variables, one for each measure, describing the local key of the music. The spellings in the voices evolve as conditionally independent Markov chains, given the hidden keys. The model represents both vertical relations through the shared key and horizontal voice-leading relations through the explicit Markov models for the voices. This conditionally independent voice model leads to an efficient dynamic programming algorithm for finding the most likely configuration of hidden variables . spellings and harmonic sequence. The model is also straightforward to train from unlabeled data, though we have not been able to demonstrate any improvement in performance due to training. Our results compare favorably with others when tested on Meredith's corpus, designed specifically for this problem. ©2007 Austrian Computer Society (OCG)."
Deliège F.; Pedersen T.B.,Fuzzy song sets for music warehouses,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599356&partnerID=40&md5=b8854ff45882b4708f82873ded8d9501,"Deliège F., Department of Computer Science, Aalborg University, Denmark; Pedersen T.B., Department of Computer Science, Aalborg University, Denmark","The emergence of music recommendation systems calls for the development of new data management technologies able to query vast music collections. In this paper, we define fuzzy song sets and an algebra to manipulate them. We present a music warehouse prototype able to perform efficient nearest neighbor searches in an arbitrary song similarity space. Using fuzzy song sets, the music warehouse offers a practical solution to the all musical data management scenarios provided: song comparisons, user musical preferences and user feedback. We investigate three practical approaches to tackle the storage issues of fuzzy song sets: Tables, arrays and bitmaps. Finally, we confront theoretical estimates to concrete implementation results and prove that, from a storage perspective, arrays and bitmaps are both effective data structure solutions. ©2007 Austrian Computer Society (OCG)."
Fujihara H.; Goto M.,A music information retrieval system based on singing voice timbre,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585146&partnerID=40&md5=7a15c1c15043fc7bd960bddafa81cd61,"Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan","We developed a music information retrieval system based on singing voice timbre, i.e., a system that can search for songs in a database that have similar vocal timbres. To achieve this, we developed a method for extracting feature vectors that represent characteristics of singing voices and calculating the vocal-timbre similarity between two songs by using a mutual information content of their feature vectors. We operated the system using 75 songs and confirmed that the system worked appropriately. According to the results of a subjective experiment, 80% of subjects judged that compared with a conventional method using MFCC, our method finds more appropriate songs that have similar vocal timbres. ©2007 Austrian Computer Society (OCG)."
Raczyński S.A.; Ono N.; Sagayama S.,Multipitch analysis with harmonic nonnegative matrix approximation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578051&partnerID=40&md5=d39561e3570d5a188cc62e8acfe3f704,"Raczyński S.A., Graduate School of Information Science and Engineering, University of Tokyo, Japan; Ono N., Graduate School of Information Science and Engineering, University of Tokyo, Japan; Sagayama S., Graduate School of Information Science and Engineering, University of Tokyo, Japan","This paper presents a new approach to multipitch analysis by utilizing the Harmonic Nonnegative Matrix Approximation, a harmonically-constrained and penalized version of the Nonnegative Matrix Approximation (NNMA) method. It also includes a description of a note onset, offset and amplitude retrieval procedure based on that technique. Compared with the previous NNMA approaches, specific initialization of the basis matrix is employed - the basis matrix is initialized with zeros everywhere but at positions corresponding to harmonic frequencies of consequent notes of the equal temperament scale. This results in the basis containing nothing but harmonically structured vectors, even after the learning process, and the activity matrix's rows containing peaks corresponding to note onset times and amplitudes. Furthermore, additional penalties of mutual uncorrelation and sparseness of rows are placed upon the activity matrix. The proposed method is able to uncover the underlying musical structure better than the previous NNMA approaches and makes the note detection process very straightforward. ©2007 Austrian Computer Society (OCG)."
Lee J.H.; Downie J.S.; Jones M.C.,Preliminary analyses of information features provided by users for identifying music,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445080&partnerID=40&md5=08357cab9974fd27f00aa16ae23c8b1c,"Lee J.H., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Jones M.C., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","This paper presents preliminary findings based on the analyses of user-provided information features found in 566 queries seeking help in the identification of particular music works or artists. Queries were drawn from the answers.google.com (Google Answers) website. The types and frequency of occurrences of different information features are compared with the results from previous studies of music queries. New feature types have also been developed to obtain a more comprehensive understanding of the kinds of information present in queries including such things as indications of uncertainty, associated use, and the ""aboutness"" of the underlying musical work. The presence of erroneous information in the queries is also discussed. ©2007 Austrian Computer Society (OCG)."
Ehmann A.F.; Downie J.S.; Jones M.C.,"The music information retrieval evaluation exchange ""Do-it- yourself"" web service",2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595747&partnerID=40&md5=99620d17e4252d7d35326b8d46d05e6b,"Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Jones M.C., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","The Do-It-Yourself (DIY) web service of the Music Information Retrieval Evaluation eXchange (MIREX) represents a means by which researchers can remotely submit, execute, and evaluate their Music Information Retrieval (MIR) algorithms against standardized datasets that are not otherwise freely distributable. Since its inception in 2005 at the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL), MIREX has, to date, required heavy interaction by IMIRSEL team members in the execution, debugging, and validation of submitted code. The goal of the MIREX DIY web service is to put such responsibilities squarely into the hands of submitters, and also enable the evaluations of algorithms yearround, as opposed to annual exchanges. ©2007 Austrian Computer Society (OCG)."
Balkema W.,Variable-size gaussian mixture models for music similarity measures,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605158&partnerID=40&md5=f67acfb91951359124ea1ea1eda100cb,"Balkema W., Robert Bosch GmbH - Corporate Research, Hildesheim, Po. Box 77 77 77 31137, Germany","An algorithm to efficiently determine an appropriate number of components for a Gaussian mixture model is presented. For determining the optimal model complexity we do not use a classical iterative procedure, but use the strong correlation between a simple clustering method (BSAS [13]) and an MDL-based method [6]. This approach is computationally efficient and prevents themodel from representing statistically irrelevant data. The performance of these variable size mixture models is evaluated with respect to hub occurrences, genre classification and computational complexity. Our variable size modelling approach marginally reduces the number of hubs, yields 3-4% better genre classification precision and is approximately 40% less computationally expensive. ©2007 Austrian Computer Society (OCG)."
Sapp C.S.,Comparative analysis of multiple musical performances,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584053&partnerID=40&md5=095714ef953e4399a2269c5bcb9151b5,"Sapp C.S., Centre for History and Analysis of Recorded Music (CHARM), University of London Royal Holloway, United Kingdom","A technique for comparing numerous performances of an identical selection of music is described. The basic methodology is to split a one-dimensional sequence into all possible sequential sub-sequences, perform some operation on these sequences, and then display a summary of the results as a two-dimensional plot; the horizontal axis being time and the vertical axis being sub-sequence length (longer lengths on top by convention). Most types of timewise data extracted from performances can be compared with this technique, although the current focus is on beat-level information for tempo and dynamics as well as commixtures of the two. The primary operation used on each sub-sequence is correlation between a reference performance and analogous segments of other performances, then selecting the best correlated performances for the summary display. The result is a useful navigational aid for coping with large numbers of performances of the same piece of music and for searching for possible influence between performances. ©2007 Austrian Computer Society (OCG)."
Aucouturier J.-J.; Pachet F.; Roy P.; Beuriv́e A.,Signal + Context = Better classification,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604587&partnerID=40&md5=05371a7e8d68500300626473d38d5dc4,"Aucouturier J.-J., Grad. School of Arts and Sciences, University of Tokyo, Japan; Pachet F., SONY CSL Paris, 75005 Paris, 6 rue Amyot, France; Roy P., SONY CSL Paris, 75005 Paris, 6 rue Amyot, France; Beuriv́e A., SONY CSL Paris, 75005 Paris, 6 rue Amyot, France","Typical signal-based approaches to extract musical descriptions from audio only have limited precision. A possible explanation is that they do not exploit context, which provides important cues in human cognitive processing of music: e.g. electric guitar is unlikely in 1930s music, children choirs rarely perform heavy metal, etc. We propose an architecture to train a large set of binary classifiers simultaneously, formany differentmusicalmetadata (genre, instrument, mood, etc.), in such a way that correlation between metadata is used to reinforce each individual classifier. The system is iterative: it uses classification decisions it made on some classification problems as new features for new, harder problems; and hybrid: it uses a signal classifier based on timbre similarity to bootstrap symbolic inference with decision trees. While further work is needed, the approach seems to outperform signal-only algorithms by 5% precision on average, and sometimes up to 15% for traditionally difficult problems such as cultural and subjective categories. ©2007 Austrian Computer Society (OCG)."
Little D.; Raffensperger D.; Pardo B.,A query by humming system that learns from experience,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605798&partnerID=40&md5=b014d941f1ab66ed1373c6a07bed36ce,"Little D., EECS Department Northwestern, University Evanston, IL 60201, United States; Raffensperger D., EECS Department Northwestern, University Evanston, IL 60201, United States; Pardo B., EECS Department Northwestern, University Evanston, IL 60201, United States","Query-by-Humming (QBH) systems transcribe a sung or hummed query and search for related musical themes in a database, returning the most similar themes. Since it is not possible to predict all individual singer profiles before system deployment, a robust QBH system should be able to adapt to different singers after deployment. Currently deployed systems do not have this capability. We describe a new QBH system that learns from user provided feedback on the search results, letting the system improve while deployed, after only a few queries. This is made possible by a trainable note segmentation system, an easily parameterized singer error model and a straight-forward genetic algorithm. Results show significant improvement in performance given only ten example queries from a particular user. ©2007 Austrian Computer Society (OCG)."
Wei B.; Zhang C.; Ogihara M.,Keyword generation for lyrics,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439114&partnerID=40&md5=7ef255fa08a21afa5fff891b2633103a,"Wei B., Comp. Sci. Dept., U. Rochester, United States; Zhang C., Comp. Sci. Dept., U. Rochester, United States; Ogihara M., Comp. Sci. Dept., U. Rochester, United States",This paper proposes a scheme for content based keyword generation of song lyrics. Syntactic as well semantic similarity is used for sentence level clustering to separate the topic from the background of a song. A method is proposed to search for a center in the semantic graph ofWord- Net for generating keywords not contained in original text. ©2007 Austrian Computer Society (OCG).
Moreau A.; Flexer A.,Drum transcription in polyphonic music using non-negative matrix factorisation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607102&partnerID=40&md5=7052a7ac8e3abbc17e81362e3d24ebba,"Moreau A., Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; Flexer A., Center for Brain Research Medical, Institute of Medical Cybernetics and Artificial Intelligence, University of Vienna, Austria",We present a system that is based on the non-negative matrix factorisation (NMF) algorithm and is able to transcribe drum onset events in polyphonic music. The magnitude spectrogram representation of the input music is divided by the NMF algorithm into source spectra and corresponding time-varying gains. Each of these source components is classified as a drum instrument or non-drum sound and a peak-picking algorithm determines the onset times. ©2007 Austrian Computer Society (OCG).
Seyerlehner K.; Widmer G.; Schnitzer D.,From rhythm patterns to perceived tempo,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574366&partnerID=40&md5=7a38c57ce68bce92009864458de6be67,"Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Schnitzer D., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","There are many MIR applications for which we would like to be able to determine the perceived tempo of a song automatically. However, automatic tempo extraction itself is still an open problem. In general there are two tempo extraction methods, either based on the estimation of interonset intervals or based on self similarity computations. To predict a tempo the most significant time-lag or the most significant inter-onset-interval is used. We propose to use existing rhythm patterns and reformulate the tempo extraction problem in terms of a nearest neighbor classification problem. Our experiments, based on three different datasets, show that this novel approach performs at least comparably to state-of-the-art tempo extraction algorithms and could be useful to get a deeper insight into the relation between perceived tempo and rhythm patterns. ©2007 Austrian Computer Society (OCG)."
Lanzelotte R.S.G.; Ballesté A.O.; Ulhoa M.,A digital collection of brazilian lundus,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649402017&partnerID=40&md5=8281827d07dda7c11dcd561df21a457a,"Lanzelotte R.S.G., Unirio - Universidade Federal do Estado do Rio de Janeiro, Graduate Music Department, Rio de Janeiro, Brazil; Ballesté A.O., LNCC - Laboratório Nacional de Computação Científica, Rio de Janeiro, Brazil; Ulhoa M., Unirio - Universidade Federal do Estado do Rio de Janeiro, Graduate Music Department, Rio de Janeiro, Brazil","Lundu is a typical Brazilian popular musical form at the 19th century. The distinguished musicologist Mozart de Araújo devoted himself to studying lundus and other forms of that period. He collected 48 lundus, which are nowadays stored in a private library, unavailable to public access. The present work describes the implementation of a digital collection of those lundus, using Dspace as the repository. Dspace is chosen in order to guarantee interoperability through the OAIPMH protocol. Metadata is generated using Dublin Core elements, fully compatible with Dspace. The digital collection provides access to the lundu score images, incipits and midi files, as well as metadata. It is the first time such a rare collection of 19th Brazilian popular music will be available on the web. As Dspace enables interoperation among repositories, a broad community may access the collection. ©2007 Austrian Computer Society (OCG)."
Marsden A.,Automatic derivation of musical structure: A tool for research on schenkerian analysis,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249245688&partnerID=40&md5=366f4bc9f1424a545025bfe7be8bf854,"Marsden A., Lancaster Institute for Contemporary Arts, Lancaster University, United Kingdom","This paper describes software to facilitate research on the automatic derivation of hierarchical (Schenkerian) musical structures from a musical surface. Many MIR tasks require information about musical structure, or would perform better if such information were available. Automatic derivation of musical structure faces two significant obstacles. Firstly, the solution space of possible structural analyses of a piece is very large. Secondly, pieces can have more than one valid structural analysis, and there is little firm agreement among music theorists about how to distinguish a good analysis. To circumvent the first of these obstacles, software has been developed which derives a tractable 'matrix' of possibilities from a musical surface (i.e., MIDI-like note-time information). The matrix is somewhat like the intermediate results of a dynamic-programming algorithm, and in a similar way it is possible to extract a particular structural analysis from the matrix by following the appropriate path from the top level to the surface. It therefore provides a tool to facilitate research on the second obstacle by allowing candidate 'goodness' metrics to be incorporated into the software and tested on actual music. ©2007 Austrian Computer Society (OCG)."
Lartillot O.; Toiviainen P.,Mir in matlab (II): A toolbox for musical feature extraction from audio,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572465&partnerID=40&md5=89ea118d8500d0103149df6f975a181b,"Lartillot O., University of Jyv̈askyl̈a, PL 35(M) 40014, Finland; Toiviainen P., University of Jyv̈askyl̈a, PL 35(M) 40014, Finland","We present the MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio files. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches - including new strategies we have developed -, that users can select and parametrize. This paper offers an overview of the set of features, related, among others, to timbre, tonality, rhythm or form, that can be extracted with the MIRtoolbox. One particular analysis is provided as an example. The toolbox also includes functions for statistical analysis, segmentation and clustering. Particular attention has been paid to the design of a syntax that offers both simplicity of use and transparent adaptiveness to a multiplicity of possible input types. Each feature extraction method can accept as argument an audio file, or any preliminary result from intermediary stages of the chain of operations. Also the same syntax can be used for analyses of single audio files, batches of files, series of audio segments, multi-channel signals, etc. For that purpose, the data and methods of the toolbox are organised in an object-oriented architecture. ©2007 Austrian Computer Society (OCG)."
Mandel M.I.; Ellis D.P.W.,A web-based game for collecting music metadata,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594016&partnerID=40&md5=9ff6c3f605cda20a75b96bf5cef46f65,"Mandel M.I., Dept. Electrical Engineering, LabROSA, Columbia University, United States; Ellis D.P.W., Dept. Electrical Engineering, LabROSA, Columbia University, United States","We have designed a web-based game to make collecting descriptions of musical excerpts fun, easy, useful, and objective. Participants describe 10 second clips of songs and score points when their descriptions match those of other participants. The rules were designed to encourage users to be thorough and the clip length was chosen to make judgments more objective and specific. Analysis of preliminary data shows that we are able to collect objective and specific descriptions of clips and that players tend to agree with one another. ©2007 Austrian Computer Society (OCG)."
Peeling P.; Cemgil A.T.; Godsill S.,A probabilistic framework for matching music representations,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572185&partnerID=40&md5=d1c7102532bfef8343f8a299a8f10cac,"Peeling P., Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom; Cemgil A.T., Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom; Godsill S., Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom","In this paper we introduce a probabilistic framework for matching different music representations (score, MIDI, audio) by incorporating models of how one musical representation might be rendered from another. We propose a dynamical hidden Markov model for the score pointer as a prior, and two observation models, the first based on matching spectrogram data to a trained template, the second detecting damped sinusoids within a frame of audio by subspace methods. The resulting Bayesian framework is robust to local variations in tempo, and can be used for a wide variety of applications. We evaluate both methods in a score alignment context by inferring the posterior distribution of the current position in the score exactly. The spectrogram method is shown to infer the score position reliably with minimal computation, and the damped sinusoid model is able to pinpoint the positions of score events in the audio with a high level of timing accuracy. ©2007 Austrian Computer Society (OCG)."
Gillet O.; Richard G.,Supervised and unsupervised sequence modelling for DRUM transcription,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650867380&partnerID=40&md5=4a20c242ce72e06ee3139abeec766989,"Gillet O., GET / Télécom Paris (ENST), CNRS LTCI, 75014 Paris, 37 rue Dareau, France; Richard G., GET / Télécom Paris (ENST), CNRS LTCI, 75014 Paris, 37 rue Dareau, France","We discuss in this paper two post-processings for drum transcription systems, which aim to model typical properties of drum sequences. Both methods operate on a symbolic representation of the sequence, which is obtained by quantizing the onsets of drum strokes on an optimal tatum grid, and by fusing the posterior probabilities produced by the drum transcription system. The first proposed method is a generalization of the N-gram model. We discuss several training and recognition strategies (style-dependent models, local models) in order to maximize the reliability and the specificity of the trained models. Alternatively, we introduce a novel unsupervised algorithm based on a complexity criterion, which finds the most regular and wellstructured sequence compatible with the acoustic scores produced by the transcription system. Both approaches are evaluated on a subset of the ENST-drums corpus, and yield performance improvements. ©2007 Austrian Computer Society (OCG)."
Feng L.; Nielsen A.B.; Hansen L.K.,Vocal segment classification in popular music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71349088259&partnerID=40&md5=bd24537dc81b9271241cda30fa417712,"Feng L., Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark; Nielsen A.B., Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark; Hansen L.K., Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark","This paper explores the vocal and non-vocal music classification problem within popular songs. A newly built labeled database covering 147 popular songs is announced. It is designed for classifying signals from 1sec time windows. Features are selected for this particular task, in order to capture both the temporal correlations and the dependencies among the feature dimensions. We systematically study the performance of a set of classifiers, including linear regression, generalized linear model, Gaussian mixture model, reduced kernel orthonormalized partial least squares and K-means on cross-validated training and test setup. The database is divided in two different ways: with/without artist overlap between training and test sets, so as to study the so called 'artist effect'. The performance and results are analyzed in depth: from error rates to sample-to-sample error correlation. A voting scheme is proposed to enhance the performance under certain conditions."
Law E.L.M.; Ahn L.V.; Dannenberg R.B.; Crawford M.,Tagatune: A game for music and sound annotation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600416&partnerID=40&md5=61047ba55e1fa95ce6b0d25271e341a3,"Law E.L.M., School of Computer Science, Carnegie Mellon University, United States; Ahn L.V., School of Computer Science, Carnegie Mellon University, United States; Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States; Crawford M., School of Computer Science, Carnegie Mellon University, United States","Annotations of audio files can be used to search and index music and sound databases, provide data for system evaluation, and generate training data for machine learning. Unfortunately, the cost of obtaining a comprehensive set of annotations manually is high. One way to lower the cost of labeling is to create games with a purpose that people will voluntarily play, producing useful metadata as a by-product. TagATune is an audio-based online game that aims to extract descriptions of sounds and music from human players. This paper presents the rationale, design and preliminary results from a pilot study using a prototype of TagATune to label a subset of the FreeSound database. ©2007 Austrian Computer Society (OCG)."
Novello A.; Mckinney M.,Assessment of perceptual music similarity,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601901&partnerID=40&md5=7d8e3d6ecedca3ed8dff4134bff7bda0,"Novello A., Philips Research Laboratories, Eindhoven, Netherlands; Mckinney M., Philips Research Laboratories, Eindhoven, Netherlands","This paper extends a study on music similarity perception presented at ISMIR last year, in which subjects ranked the similarity of excerpt-pairs presented in triads [1]. The larger number of subjects and stimuli in the current study required a modification of the methodological strategy. We use here two nested incomplete block designs in order to cover the full set of song-excerpts comparisons (triads) while limiting the experimental time per subject. In addition to the two variable factors of the previous experiment, tempo and genre, we examine here the effect of prevalent instrument timbre. We found that 69 of 78 subjects where significantly consistent in their judgments of repeated triads. Furthermore, we found significant acrosssubject consistency on all 10 repeated triads. A significant difference was found in the distributions of interand intra-genre excerpt distances. The stress values in the Shepard's plot shows evidence of increased complexity in the present study compared to the previous smaller study. ©2007 Austrian Computer Society (OCG)."
Duggan B.; O'Shea B.; Gainza M.; Cunningham P.,Machine annotation of sets of traditional irish dance tunes,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949091555&partnerID=40&md5=13e1bf3a275020547b181a785f1cd1ad,"Duggan B., DIT School of Computing, Dublin 8, Kevin St., Ireland; O'Shea B., DIT School of Computing, Dublin 8, Kevin St., Ireland; Gainza M., Audio Research Group, DIT, Dublin 8, Kevin St., Ireland; Cunningham P., School of Informatics and Computer Science, UCD, Dublin, Ireland","A set in traditional Irish music is a sequence of two or more dance tunes in the same time signature, where each tune is repeated an arbitrary number of times. A turn in a set represents the point at which either a tune repeats or a new tune is introduced. Tunes in sets are played in a segue (without a pause) and so detecting the turn is a significant challenge. This paper presents the MATS algorithm, a novel algorithm for identifying turns in sets of traditional Irish music. MATS works on digitised audio files of monophonic flute and tin-whistle music. Previous work on machine annotation of traditional music is summarised and experimental results validating the MATS algorithm are presented."
Tzanetakis G.; Jones R.; McNally K.,Stereo panning features for classifying recording production style,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605702&partnerID=40&md5=582ae21fb3ae24a2da38ea70eb512330,"Tzanetakis G., University of Victoria, Canada; Jones R., University of Victoria, Canada; McNally K., University of Victoria, Canada","Recording engineers, mixers and producers play important yet often overlooked roles in defining the sound of a particular record, artist or group. The placement of different sound sources in space using stereo panning information is an important component of the production process. Audio classification systems typically convert stereo signals to mono and to the best of our knowledge have not utilized information related to stereo panning. In this paper we propose a set of audio features that can be used to capture stereo information. These features are shown to provide statistically important information for non-trivial audio classification tasks and are compared with the traditional Mel-Frequency Cepstral Coefficients. The proposed features can be viewed as a first attempt to capture extra-musical information related to the production process through music information retrieval techniques. ©2007 Austrian Computer Society (OCG)."
Tiemann M.; Pauws S.; Vignoli F.,Ensemble learning for hybrid music recommendation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580126&partnerID=40&md5=b5403e2e3d2ab85590960184179dd30b,"Tiemann M., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Pauws S., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Vignoli F., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands","We investigate ensemble learning methods for hybrid music recommenders, combining a social and a content-based recommender algorithm in an initial experiment by applying a simple combination rule to merge recommender results. A first experiment suggests that such a combination can reduce the mean absolute prediction error compared to the used recommenders' individual errors. ©2007 Austrian Computer Society (OCG)."
Varewyck M.; Martens J.-P.,Assessment of state-of-the-art meter analysis systems with an extended meter description model,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582302&partnerID=40&md5=18358dda0bf3bdd16460041d032ac8d2,"Varewyck M., Department of Electronics and Information Systems, Ghent University (Belgium), Belgium; Martens J.-P., Department of Electronics and Information Systems, Ghent University (Belgium), Belgium","An extended meter description model capturing the hierarchical metrical structure of Western music is proposed. The model is applied for the quantitative evaluation of four state-of-the-art automatic meter analysis algorithms of musical audio. Evaluation results suggest that the best beat trackers reach a reasonable level of performance, but that none of the tested algorithms has the potential to perform a reliable bar onset tracking. Moreover, the frontends of the best over-all systems not necessarily seem to have the front-ends best encoding the time signature in their output. Therefore, further improvements of these systems should be attainable by a better combination of ideas that can be borrowed from existing algorithms. ©2007 Austrian Computer Society (OCG)."
Peng W.; Li T.; Ogihara M.,Music clustering with constraints,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606875&partnerID=40&md5=39d58f530d85f03d6d5f05f5851e25d1,"Peng W., School of Computer Science, Florida International University, United States; Li T., School of Computer Science, Florida International University, United States; Ogihara M., Department of Computer Science, University of Rochester, United States","This paper studies the problem of building clusters of music tracks in a collection of popular music in the presence of constraints. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). We present an approach based on the generalized constraint clustering algorithm by incorporating the constraints for grouping music by ""similar"" artists. The approach is evaluated on a data set consisting of 53 albums covering 41 popular artists. The ""correctness"" of the clusters generated is tested using artist similarity provided by All Music Guide. ©2007 Austrian Computer Society (OCG)."
You W.; Dannenberg R.B.,Polyphonic music note onset detection using semi-supervised learning,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601587&partnerID=40&md5=e8ae87d7e732aaed3082566d4341a1d4,"You W., Schools of Computer Science and Music, Carnegie Mellon University, United States; Dannenberg R.B., Schools of Computer Science and Music, Carnegie Mellon University, United States","Automatic note onset detection is particularly difficult in orchestral music (and polyphonic music in general). Machine learning offers one promising approach, but it is limited by the availability of labeled training data. Score-toaudio alignment, however, offers an economical way to locate onsets in recorded audio, and score data is freely available for many orchestral works in the form of standard MIDI files. Thus, large amounts of training data can be generated quickly, but it is limited by the accuracy of the alignment, which in turn is ultimately related to the problem of onset detection. Semi-supervised or bootstrapping techniques can be used to iteratively refine both onset detection functions and the data used to train the functions. We show that this approach can be used to improve and adapt a general purpose onset detection algorithm for use with orchestral music. ©2007 Austrian Computer Society (OCG)."
Hamanaka M.; Hirata K.; Tojo S.,ATTA: Implementing gttm on a computer,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436870&partnerID=40&md5=80fc17d2af5368c175b2172e95e69486,"Hamanaka M., University of Tsukuba, Japan; Hirata K., NTT Communication Sience Laboratories, Japan; Tojo S., Japan Advanced Institute of Science and Technology, Japan","We have been discussing the design principle for the implementation of GTTM and presented the semiautomatic generation techniques of grouping structure, metrical structure, and time-span tree, and the searching method for the optimal parameter value assignments. In ISMIR2007, we organize a tutorial session on the techniques for implementing music theory GTTM for summarizing our work and report it to relevant participants of the conference. Since the time of the tutorial session is not enough, we demonstrate a working automatic timespan tree analyzer ATTA in a demo session. ATTA is an integration of our work done so far; by looking at the ATTA demonstration or using ATTA, people will be able to understand the techniques for implementing GTTM as well as GTTM itself in more detail. ©2007 Austrian Computer Society (OCG)."
Cunningham S.J.; Bainbridge D.; Mckay D.,Finding new music: A diary study of everyday encounters with novel songs,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595056&partnerID=40&md5=38212dd6a78cec56444db3a2d0857b8c,"Cunningham S.J., Department of Computer Science, University of Waikato, Hamilton, New Zealand; Bainbridge D., Department of Computer Science, University of Waikato, Hamilton, New Zealand; Mckay D., University of Technology, Information Resources Swinburne, Melbourne, Australia","This paper explores how we, as individuals, purposefully or serendipitously encounter ""new music"" (that is, music that we haven't heard before) and relates these behaviours to music information retrieval activities such as music searching and music discovery via use of recommender systems. 41 participants participated in a three-day diary study, in which they recorded all incidents that brought them into contact with new music. The diaries were analyzed using a Grounded Theory approach. The results of this analysis are discussed with respect to location, time, and whether the music encounter was actively sought or occurred passively. Based on these results, we outline design implications for music information retrieval software, and suggest an extension of ""laid back"" searching. ©2007 Austrian Computer Society (OCG)."
Volk A.; Van Kranenburg P.; Garbers J.; Wiering F.; Veltkamp R.C.; Grijp L.P.,A manual annotation method for melodic similarity and the study of melody feature sets,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350483637&partnerID=40&md5=4cd05c7fddc820f3e03562f671c94549,"Volk A., Department of Information and Computing Sciences, Utrecht University, Netherlands; Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Netherlands; Garbers J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Department of Information and Computing Sciences, Utrecht University, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Netherlands; Grijp L.P., Department of Information and Computing Sciences, Utrecht University, Netherlands, Meertens Institute, Amsterdam, Netherlands",This paper describes both a newly developed method for manual annotation for aspects of melodic similarity and its use for evaluating melody features concerning their contribution to perceived similarity. The second issue is also addressed with a computational evaluation method. These approaches are applied to a corpus of folk song melodies. We show that classification of melodies could not be based on single features and that the feature sets from the literature are not sufficient to classify melodies into groups of related melodies. The manual annotations enable us to evaluate various models for melodic similarity.
Jones M.C.; Downie J.S.; Ehmann A.F.,Human similarity judgments: Implications for the design of formal evaluations,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607335&partnerID=40&md5=02af5d41e116237628cc5f93faaf7c4b,"Jones M.C., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","This paper presents findings of a series of analyses of human similarity judgments from the Symbolic Melodic Similarity, and Audio Music Similarity tasks from the Music Information Retrieval Evaluation Exchange (MIREX) 2006. The categorical judgment data generated by the evaluators is analyzed with regard to judgment stability, inter-grader reliability, and patterns of disagreement, both within and between the two tasks. An exploration of this space yields implications for the design of MIREX-like evaluations. ©2007 Austrian Computer Society (OCG)."
Flexer A.,A closer look on artist filters for musical genre classification,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580369&partnerID=40&md5=02600c30869d345cf76b457bba8cfda5,"Flexer A., Center for Brain Research, Institute of Medical Cybernetics and Artificial Intelligence, Medical University of Vienna Austria, Freyung 6/2, A-1010 Vienna, Austria",Musical genre classification is the automatic classification of audio signals into user defined labels describing pieces of music. A problem inherent to genre classification experiments in music information retrieval research is the use of songs from the same artist in both training and test sets. We show that this does not only lead to overoptimistic accuracy results but also selectively favours particular classification approaches. The advantage of using models of songs rather than models of genres vanishes when applying an artist filter. The same holds true for the use of spectral features versus fluctuation patterns for preprocessing of the audio files. ©2007 Austrian Computer Society (OCG).
Hurley N.J.; Balado F.; Mccarthy E.P.; Silvestre G.C.M.,Performance of philips audio fingerprinting under desynchronisation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576154&partnerID=40&md5=1e2ac6ce29dcc8f759c994ff0d3ab5ab,"Hurley N.J., School of Computer Science and Informatics, University College Dublin, Ireland; Balado F., School of Computer Science and Informatics, University College Dublin, Ireland; Mccarthy E.P., School of Computer Science and Informatics, University College Dublin, Ireland; Silvestre G.C.M., School of Computer Science and Informatics, University College Dublin, Ireland",An audio fingerprint is a compact representation (robust hash) of an audio signal which is linked to its perceptual content. Perceptually equivalent instances of the signal must lead to the same hash value. Fingerprinting finds application in efficient indexing of music databases. We present a theoretical analysis of the Philips audio fingerprinting method under desynchronisation for correlated stationary Gaussian sources. ©2007 Austrian Computer Society (OCG).
Yoshii K.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.,Improving efficiency and scalability of model-based music recommender system based on incremental training,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587838&partnerID=40&md5=a4dde72002fc086c4c50d9a49b3172d9,"Yoshii K., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Komatani K., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; Ogata T., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; Okuno H.G., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan","We aimed at improving the efficiency and scalability of a hybrid music recommender system based on a probabilistic generative model that integrates both collaborative data (rating scores provided by users) and content-based data (acoustic features of musical pieces). Although the hybrid system was proved to make accurate recommendations, it lacks efficiency and scalability. In other words, the entire model needs to be re-trained from scratch whenever a new score, user, or piece is added. Furthermore, the system cannot deal with practical numbers of users and pieces on an enterprise scale. To improve efficiency, we propose an incremental method that partially updates the model at low computational cost. To enhance scalability, we propose a method that first constructs a small ""core"" model over fewer virtual representatives created from real users and pieces, and then adds the real users and pieces to the core model by using the incremental method. The experimental results revealed that the proposed system was not only efficient and scalable but also outperformed the original system in terms of accuracy. ©2007 Austrian Computer Society (OCG)."
Doraisamy S.; Golzari S.; Norowi N.M.; Sulaiman M.N.B.; Udzir N.I.,A study on feature selection and classification techniques for automatic genre classification of traditional malay music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949108828&partnerID=40&md5=50fcc1ce99ad86f4f8eea749d234faab,"Doraisamy S., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Golzari S., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Norowi N.M., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Sulaiman M.N.B., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Udzir N.I., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia","Machine learning techniques for automated musical genre classification is currently widely studied. With large collections of digital musical files, one approach to classification is to classify by musical genres such as pop, rock and classical in Western music. Beat, pitch and temporal related features are extracted from audio signals and various machine learning algorithms are applied for classification. Features that resulted in better classification accuracies for Traditional Malay Music (TMM), in comparison to western music, in a previous study were beat related features. However, only the J48 classifier was used and in this study we perform a more comprehensive investigation on improving the classification of TMM. In addition, feature selection was performed for dimensionality reduction. Classification accuracies using classifiers of varying paradigms on a dataset comprising ten TMM genres were obtained. Results identify potentially useful classifiers and show the impact of adding a feature selection phase for TMM genre classification."
De Haas W.B.; Veltkamp R.C.; Wiering F.,Tonal pitch step distance: A similarity measure for chord progressions,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450287180&partnerID=40&md5=0b2869dd4855c5b9846f9f86145af52c,"De Haas W.B., Departement of Information and Computing Sciences, Utrecht University, Netherlands; Veltkamp R.C., Departement of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Departement of Information and Computing Sciences, Utrecht University, Netherlands","The computational analysis of musical harmony has received a lot of attention the last decades. Although it is widely recognized that extracting symbolic chord labels from music yields useful abstractions, and the number of chord labeling algorithms for symbolic and audio data is steadily growing, surprisingly little effort has been put into comparing sequences of chord labels. This study presents and tests a new distance function that measures the difference between chord progressions. The presented distance function is based on Lerdahl's Tonal Pitch Space [10]. It compares the harmonic changes of two sequences of chord labels over time. This distance, named the Tonal Pitch Step Distance (TPSD), is shown to be effective for retrieving similar jazz standards found in the Real Book [3]. The TPSD matches the human intuitions about harmonic similarity which is demonstrated on a set of blues variations."
Fremerey C.; Kurth F.; Müller M.; Clausen M.,A demonstration of the syncplayer system,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956333233&partnerID=40&md5=b7d0c6bb3d8d1cabb5a9c4089c00eac2,"Fremerey C., Department of Computer Science III, Bonn University, Germany; Kurth F., Department of Computer Science III, Bonn University, Germany; Müller M., Department of Computer Science III, Bonn University, Germany; Clausen M., Department of Computer Science III, Bonn University, Germany","The SyncPlayer system is an advanced audio player for multimodal presentation, browsing, and retrieval of music data. The system has been extended significantly in the last few years. In this contribution, we describe the current state of the system and demonstrate the functionalities and interactions of the novel SyncPlayer components including combined inter- and intra-document music browsing. ©2007 Austrian Computer Society (OCG)."
Ohishi Y.; Goto M.; Itou K.; Takeda K.,A stochastic representation of the dynamics of sung melody,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455186199&partnerID=40&md5=402e6988f8d0a1b34560ebe55226f080,"Ohishi Y., Graduate School of Information Science, Nagoya University, Japan; Goto M., National Institute of Advanced, Industrial Science and Technology (AIST), Japan; Itou K., Faculty of Computer and Information Sciences, Hosei University, Japan; Takeda K., Graduate School of Information Science, Nagoya University, Japan","In this paper, we propose a stochastic representation of a sung melodic contour, called stochastic phase representation (SPR), which can characterize both musical-note information and the dynamics of singing behaviors included in the melodic contour. The SPR is constructed by fitting probability distribution functions to F0 trajectories in the F0-¢F0 phase plane. Since fluctuations in singing can be easily separated by using SPR, we applied SPR to a melodic similarity measure for query-by-humming (QBH) applications. Our experimental results showed that the SPR-based similarity measure was superior to a conventional dynamic-programming-based method. ©2007 Austrian Computer Society (OCG)."
Kurth F.; Müller M.; Fremerey C.; Chang Y.-H.; Clausen M.,Automated synchronization of scanned sheet music with audio recordings,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596210&partnerID=40&md5=4c6ac9cf9134365a5594bc60102e2456,"Kurth F., Department of Computer Science III, Bonn University, Germany; Müller M., Department of Computer Science III, Bonn University, Germany; Fremerey C., Department of Computer Science III, Bonn University, Germany; Chang Y.-H., Department of Computer Science III, Bonn University, Germany; Clausen M., Department of Computer Science III, Bonn University, Germany","In this paper, we present a procedure for automatically synchronizing scanned sheet music with a corresponding CD audio recording, where suitable regions (given in pixels) of the scanned digital images are linked to time positions of the audio file. In a first step, we extract note parameters and 2D position information from the scanned images using standard software for optical music recognition (OMR).We then use a chroma-based synchronization algorithm to align the note parameters to the given audio recording. Our experiments show that even though the output of current OMR software is often erroneous, the music parameters extracted from the digital images still suffice to derive a reasonable alignment with the audio data stream. The resulting link structure can be used to highlight the current position in the scanned score or to automatically turn pages during playback of an audio recording. Such functionalities have been realized as plug-in for the SyncPlayer, which is a free prototypical software framework for bringing together various MIR techniques and applications. ©2007 Austrian Computer Society (OCG)."
Duda A.; Nürnberger A.; Stober S.,Towards query by singing/humming on audio databases,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053096604&partnerID=40&md5=76c2402ed0707dc71cf0036d0fa6dd90,"Duda A., Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany; Nürnberger A., Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany; Stober S., Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany","Current work on Query-by-Singing/Humming (QBSH) focusses mainly on databases that contain MIDI files. Here, we present an approach that works on real audio recordings that bring up additional challenges. To tackle the problem of extracting the melody of the lead vocals from recordings, we introduce a method inspired by the popular ""karaoke effect"" exploiting information about the spatial arrangement of voices and instruments in the stereo mix. The extracted signal time series are aggregated into symbolic strings preserving the local approximated values of a feature and revealing higher-level context patterns. This allows distance measures for string pattern matching to be applied in the matching process. A series of experiments are conducted to assess the discrimination and robustness of this representation. They show that the proposed approach provides a viable baseline for further development and point out several possibilities for improvement. ©2007 Austrian Computer Society (OCG)."
Martins L.G.; Burred J.J.; Tzanetakis G.; Lagrange M.,Polyphonic instrument recognition using spectral clustering,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606114&partnerID=40&md5=cf802ae42995fd874130b85301d52038,"Martins L.G., Telecommunications and Multimedia Unit, INESC Porto Porto, Portugal; Burred J.J., Communication Systems Group, Technical University of Berlin, Berlin, Germany; Tzanetakis G., Computer Science Department, University of Victoria, Victoria BC, Canada; Lagrange M., Computer Science Department, University of Victoria, Victoria BC, Canada","The identification of the instruments playing in a polyphonic music signal is an important and unsolved problem in Music Information Retrieval. In this paper, we propose a framework for the sound source separation and timbre classification of polyphonic, multi-instrumental music signals. The sound source separation method is inspired by ideas from Computational Auditory Scene Analysis and formulated as a graph partitioning problem. It utilizes a sinusoidal analysis front-end and makes use of the normalized cut, applied as a global criterion for segmenting graphs. Timbre models for six musical instruments are used for the classification of the resulting sound sources. The proposed framework is evaluated on a dataset consisting of mixtures of a variable number of simultaneous pitches and instruments, up to a maximum of four concurrent notes. ©2007 Austrian Computer Society (OCG)."
Holzapfel A.; Stylianou Y.,Beat tracking using group delay based onset detection,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349448865&partnerID=40&md5=d33bfc6ac2c42a60117a2c50ea9f5ef5,"Holzapfel A., Institute of Computer Science, FORTH, Greece, Multimedia Informatics Lab., Computer Science Department, University of Crete, Greece; Stylianou Y., Institute of Computer Science, FORTH, Greece, Multimedia Informatics Lab., Computer Science Department, University of Crete, Greece","This paper introduces a novel approach to estimate onsets in musical signals based on the phase spectrum and specifically using the average of the group delay function. A frame-by-frame analysis of a music signal provides the evolution of group delay over time, referred to as phase slope function. Onsets are then detected simply by locating the positive zero-crossings of the phase slope function. The proposed approach is compared to an amplitude-based onset detection approach in the framework of a state-of-the-art system for beat tracking. On a data set of music with less percussive content, the beat tracking accuracy achieved by the system is improved by 82% when the suggested phase-based onset detection approach is used instead of the amplitude-based approach, while on a set of music with stronger percussive characteristics both onset detection approaches provide comparable results of accuracy."
Garbers J.; Wiering F.,Towards structural alignment of folk songs,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350486119&partnerID=40&md5=6292af4e416837fca41cc536bdd9506d,"Garbers J., Utrecht University, Department of Information and Computing Sciences, Netherlands; Wiering F., Utrecht University, Department of Information and Computing Sciences, Netherlands","We describe an alignment-based similarity framework for folk song variation research. The framework makes use of phrase and meter information encoded in Humdrum scores. Local similarity measures are used to compute match scores, which are combined with gap scores to form increasingly larger alignments and higher-level similarity values. We discuss the effects of some similarity measures on the alignment of four groups of melodies that are variants of each other."
Slaney M.; White W.,Similarity based on rating data,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573578&partnerID=40&md5=647b299409d3cc0aa35e14394b34374d,"Slaney M., Yahoo Research 2821 Mission College Blvd., Santa Clara, CA 95054, United States; White W., Yahoo Media Innovation 1950 University Ave., Berkeley CA 94704, United States","This paper describes an algorithm to measure the similarity of two multimedia objects, such as songs or movies, using users' preferences. Much of the previous work on query-by-example (QBE) or music similarity uses detailed analysis of the object's content. This is difficult and it is often impossible to capture how consumers react to the music. We argue that a large collection of user's preferences is more accurate, at least in comparison to our benchmark system, at finding similar songs. We describe an algorithm based the song's rating data, and show how this approach works by measuring its performance using an objective metric based on whether the same artist performed both songs. Our similarity results are based on 1.5 million musical judgments by 380,000 users. We test our system by generating playlists using a content-based system, our rating-based system, and a random list of songs. Music listeners greatly preferred the ratings-based playlists over the content-based and random playlists. ©2007 Austrian Computer Society (OCG)."
Lamere P.; Eck D.,Using 3D visualizations to explore and discover music,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598638&partnerID=40&md5=d88cde757154d8dcb813312e65243bd6,"Lamere P., Sun Labs Sun Microsystems, Burlington, MA, United States; Eck D., Sun Labs Sun Microsystems, Burlington, MA, United States","This paper presents Search Inside the Music an application for exploring and discovering new music. Search Inside the Music uses a music similarity model and 3D visualizations to provide a user with new tools for exploring and interacting with a music collection. With Search Inside the Music, a music listener can find new music, generate interesting playlists, and interact with their music collection. ©2007 Austrian Computer Society (OCG)."
Ellis D.P.W.,Classifying music audio with timbral and chroma features,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607165&partnerID=40&md5=a11847e986c3ebd3de5479a9f5b66273,"Ellis D.P.W., Dept. Elec. Eng., LabROSA, Columbia University, United States","Music audio classification has most often been addressed by modeling the statistics of broad spectral features, which, by design, exclude pitch information and reflect mainly instrumentation. We investigate using instead beat-synchronous chroma features, designed to reflect melodic and harmonic content and be invariant to instrumentation. Chroma features are less informative for classes such as artist, but contain information that is almost entirely independent of the spectral features, and hence the two can be profitably combined: Using a simple Gaussian classifier on a 20-way pop music artist identification task, we achieve 54% accuracy with MFCCs, 30% with chroma vectors, and 57% by combining the two. All the data and Matlab code to obtain these results are available. ©2007 Austrian Computer Society (OCG)."
Ramirez R.; Perez A.; Kersten S.,Performer identification in celtic violin recordings,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949092827&partnerID=40&md5=952918b434b1d6ae6e49273e2bf16db4,"Ramirez R., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain; Perez A., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain; Kersten S., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain",We present an approach to the task of identifying performers from their playing styles. We investigate how violinists express and communicate their view of the musical content of Celtic popular pieces and how to use this information in order to automatically identify performers. We study notelevel deviations of parameters such as timing and amplitude. Our approach to performer identification consists of inducing an expressive performance model for each of the interpreters (essentially establishing a performer dependent mapping of inter-note features to a timing and amplitude expressive transformations). We present a successful performer identification case study.
Anglade A.; Tiemann M.; Vignoli F.,Virtual communities for creating shared music channels,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585271&partnerID=40&md5=c4a60a4f3dd91b28280f7faa1c295b2c,"Anglade A., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Tiemann M., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Vignoli F., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands","We present an approach to automatically create virtual communities of users with similar music tastes. Our goal is to create personalized music channels for these communities in a distributed way, so that they can for example be used in peer-to-peer networks. To find suitable techniques for creating these communities we analyze graphs created from real-world recommender datasets and identify specific properties of these datasets. Based on these properties we select and evaluate different graph-based community-extraction techniques. We select a technique that exploits identified properties to create clusters of music listeners. We validate the suitability of this technique using a music dataset and a large movie dataset. On a graph of 6,040 peers, the selected technique assigns at least 85% of the peers to optimal communities, and obtains a mean classification error of less than 0.05 over the remaining peers that are not assigned to the best community. ©2007 Austrian Computer Society (OCG)."
Roy P.; Pachet F.; Krakowski S.,Improving the classification of percussive sounds with analytical features: A case study,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583017&partnerID=40&md5=eedd341e4568b397df6fa11afeab315d,"Roy P., Sony CSL 6, Paris, rue Amyot 75 005, France; Pachet F., Sony CSL Paris, Paris, 6, rue Amyot 75 005, France; Krakowski S., Sony CSL Paris, Paris, 6, rue Amyot 75 005, France","There is an increasing need for automatically classifying sounds for MIR and interactive music applications. In the context of supervised classification, we conducted experiments with so-called analytical features, an approach that improves the performance of the general bag-of-frame scheme without loosing its generality. These analytical features are better, in a sense we define precisely than standard, general features, or even than ad hoc features designed by hand for specific problems. Our method allows us to build a large number of these features, evaluate and select them automatically for arbitrary audio classification problems. We present here a specific study concerning the analysis of Pandeiro (Brazilian tambourine) sounds. Two problems are considered: the classification of entire sounds, for MIR applications, and the classification of attack portions of the sound only, for interactive music applications. We evaluate precisely the gain obtained by analytical features on these two problems, in comparison with standard approaches. ©2007 Austrian Computer Society (OCG)."
Knopke I.; Byrd D.,Towards musicdiff: A foundation for improved optical music recognition using multiple recognizers,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578917&partnerID=40&md5=ba9f1b07ad840c0c39f8d641b73614a2,"Knopke I., School of Informatics, Indiana University, United States; Byrd D., School of Informatics and Jacobs School of Music, Indiana University, United States","This paper presents work towards a ""musicdiff"" program for comparing files representing different versions of the same piece, primarily in the context of comparing versions produced by different optical music recognition (OMR) programs. Previous work by the current authors and others strongly suggests that using multiple recognizers will make it possible to improve OMR accuracy substantially. The basicmethodology requires several stages: documents must be scanned and submitted to several OMR programs, programs whose strengths and weaknesses have previously been evaluated in detail. We discuss techniques we have implemented for normalization, alignment and rudimentary error correction. We also describe a visualization tool for comparing multiple versions on a measure-by-measure basis. ©2007 Austrian Computer Society (OCG)."
Hu X.; Bay M.; Downie J.S.,Creating a simplified music mood classification ground-truth set,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602017&partnerID=40&md5=12b70bd3c0609b588a510f145e509183,"Hu X., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Bay M., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","A standardized mood classification testbed is needed for formal cross-algorithm comparison and evaluation. In this poster, we present a simplification of the problems associated with developing a ground-truth set for the evaluation of mood-based Music Information Retrieval (MIR) systems. Using a dataset derived from Last.fm tags and the USPOP audio collection, we have applied a K-means clustering method to create a simple yet meaningful cluster-based set of high-level mood categories as well as a ground-truth dataset. ©2007 Austrian Computer Society (OCG)."
Peeters G.,Sequence representation of music structure using higher-order similarity matrix and maximum-likelihood approach,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584066&partnerID=40&md5=1fa4beab9b5e2c789a5e179d68a0cde6,"Peeters G., CNRS STMS, 75004 Paris, 1, pl. Igor Stranvinsky, France","In this paper, we present a novel method for the automatic estimation of the structure of music tracks using a sequence representation. A set of timbre-related (MFCC and Spectral Contrast) and pitch-related (Pitch Class Profile) features are first extracted from the signal leading to three similarity matrices which are then combined. We then introduce the use of higher-order (2nd and 3rd order) similarity matrices in order to reinforce the diagonals corresponding to common repetitions and reduce the background noise. Segments are then detected and a maximum-likelihood approach is proposed in order to derive simultaneously the underlying sequence representation of the music track and the most representative segment of each sequence. The proposed method is evaluated positively on the MPEG-7 ""melody repetition"" test set. ©2007 Austrian Computer Society (OCG)."
Barbedo J.G.A.; Lopes A.; Wolfe P.J.,High time-resolution estimation of multiple fundamental frequencies,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957728966&partnerID=40&md5=154992ca13a308fc4406a37d51fac8c2,"Barbedo J.G.A., School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138-2901, 33 Oxford Street, United States, School of Electrical and Computer Engineering, State University of Campinas Cidade Universitária Zeferino Vaz, Campinas, SP, C.P. 6101, CEP: 13083-970, Brazil; Lopes A., School of Electrical and Computer Engineering, State University of Campinas Cidade Universitária Zeferino Vaz, Campinas, SP, C.P. 6101, CEP: 13083-970, Brazil; Wolfe P.J., School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138-2901, 33 Oxford Street, United States","This paper presents a high time-resolution strategy to estimate multiple fundamental frequencies in musical signals. The signal is first divided into overlapping blocks, and a high-resolution estimate made of the short-term spectrum. The resulting spectrum is modified such that only the most relevant spectral components are considered, and an iterative algorithm based on earlier work by Klapuri is used to identify candidate fundamental frequencies. Finally, a context-based rule is used to improve the accuracy of fundamental frequency estimates. The performance of this technique is investigated under both noiseless and noisy conditions, and its accuracy is examined in cases where the polyphony is known and unknown a priori. ©2007 Austrian Computer Society (OCG)."
Hu X.; Downie J.S.,"Exploring mood metadata: Relationships with genre, artist and usage metadata",2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596722&partnerID=40&md5=cf8bb2a806ae4857dad1a1da75ef7650,"Hu X., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States","There is a growing interest in developing and then evaluating Music Information Retrieval (MIR) systems that can provide automated access to the mood dimension of music. Mood as a music access feature, however, is not well understood in that the terms used to describe it are not standardized and their application can be highly idiosyncratic. To better understand how we might develop methods for comprehensively developing and formally evaluating useful automated mood access techniques, we explore the relationships that mood has with genre, artist and usage metadata. Statistical analyses of term interactions across three metadata collections (AllMusicGuide.com, epinions.com and Last.fm) reveal important consistencies within the genre-mood and artist-mood relationships. These consistencies lead us to recommend a cluster-based approach that overcomes specific term-related problems by creating a relatively small set of data-derived ""mood spaces"" that could form the ground-truth for a proposed MIREX ""Automated Mood Classification"" task. ©2007 Austrian Computer Society (OCG)."
Lebosse J.; Brun L.,Audio fingerprint identification by approximate string matching,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593056&partnerID=40&md5=d588599cc72fb54e54f23c1a58bece84,"Lebosse J., France Telecom RandD, 14000 Caen, 42 rue des coutures, France; Brun L., GREYC UMR 6072, 14050 Caen, 6 boulevard du Marchal Juin, France",An audio fingerprint is a small digest of an audio file which allows to identify it among a database of candidates. This paper first presents a fingerprint extraction algorithm. The identification task is performed by a new identification scheme which combines string matching algorithms and q-grams filtration. ©2007 Austrian Computer Society (OCG).
Romming C.A.; Selfridge-Field E.,Algorithms for polyphonic music retrieval: The hausdorff metric and geometric hashing,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587765&partnerID=40&md5=93fe216502faf417e6c08cbe35616e60,"Romming C.A., Dept. of Computer Science, Stanford University, United States; Selfridge-Field E., Dept. of Music, Stanford University, United States","We consider two formulations of the computational problem of transposition-invariant, time-offset tolerant, meterinvariant, and time-scale invariant polyphonic music retrieval. We provide algorithms for both that are scalable in the sense that space requirements are asymptotically linear and queries are efficient for large databases of music. The focus is on cases where a query patternM consisting of m events is to be matched against a database N consisting of n events, and m ≪ n. The database is assumed to be polyphonic, and the algorithms support polyphonic queries. We are interested in finding exact and proximate occurrences of the query pattern. The first problem considered is that of finding the minimum directed Hausdorff distance from M to N. We give a (2 + ε)-approximation algorithm that solves this problem in O (nm) query time and O (n) space. The second problem is that of finding all maximal subset matches of M in N, and we give an algorithm that solves this problem in O (m3 (k + 1)) query time and O (w2n) space, where w represents the maximum window size and k is the number of matches. Using the same method, the problem can be solved in O(m(k + 1)) query time and O(wn) space if we do not require the time-scale invariance property. The latter query time is asymptotically optimal for the given problem. ©2007 Austrian Computer Society (OCG)."
Paulus J.; Klapuri A.,Music structure analysis using a probabilistic fitness measure and an integrated musicological model,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149101230&partnerID=40&md5=1e6eed10ff330505ac0da729124c8e18,"Paulus J., Institute of Signal Processing, Tampere University of Technology, Tampere, Finland; Klapuri A., Institute of Signal Processing, Tampere University of Technology, Tampere, Finland","This paper presents a system for recovering the sectional form of a musical piece: segmentation and labelling of musical parts such as chorus or verse. The system uses three types of acoustic features: mel-frequency cepstral coefficients, chroma, and rhythmogram. An analysed piece is first subdivided into a large amount of potential segments. The distance between each two segments is then calculated and the value is transformed to a probability that the two segments are occurrences of a same musical part. Different features are combined in the probability space and are used to define a fitness measure for a candidate structure description. Musicological knowledge of the temporal dependencies between the parts is integrated into the fitness measure. A novel search algorithm is presented for finding the description that maximises the fitness measure. The system is evaluated with a data set of 557 manually annotated popular music pieces. The results suggest that integrating the musicological model to the fitness measure leads to a more reliable labelling of the parts than performing the labelling as a post-processing step."
Raimond Y.; Abdallah S.; Sandler M.; Giasson F.,The music ontology,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596917&partnerID=40&md5=e4f05178a2fc7a53d37d4711d8a3e8f0,"Raimond Y., Centre for Digital Music, University of London Queen Mary, United Kingdom; Abdallah S., Centre for Digital Music, University of London Queen Mary, United Kingdom; Sandler M., Centre for Digital Music, University of London Queen Mary, United Kingdom; Giasson F., Zitgist LLC, United States","In this paper, we overview some Semantic Web technologies and describe the Music Ontology: a formal framework for dealing with music-related information on the Semantic Web, including editorial, cultural and acoustic information. We detail how this ontology can act as a grounding for more domain-specific knowledge representation. In addition, we describe current projects involving the Music Ontology and interlinked repositories of musicrelated knowledge. ©2007 Austrian Computer Society (OCG)."
Betser M.; Collen P.; Rault J.-B.,Audio identification using sinusoidal modeling and application to jingle detection,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606846&partnerID=40&md5=2a869060681d2fd62739a54922004752,"Betser M., France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France; Collen P., France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France; Rault J.-B., France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France","This article presents a new descriptor dedicated to Audio Identification (audioID), based on sinusoidal modeling. The core idea is an appropriate selection of the sinusoidal components of the signal to be detected. This new descriptor is robust against usual distortions found in audioID tasks. It has several advantages compared to classical subband-based descriptors including an increased robustness to additive noise, especially non-random noise such as additional speech, and a robust detection of short audio events. This descriptor is compared to a classical subband-based feature for a jingle detection task on broadcast radio. It is shown that the new introduced descriptor greatly improves the performance in terms of recall/precision. ©2007 Austrian Computer Society (OCG)."
Wright M.; Schloss W.A.; Tzanetakis G.,Analyzing Afro-Cuban rhythm using rotation-aware clave template matching with dynamic programming,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949113360&partnerID=40&md5=9ba58e7bca15c82f9e8e4ff8a800fd58,"Wright M., University of Victoria, Computer Science and Music Departments, Canada; Schloss W.A., University of Victoria, Computer Science and Music Departments, Canada; Tzanetakis G., University of Victoria, Computer Science and Music Departments, Canada","The majority of existing research in Music Information Retrieval (MIR) has focused on either popular or classical music and frequently makes assumptions that do not generalize to other music cultures. We use the term Computational Eth-nomusicology (CE) to describe the use of computer tools to assist the analysis and understanding of musics from around the world. Although existing MIR techniques can serve as a good starting point for CE, the design of effective tools can benefit from incorporating domain-specific knowledge about the musical style and culture of interest. In this paper we describe our realization of this approach in the context of studying Afro-Cuban rhythm. More specifically we show how computer analysis can help us characterize and appreciate the complexities of tracking tempo and analyzing micro-timing in these particular music styles. A novel template-based method for tempo tracking in rhythmically complex Afro-Cuban music is proposed. Although our approach is domain-specific, we believe that the concepts and ideas used could also be used for studying other music cultures after some adaptation."
Kasimi A.A.; Nichols E.; Raphael C.,A simple algorithm f generation of polyphonic piano fingerings,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863252675&partnerID=40&md5=44ef7c3996c9a523698352562089bad8,"Kasimi A.A., Dept. of Computer Science, Indiana Univ., United States; Nichols E., Dept. of Computer Science, Indiana Univ., United States; Raphael C., School of Informatics, Indiana Univ., United States","We present a novel method for assigning fingers to notes in a polyphonic piano score. Such a mapping (called a ""fingering"") is of great use to performers. To accommodate performers' unique hand shapes and sizes, our method relies on a simple, user-adjustable cost function. We use dynamic programming to search the space of all possible fingerings for the optimal fingering under this cost function. Despite the simplicity of the algorithm we achieve reasonable and useful results. ©2007 Austrian Computer Society (OCG)."
Volk A.; Garbers J.; Van Kranenburg P.; Franswiering; Veltkamp R.C.; Grijp L.P.,Applying rhythmic similarity based on inner metric analysis to folksong research,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573999&partnerID=40&md5=06af5bc947f390785e4c7b57a31aad27,"Volk A., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Garbers J., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Franswiering, Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Grijp L.P., Meertens Institute, Amsterdam, Netherlands","In this paper we investigate the role of rhythmic similarity as part of melodic similarity in the context of Folksong research. We define a rhythmic similarity measure based on Inner Metric Analysis and apply it to groups of similar melodies. The comparison with a similarity measure of the SIMILE software shows that the two models agree on the number of melodies that are considered very similar, but disagree on the less similar melodies. In general, we achieve good results with the retrieval of melodies using rhythmic information, which demonstrates that rhythmic similarity is an important factor to consider in melodic similarity. ©2007 Austrian Computer Society (OCG)."
Lidy T.; Rauber A.; Pertusa A.; Iñesta J.M.,Improving genre classification by combination of audio and symbolic descriptors using a transcription system,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596118&partnerID=40&md5=4610477846f494257dc309f5cae9d0d8,"Lidy T., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Pertusa A., Departamento de Lenguajes Y Sistemas Inforḿaticos, University of Alicante, Spain; Iñesta J.M., Departamento de Lenguajes Y Sistemas Inforḿaticos, University of Alicante, Spain","Recent research in music genre classification hints at a glass ceiling being reached using timbral audio features. To overcome this, the combination of multiple different feature sets bearing diverse characteristics is needed. We propose a new approach to extend the scope of the features: We transcribe audio data into a symbolic form using a transcription system, extract symbolic descriptors from that representation and combine them with audio features. With this method, we are able to surpass the glass ceiling and to further improve music genre classification, as shown in the experiments through three reference music databases and comparison to previously published performance results. ©2007 Austrian Computer Society (OCG)."
Kuuskankare M.; Laurson M.,VIVO - Visualizing harmonic progressions and voice-leading in PWGL,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579210&partnerID=40&md5=02874aed12520ca9d69295a8a9771bcd,"Kuuskankare M., CMT, Sibelius Academy, Finland; Laurson M., CMT, Sibelius Academy, Finland","This paper describes a novel tool called VIVO (VIsual VOice-leading) that allows to visually define harmonic progressions and voice-leading rules. VIVO comprises of a compiler and a collection of specialized visualization devices. VIVO takes advantage of several music related applications collected under the umbrella of PWGL (PWGL is a free cross-platform visual programming language for music and sound related applications). Our music notation application-Expressive Notation Package or ENP-is used here to build the user-interface used to visually define harmony and voice-leading rules. These visualizations are converted to textual rules by the VIVO compiler. Finally, our rule-based compositional system, PWGLConstraints, is used generate the final musical output using these rules. ©2007 Austrian Computer Society (OCG)."
Li B.; Leon S.D.; Fujinaga I.,Alternative digitization approach for stereo phonograph records using optical audio reconstruction,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600618&partnerID=40&md5=45ca36106864739e736ca89f0173f30e,"Li B., Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada; Leon S.D., Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada; Fujinaga I., Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada","This paper presents the first Optical Audio Reconstruction (OAR) approach for the long-term digital preservation of stereo phonograph records. OAR uses precision metrology and digital image processing to obtain and convert groove contour data into digital audio for access and preservation. This contactless and imaging-based approach has considerable advantages over the traditional mechanical methods, such as being the only optical method with the potential to restore broken stereo records. Although past efforts on monophonic phonograph records have been successful, no attempts on 33rpm long-playing stereo records (LPs) have been reported. By using a white-light interferometry optical profiler, we are able to extract stereo audio information encoded in the 3D profile of the phonograph record grooves. ©2007 Austrian Computer Society (OCG)."
Bergeron M.; Conklin D.,Structured polyphonic patterns,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649914202&partnerID=40&md5=f186b41d232fc0887df5e39487d379a2,"Bergeron M., Department of Computing, City University London, United Kingdom; Conklin D., Department of Computing, City University London, United Kingdom","This paper presents a new approach to polyphonic music retrieval, based on a structured pattern representation. Polyphonic patterns are formed by joining and layering pattern components into sequences and simultaneities. Pattern components are conjunctions of features which encode event properties or relations with other events. Relations between events that overlap in time but are not simultaneous are supported, enabling patterns to express many of the temporal relations encountered in polyphonic music. The approach also provides a mechanism for defining new features. It is illustrated and evaluated by querying for three musicological patterns in a corpus of 185 chorale harmonizations by J.S. Bach."
Burgoyne J.A.; Pugin L.; Eustace G.; Fujinaga I.,A comparative survey of image binarisation algorithms for optical recognition on degraded musical sources,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549252338&partnerID=40&md5=3b7b92dc5cab0669cdd90ea65cc66de4,"Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Eustace G., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Binarisation of greyscale images is a critical step in optical music recognition (OMR) preprocessing. Binarising music documents is particularly challenging because of the nature of music notation, even more so when the sources are degraded, e.g., with ink bleed-through from the other side of the page. This paper presents a comparative evaluation of 25 binarisation algorithms tested on a set of 100 music pages. A real-world OMR infrastructure for early music (Aruspix) was used to perform an objective, goaldirected evaluation of the algorithms' performance. Our results differ significantly from the ones obtained in studies on non-music documents, which highlights the importance of developing tools specific to our community. ©2007 Austrian Computer Society (OCG)."
Chordia P.; Godfrey M.; Rae A.,Extending content-based recommendation: The case of Indian classical music,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949096247&partnerID=40&md5=2832ca21ab6ec2c0d7196da0b3e62954,"Chordia P., Georgia Tech., United States; Godfrey M., Georgia Tech., United States; Rae A., Georgia Tech., United States","We describe a series of experiments that attempt to create a content-based similarity model suitable for making recommendations about North Indian classical music (NICM). We introduce a dataset (nicm2008) consisting of 897 tracks of NICM along with substantial ground-truth annotations, including artist, predominant instrument, tonic pitch, raag, and parent scale (thaat). Using a timbre-based similarity model derived from short-time MFCCs we find that artist R-precision is 32.69% and that the predominant instrument is correctly classified 90.30% of the time. Consistent with previous work, we find that certain tracks (""hubs"") appear falsely similar to many other tracks. We find that this problem can be attenuated by model homogenization. We also introduce the use of pitch-class distribution (PCD) features to measure melodic similarity. Its effectiveness is evaluated by raag R-precision (16.97%), thaat classification accuracy (75.83%), and comparison to reference similarity metrics. We propose that a hybrid timbral-melodic similarity model may be effective for Indian classical music recommendation. Further, this work suggests that ""hubs"" are a general features of such similarity modeling that may be partially alleviated by model homogenization."
Thul E.; Toussaint G.T.,Rhythm complexity measures: A comparison of mathematical models of human perception and performance,2008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149155605&partnerID=40&md5=51861b5cd9b11053250dd102f4288750,"Thul E., School of Computer Science, Schulich School of Music, McGill University, Montréal, Canada; Toussaint G.T., School of Computer Science, Schulich School of Music, McGill University, Montréal, Canada","Thirty two measures of rhythm complexity are compared using three widely different rhythm data sets. Twenty-two of these measures have been investigated in a limited context in the past, and ten new measures are explored here. Some of these measures are mathematically inspired, some were designed to measure syncopation, some were intended to predict various measures of human performance, some are based on constructs from music theory, such as Pressing's cognitive complexity, and others are direct measures of different aspects of human performance, such as perceptual complexity, meter complexity, and performance complexity. In each data set the rhythms are ranked either according to increasing complexity using the judgements of human subjects, or using calculations with the computational models. Spearman rank correlation coefficients are computed between all pairs of rhythm rankings. Then phylogenetic trees are used to visualize and cluster the correlation coefficients. Among the many conclusions evident from the results, there are several observations common to all three data sets that are worthy of note. The syncopation measures form a tight cluster far from other clusters. The human performance measures fall in the same cluster as the syncopation measures. The complexity measures based on statistical properties of the inter-onset-interval histograms are poor predictors of syncopation or human performance complexity. Finally, this research suggests several open problems."
Paulus J.; Klapuri A.,Combining temporal and spectral features in HMM-Based Drum transcription,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863405897&partnerID=40&md5=ea9c086858ac3110adfce37e9407ec34,"Paulus J., Institute of Signal Processing Tampere, University of Technology, Finland; Klapuri A., Institute of Signal Processing Tampere, University of Technology, Finland","To date several methods for transcribing drums from polyphonic music have been published. Majority of the features used in the transcription systems are ""spectral"": parameterising some property of the signal spectrum in a relatively short time frames. It has been shown that utilising narrow-band features describing long-term temporal evolution in conjunction with the more traditional features can improve the overall performance in speech recognition. We investigate similar utilisation of temporal features in addition to the HMM baseline. The effect of the proposed extension is evaluated with simulations on acoustic data, and the results suggest that temporal features do improve the result slightly. Demonstrational signals of the transcription results are available at http://www.cs.tut.fi/sgn/arg/paulus/demo/. ©2007 Austrian Computer Society (OCG)."
Gruhne M.; Schmidt K.; Dittmar C.,Phoneme recognition in popular music,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581013&partnerID=40&md5=806ab14d0c5b0608626a087a2c2c1a34,"Gruhne M., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; Schmidt K., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; Dittmar C., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany","Automatic lyrics synchronization for karaoke applications is a major challenge in the field of music information retrieval. An important pre-requisite in order to precisely synchronize the music and corresponding text is the detection of single phonemes in the vocal part of polyphonic music. This paper describes a system, which detects the phonemes based on a state-of-the-art audio information retrieval system with harmonics extraction and synthesizing as pre-processing method. The extraction algorithm is based on common speech recognition low-level features, such as MFCC and LPC. In order to distinguish phonemes, three different classification techniques (SVM, GMM and MLP) have been used and their results are depicted in the paper. ©2007 Austrian Computer Society (OCG)."
De Léon P.J.P.; Rizo D.; Ñesta J.M.I.,Towards a human-friendly melody characterization by automatically induced rules,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602996&partnerID=40&md5=93d0ffde866aa99e1f8fa6b383116862,"De Léon P.J.P., Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain; Rizo D., Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain; Ñesta J.M.I., Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain","There is an increasing interest in music information retrieval for reference, motive, or thumbnail extraction from a piece in order to have a compact and representative representation of the information to be retrieved. One of the main references for music is its melody. In a practical environment of symbolic format collections the information can be found in standard MIDI file format, structured as a number of tracks, usually one of them containing the melodic line, while the others contain the accompaniment. The goal of this work is to analyse how statistical rules can be used to characterize a melody in such a way that one can understand the solution of an automatic system for selecting the track containing the melody in such files. ©2007 Austrian Computer Society (OCG)."
Proutskova P.,Musical memory of the world - Data infrastructure in ethnomusicological archives,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873589894&partnerID=40&md5=98640079fa3b8c8cd0261e1c9805112b,"Proutskova P., Computing department, Goldsmiths, University of London, United Kingdom","Ethnomusicological archives build the musical memory of the world, covering the geographical and the historical aspects of music worldwide. This article gives a brief description of the nature and the functionality of ethnomusicological archives. It reflects the current state of data infrastructure (policy and technology), addressing issues of access to archives' holdings, of online visibility of music collections and of interoperability between archives. An outlook of a mutual involvement and a resulting influence at each others work between MIR community and ethnomusicological archives is given. ©2007 Austrian Computer Society (OCG)."
Schedl M.; Gerhardwidmer; Pohle T.; Seyerlehner K.,Web-Based detection of music band members and line-up,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606799&partnerID=40&md5=139ab51a322bd4d3b440f67396318185,"Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gerhardwidmer, Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We present first steps towards the automatic detection of music band members and instrumentation using web content mining techniques. To this end, we combine a named entity detection method with rule-based linguistic text analysis. We report on preliminary evaluation results and discuss limitations of the current method. ©2007 Austrian Computer Society (OCG)."
Pikrakis A.; Theodoridis S.,An application of empirical mode decomposition on tempo induction from music recordings,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856462785&partnerID=40&md5=094c8a48557ab69f39d2f571beea7997,"Pikrakis A., Dept. of Informatics and Telecommunications, University of Athens, Greece; Theodoridis S., Dept. of Informatics and Telecommunications, University of Athens, Greece","This paper presents an application of Empirical Mode Decomposition (EMD) on the induction of notated tempo from music recordings. At a first stage, EMD is employed as a means to segment music recordings into segments that exhibit similar rhythmic characteristics. At a second stage, EMD is used in order to analyze the diagonals of the Self-Similarity Matrix of each segment, so as to estimate the tempo of the recording. The proposed method has been employed on various music genres with music meters of 2/4 , 3/4 and 4/4 . Tempo has been assumed to remain approximately constant throughout each recording, ranging from 60bpm up to 220bpm. ©2007 Austrian Computer Society (OCG)."
Cont A.; Schwarz D.; Schnell N.; Raphael C.,Evaluation of real-time audio-to-score alignment,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578650&partnerID=40&md5=93c4a0507c2b864031df6d513bf7299f,"Cont A., Ircam UMR CNRS 9912, CRCA, UCSD, France; Schwarz D., Ircam-Centre Pompidou Paris, UMR CNRS 9912, France; Schnell N., Ircam-Centre Pompidou Paris, UMR CNRS 9912, France; Raphael C., Indiana University, Bloomington, IN, United States","This article explains evaluation methods for real-time audio to score alignment, or score following, that allow for the quantitative assessment of the robustness and preciseness of an algorithm. The published ground truth data base and the evaluation framework, including file formats for the score and the reference alignments, are presented. The work, started forMIREX 2006, is meant as a first step towards a standardized evaluation process contributing to the exchange and progress in this field. ©2007 Austrian Computer Society (OCG)."
Dressler K.; Streich S.,Tuning frequency estimation using circular statistics,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955836763&partnerID=40&md5=83cd79c4771cfe794950c44897cd0355,"Dressler K., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; Streich S., Music Technology Group, Pompeu Fabra University, Barcelona, Spain","In this document a new approach on tuning frequency estimation based on circular statistics is presented. Two methods are introduced: the calculation of the tuning frequency over an entire audio piece, and the estimation of an adapting reference frequency for a single voice. The results for the tuning frequency estimation look very good for audio pieces where the dominant voices are tuned close to the equal-temperament scale and exhibit only moderate frequency dynamics. For the analysis of popular western music, the method does not achieve very robust results due to the strong frequency dynamics of the human singing voice. Nevertheless, the method could be improved by excluding the singing voice from the calculation taking only the accompaniment into account. The main advantage of the proposed method lies especially in the easy computation of an adaptive reference frequency using an exponential moving average. This adaptive reference can for example be used in the quantization of the singing voice into a note representation. ©2007 Austrian Computer Society (OCG)."
Klapuri A.,Multiple fundamental frequency estimation by summing harmonic amplitudes,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444806&partnerID=40&md5=1932db44a18aba8fdce7e44ff4bd0156,"Klapuri A., Institute of Signal Processing, Tampere University of Technology, 33720 Tampere, Korkeakoulunkatu 1, Finland","This paper proposes a conceptually simple and computationally efficient fundamental frequency (F0) estimator for polyphonic music signals. The studied class of estimators calculate the salience, or strength, of a F0 candidate as a weighted sum of the amplitudes of its harmonic partials. A mapping from the Fourier spectrum to a ""F0 salience spectrum"" is found by optimization using generated training material. Based on the resulting function, three different estimators are proposed: a ""direct"" method, an iterative estimation and cancellation method, and a method that estimates multiple F0s jointly. The latter two performed as well as a considerably more complex reference method. The number of concurrent sounds is estimated along with their F0s. © 2006 University of Victoria."
Noland K.; Sandler M.,Key estimation using a Hidden Markov Model,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873438765&partnerID=40&md5=aae51db651b332aa73d878b11fd56c4f,"Noland K., Centre for Digital Music, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom","A novel technique to estimate the predominant key in a musical excerpt is proposed. The key space is modelled by a 24-state Hidden Markov Model (HMM), where each state represents one of the 24 major and minor keys, and each observation represents a chord transition, or pair of consecutive chords. The use of chord transitions as the observations models a greater temporal dependency between consecutive chords than would observations of single chords. The key transition and chord emission probabilities are initialised using the results of perceptual tests in order to reflect the human expectation of harmonic relationships. HMM parameters are then trained on a per-song basis using handannotated chord symbols, before the model for each song is decoded to give the likelihood of each key at each time frame. Examples of the algorithm as a segmentation technique are given, and its capability to estimate the overall key of a song is evaluated using a data set of 110 Beatles songs, of which 91% were correctly classified. An extension to include operation from audio data instead of chord symbols is planned, which will enable application to general music retrieval purposes. © 2006 University of Victoria."
Gillet O.; Richard G.,ENST-Drums: An extensive audio-visual database for drum signals processing,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439973&partnerID=40&md5=ab18c35af260aa6e1b6c94bd0329785e,"Gillet O., GET / ENST, CNRS LTCI, 75014 Paris, 37 Rue Dareau, France; Richard G., GET / ENST, CNRS LTCI, 75014 Paris, 37 Rue Dareau, France","One of the main bottlenecks in the progress of the Music Information Retrieval (MIR) research field is the limited access to common, large and annotated audio databases that could serve for technology development and/or evaluation. The aim of this paper is to present in detail the ENST-Drums database, emphasizing on both the content and the recording process. This audiovisual database of drum performances by three professional drummers was recorded on 8 audio channels and 2 video channels. The drum sequences are fully annotated and will be, for a large part, freely distributed for research purposes. The large variety in its content should serve research in various domains of audio signal processing involving drums, ranging from single drum event classification to complex multimodal drum track transcription and extraction from polyphonic music. © 2006 University of Victoria."
Eichner M.; Wolff M.; Hoffmann R.,Instrument classification using Hidden Markov Models,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873430088&partnerID=40&md5=5221c68619da4ca05648fb25db233a24,"Eichner M., Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany; Wolff M., Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany; Hoffmann R., Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany","In this paper we present first results on musical instrument classification using an HMM based recognizer. The final goal of our work is to automatically evaluate instruments and to classify them according to their characteristics. The first step in this direction was to train a system that is able to recognize a particular instrument among others of the same kind (e.g. guitars). The recognition is based on solo music pieces played on the instrument under various conditions. For this purpose a database was designed and is currently being recorded that comprises four instrument types: classical guitar, violin, trumpet and clarinet. We briefly describe the classifier and give first experimental results on the classification of acoustic guitars. © 2006 University of Victoria."
Decoro C.; Barutcuoglu Z.; Fiebrink R.,Bayesian aggregation for hierarchical genre classification,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74849106455&partnerID=40&md5=f25c902fc9a8ba1b3cffc5b652777856,"Decoro C., Department of Computer Science, Princeton University, United States; Barutcuoglu Z., Department of Computer Science, Princeton University, United States; Fiebrink R., Department of Computer Science, Princeton University, United States","Hierarchical taxonomies of classes arise in the analysis of many types of musical information, including genre, as a means of organizing overlapping categories at varying levels of generality. However, incorporating hierarchical structure into conventional machine learning systems presents a challenge: the use of independent binary classifiers for each class in the hierarchy can produce hierarchically inconsistent predictions. That is, an example may be assigned to a class, and not assigned to the parent of that class. This paper applies a Bayesian framework to combine, or aggregate, a hierarchy of multiple binary classifiers in a principled manner, and consequently improves performance over the hierarchy as a whole. Furthermore, such an approach allows for an arbitrarily complex hierarchy, and does not suffer from classes that are too broad or too refined. Experiments on theMIREX 2005 symbolic genre classification dataset show that our Bayesian Aggregation algorithm provides significant improvement over independent classifiers, and demonstrates superior performance compared to previous work. Our method also improves similarity search by ranking songs by similarity of hierarchical predictions to those of a query song. ©2007 Austrian Computer Society (OCG)."
Pampalk E.; Goto M.,Musicsun: A new approach to artist recommendation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954634752&partnerID=40&md5=807c8f003d43852866b2acbf761978f6,"Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST), IT AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","MusicSun is a graphical user interface to discover artists. Artists are recommended based on one or more artists selected by the user. The recommendations are computed by combining 3 different aspects of similarity. The users can change the impact of each of these aspects. In addition words are displayed which describe the artists selected by the user. The user can select one of these words to focus the search on a specific direction. In this paper we present the techniques used to compute the recommendations and the graphical user interface. Furthermore, we present the results of an evaluation with 33 users. We asked them, for example, to judge the usefulness of the different interface components and the quality of the recommendations. ©2007 Austrian Computer Society (OCG)."
Leue I.; Izmirli O.,Tempo tracking with a periodicity comb kernel,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459191&partnerID=40&md5=946f5795737e4137291d8e46fe72b613,"Leue I., Center for Arts and Technology, Connecticut College, New London, CT, 270 Mohegan Ave., United States; Izmirli O., Center for Arts and Technology, Connecticut College, New London, CT, 270 Mohegan Ave., United States","Automatic tempo extraction and beat tracking from audio is an important ability, with many applications in music information retrieval. This paper describes a method for tempo tracking which builds on current research in the field. In this algorithm, an autocorrelation surface is calculated from the output of a spectral energy flux onset novelty function. The most salient repetition rate is calculated by cross-correlating dilations of a comb-like prototype spanning multiple frames and the autocorrelation surface. The method addresses tempo tracking through time to account for pieces with variable tempos. In order to compare the performance of our method on music with strong and weak percussive onsets we have evaluated it on both classical music with and without percussion and popular music with percussion. Additionally, beats are phase-aligned and superimposed on the signal for aural evaluation. Results show the comb kernel to be a useful feature in determining the correct beat level. © 2006 University of Victoria."
Garbers J.,An integrated MIR programming and testing environment,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420675&partnerID=40&md5=a87ac8c64acaedf69a9fb14bde17b068,"Garbers J., Department of Computer and Information Sciences, Utrecht University, Netherlands","The process of shaping a music information retrieval algorithm is highly connected with implementing it and testing suitable parameterizations. Often music information retrieval scientists do not have a programmer at hand and must implement their experimental setup themselves. This paper describes an integrated tool setup OHR consisting of the music (analysis) systems OpenMusic, Humdrum and Rubato and a system for form based parametrization and comparison of algorithms. These packages and their programming environments provide the scientist with frameworks and existing libraries for implementing and testing algorithms. They differ in the programming languages that they support and in the type of testing user interfaces that they allow the scientist to build easily. The systems and their components are integrated by using their scripting languages. We sketch an example of the integrated use of these systems. © 2006 University of Victoria."
Turnbull D.; Barrington L.; Lanckriet G.,Modeling music and words using a multi-class naïve Bayes approach,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439183&partnerID=40&md5=2ff68a3a8d9628349512397c936e99ac,"Turnbull D., UC San Diego, San Diego, CA 92093, United States; Barrington L., UC San Diego, San Diego, CA 92093, United States; Lanckriet G., UC San Diego, San Diego, CA 92093, United States",We propose a query-by-text system for modeling a heterogeneous data set of music and words. We quantitatively show that our system can both annotate a novel song with semantically meaningful words and retrieve relevant unla-beled songs from a database given a text-based query. We explain two feature extraction methods useful for summarizing the audio content of a song. We describe a supervised multi-class naïve Bayes model and compare two parameter estimation techniques. Our approach is influenced by recent computer vision research on the related tasks of image annotation and retrieval. © 2006 University of Victoria.
Skowronek J.; McKinney M.F.; Van De Par S.,Ground truth for automatic music mood classification,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476490&partnerID=40&md5=5098afabc72f46be93f82bcabce86ba5,"Skowronek J., Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands; McKinney M.F., Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands; Van De Par S., Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands","Automatic music classification based on audio signals provides a core technology for tools that help users to manage and browse their music collections. Since ""mood"" is also used as a browsing criterium, automatic mood classification could support the creation of the necessary metadata. We have developed a method to obtain a reliable ""ground truth"" database for automatic music mood classification. Our results confirm that excerpt selection is a non-trivial issue and that there are some mood labels that are relatively consistent across subjects. © 2006 University of Victoria."
Hoffman M.; Cook P.R.,"Feature-based synthesis: A tool for evaluating, designing, and interacting with music IR systems",2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448602&partnerID=40&md5=35975dbb56f530977891e9b80bf0eb97,"Hoffman M., Princeton University, Computer Science Department, Princeton, NJ 08544, 35 Olden Street, United States; Cook P.R., Princeton University, Computer Science and Music Departments, Princeton, NJ 08544, 35 Olden Street, United States","We present a general framework for performing feature-based synthesis - that is, for producing audio characterized by arbitrarily specified sets of perceptually motivated, quantifiable acoustic features of the sort used in many music information retrieval systems. © 2006 University of Victoria."
Selfridge-Field E.,Social cognition and melodic persistence: Where metadata and content diverge,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873480101&partnerID=40&md5=4b973dfc835f0f287182747fa8418c34,"Selfridge-Field E., CCARH, Braun Music Center 129, Stanford University, Stanford, CA 94305-3076, United States","The automatic retrieval of members of a tune family from a database of melodies is potentially complicated by well documented divergences between textual metadata and musical content. We examine recently reported cases of such divergences in search of musical features which persist even when titles change or the melodies themselves vary. We find that apart from meter and mode, the rate of preservation of searchable musical features is low. Social and gestural factors appear to play a varying role in establishing the ""melodic"" identity of widely transmitted songs. The rapid growth of social computing bring urgency to better understanding the different ways in which ""same"" or ""similar"" can be defined. © 2006 University of Victoria."
Sordo M.; Laurier C.; Celma Ò.,Annotating music collections: Howcontent-based similarity helps to propagate labels,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049115166&partnerID=40&md5=315f6b84374a0587d373146aa6567f3c,"Sordo M., Music Technology Group, Universitat Pompeu, Fabra, Spain; Laurier C., Music Technology Group, Universitat Pompeu, Fabra, Spain; Celma Ò., Music Technology Group, Universitat Pompeu, Fabra, Spain","In this paper we present a way to annotate music collections by exploiting audio similarity. Similarity is used to propose labels (tags) to yet unlabeled songs, based on the content-based distance between them. The main goal of our work is to ease the process of annotating huge music collections, by using content-based similarity distances as a way to propagate labels among songs. We present two different experiments. The first one propagates labels that are related with the style of the piece, whereas the second experiment deals with mood labels. On the one hand, our approach shows that using a music collection annotated at 40% with styles, the collection can be automatically annotated up to 78% (that is, 40% already annotated and the rest, 38%, only using propagation), with a recall greater than 0.4. On the other hand, for a smaller music collection annotated at 30% with moods, the collection can be automatically annotated up to 65% (e.g. 30% plus 35% using propagation). ©2007 Austrian Computer Society (OCG)."
Flexer A.; Gouyon F.; Dixon S.; Widmer G.,Probabilistic combination of features for music classification,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428796&partnerID=40&md5=cbb0cbafd056991eae3bd49c9f5da67f,"Flexer A., Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, Austria; Gouyon F., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Dixon S., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria",We describe an approach to the combination of music similarity feature spaces in the context of music classification. The approach is based on taking the product of posterior probabilities obtained from separate classifiers for the different feature spaces. This allows for a different influence of the classifiers per song and an overall classification accuracy improving those resulting from individual feature spaces alone. This is demonstrated by combining spectral and rhythmic similarity for classification of ballroom dance music. © 2006 University of Victoria.
Donaldson J.; Knopke I.,Music recommendation mapping and interface based on structural network entropy,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62249195186&partnerID=40&md5=b23b2d81c4bfb5f2d250026fb38ac4ed,"Donaldson J., School of Informatics, Human Computer Interaction/Design, Indiana University, United States; Knopke I., School of Informatics Music Informatics, Indiana University, United States","Recommendation systems generally produce the results of their output to their users in the form of an ordinal list. In the interest of simplicity, these lists are often obscure, abstract, or omit many relevant metrics pertaining to the measured strength of the recommendations or the relationships the recommended items share with each other. This information is often useful for coming to a better understanding of the nature of how the items are structured according to the recommendation data. This paper describes the ZMDS algorithm, a novel way of analyzing the fundamental network structure of recommendation results. Furthermore, it also describes a dynamic plot interaction method as a recommendation browsing utility. A novel ""Recommendation Map"" web application implements both the ZMDS algorithm and the plot interface and are offered as an example of both components working together. ©2007 Austrian Computer Society (OCG)."
Suzuki M.; Hosoya T.; Ito A.; Makino S.,Music information retrieval from a singing voice based on verification of recognized hypotheses,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461837&partnerID=40&md5=663eea786556f68e1ea22290a5c52471,"Suzuki M., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan; Hosoya T., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan; Ito A., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan; Makino S., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan","Several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user's singing voice. All of these systems use only melody information for retrieval, although lyrics information is also useful for retrieval. In this paper, we propose an MIR system that uses both melody and lyrics information in the singing voice. The MIR system verifies hypotheses output by a lyrics recognizer from a melodic point of view. Each hypothesis has time alignment information between the singing voice and recognized text, and the boundaries of each note can be estimated using the information. As a result, melody information is extracted from the singing voice. On the other hand, the melody information can be calculated from the musical score of the song because the recognized text must be a part of the lyrics of the song. The hypothesis is verified by calculating the similarity between the two types of melody information. From the experimental results, the verification method increased the retrieval accuracy. Especially, it was very effective when the number of words in the user's singing voice was small. The proposed method increased the retrieval accuracy from 81.3% to 87.4% when the number of words was only three. © 2006 University of Victoria."
Lai C.; Fujinaga I.,Data dictionary: Metadata for phonograph records,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428314&partnerID=40&md5=5234951b7b3955c49ad6664d45725b20,"Lai C., Schulich School of Music, McGill University, Montreal, QC H3A 1E3, Canada; Fujinaga I., Schulich School of Music, McGill University, Montreal, QC H3A 1E3, Canada","The creation and maintenance of a metadata data dictionary is essential to large-scale digital repositories. It assists the process of data entry, ensures consistency of records, facilitates semantic compatibility and interoperability between systems, and, most importantly, forms the foundation for efficient and effective information retrieval infrastructure. In this paper we explain in detail the necessity of metadata data dictionaries to digitization projects and digital library retrieval services. We also describe the development process of our Data Dictionary for phonograph records. We then present the underlying data model of our Data Dictionary and provide information about the meaning and use of semantic units defined in the Data Dictionary. We stress the usefulness of the generation and maintenance of our Data Dictionary for MIR as it provides a means to ensure accurate, consistent, and comprehensive metadata annotation. For maximum interoperability between systems, digital repositories not only need to agree on the same metadata fields, but also the meanings of the fields. To this end, we believe our Data Dictionary is the cornerstone of optimal retrieval of music information about phonograph records. © 2006 University of Victoria."
Lai C.; Fujinaga I.; Descheneau D.; Frishkopf M.; Riley J.; Hafner J.; Mcmillan B.,Metadata infrastructure for sound recordings,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949120080&partnerID=40&md5=4ee69ed002152268928cc0c48489a4f1,"Lai C., Music Technology, McGill University, Canada; Fujinaga I., Music Technology, McGill University, Canada; Descheneau D., Department of Music, University of Alberta, Canada; Frishkopf M., Department of Music, University of Alberta, Canada; Riley J., Digital Library Program, Indiana University, United States; Hafner J., McGill University Libraries, McGill University, Canada; Mcmillan B., McGill University Libraries, McGill University, Canada","This paper describes the first iteration of a working model for searching heterogeneous distributed metadata repositories for sound recording collections, focusing on techniques used for real-time querying and harmonizing diverse metadata models. The initial model for a metadata infrastructure presented here is the first of its kind for sound recordings. ©2007 Austrian Computer Society (OCG)."
McEnnis D.; McKay C.; Fujinaga I.,Overview of OMEN,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475383&partnerID=40&md5=ae4d3801412396470263cfa2ec814fcd,"McEnnis D., Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada; McKay C., Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada; Fujinaga I., Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada","This paper introduces OMEN (On-demand Metadata Extraction Network), which addresses a fundamental problem in MIR: the lack of universal access to a large dataset containing significant amounts of copyrighted music. This is accomplished by utilizing the large collections of digitized music available at many libraries. Using OMEN, libraries will be able to perform on-demand feature extraction on site, returning feature values to researchers instead of providing direct access to the recordings themselves. This avoids copyright difficulties, since the underlying music never leaves the library that owns it. The analysis is performed using grid-style computation on library machines that are otherwise underused (e.g., devoted to patron web and catalogue use). © 2006 University of Victoria."
Hitchner S.; Murdoch J.; Tzanetakis G.,Music browsing using a tabletop display,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349581274&partnerID=40&md5=ad0eaad605f6f3394c13e7c9521de012,"Hitchner S., Computer Engineering, Univeristy of Victoria, Canada; Murdoch J., Computer Engineering, Univeristy of Victoria, Canada; Tzanetakis G., Computer Engineering, Univeristy of Victoria, Canada","The majority of work in Music Information Retrieval (MIR) follows a search/retrieval paradigm. More recently, the importance of browsing as an interaction paradigm has been realized, and several novel interfaces have been proposed. In this paper, we describe two novel interaction schemes for content-aware browsing of music collections that use a graphical tabletop interface. We further present findings from qualitative user studies. We describe our work in the context of two primary themes: music collection browsing, and collaborative (multiple simultaneous users) interaction and involvement during the browsing/ selection process. ©2007 Austrian Computer Society (OCG)."
McEnnis D.; McKay C.; Fujinaga I.,jAudio: Additions and improvements,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873437546&partnerID=40&md5=f142d1a8f0f1740fcccdc167e89b66ca,"McEnnis D., Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada; McKay C., Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada; Fujinaga I., Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada","jAudio is an application designed to extract features for use in a variety of MIR tasks. It eliminates the need for re-implementing existing feature extraction algorithms and provides a framework that greatly facilitates the development and deployment of new features. Three classes of features are presented and explained-features, metafeatures, and aggregators. A detailed description of jAudio's dependency resolution algorithm is also discussed. Finally, ways in which jAudio can be embedded in and integrated with new systems are discussed, along with a description of jAudio's ability to add new features or aggregators, potentially at runtime. © 2006 University of Victoria."
Kim Y.E.; Williamson D.S.; Pilli S.,"Towards quantifying the ""album effect"" in artist identification",2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476754&partnerID=40&md5=fc1efe579c3c40103aaee6b10fada39d,"Kim Y.E., Drexel University, Electrical and Computer Engineering, United States; Williamson D.S., Drexel University, Electrical and Computer Engineering, United States; Pilli S., Drexel University, Electrical and Computer Engineering, United States","Recent systems for automatically identifying the performing artist from the acoustic signal of music have demonstrated reasonably high accuracy when discriminating between hundreds of known artists. A well-documented issue, however, is that the performance of these systems degrades when music from different albums is used for training and evaluation. Conversely, accuracy improves when systems are trained and evaluated using music from the same album. This performance characteristic has been labeled the ""album effect"". The unfortunate corollary to this result is that the classification results of these systems are based not entirely on the music itself, but on other audio features common to the album that may be unrelated to the underlying music. We hypothesize that one of the primary reasons for this phenomenon is the production process of commercial recordings, specifically, post-production. Understanding the primary aspects of post-production, we can attempt to model its effect on the acoustic features used for classification. By quantifying and accounting for this transformation, we hope to improve future systems for automatic artist identification. © 2006 University of Victoria."
Burgoyne J.A.; Pugin L.; Kereliuk C.; Fujinaga I.,A cross-validated study of modelling strategies for automatic chord recognition in audio,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955824923&partnerID=40&md5=fc4af43b5f9a6ea4f50086b359baacf5,"Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Kereliuk C., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Although automatic chord recognition has generated a number of recent papers in MIR, nobody to date has done a proper cross validation of their recognition results. Cross validation is the most common way to establish baseline standards and make comparisons, e.g., for MIREX competitions, but a lack of labelled aligned training data has rendered it impractical. In this paper, we present a comparison of several modelling strategies for chord recognition, hiddenMarkov models (HMMs) and conditional random fields (CRFs), on a new set of aligned ground truth for the Beatles data set of Sheh and Ellis (2003). Consistent with previous work, our models use pitch class profile (PCP) vectors for audio modelling. Our results show improvement over previous literature, provide precise estimates of the performance of both old and new approaches to the problem, and suggest several avenues for future work. ©2007 Austrian Computer Society (OCG)."
Turnbull D.; Liu R.; Barrington L.; Lanckriet G.,A game-based approach for collecting semantic annotations of music,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049092472&partnerID=40&md5=96cfa26de02b44ade8f3a83c73af6282,"Turnbull D., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Liu R., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Barrington L., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Lanckriet G., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","Games based on human computation are a valuable tool for collecting semantic information about images. We show how to transfer this idea into the music domain in order to collect high-quality semantic information about songs. We present Listen Game, a online, multiplayer game that measures the semantic relationship between music and words. In the normal mode, a player sees a list of semantically related words (e.g., instruments, emotions, usages, genres) and is asked to pick the best and worst word to describe a song. In the freestyle mode, a user is asked to suggest a new word that describes the music. Each player receives realtime feedback about the agreement amongst all players. We show that we can use the data collected during a two-week pilot study of Listen Game to learn a supervised multiclass labeling (SML) model. We show that this SML model can annotate a novel song with meaningful words and retrieve relevant songs from a database of audio content. ©2007 Austrian Computer Society (OCG)."
Lee K.; Slaney M.,Automatic chord recognition from audio using an HMM with supervised learning,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462366&partnerID=40&md5=df86c3f34262a49265182c5ba3dd86cf,"Lee K., Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States; Slaney M., Yahoo Research, Sunnyvale, CA 94089, United States","In this paper, we propose a novel method for obtaining labeled training data to estimate the parameters in a supervised learning model for automatic chord recognition. To this end, we perform harmonic analysis on symbolic data to generate label files. In parallel, we generate audio data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to estimate model parameters. Experimental results show higher performance in frame-level chord recognition than the previous approaches. © 2006 University of Victoria."
Müller M.; Clausen M.,Transposition-invariant self-similarity matrices,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149182149&partnerID=40&md5=c27447834fd01d147e77990997d566c5,"Müller M., Department of Computer Science III, Bonn University, Germany; Clausen M., Department of Computer Science III, Bonn University, Germany","Self-similarity matrices have become an important tool for visualizing the repetitive structure of a music recording. Transforming an audio data stream into a feature sequence, one obtains a self-similarity matrix by pairwise comparing all features of the sequence with respect to a local cost measure. The basic idea is that similar audio segments are revealed as paths of low cost along diagonals in the resulting self-similarity matrix. It is often the case, in particular for classical music, that certain musical parts are repeated in another key. In this paper, we introduce the concept of a transposition- invariant self-similarity matrix, which reveals the repetitive structure even in the presence of key transpositions. Furthermore, we introduce an associated transposition index matrix displaying harmonic relations within the music recording. As an application, we sketch how our concept can be used for the task of audio structure analysis. ©2007 Austrian Computer Society (OCG)."
Li B.; Burgoyne J.A.; Fujinaga I.,Extending Audacity for audio annotation,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444078&partnerID=40&md5=b527b94c313655b1ec49018fce773021,"Li B., Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada; Burgoyne J.A., Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada","By implementing a cached region selection scheme and automatic label completion, we extended an open-source audio editor to become a more convenient audio annotation tool for tasks such as ground-truth annotation for audio and music classification. A usability experiment was conducted with encouraging preliminary results. © 2006 University of Victoria."
Landone C.; Harrop J.; Reiss J.,"Enabling access to sound archives through integration, enrichment and retrieval: The easaier project",2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949103833&partnerID=40&md5=d289c360a30af2b988c0a6104e1356c3,"Landone C., Centre for Digital Music, Queen Mary University of London, London E14NS, Mile End Road, United Kingdom; Harrop J., Royal Scottish Academy of Music and Drama, Glasgow G23DB, 100 Renfrew St, United Kingdom; Reiss J., Centre for Digital Music, Queen Mary University of London, London E14NS, Mile End Road, United Kingdom","Many digital sound archives suffer from problems concerning on-line access: sound materials are often held separately from other related media, they are not easily browsed and little opportunity to search the actual audio content of the material is provided. The EASAIER project aims to alleviate these problems, offering a number of solutions to support sound archive managers and users. EASAIER will enable enhanced access to sound archives, providing multiple methods of retrieval, integration with other media archives, content enrichment and enhanced access tools. ©2007 Austrian Computer Society (OCG)."
Kapur A.; Singer E.,A retrieval approach for human/robotic musical performance,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431117&partnerID=40&md5=043353a42200648e2d7bc965602e33a1,"Kapur A., University of Victoria, Victoria, BC, Canada; Singer E., LEMUR, Brooklyn, NY, United States",This paper describes a MIR-based system for live musical performance between a human and a robot. This project involves combining human computer interface with musical robotics using query/retrieval architecture to create musical rhythmic phrases towards improvisation. © 2006 University of Victoria.
Pampalk E.; Goto M.,MusicRainbow: A new user interface to discover artists using audio-based similarity and web-based labeling,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873470483&partnerID=40&md5=ac0495e3d0607a8ef8b10890aeac1fc7,"Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","In this paper we present MusicRainbow which is a simple interface for discovering artists where colors encode different types of music. MusicRainbow is based on a new audio-based approach to compute artist similarity. This approach scores 15 percentage points higher in a genre classification task than the similarity computed on track level. Using a traveling salesman algorithm, similar artists are mapped near each other on a circular rainbow. Furthermore, we present a new approach of combining this audio-based information with information from the web. In particular, we label the rainbow and summarize the artists with words extracted from web pages related to the artists. We use different vocabularies for different hierarchical levels and heuristics to select the most descriptive labels. We conclude with a discussion of the results. The first impressions are very promising. © 2006 University of Victoria."
Hamanaka M.; Lee S.,Music scope headphones: Natural user interface for selection of music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431596&partnerID=40&md5=8db61f25017c1649b8978f91c6dbc490,"Hamanaka M., Presto, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 305-8568, Mbox 604 1-1-1 Umezono, Japan; Lee S., University of Tsukuba, Tsukuba, Ibaraki, 305-8574, Tennoudai 1-1-1, Japan","This paper describes a novel audio only interface for selecting music which enables us to select songs without having to click a mouse. Using previous music players with normal headphones, we can hear only one song at a time and we thus have to play pieces individually to select the one we want to hear from numerous new music files, which involves a large number of mouse operations. The main advantage of our headphones is that they detect natural movements, such as the head or hand moving when users are listening to music and they can focus on a particular musical source that they want to hear. By moving their head left or right, listeners can hear the source from a frontal position as the digital compass detects the change in the direction they are facing. By looking up or down, the tilt sensor will detect the change in the face's angle of elevation; they can better hear the source that is allocated to a more distant or closer position. By putting their hand behind their ear, listeners can adjust the focus sensor on the headphones to focus on a particular musical source that they want to hear. © 2006 University of Victoria."
Leitich S.; Topf M.,Globe of music - Music library visualization using geosom,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349574669&partnerID=40&md5=94fc79ea1e268b43c86226ff0ac8e4f4,"Leitich S., Department of Distributed and Multimedia Systems, University of Vienna, 1010 Vienna, AT, Liebiggasse 4/3-4, Austria; Topf M., Department of Distributed and Multimedia Systems, University of Vienna, 1010 Vienna, AT, Liebiggasse 4/3-4, Austria","Music collections are commonly represented as plain textual lists of artist, title, album etc. for each contained music track. The large volume of personal music libraries makes them difficult to browse and access for users. In respect to possible information visualization techniques, no established convenient user interfaces exist. By using a spherical self-organizing map algorithm on low level audio features and processing the resulting map data, a Geographic Information System is used to visualize a music collection. This results in an aspiring music library visualization, which can be handled intuitively by the user and even provides new possibilities for accessing a music collection in the digital domain. ©2007 Austrian Computer Society (OCG)."
Gomez E.; Herrera P.,The song remains the same: Identifying versions of the same piece using tonal descriptors,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472146&partnerID=40&md5=92f34c1ada84eb98242fcc69d7e394b5,"Gomez E., Music Technology Group, Universitat Pompeu Fabra, 08003, Barcelona, Ocata, 1, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, 08003, Barcelona, Ocata, 1, Spain","Identifying versions of the same song by means of automatically extracted audio features is a complex task for a music information retrieval system, even though it may seem very simple for a human listener. The design of a system to perform this task gives the opportunity to analyze which features are relevant for music similarity. This paper focuses on the analysis of tonal similarity and its application to the identification of different versions of the same piece. This work formulates the situations where a song is ver-sioned and several musical aspects are transformed with respect to the canonical version. A quantitative evaluation is made using tonal descriptors, including chroma representations and tonality. A simple similarity measure, based on Dynamic Time Warping over transposed chroma features, yields around 55% accuracy, which exceeds by far the expected random baseline rate. © 2006 University of Victoria."
Reed J.; Lee C.-H.,A study on music genre classification based on universal acoustic models,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444148&partnerID=40&md5=8314720916bbd01fe3e44363b00a9a62,"Reed J., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States; Lee C.-H., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States","Classification of musical genres gives a useful measure of similarity and is often the most useful descriptor of a musical piece. Previous techniques to use hidden Markov models (HMMs) for automatic genre classification have used a single HMM to model an entire song or genre. This paper provides a framework to give finer segmentation of HMMs through acoustic segment modeling. Modeling each of these acoustic segments with an HMM builds a timbral dictionary in the same fashion that one would create a phonetic dictionary for speech. A symbolic transcription is created by finding the most likely sequence of symbols. These transcriptions then serve as inputs into an efficient text classifier utilized to provide a solution to the genre classification problem. This paper demonstrates that language-ignorant approaches provide results that are consistent with the current state-of-the-art for the genre classification problem. However, the finer segmentation potentially allows for ""musical language""-based syntactic rules to enhance performance. © 2006 University of Victoria."
Clifford R.; Christodoulakis M.; Crawford T.; Meredith D.; Wiggins G.,"A fast, randomised, maximal subset matching algorithm for document-level music retrieval",2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873440226&partnerID=40&md5=c4306bcb49809bdacf244f0fcac4b842,"Clifford R., University of Bristol, Merchant Venturers' Building, Bristol BS8 1UB, Woodland Road, United Kingdom; Christodoulakis M., King's College, London, London WC2R 2LS, Strand, United Kingdom; Crawford T., Goldsmiths College, University of London, London SE14 6NW, New Cross, United Kingdom; Meredith D., Goldsmiths College, University of London, London SE14 6NW, New Cross, United Kingdom; Wiggins G., Goldsmiths College, University of London, London SE14 6NW, New Cross, United Kingdom","We present MSM, a new maximal subset matching algorithm, for MIR at score level with polyphonic texts and patterns. First, we argue that the problem MSM and its ancestors, the SIA family of algorithms, solve is 3SUM-hard and, therefore, subquadratic solutions must involve approximation. MSM is such a solution; we describe it, and argue that, at O(n log n) time with no large constants, it is orders of magnitude more time-efficient than its closest competitor. We also evaluate MSM's performance on a retrieval problem addressed by the OMRAS project, and show that it outperforms OMRAS on this task by a considerable margin. © 2006 University of Victoria."
Ryynänen M.; Klapuri A.,Transcription of the singing melody in polyphonic music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873440865&partnerID=40&md5=ec910df7d50c6015fb37ab6c441de3a3,"Ryynänen M., Institute of Signal Processing, Tampere University of Technology, FI-33101 Tampere, P.O. Box 553, Finland; Klapuri A., Institute of Signal Processing, Tampere University of Technology, FI-33101 Tampere, P.O. Box 553, Finland",This paper proposes a method for the automatic transcription of singing melodies in polyphonic music. The method is based on multiple-F0 estimation followed by acoustic and musicological modeling. The acoustic model consists of separate models for singing notes and for no-melody segments. The musicological model uses key estimation and note bigrams to determine the transition probabilities between notes. Viterbi decoding produces a sequence of notes and rests as a transcription of the singing melody. The performance of the method is evaluated using the RWC popular music database for which the recall rate was 63% and precision rate 46%. A significant improvement was achieved compared to a baseline method from MIREX05 evaluations. © 2006 University of Victoria.
Peeters G.,Chroma-based estimation of musical key from audio-signal analysis,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421782&partnerID=40&md5=7faffa64c548ea676267693841726bc9,"Peeters G., Ircam - Sound Analysis/Synthesis Team, CNRS - STMS, F-75004 Paris, 1, pl. Igor Stravinsky, France","This paper deals with the automatic estimation of key (keynote and mode) of a music track from the analysis of its audio signal. Such a system usually relies on a succession of processes, each one making hypotheses about either the signal content or the music content: spectral representation, mapping to chroma, decision about the global key of the music piece. We review here the underlying hypotheses, compare them and propose improvements over current state of the art. In particular, we propose the use of a Harmonic Peak Subtraction algorithm as a front-end of the system and evaluate the performance of an approach based on hidden Markov models. We then compare our approach with other approaches in an evaluation using a database of 302 baroque, classical and romantic music tracks. © 2006 University of Victoria."
Eck D.; Bertin-Mahieux T.; Lamere P.,Autotagging music using supervised machine learning,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049167362&partnerID=40&md5=8cae4575c10c19514ec3d4421998a816,"Eck D., Sun Labs Sun Microsystems, Burlington, MA, United States; Bertin-Mahieux T., Dept. of Comp. Sci. Montreal, Univ. of Montreal, QC, Canada; Lamere P., Sun Labs Sun Microsystems, Burlington, MA, United States","Social tags are an important component of ""Web2.0"" music recommendation websites. In this paper we propose a method for predicting social tags using audio features and supervised learning. These automatically- generated tags (or ""autotags"") can furnish information about music that is untagged or poorly tagged. The tags can also serve to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. ©2007 Austrian Computer Society (OCG)."
Govaerts S.; Corthaut N.; Duval E.,Mood-ex-machina: Towards automation of moody tunes,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949525631&partnerID=40&md5=f7f0757345f187a417126be52a304971,"Govaerts S., Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium; Corthaut N., Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium; Duval E., Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium","In 2006, the rockanango system was developed for music annotation by music experts. The system allows these experts to create new musical parameters within a flat data structure [1]. Rockanango is deployed in a commercial environment of hotels, restaurants and cafés. One of the main concerns is the time it takes to manually annotate the music and to introduce new parameters. In this paper, we investigate the possibilities to assist the experts by means of automatic metadata generation. Two case studies are described. One focuses on the use of association rules, in combination with lower level metadata like mode and key. The other case study concerns the generation of a topic or subject marker for songs through harvested lyrics and a keyword generator. From our evaluation, we conclude that the generated keywords are relevant and that the music experts value them higher then laymen. Data mining techniques provide means for monitoring the metadata in terms of interparametric relationships that can be used to generate metadata. ©2007 Austrian Computer Society (OCG)."
Dehghani M.; Lovett A.M.,Efficient genre classification using qualitative representations,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873460951&partnerID=40&md5=1e2b601acfa0d6fa0f2db98f2f3a5f0a,"Dehghani M., Northwestern University, EECS Dept., Evanston, IL 60208-0834, 2145 Sheridan Rd., United States; Lovett A.M., Northwestern University, EECS Dept., Evanston, IL 60208-0834, 2145 Sheridan Rd., United States",We have constructed a system that can compute a qualitative representation of music from high-level features extracted from MusicXML files. We use two cognitively motivated computational models called SME and SEQL to build generalizations of musical genres from these representations. We then categorize novel music pieces according to the generalizations. We demonstrate the feasibility of the system with training sets much smaller than those used in previous systems. © 2006 University of Victoria.
McKay C.; Fujinaga I.,Musical genre classification: Is it worth pursuing and how can it be improved?,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873429683&partnerID=40&md5=a219590e3f6a84a31a2b1f4dd29af62a,"McKay C., Music Technology, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology, Schulich School of Music, McGill University, Montreal, QC, Canada","Research in automatic genre classification has been producing increasingly small performance gains in recent years, with the result that some have suggested that such research should be abandoned in favor of more general similarity research. It has been further argued that genre classification is of limited utility as a goal in itself because of the ambiguities and subjectivity inherent to genre. This paper presents a number of counterarguments that emphasize the importance of continuing research in automatic genre classification. Specific strategies for overcoming current performance limitations are discussed, and a brief review of background research in musicology and psychology relating to genre is presented. Insights from these highly relevant fields are generally absent from discourse within the MIR community, and it is hoped that this will help to encourage a more multi-disciplinary approach to automatic genre classification in the future. © 2006 University of Victoria."
Goto M.,AIST annotation for the RWC music database,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448585&partnerID=40&md5=fe680101994ad8086a4549a595172e9a,"Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","In this paper, we introduce our activities regarding the manual annotation of the musical pieces of the RWC Music Database. Although the RWC Music Database is widely used, its annotated descriptions are not widely available. We therefore annotated a set of music-scene descriptions consisting of the beat structure, melody line, and chorus sections. We call this AIST Annotation. We also manually synchronized standard MIDI files with the corresponding audio signals at the beat level. We hope that the AIST Annotation will contribute to further advances in the field of music information processing. © 2006 University of Victoria."
Izmirli O.,Audio key finding using low-dimensional spaces,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873482586&partnerID=40&md5=7e09958ea7e58453b0d3942b4015f1a8,"Izmirli O., Center for Arts and Technology Computer Science, Connecticut College, New London, CT, 06320, United States","This paper presents two models of audio key finding: a template based correlational model and a template based model that uses a low-dimensional tonal representation. The first model uses a confidence weighted correlation to find the most probable key. The second model is distance based and employs dimensionality reduction to the tonal representation before generating a key estimate. Experiments to determine the dependence of key finding accuracy on dimensionality are presented. Results show that low dimensional representations, compared to commonly used 12 dimensions, may be utilized for key finding without sacrificing accuracy. The first model's independently verified performance enabled it to be used as a benchmark for evaluation of the second model. Key finding accuracies for both models are given together with detailed results of the second model's performance as a function of the number of dimensions used. © 2006 University of Victoria."
Pohle T.; Knees P.; Schedl M.; Widmer G.,Meaningfully browsing music services,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62249181919&partnerID=40&md5=b36edb460e76732a835b8838884eefc7,"Pohle T., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","We present a browser application that offers the user an enhanced access to the content of music web services. Most importantly, the technique we apply aims at making it feasible to add to the automated suggestion of similar artists some intentional spin, or direction. At the heart of the algorithm, automatically derived artist descriptions are analyzed for common topics or aspects, and each artist is described by the extent to which it is associated with each of these topics. The browser application enables the user to formulate a query by means of these underlying topics by simply adjusting slider positions. The best matching artist is shown, and its web page found on the web music service is displayed. ©2007 Austrian Computer Society (OCG)."
Levy M.; Sandler M.,A semantic space for music derived from social tags,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049175658&partnerID=40&md5=265ad3561f749b45f47ea3ef85c4c290,"Levy M., Centre for Digital Music Queen Mary, University of London Mile End Road, London E1 4NS, United Kingdom; Sandler M., Centre for Digital Music Queen Mary, University of London Mile End Road, London E1 4NS, United Kingdom","In this paper we investigate social tags as a novel highvolume source of semanticmetadata formusic, using techniques from the fields of information retrieval and multivariate data analysis. We show that, despite the ad hoc and informal language of tagging, tags define a low-dimensional semantic space that is extremely well-behaved at the track level, in particular being highly organised by artist and musical genre. We introduce the use of Correspondence Analysis to visualise this semantic space, and show how it can be applied to create a browse-by-mood interface for a psychologically-motivated two-dimensional subspace representing musical emotion. ©2007 Austrian Computer Society (OCG)."
Antonopoulos I.; Pikrakis A.; Theodoridis S.; Cornelis O.; Moelants D.; Leman M.,Music retrieval by rhythmic similarity applied on greek and african traditional music,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949124063&partnerID=40&md5=5e7e45504368876e35d702510a787f6b,"Antonopoulos I., Dept. of Informatics and Telecommunications, University of Athens, Greece; Pikrakis A., Dept. of Informatics and Telecommunications, University of Athens, Greece; Theodoridis S., Dept. of Informatics and Telecommunications, University of Athens, Greece; Cornelis O., IPEM - Dept. of Musicology, Ghent University, Belgium; Moelants D., IPEM - Dept. of Musicology, Ghent University, Belgium; Leman M., IPEM - Dept. of Musicology, Ghent University, Belgium","This paper presents a method for retrieving music recordings by means of rhythmic similarity in the context of traditional Greek and African music. To this end, Self Similarity Analysis is applied either on the whole recording or on instances of a music thumbnail that can be extracted from the recording with an optional thumbnailing scheme. This type of analysis permits the extraction of a rhythmic signature per music recording. Similarity between signatures is measured with a standard Dynamic TimeWarping technique. The proposed method was evaluated on corpora of Greek and African traditional music where human improvisation plays a key role and music recordings exhibit a variety of music meters, tempi and instrumentation. ©2007 Austrian Computer Society (OCG)."
Müller M.; Mattes H.; Kurth F.,An efficient multiscale approach to audio synchronization,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428975&partnerID=40&md5=3d878889115bc44fd9ce29aafca926db,"Müller M., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Mattes H., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Kurth F., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany","We present an efficient and robust multiscale DTW (Ms-DTW) approach to music synchronization for time-aligning CD recordings of different interpretations of the same piece. The general strategy is to recursively project an alignment path computed at a coarse resolution level to the next higher level and then to refine the projected path. As main contributions, we address several crucial issues including the design and specification of robust and scalable audio features, suitable local cost measures, MsDTW levels, constraint regions, as well as sampling rate adaptation and structural enhancement strategies. Extensive experiments on Western classical music show that our MsDTW-based algorithm yields the same alignment result as the classical DTW-based strategy while significantly reducing the running time and memory requirements. Even for pieces of a duration of 10 to 15 minutes, the alignment (based on previously extracted feature sequences) can be computed in less than a second. © 2006 University of Victoria."
Serrà J.,A qualitative assessment of measures for the evaluation of a cover song identification system,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149090114&partnerID=40&md5=54c69fd31236d9e1d9bc76c6450e043f,"Serrà J., Music Technology Group, Universitat Pompeu Fabra, Spain","The evaluation of effectiveness in InformationRetrieval systems has been developed in parallel to its evolution, generating a great amount of proposals to achieve this process. This paper focuses on a particular task of Music Information Retrieval: a system for Cover Song Identification. We present a concrete example and then try to elucidate which metrics work best to evaluate such a system. We end up with two evaluation measures suitable for this problem: bpref and Normalized Lift Curves. ©2007 Austrian Computer Society (OCG)."
Yoshii K.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.,Hybrid collaborative and content-based music recommendation using probabilistic model with latent user preferences,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459337&partnerID=40&md5=66e35f4a38a0c4a5fa9ff4dc900c1ae0,"Yoshii K., Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Komatani K., Graduate School of Informatics, Kyoto University, Japan; Ogata T., Graduate School of Informatics, Kyoto University, Japan; Okuno H.G., Graduate School of Informatics, Kyoto University, Japan","This paper presents a hybrid music recommendation method that solves problems of two prominent conventional methods: collaborative filtering and content-based recommendation. The former cannot recommend musical pieces that have no ratings because recommendations are based on actual user ratings. In addition, artist variety in recommended pieces tends to be poor. The latter, which recommends musical pieces that are similar to users' favorites in terms of music content, has not been fully investigated. This induces unreliability in modeling of user preferences; the content similarity does not completely reflect the preferences. Our method integrates both rating and content data by using a Bayesian network called an aspect model. Unobservable user preferences are directly represented by introducing latent variables, which are statistically estimated. To verify our method, we conducted experiments by using actual audio signals of Japanese songs and the corresponding rating data collected from Amazon. The results showed that our method outperforms the two conventional methods in terms of recommendation accuracy and artist variety and can reasonably recommend pieces even if they have no ratings. © 2006 University of Victoria."
Marolt M.,A mid-level melody-based representation for calculating audio similarity,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459578&partnerID=40&md5=19abd5d1514e3f186e41679f96bbb55c,"Marolt M., University of Ljubljana, 1000 Ljubljana, Trzaska 25, Slovenia","We propose a mid-level melody-based representation that incorporates melodic, rhythmic and structural aspects of a music signal and is useful for calculating audio similarity measures. Most current approaches to music similarity use either low-level signal features, such as MFCCs that mostly capture timbral characteristics of music and contain little semantic information, or require symbolic representations, which are difficult to obtain from audio signals. The proposed mid-level representation is our attempt to bridge the gap between audio and symbolic domains by providing an integrated melodic, rhythmic and structural representation of music signals. The representation is based on a set of melodic fragments extracted from prominent melodic lines, it is beat-synchronous, which makes it independent of tempo variations and contains information on repetitions of short melodic phrases within the analyzed piece. We show how it can be calculated automatically from polyphonic audio signals and demonstrate its use for discovering melodic similarities between songs. We present results obtained by using the representation for finding different interpretations of songs in a music collection. © 2006 University of Victoria."
Pauws S.; Verhaegh W.; Vossen M.,Fast generation of optimal music playlists using local search,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873470750&partnerID=40&md5=8b1f0afa8932d40328984f934287a152,"Pauws S., Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands; Verhaegh W., Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands; Vossen M., Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands","We present an algorithm for use in an interactive music system that automatically generates music playlists that fit the music preferences given by a user. To this end, we introduce a formal model, define the problem of automatic playlist generation (APG) and indicate its NP-hardness. We use a local search (LS) procedure based on simulated annealing (SA) to solve the APG problem. In order to employ this LS procedure, we introduce an optimization variant of the APG problem, which includes the definition of penalty functions and a neighborhood structure. To improve upon the performance of the standard SA algorithm, we incorporated three heuristics referred to as song domain reduction, partial constraint voting, and two-level neighborhood structure. In tests, LS performed better than a constraint satisfaction (CS) solution in terms of run time, scalability and playlist quality. © 2006 University of Victoria."
Sandvold V.; Aussenac T.; Celma O.; Herrera P.,Good vibrations: Music discovery through personal musical concepts,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424941&partnerID=40&md5=5fb0629ca12baa70dffb0bddd2b9bfe4,"Sandvold V., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain; Aussenac T., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain; Celma O., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain; Herrera P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain","We present here Good Vibrations, a tool for music tagging, exploration and discovery, shaped as a media player plugin, and intended for home users. The plugin allows the quick ""invention"" of concepts and properties that can be tagged to songs. After some hours of active tagging, the plugin starts automatically proposing the proper tags to the user, who is also allowed to correct them. The plugin generates playlists according to the user-defined concepts, and recommends related music either from the user's personal collection or from the Internet (through it's connection to Foafing the Music). The plugin runs, for the moment, in Nullsoft Winamp on Windows XP systems. © 2006 University of Victoria."
Pikrakis A.; Giannakopoulos T.; Theodoridis S.,A computationally efficient speech/music discriminator for radio recordings,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873467474&partnerID=40&md5=fa35d990b471d00711bf35c9877bcb08,"Pikrakis A., University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece; Giannakopoulos T., University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece; Theodoridis S., University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece","This paper presents a speech/music discriminator for radio recordings, based on a new and computationally efficient region growing technique, that bears its origins in the field of image segmentation. The proposed scheme operates on a single feature, a variant of the spectral entropy, which is extracted from the audio recording by means of a short-term processing technique. The proposed method has been tested on recordings from radio stations broadcasting over the Internet and, despite its simplicity, has proved to yield performance results comparable to more sophisticated approaches. © 2006 University of Victoria."
Torres D.; Turnbull D.; Barrington L.; Lanckriet G.,Identifying words that are musically meaningful,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049111891&partnerID=40&md5=dae93848751f0f7e5c2fe8c9a80a21d0,"Torres D., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Turnbull D., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Barrington L., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Lanckriet G., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","A musically meaningful vocabulary is one of the keystones in building a computer audition system that can model the semantics of audio content. If a word in the vocabulary is inconsistently used by human annotators, or the word is not clearly represented by the underlying acoustic representation, the word can be considered as noisy and should be removed from the vocabulary to denoise the modeling process. This paper proposes an approach to construct a vocabulary of predictive semantic concepts based on sparse canonical component analysis (sparse CCA) . Experimental results illustrate that, by identifying musically meaningful words, we can improve the performance of a previously proposed computer audition system for music annotation and retrieval. ©2007 Austrian Computer Society (OCG)."
Chuan C.-H.; Chew E.,A dynamic programming approach to the extraction of phrase boundaries from tempo variations in expressive performances,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149181116&partnerID=40&md5=e97e2369f402b309edd19fb3dc060ab9,"Chuan C.-H., Department of Computer Science, Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States; Chew E., Integrated Media Systems Center, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA, United States","We present an approach to phrase segmentation that starts with an expressive music performance. Previous research has shown that phrases are delineated by tempo speedups and slowdowns. We propose a dynamic programming algorithm for extracting phrases from tempo information. We test two hypotheses formodeling phrase tempo shapes: a quadratic model, and a spline curve. We test the two models on phrase extraction from performances of entire classical romantic pieces namely, Chopin's Preludes Nos. 1 and 7. The algorithms determined 21 of the 26 phrase boundaries correctly from Arthur Rubinstein's and Evgeny Kissin's performances. We observe that not all tempo slowdowns signify a boundary (some are agogic accents), and multiple levels of phrasing strategies should be considered for detailed interpretation analyses. ©2007 Austrian Computer Society (OCG)."
Grund C.M.,A philosophical wish list for research in music information retrieval,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423074&partnerID=40&md5=e7fea0fae53e48c0d4dad4a10960dd37,"Grund C.M., Institute of Philosophy, Education and the Study of Religions - Philosophy, University of Southern Denmark, Odense, Denmark","Within a framework provided by the traditional trio consisting of metaphysics, epistemology and ethics, a first stab is made at a wish list for MIR-research from a philosophical point of view. Since the tools of MIR are equipped to study language and its use from a purely sonic standpoint, MIR research could result in another revealing revolution within the linguistic turn in philosophy. © 2006 University of Victoria."
Van Kranenburg P.; Garbers J.; Volk A.; Franswiering; Grijp L.; Veltkamp R.C.,Towards integration of MIR and folk song research,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949087242&partnerID=40&md5=f96ad1f8c92c7bb02937993bdabd9a46,"Van Kranenburg P., Utrecht University, Amsterdam, Netherlands; Garbers J., Utrecht University, Amsterdam, Netherlands; Volk A., Utrecht University, Amsterdam, Netherlands; Franswiering, Utrecht University, Amsterdam, Netherlands; Grijp L., Meertens Institute, Amsterdam, Netherlands; Veltkamp R.C., Utrecht University, Amsterdam, Netherlands","Folk song research (FSR) often deals with large collections of tunes that have various types of relations to each other. Computational methods can support the study of the contents of these collections. Music Information Retrieval (MIR) research provides such methods. Yet a fruitful cooperation of both disciplines is difficult to achieve. We present a role-model to structure this cooperation in which tasks and responsibilities are distributed among the roles of MIR, Computational Musicology (CM) and FSR. ©2007 Austrian Computer Society (OCG)."
Geleijnse G.; Schedl M.; Knees P.,The quest for ground truth in musical artist tagging in the social web era,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649640622&partnerID=40&md5=176058ccd6347a99e2fb72e72b521a21,"Geleijnse G., Philips Research High Tech Campus, 34 Eindhoven, Netherlands; Schedl M., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","Research in Web music information retrieval traditionally focuses on the classification, clustering or categorizing of music into genres or other subdivisions. However, current community-based web sites provide richer descriptors (i.e. tags) for all kinds of products. Although tags have no well-defined semantics, they have proven to be an effective mechanism to label and retrieve items. Moreover, these tags are community-based and hence give a description of a product through the eyes of a community rather than an expert opinion. In this work we focus on Last.fm, which is currently the largest music community web service. We investigate whether the tagging of artists is consistent with the artist similarities found with collaborative filtering techniques. As the Last.fm data shows to be both consistent and descriptive, we propose a method to use this community-based data to create a ground truth for artist tagging and artist similarity. ©2007 Austrian Computer Society (OCG)."
Pinto A.; Van Leuken R.H.; Demirci M.F.; Wiering F.; Veltkamp R.C.,Indexing music collections through graph spectra,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350498624&partnerID=40&md5=7567dcb41567fda48c0c3c442d3bd61e,"Pinto A., Dipartimento di Informatica E Comunicazione, Universit̀a Degli Studi di Milano, Italy; Van Leuken R.H., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands; Demirci M.F., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands; Wiering F., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands","Content based music retrieval opens up large collections, both for the general public and music scholars. It basically enables the user to find (groups of) similar melodies, thus facilitating musicological research of many kinds. We present a graph spectral approach, new to the music retrieval field, in which melodies are represented as graphs, based on the intervals between the notes they are composed of. These graphs are then indexed into a database using their laplacian spectra as a feature vector. This laplacian spectrum is known to be very informative about the graph, and is therefore a good representative of the original melody. Consequently, range searching around the query spectrum returns similar melodies. We present an experimental evaluation of this approach, together with a comparison with two known retrieval techniques. On our test corpus, a subset of a well documented and annotated collection of Dutch folk songs, this evaluation demonstrates the effectiveness of the overall approach. ©2007 Austrian Computer Society (OCG)."
Gómez C.; Abad-Mota S.; Ruckhaus E.,An analysis of the mongeau-sankoff algorithm for music information retrieval,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449105867&partnerID=40&md5=7432135cdeeaa01ceb279d1e8f109430,"Gómez C., Dept. of Computing and Information Technology, Universidad Simón Boĺivar, Venezuela; Abad-Mota S., Dept. of Computing and Information Technology, Universidad Simón Boĺivar, Venezuela; Ruckhaus E., Dept. of Computing and Information Technology, Universidad Simón Boĺivar, Venezuela","An essential problem in music information retrieval is to determine the similarity between two given melodies; there are several melodic similarity measures that have been proposed, among others, the Mongeau-Sankoff measure. In this work we implemented a modified version of the Mongeau-Sankoff measure. We conducted an experimental study to compare the implemented measure with other similarity measures; this evaluation was done in the context of the 2005 edition of the MIREX symbolic melodic similarity competition. The most relevant result of our work is an implementation of the Mongeau-Sankoff measure that presents greater effectiveness when compared to other current melodic similarity measures. ©2007 Austrian Computer Society (OCG)."
Frieler K.,Visualizing music on the metrical circle,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949110503&partnerID=40&md5=810d95829b55736f53f95331541b67dc,"Frieler K., Institute for Musicology, University of Hamburg, Germany","In this paper we propose a novel method, called Metrical Circle Map, for exploring the cyclic aspects of musical time. To this end, we give a short formalization introducing the notion of Metrical Markov Chains as transition probabilities of segments on the metrical circle. As an illustration we present a compact visualization of the zeroth- and first order metrical Markov transitions of 61 Irish folk songs. ©2007 Austrian Computer Society (OCG)."
Cao C.; Li M.; Liu J.; Yan Y.,Singing melody extraction in polyphonic music by harmonic tracking,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950237831&partnerID=40&md5=de6779556cd76bcfda316afafc342ecf,"Cao C., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China; Li M., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China; Liu J., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China; Yan Y., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China","This paper proposes an effective method for automatic melody extraction in polyphonic music, especially vocal melody songs. The method is based on subharmonic summation spectrum and harmonic structure tracking strategy. Performance of the method is evaluated using the LabROSA database 1 . The pitch extraction accuracy of our method is 82.2% on the whole database, while 79.4% on the vocal part. ©2007 Austrian Computer Society (OCG)."
Mayer R.; Lidy T.; Rauber A.,The map of mozart,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419255&partnerID=40&md5=3cf4504c053335196e1b6cbda9096840,"Mayer R., Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria; Lidy T., Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria","We present a study on using a Mnemonic Self-Organizing Map for clustering a very homogeneous collection of music. In particular, we create a map containing the complete works of Wolfgang Amadeus Mozart. We study and analyze the clustering capabilities of the SOM on this very focused collection. We furthermore present a web-based application for exploring the map and accessing the music it represents. © 2006 University of Victoria."
Hoashi K.; Ishizaki H.; Matsumoto K.; Sugaya F.,Content-based music retrieval using query integration for users with diverse preferences,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949224740&partnerID=40&md5=644c44b54100a504160f2b03952e0e6c,"Hoashi K., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan; Ishizaki H., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan; Matsumoto K., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan; Sugaya F., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan","This paper proposes content-based music information retrieval (MIR) methods based on user preferences, which aim to improve the accuracy of MIR for users with .diverse . preferences, i.e., users whose preferences range in songs with a wide variety of features. The proposed MIR method dynamically generates an optimal set of query vectors from the sample set of songs submitted by the user to express their preferences, based on the similarity of the songs in the sample set. Experiments conducted on a music collection with subjective user ratings verify that our proposal is effective to improve the accuracy of contentbased MIR. Furthermore, by implementing a two-step MIR algorithm which utilizes song clustering results, the efficiency of the proposed MIR method is significantly improved. ©2007 Austrian Computer Society (OCG)."
Mardirossian A.; Chew E.,Music summarization via key distributions: Analyses of similarity assessment across variations,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445933&partnerID=40&md5=7341f881a2867a57e305a7170237800e,"Mardirossian A., University of Southern California, Viterbi School of Engineering, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA 90089, United States; Chew E., University of Southern California, Viterbi School of Engineering, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA 90089, United States","This paper presents a computationally efficient method for quantifying the degree of tonal similarity between two pieces of music. The properties we examine are key frequencies and average time in key, and we propose two metrics, based on the L1 and L2 norms, for quantifying similarity using these descriptors. The methods are applied to 711 classical themes and variations over 71 variation sets by 10 composers of different genres. Quantile-quantile plots and the Kolmogorov-Smirnov measure show that the proposed metrics exhibit strongly distinct behaviour when assessing pieces from the same variation set, and those that are not. Comparisons across variation sets by the same composer, and comparisons of pieces by different composers although result in similar distributions, are derived from fundamentally different underlying distributions, according to the K-S measure. We present probabilistic analyses of the two methods based on the distributions derived empirically. When the discrimination threshold is set at 55, the probabilities of Type I and Type II errors are 18.41% and 20.56% respectively for Method 1, and 15.72% and 22.94% respectively for Method 2. Method 1 has a success rate of 99.48% when labeling pieces as dissimilar (not from the same variation set), while the corresponding rate for Method 2 is 99.45%. © 2006 University of Victoria."
Yeh C.; Bogaards N.; Roebel A.,Synthesized polyphonic music database with verifiable ground truth for multiple f0 estimation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349218803&partnerID=40&md5=de06978c486d7d3d800d6ee8ad231a14,"Yeh C., IRCAM, CNRS-STMS Paris, France; Bogaards N.; Roebel A., IRCAM, CNRS-STMS Paris, France","To study and to evaluate a multiple F0 estimation algorithm, a polyphonic database with verifiable ground truth is necessary. Real recordings with manual annotation as ground truth are often used for evaluation. However, ambiguities arise during manual annotation, which are often set up by subjective judgements. Therefore, in order to have access to verifiable ground truth, we propose a systematic method for creating a polyphonicmusic database. Multiple monophonic tracks are rendered from a given MIDI file, in which rendered samples are separated to prevent overlaps and to facilitate automatic annotation. F0s can then be reliably extracted as ground truth, which are stored using SDIF. ©2007 Austrian Computer Society (OCG)."
Bainbridge D.; Bell T.,Identifying music documents in a collection of images,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873464179&partnerID=40&md5=b9388c339f2a9e0dcc386b8282818cac,"Bainbridge D., Department of Computer Science, University of Waikato, New Zealand; Bell T., Department of Computer Science and Software Engineering, University of Canterbury, New Zealand","Digital libraries and search engines are now well-equipped to find images of documentsbased on queries. Many images of music scores are now available, often mixed up with textual documents and images. For example, using the Google ""images"" search feature, a search for ""Beethoven"" will return a number of scores and manuscripts as well as pictures of the composer. In this paper we report on an investigation into methods to mechanically determine if a particular document is indeed a score, so that the user can specify that only musical scores should be returned. The goal is to find a minimal set of features that can be used as a quick test that will be applied to large numbers of documents. A variety of filters were considered, and two promising ones (run-length ratios and Hough transform) were evaluated. We found that a method based around run-lengths in vertical scans (RL) that out-performs a comparable algorithm using the Hough transform (HT). On a test set of 1030 images, RL achieved recall and precision of 97.8% and 88.4% respectively while HT achieved 97.8% and 73.5%. In terms of processor time, RL was more than five times as fast as HT. © 2006 University of Victoria."
Livshin A.; Rodet X.,"The significance of the non-harmonic ""noise"" versus the harmonic series for musical instrument recognition",2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873452800&partnerID=40&md5=79ef1f6c1b88068c7d0fd95ab7626adf,"Livshin A., IRCAM Centre Pompidou, Paris 75004, 1 Place Stravinsky, France; Rodet X., IRCAM Centre Pompidou, Paris 75004, 1 Place Stravinsky, France","Sound produced by Musical instruments with definite pitch consists of the Harmonic Series and the non-harmonic Residual. It is common to treat the Harmonic Series as the main characteristic of the timbre of pitched musical instruments. But does the Harmonic Series indeed contain the complete information required for discriminating among different musical instruments? Could the non-harmonic Residual, the ""noise"", be used all by itself for instrument recognition? The paper begins by performing musical instrument recognition with an extensive sound collection using a large set of feature descriptors, achieving a high instrument recognition rate. Next, using Additive Analysis/Synthesis, each sound sample is resynthesized using solely its Harmonic Series. These ""Harmonic"" samples are then subtracted from the original samples to retrieve the non-harmonic Residuals. Instrument recognition is performed on the resynthesized and the ""Residual"" sound sets. The paper shows that the Harmonic Series by itself is indeed enough for achieving a high instrument recognition rate; however, the non-harmonic Residuals by themselves can also be used for distinguishing among musical instruments, although with lesser success. Using feature selection, the best 10 feature descriptors for instrument recognition out of our extensive feature set are presented for the Original, Harmonic and Residual sound sets. © 2006 University of Victoria."
Geleijnse G.; Korst J.,Efficient lyrics extraction from the Web,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873478738&partnerID=40&md5=f448ec94713610365f3112a1411bf6f4,"Geleijnse G., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Korst J., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","We present a novel method to extract lyrics from the Web. The aim is to extract a set of multiple versions of the lyrics to a song. Lyrics can be identified within a text by a regular expression. We use a projection of a document to efficiently identify lyrics within the document by mapping it to a regular expression. We describe a method to cluster the multiple versions of the lyrics by filtering out erroneous texts such as lyrics to other songs. For reasons of efficiency, we do this by comparing fingerprints instead of the texts themselves. © 2006 University of Victoria."
Schedl M.; Pohle T.; Knees P.; Widmer G.,Assigning and visualizing music genres by web-based co-occurrence analysis,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459558&partnerID=40&md5=c4be54ad845ca52b29b164dff6f6e5a1,"Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","We explore a simple, web-based method for predicting the genre of a given artist based on co-occurrence analysis, i.e. analyzing co-occurrences of artist and genre names on music-related web pages. To this end, we use the page counts provided by Google to estimate the relatedness of an arbitrary artist to each of a set of genres. We investigate four different query schemes for obtaining the page counts and two different probabilistic approaches for predicting the genre of a given artist. Evaluation is performed on two test collections, a large one with a quite general genre taxonomy and a quite small one with rather specific genres. Since our approach yields estimates for the relatedness of an artist to every genre of a given genre set, we can derive genre distributions which incorporate information about artists that cannot be assigned a single genre. This allows us to overcome the inflexible artist-genre assignment usually used in music information systems. We present a simple method to visualize such genre distributions with our Traveller's Sound Player. Finally, we briefly outline how to adapt the presented approach to extract other properties of music artists from the web. © 2006 University of Victoria."
McEnnis D.; Cunningham S.J.,Sociology and music recommendation systems,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749089902&partnerID=40&md5=981bf9a8016726f82e6b01706c112c8e,"McEnnis D., Waikato University, New Zealand; Cunningham S.J., Waikato University, New Zealand","Music recommendation systems have centred on two different approaches: content based analysis and collaborative filtering. Little attention has been paid to the reasons why these techniques have been effective. Fortunately, the social sciences have asked these questions. One of the findings of this research is that social context is much more important than previously thought. This paper introduces this body of research from sociology and its relevance to music recommendation algorithms. ©2007 Austrian Computer Society (OCG)."
Schwenninger J.; Brueckner R.; Willett D.; Hennecke M.,Language identification in vocal music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873446853&partnerID=40&md5=33d0b4715a035608679541f9c2eb693c,"Schwenninger J., Department of Electrical Engineering, University of Ulm, Ulm, Germany; Brueckner R., Harman/Becker Automotive Systems, Speech Dialog Systems, Ulm, Germany; Willett D., Harman/Becker Automotive Systems, Speech Dialog Systems, Ulm, Germany; Hennecke M., Harman/Becker Automotive Systems, Speech Dialog Systems, Ulm, Germany","Language identification is an important field in spoken language processing. The identification of the language sung or spoken in music, however, has attracted only minor attention so far. This, however, is an important task when it comes to categorizing, classifying and labelling of music data. In this paper, we review our efforts of transferring well-established techniques from spoken language identification to the area of language identification in music. We present results of distinguishing German and English sung modern music and propose and evaluate techniques designed for improving the classification performance. These techniques involve limiting the classification on song segments that appear to have vocals and on frames that are not distorted by heavy beat onsets. © 2006 University of Victoria."
Geleijnse G.; Korst J.,Web-based artist categorization,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424683&partnerID=40&md5=a669d84b6f74b90b663080ae9bf15eff,"Geleijnse G., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Korst J., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","We present a novel approach in categorizing artists into subjective categories such as genre. We base our method on co-occurrences on the web, found with the Google search engine. A direct mapping between artists and categories proved to be unreliable. We use the categories mapped to closely related artists to obtain a more reliable mapping. The method is tested on a genre classification test set with convincing results. Moreover, mood categorization is explored using the same techniques. © 2006 University of Victoria."
Mckay C.; Fujinaga I.,Jwebminer: A web-based feature extractor,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952344720&partnerID=40&md5=5d6ca4fed63cd8f48ef120a88581ed31,"Mckay C., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada","jWebMiner is a software package for extracting cultural features from the web. It is designed to be used for arbitrary types of MIR research, either as a stand-alone application or as part of the jMIR suite. It emphasizes extensibility, generality and an easy-to-use interface. At its most basic level, the software operates by using web services to extract hit counts from search engines. Functionality is available for calculating a variety of statistical features based on these counts, for variably weighting web sites or limiting searches only to particular sites, for excluding hits that do not contain particular filter terms, for defining synonym relationships between certain search strings, and for applying a number of additional search configurations. ©2007 Austrian Computer Society (OCG)."
Craft A.J.D.; A.wiggins G.; Crawford T.,Howmany beans make five? The consensus problem in music-genre classification and a new evaluation method for single-genre categorisation systems,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749112518&partnerID=40&md5=585485d7b3b2ad8c1752a365f8954e1c,"Craft A.J.D., Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom; A.wiggins G., Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom; Crawford T., Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom","Genre definition and attribution is generally considered to be subjective. This makes evaluation of any genrelabelling system intrinsically difficult, as the ground-truth against which it is compared is based upon subjective responses, with little inter-participant consensus. This paper presents a novel method of analysing the results of a genre-labelling task, and demonstrates that there are groups of genre-labelling behaviour which are selfconsistent. It is proposed that the evaluation of any genre classification system uses this modified analysis method. ©2007 Austrian Computer Society (OCG)."
Rhodes C.; Casey M.,Algorithms for determining and labelling approximate hierarchical self-similarity,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149101231&partnerID=40&md5=ea7d3c7668b668f7280e33a83111512f,"Rhodes C., Department of Computing, Goldsmiths University of London, SE14 6NW, United Kingdom; Casey M., Department of Computing, Goldsmiths University of London, SE14 6NW, United Kingdom","We describe an algorithm for finding approximate sequence similarity at all scales of interest, being explicit about our modelling assumptions and the parameters of the algorithm. We further present an algorithm for producing section labels based on the sequence similarity, and compare these labels with some expert-provided ground truth for a particular set of recordings. ©2007 Austrian Computer Society (OCG)."
Sinclair S.; Droettboom M.; Fujinaga I.,Lilypond for pyScore: Approaching a universal translator for music notation,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472186&partnerID=40&md5=61220ec2b5da2644b4b169a86a247bf3,"Sinclair S., Schulich School of Music, McGill University, Montreal, QC, Canada; Droettboom M., Johns Hopkins University, Baltimore, MD, 21218, 3400 North Charles Street, United States; Fujinaga I., Schulich School of Music, McGill University, Montreal, QC, Canada","Several languages for music notation have been defined in recent years. pyScore, a framework for translating between notation formats, and new module for it which can generate input for the LilyPond music engraving system are described. This shows the potential for developing pyScore into a ""universal translator"" for musical scores. © 2006 University of Victoria."
Reed J.; Lee C.-H.,A study on attribute-based taxonomy for music information retrieval,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952410453&partnerID=40&md5=28c424efeb8446325f407fbd7e8a1445,"Reed J., Georgia Institute of Technology Atlanta, School of Electrical and Computer Engineering, GA 30332, United States; Lee C.-H., Georgia Institute of Technology Atlanta, School of Electrical and Computer Engineering, GA 30332, United States","We propose an attribute-based taxonomy approach to providing alternative labels to music. Labels, such as genre, are often used as ground-truth for describing song similarity in music information retrieval (MIR) systems. A consistent labelling scheme is usually a key in determining quality of classifier learning in training and performance in testing of an MIR system. We examine links between conventional genre-based taxonomies and acoustical attributes available in text-based descriptions of songs. We show that the vector representation of each song based on these acoustic attributes enables a framework for unsupervised clustering of songs to produce alternative labels and quantitative measures of similarity between songs. Our experimental results demonstrate that this new set of labels are meaningful and classifiers based on these labels achieve similar or better results than those designed with existing genrebased labels. ©2007 Austrian Computer Society (OCG)."
Garbers J.; Van Kranenburg P.; Volk A.; Franswiering; Veltkamp R.C.; Grijp L.P.,Using pitch stability among a group of aligned query melodies to retrieve unidentified variant melodies,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949100249&partnerID=40&md5=72e7fec16de434a08e14a6fdf71931ae,"Garbers J., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Volk A., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Franswiering, Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Grijp L.P., Meertens Institute, Amsterdam, Netherlands","Melody identification is an important task in folk song variation research. In this paper we develop methods and tools that support researchers in finding melodies in a database that belong to the same variant group as a set of given melodies. The basic approach is to derive from the pitches of the known variants per onset a weighted pitch distribution, which quantifies pitch stability. We allow for partial matching and AND and OR queries. Technically we do so by defining a distance measure between weighted pitch distribution sequences. It is based on two applications of the Earth Mover's Distance, which is a distribution distance. We set up a distance framework and discuss musically meaningful parameterizations for two tasks: a) Study the inner-group distances between the group as a whole and single members of the group. b) Use the group's weighted pitch distribution sequence to query for variant melodies. The first experimental results seem very promising: a) The inner-group distances correlate to expert assigned subgroups. b) For variant retrieval our method works better than last year's MIREX winner. ©2007 Austrian Computer Society (OCG)."
Pohle T.; Knees P.; Schedl M.; Widmer G.,Independent component analysis for music similarity computation,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873457860&partnerID=40&md5=4348a03cc29b9f2432fa00dd51813e01,"Pohle T., Johannes Kepler University, Linz, Austria; Knees P., Johannes Kepler University, Linz, Austria; Schedl M., Johannes Kepler University, Linz, Austria; Widmer G., Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In the recent years, a number of publications have appeared that deal with automatically calculating the similarity of music tracks. Most of them are based on features that are not intuitively understandable to humans, as they do not have a musically meaningful counterpart, but are merely measures of basic physical properties of the audio signal. Furthermore, most of these algorithms do not take into account the temporal development of the audio signal, which certainly is an important aspect of music. All of them consider the musical signal as a whole, not trying to reconstruct the listening process of dividing the signal into a number of sources. In this work, we present a novel approach to fill this gap by combining a number of existing ideas. At the heart of our approach, Independent Component Analysis (ICA) decomposes an audio signal into individual parts that appear maximally independent from each other. We present one basic algorithm to use these components for similarity computations, and evaluate a number of modifications to it with respect to genre classification accuracy. Our results indicate that this approach is at least of similar quality as many existing feature extraction routines. © 2006 University of Victoria."
Cannam C.; Landone C.; Sandler M.; Bello J.P.,The Sonic Visualiser: A visualisation platform for semantic descriptors from musical signals,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453261&partnerID=40&md5=d391287f5823910c7260eff470c1ab60,"Cannam C., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom; Landone C., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom; Bello J.P., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom","Sonic Visualiser is the name for an implementation of a system to assist study and comprehension of the contents of audio data, particularly of musical recordings. It is a C++ application with a Qt4 GUI that runs on Windows, Mac, and Linux. It embodies a number of concepts which are intended to improve interaction with audio data and features, most notably with respect to the representation of time-synchronous information. The architecture of the application allows for easy integration of third party algorithms for the extraction of low and mid-level features from musical audio data. This paper describes some basic principles and functionalities of Sonic Visualiser. © 2006 University of Victoria."
Chordia P.; Rae A.,Raag recognition using pitch-class and pitch-class dyad distributions,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73249125605&partnerID=40&md5=3941db1df2b2e182b71c95280bfb7b60,"Chordia P., Department of Music, Georgia Institute of Technology, Atlanta GA 30332, 840 Mc Millan St., United States; Rae A., Department of Music, Georgia Institute of Technology, Atlanta GA 30332, 840 Mc Millan St., United States","We describe the results of the first large-scale raag recognition experiment. Raags are the central structure of Indian classical music, each consisting of a unique set of complex melodic gestures. We construct a system to recognize raags based on pitch-class distributions (PCDs) and pitch-class dyad distributions (PCDDs) calculated directly from the audio signal. A large, diverse database consisting of 20 hours of recorded performances in 31 different raags by 19 different performers was assembled to train and test the system. Classification was performed using support vector machines, maximum a posteriori (MAP) rule using a multivariate likelihood model (MVN), and Random Forests. When classification was done on 60s segments, a maximum classification accuracy of 99.0% was attained in a cross-validation experiment. In a more difficult unseen generalization experiment, accuracy was 75%. The current work clearly demonstrates the effectiveness of PCDs and PCDDs in discriminating raags, even when musical differences are subtle. ©2007 Austrian Computer Society (OCG)."
Sigurdsson S.; Petersen K.B.; Lehn-Schiøler T.,Mel frequency cepstral coefficients: An evaluation of robustness of MP3 encoded music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873450155&partnerID=40&md5=fca42317054b0fadc534c137aba8c8c8,"Sigurdsson S., Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark; Petersen K.B., Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark; Lehn-Schiøler T., Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark","In large MP3 databases, files are typically generated with different parameter settings, i.e., bit rate and sampling rates. This is of concern for MIR applications, as encoding difference can potentially confound meta-data estimation and similarity evaluation. In this paper we will discuss the influence of MP3 coding for the Mel frequency cepstral coe-ficients (MFCCs). The main result is that the widely used subset of the MFCCs is robust at bit rates equal or higher than 128 kbits/s, for the implementations we have investigated. However, for lower bit rates, e.g., 64 kbits/s, the implementation of the Mel filter bank becomes an issue. © 2006 University of Victoria."
Casey M.; Slaney M.,Song intersection by approximate nearest neighbor search,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873443097&partnerID=40&md5=70037f1134cfb7fc3b47561c4e8f08e6,"Casey M., Goldsmiths College, University of London, United Kingdom; Slaney M., Yahoo Research Inc., Sunnyvale, CA, United States","We present new methods for computing inter-song similarities using intersections between multiple audio pieces. The intersection contains portions that are similar, when one song is a derivative work of the other for example, in two different musical recordings. To scale our search to large song databases we have developed an algorithm based on locality-sensitive hashing (LSH) of sequences of audio features called audio shingles. LSH provides an efficient means to identify approximate nearest neighbors in a high-dimensional feature space. We combine these nearest neighbor estimates, each a match from a very large database of audio to a small portion of the query song, to form a measure of the approximate similarity. We demonstrate the utility of our methods on a derivative works retrieval experiment using both exact and approximate (LSH) methods. The results show that LSH is at least an order of magnitude faster than the exact nearest neighbor method and that accuracy is not impacted by the approximate method. © 2006 University of Victoria."
Lee K.; Slaney M.,A unified system for chord transcription and key extraction using hidden markov models,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849168570&partnerID=40&md5=34c4e50352608845ff0d545e9d8af989,"Lee K., Center for Computer Research in Music and Acoustics, Stanford University, United States; Slaney M., Yahoo Research, Sunnyvale, CA 94089, United States","A new approach to acoustic chord transcription and key extraction is presented. As in an isolated word recognizer in automatic speech recognition systems, we treat a musical key as a word and build a separate hidden Markov model for each key in 24 major/minor keys. In order to acquire a large set of labeled training data for supervised training, we first perform harmonic analysis on symbolic data to extract the key information and the chord labels with precise segment boundaries. In parallel, we synthesize audio from the same symbolic data whose harmonic progression are in perfect alignment with the automatically generated annotations. We then estimate the model parameters directly from the labeled training data, and build 24 key-specific HMMs. The experimental results show that the proposed model not only successfully estimates the key, but also yields higher chord recognition accuracy than a universal, key-independentmodel. ©2007 Austrian Computer Society (OCG)."
Pampalk E.; Gasser M.,An implementation of a simple playlist generator based on audio similarity measures and user feedback,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444741&partnerID=40&md5=bafce4abed833ba9b0b9ce5a34a5ad8c,"Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria","This paper presents an implementation of a simple playlist generator. An audio-based music similarity measure and simple heuristics are used to create playlists given minimum user input. The ultimate goal of this work is to conduct a field study, i.e., to run the system on the users' personal collection and study the usage behavior over a longer period of time. The functions include, for example, allowing the user to control the variance of the playlists in terms of how often the same song or songs from the same artists are repeated. © 2006 University of Victoria."
Rizo D.; Ponce De León P.J.; Pérez-Sancho C.; Pertusa A.; Inesta J.M.,A pattern recognition approach for melody track selection in MIDI files,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444584&partnerID=40&md5=2844018fc498e1d8c03fdb94f88aeae9,"Rizo D., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Ponce De León P.J., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Pérez-Sancho C., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Pertusa A., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Inesta J.M., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain","Standard MIDI files contain data that can be considered as a symbolic representation of music (a digital score), and most of them are structured as a number of tracks. One of them usually contains the melodic line of the piece, while the other tracks contain accompaniment music. The goal of this work is to identify the track that contains the melody using statistical properties of the musical content and pattern recognition techniques. Finding that track is very useful for a number of applications, like speeding up melody matching when searching in MIDI databases or motif extraction, among others. First, a set of descriptors from each track of the target file are extracted. These descriptors are the input to a random forest classifier that assigns the probability of being a melodic line to each track. The track with the highest probability is selected as the one containing the melodic line of that MIDI file. Promising results have been obtained testing a number of databases of different music styles. © 2006 University of Victoria."
Karydis I.; Nanopoulos A.; Papadopoulos A.N.; Cambouropoulos E.,VISA: The voice integration/segregation algorithm,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50349100739&partnerID=40&md5=debe1519eec08495c087a4202dcbdc73,"Karydis I., Department of Informatics Aristotle, University of Thessaloniki, Greece; Nanopoulos A., Department of Informatics Aristotle, University of Thessaloniki, Greece; Papadopoulos A.N., Department of Informatics Aristotle, University of Thessaloniki, Greece; Cambouropoulos E., Dept. of Music Studies Aristotle, Univ. of Thessaloniki, Greece","Listeners are capable to perceive multiple voices in music. Adopting a perceptual view of musical 'voice' that corresponds to the notion of auditory stream, a computational model is developed that splits musical scores (symbolic musical data) into different voices. A single 'voice' may consist of more than one synchronous notes that are perceived as belonging to the same auditory stream; in this sense, the proposed algorithm, may separate a given musical work into fewer voices than the maximum number of notes in the greatest chord. This is paramount, among other, for developing MIR systems that enable pattern recognition and extraction within musically pertinent 'voices' (e.g. melodic lines). The algorithm is tested against a small dataset that acts as groundtruth. ©2007 Austrian Computer Society (OCG)."
Kapur A.; Percival G.; Lagrange M.; Tzanetakis G.,Pedagogical transcription for multimodal sitar performance,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52649144788&partnerID=40&md5=b2eb6574606a4ff382dba08c535398a6,"Kapur A., Department of Computer Science, University of Victoria, BC, Canada; Percival G., Department of Computer Science, University of Victoria, BC, Canada; Lagrange M., Department of Computer Science, University of Victoria, BC, Canada; Tzanetakis G., Department of Computer Science, University of Victoria, BC, Canada","Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques and the creation of annotated guidelines for students. We propose the use of modified traditional instruments enhanced with sensors which can obtain such data; as a case study we examine the sitar. ©2007 Austrian Computer Society (OCG)."
Mauch M.; Dixon S.; Harte C.; Casey M.; Fields B.,Discovering chord idioms through beatles and real book songs,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593757&doi=10.1111%2fjosi.12045&partnerID=40&md5=1505ead36e2cf967942b0eca5770c57b,"Mauch M., Centre for Digital Music, University of London Queen Mary, United Kingdom; Dixon S., Centre for Digital Music, University of London Queen Mary, United Kingdom; Harte C., Centre for Digital Music, University of London Queen Mary, United Kingdom; Casey M., Computing Department, Goldsmiths, University of London, United Kingdom; Fields B., Computing Department, Goldsmiths, University of London, United Kingdom","Modern collections of symbolic and audio music content provide unprecedented possibilities for musicological research, but traditional qualitative evaluation methods cannot realistically cope with such amounts of data. We are interested in harmonic analysis and propose key-independent chord idioms derived from a bottom-up analysis of musical data as a new subject of musicological interest. In order to motivate future research on audio chord idioms and on probabilistic models of harmony we perform a quantitative study of chord progressions in two popular music collections. In particular, we extract common subsequences of chord classes from symbolic data, independent of key and context, and order them by frequency of occurrence, thus enabling us to identify chord idioms. We make musicological observations on selected chord idioms from the collections. ©2007 Austrian Computer Society (OCG)."
Müllensiefen D.; Lewis D.; Rhodes C.; Wiggins G.,Evaluating a chord-labelling algorithm,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64649098119&partnerID=40&md5=63ddf2066d646a2a163e9027f3897706,"Müllensiefen D., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom; Lewis D., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom; Rhodes C., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom; Wiggins G., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom","This paper outlines a method for evaluating a new chordlabelling algorithmusing symbolic data as input. Excerpts from full-score transcriptions of 40 pop songs are used. The accuracy of the algorithm's output is compared with that of chord labels from published song books, as assessed by experts in pop music theory. We are interested not only in the accuracy of the two sets of labels but also in the question of potential harmonic ambiguity as reflected the judges' assessments. We focus, in this short paper, on outlining the general approach of this research project. ©2007 Austrian Computer Society (OCG)."
Seifert F.; Rasch K.; Rentzsch M.,Tempo induction by stream-based evaluation of musical events,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873450047&partnerID=40&md5=472303e735d5a5bf0fe91a19b45ab9b4,"Seifert F., Department of Computer Science, University of Technology, Chemnitz, 09107, Germany; Rasch K., Department of Computer Science, University of Technology, Chemnitz, 09107, Germany; Rentzsch M., Department of Computer Science, University of Technology, Chemnitz, 09107, Germany","We present an approach for tempo induction that is based on a more perception-oriented analysis of inter-onset intervals. Therefore we utilize auditory grouping concepts and define some rules for their formation. Finally, we show preliminary results that confirm our aim of improving the quality of tempo induction by reducing the amount of perceptually irrelevant data. © 2006 University of Victoria."
Gouyon F.; Dixon S.,ISMIR 2006 tutorial: Computational rhythm description,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873464724&partnerID=40&md5=6caa480134399a6fb7b95a20814ef0ba,"Gouyon F., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Dixon S., Austrian Research Institute for Artificial Intelligence, Vienna, Austria",[No abstract available]
Mountain R.,Name that mood! Describe that tune! Invitation to the IMP,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461576&partnerID=40&md5=9840277a862dfc50b430220ddf3b0702,"Mountain R., Concordia University, Montreal, QC, H4A 3C5, 7141 Sherbrooke St. W, Canada","The ongoing research project The Interactive Multimedia Playroom (IMP) was established to stimulate discourse about issues relating to our perception and description of sounds in artistic and multimedia contexts. Although it was originally conceived to help develop better analytical tools for music, the unique and playful design is well-adapted to helping establish common references for potential collaborators in media arts. As the team working on the project development includes experts in psychology as well as creative artists and theorists, the format of the project is being designed to maximize its transfer to psychological studies. Unlike most psychological studies, however, we are particularly interested in the reactions of those intimately involved in the arts, and ask participants to comment on the suitability of the terminology, perceived relevance of the questions, etc. It is believed that the issues being addressed by the project are fundamental ones which could have high relevance for MIR research, including descriptors, sound-image associations, and the recognition of salient characteristics of a musical excerpt. © 2006 University of Victoria."
Cahill M.; Maidín D.O.,Assessing the performance of melodic similarity algorithms using human judgments of similarity,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423258&partnerID=40&md5=64eecff31242f4d5c96fd68f14dffcb9,"Cahill M., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland; Maidín D.O., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","This paper outlines a project to identify reliable algorithms for measuring melodic similarity by using melodies extracted from a piece of music in Theme and Variations form, for which human judgements of similarity have been gathered. © 2006 University of Victoria."
Castro B.M.; Alonso L.B.N.; Ferneda E.; Da Cunha M.B.; Cruz F.W.; Brandão M.D.C.P.,BDB-MUS: A project for the preservation of Brazilian musical heritage,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449229&partnerID=40&md5=a048cfaf1a8f2dd51d0b45ba80ae112a,"Castro B.M., University of Brasilia, Music Department, Brasília, Brazil; Alonso L.B.N., Catholic University of Brasilia, Knowledge Management and IT, Brasília, Brazil; Ferneda E., Catholic University of Brasilia, Knowledge Management and IT, Brasília, Brazil; Da Cunha M.B., University of Brasilia, Information Science Department, Brasília, Brazil; Cruz F.W., University of Brasilia, Information Science Department, Brasília, Brazil; Brandão M.D.C.P., University of Brasilia, Computer Science Department, Brasília, Brazil","This poster proposes a discussion on concepts evolving from the role of digital libraries on the preservation of tangible and intangible cultural inheritance, including concepts developed in 2003 by UNESCO and the World Summit on the Information Society. It further describes the construction and design process leading to the development of BDB-MUS - Brazilian Digital Music Library, which aims to establish national recommendations on metadata attributions, and to develop means for appropriation and retrieval of musical sources. The poster further explores the concept of digital music or culture within the aims and objectives of the project. © 2006 University of Victoria."
Mohri M.; Moreno P.; Weinstein E.,"Robust music identification, detection, and analysis",2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651107652&partnerID=40&md5=12c957a9f7fc929c1becb4eb8536a64e,"Mohri M., Courant Institute of Mathematical Sciences, New York, NY 10012, 251 Mercer Street, United States, Google Inc., New York, NY 10011, 76 Ninth Avenue, United States; Moreno P., Google Inc., New York, NY 10011, 76 Ninth Avenue, United States; Weinstein E., Courant Institute of Mathematical Sciences, New York, NY 10012, 251 Mercer Street, United States, Google Inc., New York, NY 10011, 76 Ninth Avenue, United States","In previous work, we presented a new approach to music identification based on finite-state transducers and Gaussian mixture models. Here, we expand this work and study the performance of our system in the presence of noise and distortions. We also evaluate a song detection method based on a universal background model in combination with a support vector machine classifier and provide some insight into why our transducer representation allows for accurate identification even when only a short song snippet is available. ©2007 Austrian Computer Society (OCG)."
Skowronek J.; McKinney M.; Par S.V.D.,A demonstrator for automatic music mood estimation,2007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450256709&partnerID=40&md5=81dae786195d118a65196353c5075de4,"Skowronek J., Philips Research Laboratories, Hightech Campus 36, 5656 AE Eindhoven, Netherlands; McKinney M., Philips Research Laboratories, Hightech Campus 36, 5656 AE Eindhoven, Netherlands; Par S.V.D., Philips Research Laboratories, Hightech Campus 36, 5656 AE Eindhoven, Netherlands","Interest in automatic music mood classification is increasing because it could enable people to browse and manage their music collections by means of the music's emotional expression complementary to the widely used music genres. We continue our work on designing a well defined ground-truth database for music mood classification and show a demonstrator of automatic mood estimation. While a subjective evaluation of this algorithm on arbitrary music is ongoing, the initial classification results are encouraging and suggest that an automatic predicition of music mood is possible. ©2007 Austrian Computer Society (OCG)."
Itoyama K.; Kitahara T.; Komatani K.; Ogata T.; Okuno H.G.,Automatic feature weighting in automatic transcription of specified part in polyphonic music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873482135&partnerID=40&md5=21686db8b1986d7f87a0b1c9bdfe7e51,"Itoyama K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Kitahara T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","We studied the problem of automatic music transcription (AMT) for polyphonic music. AMT is an important task for music information retrieval because AMT results enable retrieving musical pieces, high-level annotation, demixing, etc. We attempted to transcribe a part played by an instrument specified by users (specified part tracking). Only two timbre models are required in the specified part tracking to identify the specified musical instrument even when the number of instruments increases. This transcription is formulated into a time-series classification problem with multiple features. We furthermore attempted to automatically estimate weights of the features, because the importance of these features varies for each musical signal. We estimated quasi-optimal weights of the features using a genetic algorithm for each musical signal. We tested our AMT system using trio stereo musical signals. Accuracies with our feature weighting method were 69.8% on average, whereas those without feature weighting were 66.0%. © 2006 University of Victoria."
Van De Par S.; McKinney M.; Redert A.,Musical key extraction from audio using profile training,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872709732&partnerID=40&md5=830dc45e16d7bcf403195a85268e7ad6,"Van De Par S., Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands; McKinney M., Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands; Redert A., Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands","A new method is presented for extracting the musical key from raw audio data. The method is based on the extraction of chromagrams using a new approach for tonal component selection taking into account auditory masking. The extracted chromagrams were used to train three key profiles for major and three key profiles for minor keys. The three trained key profiles differ in their temporal weighting of information across the duration of the song. One profile is based on uniform weighting while the other two apply emphasis on the beginning and ending of the song, respectively. The actual key extraction is based on comparing the key profiles with three average chromagrams that were extracted from a particular piece of music using the same temporal weighting functions as used for the key profile training. A correct key classification of 98% was achieved using non-overlapping test and training sets drawn from a larger set of 237 CD recordings of classical piano sonatas. © 2006 University of Victoria."
Robine M.; Lagrange M.,Evaluation of the technical level of saxophone performers by considering the evolution of spectral parameters of the sound,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849019751&partnerID=40&md5=f653f2a5a9fa1a3c055019fdf83c55d2,"Robine M., SCRIME, LaBRI, Université Bordeaux 1, F-33405 Talence Cedex, 351 cours de la Libération, France; Lagrange M., SCRIME, LaBRI, Université Bordeaux 1, F-33405 Talence Cedex, 351 cours de la Libération, France","We introduce in this paper a new method to evaluate the technical level of a musical performer, by considering only the evolutions of the spectral parameters during one tone. The proposed protocol may be considered as front end for music pedagogy related softwares that intend to provide feedback to the performer. Although this study only considers alto saxophone recordings, the evaluation protocol intends to be as generic as possible and may surely be considered for wider range of classical instruments from winds to bowed strings. © 2006 University of Victoria."
Lidy T.; Rauber A.,Visually profiling radio stations,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873434289&partnerID=40&md5=0a11cab225854c0326788cea8fa51116,"Lidy T., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria","The overwhelming number of radio stations, both online and over the air, makes the choice of an appropriate program difficult. By profiling the program content of radio stations using Self-Organizing Maps we provide a reflection of a station's program type and give potential listeners a visual clue for selecting radio stations. Profiles of current broadcasts indicate which program type a station is currently playing. By creating radio station maps it is possible to directly pick a specific program type instead of having to search for a suitable radio station. © 2006 University of Victoria."
Van Kranenburg P.,Composer attribution by quantifying compositional strategies,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958086951&partnerID=40&md5=ec606548ec7410987799a75891e4e435,"Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Netherlands","Taking a theory of musical style, developed by Leonard B. Meyer, as a starting point, an experiment is described in which statistical pattern recognition algorithms are used to characterize a particular musical style with respect to other styles. The resulting description can be used in authorship discussions. In the current study, a number of disputed organ works from the Bach catalog is used to illustrate the possibilities of this approach. © 2006 University of Victoria."
Whiteley N.; Cemgil A.T.; Godsill S.,Bayesian modelling of temporal structure in musical audio,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549083232&partnerID=40&md5=899d1bdb7825d18f36285694c9b92080,"Whiteley N., Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom; Cemgil A.T., Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom; Godsill S., Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom","This paper presents a probabilistic model of temporal structure in music which allows joint inference of tempo, meter and rhythmic pattern. The framework of the model naturally quantifies these three musical concepts in terms of hidden state-variables, allowing resolution of otherwise apparent ambiguities in musical structure. At the heart of the system is a probabilistic model of a hypothetical 'bar-pointer' which maps an input signal to one cycle of a latent, periodic rhythmical pattern. The system flexibly accommodates different input signals via two observation models: a Poisson points model for use with MIDI onset data and a Gaussian process model for use with raw audio signals. The discrete state-space permits exact computation of posterior probability distributions for the quantities of interest. Results are presented for both observation models, demonstrating the ability of the system to correctly detect changes in rhythmic pattern and meter, whilst tracking tempo. © 2006 University of Victoria."
Neumayer R.; Dittenbach M.; Rauber A.,"Playsom and PocketSOMPlayer, alternative interfaces to large music collections",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873540722&partnerID=40&md5=47fee4ba38a2e893fe8da77e3c66386e,"Neumayer R., Vienna University of Technology, Department of Software Technology and Interactive Systems, A 1040 Wien, Favoritenstr. 9-11 / 188, Austria; Dittenbach M., ECommerce Competence Center, ISpaces Group, A-1220 Wien, Donau-City Strasse 1, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, A 1040 Wien, Favoritenstr. 9-11 / 188, Austria","With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. In this paper we present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information. © 2005 Queen Mary, University of London."
Walser R.Y.,Herding folksongs,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873565127&partnerID=40&md5=733ed035f72977533ba44af7d01b26d1,"Walser R.Y., James Madison Carpenter Project, Elphinstone Institute, University of Aberdeen, Aberdeen, AB24 3EB, 24 High Street, United Kingdom","Cataloging a large, multi-media collection of traditional song and drama in preparation for online presentation highlights issues of song identity and access in the context of contemporary digitized archives. In the James Madison Carpenter collection a particular folksong sung by a particular individual may exist in multiple manifestations: typed song text, sound recording(s), and/or manuscript music notation. While controlled vocabulary references such as Child and Roud numbers provide a degree of identification, such narrative- and text-centric tools are only partly effective in differentiating folkloric materials. Additional means are needed for identifying and controlling folk materials which are distinguished by other aspects of the song such as melody or non-narrative text. The Carpenter project team's experience with Encoded Archival Description (EAD) illustrates the value of this platform-independent, widely recognized standard and suggests opportunities for further developments particularly suited to locating and retrieving folk music materials. © 2005 Queen Mary, University of London."
Bergstra J.; Lacoste A.; Eck D.,Predicting genre labels for artists using FreeDB,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049085544&partnerID=40&md5=49f8a5ec5eb3f3b87611bffadc3136c8,"Bergstra J., Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada; Lacoste A., Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada; Eck D., Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada","This paper explores the value of FreeDB as a source of genre and music similarity information. FreeDB is a public, dynamic, uncurated database for identifying and labelling CDs with album, song, artist and genre information. One quality of FreeDB is that there is high variance in, e.g., the genre labels assigned to a particular disc. We investigate here the ability to use these genre labels to predict a more constrained set of ""canonical"" genres as decided by the curated but private database AllMusic (i.e. multi-class learning). This work is relevant for study in music similarity: we present an automatic, data-driven method for embedding artists in a continuous space that corresponds to genre similarity judgements over a large population of music fans. At the same time, we observe that FreeDB is a valuable resource to researchers developing music classification algorithms; it serves as a reference for what music is popular over a large population, and provides relevant targets for supervised learning algorithms. © 2006 University of Victoria."
Herrera P.; Celma O.; Massaguer J.; Cano P.; Gómez E.; Gouyon F.; Koppenberger M.; García D.; García J.-P.; Wack N.,MUCOSA: A music content semantic annotator,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473006&partnerID=40&md5=1b8c3cad5412fabe50881a17e090e69e,"Herrera P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Celma O., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Massaguer J., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Cano P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Gómez E., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Gouyon F., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Koppenberger M., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; García D., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; García J.-P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Wack N., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain","MUCOSA (Music Content Semantic Annotator) is an environment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with microannotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. acrossfiles annotations), and a collaborative annotation subsystem, which manages large-scale annotation tasks that can be shared among different research centres. The annotation client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes tools for automatic generation of unary descriptors, invention of new descriptors, and propagation of descriptors across sub-collections or playlists. Finally, the collaborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results between several research institutions. A collection of annotated songs is available, as a ""starter pack"" to all the individuals or institutions that are eager to join this initiative. © 2005 Queen Mary, University of London."
Degroeve S.; Tanghe K.; De Baets B.; Leman M.; Martens J.-P.,A simulated annealing optimization of audio features for drum classification,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873550802&partnerID=40&md5=bb01df6bca36fa307670372193bd6b77,"Degroeve S., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Tanghe K., Department of Musicology (IPEM), Ghent University, Belgium; De Baets B., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Leman M., Department of Musicology (IPEM), Ghent University, Belgium; Martens J.-P., Department of Electronics and Information Systems (ELIS), Ghent University, Belgium","Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments. © 2005 Queen Mary, University of London."
Lai C.; Li B.; Fujinaga I.,Preservation digitization of David Edelberg's Handel LP collection: A pilot project,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476874&partnerID=40&md5=da9e8d5bc7a41fca10bc38d32632fd73,"Lai C., Music Technology, Faculty of Music, McGill University, Montreal, Canada; Li B., Music Technology, Faculty of Music, McGill University, Montreal, Canada; Fujinaga I., Music Technology, Faculty of Music, McGill University, Montreal, Canada","This paper describes the digitization process for building an online collection of LPs and the procedure for creating the ground-truth data essential for developing an automated metadata and content capturing system. © 2005 Queen Mary, University of London."
Gerhard D.,Pitch track target deviation in natural singing,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873545056&partnerID=40&md5=ea3fbc53f699bfbb903264e80da77201,"Gerhard D., Department of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada, Department of Music, University of Regina, Regina, SK S4S 0A2, Canada","Unlike fixed-pitch instruments such as the piano, human singing can stray from a target pitch by as much as a semitone while still being perceived as a single fixed note. This paper presents a study of the difference between target pitch and actualized pitch in natural singing. A set of 50 subjects singing the same melody and lyric is used to compare utterance styles. An algorithm for alignment of idealized template pitch tracks to measured frequency tracks is presented. Specific examples are discussed, and generalizations are made with respect to the types of deviations typical in human singing. Demographics, including the skill of the singer, are presented and discussed in the context of the pitch track deviation from the ideal. © 2005 Queen Mary, University of London."
Hazan A.; Grachten M.; Ramirez R.,Evolving performance models by performance similarity: Beyond note-to-note transformations,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950624242&partnerID=40&md5=fe7a2a1deaf5d9f3472fa273c7c8e911,"Hazan A., Music Technology Group, Pompeu Fabra University, 08001 Barcelona, Ocata 1, Spain; Grachten M., Artificial Intelligence Research Institute, Spanish Council for Scientific Research (IIIA - CSIC), Campus UAB, 08193 Bellaterra, Spain; Ramirez R., Music Technology Group, Pompeu Fabra University, 08001 Barcelona, Ocata 1, Spain","This paper focuses on expressive music performance modeling. We induce a population of score-driven performance models using a database of annotated performances extracted from saxophone acoustic recordings of jazz standards. In addition to note-to-note timing transformations that are invariably introduced in human renditions, more extensive alterations that lead to insertions and deletions of notes are usual in jazz performance. In spite of this, inductive approaches usually treat these latter alterations as artifacts. As a first step, we integrate part of the alterations occurring in jazz performances in an evolutionary regression tree model based on strongly typed genetic programming (STGP). This is made possible (i) by creating a new regression data type that includes a range of melodic alterations and (ii) by using a similarity measurement based on an edit-distance fit to human performance similarity judgments. Finally, we present the results of both learning and generalization experiments using a set of standards from the Real Book. © 2006 University of Victoria."
Hamanaka M.; Hirata K.; Tojo S.,ATTA: Automatic time-span tree analyzer based on extended GTTM,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873553238&partnerID=40&md5=0b43a683d4e3a34e252df542a43eeefd,"Hamanaka M., PRESTO, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 1-1-1 Umezono, Japan; Hirata K., NTT Communication Science Laboratories, Keihanna, Science City, Kyoto, 2-4, Hikaridai, Seikacho, Japan; Tojo S., Japan Advanced Institute of Science and Technoloty, Nomi, Ishikawa, 1-1, Asahidai, Japan","This paper describes a music analyzing system called the automatic time-span tree analyzer (ATTA), which we have developed. The ATTA derives a time-span tree that assigns a hierarchy of 'structural importance' to the notes of a piece of music based on the generative theory of tonal music (GTTM). Although the time-span tree has been applied with music summarization and collaborative music creation systems, these systems use time-span trees manually analyzed by experts in musicology. Previous systems based on GTTM cannot acquire a timespan tree without manual application of most of the rules, because GTTM does not resolve much of the ambiguity that exists with the application of the rules. To solve this problem, we propose a novel computational model of the GTTM that re-formalizes the rules with computer implementation. The main advantage of our approach is that we can introduce adjustable parameters, which enables us to assign priority to the rules. Our analyzer automatically acquires time-span trees by configuring the parameters that cover 26 rules out of 36 GTTM rules for constructing a time-span tree. Experimental results showed that after these parameters were tuned, our method outperformed a baseline performance. We hope to distribute the time-span tree as the content for various musical tasks, such as searching and arranging music. © 2005 Queen Mary, University of London."
Wen X.; Sandler M.,A partial searching algorithm and its application for polyphonic music transcription,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873549899&partnerID=40&md5=8c1db6da15796a65136358d8e235ec12,"Wen X., Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom","This paper proposes an algorithm for studying spectral contents of pitched sounds in real-world recordings. We assume that the 2nd-order difference, w.r.t. partial index, of a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. Given a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be associated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the algorithm to make it work with reverberant background, such as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level. © 2005 Queen Mary, University of London."
McKay C.; Fiebrink R.; McEnnis D.; Li B.; Fujinaga I.,ACE: A framework for optimizing music classification,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873552024&partnerID=40&md5=6909337a7495b05467d61e3caeca1566,"McKay C., Music Technology, McGill University, Montreal, Canada; Fiebrink R., Music Technology, McGill University, Montreal, Canada; McEnnis D., Music Technology, McGill University, Montreal, Canada; Li B., Music Technology, McGill University, Montreal, Canada; Fujinaga I., Music Technology, McGill University, Montreal, Canada","This paper presents ACE (Autonomous Classification Engine), a framework for using and optimizing classifiers. Given a set of feature vectors, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality reduction techniques in order to arrive at a good configuration for the problem at hand. In addition to evaluating classification methodologies in terms of success rates, functionality is also being incorporated into ACE allowing users to specify constraints on training and classification times as well as on the amount of time that ACE has to arrive at a solution. ACE is designed to facilitate classification for those new to pattern recognition as well as provide flexibility for those with more experience. ACE is packaged with audio and MIDI feature extraction software, although it can certainly be used with existing feature extractors. This paper includes a discussion of ways in which existing general-purpose classification software can be adapted to meet the needs of music researchers and shows how these ideas have been implemented in ACE. A standardized XML format for communicating features and other information to classifiers is proposed. A special emphasis is placed on the potential of classifier ensembles, which have remained largely untapped by the MIR community to date. A brief theoretical discussion of ensemble classification is presented in order to promote this powerful approach. © 2005 Queen Mary, University of London."
Doraisamy S.; Adnan H.; Norowi N.M.,Towards a MIR system for Malaysian music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549105544&partnerID=40&md5=1e24eab54589a60cf88b34baf72ab0f2,"Doraisamy S., Dept. of Multimedia, Faculty of Comp. Sc. and IT, University Putra Malaysia, Malaysia; Adnan H., Dept. of Music, National Arts Academy, Ministry of Culture, Arts and Heritage, Malaysia; Norowi N.M., Dept. of Multimedia, Faculty of Comp. Sc. and IT, University Putra Malaysia, Malaysia","Systems for the archival of musical documents digitally and development of digital music libraries are currently being researched and developed extensively. However, adapting these systems for the archival and retrieval of Malaysian music materials might not be as straightforward due to the distinct differences in musical structure and modes of non-Western music. This paper covers the motivations for the creation of a MIR system for Malaysian Music and outlines the plans for its development. © 2006 University of Victoria."
Winget M.,Heroic frogs save the bow: Performing musician's annotation and interaction behavior with written music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866296333&partnerID=40&md5=8acaf8cb65632c9df6bbd503b3bfffaa,"Winget M., ISchool, University of Texas at Austin, 1 University Station, D7000, Austin, TX 78712-0390, United States","Although there have been a number of fairly recent studies in which researchers have explored the information seeking and management behaviors of people interacting with musical retrieval systems, there have been very few published studies of the interaction and use behaviors of musicians themselves. The qualitative research study reported here seeks to correct this deficiency in the literature. Drawing on data collected from nearly 300 annotated parts representing 15 unique works, and 20 musician interviews, we make a number of functionality recommendations for constructive music digital library tool development. For example, all musicians annotate their written music, although this action seems to become more important as the musician becomes more skilled. Musicians' annotations are comprehensible to anyone who can read music, and are valuable as records of interpretation, interaction, and performance. Musicians annotate at the note (rather than at the phrase or movement) level, their annotations are standardized and formal, and are largely non-text. Music digital libraries that cater to musicians should attempt to provide annotation tools that work at the micro level, and extend the symbolic language of the primary document. Furthermore, preserving the annotations for future use would prove valuable for performance students, professionals, and historians alike. © 2006 University of Victoria."
Lehn-Schiøler T.; Arenas-García J.; Petersen K.B.; Hansen L.K.,A genre classification plug-in for data collection,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849113023&partnerID=40&md5=9f7fe7bf0f2185592c6b340f50565ac7,"Lehn-Schiøler T., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark; Arenas-García J., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark; Petersen K.B., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark; Hansen L.K., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark",This demonstration illustrates how the methods developed in the MIR community can be used to provide real-time feedback to music users. By creating a genre classifier plugin for a popular media player we present users with relevant information as they play their songs. The plug-in can furthermore be used as a data collection platform. After informed consent from a selected set of users the plug-in will report on music consumption behavior back to a central server. © 2006 University of Victoria.
Chai W.; Vercoe B.,Detection of key change in classical piano music,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873544235&partnerID=40&md5=ae44b88f2168b159b684b7e945588aba,"Chai W., MIT Media Laboratory, Cambridge MA, United States; Vercoe B., MIT Media Laboratory, Cambridge MA, United States","onality is an important aspect of musical structure. Detecting key of music is one of the major tasks in tonal analysis and will benefit semantic segmentation of music for indexing and searching. This paper presents an HMM-based approach for segmenting musical signals based on key change and identifying the key of each segment. Classical piano music was used in the experiment. The performance, evaluated by three proposed measures (recall, precision and label accuracy), demonstrates the promise of the method. © 2005 Queen Mary, University of London."
McKay C.; McEnnis D.; Fujinaga I.,A large publicly accessible prototype audio database for music research,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849086365&partnerID=40&md5=17933b837dcb39c11bc00d2cda6cbb09,"McKay C., McGill University, Montreal, QC, Canada; McEnnis D., McGill University, Montreal, QC, Canada; Fujinaga I., McGill University, Montreal, QC, Canada","This paper introduces Codaich, a large and diverse publicly accessible database of musical recordings for use in music information retrieval (MIR) research. The issues that must be dealt with when constructing such a database are discussed, as are ways of addressing these problems. It is suggested that copyright restrictions may be overcome by allowing users to make customized feature extraction queries rather than allowing direct access to recordings themselves. The jMusicMetaManager software is introduced as a tool for improving metadata associated with recordings by automatically detecting inconsistencies and redundancies. © 2006 University of Victoria."
Burgoyne J.A.; Saul L.K.,Learning harmonic relationships in digital audio with dirichlet-based hidden Markov models,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873542451&partnerID=40&md5=c175cbd2bef47fd1960f1c2e2ec3e825,"Burgoyne J.A., Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, United States; Saul L.K., Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, United States","Harmonic analysis is a standard musicological tool for understanding many pieces of Western classical music and making comparisons among them. Traditionally, this analysis is done on paper scores, and most past research in machine-assisted analysis has begun with digital representations of them. Human music students are also taught to hear their musical analyses, however, in both musical recordings and performances. Our approach attempts to teach machines to do the same, beginning with a corpus of recorded Mozart symphonies. The audio files are first transformed into an ordered series of normalized pitch class profile (PCP) vectors. Simplified rules of tonal harmony are encoded in a transition matrix. Classical music tends to change key more frequently than popular music, and so these rules account not only for chords, as most previous work has done, but also for the keys in which they function. A hidden Markov model (HMM) is used with this transition matrix to train Dirichlet distributions for major and minor keys on the PCP vectors. The system tracks chords and keys successfully and shows promise for a real-time implementation. © 2005 Queen Mary, University of London."
Pampalk E.; Pohle T.; Widmer G.,Dynamic playlist generation based on skipping behavior,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533505&partnerID=40&md5=687e5aa1151d7b736f5f83820a39a421,"Pampalk E., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Pohle T., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","Common approaches to creating playlists are to randomly shuffle a collection (e.g. iPod shuffle) or manually select songs. In this paper we present and evaluate heuristics to adapt playlists automatically given a song to start with (seed song) and immediate user feedback. Instead of rich metadata we use audio-based similarity. The user gives feedback by pressing a skip button if the user dislikes the current song. Songs similar to skipped songs are removed, while songs similar to accepted ones are added to the playlist. We evaluate the heuristics with hypothetical use cases. For each use case we assume a specific user behavior (e.g. the user always skips songs by a particular artist). Our results show that using audio similarity and simple heuristics it is possible to drastically reduce the number of necessary skips. © 2005 Queen Mary, University of London."
Paiement J.-F.; Eck D.; Bengio S.,A probabilistic model for chord progressions,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873532933&partnerID=40&md5=d5bbef18face349d7a8b0b70f1aedec1,"Paiement J.-F., IDIAP Research Institute, CH-1920 Martigny, Rue du Simplon 4, Switzerland; Eck D., Dep. of Computer Science and Operations, Research University of Montreal, Montréal, QC H3C 3J7, CP 6128 succ Centre-Ville, Canada; Bengio S., IDIAP Research Institute, CH-1920 Martigny, Rue du Simplon 4, Switzerland","Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progressions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies. © 2005 Queen Mary, University of London."
Oliver N.; Kreger-Stickles L.,PAPA: Physiology and purpose-aware automatic playlist generation,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416236&partnerID=40&md5=f8a836a9f21a700bec53e06a9fb8027c,"Oliver N., Microsoft Research, Redmond, WA 98052, One Microsoft Way, United States; Kreger-Stickles L., Microsoft Research, Redmond, WA 98052, One Microsoft Way, United States","In this paper we present PAPA, a novel approach for automatically generating playlists. The proposed framework utilizes the user's physiological response to music, together with traditional song meta-data to generate a playlist the user will not only enjoy, but which will assist him or her in achieving various user-defined goals (""purpose""). In addition to outlining the generic framework, we present an exemplary application named MPTrain that (1) creates a playlist in real-time to assist users in achieving specific exercise goals; and (2) incorporates the user's physiological response to the music to determine the next song to play. © 2006 University of Victoria."
Ruppin A.; Yeshurun H.,MIDI music genre classification by invariant features,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349120625&partnerID=40&md5=3d5d9fe3f00f10a33a65d18d58864b78,"Ruppin A., School of Computer Science, Tel Aviv University, Israel; Yeshurun H., School of Computer Science, Tel Aviv University, Israel","MIDI music genre classification methods are largely based on generic text classification techniques. We attempt to leverage music domain knowledge in order to improve classification results. We combine techniques of selection and extraction of musically invariant features with classification using compression distance similarity metric, which is an approximation of the theoretical, yet computationally intractable, Kolmogorov complexity. We introduce several methods for extracting features which are invariant under certain transformations commonly found in music. These methods, combined with data compression, generate a lossy compressed representation which attempts to preserve feature invariance. We analyze the performance of each method, thus gaining insight into the features that are significant to the human perception of music. © 2006 University of Victoria."
Peeters G.,Rhythm classification using spectral rhythm patterns,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873556216&partnerID=40&md5=aa9903154e56fd5da9b7529ef211ed22,"Peeters G., IRCAM, Sound Analysis/Synthesis Team, 75004 Paris, 1, pl. Igor Stravinsky, France","In this paper, we study the use of spectral patterns to represent the characteristics of the rhythm of an audio signal. A function representing the position of onsets over time is first extracted from the audio signal. From this function we compute at each time a vector which represents the characteristics of the local rhythm. Three feature sets are studied for this vector. They are derived from the amplitude of the Discrete Fourier Transform, the Auto- Correlation Function and the product of the DFT and of a Frequency-Mapped ACF. The vectors are then sampled at some specific frequencies, which represents various ratios of the local tempo. The ability of the three feature sets to represent the rhythm characteristics of an audio item is evaluated through a classification task. We show that using such simple spectral representations allows obtaining results comparable to the state of the art. © 2005 Queen Mary, University of London."
Lübbers D.,SoniXplorer: Combining visualization and auralization for content-based exploration of music collections,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873549800&partnerID=40&md5=4e1418ea9955aad9705b4d64e786f968,"Lübbers D., Lehrstuhl Informatik v, RWTH Aachen University, 52072 Aachen, Ahornstr. 55, Germany","Music can be described best by music. However, current research in the design of user interfaces for the exploration of music collections has mainly focused on visualization aspects ignoring possible benefits from spatialized music playback. We describe our first development steps towards two novel user-interface designs: The Sonic Radar arranges a fixed number of prototypes resulting from a content-based clustering process in a circle around the user's standpoint. To derive an auralization of the scene, we introduce the concept of an aural focus of perception that adapts well-known principles from the visual domain. The Sonic SOM is based on Kohonen's Self-Organizing Map. It helps the user in understanding the structure of his music collection by positioning titles on a two-dimensional grid according to their high-dimensional similarity. We show how our auralization concept can be adapted to extend this visualization technique and thereby support multimodal navigation. © 2005 Queen Mary, University of London."
Liang W.; Zhang S.; Xu B.,A hierarchical approach for audio stream segmentation and classification,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873545843&partnerID=40&md5=8c01bcfdfe1f8f2c2705269f207b6d54,"Liang W., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Zhang S., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Xu B., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","This paper describes a hierarchical approach for fast audio stream segmentation and classification. With this approach, the audio stream is firstly segmented into audio clips by MBCR (Multiple sub-Bands spectrum Centroid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips into six pre-defined categories in terms of discriminative background sounds, which is pure speech, pure music, song, speech with music, speech with noise and silence. The experiments on real TV program recordings showed that this approach has higher accuracy and recall rate for audio classification with a fast speed under noise environments. © 2005 Queen Mary, University of London."
Pugin L.,Optical music recognition of early typographic prints using Hidden Markov Models,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348985923&partnerID=40&md5=905c6d1ff813a1c801deb5bffdbee3df,"Pugin L., Music Technology Area, Schulich School of Music, McGill University, Montreal, Canada","Music printed with movable type (typographic music) from the 16th and 17th centuries contains specific graphic features. In this paper, we present a technique and associated experiments for performing optical music recognition on such music prints using Hidden Markov Models (HMM). Our original approach avoids the difficult and unreliable removal of staff lines usually required before processing. The modeling of symbols on the staff is based on low-level simple features. We show that, using our technique, these features are robust enough to obtain good recognition rates even with poor quality images scanned from microfilm of originals. The music content retrieved by the optical recognition process can be put to significant use in, for example, the creation of searchable digital music libraries. © 2006 University of Victoria."
Arenas-García J.; Larsen J.; Hansen L.K.; Meng A.,Optimal filtering of dynamics in short-time features for music organization,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549082216&partnerID=40&md5=7bfa1fed69b827841248e2d1da19ec18,"Arenas-García J., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark; Larsen J., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark; Hansen L.K., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark; Meng A., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark","There is an increasing interest in customizable methods for organizing music collections. Relevant music characterization can be obtained from short-time features, but it is not obvious how to combine them to get useful information. In this work, a novel method, denoted as the Positive Constrained Orthonormalized Partial Least Squares (POPLS), is proposed. Working on the periodograms of MFCCs time series, this supervised method finds optimal filters which pick up the most discriminative temporal information for any music organization task. Two examples are presented in the paper, the first being a simple proof-of-concept, where an altosax with and without vibrato is modelled. A more complex 11 music genre classification setup is also investigated to illustrate the robustness and validity of the proposed method on larger datasets. Both experiments showed the good properties of our method, as well as superior performance when compared to a fixed filter bank approach suggested previously in the MIR literature. We think that the proposed method is a natural step towards a customized MIR application that generalizes well to a wide range of different music organization tasks. © 2006 University of Victoria."
Laplante A.; Downie J.S.,Everyday life music information-seeking behaviour of young adults,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951199427&partnerID=40&md5=78465f30df1c793cba902aa925c063a8,"Laplante A., McGill University, Graduate School of Library and Information Studies, Canada; Downie J.S., University of Illinois, Graduate School of Library and Information Science, Urbana-Champaign, United States","This poster presents the preliminary results of an ongoing qualitative study on the everyday-life music information seeking behaviour of young adults. The data were collected through in-depth interviews and analyzed following a grounded theory approach. The analysis showed a strong penchant for informal channels (e.g., friends, relative) and, conversely, a distrust of experts. It also emerged that music seeking was mostly motivated by curiosity rather than by actual information needs, which in turn explains why browsing is such a popular strategy. © 2006 University of Victoria."
Temperley D.,A probabilistic model of melody perception,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549033114&partnerID=40&md5=8a2429bb4deba735c5b6ce1264a6f2a9,"Temperley D., Eastman School of Music, Rochester, NY 14604, 26 Gibbs St., United States","This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. (A ""melody"" is defined here as a sequence of pitches, without rhythmic information.) The model uses Bayesian reasoning. A generative probabilistic model is proposed, based on three principles: 1) melodies tend to remain within a narrow pitch range; 2) note-to-note intervals within a melody tend to be small; 3) notes tend to conform to a distribution (or ""key-profile"") that depends on the key. The model is tested in three ways: on a key-finding task, on a melodic expectation task, and on an error-detection task. © 2006 University of Victoria."
Essid S.; Richard G.; David B.,Inferring efficient hierarchical taxonomies for MIR tasks: Application to musical instruments,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873543378&partnerID=40&md5=f3935ed1a38ed5d1b16796551f8a3408,"Essid S., GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France; Richard G., GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France; David B., GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France","A number of approaches for automatic audio classification are based on hierarchical taxonomies since it is acknowledged that improved performance can be thereby obtained. In this paper, we propose a new strategy to automatically acquire hierarchical taxonomies, using machine learning methods, which are expected to maximize the performance of subsequent classification. It is shown that the optimal hierarchical taxonomy of musical instruments (in the sense of inter-class distances) does not follow the traditional and more intuitive instrument classification into instrument families. © 2005 Queen Mary, University of London."
Hu X.; Downie J.S.; West K.; Ehmann A.,Mining music reviews: Promising preliminary results,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873531799&partnerID=40&md5=e49393d424ab0606f171a86d326fba62,"Hu X., GSLIS, University of Illinois at Urbana-Champaign, United States; Downie J.S., GSLIS, University of Illinois at Urbana-Champaign, United States; West K., School of Computing Sciences, University of East Anglia, United Kingdom; Ehmann A., Electrical Engineering, University of Illinois at Urbana-Champaign, United States","In this paper we present a system for the automatic mining of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genre of the music reviewed and to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically mine arbitrary bodies of text, such as weblogs (blogs) for musically relevant information. © 2005 Queen Mary, University of London."
Casagrande N.; Eck D.; Kégl B.,Frame-level speech/music discrimination using AdaBoost,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873550748&partnerID=40&md5=78788946a659f18f50e31296bbed908f,"Casagrande N., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; Eck D., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; Kégl B., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada","In this paper we adapt an AdaBoost-based image processing algorithm to the task of predicting whether an audio signal contains speech or music. We derive a frame-level discriminator that is both fast and accurate. Using a simple FFT and no built-in prior knowledge of signal structure we obtain an accuracy of 88% on frames sampled at 20ms intervals. When we smooth the output of the classifier with the output of the previous 40 frames our forecast rate rises to 93% on the Scheirer-Slaney (Scheirer and Slaney, 1997) database. To demonstrate the efficiency and effectiveness of the model, we have implemented it as a graphical real-time plugin to the popular Winamp audio player. © 2005 Queen Mary, University of London."
Pickens J.,Classifier combination for capturing musical variation,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873549357&partnerID=40&md5=2cde14cd73869e0324c637921865feb4,"Pickens J., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom","At its heart, music information retrieval is characterized by the need to find the similarity between pieces of music. However, .similar. does not mean .the same.. Therefore, techniques for approximate matching are crucial to the development of good music information retrieval systems. Yet as one increases the level of approximation, one finds not only additional similar, relevant music, but also a larger number of not-as-similar, non-relevant music. The purpose of this work is to show that if two different retrieval systems do approximate matching in different manners, and both give decent results, they can be combined to give results better than either system individually. One need not sacrifice accuracy for the sake of fiexibility. © 2005 Queen Mary, University of London."
Kirlin P.B.; Utgoff P.E.,VoiSe: Learning to segregate voices in explicit and implicit polyphony,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873548940&partnerID=40&md5=e35ccfeb0af3b9e58b4540dda29e7dfe,"Kirlin P.B., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States; Utgoff P.E., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States","Finding multiple occurrences of themes and patterns in music can be hampered due to polyphonic textures. This is caused by the complexity of music that weaves multiple independent lines of music together. We present and demonstrate a system, VoiSe, that is capable of isolating individual voices in both explicit and implicit polyphonic music. VoiSe is designed to work on a symbolic representation of a music score, and consists of two components: a same-voice predicate implemented as a learned decision tree, and a hard-coded voice numbering algorithm. © 2005 Queen Mary, University of London."
Nichols E.; Raphael C.,Globally optimal audio partitioning,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956672342&partnerID=40&md5=3ca9984b91ed6dc07ba9fac454ae83a5,"Nichols E., Dept. of Computer Science, Indiana Univ., United States; Raphael C., School of Informatics, Indiana Univ., United States","We present a technique for partitioning an audio file into maximally-sized segments having nearly uniform spectral content, ideally corresponding to notes or chords. Our method uses dynamic programming to globally optimize a measure of simplicity or homogeneity of the intervals in the partition. Here we have focused on an entropy-like measure, though there is considerable flexibility in choosing this measure. Experiments are presented for several musical scenarios1. © 2006 University of Victoria."
Lee J.H.; Jones M.C.; Downie J.S.,Factors affecting response rates for real-life MIR queries,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873418427&partnerID=40&md5=b2706fc7b95974c80f8f972c305b44b6,"Lee J.H., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Jones M.C., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States",In this poster we present preliminary findings of an exploratory study of natural language music information queries posted to the Google Answers web site. We discuss the proportion of queries answered as a function of time and attempt to identify factors which affect the probability of a query being answered. © 2006 University of Victoria.
Collins N.,Using a pitch detector for onset detection,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873551830&partnerID=40&md5=b9b72631b3c9b82a3bd52d0621333651,"Collins N., University of Cambridge, Centre for Music and Science, Cambridge, CB3 9DP, 11 West Road, United Kingdom","A segmentation strategy is explored for monophonic instrumental pitched non-percussive material (PNP) which proceeds from the assertion that human-like event analysis can be founded on a notion of stable pitch percept. A constant-Q pitch detector following the work of Brown and Puckette provides pitch tracks which are post processed in such a way as to identify likely transitions between notes. A core part of this preparation of the pitch detector signal is an algorithm for vibrato suppression. An evaluation task is undertaken on slow attack and high vibrato PNP source files with human annotated onsets, exemplars of a difficult case in monophonic source segmentation. The pitch track onset detection algorithm shows an improvement over the previous best performing algorithm from a recent comparison study of onset detectors. Whilst further timbral cues must play a part in a general solution, the method shows promise as a component of a note event analysis system. © 2005 Queen Mary, University of London."
Goto M.; Goto T.,"Musicream: New music playback interface for streaming, sticking, sorting, and recalling musical pieces",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873539321&partnerID=40&md5=4cff6b96fa3d397c928d57f07d4284d8,"Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Goto T., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","This paper describes a novel music playback interface, called Musicream, which lets a user unexpectedly come across various musical pieces similar to those liked by the user. With most previous ""query-by-example"" interfaces used for similarity-based searching, for the same query and music collection a user will always receive the same list of musical pieces ranked by their similarity and opportunities to encounter unfamiliar musical pieces in the collection are limited. Musicream facilitates active, flexible, and unexpected encounters with musical pieces by providing four functions: the music-disc streaming function which creates a flow of many musical-piece entities (discs) from a (huge) music collection, the similaritybased sticking function which allows a user to easily pick out and listen to similar pieces from the flow, the metaplaylist function which can generate a playlist of playlists (ordered lists of pieces) while editing them with a high degree of freedom, and the time-machine function which automatically records all Musicream activities and allows a user to visit and retrieve a past state as if using a time machine. In our experiments, these functions were used seamlessly to achieve active and creative querying and browsing of music collections, confirming the effectiveness of Musicream. © 2005 Queen Mary, University of London."
Bruderer M.J.; McKinney M.; Kohlrausch A.,Structural boundary perception in popular music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149149378&partnerID=40&md5=7ccd0272975f24804da7fda17ce980fd,"Bruderer M.J., Technische Universiteit Eindhoven, 5600 Eindhoven, Postbus 513, Netherlands; McKinney M., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4 (WO-02), Netherlands; Kohlrausch A., Technische Universiteit Eindhoven, 5600 Eindhoven, Postbus 513, Netherlands, Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4 (WO-02), Netherlands","The automatic extraction of musical structure from audio is an important aspect for many music information retrieval (MIR) systems. The criteria on which structural elements in music are defined in MIR systems is often not clearly stated but typically stem from (music) theoretical or signal-based properties. In many cases, however, perceptual-based criteria are the most relevant and systems need to be trained on or modeled after the perception of structural elements in music. Here, we investigate the perception of structural boundaries to Western popular music and examine the musical cues responsible for their perception. We make links to music theoretical descriptions of structural boundaries and to computational methods for extracting structure. The methods and data presented here are useful for developing and training systems for the automatic extraction of musical structure as it is perceived by listeners. © 2006 University of Victoria."
Mesaros A.; Astola J.,The Mel-Frequency Cepstral Coefficients in the context of singer identification,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873530192&partnerID=40&md5=703f059ec550961d0c2091ed95e5bcd6,"Mesaros A., Technical University of Cluj Napoca, Communications Department, Cluj Napoca, Romania, Institute of Signal Processing, Tampere University of Technology, Tampere, Finland; Astola J., Institute of Signal Processing, Tampere University of Technology, Tampere, Finland","The singing voice is the oldest and most complex musical instrument. A familiar singer's voice is easily recognizable for humans, even when hearing a song for the first time. On the other hand, for automatic identification this is a difficult task among sound source identification applications. The signal processing techniques aim to extract features that are related to identity characteristics. The research presented in this paper considers 32 Mel-Frequency Cepstral Coefficients in two subsets: the low order MFCCs characterizing the vocal tract resonances and the high order MFCCs related to the glottal wave shape. We explore possibilities to identify and discriminate singers using the two sets. Based on the results we can affirm that both subsets have their contribution in defining the identity of the voice, but the high order subset is more robust to changes in singing style. © 2005 Queen Mary, University of London."
Novello A.; McKinney M.F.; Kohlrausch A.,Perceptual evaluation of music similarity,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960936062&partnerID=40&md5=1c7f568c512bb6f1c858af9f833406bf,"Novello A., Philips Research Laboratories, Eindhoven, Netherlands; McKinney M.F., Philips Research Laboratories, Eindhoven, Netherlands; Kohlrausch A., Philips Research Laboratories, Eindhoven, Netherlands, Human Technology Interaction, Technische Universiteit Eindhoven, Eindhoven, Netherlands","This paper presents an empirical method for assessing music similarity on a set of stimuli using triadic comparisons in a balanced incomplete block design. We first evaluated the consistency of subjects in their rankings and then the concordance across subjects. The concordance was also evaluated for different subject populations to assess the influence of experience of the subject with the musical material. We finally analysed subjects' ranking by the means of multidimensional scaling. Similarity judgments were found to be rather concordant across subjects. Significant differences between musicians and non-musicians and between subjects being familiar or non-familiar with the music were found for a small number of cases. Multidimensional scaling reveals a proximity of songs belonging to the same genre, congruent with the idea of genre being a perceptual dimension in subjects' similarity ranking. © 2006 University of Victoria."
Vembu S.; Baumann S.,Separation of vocals from polyphonic audio recordings,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873538214&partnerID=40&md5=26513d2047f384c6853a8358fc002df0,"Vembu S., German Research Centre for AI, 67663 Kaiserslautern, Erwin-Schroedinger-Str. 57, Germany; Baumann S., German Research Centre for AI, 67663 Kaiserslautern, Erwin-Schroedinger-Str. 57, Germany","Source separation techniques like independent component analysis and the more recent non-negative matrix factorization are gaining widespread use for the monaural separation of individual tracks present in a music sample. The underlying principle behind these approaches characterises only stationary signals and fails to separate nonstationary sources like speech or vocals. In this paper, we make an attempt to solve this problem and propose solutions to the extraction of vocal tracks from polyphonic audio recordings. We also present techniques to identify vocal sections in a music sample and design a classifier to perform a vocal-nonvocal segmentation task. Finally, we describe an application wherein we try to extract the melody from the separated vocal track using existing monophonic transcription techniques. The experimental work leads us to the conclusion that the quality of vocal source separation, albeit satisfactory, is not sufficient enough for further F0 analysis to extract the melody line from the vocal track. We identify areas that need further investigation to improve the quality of vocal source separation. © 2005 Queen Mary, University of London."
Downie J.S.; West K.; Ehmann A.; Vincent E.,The 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005): Preliminary overview,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873563532&partnerID=40&md5=ea43331e38649defd76041b719aa6d42,"Downie J.S., GSLIS, University of Illinois at Urbana-Champaign, United States; West K., School of Computing Sciences, University of East Anglia, United Kingdom; Ehmann A., Electrical Engineering, University of Illinois at Urbana-Champaign, United States; Vincent E., Electronic Engineering, Queen Mary University of London, United Kingdom","This paper is an extended abstract which provides a brief preliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper concludes with a listing of targets items to be undertaken before MIREX 2006 to ensure the ongoing success of the MIREX framework. © 2005 Queen Mary, University of London."
Lidy T.; Rauber A.,Evaluation of feature extractors and psycho-acoustic transformations for music genre classification,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873536955&partnerID=40&md5=a1840af629d22735f583970819d7231a,"Lidy T., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria","We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy. © 2005 Queen Mary, University of London."
Riley J.,Exploiting musical connections: A proposal for support of work relationships in a Digital Music Library,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533846&partnerID=40&md5=3e9a8c0a80a55a785c22b9fe14945ee8,"Riley J., Indiana University Digital Library Program, Bloomington, IN 47404, 1320 E 10th St, E170, United States","Musical works in the Western art music tradition exist in a complex, inter-related web. Works that are derivative or part of another work are common; however, most music information retrieval systems, including traditional library catalogs, don't use these essential relationships to improve search results or provide information about them to end-users. As part of the NSF-funded Variations2 Digital Music Library project at Indiana University, we have developed a set of functional requirements defining how derivative and whole/part relationships between musical works should be acted upon in search results, and how these results should be displayed. This paper describes recent research into these relationships, provides examples why they are important in Western art music, outlines how Variations2 or any other music information retrieval system could use these relationships in matching user queries, and describes optimal displays of these relationships to end-users. © 2005 Queen Mary, University of London."
West K.; Cox S.,Finding an optimal segmentation for audio genre classification,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873531705&partnerID=40&md5=7bd071c4cb768ede45e4dc2fba692ae3,"West K., School of Computing Sciences, University of East Anglia, Norwich, NR4 7TJ, United Kingdom; Cox S., School of Computing Sciences, University of East Anglia, Norwich, NR4 7TJ, United Kingdom","In the automatic classification of music many different segmentations of the audio signal have been used to calculate features. These include individual short frames (23 ms), longer frames (200 ms), short sliding textural windows (1 sec) of a stream of 23 ms frames, large fixed windows (10 sec) and whole files. In this work we present an evaluation of these different segmentations, showing that they are sub-optimal for genre classification and introduce the use of an onset detection based segmentation, which appears to outperform all of the fixed and sliding windows segmentation schemes in terms of classification accuracy and model size. © 2005 Queen Mary, University of London."
Hu X.; Downie J.S.; Ehmann A.F.,Exploiting recommended usage metadata: Exploratory analyses,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849091064&partnerID=40&md5=05e826d31b6df7b6dac33a54dfa474a2,"Hu X., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign, United States","In this paper, we conduct a series of exploratory analyses on the user-recommended usages of music as generated by 1,042 reviewers who have posted to www.epinions.com. Using hierarchical clustering methods on data derived from the co-occurrence analyses of usage and genre, usage and artist, and usage and album, we are able to conclude that further investigation of user-recommended usage metadata is warranted, especially with regard to its implications for future iterations of the Music Information Retrieval Evaluation eXchange (MIREX). © 2006 University of Victoria."
Byrd D.; Schindele M.,Prospects for improving OMR with multiple recognizers,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56549094127&partnerID=40&md5=9e74f44ca856011c41449eba8d1d3999,"Byrd D., School of Informatics, Indiana University, Bloomington, United States, School of Music, Indiana University, Bloomington, United States; Schindele M., School of Music, Indiana University, Bloomington, United States","OMR (Optical Music Recognition) programs have been available for years, but they still leave much to be desired in terms of accuracy. We studied the feasibility of achieving substantially better accuracy by using the output of several programs to ""triangulate"" and get better results than any of the individual programs; this multiple-recognizer approach has had some success with other media but, to our knowledge, has never been tried for music. A major obstacle is that the complexity of music notation is such that evaluating OMR accuracy is difficult for any but the simplest music. Nonetheless, existing programs have serious enough limitations that the multiple-recognizer approach is promising. © 2006 University of Victoria."
Mandel M.I.; Ellis D.P.W.,Song-level features and support vector machines for music classification,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873528643&partnerID=40&md5=5c3fde2d321ca5571816a8bd54b6eb90,"Mandel M.I., LabROSA, Dept. of Elec. Eng., Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Dept. of Elec. Eng., Columbia University, New York, NY, United States","Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplarbased classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums. © 2005 Queen Mary, University of London."
Cantone D.; Cristofaro S.; Faro S.,"Solving the (δ, α)-approximate matching problem under transposition invariance in musical sequences",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873563422&partnerID=40&md5=7493d636f9c572501dd4314333bc5825,"Cantone D., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Cristofaro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Faro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy","The δ-approximate matching problem arises in many questions concerning musical information retrieval and musical analysis. In the case in which gaps are not allowed between consecutive pitches of the melody, transposition invariance is automatically taken care of, provided that the musical melodies are encoded using the pitch interval encoding. However, in the case in which nonnull gaps are allowed between consecutive pitches of the melodies, transposition invariance is not dealt with properly by the algorithms present in literature. In this paper, we propose two slightly different variants of the approximate matching problem under transposition invariance and for each of them provide an algorithm, obtained by adapting an efficient algorithm for the δ-approximate matching problem with α-bounded gaps. © 2005 Queen Mary, University of London."
McEnnis D.; McKay C.; Fujinaga I.; Depalle P.,Audio: A feature extraction library,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533157&partnerID=40&md5=8b39cbf46adc5387120a5207aa33aca5,"McEnnis D., Faculty of Music, McGill University, Montreal, Canada; McKay C., Faculty of Music, McGill University, Montreal, Canada; Fujinaga I., Faculty of Music, McGill University, Montreal, Canada; Depalle P., Faculty of Music, McGill University, Montreal, Canada","jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straight forward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio files as input. In the GUI, users select the features that they wish to have extracted-letting jAudio take care of all dependency problems-and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML file or an ARFF file depending on the user's preference. © 2005 Queen Mary, University of London."
Cabral G.; Pachet F.; Briot J.-P.,Automatic X traditional descriptor extraction: The case of chord recognition,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873560554&partnerID=40&md5=6524c7c671542501318c5967a486c2f7,"Cabral G., LIP6 - Paris 6, 75018 Paris, 8 Rue du Capitaine Scott, France; Pachet F., Sony CSL Paris, 75005 Paris, 6 Rue Amyot, France; Briot J.-P., LIP6 - Paris 6, 75018 Paris, 8 Rue du Capitaine Scott, France","Audio descriptor extraction is the activity of finding mathematical models which describe properties of the sound, requiring signal processing skills. The scientific literature presents a vast collection of descriptors (e.g. energy, tempo, tonality) each one representing a significant effort of research in finding an appropriate descriptor for a particular application. The Extractor Discovery System (EDS) [1] is a recent approach for the discovery of such descriptors, which aim is to extract them automatically. This system can be useful for both non experts - who can let the system work fully automatically - and experts - who can start the system with an initial solution expecting it to enhance their results. Nevertheless, EDS still needs to be massively tested. We consider that its comparison with the results of problems already studied would be very useful to validate it as an effective tool. This work intends to perform the first part of this validation, comparing the results from classic approaches with EDS results when operated by a completely naïve user building a guitar chord recognizer. © 2005 Queen Mary, University of London."
Abdallah S.; Noland K.; Sandler M.; Casey M.; Rhodes C.,Theory and evaluation of a Bayesian music structure extractor,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873541730&partnerID=40&md5=a4bf002f6a58afd3977ce55c34557fa5,"Abdallah S., Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; Noland K., Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; Casey M., Centre for Cognition, Computation and Culture, Goldsmiths College, University of London, London SE14 6NW, New Cross Gate, United Kingdom; Rhodes C., Centre for Cognition, Computation and Culture, Goldsmiths College, University of London, London SE14 6NW, New Cross Gate, United Kingdom","We introduce a new model for extracting classified structural segments, such as intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify signal frames on the basis of their audio properties and then to agglomerate contiguous runs of similarly classified frames into texturally homogenous (or 'self-similar') segments which inherit the classificaton of their consituent frames. Our work extends previous work on automatic structure extraction by addressing the classification problem using using an unsupervised Bayesian clustering model, the parameters of which are estimated using a variant of the expectation maximisation (EM) algorithm which includes deterministic annealing to help avoid local optima. The model identifies and classifies all the segments in a song, not just the chorus or longest segment. We discuss the theory, implementation, and evaluation of the model, and test its performance against a ground truth of human judgements. Using an analogue of a precisionrecall graph for segment boundaries, our results indicate an optimal trade-off point at approximately 80% precision for 80% recall. © 2005 Queen Mary, University of London."
Harte C.; Sandler M.; Abdallah S.; Gómez E.,Symbolic representation of musical chords: A proposed syntax for text annotations,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873531256&partnerID=40&md5=d7d21bce72b2d0c1ea3fb24e8e05a8f1,"Harte C., Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; Abdallah S., Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; Gómez E., Music Technology Group, IUA, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs. When designing feature extraction algorithms, it is important to have a hand annotated test set providing a ground truth to compare results against. Hand labelling of chords in music files is a long and arduous task and there is no standard annotation methodology, which causes difficulties sharing with existing annotations. In this paper we address this problem by defining a rigid, contextindependent syntax for representing chord symbols in text, supported with a new database of annotations using this system. © 2005 Queen Mary, University of London."
Celma O.; Cano P.; Herrera P.,Search sounds: An audio crawler focused on weblogs,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863753638&partnerID=40&md5=93063ae05ccd4a40d67b4db457be4a2b,"Celma O., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain; Cano P., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain","In this paper we present a focused audio crawler that mines audio weblogs (MP3 blogs). This source of semi-structured information contains links to audio files, plus some textual information that is referring to the media file. A retrieval system-that exploits the mined data-fetches relevant audio files related to user's text query. Based on these results, the user can navigate and discover new music by means of content-based audio similarity. The system is available at: http://www.searchsounds.net. © 2006 University of Victoria."
Seppänen J.; Eronen A.; Hiipakka J.,Joint beat & tatum tracking from music signals,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049217163&partnerID=40&md5=1f0694591c69a2ef081c23181a12150e,"Seppänen J., Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland; Eronen A., Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland; Hiipakka J., Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland","This paper presents a method for extracting two key metrical properties, the beat and the tatum, from acoustic signals of popular music. The method is computationally very efficient while performing comparably to earlier methods. High efficiency is achieved through multirate accent analysis, discrete cosine transform periodicity analysis, and phase estimation by adaptive comb filtering. During analysis, the music signals are first represented in terms of accentuation on four frequency subbands, and then the accent signals are transformed into periodicity domain. Beat and tatum periods and phases are estimated in a probabilistic setting, incorporating primitive musicological knowledge of beat-tatum relations, the prior distributions, and the temporal continuities of beats and tatums. In an evaluation with 192 songs, the beat tracking accuracy of the proposed method was found comparable to the state of the art. Complexity evaluation showed that the computational cost is less than 1% of earlier methods. The authors have written a real-time implementation of the method for the S60 smartphone platform. © 2006 University of Victoria."
Kurth F.; Gehrmann T.; Müller M.,The cyclic beat spectrum: Tempo-related audio features for time-scale invariant audio identification,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549014432&partnerID=40&md5=7a9f49e0f88e871abdc64cf2ff704894,"Kurth F., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Gehrmann T., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Müller M., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany","In this paper, we present a novel set of tempo-related audio features for applications in audio retrieval. As opposed to existing feature sets commonly used in the retrieval domain which mainly focus on local spectral characteristics of the audio signal, our features capture its local temporal behaviour w.r.t. tempo, rhythm, and meter. As a key component to obtaining a high level of feature robustness we introduce the cyclic beat spectrum (CBS) consisting of residual tempo classes which are constructed similarly to the well-known pitch chroma classes. We illustrate the use of the newly constructed features by applying them to robust time-scale invariant audio identification. © 2006 University of Victoria."
Van Gulik R.; Vignoli F.,Visual playlist generation on the artist map,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873529473&partnerID=40&md5=4bbd1b876e60720cd9f77c6e3b4586d8,"Van Gulik R., Institute of Information and Computing Sciences, Utrecht University, 3584 CH, Utrecht, Padualaan 14, Netherlands; Vignoli F., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","This paper describes a visual playlist creation method based on a previously designed visualization technique for large music collections. The method gives users high-level control over the contents of a playlist as well as the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly portable. Future work includes an extensive user evaluation to compare the described method with alternative techniques and to measure its qualities, such as the perceived ease of use and perceived usefulness. © 2005 Queen Mary, University of London."
Jacobson K.,A multifaceted approach to music similarity,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449442276&partnerID=40&md5=455012b0ce143ad24560cdbbc685494d,"Jacobson K., University of Miami, Coral Gables, FL, United States","Previous work has explored the concept of music similarity measures and a variety of methods have been proposed for calculating such measures. This paper describes a system for music similarity which attempts to model and compare some of the more musically salient features of a set of audio signals. A model for timbre and a model for rhythm are implemented directly from previous work, and a model for song structure is developed. The different models are weighted and combined to provide an overall music similarity measure. The system is tested on a small set of popular music files spanning eleven different genres. The system is tuned to estimate genre boundaries using multidimensional scaling - a technique that allows for quick visualization of similarity data. An ""automatic DJ"" application, that generates playlists based on the music similarity models, serves as a subjective evaluation for the system. © 2006 University of Victoria."
Meng A.; Shawe-Taylor J.,An investigation of feature models for music genre classification using the support vector classifier,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533162&partnerID=40&md5=95ef28f2c209905d217284f7d2907ee1,"Meng A., Informatics and Mathematical Modelling - B321, Technical University of Denmark, Denmark; Shawe-Taylor J., University of Southampton, United Kingdom","In music genre classification the decision time is typically of the order of several seconds, however, most automatic music genre classification systems focus on short time features derived from 10-50ms. This work investigates two models, the multivariate Gaussian model and the multivariate autoregressive model for modelling short time features. Furthermore, it was investigated how these models can be integrated over a segment of short time features into a kernel such that a support vector machine can be applied. Two kernels with this property were considered, the convolution kernel and product probability kernel. In order to examine the different methods an 11 genre music setup was utilized. In this setup the Mel Frequency Cepstral Coefficients were used as short time features. The accuracy of the best performing model on this data set was ∼ 44% compared to a human performance of ∼ 52% on the same data set. © 2005 Queen Mary, University of London."
Paiva R.P.; Mendes T.; Cardoso A.,On the detection of melody notes in polyphonic audio,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873541859&partnerID=40&md5=3aa155377d51079d7196ac24993be15b,"Paiva R.P., CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal; Mendes T., CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal; Cardoso A., CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal","This paper describes a method for melody detection in polyphonic musical signals. Our approach starts by obtaining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, lowsalience and harmonically related notes are then eliminated. Finally, the notes comprising the melody are extracted. This is the main topic of this paper. We select the melody notes by making use of note saliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour. Finally, false positives in the extracted melody should be eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or durations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives. © 2005 Queen Mary, University of London."
Tsai W.-H.; Yu H.-M.; Wang H.-M.,A query-by-example technique for retrieving cover versions of popular songs with similar melodies,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873535580&partnerID=40&md5=69542e2f3f07fdca142f952b8dbf74bf,"Tsai W.-H., Institute of Information Science, Academia Sinica, Taipei, Taiwan; Yu H.-M., Institute of Information Science, Academia Sinica, Taipei, Taiwan; Wang H.-M., Institute of Information Science, Academia Sinica, Taipei, Taiwan","Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of songs specified by users. The technique enables users to listen to songs with an identical tune, but performed by different singers, in different languages, genres, and so on. The proposed system is built on a query-by-example framework, which takes a fragment of the song submitted by the user as input, and returns songs similar to the query in terms of the main melody as output. To handle the likely discrepancies, e.g., tempos, transpositions, and accompaniments between cover versions and the original song, methods are presented to remove the non-vocal portions of the song, extract the sung notes from the accompanied vocals, and compare the similarities between the sung note sequences. © 2005 Queen Mary, University of London."
Moelants D.; Cornelis O.; Leman M.; Gansemans J.; De Caluwe R.; De Tré G.; Matthé T.; Hallez A.,Problems and opportunities of applying data-& audio-mining techniques to ethnic music,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949101693&partnerID=40&md5=779f258c444c13d14de4acdfea21f919,"Moelants D., IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; Cornelis O., IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; Leman M., IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; Gansemans J., Dept. of Cultural Anthropology, Royal Museum of Central-Africa, B-3080 Tervuren, Leuvensesteenweg 13, Belgium; De Caluwe R., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium; De Tré G., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium; Matthé T., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium; Hallez A., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium","Current research in music information retrieval focuses on Western music. In music from other cultures, both musical structures and thinking about music can be very different. This creates problems for both the analysis of musical features and the construction of databases. On the other hand, a well-documented digitization offers interesting opportunities for the study and spread of 'endangered' music. Here, some general problems regarding the digital indexation of ethnic music are given, illustrated with a method for describing pitch structure, comparing Western standards with African music found in the digitization of the archives of the Royal Museum of Central-Africa in Tervuren (Brussels). © 2006 University of Victoria."
Chuan C.-H.; Chew E.,Fuzzy analysis in pitch class determination for polyphonic audio key finding,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473031&partnerID=40&md5=cfa65f8fff0342a328def96fe611aad0,"Chuan C.-H., Department of Computer Science, University of Southern California, Integrated Media Systems Center, Los Angeles, CA 90089, United States; Chew E., Epstein Dep of Industrial and Systems Eng., University of Southern California, Integrated Media Systems Center, Los Angeles, CA 90089, United States","This paper presents a fuzzy analysis technique for pitch class determination that improves the accuracy of key finding from audio information. Errors in audio key finding, typically incorrect assignments of closely related keys, commonly result from imprecise pitch class determination and biases introduced by the quality of the sound. Our technique is motivated by hypotheses on the sources of audio key finding errors, and uses fuzzy analysis to reduce the errors caused by noisy detection of lower pitches, and to refine the biased raw frequency data, in order to extract more correct pitch classes. We compare the proposed system to two others, an earlier one employing only peak detection from FFT results, and another providing direct key finding from MIDI. All three used the same key finding algorithm (Chew's Spiral Array CEG algorithm) and the same 410 classical music pieces (ranging from Baroque to Contemporary). Considering only the first 15 seconds of music in each piece, the proposed fuzzy analysis technique outperforms the peak detection method by 12.18% on average, matches the performance of direct key finding from MIDI 41.73% of the time, and achieves an overall maximum correct rate of 75.25% (compared to 80.34% for MIDI key finding). © 2005 Queen Mary, University of London."
Vignoli F.; Pauws S.,A music retrieval system based on user-driven similarity and its evaluation,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873536012&partnerID=40&md5=addffb0aeefc850c4cf33a0dcb5d99a4,"Vignoli F., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Pauws S., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Large music collections require new ways to let users interact with their music. The concept of finding 'similar' songs, albums, or artists provides handles to users for easy navigation and instant retrieval. This paper presents the realization and user evaluation of a music retrieval music that sorts songs on the basis of similarity to a given seed song. Similarity is based on a userweighted combination of timbre, genre, tempo, year, and mood. A conclusive user evaluation assessed the usability of the system in comparison to two control systems in which the user control of defining the similarity measure was diminished. © 2005 Queen Mary, University of London."
Stenzel R.; Kamps T.,Improving content-based similarity measures by training a collaborative model,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873557073&partnerID=40&md5=b7954a44e993f4db483f57ea1b4f8a37,"Stenzel R., Fraunhofer IPSI, D-64293 Darmstadt, Dolivostr. 15, Germany; Kamps T., Fraunhofer IPSI, D-64293 Darmstadt, Dolivostr. 15, Germany","We observed that for multimedia data - especially music - collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with <250,000,000 collaborative data points crawled from the web and <190,000 songs annotated with content-based sound feature sets. A song mentioned in a playlist is regarded as one collaborative data point. In this paper we present a novel approach to bridging the performance gap between collaborative and contentbased similarity measures. In the initial training phase a model vector for each song is computed, based on collaborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using explicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale ecommerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vectors. These predicted model vectors are finally used to compute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that offer a collaborative similarity recommender as service to their customers. © 2005 Queen Mary, University of London."
Lartillot O.,Efficient extraction of closed motivic patterns in multi-dimensional symbolic representations of music,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873559974&partnerID=40&md5=d52cfb87f177ae162c5bbfdf73a322e3,"Lartillot O., University of Jyväskylä, 40014 Jyväskylä, PL 35(A), Finland","An efficient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is usually filtered using global selective mechanisms, based on pattern frequency and length. The proposed approach is founded instead on the concept of closed pattern, and insures lossless compression through an adaptive selection of most specific descriptions in the multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling the filtering of another form of combinatorial redundancy provoked by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalising particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but requires instead a complex and detailed modelling of the cognitive system ruling the listening processes. The resulting algorithm is able to offer for the first time compact and relevant motivic analyses of monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex motivic transformations. © 2005 Queen Mary, University of London."
Bello J.P.; Pickens J.,A robust mid-level representation for harmonic content in music signals,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873553947&partnerID=40&md5=bf44cc6e62eafcd7f15e74da8faeaefa,"Bello J.P., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, United Kingdom; Pickens J., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, United Kingdom","When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research. © 2005 Queen Mary, University of London."
Kuuskankare M.; Laurson M.,Annotating musical scores in ENP,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873547151&partnerID=40&md5=6c677df3b8273956fedec8cbc8517953,"Kuuskankare M., Department of Doctoral Studies in Musical Performance and Research, Sibelius Academy, Finland; Laurson M., Centre for Music and Technology, Sibelius Academy, Finland","The focus of this paper is on ENP-expressions that can be used for annotating ENP scores with user definable information. ENP is a music notation program written in Lisp and CLOS with a special focus on compositional and music analytical applications. We present number of built-in expressions suitable for visualizing, for example, music analytical information as a part of music notation. A Lisp and CLOS based system for creating user-definable annotation information is also presented along with some sample algorithms. Finally, our system for automatically analyzing and annotating an ENP score is illustrated through several examples including some dealing with music information retrieval. © 2005 Queen Mary, University of London."
Riley J.; Mayer C.A.,Ask a librarian: The role of librarians in the music information retrieval community,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873414891&partnerID=40&md5=fb83dd9adf8107681a6ea48b97e365d5,"Riley J., Indiana University, Bloomington, IN 47405, 1320 E. 10th St. E170, United States; Mayer C.A., University of Maryland, 2511 Clarice Smith Performing Arts Center, College Park, MD 20742, United States","Participation from music librarians has been sparse in the first six ISMIR conferences, despite many potential areas of common interest. This paper makes an argument for the benefit to both the library and Music IR communities of increased representation of librarians at ISMIR. An analysis of conference programs and primary publications of two music library organizations to determine topics from the library literature relevant to Music IR research is presented. A discussion follows of expertise music librarians could potentially contribute to Music IR research and the ways in which Music IR research could further the work of music librarians, in each of the topics represented in the library literature. © 2006 University of Victoria."
Woodruff J.; Pardo B.; Dannenberg R.,Remixing stereo music with score-informed source separation,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449136628&partnerID=40&md5=2d5b5d70f327274bbf448faed4d8c1c0,"Woodruff J., Music Technology, School of Music, Northwestern University, Evanston, IL 60208, United States; Pardo B., Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, United States; Dannenberg R., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Musicians and recording engineers are often interested in manipulating and processing individual instrumental parts within an existing recording to create a remix of the recording. When individual source tracks for a stereo mixture are unavailable, remixing is typically difficult or impossible, since one cannot isolate the individual parts. We describe a method of informed source separation that uses knowledge of the written score and spatial information from an anechoic, stereo mixture to isolate individual sound sources, allowing remixing of stereo mixtures without access to the original source tracks. This method is tested on a corpus of string quartet performances, artificially created using Bach four-part chorale harmonizations and sample violin, viola and cello recordings. System performance is compared in cases where the algorithm has knowledge of the score and those in which it operates blindly. The results show that source separation performance is markedly improved when the algorithm has access to a well-aligned score. © 2006 University of Victoria."
Cont A.,Realtime multiple pitch observation using sparse non-negative constraints,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449658600&partnerID=40&md5=29fcd64ed6b5db127bbf9c1d05ebc8bc,"Cont A., Ircam, Realtime Applications Team, Paris, France, Center for Research in Computing and the Arts, UCSD, San Diego, United States","In this paper we introduce a new approach for realtime multiple pitch observation of musical instruments. The proposed algorithm is quite different from others in the literature both in its purpose and approach. It is destined not for continuous multiple f0 recognition but rather for projection of the ongoing spectrum to learned pitch templates. The decomposition algorithm on the other hand, does not compromise signal processing models for pitches and consists of an algorithm for efficient decomposition of a spectrum using known pitch structures and based on sparse non-negative constraints. After introducing the algorithm along with evaluations, a real-time implementation of the algorithm is provided for free download for the MaxMSP realtime programming environment. © 2006 University of Victoria."
Cunningham S.J.; Bainbridge D.; Falconer A.,'More of an art than a science': Supporting the creation of playlists and mixes,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424976&partnerID=40&md5=e340307d171e84e98ea68eec998def3e,"Cunningham S.J., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Bainbridge D., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Falconer A., not available, Hamilton, 17 Ernest Road, New Zealand","This paper presents an analysis of how people construct playlists and mixes. Interviews with practitioners and postings made to a web site are analyzed using a grounded theory approach to extract themes and categorizations. The information sought is often encapsulated as music information retrieval tasks, albeit not as the traditional ""known item search"" paradigm. The collated data is analyzed and trends identified and discussed in relation to music information retrieval algorithms that could help support such activity. © 2006 University of Victoria."
Dhanaraj R.; Logan B.,Automatic prediction of hit songs,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873556775&partnerID=40&md5=b2a699b2d076553956644982c2ecb272,"Dhanaraj R., Research Science Institute Intern, Hewlett Packard Labs., One Cambridge Center, Cambridge MA, United States; Logan B., Research Science Institute Intern, Hewlett Packard Labs., One Cambridge Center, Cambridge MA, United States","We explore the automatic analysis of music to identify likely hit songs. We extract both acoustic and lyric information from each song and separate hits from non-hits using standard classifiers, specifically Support Vector Machines and boosting classifiers. Our features are based on global sounds learnt in an unsupervised fashion from acoustic data or global topics learnt from a lyrics database. Experiments on a corpus of 1700 songs demonstrate performance that is much better than random. The lyricbased features are slightly more useful than the acoustic features in correctly identifying hit songs. Concatenating the two features does not produce significant improvements. Analysis of the lyric-based features shows that the absence of certain semantic information indicates that a song is more likely to be a hit. © 2005 Queen Mary, University of London."
Jang J.-S.R.; Hsu C.-L.; Lee H.-R.,Continuous HMM and its enhancement for singing/humming query retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873554994&partnerID=40&md5=1e3e938df824f9ffe17390a815b3bbcb,"Jang J.-S.R., Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; Hsu C.-L., Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; Lee H.-R., Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan","The use of HMM (Hidden Markov Models) for speech recognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by singing/humming, has seldom been reported, partly due to the difference in acoustic characteristics between speech and singing/humming inputs. This paper will derive the formula of CHMM training for frame-based MRAI. In particular, we shall propose enhancement to CHMM and demonstrate that with the enhancement scheme, CHMM can compare favourably with DTW in both efficiency and effectiveness. © 2005 Queen Mary, University of London."
Loscos A.; Wang Y.; Jie W.; Boo J.,Low level descriptors for automatic violin transcription,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849011819&partnerID=40&md5=475ec4fa14e104d1456334ce07e799b2,"Loscos A., MTG, Universitat Pompeu Fabra, Spain, National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore; Wang Y., National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore; Jie W., National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore; Boo J., National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore",On top of previous work in automatic violin transcription we present a set of straight forward low level descriptors for assisting the transcription techniques and saving computational cost. Proposed descriptors have been tested against a database of 1500 violin notes and double stops. © 2006 University of Victoria.
Madsen S.T.; Widmer G.,Separating voices in MIDI,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50349094309&partnerID=40&md5=225f902de686cbaa2bd84f5fe8edb61f,"Madsen S.T., Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, A-4040 Linz, Altenbergerstraße 69, Austria",This paper presents an algorithm for converting midi events into logical voices. The algorithm is fundamentally based on the pitch proximity principle. New heuristics are introduced and evaluated in order to handle unsolved situations. The algorithm is tested on ground truth data: inventions and fugues by J. S. Bach. Due to its left to right processing it also runs on real time input. © 2006 University of Victoria.
Fiebrink R.; McKay C.; Fujinaga I.,Combining D2K and JGAP for efficient feature weighting for classification tasks in music information retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873563446&partnerID=40&md5=3dceca48d525728557974e5c0e6cbd0b,"Fiebrink R., Music Technology, McGill University, Montreal, Canada; McKay C., Music Technology, McGill University, Montreal, Canada; Fujinaga I., Music Technology, McGill University, Montreal, Canada","Music classification continues to be an important component of music information retrieval research. An underutilized tool for improving the performance of classifiers is feature weighting. A major reason for its unpopularity, despite its benefits, is the potentially infinite calculation time it requires to achieve optimal results. Genetic algorithms offer potentially sub-optimal but reasonable solutions at much reduced calculation time, yet they are still quite costly. We investigate the advantages of implementing genetic algorithms in a parallel computing environment to make feature weighting an affordable instrument for researchers in MIR. © 2005 Queen Mary, University of London."
Poliner G.E.; Ellis D.P.W.,A classification approach to melody transcription,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873560057&partnerID=40&md5=462d9ae85877840d4b57a73a164621a8,"Poliner G.E., LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027, United States; Ellis D.P.W., LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027, United States","Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. In contrast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Support Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems. © 2005 Queen Mary, University of London."
Kapur A.; McWalter R.I.; Tzanetakis G.,New music interfaces for rhythm-based retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873541438&partnerID=40&md5=de5bad91098c185fc5e49249b04b6d2a,"Kapur A., University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; McWalter R.I., University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; Tzanetakis G., University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada","In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of perfomance practice. © 2005 Queen Mary, University of London."
Fujihara H.; Kitahara T.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.,Singer identification based on accompaniment sound reduction and reliable frame selection,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533890&partnerID=40&md5=5e6d6a2a6868331d8a3b7782113dcecd,"Fujihara H., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Kitahara T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper describes a method for automatic singer identification from polyphonic musical audio signals including sounds of various instruments. Because singing voices play an important role in musical pieces with a vocal part, the identification of singer names is useful for music information retrieval systems. The main problem in automatically identifying singers is the negative influences caused by accompaniment sounds. To solve this problem, we developed two methods, accompaniment sound reduction and reliable frame selection. The former method makes it possible to identify the singer of a singing voice after reducing accompaniment sounds. It first extracts harmonic components of the predominant melody from sound mixtures and then resynthesizes the melody by using a sinusoidal model driven by those components. The latter method then judges whether each frame of the obtained melody is reliable (i.e. little influenced by accompaniment sound) or not by using two Gaussian mixture models for vocal and non-vocal frames. It enables the singer identification using only reliable vocal portions of musical pieces. Experimental results with forty popular-music songs by ten singers showed that our method was able to reduce the influences of accompaniment sounds and achieved an accuracy of 95%, while the accuracy for a conventional method was 53%. © 2005 Queen Mary, University of London."
Müller M.; Kurth F.; Clausen M.,Audio matching via chroma-based statistical features,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873540864&partnerID=40&md5=83e4d9abac077b4b94ee723957789f78,"Müller M., Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Kurth F., Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Clausen M., Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany","In this paper, we describe an efficient method for audio matching which performs effectively for a wide range of classical music. The basic goal of audio matching can be described as follows: consider an audio database containing several CD recordings for one and the same piece of music interpreted by various musicians. Then, given a short query audio clip of one interpretation, the goal is to automatically retrieve the corresponding excerpts from the other interpretations. To solve this problem, we introduce a new type of chroma-based audio feature that strongly correlates to the harmonic progression of the audio signal. Our feature shows a high degree of robustness to variations in parameters such as dynamics, timbre, articulation, and local tempo deviations. As another contribution, we describe a robust matching procedure, which allows to handle global tempo variations. Finally, we give a detailed account on our experiments, which have been carried out on a database of more than 110 hours of audio comprising a wide range of classical music. © 2005 Queen Mary, University of London."
Li M.; Sleep R.,Genre classification via an LZ78-based string kernel,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873559624&partnerID=40&md5=cc6c4f01e4c0432ad239c35768b2918a,"Li M., School of Computing Sciences, University of East Anglia, Norwich NR47TJ, United Kingdom; Sleep R., School of Computing Sciences, University of East Anglia, Norwich NR47TJ, United Kingdom","We develop the notion of normalized information distance (NID) [7] into a kernel distance suitable for use with a Support Vector Machine classifier, and demonstrate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is efficient to compute, yet generates an accuracy which compares well with recent works. © 2005 Queen Mary, University of London."
Norowi N.M.; Doraisamy S.; Wirza R.,Factors affecting automatic genre classification: An investigation incorporating non-western musical forms,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873543336&partnerID=40&md5=848ddf6c074b9bad6fc8cc90bd36be64,"Norowi N.M., Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia; Doraisamy S., Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia; Wirza R., Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia","The number of studies investigating automated genre classification is growing following the increasing amounts of digital audio data available. The underlying techniques to perform automated genre classification in general include feature extraction and classification. In this study, MARSYAS was used to extract audio features and the suite of tools available in WEKA was used for the classification. This study investigates the factors affecting automated genre classification. As for the dataset, most studies in this area work with western genres and traditional Malay music is incorporated in this study. Eight genres were introduced; Dikir Barat, Etnik Sabah, Inang, Joget, Keroncong, Tumbuk Kalang, Wayang Kulit, and Zapin. A total of 417 tracks from various Audio Compact Discs were collected and used as the dataset. Results show that various factors such as the musical features extracted, classifiers employed, the size of the dataset, excerpt length, excerpt location and test set parameters improve classification results. © 2005 Queen Mary, University of London."
Pauws S.; Van De Wijdeven S.,User evaluation of a new interactive playlist generation concept,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873535158&partnerID=40&md5=62d050ead4aeb73fcddb7380e012e2c1,"Pauws S., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Van De Wijdeven S., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Selecting the 'right' songs and putting them in the 'right' order are key to a great music listening or dance experience. 'SatisFly' is an interactive playlist generation system in which the user can tell what kind of songs should be contained in what order in the playlist, while she navigates through the music collection. The system uses constraint satisfaction to generate a playlist that meets all user wishes. In a user evaluation, it was found that users created high-quality playlists in a swift way and with little effort using the system, while still having complete control on their music choices. The novel interactive way of creating a playlist, while browsing through the music collection, was highly appreciated. Ease of navigation through a music collection is still an issue that needs further attention. © 2005 Queen Mary, University of London."
Adams N.; Marquez D.; Wakefield G.,Iterative deepening for melody alignment and retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873542125&partnerID=40&md5=b50b45492ac9883f33f5aac3cb0ace8c,"Adams N., Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States; Marquez D., Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States; Wakefield G., Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States","For melodic theme retrieval there is a fundamental tradeoff between retrieval performance and retrieval speed. Melodic representations of large dimension yield the best retrieval performance, but at high computational cost, and vice versa. In the present work we explore the use of iterative deepening to achieve robust retrieval performance, but without the accompanying computational burden. In particular, we propose the use of a smooth pitch contour that facilitates query and target representations of variable length. We implement an iterative query-by-humming system that yields a dramatic increase in speed, without degrading performance compared to contemporary retrieval systems. Furthermore, we expand the conventional iterative framework to retain the alignment paths found in each iteration. These alignment paths are used to adapt the alignment window of subsequent iterations, further expediting retrieval without degrading performance. © 2005 Queen Mary, University of London."
Corthaut N.; Govaerts S.; Duval E.,Moody tunes: The rockanango project,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849161806&partnerID=40&md5=16d1f887b66ef4c5906879595bd04521,"Corthaut N., K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; Govaerts S., K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; Duval E., K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium","Wouldn' t it be nice if we had a tool that could offer people the right music for a specific time and place? For HORECA (hotel, restaurants and cafés) businesses, providing appropriate music is often not just nice, but essential. Typically this boils down to music that matches a certain situation on desired atmospheres, this will be defined as a musical context (MC). The developed tool, a music player, meeting the specific needs of HORECA, allows creation and management of those contexts. The user creates a musical context by selecting a number of appropriate atmospheres and can fine-tune the context with additional musical properties. The atmospheres are defined by a group of music experts, composed of DJ' s, music teachers, musicians, etc., who also manually annotate the properties of all musical content. To assist the music experts, a specially developed tool allows them to categorise and annotate the songs and evaluate their results. We provide insight on how we constructed and implemented our metadata schema and look at some existing schemas. The evaluation shows the economic value of such a system in the specific context of a HORECA business. © 2006 University of Victoria."
Mörchen F.; Ultsch A.; Nöcker M.; Stamm C.,Databionic visualization of music collections according to perceptual distance,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873562110&partnerID=40&md5=c18db285bd6697131a126b4ded7e62a7,"Mörchen F., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany; Ultsch A., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany; Nöcker M., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany; Stamm C., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany","We describe the MusicMiner system for organizing large collections of music with databionic mining techniques. Low level audio features are extracted from the raw audio data on short time windows during which the sound is assumed to be stationary. Static and temporal statistics were consistently and systematically used for aggregation of low level features to form high level features. A supervised feature selection targeted to model perceptual distance between different sounding music lead to a small set of non-redundant sound features. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map, displaying local sound differences based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song. © 2005 Queen Mary, University of London."
Homburg H.; Mierswa I.; Möller B.; Morik K.; Wurst M.,A benchmark dataset for audio classification and clustering,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873546054&partnerID=40&md5=9ec26b9cab5b262b6196d1e481cce5f5,"Homburg H., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Mierswa I., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Möller B., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Morik K., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Wurst M., University of Dortmund, AI Unit, 44221 Dortmund, Germany","We present a freely available benchmark dataset for audio classification and clustering. This dataset consists of 10 seconds samples of 1886 songs obtained from the Garageband site. Beside the audio clips themselves, textual meta data is provided for the individual songs. The songs are classified into 9 genres. In addition to the genre information, our dataset also consists of 24 hierarchical cluster models created manually by a group of users. This enables a user centric evaluation of audio classification and clustering algorithms and gives researchers the opportunity to test the performance of their methods on heterogeneous data. We first give a motivation for assembling our benchmark dataset. Then we describe the dataset and its elements in more detail. Finally, we present some initial results using a set of audio features generated by a feature construction approach. © 2005 Queen Mary, University of London."
Pampalk E.; Flexer A.; Widmer G.,Improvements of audio-based music similarity and genre classificaton,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873545114&partnerID=40&md5=aa02fafe088cccdada6e7b8282ac532b,"Pampalk E., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria, Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","Audio-based music similarity measures can be applied to automatically generate playlists or recommendations. In this paper spectral similarity is combined with complementary information from fluctuation patterns including two new descriptors derived thereof. The performance is evaluated in a series of experiments on four music collections. The evaluations are based on genre classification, assuming that very similar tracks belong to the same genre. The main findings are that, (1) although the improvements are substantial on two of the four collections our extensive experiments confirm earlier findings that we are approaching the limit of how far we can get using simple audio statistics. (2)We have found that evaluating similarity through genre classification is biased by the music collection (and genre taxonomy) used. Furthermore, (3) in a cross validation no pieces from the same artist should be in both training and test set. © 2005 Queen Mary, University of London."
Bosma M.; Veltkamp R.C.; Wiering F.,Muugle: A modular music information retrieval framework,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350486820&partnerID=40&md5=43b93507757b91854eda76716c84d2d9,"Bosma M., Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands; Veltkamp R.C., Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands; Wiering F., Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands",Muugle (Musical Utrecht University Global Lookup Engine) is a modular framework that allows the comparison of different MIR techniques and usability studies. A system overview and a discussion of a pilot usability experiment are given. A demo version of the framework can be found on http://give-lab.cs.uu.nl/ muugle. © 2006 University of Victoria.
Li Y.; Wang D.,Singing voice separation from monaural recordings,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41649099242&partnerID=40&md5=b10f7985353c8ab6674070e20b2d0be9,"Li Y., Department of Computer Science and Engineering, Ohio State University, United States; Wang D., Department of Computer Science and Engineering, Center for Cognitive Science, Ohio State University, United States","Separating singing voice from music accompaniment has wide applications in areas such as automatic lyrics recognition and alignment, singer identification, and music information retrieval. Compared to the extensive studies of speech separation, singing voice separation has been little explored. We propose a system to separate singing voice from music accompaniment from monaural recordings. The system has three stages. The singing voice detection stage partitions and classifies an input into vocal and non-vocal portions. Then the predominant pitch detection stage detects the pitch contour of the singing voice for vocal portions. Finally the separation stage uses the detected pitch contour to group the time-frequency segments of the singing voice. Quantitative results show that the system performs well in singing voice separation. © 2006 University of Victoria."
Lerch A.,On the requirement of automatic tuning frequency estimation,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866326330&partnerID=40&md5=68bf4c494611f2c58499cf50b1f5a268,,The deviation of the tuning frequency from the standard tuning frequency 440 Hz is evaluated for a database of classical music. It is discussed if and under what circumstances such a deviation may affect the robustness of pitch-based systems for musical content analysis. © 2006 University of Victoria.
Hosoya T.; Suzuki M.; Ito A.; Makino S.,Lyrics recognition from a singing voice based on finite state automaton for music information retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873550660&partnerID=40&md5=04d67e9945ffa79c90f690afe7a65cde,"Hosoya T., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan; Suzuki M., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan; Ito A., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan; Makino S., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan","Recently, several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user's singing voice. All of these systems use only the melody information for retrieval. Although the lyrics information is useful for retrieval, there have been few attempts to exploit lyrics in the user's input. In order to develop a MIR system that uses lyrics and melody information, lyrics recognition is needed. Lyrics recognition from a singing voice is achieved by similar technology to that of speech recognition. The difference between lyrics recognition and general speech recognition is that the input lyrics are a part of the lyrics of songs in a database. To exploit linguistic constraints maximally, we described the recognition grammar using a finite state automaton (FSA) that accepts only lyrics in the database. In addition, we carried out a ""singing voice adaptation"" using a speaker adaptation technique. In our experimental results, about 86% retrieval accuracy was obtained. © 2005 Queen Mary, University of London."
Eck D.; Casagrande N.,Finding meter in music using an autocorrelation phase matrix and shannon entropy,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873555550&partnerID=40&md5=e78387d927d4cfac7ff69ce90ca901ff,"Eck D., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; Casagrande N., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada","This paper introduces a novel way to detect metrical structure in music. We introduce a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting autocorrelation phase matrix is useful for several tasks involving metrical structure. First we can use the matrix to enhance standard autocorrelation by calculating the Shannon entropy at each lag. This approach yields improved results for autocorrelationbased tempo induction. Second, we can efficiently search the matrix for combinations of lags that suggest particular metrical hierarchies. This approach yields a good model for predicting the meter of a piece of music. Finally we can use the phase information in the matrix to align a candidate meter with music, making it possible to perform beat induction with an autocorrelation-based model. We present results for several meter prediction and tempo induction datasets, demonstrating that the approach is competitive with models designed specifically for these tasks. We also present preliminary beat induction results on a small set of artificial patterns. © 2005 Queen Mary, University of London."
Fiebrink R.; Fujinaga I.,Feature selection pitfalls and music classification,2006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749096588&partnerID=40&md5=8b2dbc0deff54851ea1a1fc7aa315c73,"Fiebrink R., Music Technology, McGill University, Montreal, Canada; Fujinaga I., Music Technology, McGill University, Montreal, Canada","Previous work has employed an approach to the evaluation of wrapper feature selection methods that may overstate their ability to improve classification accuracy, because of a phenomenon akin to overfitting. This paper discusses this phenomenon in the context of recent work in machine learning, demonstrates that previous work in MIR has indeed exaggerated the efficacy of feature selection for music classification, and presents new testing providing a more realistic analysis of feature selection's impact on music classification accuracy. © 2006 University of Victoria."
Celma O.; Ramírez M.; Herrera P.,Foafing the Music: A music recommendation system based on RSS feeds and user preferences,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449799861&partnerID=40&md5=cf4b7ab0f9456330da96c4e20f388001,"Celma O., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain; Ramírez M., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain","In this paper we give an overview of the Foafing the Music system. The system uses the Friend of a Friend (FOAF) and Rich Site Summary (RSS) vocabularies for recommending music to a user, depending on her musical tastes. Music information (new album releases, related artists' news and available audio) is gathered from thousands of RSS feeds .an XML format for syndicating Web content. On the other hand, FOAF documents are used to define user preferences. The presented system provides music discovery by means of: user profiling .defined in the user's FOAF description., context-based information .extracted from music related RSS feeds. and content-based descriptions .extracted from the audio itself © 2005 Queen Mary, University of London."
Grund C.M.,"Music information retrieval, memory and culture: Some philosophical remarks",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436506&partnerID=40&md5=0c3a386c0c52ce7c9b74bf58c7d48c99,"Grund C.M., Institute of Philosophy, Education and the Study of Religions Philosophy, University of Southern Denmark, Odense, Denmark","The burgeoning field of Music Information Retrieval (MIR) raises issues which are of interest within traditional areas of discussion in philosophy of music and of philosophy of culture in general. The purpose of this paper is twofold: the first goal is to highlight and briefly discuss a selection of these issues, while the second is to make a case for increased mutual awareness of each other on the parts of MIR and of humanistic research. Many traditional debates within the latter receive infusions of new perspectives from MIR, while research within MIR could be fruitfully pointed in directions suggested by questions of interest within traditional research in the humanities, e.g. the relationship of individual memory to cultural memory, issues regarding crosscultural understanding and the importance of authenticity in artistic contexts. © 2005 Queen Mary, University of London."
Bertin N.; De Cheveigné A.,Scalable metadata and quick retrieval of audio signals,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949181052&partnerID=40&md5=8592bf4555c0f1493a209f0be2f68910,"Bertin N., Equipe Audition, CNRS UMR 8581, ENS (DEC), 75005 Paris, 29, rue d'Ulm, France; De Cheveigné A., Equipe Audition, CNRS UMR 8581, ENS (DEC), 75005 Paris, 29, rue d'Ulm, France","Audio search algorithms have reached a degree of speed and accuracy that allows them to search efficiently within large databases of audio. For speed, algorithms generally depend on precalculated indexing metadata. Unfortunately, the size of the metadata follows the same exponential trend as the audio data itself, and this may lead to an exponential increase in storage cost and search time. The concept of scalable metadata has been introduced to allow metadata to adjust to such trends and alleviate the effects of forseeable increases of data and metadata size. Here, we argue that scalability fits the needs of the hierarchical structures that allow fast search, and illustrate this by adapting a state-of-the-art search algorithm to a scalable indexing structure. Scalability allows search algorithms to adapt to the increase of database size without loss of performance. © 2005 Queen Mary, University of London."
Orio N.; Neve G.,Experiments on segmentation techniques for music documents indexing,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751506470&partnerID=40&md5=03bdbff7a4fa72c8b5b8b6091818e7f7,"Orio N., Department of Information Engineering, 35131 Padova, Via Gradenigo, 6/A, Italy; Neve G., Department of Information Engineering, 35131 Padova, Via Gradenigo, 6/A, Italy","This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units, which can be used as content descriptors of music documents. Four approaches have been implemented and compared on a test collection of real documents and queries, showing their impact on index term size and on retrieval effectiveness. From the results, simple but extensive approaches seem to give better performances than more sophisticated segmentation algorithms. © 2005 Queen Mary, University of London."
Schedl M.; Knees P.; Widmer G.,Discovering and visualizing prototypical artists by web-based co-occurrence analysis,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445415&partnerID=40&md5=5bda80a942d051358ce2487cc9a9f876,"Schedl M., Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (ÖFAI), A-1010 Vienna, Austria; Knees P., Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (ÖFAI), A-1010 Vienna, Austria","Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository. © 2005 Queen Mary, University of London."
Pardo B.; Sanghi M.,Polyphonic musical sequence alignment for database search,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749093627&partnerID=40&md5=a86dae1c8bc714ae1970736800a417fd,"Pardo B., Computer Science Department, Northwestern University, Evanston, IL, 1890 Maple Ave., United States; Sanghi M., Computer Science Department, Northwestern University, Evanston, IL, 1890 Maple Ave., United States","Finding the best matching database target to a melodic query has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces. However, most tonal music is polyphonic, with multiple concurrent musical lines. Such pieces are not adequately represented as strings. Moreover, users often represent polyphonic pieces in their queries by skipping from one part (the soprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. These approaches are compared using synthetic queries on a database of Bach pieces. Results indicate that when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not. © 2005 Queen Mary, University of London."
Amatriain X.; Massaguer J.; Garcia D.; Mosquera I.,The CLAM Annotator: A cross-platform audio descriptors editing tool,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949223170&partnerID=40&md5=454c258e2f647b8a51a863f6e11ca844,"Amatriain X., CREATE, University of California, Santa Barbara CA 93106, United States; Massaguer J., Universitat Pompeu Fabra, Barcelona, Psg. Circumvalacio, 18, Spain; Garcia D., Universitat Pompeu Fabra, Barcelona, Psg. Circumvalacio, 18, Spain; Mosquera I., Universitat Pompeu Fabra, Barcelona, Psg. Circumvalacio, 18, Spain","This paper presents the CLAM Annotator tool. This application has been developed in the context of the CLAM framework and can be used to manually edit any previously computed audio descriptors. The application offers a convenient GUI that allows to edit low-level frame descriptors, global descriptors of any kind and segmentation marks. It is designed in such a way that the interface adapts itself to a user-defined schema, offering possibilities to a large range of applications. © 2005 Queen Mary, University of London."
Bray S.; Tzanetakis G.,Distributed audio feature extraction for music,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247202817&partnerID=40&md5=3308ea01bdc0ebf7eb8bcb2078d3aa5b,"Bray S., Computer Science Department, University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; Tzanetakis G., Computer Science Department, University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada","One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area were computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented. © 2005 Queen Mary, University of London."
Cantone D.; Cristofaro S.; Faro S.,"On tuning the (δ, α)-SEQUENTIAL-SAMPLING algorithm for δ-approximate matching with α-bounded gaps in musical sequences",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049084711&partnerID=40&md5=2b473e9297c7c9be29fa38b536705867,"Cantone D., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Cristofaro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Faro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy","We present a very efficient variant of the (δ, α)- SEQUENTIAL-SAMPLING algorithm, recently introduced by the authors, for the δ-approximate string matching problemwith α-bounded gaps, which often arises in many questions on musical information retrieval and musical analysis. Though it retains the same worst-case O(mn)-time and O(mα)-space complexity of its progenitor to compute the number of distinct δ-approximate α-gapped occurrences of a pattern of lengthmat each position in a text of length n, our new variant achieves an average O(n)-time complexity in practical cases. Extensive experimentations indicate that our algorithm is more efficient than existing solutions for the same problem, especially in the case of long patterns. © 2005 Queen Mary, University of London."
Kurth F.; Müller M.; Damm D.; Fremerey C.; Ribbrock A.; Clausen M.,SyncPlayer - An advanced system for multimodal music access,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349282433&partnerID=40&md5=261538d120abf7fb94658b604594fbdd,"Kurth F., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Müller M., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Damm D., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Fremerey C., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Ribbrock A., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Clausen M., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany","In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The recording is then identified by the SyncPlayer server, a process which is performed entirely content-based. Subsequently, the server delivers music-related data like scores or lyrics to the client, which are then displayed synchronously with audio playback using a multimodal visualization plug-in. In addition to visualization, the system provides functionality for contentbased music retrieval and semi-manual content annotation. To the best of our knowledge, our system is moreover the first to systematically exploit automatically generated synchronization data for content-based symbolic browsing in high quality audio recordings. SyncPlayer has already proved to be a valuable tool for evaluating algorithms in MIR research on a larger scale. In this paper, we describe the technical background of the SyncPlayer framework in detail. We also give an overview of the underlying MIR techniques of audio matching, music synchronization, and text-based retrieval that are incorporated in the current version of the system. © 2005 Queen Mary, University of London."
Doets P.J.O.; Lagendijk R.L.,Extracting quality parameters for compressed audio from fingerprints,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645683098&partnerID=40&md5=239de6c7937197a95fb07c74250be055,"Doets P.J.O., Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering,Mathematics and Computer Science, Delft University of Technology, 2600 GA Delft, P.O. Box 5031, Netherlands; Lagendijk R.L., Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering,Mathematics and Computer Science, Delft University of Technology, 2600 GA Delft, P.O. Box 5031, Netherlands","An audio fingerprint is a compact yet very robust representation of the perceptually relevant parts of audio content. It can be used to identify audio, even when of severely distorted. Audio compression causes small changes in the fingerprint. We aim to exploit these small fingerprint differences due to compression to assess the perceptual quality of the compressed audio file. Analysis shows that for uncorrelated signals the Bit Error Rate (BER) is approximately inversely proportional to the square root of the Signal-to-Noise Ratio (SNR) of the signal. Experiments using real music confirm this relation. Further experiments show how the various local spectral characteristics cause a large variation in the behavior of the fingerprint difference as a function of SNR or the bitrate set for compression. © 2005 Queen Mary, University of London."
Saito S.; Kameoka H.; Nishimoto T.; Sagayama S.,Specmurt analysis of multi-pitch music signals with adaptive estimation of common harmonic structure,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449098327&partnerID=40&md5=b5ba07704da73a3e0b2e4b3dddae9725,"Saito S., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan; Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan; Nishimoto T., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan","This paper describes a multi-pitch analysis method using specmurt analysis with iterative estimation of the quasioptimal common harmonic structure function. Specmurt analysis (Sagayama et al., 2004) is based upon the idea that superimposed harmonic structure pattern can be expressed as a convolution of two components, a fundamental frequency distribution and a 'common harmonic structure' function if each underlying tone component has similar harmonic structure pattern. As proved in our previous work (Sagayama et al., 2004) inappropriate common structure function leads to inaccurate analysis results. The iterative algorithm proposed in this paper automatically chooses a proper structure, which results in finding concurrent multiple fundamental frequencies and reduces the dependency on heuristically chosen initial common harmonic structure. The experimental evaluation showed promising results. © 2005 Queen Mary, University of London."
Lobb R.; Bell T.; Bainbridge D.,Fast capture of sheet music for an agile digital music library,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866006591&partnerID=40&md5=f708d1250b088cbecf2015cc60f66bbd,"Lobb R., Department of Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand; Bell T., Department of Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand; Bainbridge D., Department of Computer Science, University of Waikato, Hamilton, New Zealand","A personal digital music library needs to be .agile., that is, it needs to make it easy to capture and index material on the fly. A digital camera is a particularly effective way of achieving this, but there are several issues with the quality of the captured image, including distortions in the shape of the image due to the camera not being aligned properly with the page, non-planarity of the page, lens distortion from close-up shots, and inconsistent lighting across the page. In this paper we explore ways to improve the quality of music images captured by a digital camera or an inexpensive scanner, where the user is not expected to pay a lot of attention to the process. Such pre-processing will significantly aid Music Information Retrieval indexing through Optical Music Recognition, for example. The research presented here is primarily based around using a Fast Fourier Transform (FFT) to determine the orientation of the page. We find that a windowed FFT is effective at correcting rotational errors, and we make significant progress towards removing perspective distortion introduced by the camera not being parallel with the music. © 2005 Queen Mary, University of London."
Chordia P.,Segmentation and recognition of tabla strokes,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872732949&partnerID=40&md5=0446ce7792037f891ccde6a773fc6852,"Chordia P., CCRMA, Stanford University, Stanford CA 94305, 660 Lomita Dr., United States","A system that segments and labels tabla strokes from real performances is described. Performance is evaluated on a large database taken from three performers under different recording conditions, containing a total of 16,834 strokes. The current work extends previous work by Gillet and Richard (2003) on categorizing tabla strokes, by using a larger, more diverse database that includes their data as a benchmark, and by testing neural networks and treebased classification methods. First, the time-domain signal was segmented using complex-domain thresholding that looked for sudden changes in amplitude and phase discontinuities. At the optimal point on the ROC curve, false positives were less than 1% and false negatives were less than 2%. Then, classification was performed using a multivariate Gaussian model (mv gauss) as well as non-parametric techniques such as probabilistic neural networks (pnn), feed-forward neural networks (ffnn), and tree-based classifiers. Two evaluation protocols were used. The first used 10-fold cross validation. The recognition rate averaged over several experiments that contained 10-15 classes was 92% for the mv gauss, 94% for the ffnn and pnn, and 84% for the tree based classifier. To test generalization, a more difficult independent evaluation was undertaken in which no test strokes came from the same recording as the training strokes. The average recognition rate over a wide variety of test conditions was 76% for the mv gauss, 83% for the ffnn, 76% for the pnn, and 66% for the tree classifier. © 2005 Queen Mary, University of London."
Cahill M.; Ó Maidín D.,Melodic similarity algorithms - Using similarity ratings for development and early evaluation,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949607193&partnerID=40&md5=5278af8ca26074b98a633ba038be3d71,"Cahill M., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland; Ó Maidín D., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","This paper focuses on gathering similarity ratings for use in the construction, optimization and evaluation of melodic similarity algorithms. The approach involves conducting listening experiments to gather these ratings for a piece in Theme and Variation form. © 2005 Queen Mary, University of London."
Wood G.; O'Keefe S.,On techniques for content-based visual annotation to aid intra-track music navigation,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547703973&partnerID=40&md5=12fbd0e0b9db83abbae07905700edd6d,"Wood G., University of York, York YO10 5DD, United Kingdom; O'Keefe S., University of York, York YO10 5DD, United Kingdom","Despite the fact that people are increasingly listening to music electronically, the core interface of the common tools for playing the music have had very little improvement. In particular the tools for intra-track navigation have remained basically static, not taking advantage of recent studies into the field of audio jisting, summarising and segmentation. We introduce a novel mechanism for musical audio linear summarisation and modify a widely used open source media player to utilise several music information retrieval techniques directly in the graphical user interface. With a broad range of music, we provide a qualitative discussion on several techniques used for contentbased music information retrieval and perform quantitative investigation to their usefulness. © 2005 Queen Mary, University of London."
Isaacson E.,What you see is what you get: On visualizing music,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952690386&partnerID=40&md5=a9d66a5128a2ef57a7d093dbf30af595,"Isaacson E., Indiana University, School of Music, Department of Music Theory, Bloomington, IN 47405, United States","Though music is fundamentally an aural phenomenon, we often communicate about music through visual means. The paper examines a number of visualization techniques developed for music, focusing especially on those developed for music analysis by specialists in the field, but also looking at some less successful approaches. It is hoped that, by presenting them in this way, those in the MIR community will develop a greater awareness of the kinds of musical problems music scholars are concerned with, and might lend a hand toward addressing them © 2005 Queen Mary, University of London."
Dannenberg R.B.,"Toward automated holistic beat tracking, music analysis, and understanding",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547404622&partnerID=40&md5=11b7d0de98814d778cde87afd5b3c6cb,"Dannenberg R.B., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Most music processing attempts to focus on one particular feature or structural element such as pitch, beat location, tempo, or genre. This hierarchical approach, in which music is separated into elements that are analyzed independently, is convenient for the scientific researcher, but is at odds with intuition about music perception. Music is interconnected at many levels, and the interplay of melody, harmony, and rhythm are important in perception. As a first step toward more holistic music analysis, music structure is used to constrain a beat tracking program. With structural information, the simple beat tracker, working with audio input, shows a large improvement. The implications of this work for other music analysis problems are discussed. © 2005 Queen Mary, University of London."
Dixon S.; Widmer G.,MATCH: A music alignment tool chest,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870607867&partnerID=40&md5=34fcd410b321e8a2ca98c92533b33d4a,"Dixon S., Austrian Research Institute for Artificial Intelligence, Vienna 1010, Freyung 6/6, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Altenberger Str 69, Austria","We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasises note onsets in the alignment process. In tests with Classical and Romantic piano music, the average alignment error was 41ms (median 20ms), with only 2 out of 683 test cases failing to align. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed. © 2005 Queen Mary, University of London."
Kameoka H.; Nishimoto T.; Sagayama S.,Harmonic-temporal-structured clustering via deterministic annealing EM algorithm for audio feature extraction,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846191496&partnerID=40&md5=5e5b21f78c997c621c3c5568b2505136,"Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Nishimoto T., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan","This paper proposes ""harmonic-temporal structured clustering (HTC) method"", that allows simultaneous estimation of pitch, intensity, onset, duration, etc., of each underlying source in multi-stream audio signal, which we expect to be an effective feature extraction for MIR systems. STC decomposes the energy patterns diffused in timefrequency space, i.e., a time series of power spectrum, into distinct clusters such that each of them is originated from a single sound stream. It becomes clear that the problem is equivalent to geometrically approximating the observed time series of power spectrum by superimposed harmonictemporal structured models (HTMs), whose parameters are directly associated with the specific acoustic characteristics. The update equations in DA(Deterministic Annealing) EM algorithm for the optimal parameter convergence are derived by formulating the model with Gaussian kernel representation. The experiment showed promising results, and verified the potential of the proposed method. © 2005 Queen Mary, University of London."
Heydarian P.; Reiss J.D.,The Persian music and the Santur instrument,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949101135&partnerID=40&md5=8262a054397da849c0e633a59f5f83cf,"Heydarian P., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom; Reiss J.D., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom","Persian music has had a profound effect on various Eastern musical cultures, and also influenced Southern European and Northern African music. The Santur, a hammered dulcimer, is one of the most important instruments in Persia. In this paper, Persian music and the Santur instrument are explained and analysed. Techniques for fundamental frequency detection are applied to data acquired from the Santur and results are reported. © 2005 Queen Mary, University of London."
Muñoz-Expósito J.E.; Garcia-Galán S.; Ruiz-Reyes N.; Vera-Candeas P.; Rivas-Peña F.,Speech/music discrimination using a single Warped LPC-based feature,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350641966&partnerID=40&md5=6a3b91c396463e2f9f916a0720a02ce4,"Muñoz-Expósito J.E., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Garcia-Galán S., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Ruiz-Reyes N., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Vera-Candeas P., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Rivas-Peña F., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain","Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in Tzanetakis and Cook (2002) is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for real-time multimedia applications. © 2005 Queen Mary, University of London."
Weyde T.; Datzko C.,Efficient melody retrieval with motif contour classes,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64049088858&partnerID=40&md5=32d5e3063f6703807f0ddd7f725cd22a,"Weyde T., City University, School of Informatics, Department of Computing, London, United Kingdom; Datzko C., University of Osnabrück, Research Department of Music and Media Technology, Osnabrück, Germany","This paper describes the use of motif contour classes for efficient retrieval of melodies from music collections. Instead of extracting incipits or themes, complete monophonic pieces are indexed for their motifs, using classes of motif contours. Similarity relations between these classes can be used for a very efficient search. This can serve as a first level search, which can be refined by using more computationally intensive comparisons on its results. The model introduced has been implemented and tested using the MUSITECH framework. We present empirical and analytical results on the retrieval quality, the complexity, and quality/efficiency trade-off. © 2005 Queen Mary, University of London."
Flexer A.; Pampalk E.; Widmer G.,Novelty detection based on spectral similarity of songs,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644630695&partnerID=40&md5=9fe873b081217e4b76b6082180c19cd3,"Flexer A., Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, A-1010 Vienna, Freyung 6/2, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6 /6, Austria; Pampalk E., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6 /6, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6 /6, Austria, Department of Computational Perception, Johannes Kepler University, A-4040 Linz, Altenberger Str. 69, Austria","We are introducing novelty detection, i.e. the automatic identification of new or unknown data not covered by the training data, to the field of music information retrieval. Two methods for novelty detection - one based solely on the similarity information and one also utilizing genre label information - are evaluated within the context of genre classification based on spectral similarity. Both are shown to perform equally well. © 2005 Queen Mary, University of London."
Raphael C.,A graphical model for recognizing sung melodies,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846186295&partnerID=40&md5=694072e99159d33b922a8608cf5be8e6,"Raphael C., School of Informatics, Indiana Univ., Bloomington, IN 47408, United States","A method is presented for automatic transcription of sung melodic fragments to score-like representation, including metric values and pitch. A joint model for pitch, rhythm, segmentation, and tempo is defined for a sung fragment. We then discuss the identification of the globally optimal musical transcription, given the observed audio data. A post process estimates the location of the tonic, so the transcription can be presented into they key of C. Experimental results are presented for a small test collection. © 2005 Queen Mary, University of London."
Pikrakis A.; Theodoridis S.,A novel HMM approach to melody spotting in rawaudio recordings,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750995368&partnerID=40&md5=9e9c7176f5154ba7361a2ef079ea4f62,"Pikrakis A., Dept. of Informatics and Telecommunications, University of Athens, TYPA Buildings, 15784, Athens, Panepistimioupolis, Greece; Theodoridis S., Dept. of Informatics and Telecommunications, University of Athens, TYPA Buildings, 15784, Athens, Panepistimioupolis, Greece","This paper presents a melody spotting system based on Variable Duration Hidden Markov Models (VDHMM's), capable of locating monophonic melodies in a database of raw audio recordings. The audio recordings may either contain a single instrument performing in solo mode, or an ensemble of instruments where one of the instruments has a leading role. The melody to be spotted is presented to the system as a sequence of note durations and music intervals. In the sequel, this sequence is treated as a pattern prototype and based on it, a VDHMM is constructed. The probabilities of the associated VDHMM are determined according to a set of rules that account (a) for the allowable note duration flexibility and (b) with possible structural deviations from the prototype pattern. In addition, for each raw audio recording in the database, a sequence of note durations and music intervals is extracted by means of a multi pitch tracking algorithm. These sequences are subsequently fed as input to the constructed VDHMM that models the pattern to be located. The VDHMM employs an enhanced Viterbi algorithm, previously introduced by the authors, in order to account for pitch tracking errors and performance improvisations of the instrument players. For each audio recording in the database, the best-state sequence generated by the enhanced Viterbi algorithm is further post-processed in order to locate occurrences of the melody which is searched. Our method has been successfully tested with a variety of cello recordings in the context ofWestern Classical music, as well as with Greek traditional multi-instrument recordings, in which clarinet has a leading role. © 2005 Queen Mary, University of London."
Tanghe K.; Lesaffre M.; Degroeve S.; Leman M.; De Baets B.; Martens J.-P.,Collecting ground truth annotations for drum detection in polyphonic music,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35648949771&partnerID=40&md5=a52ece37446fcefb46acf2bb8400f3b8,"Tanghe K., IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; Lesaffre M., IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; Degroeve S., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Leman M., IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; De Baets B., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Martens J.-P., Department of Electronics and Information Systems, Ghent University, Belgium","In order to train and test algorithms that can automatically detect drum events in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for 49 real-world music fragments containing different drum event types. Apart from the drum events, the beat was also annotated. The annotators were experienced drummers or percussionists. This paper is primarily aimed towards other drum detection researchers, but might also be of interest to others dealing with automatic music analysis, manual annotation and data gathering. Its purpose is threefold: providing annotation data for algorithm training and evaluation, describing a practical way of setting up a drum annotation task, and reporting issues that came up during the annotation sessions while at the same time providing some thoughts on important points that could be taken into account when setting up similar tasks in the future. © 2005 Queen Mary, University of London."
Aucouturier J.-J.; Pachet F.,Ringomatic: A real-time interactive drummer using constraint-satisfaction and drum sound descriptors,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872720146&partnerID=40&md5=5833a9523be1057cdef90139a1f9b922,"Aucouturier J.-J., SONY CSL Paris, 75005 Paris, 6, rue Amyot, France; Pachet F., SONY CSL Paris, 75005 Paris, 6, rue Amyot, France","We describe a real-time musical agent that generates an audio drum-track by concatenating audio segments automatically extracted from pre-existing musical files. The drum-track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata automatically extracted from the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We report on several drum track audio descriptors designed for the system. We also describe a basic mecanism for controlling the tradeoff between the agent's autonomy and reactivity, which we illustrate with experiments made in the context of a virtual duet between the system and a human pianist. © 2005 Queen Mary, University of London."
Pickens J.; Iliopoulos C.,Markov random fields and maximum entropy modeling for music information retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849091441&partnerID=40&md5=a795ebe6b7862b706ae0f6badaa7b66a,"Pickens J., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Iliopoulos C., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom","Music information retrieval is characterized by a number of various user information needs. Systems are being developed that allow searchers to find melodies, rhythms, genres, and singers or artists, to name but a few. At the heart of all these systems is the need to find models or measures that answer the question .how similar are two given pieces of music.. However, similarity has a variety of meanings depending on the nature of the system being developed. More importantly, the features extracted from a music source are often either single-dimensional (i.e.: only pitch, or only rhythm, or only timbre) or else assumed to be orthogonal. In this paper we present a framework for developing systems which combine a wide variety of non-independent features without having to make the independence assumption. As evidence of effectiveness, we evaluate the system on the polyphonic theme similarity task over symbolic data. Nevertheless, we emphasize that the framework is general, and can handle a range of music information retrieval tasks. © 2005 Queen Mary, University of London."
Cunningham S.J.; Downie J.S.; Bainbridge D.,"The pain, the pain: Modelling music information behavior and the songs we hate",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845923163&partnerID=40&md5=2fb1c821f2e0744fe585a7bed93d50a7,"Cunningham S.J., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Downie J.S., GSLIS, University of Illinois at Urbana-Champaign, United States; Bainbridge D., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","The paper presents a grounded theory analysis of 395 user responses to the survey question, ""What is the worst song ever?"" Important factors uncovered include: lyric quality, the ""earworm"" effect, voice quality, the influence of associated music videos, over-exposure, perceptions of pretentiousness, and associations with unpleasant personal experiences. © 2005 Queen Mary, University of London."
Dalitz C.; Karsten T.,Using the Gamera framework for building a lute tablature recognition system,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849156559&partnerID=40&md5=bc4b89e57b70f48b05a4cfda0ceaa01a,"Dalitz C., Niederrhein University of Applied Sciences, 47805 Krefeld, Reinarzstr. 49, Germany; Karsten T., Niederrhein University of Applied Sciences, 47805 Krefeld, Reinarzstr. 49, Germany","In this article we describe an optical recognition system for historic lute tablature prints that we have built with the aid of the Gamera toolkit for document analysis and recognition. We give recognition rates for various historic sources and show that our system works quite well on printed tablature sources using movable types. For engraved and manuscript sources, we discuss some principal current limitations of our system and Gamera. © 2005 Queen Mary, University of London."
Yu Y.; Watanabe C.; Joe K.,Towards a fast and efficient match algorithm for content-based music retrieval on acoustic data,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149143823&partnerID=40&md5=2b3329e6b79a3707407c9fbd7f215331,"Yu Y., Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan; Watanabe C., Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan; Joe K., Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan","In this paper we present a fast and efficient match algorithm, which consists of two key techniques: Spectral Correlation Based Feature Merge(SCBFM) and Two-Step Retrieval(TSR). SCBFM can remove the redundant information. In consequence, the resulting feature sequence has a smaller size, requiring less storage and computation. In addition, most of the tempo variation is removed; thus a much simpler sequence match method can be adopted. Also, TSR relies on the characteristics of Mel-Frequency Cepstral Coefficient(MFCC), where the precise match in the second step depends on the first step to filter out most of the dissimilar references with only the low order MFCC feature. As a result, the whole retrieval speed can be further improved. The experimental evaluation verifies that SCBFM-TSR yields more meaningful results in comparatively short time. The experiment results are analyzed with a theoretical approach that seeks to find the relation between Spectral Correlation(SC) threshold and storage, computation. © 2005 Queen Mary, University of London."
Gillet O.; Richard G.,Drum track transcription of polyphonic music using noise subspace projection,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947708506&partnerID=40&md5=b9469f731b3f1695eea1fdc368d4b6ef,"Gillet O., GET / Télécom Paris, CNRS, LTCI, 75014 Paris, 37 rue Dareau, France; Richard G., GET / Télécom Paris, CNRS, LTCI, 75014 Paris, 37 rue Dareau, France","This paper presents a novel drum transcription system for polyphonic music. The use of a band-wise harmonic/noise decomposition allows the suppression of the deterministic part of the signal, which is mainly contributed by nonrhythmic instruments. The transcription is then performed on the residual noise signal, which contains most of the rhythmic information. This signal is segmented, and the events associated to each onset are classified by support vector machines (SVM) with probabilistic outputs. The features used for classification are directly extracted from the sub-band signals. An additional pre-processing stage in which the instances are reclassified using a localized model was also tested. This transcription method is evaluated on ten test sequences, each of them being performed by two drummers and being available with different mixing settings. The whole system achieves precision and recall rates of 84% for the bass drum and snare drum detection tasks. © 2005 Queen Mary, University of London."
Hu N.; Dannenberg R.B.,A bootstrap method for training an accurate audio segmenter,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955537796&partnerID=40&md5=516d9e2aa06f56e6ef3f947e992e3d40,"Hu N., Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States; Dannenberg R.B., Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States","Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and handlabeling is quite difficult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are first used to find the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to refine the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data. © 2005 Queen Mary, University of London."
Knees P.; Schedl M.; Widmer G.,Multiple lyrics alignment: Automatic retrieval of song lyrics,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43249122908&partnerID=40&md5=cfc6f0bab1fbdb2c858f656b1b4d18f5,"Knees P., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Austria","We present an approach to automatically retrieve and extract lyrics of arbitrary songs from the Internet. It is intended to provide easy and convenient access to lyrics for users, as well as a basis for further research based on lyrics, e.g. semantic analysis. Due to the fact that many lyrics found on the web suffer from individual errors like typos, we make use of multiple versions from different sources to eliminate mistakes. This is accomplished by Multiple Sequence Alignment. The different sites are aligned and examined for matching sequences of words, finding those parts on the pages that are likely to contain the lyrics. This provides a means to find the most probable version of lyrics, i.e. a version with highest consensus among different sources. © 2005 Queen Mary, University of London."
Liang W.; Zhang S.; Xu B.,A histogram algorithm for fast audio retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547544593&partnerID=40&md5=cb3254b08e00dd507aeed58ddbbe6dda,"Liang W., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Zhang S., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Xu B., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","This paper describes a fast audio detection method for specific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio features based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detection in TV program showed that the algorithm can achieve a very high overall precision and recall rate both about 97% with very fast search time about 1/40 on real time. © 2005 Queen Mary, University of London."
Sapp C.S.,Online database of scores in the Humdrum file format,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149277175&partnerID=40&md5=7e4741ca077d5801931bd478d7746a7a,"Sapp C.S., Center for Computer Assisted Research in the Humanities, Stanford University, United States, Centre for the History and Analysis of Recorded Music, Royal Holloway, University of London, United Kingdom","KernScores, an online library of musical data currently consisting of over 5 million notes, has been created to assist projects dealing with the computational analysis of musical scores. The online scores are in a format suitable for processing with the Humdrum Toolkit for Music Research, but the website also provides automatic translations into several other popular data formats for digital musical scores. © 2005 Queen Mary, University of London."
Lee J.H.; Downie J.S.; Cunningham S.J.,Challenges in cross-cultural/multilingual music information seeking,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052526600&partnerID=40&md5=7939e907c47da945068a575ebb465734,"Lee J.H., Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; Cunningham S.J., Dept. of Computer Science, University of Waikato, New Zealand","Understanding and meeting the needs of a broad range of music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multilingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from a Korean knowledge search portal Naver (knowledge) iN and 150 queries from Google Answers website. We conclude that new sets of access points must be developed to accommodate music queries that cross cultural or language boundaries. © 2005 Queen Mary, University of London."
Kitahara T.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.,"Instrument identification in polyphonic music: Feature weighting with mixed sounds, pitch-dependent timbre modeling, and use of musical context",2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846174759&partnerID=40&md5=e0df96474b1e9b7c05db0e4826d64e8d,"Kitahara T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper addresses the problem of identifying musical instruments in polyphonic music. Musical instrument identification (MII) is an improtant task in music information retrieval because MII results make it possible to automatically retrieving certain types of music (e.g., piano sonata, string quartet). Only a few studies, however, have dealt with MII in polyphonic music. In MII in polyphonic music, there are three issues: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. For the first issue, templates of feature vectors representing timbres are extracted from not only isolated sounds but also sound mixtures. Because some features are not robust in the mixtures, features are weighted according to their robustness by using linear discriminant analysis. For the second issue, we use an F0-dependent multivariate normal distribution, which approximates the pitch dependency as a function of fundamental frequency. For the third issue, when the instrument of each note is identified, the a priori probablity of the note is calculated from the a posteriori probabilities of temporally neighboring notes. Experimental results showed that recognition rates were improved from 60.8% to 85.8% for trio music and from 65.5% to 91.1% for duo music. © 2005 Queen Mary, University of London."
Toiviainen P.; Eerola T.,Classification of musical metre with autocorrelation and discriminant functions,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949088981&partnerID=40&md5=8a74fe6a07b54b7d68577a4111716b98,"Toiviainen P., Department of Music, University of Jyväskylä, Finland; Eerola T., Department of Music, University of Jyväskylä, Finland","The performance of autocorrelation-based metre induction was tested with two large collections of folk melodies, consisting of approximately 13,000 melodies in MIDI file format, for which the correct metres were available. The analysis included a number of melodic accents assumed to contribute to metric structure. The performance was measured by the proportion of melodies whose metre was correctly classified by Multiple Discriminant Analysis. Overall, the method predicted notated metre with an accuracy of 75 % for classification into nine categories of metre. The most frequent confusions were made within the groups of duple and triple/ compound metres, whereas confusions across these groups where significantly less frequent. In addition to note onset locations and note durations, Thomassen's melodic accent was found to be an important predictor of notated metre. © 2005 Queen Mary, University of London."
Sinyor E.; McKay C.; Fiebrink R.; McEnnis D.; Fujinaga I.,Beatbox classification using ACE,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872711201&partnerID=40&md5=6692b5d5a141197ec86f69aa8d606b12,"Sinyor E., Music Technology, McGill University, Montreal, QC, Canada; McKay C., Music Technology, McGill University, Montreal, QC, Canada; Fiebrink R., Music Technology, McGill University, Montreal, QC, Canada; McEnnis D., Music Technology, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology, McGill University, Montreal, QC, Canada","This paper describes the use of the Autonomous Classification Engine (ACE) to classify beatboxing (vocal percussion) sounds. A set of unvoiced percussion sounds belonging to five classes (bass drum, open hihat, closed hihat and two types of snare drum) were recorded and manually segmented. ACE was used to compare various classification techniques, both with and without feature selection. The best result was 95.55% accuracy using AdaBoost with C4.5 decision tress. © 2005 Queen Mary, University of London."
Lampropoulos A.S.; Lampropoulou P.S.; Tsihrintzis G.A.,Musical genre classification enhanced by improved source separation techniques,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750683676&partnerID=40&md5=59dd2ca05a93a12fcce461e41b0c0ea6,"Lampropoulos A.S., University of Piraeus, Department of Informatics, Piraeus 185 34, Greece; Lampropoulou P.S., University of Piraeus, Department of Informatics, Piraeus 185 34, Greece; Tsihrintzis G.A., University of Piraeus, Department of Informatics, Piraeus 185 34, Greece","We present a system for musical genre classification based on audio features extracted from signals which correspond to distinct musical instrument sources. For the separation of the musical sources, we propose an innovative technique in which the convolutive sparse coding algorithm is applied to several portions of the audio signal. The system is evaluated and its performance is assessed. © 2005 Queen Mary, University of London."
Cambouropoulos E.; Crochemore M.; Iliopoulos C.; Mohamed M.; Sagot M.-F.,A pattern extraction algorithm for abstract melodic representations that allow partial overlapping of intervallic categories,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847137305&partnerID=40&md5=5d943b87c919f337b40c3837505bc083,"Cambouropoulos E., Department of Music Studies, University of Thessaloniki, 540006, Thessaloniki, Greece; Crochemore M., Institut Gaspard-Monge, University of Marne-la-Vallée, 77454 Marne-la-Vallée cedex 2, France, Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Iliopoulos C., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Mohamed M., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Sagot M.-F., INRIA Rhône-Alpes, Université Claude Bernard, 69622 Villeurbanne cedex, 43 Bd du 11 novembre 1918, France","This paper proposes an efficient pattern extraction algorithm that can be applied on melodic sequences that are represented as strings of abstract intervallic symbols; the melodic representation introduces special ""don't care"" symbols for intervals that may belong to two partially overlapping intervallic categories. As a special case the well established ""step-leap"" representation is examined. In the step-leap representation, each melodic diatonic interval is classified as a step (±s), a leap (±l) or a unison (u). Binary don't care symbols are introduced to represent the possible overlapping between the various abstract categories e.g. * = s, * = l and # = -s, # = -l. For such a sequence, we are interested in finding maximal repeating pairs and repetitions with a hole (two matching subsequences separated with an intervening non-matching symbol). We propose an O(n + d(n - d) + z)-time algorithm for computing all such repetitions in a given sequence x = x[1..n] with d binary don't care symbols, where z is the output size. © 2005 Queen Mary, University of London."
Izmirli O.,Tonal similarity from audio using a template based attractor model,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64949107887&partnerID=40&md5=ce9650cad2508ff62b8024930473a767,"Izmirli O., Center for Arts and Technology, Connecticut College, New London CT, 270 Mohegan Ave., United States","A model that calculates similarity of tonal evolution among pieces in an audio database is presented. The model employs a template based key finding algorithm. This algorithm is used in a sliding window fashion to obtain a sequence of tonal center estimates that delineate the trajectory of tonal evolution in tonal space. A chroma based representation is used to capture tonality information. Templates are formed from instrument sounds weighted according to pitch distribution profiles. For each window in the input audio, the chroma based representation is interpreted with respect to the precalculated templates that serve as attractor points in tonal space. This leads to a discretization in both time and tonal space making the output representation compact. Local and global variations in tempo are accounted for using dynamic time warping that employs a special type of music theoretical distance measure. Evaluation is given in two stages. The first is evaluation of the key finding model to assess its performance in key finding for raw audio input. The second is based on cross validation testing for pieces that have multiple performances in the database to determine the success of recall by distance. © 2005 Queen Mary, University of London."
Knopke I.,Geospatial location of music and sound files for music information retrieval,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861011184&partnerID=40&md5=fda945790c07974255f257fe2d481f7e,"Knopke I., McGill Music Technology, Montréal, QC, Canada","A relatively new avenue of Web-based information retrieval research, intended to semantically improve information extraction, is the idea of using geographical information to accurately locate resources. This paper introduces a technique for locating sound and music files geographically. It uses information extracted from the Web relating to audio resources and combines it with geospatial location data to provide new information about audio usage in various countries. The results presented here illustrate the enormous potential for MIR to use the vast amount of audio materials on the Web within a physical and geographical context. Statistics of audio usage around the world are provided, as well as examples of other applications of these techniques. © 2005 Queen Mary, University of London."
Typke R.; Wiering F.; Veltkamp R.C.,A survey of music information retrieval systems,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649146511&partnerID=40&md5=5ce0f7cc7b0d3048cae77fe825fe59d1,"Typke R., Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands; Wiering F., Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands; Veltkamp R.C., Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands","This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks. © 2005 Queen Mary, University of London."
Parker C.,Applications of binary classification and adaptive boosting to the query-by-humming problem,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849093376&partnerID=40&md5=e484effc5522be136b5c49dde66a4bbd,"Parker C., Oregon State University, 102 Dearborn Hall, Corvallis, OR 97333, United States","In the. query-by-humming. problem, we attempt to retrieve a speci c song from a target set based on a sung query. Recent evaluations of query-by-humming systems show that the state-of-the-art algorithm is a simple dynamic programming-based interval matching technique. Other techniques based on hidden Markov models are far more expensive computationally and do not appear to offer significant increases in performance. Here, we borrow techniques from artificial intelligence to create an algorithm able to outperform the current state-of-the-art with only a negligible increase in running time.© 2005 Queen Mary, University of London."
Karydis I.; Nanopoulos A.; Katsaros D.; Manolopoulos Y.; Papadopoulos A.,Content-based music information retrieval in wireless ad-hoc networks,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149111533&partnerID=40&md5=f39212feb53fdb77266d73ff5faae240,"Karydis I., Aristotle University, Thessaloniki 54124, Greece; Nanopoulos A., Aristotle University, Thessaloniki 54124, Greece; Katsaros D., Aristotle University, Thessaloniki 54124, Greece; Manolopoulos Y., Aristotle University, Thessaloniki 54124, Greece; Papadopoulos A., Aristotle University, Thessaloniki 54124, Greece","This paper, introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks. We investigate for the first time the challenges posed by the wireless medium and recognise the factors that require optimisation. We propose novel techniques, which attain a significant reduction in both response times and traffic, compared to naive approaches. Extensive experimental results illustrate the appropriateness and efficiency of the proposed method in this bandwidth-starving and volatile, due to mobility, environment. © 2005 Queen Mary, University of London."
Meredith D.; Wiggins G.A.,Comparing pitch spelling algorithms,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748496003&partnerID=40&md5=244131775376f60bd3f7538da3fba0ab,"Meredith D., Department of Computing, Goldsmiths' College, University of London, London, SE14 6NW, New Cross, United Kingdom; Wiggins G.A., Department of Computing, Goldsmiths' College, University of London, London, SE14 6NW, New Cross, United Kingdom","A pitch spelling algorithm predicts the pitch names of the notes in a musical passage when given the onset-time, MIDI note number and possibly the duration and voice of each note. Various versions of the algorithms of Longuet-Higgins, Cambouropoulos, Temperley and Sleator, Chew and Chen, and Meredith were run on a corpus containing 195972 notes, equally divided between eight classical and baroque composers. The standard deviation of the accuracies achieved by each algorithm over the eight composers was used as a measure of its style dependence (SD). Meredith's ps1303 was the most accurate algorithm, spelling 99.43% of the notes correctly (SD = 0.54). The best version of Chew and Chen's algorithm was the least dependent on style (SD = 0.35) and spelt 99.15% of the notes correctly. A new version of Cambouropoulos's algorithm, combining features of all three versions described by Cambouropoulos himself, also spelt 99.15% of the notes correctly (SD = 0.47). The best version of Temperley and Sleator's algorithm spelt 97.79% of the notes correctly, but nearly 70% of its errors were due to a single sudden enharmonic change. Longuet-Higgins's algorithm spelt 98.21% of the notes correctly (SD = 1.79) but only when it processed the music a voice at a time. © 2005 Queen Mary, University of London."
Scaringella N.; Zoia G.,On the modeling of time information for automatic genre recognition systems in audio signals,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644624152&partnerID=40&md5=ce1f1039928d5f7c94d40f93f57fe706,"Scaringella N., Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne EPFL, Lausanne, CH-1015, Switzerland; Zoia G., Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne EPFL, Lausanne, CH-1015, Switzerland","The creation of huge databases coming from both restoration of existing analogue archives and new content is demanding fast and more and more reliable tools for content analysis and description, to be used for searches, content queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music catalogues, libraries and shops. Despite their use musical genres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the same pattern recognition architecture: extracting features from chunks of audio signal and classifying features independently. In this paper, we focus instead on the low-level temporal relationships between chunks when classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music segments to consolidate classification consistency by reducing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classification results are reported for a database of 1400 songs evenly distributed over 7 genres. © 2005 Queen Mary, University of London."
Roy P.; Aucouturier J.-J.; Pachet F.; Beurivé A.,Exploiting the tradeoff between precision and cpu-time to speed up nearest neighbor search,2005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047196692&partnerID=40&md5=b8deee40b9f8cb1b63b0b94a8a947be3,"Roy P., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France; Aucouturier J.-J., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France; Pachet F., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France; Beurivé A., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France","We describe an incremental filtering algorithm to quickly compute the N nearest neighbors according to a similarity measure in a metric space. The algorithm exploits an intrinsic property of a large class of similarity measures for which some parameter p has a positive influence both on the precision and the cpu cost (precision-cputime tradeoff ). The algorithm uses successive approximations of the measure to compute first cheap distances on the whole set of possible items, thenmore andmore expensivemeasures on smaller and smaller sets. We illustrate the algorithm on the case of a timbre similarity algorithm, which compares gaussian mixture models using a Monte Carlo approximation of the Kullback-Leibler distance, where p is the number of points drawn from the distributions. We describe several Monte Carlo algorithmic variants, which improve the convergence speed of the approximation. On this problem, the algorithm performs more than 30 times faster than the naive approach. © 2005 Queen Mary, University of London."
David Bainbridge 0001,The role of Music IR in the New Zealand Digital Music Library project.,2000,https://doi.org/10.5281/zenodo.1416260,"David Bainbridge, Department of Computer Science, University of Waikato","This extended abstract describes the computer music work that forms part of the New Zealand Digital Library (NZDL) project. The music work focuses on data acquisition, retrieval, presentation, and scalability. The MELDEX system, a web-based system, supports searching through text and sung queries, and browsing through metadata lists. The system includes collections of popular tunes derived from sheet music, folksongs, and MIDI files. The software implements a distributed architecture and allows for different indexes to be served to the same collection on different computers. The acquisition methods include automatic conversion of sheet music using Optical Music Recognition (OMR) software, online MIDI files, and existing databases of music in symbolic form. The OMR software has been extended to merge reconstructed scores with MIDI renditions to improve accuracy. Acquiring symbolic music information from MIDI files is simpler compared to sheet music."
Eloi Batlle;Pedro Cano,Automatic Segmentation for Music Classification using Competitive Hidden Markov Models.,2000,https://doi.org/10.5281/zenodo.1416764,"Eloi Batlle, Audiovisual Institute. Universitat Pompeu Fabra;Pedro Cano, Audiovisual Institute. Universitat Pompeu Fabra","Music information retrieval has become a major topic in recent years, leading to the growth of audio databases. However, the usefulness of these databases relies on their organization and structure. In this paper, we propose a new audio classification tool that can automatically segment audio material in an unsupervised way. The segments obtained are labeled based on their psychoacoustic properties, allowing for fast indexing and retrieval of audio fragments. We use competitive hidden Markov models as the main classification engine, eliminating the need for previous labeled data. Our results show that these models converge to a realistic segmentation architecture."
Juan Pablo Bello;Giuliano Monti;Mark B. Sandler,Techniques for Automatic Music Transcription.,2000,https://doi.org/10.5281/zenodo.1414872,"Juan Pablo Bello, Department of Electronic Engineering, King’s College London;Giuliano Monti, Department of Electronic Engineering, King’s College London;Mark Sandler, Department of Electronic Engineering, King’s College London","Two systems for automatic music transcription are reviewed in this paper. The first system performs monophonic transcription using an autocorrelation pitch tracker. It takes advantage of heuristic parameters related to the similarity between image and sound in the collector. The second system is able to analyze simple polyphonic tracks using a blackboard system. It receives input from a segmentation routine and includes a neural network chord recognizer. Examples are provided to illustrate the performance and weaknesses of the current implementation, and next steps for further development are defined."
Daniel Bendor;Mark B. Sandler,Time Domain Extraction of Vibrato from Monophonic Instruments.,2000,https://doi.org/10.5281/zenodo.1416810,"Daniel Bendor, Undergraduate School of Electrical Engineering, University of Maryland at College Park;Mark Sandler, Department of Electronic Engineering, King's College London","Vibrato is an essential ingredient in the expressive nature of many musical instruments. The goal of this research is to extract information describing the amplitude, frequency, and phase of the vibrato from a section of monophonic music. The amplitude and frequency modulation signals can be extracted using envelope and pitch extraction techniques respectively. The unfiltered vibrato signal is found by sliding a window and calculating the standard deviation within the window. The signal can then be filtered to obtain the original vibrato signal causing the amplitude modulation. Pitch extraction can be done using a bandpass filter."
Alain Bonardi,IR for Contemporary Music: What the Musicologist Needs.,2000,https://doi.org/10.5281/zenodo.1415912,"Alain Bonardi, IRCAM","Active listening in contemporary music involves more than just receiving musical information. It is an interactive process between the listener and the musical documents, including automatic music information research and extraction, to uncover the composer's intentions. Computer-assisted listening for musicologists, such as in IRCAM's digital library, provides a framework that allows for varying representations of music and the ability to associate different contexts. This enables the musicologist to analyze and interpret the music more effectively."
Wei Chai;Barry Vercoe,Using User Models in Music Information Retrieval Systems.,2000,https://doi.org/10.5281/zenodo.1415898,"Wei Chai, Media Laboratory, Massachusetts Institute of Technology;Barry Vercoe, Media Laboratory, Massachusetts Institute of Technology","Most websites providing music services only support category-based browsing and/or text-based searching. There has been some research to improve the interface either for pull applications, e.g. query-by-humming systems, or for push applications, e.g. collaborative-filtering-based or feature-based music recommendation systems. However, for content-based search or feature-based filtering systems, one important problem is to describe music by its parameters or features, so that search engines or information filtering agents can use them to measure the similarity of the target (user's query or preference) and the candidates. MPEG7 (formally called ""Multimedia Content Description Interface"") is an international standard, which describes the multimedia content data to allow universal indexing, retrieval, filtering, control, and other activities supported by rich metadata. However, the metadata about the multimedia content itself are still insufficient, because many features of multimedia content are quite perceptual and user-dependent. For example, emotional features are very important for multimedia retrieval, but they are hard to be described by a universal model since different users may have different emotional responses to the same multimedia content. We therefore turn to user modeling techniques and representations to describe the properties of each user, so that the retrieval will be more accurate. Besides, user modeling can be used to reduce the search space, make push service easier and improve the user interface."
Arbee L. P. Chen,"Music Representation, Indexing and Retrieval at NTHU.",2000,https://doi.org/10.5281/zenodo.1417981,"Arbee L.P. Chen, Department of Computer Science, National Tsing Hua University","In this extended abstract, the work on the representation, indexing, and retrieval of music data is summarized. The focus is on treating rhythm, melody, and chords as music features and developing data structures and algorithms for efficient matching and retrieval. Techniques for retrieving songs by music segments are presented, along with multi-feature index structures for searching on different features. The problem of feature extraction is also studied, with a focus on discovering repeating patterns in music objects. The feasibility of the proposed concepts is illustrated through the implementation of a prototype system called Muse."
G. Sayeed Choudhury;M. Droetboom;Tim DiLauro;Ichiro Fujinaga;Brian Harrington,Optical Music Recognition System within a Large-Scale Digitization Project.,2000,https://doi.org/10.5281/zenodo.1415730,"G. Sayeed Choudhury, ",""""""
Michael Clausen;R. Engelbrecht;D. Meyer;J. Schmitz,PROMS: A Web-based Tool for Searching in Polyphonic Music.,2000,https://doi.org/10.5281/zenodo.1417139,"M. Clausen, Institut f¨ur Informatik V, Universit¨at Bonn;R. Engelbrecht, Institut f¨ur Informatik V, Universit¨at Bonn;D. Meyer, Institut f¨ur Informatik V, Universit¨at Bonn;J. Schmitz, Institut f¨ur Informatik V, Universit¨at Bonn","One major task of a digital music library (DML) is to provide techniques to locate a queried musical pattern in all pieces of music in the database containing that pattern. Existing DMLs work with melody databases relying on score-like information and mainly take into account pitch information. However, we believe that both pitch and rhythm are crucial for recognizing melodies, especially in the context of polyphonic music. PROMS is a web-based computer-music service that aims to design and implement procedures for music search. It is set-oriented and uses single notes as basic objects, specified by onset time, pitch, and duration. The database consists of a sequence of pieces of music in the MIDI format, and a query is a finite set of notes."
Dave Cliff;Heppie Freeburn,Exploration of Point-Distribution Models for Similarity-based Classification and Indexing of Polyphonic Music.,2000,https://doi.org/10.5281/zenodo.1416572,"Dave Cliff, Hewlett-Packard Labs;Heppie Freeburn, Hewlett-Packard Labs","Similarity-based classification and indexing of polyphonic music remains a challenge in music information retrieval systems. This paper explores the use of high-order multivariate statistical techniques, specifically point distribution models (PDMs), for similarity-based classification of polyphonic music in digital audio files. The goal is to apply PDMs to musical similarity by creating neural networks that approximate the statistical processing. However, the results so far are inconclusive and negative. The paper encourages further exploration of PDMs in the music retrieval community."
Maxime Crochemore;Costas S. Iliopoulos;Yoan J. Pinzón,Finding Motifs with Gaps.,2000,https://doi.org/10.5281/zenodo.1415986,"Maxime Crochemore, Institut Gaspard-Monge, Laboratoire d'informatique, Université de Marne-la-Vallée;Wojciech Rytter, Uniwersytet Warszawski, Banacha 2, 02--097, Warszawa, Poland, and Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, UK.;Costas S. Iliopoulos and Yoan J. Pinzon, Dept. Computer Science, King's College London, London WC2R 2LS, UK, and School of Computing, Curtin University of Technology, GPO Box 1987 U, WA. Australia","This paper focuses on a set of string pattern-matching problems that arise in musical analysis, particularly in musical information retrieval. The paper discusses the flexibility required in score searching, especially in polyphonic music where there may be gaps between events in different voices. The paper presents a mathematical treatment of allowing gaps in the query and the score being searched, represented by the constant α. The paper also defines the problem of matching with gaps and discusses various conditions and variants of the problem. Efficient algorithms and implementations for these variants have been designed."
Jon W. Dunn,Beyond VARIATIONS: Creating a Digital Music Library.,2000,https://doi.org/10.5281/zenodo.1415148,"Jon W. Dunn, Indiana University","This presentation focuses on the work being done at Indiana University in the area of digital music libraries, specifically the VARIATIONS system. The VARIATIONS system provides online access to sound recordings from the library's collections, with a focus on musical repertoire central to the teaching mission of the Indiana University School of Music. The collection includes over 6800 sound recording titles, covering a broad range of musical material. Indiana University is embarking on a project to expand the VARIATIONS system and create a Digital Music Library, funded by the National Science Foundation and National Endowment for the Humanities. The project aims to develop applications for music education and research, as well as research in the areas of instruction, usability, and intellectual property rights."
Jonathan Foote,ARTHUR: Retrieving Orchestral Music by Long-Term Structure.,2000,https://doi.org/10.5281/zenodo.1416644,"Jonathan Foote, FX Palo Alto Laboratory, Inc.","We introduce an audio retrieval-by-example system for orchestral music. Unlike many other approaches, this system is based on analysis of the audio waveform and does not rely on symbolic or MIDI representations. ARTHUR retrieves audio on the basis of long-term structure, specifically the variation of soft and louder passages. The long-term structure is determined from envelope of audio energy versus time in one or more frequency bands. Similarity between energy profiles is calculated using dynamic programming. Given an example audio document, other documents in a collection can be ranked by similarity of their energy profiles. Experiments are presented for a modest corpus that demonstrate excellent results in retrieving different performances of the same orchestral work, given an example performance or short excerpt as a query."
Anastasia Georgaki;Spyros Raptis;Stelios Bakamidis,A Music Interface for Visually Impaired People in the WEDELMUSIC Environment. Design and Architecture.,2000,https://doi.org/10.5281/zenodo.1417291,"Anastasia Georgaki, Institute for Language and Speech Processing, Speech Technology Department;Spyros Raptis, Institute for Language and Speech Processing, Speech Technology Department;Stelios Bakamidis, Institute for Language and Speech Processing, Speech Technology Department","In this poster, the authors present the architecture of a new music interface for blind musicians in the WEDELMUSIC environment. The interface aims to facilitate access to musical databases and allow visually impaired individuals to edit and create musical scores. The architecture includes modules such as a Braille printer, a spoken music interpreter, and various data interpretations and output devices."
Michael Good,Representing Music Using XML.,2000,https://doi.org/10.5281/zenodo.1415032,"Michael Good, Recordare","Why does the world need another music representation language? Beyond MIDI describes over 20 different languages or musical codes (Selfridge-Field, 1997). Most commercial music programs have their own internal, proprietary music representation and file format. Music's complexity has led to this proliferation of languages and formats. Sequencers, notation programs, analysis tools, and retrieval tools all need musical information optimized in different ways. Yet no music interchange language has been widely adopted since MIDI. MIDI has contributed to enormous growth in the electronic music industry, but has many well-known limitations for notation, analysis, and retrieval. These include its lack of representation of musical concepts such as rests and enharmonic pitches (distinguishing Db from C#), as well as notation concepts such as stem direction and beaming. Other interchange formats such as NIFF and SMDL overcome these restrictions, but have not been widely adopted. Successful interchange formats such as MIDI and HTML share a common trait that NIFF and SMDL lack. MIDI and HTML skillfully balance simplicity and power. They are simple enough for many people to learn, and powerful enough for many real-world applications. The simplicity makes it easy for software developers to implement the standards and to develop encoding tools for musicians. This helps circumvent the “chicken-and-egg” problem with new formats. XML (Extensible Markup Language) is a World Wide Web Consortium (W3C) recommendation for representing structured data in text, designed for ease of usage over the Internet by a wide variety of applications. XML is a meta-markup language that lets designers and communities develop their own representation languages for different applications. Like HTML and MIDI, it balances simplicity and power in a way that has made it very attractive to software developers. The common base of XML technology lets developers of new languages focus on representation issues instead of low-level software development. All XML-based languages can be processed by a variety of XML tools available from multiple vendors. Since XML files are text files, users of XML files always have generic text-based tools available as a lowest common denominator. XML documents are represented in Unicode, providing support for international score exchange. MusicXML is an XML-based music interchange language. It represents common western musical notation from the 17th century onwards, including both classical and popular music. The language is designed to be extensible to future coverage of early music and less standard 20th and 21st century scores. Non-western musical notations would use a separate XML language. As an interchange language, it is designed to be sufficient, not optimal, for diverse musical applications. MusicXML is not intended to supersede other languages that are optimized for specific musical applications, but to support sharing of musical data between applications. The current MusicXML software runs on Windows. As of September 2000, it reads 100% of the MuseData format plus portions of NIFF and Finale’s Enigma Transportable Files (ETF). It writes to Standard MIDI Files in Format 1, MuseData files, and Sibelius. The NIFF, ETF, and MIDI converters use XML versions of these languages as intermediate structures. MusicXML is defined using an XML Document Type Definition (DTD) at www.musicxml.com/xml.html. XML Schemas address some shortcomings of DTDs, but are not yet a W3C recommendation."
Perfecto Herrera-Boyer;Xavier Amatriain;Eloi Batlle;Xavier Serra,Towards Instrument Segmentation for Music Content Description: a Critical Review of Instrument Classification Techniques.,2000,https://doi.org/10.5281/zenodo.1416768,"Perfecto Herrera, Audiovisual Institute - Pompeu Fabra University;Xavier Amatriain, Audiovisual Institute - Pompeu Fabra University;Eloi Batlle, Audiovisual Institute - Pompeu Fabra University;Xavier Serra, Audiovisual Institute - Pompeu Fabra University","A system capable of describing the musical content of any kind of sound file or sound stream, as it is supposed to be done in MPEG7-compliant applications, should provide an account of the different moments where a certain instrument can be listened to. In this paper we concentrate on reviewing the different techniques that have been so far proposed for automatic classification of musical instruments. As most of the techniques to be discussed are usable only in ""solo"" performances we will evaluate their applicability to the more complex case of describing sound mixes. We conclude this survey discussing the necessity of developing new strategies for classifying sound mixes without a priori separation of sound sources."
David Huron,Perceptual and Cognitive Applications in Music Information Retrieval.,2000,https://doi.org/10.5281/zenodo.1414794,"David Huron, Cognitive and Systematic Musicology Laboratory, School of Music, Ohio State University","Music librarians and cataloguers have traditionally created indexes for accessing musical works based on standard reference information. However, these standard reference tags have limited applicability in most music-related queries. Music serves a variety of purposes, such as targeting a certain clientele, setting a certain tempo, conveying a certain mood, or motivating a patient. The most useful retrieval indexes for music are those that facilitate searching based on social and psychological functions, focusing on stylistic, mood, and similarity information. Building musical web crawlers that index music-related files will require drawing on research in music perception and cognition, particularly for tasks such as music summarization and mood characterization."
Mari Itoh,Subject Search for Music: Quantitative Analysis of Access Point Selection.,2000,https://doi.org/10.5281/zenodo.1414950,"Smiraglia, ",Non-book materials have unique characteristics in subject analysis that influence their information retrieval process. This study aims to explore useful tools for subject search of music scores in an online environment. The purpose of this survey is to comprehend how searchers selected access points for examining what search tools are most effective and necessary for searching music. The survey analyzed transaction logs from an academic library of music and found that more than half of the initial searches were combinations of access points.
Özgür Izmirli,Using a Spectral Flatness Based Feature for Audio Segmentation and Retrieval.,2000,https://doi.org/10.5281/zenodo.1416438,"Ozgur Izmirli, Center for Arts and Technology, Department of Mathematics and Computer Science, Connecticut College","A method that utilizes a spectral flatness based tonality feature for segmentation and content-based retrieval of audio is outlined. The method uses the tonality measure which is derived from the discrete bark spectrum as a means of detecting transitions between tonal and noise-like parts of the audio input. Segmentation is performed by determining the times of these transitions, hence providing reference points for search purposes. Search is carried out by pivoting the query information on these reference points. The cumulative distance between the tonality pattern in successive frames of the query and the candidate sound fragments is used as a measure of similarity."
Kjell Lemström;Sami Perttu,SEMEX - An efficient Music Retrieval Prototype.,2000,https://doi.org/10.5281/zenodo.1415908,"Kjell Lemström, Department of Computer Science, University of Helsinki;Sami Perttu, Department of Computer Science, University of Helsinki","We present an efficient prototype for music information retrieval. The prototype uses bit-parallel algorithms for locating transposition invariant matches of monophonic query melodies within monophonic or polyphonic music stored in a database. When dealing with monophonic music, we employ a fast approximate bit-parallel algorithm with special edit distance metrics. The fast scanning phase is succeeded by verification where a separate metrics is used for ranking matches. We also offer the possibility to search for exact occurrences of a 'distributed' melody within polyphonic databases via a bit-parallel filtering technique. In our experiments with a database of 2 million musical elements (notes in a monophonic and chords in a polyphonic database) the responses were obtained within one second in both cases. Furthermore, our prototype is capable of using various interval classes in matching, producing more approximation when it is needed."
Francis J. Kiernan,Score-based Style Recognition Using Artificial Neural Networks.,2000,https://doi.org/10.5281/zenodo.1416626,"Francis J. Kiernan, Pythagoras Graduate School","The original idea of this research was to develop a system for musicological analysis that could assist in resolving issues of compositional authenticity. The system uses a data extraction engine to gather statistical information from a musical score, and then uses a neural network to form an abstract impression of habitual characteristics within the composition. The system does not aim to model human musical perception. The system was initially developed to explore the authenticity of flute compositions attributed to Frederick II ""The Great"". A rule base was compiled based on information from performers and musicologists, and this rule base was implemented into the system. The methodology involved creating a corpus of score representations in ALMA, using intervallic difference of note relations and horizontal pitch-class snapshots to analyze the tonal contents of each measure. Vertical pitch-class analysis and monitoring the frequency of note occurrence over time were also used. The sequence recognition algorithm checked for recurring interval sequences on the major metric points of the melody."
Youngmoo E. Kim;Wei Chai;Ricardo García;Barry Vercoe,Analysis of a Contour-based Representation for Melody.,2000,https://doi.org/10.5281/zenodo.1416760,"Youngmoo E. Kim, Machine Listening Group, MIT Media Lab;Wei Chai, Machine Listening Group, MIT Media Lab;Ricardo Garcia, Machine Listening Group, MIT Media Lab;Barry Vercoe, Machine Listening Group, MIT Media Lab","Identifying a musical work from a melodic fragment is a task that most people are able to accomplish with relative ease. For some time now researchers have worked to give computers this ability as well, as it would be the cornerstone of any query-by-humming system. To accomplish this, it is reasonable to study how humans are able to perform this task, and to assess what features we use to determine melodic similarity. Research has shown that melodic contour is an important feature in determining melodic similarity, but it is also clear that rhythmic information is important as well. The goal of this research is to explore what variation of contour and rhythmic information can result in the most efficient, robust, and scalable representation for melody. We intend for this to be the basis of a query-by-humming system that will be used to test the validity of our proposed representation."
Steve Larson,"Searching for Meaning: Melodic Patterns, Combinations, and Embellishments.",2000,https://doi.org/10.5281/zenodo.1415738,"Steve Larson, University of Oregon","I am interested in the search for musical patterns -- not so much because I want to find particular patterns, but because I want to understand musical meaning and I believe that musical meaning is something that listeners create when they relate musical patterns to one another, and when they relate musical patterns to other sorts of patterns. I describe a theory of musical meaning that argues that experienced listeners of tonal music hear musical motion metaphorically, as purposeful action within a dynamic field of musical forces (musical gravity, magnetism, and inertia). That theory has been used to clarify issues in Schenkerian theory, to analyze music in many styles, to improve the training of musicians, to account for experimental results in melodic expectation, to explain striking regularities in published analyses of tonal music, and even to illuminate the phenomenon of ""swing"" in jazz. The assumptions of that theory generate a small set of patterns and pattern combinations. This small set of patterns crops up over and over again in tonal music. Recognizing these patterns and their significance requires seeing how they are embellished in particular pieces. I illustrate these patterns, their combinations and embellishments, and something about their meaning by analyzing several folk songs, including ""Ah, vous dirai-je, Maman"" and Mozart's variations on it. The ubiquity of this small set of patterns raises interesting questions about searching for musical patterns, about the role of computers in information retrieval vs their role in musical artificial intelligence, and about musical meaning. My presentation ends with a series of such questions."
Mary Levering,"Intellectual Property Rights in Musical Works: Overview, Digital Library Issues and Related Initiatives.",2000,https://doi.org/10.5281/zenodo.1416786,"Mary Levering, U.S. Copyright Office, Library of Congress","Our nation's Founding Fathers incorporated copyright principles into the U.S. Constitution to foster the creation and dissemination of intellectual works for the public good. The federal copyright law supports these goals by granting creators and owners of musical works an exclusive bundle of rights for limited periods of time. It is important for all potential users, including academics and scholars, to be aware of these rights when using copyrighted musical works in electronic formats. Understanding the range of exclusive rights, statutory limitations, fair use doctrine, authorization requirements, and how to secure permissions efficiently have become increasingly important."
Karen Lin;Tim Bell,Integrating Paper and Digital Music Information Systems.,2000,https://doi.org/10.5281/zenodo.1416544,"Karen Lin, University of Canterbury, Christchurch, New Zealand;Tim Bell, University of Canterbury, Christchurch, New Zealand","Active musicians often prefer using paper-based music information retrieval systems over digital ones, despite the advantages of digital tools. In this paper, the authors propose a model that integrates paper and digital domains, offering musicians the benefits of both. They conducted a survey to identify the challenges and potential of working with digital tools, and propose a system that simplifies the process of moving music documents between the two domains using color printing and scanning technology. The model divides the digital state into image data and semantic data."
Beth Logan,Mel Frequency Cepstral Coefficients for Music Modeling.,2000,https://doi.org/10.5281/zenodo.1416444,"Beth Logan, Cambridge Research Laboratory","We examine in some detail Mel Frequency Cepstral Coefficients (MFCCs) - the dominant features used for speech recognition - and investigate their applicability to modeling music.In particular, we examine two of the main assumptions of the process of forming MFCCs: the use of the Mel frequency scale to model the spectra; and the use of the Discrete Cosine Transform (DCT) to decorrelate the Mel-spectral vectors. We examine the first assumption in the context of speech/music discrimination. Our results show that the use of the Mel scale for modeling music is at least not harmful for this problem, although further experimentation is needed to verify that this is the optimal scale in the general case. We investigate the second assumption by examining the basis vectors of the theoretically optimal transform to decorrelate music and speech spectral vectors. Our results demonstrate that the use of the DCT to decorrelate vectors is appropriate for both speech and music spectra."
Donald MacLellan;Carola Boehm,MuTaTeD'll: A System for Music Information Retrieval of Encoded Music.,2000,https://doi.org/10.5281/zenodo.1417755,"Donald MacLellan, Department of Music, University of Glasgow;Carola Boehm, Department of Music, University of Glasgow","MuTaTeD'II started in November 1999, building on the results of the MuTaTeD project. Its aim is to design and implement a music information retrieval system with delivery/access services for encoded music. The prototype service will provide a user friendly, web-based search/browse/query interface to access musical content."
Jeremy Pickens,A Comparison of Language Modeling and Probabilistic Text Information Retrieval Approaches to Monophonic Music Retrieval.,2000,https://doi.org/10.5281/zenodo.1415100,"Jeremy Pickens, Department of Computer Science, University of Massachusetts","With interest in music information retrieval increasing the need for retrieval systems unique to music is also growing. Despite its unique properties music shares many similarities with text. The goal of this paper is to explore some of the capabilities and limitations of current text information retrieval systems as applied to the task of music retrieval. Monophonic music is converted into text and retrieval experiments are run using two different text information retrieval systems in various configurations. Finally, we will discuss whether the techniques applied here are generalizable to the larger problem of polyphonic music retrieval."
Perry Roland,XML4MIR: Extensible Markup Language for Music Information Retrieval.,2000,https://doi.org/10.5281/zenodo.1417167,"Perry Roland, University of Virginia",This paper evaluates the role of standards in information exchange and suggests the adoption of XML standards for music representation and meta-data to serve as the basis for music information retrieval.
Jochen Schimmelpfennig;Frank Kurth,MCML - Music Contents Markup Language.,2000,https://doi.org/10.5281/zenodo.1415526,"Jochen Schimmelpfennig and Frank Kurth, University of Bonn","We present an XML-based description interface for various types of musical contents - MCML, Music Contents Markup Language. An application of MCML currently developed within our group is a music browser system which enables a content based navigation in digital music files. Another major application of a music contents annotation interface is the description and handling of query results to digital music libraries."
Eleanor Selfridge-Field,What Motivates a Musical Query?.,2000,https://doi.org/10.5281/zenodo.1415970,,The abstract is not available in this context.
Perry R. Cook;George Tzanetakis,Audio Information Retrieval (AIR) Tools.,2000,https://doi.org/10.5281/zenodo.1416716,,""""""
Alexandra L. Uitdenbogerd,"Music IR: Past, Present, and Future.",2000,https://doi.org/10.5281/zenodo.1417545,"Alexandra L. Uitdenbogerd, Department of Computer Science, RMIT University;Abhijit Chattaraj, Department of Computer Science, RMIT University;Justin Zobel, Department of Computer Science, RMIT University","Music Information Retrieval (MIR) has a long history, dating back to the 1960s. It is rooted in information retrieval, musicology, and music psychology. Over the years, various techniques have been applied to measure stylistic parameters of composers and general similarity of musical works. The current trend in MIR research is the development of systems that allow the location of answers to queries presented as hummed melodies or entered in some other way. However, comparing systems is challenging due to the lack of a common data set, queries, or relevance judgments. MIR systems can be evaluated similarly to other systems, with the definition of queries, answers, and relevance depending on the user's needs."
Alexandra L. Uitdenbogerd;Justin Zobel,Music Ranking Techniques Evaluated.,2000,https://doi.org/10.5281/zenodo.1414990,"Alexandra L. Uitdenbogerd, Department of Computer Science, RMIT University;Justin Zobel, Department of Computer Science, RMIT University","Several techniques have been proposed for matching melody queries to stored music. In this paper, we explore a broader range of n-gram techniques and test them with both manual queries and queries automatically extracted from MIDI files. Our experiments show that alternative n-gram matching techniques, such as simply counting the number of 5-grams in common between query and stored piece of music, can be as effective as local alignment. N-grams are particularly effective for short and manual queries, while local alignment is superior for automatic queries."
Thomas von Schroeter;Shyamala Doraisamy;Stefan M. Rüger,From Raw Polyphonic Audio to Locating Recurring Themes.,2000,https://doi.org/10.5281/zenodo.1416124,"Thomas von Schroeter, T H Huxley School of Environment, Earth Sciences and Engineering Imperial College of Science, Technology and Medicine Prince Consort Road, London SW7 2BZ, England;Shyamala Doraisamy, Department of Multimedia Faculty of Computer Science and Information Technology University Putra Malaysia, 43400 UPM Serdang, Selangor D.E., Malaysia;Stefan M R¨uger, Department of Computing Imperial College of Science, Technology and Medicine 180 Queen’s Gate, London SW7 2BZ, England",We present research studies of two related strands in content-based music retrieval: the automatic transcription of raw audio from a single polyphonic instrument with discrete pitch (eg piano) and the location of recurring themes from a Humdrum score.
Eric Allamanche,Content-based Identification of Audio Material Using MPEG-7 Low Level Description.,2001,https://doi.org/10.5281/zenodo.1417853,"Eric Allamanche, Fraunhofer IIS-A;Bernhard Fröba, Fraunhofer IIS-A;Jürgen Herre, Fraunhofer IIS-A;Thorsten Kastner, Fraunhofer IIS-A;Oliver Hellmuth, Fraunhofer IIS-A;Markus Cremer, Fraunhofer IIS-A / AEMT","Along with investigating similarity metrics between audio material, the topic of robust matching of pairs of audio content has gained wide interest recently. In particular, if this matching process is carried out using a compact representation of the audio content (""audio fingerprint""), it is possible to identify unknown audio material by means of matching it to a database with the fingerprints of registered works. This paper presents a system for reliable, fast and robust identification of audio material which can be run on the resources provided by today's standard computing platforms. The system is based on a general pattern recognition paradigm and exploits low level signal features standardized within the MPEG-7 framework, thus enabling interoperability on a world-wide scale. Compared to similar systems, particular attention is given to issues of robustness with respect to common signal distortions, i.e. recognition performance for processed/modified audio signals. The system's current performance figures are benchmarked for a range of real-world signal distortions, including low bitrate coding and transmission over an acoustic channel. A number of interesting applications are discussed."
Jérôme Barthélemy,Figured Bass and Tonality Recognition.,2001,https://doi.org/10.5281/zenodo.1417161,"Jerome Barthélemy, Ircam;Alain Bonardi, Ircam","In the course of the WedelMusic project, the authors are implementing retrieval engines based on musical content extracted from a musical score. This includes main melodic motives, harmony, and tonality. The paper reviews previous research in harmonic analysis of tonal music and presents a method for automated harmonic analysis based on the extraction of a figured bass. The figured bass is determined using a template-matching algorithm, and tonality recognition is addressed using a simple algorithm based on the figured bass. The limitations of the method are discussed, and results are compared to previous research. Potential uses for Music Information Retrieval are also discussed."
William P. Birmingham,MUSART: Music Retrieval Via Aural Queries.,2001,https://doi.org/10.5281/zenodo.1415810,"William P. Birmingham, University of Michigan;Roger B. Dannenberg, Carnegie Mellon University;Gregory H. Wakefield, University of Michigan;Mark Bartsch, University of Michigan;David Bykowski, University of Michigan;Dominic Mazzoni, University of Michigan;Colin Meek, University of Michigan;Maureen Mellody, University of Michigan;William Rand, University of Michigan","MUSART is a research project developing and studying new techniques for music information retrieval. The MUSART architecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling, melodic contour, and phonetic streams for music retrieval. To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music. Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality."
Arbee L. P. Chen,Building a Platform for Performance Study of Various Music Information Retrieval Approaches.,2001,https://doi.org/10.5281/zenodo.1414822,"Jia-Lien Hsu, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan 300, R.O.C.;Arbee L.P. Chen, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan 300, R.O.C.","In this paper, we describe the Ultima project which aims to construct a platform for evaluating various approaches of music information retrieval. Three approaches with the corresponding tree-based, list-based, and (n-gram+tree)-based index structures are implemented. A series of experiments has been carried out. With the support of the experiment results, we compare the performance of index construction and query processing of the three approaches and give a summary for efficient content-based music information retrieval."
Roger B. Dannenberg,Music Information Retrieval as Music Understanding.,2001,https://doi.org/10.5281/zenodo.1418263,"Roger B. Dannenberg, Carnegie Mellon University","Much of the difficulty in Music Information Retrieval can be traced to problems of good music representations, understanding music structure, and adequate models of music perception. In short, the central problem of Music Information Retrieval is Music Understanding, a topic that also forms the basis for much of the work in the fields of Computer Music and Music Perception. It is important for all of these fields to communicate and share results. With this goal in mind, the author’s work on Music Understanding in interactive systems, including computer accompaniment and style recognition, is discussed."
Shyamala Doraisamy,An Approach Towards A Polyphonic Music Retrieval System.,2001,https://doi.org/10.5281/zenodo.1415622,"Shyamala Doraisamy, Dept. of Computing, Imperial College;Stefan M Rüger, Dept. of Computing, Imperial College","Most research on music retrieval systems is based on monophonic musical sequences. In this paper, we investigate techniques for a full polyphonic music retrieval system. A method for indexing polyphonic music data files using the pitch and rhythm dimensions of music information is introduced. Our strategy is to use all combinations of monophonic musical sequences from polyphonic music data. ‘Musical words’ are then obtained using the n-gram approach enabling text retrieval methods to be used for polyphonic music retrieval. Here we extend the n-gram technique to encode rhythmic as well as interval information, using the ratios of onset time differences between two adjacent pairs of pitch events. In studying the precision in which intervals are to be represented, a mapping function is formulated in dividing intervals into smaller classes. To overcome the quantisation problems that arise with using rhythmic information from performance data, an encoding mechanism using ratio bins is also adopted. We present results from retrieval experiments with a database of 3096 polyphonic pieces."
Matthew J. Dovey,A Technique for Regular Expression Style Searching in Polyphonic Music.,2001,https://doi.org/10.5281/zenodo.1416140,"Matthew J. Dovey, Visiting Research Fellow, Dept. of Computer Science, Kings College, London","This paper discusses ongoing investigative work on integrating two systems as part of the NSF/JISC funded OMRAS project into polyphonic searching of music. It describes a simple and efficient algorithm for locating a polyphonic query within a large polyphonic text and ways in which this algorithm can be modified to allow more freedom in how a match is made, allowing queries involving polyphonic regular expressions to be located in the text."
Michael Droettboom,Expressive and Efficient Retrieval of Symbolic Musical Data.,2001,https://doi.org/10.5281/zenodo.1417741,"Michael Droettboom, The Peabody Institute, The Johns Hopkins University;Ichiro Fujinaga, The Peabody Institute, The Johns Hopkins University;Karl MacMillan, The Peabody Institute, The Johns Hopkins University;Mark Patton, Digital Knowledge Center, Milton S. Eisenhower Library, The Johns Hopkins University;James Warner, Digital Knowledge Center, Milton S. Eisenhower Library, The Johns Hopkins University;G. Sayeed Choudhury, Digital Knowledge Center, Milton S. Eisenhower Library, The Johns Hopkins University;Tim DiLauro, Digital Knowledge Center, Milton S. Eisenhower Library, The Johns Hopkins University","The ideal content-based musical search engine for large corpora must be both expressive enough to meet the needs of a diverse user base and efficient enough to perform queries in a reasonable amount of time. In this paper, we present such a system, based on an existing advanced natural language search engine. In our design, musically meaningful searching is simply a special case of more general search techniques. This approach has allowed us to create an extremely powerful and fast search engine with minimal effort."
Adriane Durey,Melody Spotting Using Hidden Markov Models.,2001,https://doi.org/10.5281/zenodo.1415680,"Adriane Swalm Durey, Center for Signal and Image Processing, School of Electrical and Computer Engineering, Georgia Institute of Technology;Mark A. Clements, Center for Signal and Image Processing, School of Electrical and Computer Engineering, Georgia Institute of Technology","Melody spotting is an important task in music information retrieval. In this paper, we propose a melody spotting system based on hidden Markov models (HMMs). We use a set of acoustic features to model the melodic patterns in music recordings. Experimental results show that our proposed system achieves high accuracy in detecting the presence of a melody in music recordings."
Ludger Hofmann-Engl,Towards a Cognitive Model of Melodic Similarity.,2001,https://doi.org/10.5281/zenodo.1417359,"Ludger Hofmann-Engl, Keele University UK","In recent years, there has been increased interest in melodic similarity due to the importance of music information retrieval (MIR). However, little attention has been paid to cognitive or perceptual aspects of melodic similarity. This paper proposes a cognitive model of melodic similarity, focusing on the pitch aspect and suggesting the use of a term called ""meloton"" instead of pitch. The paper suggests approaching melotonic similarity from a transformational angle, where transformations are executed as reflections and translations. The paper concludes that melotonic similarity is a multi-faceted phenomenon that requires the development of flexible models."
Holger H. Hoos;Kai Renz;Marko Görg,GUIDO/MIR - an Experimental Musical Information Retrieval System based on GUIDO Music Notation.,2001,https://doi.org/10.5281/zenodo.1417517,"Holger H. Hoos, ",""""""
Andreas Kornstädt,The JRing System for Computer-Assisted Musicological Analysis.,2001,https://doi.org/10.5281/zenodo.1416100,"Andreas Kornstädt, Arbeitsbereich Softwaretechnik (SWT), Fachbereich Informatik, Universität Hamburg","Among other factors, high complexity and mandatory expert computer knowledge make many music IR and music analysis systems unsuitable for the majority of largely computer-illiterate musicologists. The JRing system offers highly flexible yet intuitively usable search and comparison operations to aid musicologists during score analysis. This paper discusses the requirement analysis that led to JRing’s inception, its IR tools and graphical user interface plus the kind of musical material it works on and the Humdrum-based technical realization of IR operations."
Adam Lindsay;Youngmoo Kim,"Adventures in Standardization, or How We Learned to Stop Worrying and Love MPEG-7.",2001,https://doi.org/10.5281/zenodo.1418071,"Adam Lindsay, Computing Department, Lancaster University;Youngmoo Kim, MIT Media Lab","The authors give a brief account of their combined 7+ years in multimedia standardization, namely in the MPEG arena. They discuss specifics on musical content description in MPEG-7 Audio and other items relevant to Music Information Retrieval among the MPEG-7 Multimedia Description Schemes. In the presentation, they will give a historical overview of the MPEG-7 standard, its motivations, and what led to its current state."
Colin Meek,Thematic Extractor.,2001,https://doi.org/10.5281/zenodo.1414828,"Colin Meek, University of Michigan;William P. Birmingham, University of Michigan","We have created a system that identifies musical keywords or themes. The system searches for all patterns composed of melodic repetition in a piece. This process uncovers a large number of patterns, which are then filtered and rated according to perceptually significant characteristics. The top-ranked patterns correspond to important thematic or motivic musical content. The system operates across a broad range of styles and relies on no meta-data, allowing it to independently and efficiently catalog multimedia data."
Takuichi Nishimura,Music Signal Spotting Retrieval by a Humming Query Using Start Frame Feature Dependent Continuous Dynamic Programming.,2001,https://doi.org/10.5281/zenodo.1417191,"Takuichi Nishimura, Real World Computing Partnership / National Institute of Advanced Industrial Science and Technology;J. Xin Zhang, Media Drive Co.;Hiroki Hashiguchi, Real World Computing Partnership;Masataka Goto, National Institute of Advanced Industrial Science and Technology / PRESTO, JST;Junko Takita, Mathematical Systems Inc.;Ryuichi Oka, Real World Computing Partnership","We have developed a music retrieval method that takes a humming query and finds similar audio intervals (segments) in a music audio database. This method can also address a personally recorded video database containing melodies in its audio track. Our previous retrieving method took too much time to retrieve a segment: for example, a 60-minute database required about 10-minute computation on a personal computer. In this paper, we propose a new high-speed retrieving method, called start frame feature dependent continuous Dynamic Programming, which assumes that the pitch of the interval start point is accurate. Test results show that the proposed method reduces retrieval time to about 1/40 of present methods."
Donncha Ó Maidín,Score Processing For MIR.,2001,https://doi.org/10.5281/zenodo.1416442,"Donncha S. Ó Maidín, Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick;Margaret Cahill, Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick","The focus of this paper is on the design and use of a music score representation. The structure of the representation is discussed and illustrated with sample algorithms, including some from music information retrieval. The score representation was designed for the development of general algorithms and applications. The common container-iterator paradigm is used, in which the score is modelled as a container of objects, such as clefs, key signatures, time signatures, notes, rests and barlines. Access to objects within the score is achieved through iterators. These iterators provide the developer with a mechanism for accessing the information content of the score. The iterators are designed to achieve a high level of data hiding, so that the user is shielded from the substantial underlying complexity of score representation, while at the same time, having access to the score’s full information content."
François Pachet,A Naturalist Approach to Music File Name Analysis.,2001,https://doi.org/10.5281/zenodo.1415856,"François Pachet, Sony CSL-Paris;Damien Laigre, Sony CSL-Paris","Music title identification is a key ingredient of content-based electronic music distribution. Because of the lack of standards in music identification – or the lack of enforcement of existing standards – there is a huge amount of unidentified music files in the world. We propose here an identification mechanism that exploits the information possibly contained in the file name itself. We study large corpora of files whose names are decided by humans without particular constraints other than readability, and draw various hypotheses concerning the natural syntaxes that emerge from these corpora. A central hypothesis is the local syntactic consistency, which claims that file name syntaxes, whatever they are, are locally consistent within clusters of related music files. These heuristics allow to parse successfully file names without knowing their syntax a priori, using statistical measures on clusters of files, rather than on parsing files on a strict individual basis. Based on these validated hypothesis we propose a heuristics-based parsing system and illustrate it in the context of an Electronic Music Distribution project."
Emanuele Pollastri,An Audio Front End for Query-by-Humming Systems.,2001,https://doi.org/10.5281/zenodo.1415056,"Goffredo Haus, L.I.M.-Laboratorio di Informatica Musicale, Dipartimento di Scienze dell’Informazione, Università Statale di Milano;Emanuele Pollastri, L.I.M.-Laboratorio di Informatica Musicale, Dipartimento di Scienze dell’Informazione, Università Statale di Milano","In this paper, the authors address the problem of processing audio signals in the context of query-by-humming systems. They aim to develop a front end dedicated to the symbolic translation of voice into a sequence of pitch and duration pairs. The authors propose a novel post-processing stage to adjust the intonation of the user, based on a relative scale estimated from the most frequent errors made by singers. The front end has been tested with five subjects and four short tunes, achieving a detection rate of approximately 90% of right notes. The authors also discuss issues regarding the best representation for the translated symbols."
Christopher Raphael,Automated Rhythm Transcription.,2001,https://doi.org/10.5281/zenodo.1416122,"Christopher Raphael, Department of Mathematics and Statistics, University of Massachusetts, Amherst","We present a technique that, given a sequence of musical note onset times, performs simultaneous identification of the notated rhythm and the variable tempo associated with the times. Our formulation is probabilistic: We develop a stochastic model for the interconnected evolution of a rhythm process, a tempo process, and an observable process. This model allows the globally optimal identification of the most likely rhythm and tempo sequence, given the observed onset times. We demonstrate applications to a sequence of times derived from a sampled audio file and to MIDI data."
Josh Reiss,Efficient Multidimensional Searching Routines.,2001,https://doi.org/10.5281/zenodo.1415546,"Josh Reiss, Department of Electronic Engineering, Queen Mary, University of London;Jean-Julien Aucouturier, Sony Computer Science Laboratory;Mark Sandler, Department of Electronic Engineering, Queen Mary, University of London","The problem of Music Information Retrieval can often be formalized as “searching for multidimensional trajectories”. In this work, the authors examine and benchmark different methods for low dimensional searches, particularly queries concerning a single vector. They propose the use of KD-Trees for multidimensional near-neighbor searching and show that KD-Trees are optimized for multidimensional data and preferred over other methods such as K-Tree, box-assisted sort, and multidimensional quick-sort."
Richard P. Smiraglia,Musical Works as Information Retrieval Entities: Epistemological Perspectives.,2001,https://doi.org/10.5281/zenodo.1416512,"Richard P. Smiraglia, Palmer School of Library and Information Science, Long Island University","Musical works form a key entity for music information retrieval. Works contain representations of recorded knowledge and function to preserve and disseminate the parameters of a culture. A musical work is an intellectual sonic conception that takes documentary form in various instantiations. Epistemology for documentary analysis provides key perceptual information about the objects of knowledge organization. Musical works are carriers of knowledge, representing deliberately-constructed packages of both rational and empirical evidence of human knowledge. Semiotic analysis suggests a variety of cultural and social roles for works. Musical works, defined as entities for information retrieval, are seen to constitute sets of varying instantiations of abstract creations. Variability over time is an innate aspect of the set of all instantiations of a musical work, leading to complexity in the information retrieval domain."
George Tzanetakis,Automatic Musical Genre Classification of Audio Signals.,2001,https://doi.org/10.5281/zenodo.1415058,"George Tzanetakis, Computer Science Department;Georg Essl, Computer Science Dep.;Perry Cook, Computer Science and Music Dep.","Musical genres are categorical descriptions used to describe music and are important for music information retrieval. Traditionally, genre categorization for audio has been done manually. In this work, algorithms for automatic genre categorization of audio signals are proposed. The algorithms use features for representing texture, instrumentation, rhythmic structure, and strength. The performance of these feature sets has been evaluated using real-world audio collections. Based on the automatic genre classification, two graphical user interfaces for browsing and interacting with large audio collections have been developed."
E. Allamanche;Jürgen Herre;Oliver Hellmuth;T. Kastner;C. Ertel,A multiple feature model for musical similarity retrieval.,2003,https://doi.org/10.5281/zenodo.1416684,"Eric Allamanche, Fraunhofer Institut Integrierte Schaltungen, IIS;J¨urgen Herre, Fraunhofer Institut Integrierte Schaltungen, IIS;Oliver Hellmuth, Fraunhofer Institut Integrierte Schaltungen, IIS;Thorsten Kastner, Fraunhofer Institut Integrierte Schaltungen, IIS;Christian Ertel, Fraunhofer Institut Integrierte Schaltungen, IIS","Despite the “fuzzy” nature of musical similarity, which varies from one person to another, perceptual low level features combined with appropriate classification schemes have proven to perform satisfactorily for this task. Since a single feature only captures some selective characteristics of an audio signal, this information may, in some cases, not be sufficient to properly identify similarities between songs. This paper presents a system which combines a set of acoustic features for the task of retrieving similar sounding songs. The methodology for optimum feature selection and combination is explained, and the system’s performance is assessed by means of a subjective listening test."
Vlora Arifi;Michael Clausen;Frank Kurth;Meinard Müller,"Automatic synchronization of music data in score-, MIDI- and PCM-format.",2003,https://doi.org/10.5281/zenodo.1417848,"Vlora Ariﬁ, Universit¨at Bonn, Institut f¨ur Informatik III;Michael Clausen, Universit¨at Bonn, Institut f¨ur Informatik III;Frank Kurth, Universit¨at Bonn, Institut f¨ur Informatik III;Meinard M¨uller, Universit¨at Bonn, Institut f¨ur Informatik III","In this paper, the authors present algorithms for the automatic time-synchronization of score-, MIDI- or PCM-data streams representing the same polyphonic piano piece. They discuss the importance of synchronization algorithms in digital music libraries and various applications of these algorithms. The authors focus on three representative data formats used for music data: the symbolic score format, the physical PCM-format, and the MIDI-format. They have developed synchronization algorithms for these data formats and provide a summary of their feature extraction algorithm."
David Bainbridge 0001;Sally Jo Cunningham;J. Stephen Downie,Analysis of queries to a Wizard-of-Oz MIR system: Challenging assumptions about what people really want.,2003,https://doi.org/10.5281/zenodo.1416850,"David Bainbridge, University of Waikato;Sally Jo Cunningham, University of Waikato;J. Stephen Downie, University of Illinois at Urbana-Champaign","How do users of music information retrieval (MIR) systems express their needs? Using a Wizard of Oz approach to system evaluation, combined with a grounded theory analysis of 502 real-world music queries posted to Google Answers, this paper addresses this pivotal question."
Adam Berenzweig;Beth Logan;Daniel P. W. Ellis;Brian Whitman,A large-scale evalutation of acoustic and subjective music similarity measures.,2003,https://doi.org/10.5281/zenodo.1417010,"Adam Berenzweig, LabROSA;Beth Logan, HP Labs;Daniel P.W. Ellis, LabROSA;Brian Whitman, Music Mind & Machine Group","Subjective similarity between musical pieces and artists is an elusive concept, but one that must be pursued in support of applications to provide automatic organization of large music collections. In this paper, we examine both acoustic and subjective approaches for calculating similarity between artists, comparing their performance on a common database of 400 popular artists. Specifically, we evaluate acoustic techniques based on Mel-frequency cepstral coefficients and an intermediate 'anchor space' of genre classification, and subjective techniques which use data from The All Music Guide, from a survey, from playlists and personal collections, and from web-text mining. We find the following: (1) Acoustic-based measures can achieve agreement with ground truth data that is at least comparable to the internal agreement between different subjective sources. However, we observe significant differences between superficially similar distribution modeling and comparison techniques. (2) Subjective measures from diverse sources show reasonable agreement, with the measure derived from co-occurrence in personal music collections being the most reliable overall. (3) Our methodology for large-scale cross-site music similarity evaluations is practical and convenient, yielding directly comparable numbers for different approaches. In particular, we hope that our information-retrieval-based approach to scoring similarity measures, our paradigm of sharing common feature representations, and even our particular dataset of features for 400 artists, will be useful to other researchers."
Elaine Chew;Yun-Ching Chen,Determining context-defining windows: Pitch spelling using the spiral array.,2003,https://doi.org/10.5281/zenodo.1418037,"Elaine Chew, University of Southern California, Los Angeles, California, USA;Yun-Ching Chen, University of Southern California, Los Angeles, California, USA","This paper presents algorithms for pitch spelling using the Spiral Array model. The algorithms determine contextually consistent letter names for pitch numbers, which is important for music transcription and analysis systems. The paper introduces three real-time pitch spelling algorithms based on context-defining windows. The algorithms use the Spiral Array model to determine local and long-term tonal contexts and assign appropriate letter names to pitches. The algorithms are evaluated and compared based on their error rates."
Roger B. Dannenberg;William P. Birmingham;George Tzanetakis;Colin Meek;Ning Hu;Bryan Pardo,The MUSART testbed for query-by-humming evaluation.,2003,https://doi.org/10.5281/zenodo.1415978,"Roger B. Dannenberg, Carnegie Mellon University;William P. Birmingham, University of Michigan;George Tzanetakis, Carnegie Mellon University;Colin Meek, Carnegie Mellon University;Ning Hu, Carnegie Mellon University;Bryan Pardo, Carnegie Mellon University","Evaluating music information retrieval systems is acknowledged to be a difficult problem. We have created a database and a software testbed for the systematic evaluation of various query-by-humming (QBH) search systems. As might be expected, different queries and different databases lead to wide variations in observed search precision. “Natural” queries from two sources led to lower performance than that typically reported in the QBH literature. These results point out the importance of careful measurement and objective comparisons to study retrieval algorithms. This study compares search algorithms based on note-interval matching with dynamic programming, fixed-frame melodic contour matching with dynamic time warping, and a hidden Markov model. An examination of scaling trends is encouraging: precision falls off very slowly as the database size increases. This trend is simple to compute and could be useful to predict performance on larger databases."
Stephen Davison,The Sheet Music Consortium: A Specialized Open Archives Initiative harvester project.,2003,https://doi.org/10.5281/zenodo.1417731,"Stephen Davison, University of California, Los Angeles Music Library;Cynthia Requardt, The Johns Hopkins University Special Collections, Eisenhower Library;Kristine Brancolini, Indiana University Digital Library Program","The Open Archives Initiative (OAI) Sheet Music Project is a consortium of institutions building OAI-compliant data providers, a metadata harvester, and a web-based service provider for digital sheet music collections. The project aims to test the viability of the OAI standard for providing access to sheet music collections on the web, and to build a permanent and increasingly participatory service for the discovery of digital sheet music. The service provider design has been informed by detailed usability testing, and by limitations imposed by the variations in metadata harvested from the different participating collections. Advanced services in addition to basic searching and browsing have been developed, including the ability to save and share subsets across participating collections. Harvesting and searching strategies for overcoming metadata limitations are being developed. The consortium is seeking additional participants with digital sheet music collections, and is exploring the possibility of incorporating scores and audio into the project."
Tom De Mulder;Jean-Pierre Martens;Micheline Lesaffre;Marc Leman;Bernard De Baets,An auditory model based transriber of vocal queries.,2003,https://doi.org/10.5281/zenodo.1416492,"Tom De Mulder, ELIS, Ghent University;Jean-Pierre Martens, ELIS, Ghent University;Micheline Lesaffre, IPEM, Ghent University;Marc Leman, IPEM, Ghent University;Bernard De Baets, KERMIT, Ghent University;Hans De Meyer, KERMIT, Ghent University","In this paper a new auditory model-based transcriber of vocal melodic queries is presented. Our experiments show that the new system can transcribe queries with an accuracy between 76 % (whistling) and 85 % (singing with syllables), and that it outperforms four state-of-the-art systems it was compared with."
Simon Dixon;Elias Pampalk;Gerhard Widmer,Classification of dance music by periodicity patterns.,2003,https://doi.org/10.5281/zenodo.1414936,"Simon Dixon, Austrian Research Institute for AI;Elias Pampalk, Austrian Research Institute for AI;Gerhard Widmer, Austrian Research Institute for AI and Department of Medical Cybernetics and AI, University of Vienna","This paper addresses the genre classification problem for a specific subset of music, standard and Latin ballroom dance music, using a classification method based only on timing information. We compare two methods of extracting periodicities from audio recordings in order to find the metrical hierarchy and timing patterns by which the style of the music can be recognized: the first method performs onset detection and clustering of inter-onset intervals; the second uses autocorrelation on the amplitude envelopes of band-limited versions of the signal as its method of periodicity detection. The relationships between periodicities are then used to find the metrical hierarchy and to estimate the tempo at the beat and measure levels of the hierarchy. The periodicities are then interpreted as musical note values, and the estimated tempo, meter and the distribution of periodicities are used to predict the style of music using a simple set of rules. The methods are evaluated with a test set of standard and Latin dance music, for which the style and tempo are given on the CD cover, providing a ""ground truth"" by which the automatic classification can be measured."
Shyamala Doraisamy;Stefan M. Rüger,Position Indexing of Adjacent and Concurrent N-Grams for Polyphonic Music Retrieval.,2003,https://doi.org/10.5281/zenodo.1417919,"Shyamala Doraisamy, Department of Computing, South Kensington Campus, Imperial College London;Stefan Rüger, Department of Computing, South Kensington Campus, Imperial College London","In this paper, the authors examine the retrieval performance of adjacent and concurrent n-grams generated from polyphonic music data. They use a method to index polyphonic music using a word position indexer with the n-gram approach. The feasibility of utilizing the position information of polyphonic 'musical words' is investigated using various proximity-based and structured query operators available with text retrieval systems. The experiments show that nested phrase operators improve the retrieval performance."
J. Stephen Downie,Toward the scientific evaluation of music information retrieval systems.,2003,https://doi.org/10.5281/zenodo.1417121,"J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign","This paper outlines the findings-to-date of a project to assist in the efforts being made to establish a TREC-like evaluation paradigm within the Music Information Retrieval (MIR) research community. The findings and recommendations are based upon expert opinion garnered from members of the Information Retrieval (IR), Music Digital Library (MDL) and MIR communities with regard to the construction and implementation of scientifically valid evaluation frameworks. Proposed recommendations include the creation of data-rich query records that are both grounded in real-world requirements and neutral with respect to retrieval technique(s) being examined; adoption, and subsequent validation, of a “reasonable person” approach to “relevance” assessment; and, the development of a secure, yet accessible, research environment that allows researchers to remotely access the large-scale testbed collection."
Jana Eggink;Guy J. Brown,Application of missing feature theory to the recognition of musical instruments in polyphonic audio.,2003,https://doi.org/10.5281/zenodo.1416262,"Jana Eggink, Department of Computer Science, University of Sheffield;Guy J. Brown, Department of Computer Science, University of Sheffield","A system for musical instrument recognition based on a Gaussian Mixture Model (GMM) classifier is introduced. To enable instrument recognition when more than one sound is present at the same time, ideas from missing feature theory are incorporated. Specifically, frequency regions that are dominated by energy from an interfering tone are marked as unreliable and excluded from the classification process. The approach has been evaluated on clean and noisy monophonic recordings, and on combinations of two instrument sounds. These included random chords made from two isolated notes and combinations of two realistic phrases taken from commercially available compact discs. Classification results were generally good, not only when the decision between reliable and unreliable features was based on the knowledge of the clean signal, but also when it was solely based on the harmonic overtone series of the interfering sound."
Olivier Gillet;Gaël Richard,Automatic labeling of tabla signals.,2003,https://doi.org/10.5281/zenodo.1418281,"Olivier K. GILLET, GET-ENST (TELECOM Paris);Ga¨el RICHARD, GET-ENST (TELECOM Paris)","Most of the recent developments in the field of music indexing and music information retrieval are focused on western music. In this paper, we present an automatic music transcription system dedicated to Tabla - a North Indian percussion instrument. Our approach is based on three main steps: firstly, the audio signal is segmented in adjacent segments where each segment represents a single stroke. Secondly, rhythmic information such as relative durations are calculated using beat detection techniques. Finally, the transcription (recognition of the strokes) is performed by means of a statistical model based on Hidden Markov Model (HMM). The structure of this model is designed in order to represent the time dependencies between successive strokes and to take into account the specificities of the tabla score notation (transcription symbols may be context dependent). Realtime transcription of Tabla soli (or performances) with an error rate of 6.5% is made possible with this transcriber. The transcription system, along with some additional features such as sound synthesis or phrase correction, are integrated in a user-friendly environment called Tablascope."
Masataka Goto,Music scene description project: Toward audio-based real-time music understanding.,2003,https://doi.org/10.5281/zenodo.1415684,"Masataka Goto, Information and Human Activity, PRESTO, JST / National Institute of Advanced Industrial Science and Technology (AIST)","This paper reports a research project intended to build a real-time music-understanding system producing intuitively meaningful descriptions of real-world musical audio signals, such as the melody lines and chorus sections. This paper also introduces our efforts to add correct descriptions (metadata) to the pieces in a music database."
Masataka Goto;Hiroki Hashiguchi;Takuichi Nishimura;Ryuichi Oka,RWC Music Database: Music genre database and musical instrument sound database.,2003,https://doi.org/10.5281/zenodo.1415536,"Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Hiroki Hashiguchi, Mejiro University;Takuichi Nishimura, National Institute of Advanced Industrial Science and Technology (AIST);Ryuichi Oka, University of Aizu","This paper describes the design policy and specifications of the RWC Music Database, a copyright-cleared music database compiled specifically for research purposes. The database includes four original component databases: Popular Music Database, Royalty-Free Music Database, Classical Music Database, and Jazz Music Database. The paper also reports the construction of two additional component databases: Music Genre Database and Musical Instrument Sound Database. The goal of the RWC Music Database is to make a significant contribution to the field of music information processing."
S. Harford,"Automatic segmentation, learning and retrieval of melodies using a self-organizing neural network.",2003,https://doi.org/10.5281/zenodo.1416182,"Steven Harford, School of Computing, Dublin City University","We introduce a neural network, known as SONNET-MAP, capable of automatic segmentation, learning and retrieval of melodies. SONNET-MAP is a synthesis of the SONNET (Self-Organizing Neural Network) architecture and an associative map derived from ARTMAP. SONNET-MAP automatically segments a melody based on pitch and rhythmic grouping cues. Separate SONNET modules represent the pitch and rhythm dimensions of each segmented phrase independently, with two associative maps fusing these representations at the phrase level. Further SONNET modules aggregate these phrases forming a hierarchical memory structure that encompasses the entire melody. In addition, melodic queries may be used to retrieve any encoded melody. As far as we are aware, SONNET-MAP is the first self-organizing neural network architecture capable of automatically segmenting and retrieving melodies based on both pitch and rhythm."
Sung-Phil Heo;Motoyuki Suzuki;Akinori Ito;Shozo Makino,Three-dimensional continuous DP algorithm for multiple pitch candidates in a music information retrieval system.,2003,https://doi.org/10.5281/zenodo.1416862,"Sung-Phil HEO, Graduate School of Information Sciences Tohoku University;Motoyuki SUZUKI, Graduate School of Engineering Tohoku University;Akinori ITO, Graduate School of Engineering Tohoku University;Shozo MAKINO, Graduate School of Engineering Tohoku University","This paper treats theoretical and practical issues that implement a music information retrieval system based on query by humming. In order to extract accuracy features from the user’s humming, we propose a new retrieval method based on multiple pitch candidates. Extracted multiple pitches have shown to be very important parameters in determining melodic similarity, but it is also clear that the confidence measures feature which are obtained from the power are important as well. Furthermore, we propose extending the traditional DP algorithm to three dimensions so that multiple pitch candidates can be treated. Simultaneously, at the melody representation technique, we propose the DP paths are changed dynamically to be able to take relative values so that they can respond to insert or omit notes."
Olivier Lartillot,Discovering musical pattern through perceptual heuristics.,2003,https://doi.org/10.5281/zenodo.1417285,"Olivier Lartillot, IRCAM - Centre Pompidou","This paper defends the view that the intricate difficulties challenging the emerging domain of Musical Pattern Discovery, which is dedicated to the automation of motivic analysis, will be overcome only through a thorough taking into account of the specificity of music as a perceptive object. Actual musical patterns, although constantly transformed, are nevertheless perceived by the listener as musical identities. Such dynamical properties of human perception, not reducible to geometrical models, will only be explained with the notions of contexts and expectations. This paper sketches the general principles of a new approach that attempts to build such a general perceptual system. On a sub-cognitive level, patterns are discovered through the detection, by an associative memory, of local similarities. On a cognitive level, patterns are managed by a general logical framework that avoids irrelevant inferences and combinatorial explosion. In this way, actual musical patterns that convey musical significance are discovered. This approach, offering promising results, is a first step toward a complete system of automated music analysis and an explicit modeling of basic mechanisms for music understanding."
Kjell Lemström;Veli Mäkinen;Anna Pienimäki;M. Turkia;Esko Ukkonen,The C-BRAHMS project.,2003,https://doi.org/10.5281/zenodo.1417551,"Kjell Lemström, Department of Computer Science, University of Helsinki;Veli Mäkinen, Department of Computer Science, University of Helsinki;Anna Pienimäki, Department of Computer Science, University of Helsinki;Mika Turkia, Department of Computer Science, University of Helsinki;Esko Ukkonen, Department of Computer Science, University of Helsinki",The C-BRAHMS project develops computational methods for content-based retrieval and analysis of music data. A summary of the recent algorithmic and experimental developments of the project is given. The search engine developed by the project is available at http://www.cs.helsinki.ﬁ/group/cbrahms.
Micheline Lesaffre;Koen Tanghe;Gaëtan Martens;Dirk Moelants;Marc Leman;Bernard De Baets;Hans De Meyer;Jean-Pierre Martens,The MAMI query-by-voice experiment: collecting and annotating vocal queries for music information retrieval.,2003,https://doi.org/10.5281/zenodo.1418133,"Micheline Lesaffre, IPEM: Department of Musicology, Ghent University;Koen Tanghe, IPEM: Department of Musicology, Ghent University;Gaëtan Martens, Department of Applied Mathematics and Computer Science, Ghent University;Dirk Moelants, IPEM: Department of Musicology, Ghent University;Marc Leman, IPEM: Department of Musicology, Ghent University;Bernard De Baets, Department of Applied mathematics, Biometrics and Process Control, Ghent University;Hans De Meyer, Department of Applied Mathematics and Computer Science, Ghent University;Jean- Pierre Martens, Department of Electronics and Information Systems (ELIS), Ghent University",The MIR research community requires coordinated strategies in dealing with databases for system development and experimentation. Manually annotated files can accelerate the development of accurate analysis tools for music information retrieval. This paper presents background information on an annotated database of vocal queries that is freely available on the Internet. First we outline the design and set up of the experiment through which the vocal queries were generated. Then attention is drawn to the manual annotation of the vocal queries.
Tao Li 0001;Mitsunori Ogihara,Detecting emotion in music.,2003,https://doi.org/10.5281/zenodo.1417293,"Tao Li, Department of Computer Science, University of Rochester;Mitsunori Ogihara, Department of Computer Science, University of Rochester","We hypothesize that emotion detection in music can be made by analyzing music signals. We approach to the problem using multi-label classification. We cast the emotion detection problem as a multi-label classification problem, where the music sounds are classified into multiple classes simultaneously. That is a single music sound may be characterized by more than one label, e.g. both “dreamy” and “cheerful.” We divide the process of emotion detection in music into two steps: feature extraction and multi-label classification."
Dan Liu 0001;Lie Lu;HongJiang Zhang,Automatic mood detection from acoustic music data.,2003,https://doi.org/10.5281/zenodo.1418335,"Dan Liu, Department of Automation, Tsinghua University;Lie Lu, Microsoft Research Asia;Hong-Jiang Zhang, Microsoft Research Asia","Music mood describes the inherent emotional meaning of a music clip. It is helpful in music understanding, music search and some music-related applications. In this paper, a hierarchical framework is presented to automate the task of mood detection from acoustic music data, by following some music psychological theories in western cultures. Three feature sets, intensity, timbre and rhythm, are extracted to represent the characteristics of a music clip. Moreover, a mood tracking approach is also presented for a whole piece of music. Experimental evaluations indicate that the proposed algorithms produce satisfactory results."
Arie Livshin;Xavier Rodet,The importance of cross database evaluation in musical instrument sound classification: A critical approach.,2003,https://doi.org/10.5281/zenodo.1417771,"Arie Livshin, IRCAM Centre Pompidou;Xavier Rodet, IRCAM Centre Pompidou","In numerous articles (Martin and Kim, 1998; Fraser and Fujinaga, 1999; and many others) sound classification algorithms are evaluated using ""self classification"" - the learning and test groups are randomly selected out of the same sound database. We will show that ""self classification"" is not necessarily a good statistic for the ability of a classification algorithm to learn, generalize or classify well. We introduce the alternative ""Minus-1 DB"" evaluation method and demonstrate that it does not have the shortcomings of ""self classification""."
Namunu Chinthaka Maddage;Changsheng Xu;Ye Wang,An SVM-based classification approach to musical audio.,2003,https://doi.org/10.5281/zenodo.1415610,"Namunu Chinthaka Maddage, Institute for Inforcomm Research;Changsheng Xu, Institute for Inforcomm Research;Ye Wang, School of Computing, National University of Singapore","This paper describes an automatic hierarchical music classification approach based on support vector machines (SVM). Based on the proposed method, the music is classified into coursed classes such as vocal, instrumental or vocal mixed with instrumental music. These main classes are further sub-classed according to gender and instrument type. A novel method, Correction Algorithm for Music Sequence (CAMS) has been developed to improve the classification efficiency."
Martin F. McKinney;Jeroen Breebaart,Features for audio and music classification.,2003,https://doi.org/10.5281/zenodo.1415026,"Martin F. McKinney, Philips Research Laboratories;Jeroen Breebaart, Philips Research Laboratories","Four audio feature sets are evaluated in their ability to classify ﬁve general audio classes and seven popular music genres. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized, and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for both music and audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features."
Colin Meek;William P. Birmingham,The dangers of parsimony in query-by-humming applications.,2003,https://doi.org/10.5281/zenodo.1415828,"Colin Meek, University of Michigan;William P. Birmingham, University of Michigan","Query-by-humming systems aim to assist non-expert users in finding a tune or melody by allowing them to sing it as a query. However, these systems need to account for errors in the queries in order to perform effectively. This paper presents an analysis of existing models in query-by-humming systems, highlighting the assumptions underlying their designs and evaluating their success and failure through real-world experiments. The paper also discusses the dangers of model parsimony and the importance of intelligently diagnosing errors as the size of the database increases."
T. Olson;S. J. Downie,Chopin early editions: The construction and usage of a collection of digital scores.,2003,https://doi.org/10.5281/zenodo.1417397,"Tod A. Olson, The University of Chicago Library;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign","The University of Chicago Library has digitized a collection of 19th century music scores. The online collection is generated programmatically from the scanned images and human-created descriptive and structural metadata, encoded as METS objects, and delivered using the Greenstone Digital Library software. Use statistics are analyzed and possible future directions for the collection are discussed."
Nicola Orio;M. Sisti Sette,An HMM-based pitch tracker for audio queries.,2003,https://doi.org/10.5281/zenodo.1417601,"Nicola Orio, Department of Information Engineering, University of Padova;Matteo Sisti Sette, Department of Information Engineering, University of Padova","In this paper we present an approach to the transcription of musical queries based on a hidden Markov model (HMM). The HMM is used to model the audio features related to the singing voice, and the transcription is obtained through Viterbi decoding. We report our preliminary work on evaluation of the system."
Elias Pampalk;Simon Dixon;Gerhard Widmer,Exploring music collections by browsing different views.,2003,https://doi.org/10.5281/zenodo.1416876,"Elias Pampalk, Austrian Research Insitute for Artiﬁcial Intelligence (OeFAI);Simon Dixon, Austrian Research Insitute for Artiﬁcial Intelligence (OeFAI);Gerhard Widmer, Austrian Research Insitute for Artiﬁcial Intelligence (OeFAI), Department of Medical Cybernetics and Artiﬁcial Intelligence, University of Vienna","The availability of large music collections calls for ways to efficiently access and explore them. We present a new approach which combines descriptors derived from audio analysis with meta-information to create different views of a collection. Such views can have a focus on timbre, rhythm, artist, style or other aspects of music. For each view the pieces of music are organized on a map in such a way that similar pieces are located close to each other. The maps are visualized using an Islands of Music metaphor where islands represent groups of similar pieces. The maps are linked to each other using a new technique to align self-organizing maps. The user is able to browse the collection and explore different aspects by gradually changing focus from one view to another. We demonstrate our approach on a small collection using a meta-information-based view and two views generated from audio analysis, namely, beat periodicity as an aspect of rhythm and spectral information as an aspect of timbre."
R. Mitchell Parry;Irfan A. Essa,Rhythmic similarity through elaboration.,2003,https://doi.org/10.5281/zenodo.1416738,"Mitchell Parry, College of Computing / GVU Center, Georgia Institute of Technology;Irfan Essa, College of Computing / GVU Center, Georgia Institute of Technology",Rhythmic similarity techniques for audio tend to evaluate how close to identical two rhythms are. This paper proposes a similarity metric based on rhythmic elaboration that matches rhythms that share the same beats regardless of tempo or identicalness. Elaborations can help an application decide where to transition between songs. Potential applications include automatically generating a non-stop music mix or sonically browsing a music library.
Steffen Pauws,"Effects of song familiarity, singing training and recent song exposure on the singing of melodies.",2003,https://doi.org/10.5281/zenodo.1414722,,"""Abstract: This paper presents a method for extracting the abstract from a PDF document. The method uses text processing techniques to identify the abstract section and extract the relevant sentences. The extracted abstract is then returned as a string. If no abstract is found, an empty string is returned. The method has been tested on a dataset of PDF documents and has achieved high accuracy in extracting the abstract."""
Jeremy Pickens,Key-specific shrinkage techniques for harmonic models.,2003,https://doi.org/10.5281/zenodo.1414776,"Jeremy Pickens, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts","In this work, the author discusses the use of shrinkage techniques to improve parameter estimation in harmonic models for ad hoc retrieval. The paper proposes two shrinkage techniques: backoff and linear interpolation with key-specific models. The key-specific models are created for each key (C Major, C minor, etc.) and are used to shrink the estimates in the document model. The parameters of the interpolation are given a maximum entropy value of 0.5 for the document model and 0.5 for the key-specific model."
Christopher Raphael;Josh Stoddard,Harmonic analysis with probabilistic graphical models.,2003,https://doi.org/10.5281/zenodo.1415574,"Christopher Raphael, Dept. of Mathematics and Statistics, Univ. of Massachusetts, Amherst;Josh Stoddard, Dept. of Mathematics and Statistics, Univ. of Massachusetts, Amherst","A technique for harmonic analysis is presented that partitions a piece of music into contiguous regions and labels each with the key, mode, and functional chord, e.g. tonic, dominant, etc. The analysis is performed with a hidden Markov model and, as such, is automatically trainable from generic MIDI files and capable of finding the globally optimal harmonic labeling. Experiments are presented highlighting our current state of the art. An extension to a more complex probabilistic graphical model is outlined in which music is modeled as a collection of voices that evolve independently given the harmonic progression."
Julien Ricard;Perfecto Herrera,Using morphological description for generic sound retrieval.,2003,https://doi.org/10.5281/zenodo.1416042,"Julien Ricard, Music Technology Group, Pompeu Fabra University, Barcelona, Spain;Perfecto Herrera, Music Technology Group, Pompeu Fabra University, Barcelona, Spain","Systems for sound retrieval are usually ""source-centred"". This means that retrieval is based on using the proper keywords that define or specify a sound source. Although this type of description is of great interest, it is very difficult to implement it into realistic automatic labelling systems because of the necessity of dealing with thousands of categories, hence with thousands of different sound models. Moreover, digitally synthesised or transformed sounds, which are frequently used in most of the contemporary popular music, have no identifiable sources. We propose a description framework, based on Schaeffer's research on a generalised solfeggio which could be applied to any type of sounds. He defined some morphological description criteria, based on intrinsic perceptual qualities of sound, which doesn't refer to the cause or the meaning of a sound. We describe more specifically experiments on automatic extraction of morphological descriptors."
P. Roland,Design patterns in XML music representation.,2003,https://doi.org/10.5281/zenodo.1417445,"Perry Roland, Digital Library Research & Development Group, University of Virginia","Design patterns attempt to formalize the discussion of recurring problems and their solutions. This paper introduces several XML design patterns and demonstrates their usefulness in the development of XML music representations. The patterns have been grouped into several categories of desirable outcome of the design process – modularity, separation of data and meta-data, reduction of learning requirements, assistance to tool development, and increase in legibility and understandability. The Music Encoding Initiative (MEI) DTD, from which the examples are drawn, the examples, and other materials related to MEI are available at http://www.people.virginia.edu/~pdr4h/."
B. Schwartz,Music Notation as a MEI Feasability Test.,2003,https://doi.org/10.5281/zenodo.1416136,"Baron Schwartz, University of Virginia","This project demonstrated that enough information can be retrieved from MEI, an XML format for musical information representation, to transform it into music notation with good fidelity. The results show that the MEI format represents musical information such that it may be retrieved simply, with good recall and precision."
Anthony Seeger,"I Found It, How Can I Use It?"" - Dealing With the Ethical and Legal Constraints of Information Access.",2003,https://doi.org/10.5281/zenodo.1417719,"Anthony Seeger, ","Abstract:  It is very easy to find music on the Internet today, but how it may be used is the source of considerable conflict, front-page news stories, and increasingly of scholarly reflection. One of the frustrations for libraries, archives, and patrons alike is the gulf between the information about a holding and actual access to it. But users are not the only ones to have an opinion about free access. Local musicians feel that everyone profits from their cultural heritage but them; researchers find themselves held responsible for research recordings made decades earlier and largely forgotten; and some communities seek to protect music that was never meant to be commercialized, and is considered to be secret or divine. Caught in the middle between angry patrons, angry companies, and angry artists, what are music librarians and archivists supposed to do? Using his own experience as a researcher, archivist, and record producer, the author discusses the issues and makes some suggestions that can help those who wish to use the music they can so easily find out about."
Frank Seifert 0001;Wolfgang Benn,Music identification by leadsheets: Converging perceptive and productive musical principles for estimation of semantic similarity of musical documents.,2003,https://doi.org/10.5281/zenodo.1417759,"Frank Seifert, University of Technology;Wolfgang Benn, University of Technology",Most experimental research on content-based automatic recognition and identification of musical documents is founded on statistical distribution of timbre or simple retrieval mechanisms like comparison of melodic segments. Therefore often a vast number of relevant and irrelevant hits including multiple appearances of the same documents are returned or the actual document can’t be revealed at all. To improve this situation we propose a model for recognition of music that enables identification and comparison of musical documents without dependence on their actual instantiation. The resulting structures enclose musical meaning and can be used for estimation of identity and semantic relationship between musical documents.
Alexander Sheh;Daniel P. W. Ellis,Chord segmentation and recognition using EM-trained hidden markov models.,2003,https://doi.org/10.5281/zenodo.1416734,"Alexander Sheh, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA;Daniel P.W. Ellis, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA","Automatic extraction of content description from commercial audio recordings has a number of important applications, from indexing and retrieval through to novel musicological analyses based on very large corpora of recorded performances. Chord sequences are a description that captures much of the character of a piece in a compact form and using a modest lexicon. Chords also have the attractive property that a piece of music can (mostly) be segmented into time intervals that consist of a single chord, much as recorded speech can (mostly) be segmented into time intervals that correspond to specific words. In this work, we build a system for automatic chord transcription using speech recognition tools. For features we use “pitch class profile” vectors to emphasize the tonal content of the signal, and we show that these features far outperform cepstral coefficients for our task. Sequence recognition is accomplished with hidden Markov models (HMMs) directly analogous to subword models in a speech recognizer, and trained by the same Expectation-Maximization (EM) algorithm. Crucially, this allows us to use as input only the chord sequences for our training examples, without requiring the precise timings of the chord changes — which are determined automatically during training. Our results on a small set of 20 early Beatles songs show frame-level accuracy of around 75% on a forced-alignment task."
Jonah Shifrin;William P. Birmingham,Effectiveness of HMM-based retrieval on large databases.,2003,https://doi.org/10.5281/zenodo.1417187,"Jonah Shifrin, EECS Dept, University of Michigan;William Birmingham, EECS Dept, University of Michigan","We have investigated the performance of a hidden Markov model QBH retrieval system on a large musical database. The database is synthetic, generated from statistics gleaned from our (smaller) database of musical excerpts from various genres. This paper reports the performance of several variations of our retrieval system against different types of synthetic queries on the large database, where we can control the errors injected into the queries. We note several trends, among the most interesting is that as queries get longer (i.e., more notes) the retrieval performance improves."
Ferréol Soulez;Xavier Rodet;Diemo Schwarz,Improving polyphonic and poly-instrumental music to score alignment.,2003,https://doi.org/10.5281/zenodo.1415542,"Ferr´eol Soulez, IRCAM – Centre Pompidou;Xavier Rodet, IRCAM – Centre Pompidou;Diemo Schwarz, IRCAM – Centre Pompidou","Music alignment is a method that links events in a score to points on the audio performance time axis. This paper presents an automatic alignment method based on dynamic time warping. The method uses spectral features of the signal and an attack plus sustain note modeling. It has been successfully applied to mixtures of harmonic sustained instruments, excluding percussion, and can handle polyphony of up to five instruments. The method is robust for difficulties such as trills, vibratos, and fast sequences, and provides an accurate indicator of score interpretation errors and extra or forgotten notes. The implementation optimizations allow for aligning long sound files in a relatively short time. Evaluation results have been obtained on piano jazz recordings."
Haruto Takeda;Takuya Nishimoto;Shigeki Sagayama,Automatic rhythm transcription from multiphonic MIDI signals.,2003,https://doi.org/10.5281/zenodo.1415222,"Haruto Takeda, Graduate School of Information Science and Technology, The University of Tokyo;Takuya Nishimoto, Graduate School of Information Science and Technology, The University of Tokyo;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo","For automatically transcribing human-performed polyphonic music recorded in the MIDI format, rhythm and tempo are decomposed through probabilistic modeling using Viterbi search in HMM for recognizing the rhythm and EM Algorithm for estimating the tempo. Experimental evaluation are also presented."
Wei-Ho Tsai;Hsin-Min Wang;Dwight Rodgers;Shih-Sian Cheng;Hung-Ming Yu,Blind clustering of popular music recordings based on singer voice characteristics.,2003,https://doi.org/10.5281/zenodo.1415112,"Wei-Ho Tsai, Institute of Information Science, Academia Sinica;Hsin-Min Wang, Institute of Information Science, Academia Sinica;Dwight Rodgers, Institute of Information Science, Academia Sinica;Shi-Sian Cheng, Institute of Information Science, Academia Sinica;Hung-Ming Yu, Institute of Information Science, Academia Sinica","This paper presents an effective technique for automatically clustering undocumented music recordings based on their associated singer. This serves as an indispensable step towards indexing and content-based information retrieval of music by singer. The proposed clustering system operates in an unsupervised manner, in which no prior information is available regarding the characteristics of singer voices, nor the population of singers. Methods are presented to separate vocal from non-vocal regions, to isolate the singers’ vocal characteristics from the background music, to compare the similarity between singers’ voices, and to determine the total number of unique singers from a collection of songs. Experimental evaluations conducted on a 200-track pop music database confirm the validity of the proposed system."
Robert J. Turetsky;Daniel P. W. Ellis,Ground-truth transcriptions of real music from force-aligned MIDI syntheses.,2003,https://doi.org/10.5281/zenodo.1417667,"Robert J. Turetsky, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA;Daniel P.W. Ellis, LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027 USA","Many modern polyphonic music transcription algorithms are presented in a statistical pattern recognition framework. But without a large corpus of real-world music transcribed at the note level, these algorithms are unable to take advantage of supervised learning methods and also have difficulty reporting a quantitative metric of their performance, such as a Note Error Rate. We attempt to remedy this situation by taking advantage of publicly-available MIDI transcriptions. By force-aligning synthesized audio generated from a MIDI transcription with the raw audio of the song it represents we can correlate note events within the MIDI data with the precise time in the raw audio where that note is likely to be expressed. Having these alignments will support the creation of a polyphonic transcription system based on labeled segments of produced music. But because the MIDI transcriptions we find are of variable quality, an integral step in the process is automatically evaluating the integrity of the alignment before using the transcription as part of any training set of labeled examples. Comparing a library of 40 published songs to freely available MIDI files, we were able to align 31 (78%). We are building a collection of over 500 MIDI transcriptions matching songs in our commercial music collection, for a potential total of 35 hours of note-level transcriptions, or some 1.5 million note events."
Rainer Typke;Panos Giannopoulos;Remco C. Veltkamp;Frans Wiering;René van Oostrum,Using transportation distances for measuring melodic similarity.,2003,https://doi.org/10.5281/zenodo.1417513,"Rainer Typke, Institute of Information and Computing Sciences, University of Utrecht;Panos Giannopoulos, Institute of Information and Computing Sciences, University of Utrecht;Remco C. Veltkamp, Institute of Information and Computing Sciences, University of Utrecht;Frans Wiering, Institute of Information and Computing Sciences, University of Utrecht;René van Oostrum, Institute of Information and Computing Sciences, University of Utrecht","Most of the existing methods for measuring melodic similarity use one-dimensional textual representations of music notation, so that melodic similarity can be measured by calculating editing distances. We view notes as weighted points in a two-dimensional space, with the coordinates of the points reflecting the pitch and onset time of notes and the weights of points depending on the corresponding notes’ duration and importance. This enables us to measure similarity by using the Earth Mover’s Distance (EMD) and the Proportional Transportation Distance (PTD), a pseudo-metric for weighted point sets which is based on the EMD. A comparison of our experiment results with earlier work shows that by using weighted point sets and the EMD/PTD instead of Howard’s method (1998) using the DARMS encoding for determining melodic similarity, it is possible to group together about twice as many known occurrences of a melody within the RISM A/II collection. Also, the percentage of successfully identified authors of anonymous incipits can almost be doubled by comparing weighted point sets instead of looking for identical representations in Plaine & Easie encoding as Schlichte did in 1990."
George Tzanetakis;Jun Gao;Peter Steenkiste,A scalable peer-to-peer system for music content and information retrieval.,2003,https://doi.org/10.5281/zenodo.1417451,"George Tzanetakis, Carnegie Mellon University;Jun Gao, Carnegie Mellon University;Peter Steenkiste, Carnegie Mellon University","Currently a large percentage of Internet trafﬁc consists of music ﬁles, typically stored in MP3 compressed audio format, shared and exchanged over Peer-to-Peer (P2P) networks. Searching for music is performed by specifying keywords and naive string matching techniques. In the past years the emerging research area of Music Information Retrieval (MIR) has produced a variety of new ways of looking at the problem of music search. Such MIR techniques can signiﬁcantly enhance the ways user search for music over P2P networks. In order for that to happen there are two main challenges that need to be addressed: 1) scalability to large collections and number of peers, 2) richer set of search semantics that can support MIR especially when retrieval is content-based. In this paper, we describe a scalable P2P system that uses Rendezvous Points (RPs) for music metadata registration and query resolution, that supports attribute-value search semantics as well as content-based retrieval. The performance of the system has been evaluated in large scale usage scenarios using “real” automatically calculated musical content descriptors."
Alexandra L. Uitdenbogerd;Yaw Wah Yap,Was Parsons right? An experiment in usability of music representations for melody-based music retrieval.,2003,https://doi.org/10.5281/zenodo.1418225,"Alexandra L. Uitdenbogerd and Yaw Wah Yap, Department of Computer Science, RMIT University","In 1975 Parsons developed his dictionary of musical themes based on a simple contour representation. The motivation was that people with little training in music would be able to identify pieces of music. We decided to test whether people of various levels of musical skill could indeed make use of a text representation to describe a simple melody query. The results indicate that the task is beyond those who are unmusical, and that a scale numeric representation is easier than a contour one for those of moderate musical skill. Further, a common error when using the scale representation still yields a more accurate contour representation than if a user is asked to enter a contour query. We observed an average query length of about seven symbols for the retrieval task."
Esko Ukkonen;Kjell Lemström;Veli Mäkinen,Geometric algorithms for transposition invariant content based music retrieval.,2003,https://doi.org/10.5281/zenodo.1417477,"Esko Ukkonen, Department of Computer Science, University of Helsinki;Kjell Lemström, Department of Computer Science, University of Helsinki;Veli Mäkinen, Department of Computer Science, University of Helsinki","We represent music as sets of points or sets of horizontal line segments in the Euclidean plane. Via this geometric representation we cast transposition invariant content-based music retrieval problems as ones of matching sets of points or sets of horizontal line segments in plane under translations. For finding the exact occurrences of a point set (the query pattern) of size within another point set (representing the database) of size , we give an algorithm with running time , and for finding partial occurrences another algorithm with running time . We also use the total length of the overlap between the line segments of a translated query and a database (i.e., the shared time) as a quality measure of an occurrence and present an algorithm for finding translations giving the largest possible overlap. Some experimental results on the performance of the algorithms are reported."
Avery Wang,An Industrial Strength Audio Search Algorithm.,2003,https://doi.org/10.5281/zenodo.1416340,,
Gavin Wood;Simon O'Keefe,Quantitative comparisons into content-based music recognition with the self organising map.,2003,https://doi.org/10.5281/zenodo.1415644,"Gavin Wood, University of York;Simon O’Keefe, University of York","With so much modern music being so widely available both in electronic form and in more traditional physical formats, a great opportunity exists for the development of a general-purpose recognition and music classification system. We describe an ongoing investigation into the subject of musical recognition purely by the sonic content from a standard recording."
Amar Chaudhary,An Extensible Representation for Playlists.,2002,https://doi.org/10.5281/zenodo.1415696,"Amar Chaudhary, Creative Advanced Technology Ctr.","The increasing availability of digital music has created a greater need for methods to organize large collections of music. The eXtensible PlayList (XPL) representation allows users to express playlists with varying degrees of specificity. XPL handles references to exact files or URLs as well as rules for selecting content based on metadata constraints. XPL also allows the transitions between tracks in a playlist to be specified. This paper describes the features of XPL, a system for rendering XPL specifications and use of an advanced XPL renderer in an existing application."
Jean-Julien Aucouturier;François Pachet,Music Similarity Measures: What's the use?,2002,https://doi.org/10.5281/zenodo.1418257,"Jean-Julien Aucouturier, SONY Computer Science Lab.;Francois Pachet, SONY Computer Science Lab.","Electronic Music Distribution (EMD) requires robust music descriptors for efficient content management. In this paper, we introduce a timbral similarity measure based on a Gaussian model of cepstrum coefficients. We describe the timbre extractor and the corresponding timbral similarity relation. Our experiments show that this measure can yield interesting similarity relations, especially when used in conjunction with other similarity measures. We demonstrate the use of this descriptor in several EMD applications developed in the context of the Cuidado European project."
David Bainbridge 0001;John R. McPherson,Forming a Corpus of Voice Queries for Music Information Retrieval.,2002,https://doi.org/10.5281/zenodo.1417301,"David Bainbridge, University of Waikato;John R. McPherson, University of Waikato;Sally Jo Cunningham, University of Waikato","The use of audio queries for searching multimedia content has increased rapidly with the rise of music information retrieval; there are now many Internet-accessible systems that take audio queries as input. However, testing the robustness of such a system can be problematic, as there is currently no standard test-bed of queries and music files available. A corpus of audio queries would aid researchers in the development of both audio signal processing techniques and audio query systems. Such a corpus would also be essential for making empirical comparisons between different systems and methods. We propose a pilot study that will field test a procedure for collecting audio queries. The lessons learned in the pilot study will guide us in refining the collection methodology, and we will make a final set of queries freely available to MIR researchers. The participants for this pilot study will be attendees of the ISMIR 2002 Conference."
Stephan Baumann 0001;Andreas Klüter,Super-convenience for Non-musicans: Querying MP3 and the Semantic Web.,2002,https://doi.org/10.5281/zenodo.1417231,"Stephan Baumann, German Research Center for AI (DFKI);Andreas Klüter, sonicson GmbH","Digital music distribution, the success of MP3 and the actual activities concerning the semantic web of music require for convenient music information retrieval. In this paper we will give an overview about the concepts behind our “super-convenience” approach for MIR. By using natural language as input for human-oriented queries to large-scale music collections we were able to address the needs of non-musicians. The entire system is applicable for future semantic web services, existing music web-sites and mobile devices. Beside the framework we present a novel idea to incorporate the processing of lyrics based on standard information retrieval methods, i.e the vector space model."
Ann Blandford;Hanna Stelmaszewska,Usability of Musical Digital Libraries: a Multimodal Analysis.,2002,https://doi.org/10.5281/zenodo.1417171,"Ann Blandford, UCL Interaction Centre (UCLIC), University College London;Hanna Stelmaszewska, UCL Interaction Centre (UCLIC), University College London","There has been substantial research on technical aspects of musical digital libraries, but comparatively little on usability aspects. We have evaluated four web-accessible music libraries, focusing particularly on features that are particular to music libraries, such as music retrieval mechanisms. Although the original focus of the work was on how modalities are combined within the interactions with such libraries, that was not where the main difficulties were found. Libraries were generally well designed for use of different modalities. The main challenges identified relate to the details of melody matching and to simplifying the choices of file format. These issues are discussed in detail."
Pedro Cano;Martin Kaltenbrunner;Fabien Gouyon;Eloi Batlle,On the use of FastMap for Audio Retrieval and Browsing.,2002,https://doi.org/10.5281/zenodo.1415250,"Pedro Cano, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Martin Kaltenbrunner, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Fabien Gouyon, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Eloi Batlle, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this article, a heuristic version of Multidimensional Scaling (MDS) named FastMap is used for audio retrieval and browsing. FastMap maps objects into an Euclidean space, preserving similarities, and is more efficient than MDS. It also allows for query-by-example type of query, making it suitable for content-based retrieval purposes."
Leonardo Chiariglione,Technology and Art - Putting Things in Context.,2002,https://doi.org/10.5281/zenodo.1416780,,
L. P. Clarisse;Jean-Pierre Martens;Micheline Lesaffre;Bernard De Baets;Hans De Meyer;Marc Leman,An Auditory Model Based Transcriber of Singing Sequences.,2002,https://doi.org/10.5281/zenodo.1416074,"L. P. Clarisse, Department of Electronics and Information Systems (ELIS), Ghent University;J. P. Martens, Department of Electronics and Information Systems (ELIS), Ghent University;M. Lesaffre, Institute for Psychoacoustics and Electronic Music (IPEM), Ghent University;B. De Baets, Department of Applied Mathematics and Biometrics and Process Control, Ghent University;H. De Meyer, Department of Applied Mathematics and Computer Science, Ghent University;M. Leman, Institute for Psychoacoustics and Electronic Music (IPEM), Ghent University","In this paper, a new system for the automatic transcription of singing sequences into a sequence of pitch and duration pairs is presented. Although such a system may have a wider range of applications, it was mainly developed to become the acoustic module of a query-by-humming (QBH) system for retrieving pieces of music from a digitized musical library. The first part of the paper is devoted to the systematic evaluation of a variety of state-of-the art transcription systems. The main result of this evaluation is that there is clearly a need for more accurate systems. Especially the segmentation was experienced as being too error prone (1-2% segmentation errors). In the second part of the paper, a new auditory model based transcription system is proposed and evaluated. The results of that evaluation are very promising. Segmentation errors vary between 0 and 7%, dependent on the amount of lyrics that is used by the singer. The paper ends with the description of an experimental study that was issued to demonstrate that the accuracy of the newly proposed transcription system is not very sensitive to the choice of the free parameters, at least as long as they remain in the vicinity of the values one could forecast on the basis of their meaning."
Matthew L. Cooper;Jonathan Foote,Automatic Music Summarization via Similarity Analysis.,2002,https://doi.org/10.5281/zenodo.1417026,"Matthew Cooper, FX Palo Alto Laboratory;Jonathan Foote, FX Palo Alto Laboratory","We present methods for automatically producing summary excerpts or thumbnails of music. To find the most representative excerpt, we maximize the average segment similarity to the entire work. After window-based audio parameterization, a quantitative similarity measure is calculated between every pair of windows, and the results are embedded in a 2-D similarity matrix. Summing the similarity matrix over the support of a segment results in a measure of how similar that segment is to the whole. This measure is maximized to find the segment that best represents the entire work. We discuss variations on the method, and present experimental results for orchestral music, popular songs, and jazz. These results demonstrate that the method finds significantly representative excerpts, using very few assumptions about the source audio."
Roger B. Dannenberg;Ning Hu,Pattern Discovery Techniques for Music Audio.,2002,https://doi.org/10.5281/zenodo.1417177,"Roger B. Dannenberg and Ning Hu, School of Computer Science, Carnegie Mellon University","Human listeners are able to recognize structure in music through the perception of repetition and other relationships within a piece of music. This work aims to automate the task of music analysis. Music is “explained” in terms of embedded relationships, especially repetition of segments or phrases. The steps in this process are the transcription of audio into a representation with a similarity or distance metric, the search for similar segments, forming clusters of similar segments, and explaining music in terms of these clusters. Several transcription methods are considered: monophonic pitch estimation, chroma (spectral) representation, and polyphonic transcription followed by harmonic analysis. Also, several algorithms that search for similar segments are described. These techniques can be used to perform an analysis of musical structure, as illustrated by examples."
Dave Datta,Managing Metadata.,2002,https://doi.org/10.5281/zenodo.1415230,"David Datta, All Media Guide","ABSTRACT The All Media Guide (AMG) is a technology company that maintains the world’s largest database of metadata relating to the entertainment industries. This document describes some of the goals of AMG, the issues uncovered during the evolution of our databases, and discusses some of the implementations we have chosen."
Shyamala Doraisamy;Stefan M. Rüger,A Comparative and Fault-tolerance Study of the Use of N-grams with Polyphonic Music.,2002,https://doi.org/10.5281/zenodo.1416022,"Shyamala Doraisamy, Dept. of Computing, Imperial College;Stefan Rüger, Dept. of Computing, Imperial College","In this paper we investigate the retrieval performance of monophonic queries made on a polyphonic music database using the n-gram approach for full-music indexing. The pitch and rhythm dimensions of music are used, and the musical words (a term coined by Downie [2]) generated enable text retrieval methods to be used with music retrieval. We outline an experimental framework for a comparative and fault-tolerance study of various n-gramming strategies and encoding precision using six experimental databases. For monophonic queries we focus in particular on query-by-humming (QBH) systems. Error models addressed in several QBH studies are surveyed for the fault-tolerance study. Our experiments show that different n-gramming strategies and encoding precision differ widely in their effectiveness. We present the results of our comparative and fault-tolerance study on a collection of 5380 polyphonic music pieces encoded in the MIDI format."
J. Stephen Downie;Sally Jo Cunningham,Toward a Theory of Music Information Retrieval Queries: System Design Implications.,2002,https://doi.org/10.5281/zenodo.1417565,"J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;Sally Jo Cunningham, Department of Computer Science, University of Waikato","This paper analyzes a set of 161 music-related information requests posted to the rec.music.country.old-time newsgroup. The results of this analysis suggest that similar studies of 'native' music information requests can be used to inform the design of effective, usable music information retrieval interfaces."
Daniel P. W. Ellis;Brian Whitman;Adam Berenzweig;Steve Lawrence,The Quest for Ground Truth in Musical Artist Similarity.,2002,https://doi.org/10.5281/zenodo.1415602,"Daniel P.W. Ellis, Columbia University;Brian Whitman, MIT Media Lab;Adam Berenzweig, Columbia University;Steve Lawrence, NEC Research Institute","It would be interesting and valuable to devise an automatic measure of the similarity between two musicians based only on an analysis of their recordings. To develop such a measure, however, presupposes some ‘ground truth’ training data describing the actual similarity between certain pairs of artists that constitute the desired output of the measure. Since artist similarity is wholly subjective, such data is not easily obtained. In this paper, we describe several attempts to construct a full matrix of similarity measures between a set of some 400 popular artists by regularizing limited subjective judgment data. We also detail our attempts to evaluate these measures by comparison with direct subjective similarity judgments collected via a web-based survey in April 2002. Overall, we find that subjective artist similarities are quite variable between users—casting doubt on the concept of a single ‘ground truth’. Our best measure, however, gives reasonable agreement with the subjective data, and forms a useable stand-in. In addition, our evaluation methodology may be useful for comparing other measures of artist similarity."
Yazhong Feng;Yueting Zhuang;Yunhe Pan,Popular Music Retrieval by Independent Component Analysis.,2002,https://doi.org/10.5281/zenodo.1416098,"Yazhong Feng, Department of Computer Science, Zhejiang University;Yueting Zhuang, Department of Computer Science, Zhejiang University;Yunhe Pan, Department of Computer Science, Zhejiang University",""""""
Jonathan Foote;Matthew L. Cooper;Unjung Nam,Audio Retrieval by Rhythmic Similarity.,2002,https://doi.org/10.5281/zenodo.1417603,"Jonathan Foote, FX Palo Alto Laboratory, Inc.;Matthew Cooper, FX Palo Alto Laboratory, Inc.;Unjung Nam, Department of Music, Stanford University","We present a method for characterizing both the rhythm and tempo of music. We also present ways to quantitatively measure the rhythmic similarity between two or more works of music. This allows rhythmically similar works to be retrieved from a large collection. A related application is to sequence music by rhythmic similarity, thus providing an automatic “disc jockey” function for musical libraries. Besides specific analysis and retrieval methods, we present small-scale experiments that demonstrate ranking and retrieving musical audio by rhythmic similarity."
Ichiro Fujinaga;Jenn Riley,Digital Image Capture of Musical Scores.,2002,https://doi.org/10.5281/zenodo.1416554,"Ichiro Fujinaga, Peabody Conservatory of Music, Johns Hopkins University;Jenn Riley, Digital Library Program, Indiana University","Musical scores have small details and complex markings, and are difficult to digitally capture and deliver well. All capture decisions should be made with a clear idea of the purpose of the resulting digital images, but master images must be flexible enough to fulfill unanticipated future uses. In order to provide a framework for decision-making in musical score digitization projects, best practices for detail and color capture are presented. Recommendations for file formats for archival storage, web delivery and printing of musical materials are presented."
Joe Futrelle;J. Stephen Downie,Interdisciplinary Communities and Research Issues in Music Information Retrieval.,2002,https://doi.org/10.5281/zenodo.1416406,"Joe Futrelle, Graduate School of Library and Information Science, University of Illinois;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois","Music Information Retrieval (MIR) is an interdisciplinary research area that has grown out of the need to manage burgeoning collections of music in digital form. Its diverse disciplinary communities have yet to articulate a common research agenda or agree on methodological principles and metrics of success. In order for MIR to succeed, researchers need to work with real user communities and develop research resources such as reference music collections, so that the wide variety of techniques being developed in MIR can be meaningfully compared with one another. Out of these efforts, a common MIR practice can emerge."
Gordon Geekie,Carnatic Ragas as Music Information Retrieval Entities.,2002,https://doi.org/10.5281/zenodo.1415994,"Gordon Geekie, Department of Information and Communication, Manchester Metropolitan University","Carnatic music is the ‘art’ music of the four southern States of India (Andhra Pradesh, Karnataka, Kerala and Tamilnadu). One difference between Carnatic music and the better-known Hindusthani music of North India is its embeddedness in a religious-philosophical context. This context crucially determines the objects of knowledge organization and the indigenous theory of musical affect. The author presents the view that a digital library of Carnatic music should contain the objects of knowledge organization and their interrelationships as conceived by indigenous practitioners and audiences, rather than by Western specialists or North Indian practitioners. The author demonstrates how three features of Carnatic music (viz. aural transmission, improvisation and cultural context) have particular implications for the development of a digital library. Aural transmission results in musical documents being less important sources of information than recordings. Improvisation results in a highly transformational and often ambiguous relationship between (intra)musical signifiers and signified, causing problems of classification and machine recognition. The cultural context favours the prioritisation of emotional affect over introductory ease of listening and even technical recording quality in the selection of the recordings to be included in a digital library of Carnatic music."
Masataka Goto;Hiroki Hashiguchi;Takuichi Nishimura;Ryuichi Oka,"RWC Music Database: Popular, Classical and Jazz Music Databases.",2002,https://doi.org/10.5281/zenodo.1416474,"Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST) & “Information and Human Activity”, PRESTO, JST;Hiroki Hashiguchi, Mejiro University;Takuichi Nishimura, National Institute of Advanced Industrial Science and Technology (AIST);Ryuichi Oka, University of Aizu","This paper describes the design policy and specifications of the RWC Music Database, a music database (DB) that is available to researchers for common use and research purposes. The RWC Music Database contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). These DBs consist of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. The goal of the RWC Music Database is to encourage further advances in musical information processing research."
Jaap Haitsma;Ton Kalker,A Highly Robust Audio Fingerprinting System.,2002,https://doi.org/10.5281/zenodo.1417973,"Jaap Haitsma, Philips Research;Ton Kalker, Philips Research","Imagine the following situation. You’re in your car, listening to the radio and suddenly you hear a song that catches your attention. It’s the best new song you have heard for a long time, but you missed the announcement and don’t recognize the artist. Still, you would like to know more about this music. What should you do? You could call the radio station, but that’s too cumbersome. Wouldn’t it be nice if you could push a few buttons on your mobile phone and a few seconds later the phone would respond with the name of the artist and the title of the music you’re listening to? Perhaps even sending an email to your default email address with some supplemental information. In this paper we present an audio fingerprinting system, which makes the above scenario possible. By using the fingerprint of an unknown audio clip as a query on a fingerprint database, which contains the fingerprints of a large library of songs, the audio clip can be identified. At the core of the presented system are a highly robust fingerprint extraction method and a very efficient fingerprint search strategy, which enables searching a large fingerprint database with only limited computing resources."
Toni Heittola;Anssi Klapuri,Locating Segments with Drums in Music Signals.,2002,https://doi.org/10.5281/zenodo.1418137,"Toni Heittola, Tampere University of Technology;Anssi Klapuri, Tampere University of Technology","A system is described which segments musical signals according to the presence or absence of drum instruments. Two different yet approximately equally accurate approaches were taken to solve the problem. The first is based on periodicity detection in the amplitude envelopes of the signal at subbands. The band-wise periodicity estimates are aggregated into a summary autocorrelation function, the characteristics of which reveal the drums. The other mechanism applies straightforward acoustic pattern recognition with mel-frequency cepstrum coefficients as features and a Gaussian mixture model classifier. The integrated system achieves 88 % correct segmentation over a database of 28 hours of music from different musical genres. For the both methods, errors occur for borderline cases with soft percussive-like drum accompaniment, or transient-like instrumentation without drums."
Harriette Hemmasi,Why not MARC?,2002,https://doi.org/10.5281/zenodo.1417491,"Harriette Hemmasi, Indiana University","Traditional library cataloging records in the United States, based on AACR2R cataloging rules and MARC standards, constitute a solid foundation for many of the descriptive metadata elements needed for searching and retrieving works of music. However, there are significant weaknesses associated with these records and the online environment in which they live as users seek access to digitized representations of music. While music metadata in the library catalog records offer less than a perfect solution, they can and should have an important role in the total solution. Variations2, the Indiana University Digital Music Library, builds on the advantages of AACR2R and MARC and offers a domain-specific data model and search environment that address many of the identified problems."
Keiji Hirata;Shu Matsuda,Interactive Music Summarization based on GTTM.,2002,https://doi.org/10.5281/zenodo.1417481,"Keiji Hirata, NTT Communication Science Laboratories;Shu Matsuda, Digital Art Creation","This paper presents a music summarization system called ""Papipuun"" that performs quick listening by skipping musical phrases. The system uses a method for representing polyphony based on time-span reduction in the generative theory of tonal music (GTTM) and the deductive object-oriented database (DOOD). The system allows users to interactively analyze and summarize music, producing high-quality summarizations that reflect the atmosphere of the entire piece."
Hui Jin;H. V. Jagadish,Indexing Hidden Markov Models for Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1418259,"Hui Jin, The University of Michigan;H. V. Jagadish, The University of Michigan",ABSTRACT
Jürgen Kilian;Holger H. Hoos,Voice Separation - A Local Optimization Approach.,2002,https://doi.org/10.5281/zenodo.1417645,"J¨urgen Kilian, Darmstadt University of Technology;Holger H. Hoos, University of British Columbia","Voice separation, along with tempo detection and quantisation, is one of the basic problems of computer-based transcription of music. An adequate separation of notes into different voices is crucial for obtaining readable and usable scores from performances of polyphonic music recorded on keyboard (or other polyphonic) instruments; for improving quantisation results within a transcription system; and in the context of music retrieval systems that primarily support monophonic queries. In this paper we propose a new voice separation algorithm based on a stochastic local search method. Different from many previous approaches, our algorithm allows chords in the individual voices; its behaviour is controlled by a small number of intuitive and musically motivated parameters; and it is fast enough to allow interactive optimisation of the result by adjusting the parameters in real-time. We demonstrate that compared to existing approaches, our new algorithm generates better solutions for a number of typical voice separation problems. We also show how by changing its parameters it is possible to create score output suitable for different needs, piano-style orchestral scores."
Ja-Young Kim;Nicholas J. Belkin,Categories of Music Description and Search Terms and Phrases Used by Non-Music Experts.,2002,https://doi.org/10.5281/zenodo.1417763,"Ja-Young Kim, School of Communication, Information and Library Studies, Rutgers University;Nicholas J. Belkin, School of Communication, Information and Library Studies, Rutgers University","Previous research has demonstrated that people listen to music for various reasons. The purpose of this study was to investigate people’s perception of music, and thus their music information needs. These ideas were examined by presenting 22 participants with 7 classical musical pieces, asking one-half of them to write words descriptive of each piece, and the other half words they would use if searching for each piece. All the words used by all subjects in both tasks were classified into 7 categories. The two most frequently appearing categories were emotions and occasions or filmed events regardless of the task type. These subjects, none of whom had formal training in music, almost never used words related to formal features of music, rather using words indicating other features, most of which have not been considered in existing or proposed music IR systems. These results suggest that music IR research should be extended to consider needs other than finding known items, or items identified by formal characteristics, and that understanding music information needs of users should be prioritized to design more sophisticated music IR systems."
Youngmoo E. Kim;Brian Whitman,Singer Identification in Popular Music using Warped Linear Prediction.,2002,https://doi.org/10.5281/zenodo.1416954,"Youngmoo E. Kim, MIT Media Lab;Brian Whitman, MIT Media Lab","In most popular music, the vocals sung by the lead singer are the focal point of the song. The unique qualities of a singer’s voice make it relatively easy for us to identify a song as belonging to that particular artist. With little training, if one is familiar with a particular singer’s voice one can usually recognize that voice in other pieces, even when hearing a song for the first time. The research presented in this paper attempts to automatically establish the identity of a singer using acoustic features extracted from songs in a database of popular music. As a first step, an untrained algorithm for automatically extracting vocal segments from within songs is presented. Once these vocal segments are identified, they are presented to a singer identification system that has been trained on data taken from other songs by the same artists in the database."
Olivier Lartillot,Integrating Pattern Matching into an Analogy-Oriented Pattern Discovery Framework.,2002,https://doi.org/10.5281/zenodo.1417048,"Olivier Lartillot, Ircam – Centre Pompidou","We claim that the core mechanism of a sufficiently general MIR system should be expressed in symbolic terms. We defend the idea that music database should be pre-analyzed before being scanned for MIR queries. We suggest a new vision of automated pattern analysis that generalizes the multiple viewpoint approach by adding a new paradigm based on analogy and temporal approach of musical scores. Through a chronological scanning of the score, analogies are inferred between local relationships — namely, notes and intervals — and global structures — namely, patterns — whose paradigms are stored inside an abstract pattern trie (APT). Basic mechanisms for inference of new patterns are described. The same pattern-matching algorithm used for pattern discovery during pre-analysis of musical works is reused during MIR applications. Such an elastic vision of music enables a generalized understanding of its plastic expression. This project, in an early stage, introduces a broader paradigm of automated music analysis."
Jin Ha Lee;J. Stephen Downie;Allen Renear,Representing Traditional Korean Music Notation in XML.,2002,https://doi.org/10.5281/zenodo.1418277,"Jin Ha Lee, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;Allen Renear, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign","XML promises to provide a powerful interoperable general framework for the development of music representation systems. Unfortunately current XML encoding systems for music focus almost exclusively on Western music from the 17th century onwards, and on the Western notation system, Common Music Notation (CMN). This is regrettably limiting, with cultural, theoretical, and practical consequences for MIR. In order to ensure that music information retrieval (MIR) systems have full theoretic generality, and wide practical application, we have begun a project to explore the representation, in XML, of a genre of traditional Korean music which has a distinctive notation system called Chôngganbo. Our project takes seriously the specific notational expression of musical intention and intends to ultimately contribute to the analysis of theoretical issues in music representation, as well as to the improvement of methods for representing Korean music specifically."
Beth Logan,Content-Based Playlist Generation: Exploratory Experiments.,2002,https://doi.org/10.5281/zenodo.1418061,"Beth Logan, Hewlett-Packard Labs","ABSTRACT The popularity of the MP3 compression format has changed the way people store, access and acquire music. It is now possible to carry hundreds of hours of music on a small device. Through the Web potentially millions of hours are ubiquitously available. This change in scale of accessible music from the traditional album to millions of songs raises many unanswered questions of how to efficiently access and discover this data and best present music to the user. Ideally, we imagine a system which can automatically sense a user’s mood or desires and play suitable music from a massive repository of available songs. The system would also respond to and learn from user feedback and be able to suggest suitable new songs from other repositories."
Thomas Mandl 0001;Christa Womser-Hacker,Learning to cope with Diversity in Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1416560,"Thomas Mandl, Information Science, University of Hildesheim;Christa Womser-Hacker, Information Science, University of Hildesheim","Designers of music retrieval systems are faced with a multitude of choices and criteria. The complexity of music as a formal system and cultural phenomenon poses challenges for computational representation. Music lacks separators like words or phrases, making content-based indexing difficult. Music retrieval often relies on similarity, but determining the theme of a song can be subjective. Fusion methods, such as the MIMOR model, aim to improve retrieval performance by integrating multiple algorithms and considering user preferences."
Matija Marolt;Sasa Divjak,On detecting repeated notes in piano music.,2002,https://doi.org/10.5281/zenodo.1416078,"Matija Marolt, University of Ljubljana Faculty of Computer and Information Science;Sasa Divjak, University of Ljubljana Faculty of Computer and Information Science","One of the problems encountered in music transcription is to produce an algorithm that detects whether a note should be repeated, when a new onset is found during its duration, or not; with other words whether two or more shorter notes should be produced instead of a single longer note. The paper describes our approach to solving this problem, implemented within our system for transcription of piano music [4]. The approach is based on a multilayer perceptron neural network, trained to recognize repeated notes. We compare this method to a more naive method that tracks the amplitude of the first partial of each note and also present performance statistics of our system on transcriptions of several real piano recordings."
John R. McPherson,Introducing Feedback into an Optical Music Recogniition System.,2002,https://doi.org/10.5281/zenodo.1417725,"John R. McPherson, Department of Computer Science, University of Waikato","Optical Music Recognition is the process of converting a graphical representation of music (such as sheet music) into a symbolic format (for example, a format that is understood by music software). Music notation is rich in structural information, and the relative positions of objects can often help to identify them. When objects are unidentified or mis-identified, many current systems ""coerce"" the set of objects into some semantic representation, for example by modifying the detected durations. This could cause correctly identified symbols to be modified. The knowledge that the current set of identified symbols cannot be semantically parsed could instead be used to re-examine some of the symbols before deciding whether or not the classification is correct. This paper describes work in progress involving the use of feedback between the various phases of the optical music recognition process to automatically correct mistakes, such as symbolic classification errors or mis-detected staff systems."
Colin Meek;William P. Birmingham,Johnny Can't Sing: A Comprehensive Error Model for Sung Music Queries.,2002,https://doi.org/10.5281/zenodo.1418065,"Colin Meek and William Birmingham, University of Michigan","We propose a model for errors in sung queries, a variant of the Hidden Markov Model (HMM). This is related to the problem of identifying the degree of similarity between a query and a potential target in a database of musical works, in the music retrieval framework. The model comprehensively expresses the types of error or variation between target and query: cumulative and non-cumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. Results of experiments demonstrating the robustness of the model are presented."
Massimo Melucci;Nicola Orio,A Comparison of Manual and Automatic Melody Segmentation.,2002,https://doi.org/10.5281/zenodo.1416914,"Massimo Melucci, University of Padua;Nicola Orio, University of Padua",A Comparison of Manual and Automatic Melody Segmentation
Thomas Noll;Jörg Garbers;Karin Höthker;Christian Spevak;Tillman Weyde,Opuscope - Towards a Corpus-Based Music Repository.,2002,https://doi.org/10.5281/zenodo.1417411,"Thomas Noll, J¨org Garbers, Technische Universit¨at Berlin;Karin H¨othker, Christian Spevak, Universit¨at Karlsruhe;Tillman Weyde, Universit¨at Osnabr¨uck","Opuscope is an initiative aimed at sharing musical corpora and their analyses between researchers. The Opuscope repository will contain high-quality musical corpora that can be annotated with hand-made or algorithmic musical analyses. This allows for the use of analytical results obtained by others as a starting point for one's own investigations, and for easy comparison of experiments performed on Opuscope corpora with other approaches. Opuscope aims to provide a standardized benchmark collection for the evaluation of retrieval algorithms and to facilitate the exchange of complex music-analytical structures."
Bryan Pardo;William P. Birmingham,Encoding Timing Information for Musical Query Matching.,2002,https://doi.org/10.5281/zenodo.1415776,"Bryan Pardo, University of Michigan;William Birmingham, University of Michigan",We compare representing note timing as Inter Onset Intervals (IOIs) and as the ratio of adjacent IOI values. A variety of log2 and linear quantizations of IOI and IOI ratios are considered for each representation. The utility of encoding with a particular quantization is measured by the ability of a simple string-matcher to differentiate between themes in a melodic corpus. Results indicate that time is best represented by IOI ratios quantized to a logarithmic scale.
Jouni Paulus;Anssi Klapuri,Measuring the similarity of Rhythmic Patterns.,2002,https://doi.org/10.5281/zenodo.1414712,"Jouni Paulus, Tampere University of Technology Institute of Signal Processing;Anssi Klapuri, Tampere University of Technology Institute of Signal Processing","A system is described which measures the similarity of two arbitrary rhythmic patterns. The patterns are represented as acoustic signals, and are not assumed to have been performed with similar sound sets. Two novel methods are presented that constitute the algorithmic core of the system. First, a probabilistic musical meter estimation process is described, which segments a continuous musical signal into patterns. As a side-product, the method outputs tatum, tactus (beat), and measure lengths. A subsequent process performs the actual similarity measurements. Acoustic features are extracted which model the fluctuation of loudness and brightness within the pattern, and dynamic time warping is then applied to align the patterns to be compared. In simulations, the system behaved consistently by assigning high similarity measures to similar musical rhythms, even when performed using different sound sets."
Steffen Pauws,"CubyHum: a fully operational ""query by humming"" system.",2002,https://doi.org/10.5281/zenodo.1415614,"Steffen Pauws, Philips Research Eindhoven","'Query by humming' is an interaction concept in which the identity of a song has to be revealed fast and orderly from a given sung input using a large database of known melodies. In short, it tries to detect the pitches in a sung melody and compares these pitches with symbolic representations of the known melodies. Melodies that are similar to the sung pitches are retrieved. Approximate pattern matching in the melody comparison process compensates for the errors in the sung melody by using classical dynamic programming. A filtering method is used to save computation in the dynamic programming framework. This paper presents the algorithms for pitch detection, note onset detection, quantization, melody encoding and approximate pattern matching as they have been implemented in the CubyHum software system. Since human reproduction of melodies is imperfect, findings from an experimental singing study were a crucial input to the development of the algorithms. Future research should pay special attention to the reliable detection of note onsets in any preferred singing style. In addition, research on index methods and fast bit-parallelism algorithms for approximate pattern matching need to be further pursued to decrease computational requirements when dealing with large melody databases."
Steffen Pauws;Berry Eggen,PATS: Realization and user evaluation of an automatic playlist generator.,2002,https://doi.org/10.5281/zenodo.1417971,"Steffen Pauws, Philips Research Eindhoven;Berry Eggen, Philips Research Eindhoven, and Technische Universiteit Eindhoven / Faculty of Industrial Design","A means to ease selecting preferred music referred to as Personalized Automatic Track Selection (PATS) has been developed. PATS generates playlists that suit a particular context-of-use, that is, the real-world environment in which the music is heard. To create playlists, it uses a dynamic clustering method in which songs are grouped based on their attribute similarity. The similarity measure selectively weighs attribute-values, as not all attribute-values are equally important in a context-of-use. An inductive learning algorithm is used to reveal the most important attribute-values for a context-of-use from preference feedback of the user. In a controlled user experiment, the quality of PATS-compiled and randomly assembled playlists for jazz music was assessed in two contexts-of-use. The quality of the randomly assembled playlists was used as base-line. The two contexts-of-use were ‘listening to soft music’ and ‘listening to lively music’. Playlist quality was measured by precision (songs that suit the context-of-use), coverage (songs that suit the context-of-use but that were not already contained in previous playlists) and a rating score. Results showed that PATS playlists contained increasingly more preferred music (increasingly higher precision), covered more preferred music in the collection (higher coverage), and were rated higher than randomly assembled playlists."
Geoffroy Peeters;Amaury La Burthe;Xavier Rodet,Toward Automatic Music Audio Summary Generation from Signal Analysis.,2002,https://doi.org/10.5281/zenodo.1417885,"Geoffroy Peeters, IRCAM;Amaury La Burthe, IRCAM;Xavier Rodet, IRCAM","This paper deals with the automatic generation of music audio summaries from signal analysis without the use of any other information. The strategy employed here is to consider the audio signal as a succession of “states” (at various scales) corresponding to the structure (at various scales) of a piece of music. This is, of course, only applicable to certain kinds of musical genres based on some kind of repetition. From the audio signal, we first derive dynamic features representing the time evolution of the energy content in various frequency bands. These features constitute our observations from which we derive a representation of the music in terms of “states”. Since human segmentation and grouping performs better upon subsequent hearings, this “natural” approach is followed here. The first pass of the proposed algorithm uses segmentation in order to create “templates”. The second pass uses these templates in order to propose a structure of the music using unsupervised learning methods (K-means and hidden Markov model). The audio summary is finally constructed by choosing a representative example of each state. Further refinements of the summary audio signal construction, uses overlap-add, and a tempo detection/beat alignment in order to improve the audio quality of the created summary."
Jeremy Pickens;Juan Pablo Bello;Tim Crawford;Matthew J. Dovey;Giuliano Monti;Mark B. Sandler,Polyphonic Score Retrieval Using Polyphonic Audio Queries: A Harmonic Modeling Approach.,2002,https://doi.org/10.5281/zenodo.1418091,"Jeremy Pickens, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst;Juan Pablo Bello, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst;Giuliano Monti, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst;Tim Crawford, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst;Matthew Dovey, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst;Mark Sandler, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst;Don Byrd, Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst","This paper extends the familiar “query by humming” music retrieval framework into the polyphonic realm. As humming in multiple voices is quite difficult, the task is more accurately described as “query by audio example”, onto a collection of scores. To our knowledge, we are the first to use polyphonic audio queries to retrieve from polyphonic symbolic collections. Furthermore, as our results will show, we will not only use an audio query to retrieve a known-item symbolic piece, but we will use it to retrieve an entire set of real-world composed variations on that piece, also in the symbolic format. The harmonic modeling approach which forms the basis of this work is a new and valuable technique which has both wide applicability and future potential."
Anna Pienimäki,Indexing Music Databases Using Automatic Extraction of Frequent Phrases.,2002,https://doi.org/10.5281/zenodo.1416632,"Anna Pienim¨aki, Department of Computer Science, University of Helsinki","The Music Information Retrieval methods can be classiﬁed into online and ofﬂine methods. The main drawback in most of the ofﬂine algorithms is the space the indexing structure requires. The amount of data stored into the structure can however be reduced by storing only the suitable index terms or phrases instead of the whole contents of the database. Repetition is agreed to be one of the most important factors of musical meaningfulness. Therefore repetitive musical phrases are suitable for indexing purposes. The extraction of such phrases can be done by applying an existing text mining method to musical data. Because of the differences between text and musical data the application requires some technical modiﬁcation of the method. This paper introduces a text mining-based music database indexing method that extracts maximal frequent phrases from musical data and sorts them by their length, frequency and personality. The implementation of the method found three different types of phrases from the test corpus consisting of Irish folk music tunes. The suitable two types of phrases out of three are easily recognized and separated from the set of all phrases to form an index data for the database."
Emanuele Pollastri,Some Considerations About Processing Singing Voice for Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1416494,"Emanuele Pollastri, Dipartimento di Scienze dell’Informazione Università degli Studi di Milano","The audio processing and post-processing of singing hold a fundamental role in the context of query-by-humming applications. Through the analysis of a sung query, we should perform some kind of meta-information extraction and this topic deserves the interest of the present paper. Considering the raw output of a pitch tracking algorithm, the issues of note estimation and the study of singing accuracy have been addressed. Further, we report an experiment on the deviations from pure tone intonation in performances of untrained singers."
Christopher Raphael,Automatic Transcription of Piano Music.,2002,https://doi.org/10.5281/zenodo.1414952,"Christopher Raphael, Univ. of Massachusetts, Amherst","A hidden Markov model approach to piano music transcription is presented. The main difficulty in applying traditional HMM techniques is the large number of chord hypotheses that must be considered. We address this problem by using a trained likelihood model to generate reasonable hypotheses for each frame and construct the search graph out of these hypotheses. Results are presented using a recording of a movement from Mozart’s Sonata 18, K. 570."
Andreas Rauber;Elias Pampalk;Dieter Merkl,Using Psycho-Acoustic Models and Self-Organizing Maps to Create a Hierarchical Structuring of Music by Musical Styles.,2002,https://doi.org/10.5281/zenodo.1417143,"Andreas Rauber, Dept. of Software Technology, Vienna Univ. of Technology;Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence;Dieter Merkl, Dept. of Software Technology, Vienna Univ. of Technology","With the advent of large musical archives, there is a need to provide an organization of these archives that allows users to browse and explore its contents. In this paper, the authors propose an approach to automatically create a hierarchical organization of music archives based on their perceived sound similarity. They extract characteristics of frequency spectra and transform them according to psycho-acoustic models. They then use a Growing Hierarchical Self-Organizing Map to create the hierarchical organization, which offers an interface for interactive exploration and retrieval of music according to perceived sound similarity."
Eric Schreier,About this Business of Metadata.,2002,https://doi.org/10.5281/zenodo.1414742,"Eric D. Scheirer, Bose Corporation","A brief discussion presents some of the opportunities and challenges involved with creating metadata-centric businesses that bring Music Information Retrieval technologies to the marketplace. In particular, two related difficulties -- that of the difficulty of proving incremental value for new metadata systems, and that of the relative influidity of the marketplace for MIR -- are highlighted. Potential directions for resolving these issues are also discussed."
Jungmin Song;So-Young Bae;Kyoungro Yoon,Mid-Level Music Melody Representation of Polyphonic Audio for Query-by-Humming System.,2002,https://doi.org/10.5281/zenodo.1418309,"Jungmin Song, LG Electronics;So Young Bae, LG Electronics;Kyoungro Yoon, LG Electronics","Recently a great attention is paid to content-based multimedia retrieval that enables users to find and locate audio-visual materials according to the intrinsic characteristics of the target. Query-by-humming (QBH) is also an application that makes retrieval based on major characteristics of music, that is, ""melody"". There have been some researches on QBH system, most of which are to retrieve music from symbolic music data by humming query. However, when the usability of technology is taken into consideration, retrieval of music in the form of polyphonic raw audio would be more useful and needed in the applications such as internet music search or music juke box, where the music data is stored not in symbolic form but in raw digital audio signal because such music data is more natural format for consumption. Our focus is on the realization of query-by-humming technology for an easy-to-use application, which entails full automation of all the processes of the system, including melody information extraction from polyphonic raw audio. In our system, melody feature of music database and humming is not represented by distinct note information but by the probability of note occurrence. Similarity is then measured between the melody features of humming and music data using DP matching method. This paper presents developed algorithms and experimental results for key steps of QBH system including the melody feature extraction method from polyphonic audio and humming, their representation for matching, and matching method between represented melody information from polyphonic audio and humming."
Timo Sorsa;Katriina Halonen,Mobile Melody Recognition System with Voice-Only User Interface.,2002,https://doi.org/10.5281/zenodo.1415866,"Timo Sorsa, Nokia Research Center;Katriina Halonen, Nokia Research Center","A melody recognition system with a voice-only user interface is presented in this paper. By integrating speech recognition and melody recognition technology, the authors have built an end-to-end melody retrieval system that allows users to do voice-controlled melodic queries and melody generation using a dial-in service with a mobile phone."
George Tzanetakis;Andrey Ermolinskiy;Perry R. Cook,Pitch Histograms in Audio and Symbolic Music Information Retrieval.,2002,https://doi.org/10.5281/zenodo.1416146,"George Tzanetakis, Computer Science Department;Andrey Ermolinskyi, Computer Science Department;Perry Cook, Computer Science Department","In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques. In this paper, Pitch Histograms are defined and proposed as a way to represent the pitch content of music signals both in symbolic and audio form. This representation is evaluated in the context of automatic musical genre classification. A multiple-pitch detection algorithm for polyphonic signals is used to calculate Pitch Histograms for audio signals. In order to evaluate the extent and significance of errors resulting from the automatic multiple-pitch detection, automatic musical genre classification results from symbolic and audio data are compared. The comparison indicates that Pitch Histograms provide valuable information for musical genre classification. The results obtained for both symbolic and audio cases indicate that although pitch errors degrade classification performance for the audio case, Pitch Histograms can be effectively used for classification in both cases."
Alexandra L. Uitdenbogerd;Ron G. van Schyndel,A Review of Factors Affecting Music Recommender Success.,2002,https://doi.org/10.5281/zenodo.1417783,"Alexandra Uitdenbogerd and Ron van Schyndel, Department of Computer Science, RMIT University",A Review of Factors Affecting Music Recommender Success
Hugues Vinet;Perfecto Herrera;François Pachet,The CUIDADO Project.,2002,https://doi.org/10.5281/zenodo.1416940,"Hugues Vinet, IRCAM;Perfecto Herrera, IUA-Universitat Pompeu Fabra;François Pachet, SONY-CSL","The CUIDADO Project (Content-based Unified Interfaces and Descriptors for Audio/music Databases available Online) aims at developing a new chain of applications through the use of audio/music content descriptors, in the spirit of the MPEG-7 standard. The project includes the design of appropriate description structures, the development of extractors for deriving high-level information from audio signals, and the design and implementation of two applications: the Sound Palette and the Music Browser. These applications include new features, which systematically exploit high-level descriptors and provide users with content-based access to large catalogues of audio/music material. The Sound Palette focuses on audio samples and targets professional users, whereas the Music Browser addresses a broader user target through the management of Popular music titles. After a presentation of the project objectives and methodology, we describe the original features of the two applications based on the systematic use of descriptors and the technical architecture framework on which they rely."
Chaokun Wang;Jianzhong Li;Shengfei Shi,A Kind of Content-Based Music Information Retrieval Method in Peer-to-peer Environment.,2002,https://doi.org/10.5281/zenodo.1417441,"Chaokun Wang, Department of Computer Science and Engineering, Harbin Institute of Technology;Jianzhong Li, Department of Computer Science and Engineering, Harbin Institute of Technology;Shengfei Shi, Department of Computer Science and Engineering, Harbin Institute of Technology","In this paper, we propose four peer-to-peer models for content-based music information retrieval (CBMIR) and carefully evaluate them on network load, retrieval time, system update, and robustness qualitatively and quantitatively. We also present the architecture of QUIND, a content-based peer-to-peer music information retrieval system, which combines content-based music retrieval technologies and peer-to-peer environments. QUIND has strong robustness and good expansibility, allowing users to retrieve similar music quickly and accurately based on the content of music."
Brian Whitman;Paris Smaragdis,Combining Musical and Cultural Features for Intelligent Style Detection.,2002,https://doi.org/10.5281/zenodo.1417471,"Brian Whitman, MIT Media Lab;Paris Smaragdis, MIT Media Lab","In this paper, the authors present an automatic style detection system that combines acoustic content and community metadata to identify different music styles. The system uses descriptive textual features extracted from automated web crawls to augment the community metadata. The combined model performs well in identifying previously edited style clusters and can be used to cluster new sets of artists."
Geraint A. Wiggins;Kjell Lemström;David Meredith 0001,"SIA(M)ESE: An Algorithm for Transposition Invariant, Polyphonic Content-Based Music Retrieval.",2002,https://doi.org/10.5281/zenodo.1415960,"Geraint A. Wiggins, Department of Computing, City University, London;Kjell Lemström, Department of Computer Science, University of Helsinki;David Meredith, Department of Computing, City University, London","We introduce a novel algorithm for transposition-invariant content-based polyphonic music retrieval. Our SIA(M)ESE algorithm is capable of finding transposition invariant occurrences of a given template, in a database of polyphonic music called a dataset. We allow arbitrary gapping, i.e., between musical events in the dataset that have been found to match points in the template, there may be any finite number of other intervening events. SIA(M)ESE can be implemented so that it finds all transposition-invariant complete matches for a d-dimensional template of size m in a n-dimensional dataset of size p in a worst-case running time of O(mnp); another implementation finds even the incomplete matches in O(mnp) time. The algorithm is generalizable to any arbitrary, multidimensional translation invariant pattern matching problem, where the events are representable by points in a multidimensional dataset."
Cheng Yang,MACSIS: A Scalable Acoustic Index for Content-Based Music Retrieval.,2002,https://doi.org/10.5281/zenodo.1416662,"Cheng Yang, Stanford University",The MACSIS Acoustic Indexing Framework for Music Retrieval: An Experimental Study
Jin Ha Lee;J. Stephen Downie;Sally Jo Cunningham,Challenges in Cross-Cultural/Multilingual Music Information Seeking.,2005,https://doi.org/10.5281/zenodo.1416706,"Jin Ha Lee, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;Sally Jo Cunningham, Dept. of Computer Science, University of Waikato, New Zealand",Understanding and meeting the needs of a broad range of music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multilingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from a Korean knowledge search portal Naver 지식 (knowledge) iN and 150 queries from Google Answers website. We conclude that new sets of access points must be developed to accommodate music queries that cross cultural or language boundaries.
Cynthia M. Grund,"Music Information Retrieval, Memory and Culture: Some Philosohpical Remarks.",2005,https://doi.org/10.5281/zenodo.1415626,"Cynthia M. Grund, Institute of Philosophy, Education and the Study of Religions, University of Southern Denmark","The purpose of this paper is twofold: the first goal is to highlight and briefly discuss a selection of these issues, while the second is to make a case for increased mutual awareness of each other on the parts of MIR and of humanistic research. Many traditional debates within the latter receive infusions of new perspectives from MIR, while research within MIR could be fruitfully pointed in directions suggested by questions of interest within traditional research in the humanities, e.g. the relationship of individual memory to cultural memory, issues regarding cross-cultural understanding and the importance of authenticity in artistic contexts."
Noris Mohd. Norowi;Shyamala Doraisamy;Rahmita Wirza O. K. Rahmat,Factors Affecting Automatic Genre Classification: An Investigation Incorporating Non-Western Musical Forms.,2005,https://doi.org/10.5281/zenodo.1418067,"Noris Mohd Norowi, Faculty of Computer Science and Information Technology, University Putra Malaysia;Shyamala Doraisamy, Faculty of Computer Science and Information Technology, University Putra Malaysia;Rahmita Wirza, Faculty of Computer Science and Information Technology, University Putra Malaysia","The number of studies investigating automated genre classification is growing following the increasing amounts of digital audio data available. The underlying techniques to perform automated genre classification in general include feature extraction and classification. This study investigates the factors affecting automated genre classification. As for the dataset, most studies in this area work with western genres and traditional Malay music is incorporated in this study. Eight genres were introduced; Dikir Barat, Etnik Sabah, Inang, Joget, Keroncong, Tumbuk Kalang, Wayang Kulit, and Zapin. A total of 417 tracks from various Audio Compact Discs were collected and used as the dataset. Results show that various factors such as the musical features extracted, classifiers employed, the size of the dataset, excerpt length, excerpt location and test set parameters improve classification results."
Markus Schedl;Peter Knees;Gerhard Widmer,Discovering and Visualizing Prototypical Artists by Web-Based Co-Occurrence Analysis.,2005,https://doi.org/10.5281/zenodo.1418315,"Markus Schedl, Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria;Peter Knees, Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria;Gerhard Widmer, Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria","Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository."
Ian Knopke,Geospatial Location of Music and Sound Files for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1417765,"Ian Knopke, McGill Music Technology","A relatively new avenue of Web-based information retrieval research is the use of geographical information to locate resources. This paper introduces a technique for locating sound and music files geographically by combining information extracted from the Web with geospatial location data. The results demonstrate the potential for Music Information Retrieval (MIR) to utilize the vast amount of audio materials on the Web within a physical and geographical context. The paper also discusses potential applications of these techniques, such as geospatial music web browsing, music marketing, and bandwidth optimization."
Thomas Lidy;Andreas Rauber,Evaluation of Feature Extractors and Psycho-Acoustic Transformations for Music Genre Classification.,2005,https://doi.org/10.5281/zenodo.1416856,"Thomas Lidy, Vienna University of Technology;Andreas Rauber, Vienna University of Technology","We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy."
Cory McKay;Rebecca Fiebrink;Daniel McEnnis;Beinan Li;Ichiro Fujinaga,ACE: A Framework for Optimizing Music Classification.,2005,https://doi.org/10.5281/zenodo.1415720,"Cory McKay, Music Technology, McGill University;Rebecca Fiebrink, Music Technology, McGill University;Daniel McEnnis, Music Technology, McGill University;Beinan Li, Music Technology, McGill University;Ichiro Fujinaga, Music Technology, McGill University","This paper presents ACE (Autonomous Classification Engine), a framework for using and optimizing classifiers. Given a set of feature vectors, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality reduction techniques in order to arrive at a good configuration for the problem at hand. In addition to evaluating classification methodologies in terms of success rates, functionality is also being incorporated into ACE allowing users to specify constraints on training and classification times as well as on the amount of time that ACE has to arrive at a solution. ACE is designed to facilitate classification for those new to pattern recognition as well as provide flexibility for those with more experience. ACE is packaged with audio and MIDI feature extraction software, although it can certainly be used with existing feature extractors. This paper includes a discussion of ways in which existing general-purpose classification software can be adapted to meet the needs of music researchers and shows how these ideas have been implemented in ACE. A standardized XML format for communicating features and other information to classifiers is proposed. A special emphasis is placed on the potential of classifier ensembles, which have remained largely untapped by the MIR community to date. A brief theoretical discussion of ensemble classification is presented in order to promote this powerful approach."
Koen Tanghe;Micheline Lesaffre;Sven Degroeve;Marc Leman;Bernard De Baets;Jean-Pierre Martens,Collecting Ground Truth Annotations for Drum Detection in Polyphonic Music.,2005,https://doi.org/10.5281/zenodo.1417715,"Koen Tanghe, IPEM, Department of Musicology, Ghent University;Micheline Lesaffre, IPEM, Department of Musicology, Ghent University;Sven Degroeve, Department of Applied Mathematics, Biometrics and Process Control, Ghent University;Marc Leman, IPEM, Department of Musicology, Ghent University;Bernard De Baets, Department of Applied Mathematics, Biometrics and Process Control, Ghent University;Jean-Pierre Martens, Department of Electronics and Information Systems, Ghent University","In order to train and test algorithms for drum detection in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for real-world music fragments containing different drum event types. The annotators were experienced drummers or percussionists. The purpose of this paper is to provide annotation data for algorithm training and evaluation, describe a practical way of setting up a drum annotation task, and report issues that came up during the annotation sessions."
Gavin Wood;Simon O'Keefe,On Techniques for Content-Based Visual Annotation to Aid Intra-Track Music Navigation.,2005,https://doi.org/10.5281/zenodo.1417401,"Gavin Wood, University of York;Simon O’Keefe, University of York","Despite the fact that people are increasingly listening to music electronically, the core interface of the common tools for playing the music have had very little improvement. In particular the tools for intra-track navigation have remained basically static, not taking advantage of recent studies into the field of audio jisting, summarising and segmentation. We introduce a novel mechanism for musical audio linear summarisation and modify a widely used open source media player to utilise several music information retrieval techniques directly in the graphical user interface. With a broad range of music, we provide a qualitative discussion on several techniques used for content-based music information retrieval and perform quantitative investigation to their usefulness."
Christopher Harte;Mark B. Sandler;Samer A. Abdallah;Emilia Gómez,Symbolic Representation of Musical Chords: A Proposed Syntax for Text Annotations.,2005,https://doi.org/10.5281/zenodo.1415114,"Christopher Harte, Mark Sandler and Samer Abdallah, Centre for Digital Music, Queen Mary, University of London;Emilia G´omez, Music Technology Group, IUA, Universitat Pompeu Fabra","In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs."
Mika Kuuskankare;Mikael Laurson,Annotating Musical Scores in ENP.,2005,https://doi.org/10.5281/zenodo.1417805,"Mika Kuuskankare, Sibelius Academy;Mikael Laurson, Sibelius Academy","The focus of this paper is on ENP-expressions that can be used for annotating ENP scores with user deﬁnable information. ENP is a music notation program written in Lisp and CLOS with a special focus on compositional and music analytical applications. We present a number of built-in expressions suitable for visualizing, for example, music analytical information as a part of music notation. A Lisp and CLOS based system for creating user-deﬁnable annotation information is also presented along with some sample algorithms. Finally, our system for automatically analyzing and annotating an ENP score is illustrated through several examples including some dealing with music information retrieval."
Perfecto Herrera;Òscar Celma;Jordi Massaguer;Pedro Cano;Emilia Gómez;Fabien Gouyon;Markus Koppenberger,MUCOSA: A Music Content Semantic Annotator.,2005,https://doi.org/10.5281/zenodo.1415980,"Perfecto Herrera, Universitat Pompeu Fabra;Òscar Celma, Universitat Pompeu Fabra;Jordi Massaguer, Universitat Pompeu Fabra;Pedro Cano, Universitat Pompeu Fabra;Emilia Gómez, Universitat Pompeu Fabra;Fabien Gouyon, Universitat Pompeu Fabra;Markus Koppenberger, Universitat Pompeu Fabra;David García, Universitat Pompeu Fabra;José-Pedro García, Universitat Pompeu Fabra;Nicolas Wack, Universitat Pompeu Fabra","MUCOSA (Music Content Semantic Annotator) is an environment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with micro-annotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. across-files annotations), and a collaborative annotation subsystem, which manages large-scale annotation tasks that can be shared among different research centres. The annotation client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes tools for automatic generation of unary descriptors, invention of new descriptors, and propagation of descriptors across sub-collections or playlists. Finally, the collaborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results between several research institutions. A collection of annotated songs is available, as a “starter pack” to all the individuals or institutions that are eager to join this initiative."
Shoichiro Saito;Hirokazu Kameoka;Takuya Nishimoto;Shigeki Sagayama,Specmurt Analysis of Multi-Pitch Music Signals with Adaptive Estimation of Common Harmonic Structure .,2005,https://doi.org/10.5281/zenodo.1417707,"Shoichiro Saito, Graduate School of Information Science and Technology, The University of Tokyo;Hirokazu Kameoka, Graduate School of Information Science and Technology, The University of Tokyo;Takuya Nishimoto, Graduate School of Information Science and Technology, The University of Tokyo;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo","This paper describes a multi-pitch analysis method using specmurt analysis with iterative estimation of the quasi-optimal common harmonic structure function. The iterative algorithm proposed in this paper automatically chooses a proper structure, which results in finding concurrent multiple fundamental frequencies and reduces the dependency on heuristically chosen initial common harmonic structure. The experimental evaluation showed promising results."
Olivier Gillet;Gaël Richard,Drum Track Transcription of Polyphonic Music Using Noise Subspace Projection.,2005,https://doi.org/10.5281/zenodo.1415606,"Olivier Gillet, GET / T´el´ecom Paris CNRS LTCI;Ga¨el Richard, GET / T´el´ecom Paris CNRS LTCI","This paper presents a novel drum transcription system for polyphonic music. The system uses a band-wise harmonic/noise decomposition to suppress the deterministic part of the signal contributed by non-rhythmic instruments. The transcription is then performed on the residual noise signal, which contains most of the rhythmic information. The events associated with each onset are classified using support vector machines (SVM) with probabilistic outputs. The system achieves precision and recall rates of 84% for the bass drum and snare drum detection tasks."
Nick Collins,Using a Pitch Detector for Onset Detection.,2005,https://doi.org/10.5281/zenodo.1417309,"Nick Collins, University of Cambridge","A segmentation strategy is explored for monophonic instrumental pitched non-percussive material (PNP) which proceeds from the assertion that human-like event analysis can be founded on a notion of stable pitch percept. A constant-Q pitch detector following the work of Brown and Puckette provides pitch tracks which are post processed in such a way as to identify likely transitions between notes. A core part of this preparation of the pitch detector signal is an algorithm for vibrato suppression. An evaluation task is undertaken on slow attack and high vibrato PNP source files with human annotated onsets, exemplars of a difficult case in monophonic source segmentation. The pitch track onset detection algorithm shows an improvement over the previous best performing algorithm from a recent comparison study of onset detectors. Whilst further timbral cues must play a part in a general solution, the method shows promise as a component of a note event analysis system."
Parag Chordia,Segmentation and Recognition of Tabla Strokes.,2005,https://doi.org/10.5281/zenodo.1416000,"Parag Chordia, CCRMA, Stanford University","A system that segments and labels tabla strokes from real performances is described. Performance is evaluated on a large database taken from three performers under different recording conditions, containing a total of 16,834 strokes. The current work extends previous work by Gillet and Richard (2003) on categorizing tabla strokes, by using a larger, more diverse database that includes their data as a benchmark, and by testing neural networks and tree-based classification methods. First, the time-domain signal was segmented using complex-domain thresholding that looked for sudden changes in amplitude and phase discontinuities. At the optimal point on the ROC curve, false positives were less than 1% and false negatives were less than 2%. Then, classification was performed using a multivariate Gaussian model (mv gauss) as well as non-parametric techniques such as probabilistic neural networks (pnn), feed-forward neural networks (ffnn), and tree-based classifiers. Two evaluation protocols were used. The first used 10-fold cross validation. The recognition rate averaged over several experiments that contained 10-15 classes was 92% for the mv gauss, 94% for the ffnn and pnn, and 84% for the tree based classifier. To test generalization, a more difficult independent evaluation was undertaken in which no test strokes came from the same recording as the training strokes. The average recognition rate over a wide variety of test conditions was 76% for the mv gauss, 83% for the ffnn, 76% for the pnn, and 66% for the tree classifier."
Hirokazu Kameoka;Takuya Nishimoto;Shigeki Sagayama,Harmonic-Temporal Clustering via Deterministic Annealing EM Algorithm for Audio Feature Extraction.,2005,https://doi.org/10.5281/zenodo.1417629,"Hirokazu Kameoka, Graduate School of Information Science and Technology, The University of Tokyo;Takuya Nishimoto, Graduate School of Information Science and Technology, The University of Tokyo;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo","This paper proposes “harmonic-temporal structured clustering (HTC) method”, that allows simultaneous estimation of pitch, intensity, onset, duration, etc., of each underlying source in multi-stream audio signal, which we expect to be an effective feature extraction for MIR systems. STC decomposes the energy patterns diffused in time-frequency space, i.e., a time series of power spectrum, into distinct clusters such that each of them is originated from a single sound stream. It becomes clear that the problem is equivalent to geometrically approximating the observed time series of power spectrum by superimposed harmonic-temporal structured models (HTMs), whose parameters are directly associated with the specific acoustic characteristics. The update equations in DA(Deterministic Annealing)EM algorithm for the optimal parameter convergence are derived by formulating the model with Gaussian kernel representation. The experiment showed promising results, and verified the potential of the proposed method."
Jenn Riley,Exploiting Musical Connections: A Proposal for Support of Work Relationships in a Digital Music Library.,2005,https://doi.org/10.5281/zenodo.1415660,"Jenn Riley, Indiana University Digital Library Program","ABSTRACT Musical works in the Western art music tradition exist in a complex, inter-related web. Works that are derivative or part of another work are common; however, most music information retrieval systems, including traditional library catalogs, don’t use these essential relationships to improve search results or provide information about them to end-users. As part of the NSF-funded Variations2 Digital Music Library project at Indiana University, we have developed a set of functional requirements defining how derivative and whole/part relationships between musical works should be acted upon in search results, and how these results should be displayed. This paper describes recent research into these relationships, provides examples why they are important in Western art music, outlines how Variations2 or any other music information retrieval system could use these relationships in matching user queries, and describes optimal displays of these relationships to end-users."
Ajay Kapur;Richard I. McWalter;George Tzanetakis,New Music Interfaces for Rhythm-Based Retrieval.,2005,https://doi.org/10.5281/zenodo.1418313,"Ajay Kapur, University of Victoria;Richard I. McWalter, University of Victoria;George Tzanetakis, University of Victoria","In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and Indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of performance practice."
Ioannis Karydis;Alexandros Nanopoulos;Apostolos N. Papadopoulos;Dimitrios Katsaros 0001;Yannis Manolopoulos,Content-Based Music Information Retrieval in Wireless Ad-Hoc Networks.,2005,https://doi.org/10.5281/zenodo.1417665,"Ioannis Karydis, Aristotle University, Thessaloniki 54124, Greece;Alexandros Nanopoulos, Aristotle University, Thessaloniki 54124, Greece;Dimitrios Katsaros, Aristotle University, Thessaloniki 54124, Greece;Yannis Manolopoulos, Aristotle University, Thessaloniki 54124, Greece;Apostolos Papadopoulos, ",This paper introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks. The authors investigate the challenges posed by the wireless medium and propose novel techniques that reduce response times and traffic compared to naive approaches. Experimental results demonstrate the effectiveness and efficiency of the proposed method in this bandwidth-starving and volatile environment.
Richard Lobb;Tim Bell;David Bainbridge 0001,Fast Capture of Sheet Music for an Agile Digital Music Library.,2005,https://doi.org/10.5281/zenodo.1417989,"Richard Lobb, Department of Computer Science and Software Engineering, University of Canterbury;Tim Bell, Department of Computer Science and Software Engineering, University of Canterbury;David Bainbridge, Department of Computer Science, University of Waikato","A personal digital music library needs to be “agile”, that is, it needs to make it easy to capture and index material on the ﬂy. A digital camera is a particularly effective way of achieving this, but there are several issues with the quality of the captured image, including distortions in the shape of the image due to the camera not being aligned properly with the page, non-planarity of the page, lens distortion from close-up shots, and inconsistent lighting across the page. In this paper we explore ways to improve the quality of music images captured by a digital camera or an inexpensive scanner, where the user is not expected to pay a lot of attention to the process. Such pre-processing will significantly aid Music Information Retrieval indexing through Optical Music Recognition, for example. The research presented here is primarily based around using a Fast Fourier Transform (FFT) to determine the orientation of the page. We ﬁnd that a windowed FFT is effective at correcting rotational errors, and we make significant progress towards removing perspective distortion introduced by the camera not being parallel with the music."
Rainer Typke;Frans Wiering;Remco C. Veltkamp,A Survey of Music Information Retrieval Systems.,2005,https://doi.org/10.5281/zenodo.1417383,"Rainer Typke, Universiteit Utrecht;Frans Wiering, Universiteit Utrecht;Remco C. Veltkamp, Universiteit Utrecht","This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks."
Graham E. Poliner;Daniel P. W. Ellis,A Classification Approach to Melody Transcription.,2005,https://doi.org/10.5281/zenodo.1414796,"Graham E. Poliner and Daniel P.W. Ellis, LabROSA, Dept. of Electrical Engineering, Columbia University","Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. In contrast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Support Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems."
Emilios Cambouropoulos;Maxime Crochemore;Costas S. Iliopoulos;Manal Mohamed;Marie-France Sagot,A Pattern Extraction Algorithm for Abstract Melodic Representations that Allow Partial Overlapping of Intervallic Categories.,2005,https://doi.org/10.5281/zenodo.1415008,"Emilios Cambouropoulos, Department of Music Studies, University of Thessaloniki;Maxime Crochemore, Institut Gaspard-Monge, University of Marne-la-Vallée;Costas Iliopoulos, Department of Computer Science, King’s College London;Manal Mohamed, Department of Computer Science, King’s College London;Marie-France Sagot, INRIA Rhône-Alpes, Université Claude Bernard","This paper proposes an efficient pattern extraction algorithm that can be applied on melodic sequences that are represented as strings of abstract intervallic symbols; the melodic representation introduces special “don’t care” symbols for intervals that may belong to two partially overlapping intervallic categories. As a special case the well established “step-leap” representation is examined. In the step-leap representation, each melodic diatonic interval is classified as a step (±s), a leap (±l) or a unison (u). Binary don’t care symbols are introduced to represent the possible overlapping between the various abstract categories e.g. ∗ = s, ∗ = l and # = −s, # = −l. For such a sequence, we are interested in finding maximal repeating pairs and repetitions with a hole (two matching subsequences separated with an intervening non-matching symbol). We propose an O(n + d(n − d) + z)-time algorithm for computing all such repetitions in a given sequence x = x[1..n] with d binary don’t care symbols, where z is the output size."
Rui Pedro Paiva,On the Detection of Melody Notes in Polyphonic Audio.,2005,https://doi.org/10.5281/zenodo.1417681,"Rui Pedro Paiva, CISUC – Centre for Informatics and Systems of the University of Coimbra;Teresa Mendes, CISUC – Centre for Informatics and Systems of the University of Coimbra;Amílcar Cardoso, CISUC – Centre for Informatics and Systems of the University of Coimbra","This paper describes a method for melody detection in polyphonic musical signals. Our approach starts by obtaining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, low-salience and harmonically related notes are then eliminated. Finally, the notes comprising the melody are extracted. This is the main topic of this paper. We select the melody notes by making use of note saliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour. Finally, false positives in the extracted melody should be eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or durations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives."
Wei-Ho Tsai;Hung-Ming Yu;Hsin-Min Wang,Query-By-Example Technique for Retrieving Cover Versions of Popular Songs with Similar Melodies.,2005,https://doi.org/10.5281/zenodo.1415200,"Wei-Ho Tsai, Institute of Information Science, Academia Sinica;Hung-Ming Yu, Institute of Information Science, Academia Sinica;Hsin-Min Wang, Institute of Information Science, Academia Sinica","Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of songs specified by users. The technique enables users to listen to songs with an identical tune, but performed by different singers, in different languages, genres, and so on. The proposed system is built on a query-by-example framework, which takes a fragment of the song submitted by the user as input, and returns songs similar to the query in terms of the main melody as output. To handle the likely discrepancies, e.g., tempos, transpositions, and accompaniments between cover versions and the original song, methods are presented to remove the non-vocal portions of the song, extract the sung notes from the accompanied vocals, and compare the similarities between the sung note sequences."
Olivier Lartillot,Efficient Extraction of Closed Motivic Patterns in Multi-Dimensional Symbolic Representations of Music.,2005,https://doi.org/10.5281/zenodo.1418129,"Olivier Lartillot, University of Jyv¨askyl¨a","An efficient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is usually filtered using global selective mechanisms, based on pattern frequency and length. The proposed approach is founded instead on the concept of closed pattern, and ensures lossless compression through an adaptive selection of most specific descriptions in the multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling the filtering of another form of combinatorial redundancy provoked by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalizing particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but requires instead a complex and detailed modeling of the cognitive system ruling the listening processes. The resulting algorithm is able to offer for the first time compact and relevant motivic analyses of monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex motivic transformations."
Norman H. Adams;Daniela Marquez;Gregory H. Wakefield,Iterative Deepening for Melody Alignment and Retrieval.,2005,https://doi.org/10.5281/zenodo.1415712,"Norman Adams, University of Michigan;Daniela Marquez, University of Michigan;Gregory Wakefield, University of Michigan","For melodic theme retrieval there is a fundamental trade-off between retrieval performance and retrieval speed. Melodic representations of large dimension yield the best retrieval performance, but at high computational cost, and vice versa. In the present work we explore the use of iterative deepening to achieve robust retrieval performance, but without the accompanying computational burden. In particular, we propose the use of a smooth pitch contour that facilitates query and target representations of variable length. We implement an iterative query-by-humming system that yields a dramatic increase in speed, without degrading performance compared to contemporary retrieval systems. Furthermore, we expand the conventional iterative framework to retain the alignment paths found in each iteration. These alignment paths are used to adapt the alignment window of subsequent iterations, further expediting retrieval without degrading performance."
Jeremy Pickens;Costas S. Iliopoulos,Markov Random Fields and Maximum Entropy Modeling for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1414716,"Jeremy Pickens, Department of Computer Science, King’s College London;Costas Iliopoulos, Department of Computer Science, King’s College London","Music information retrieval is a complex task that involves finding models or measures to determine the similarity between two pieces of music. However, the features extracted from music sources are often single-dimensional or assumed to be orthogonal. In this paper, the authors present a framework that allows for the combination of non-independent features without making the independence assumption. The effectiveness of the framework is demonstrated through evaluation on the polyphonic theme similarity task. The framework is general and can be applied to a range of music information retrieval tasks."
Bryan Pardo;Manan Sanghi,Polyphonic Musical Sequence Alignment for Database Search.,2005,https://doi.org/10.5281/zenodo.1417909,"Bryan Pardo, Computer Science Department Northwestern University;Manan Sanghi, Computer Science Department Northwestern University","Finding the best matching database target to a melodic query has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces. However, most tonal music is polyphonic, with multiple concurrent musical lines. Such pieces are not adequately represented as strings. Moreover, users often represent polyphonic pieces in their queries by skipping from one part (the soprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. These approaches are compared using synthetic queries on a database of Bach pieces. Results indicate that when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not."
Ning Hu;Roger B. Dannenberg,A Bootstrap Method for Training an Accurate Audio Segmenter.,2005,https://doi.org/10.5281/zenodo.1416200,"Ning Hu, Carnegie Mellon University;Roger B. Dannenberg, Carnegie Mellon University","Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and hand-labeling is quite difficult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are first used to find the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to refine the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data."
Pierre Roy;Jean-Julien Aucouturier;François Pachet;Anthony Beurivé,Exploiting the Tradeoff Between Precision and Cpu-Time to Speed Up Nearest Neighbor Search.,2005,https://doi.org/10.5281/zenodo.1417453,"Pierre Roy, SONY Computer Science Laboratory Paris;Jean-Julien Aucouturier, SONY Computer Science Laboratory Paris;Franc¸ois Pachet, SONY Computer Science Laboratory Paris;Anthony Beuriv´e, SONY Computer Science Laboratory Paris","We describe an incremental ﬁltering algorithm to quickly compute the N nearest neighbors according to a similarity measure in a metric space. The algorithm exploits an intrinsic property of a large class of similarity measures for which some parameter p has a positive influence both on the precision and the cpu cost (precision-cputime trade-off). The algorithm uses successive approximations of the measure to compute ﬁrst cheap distances on the whole set of possible items, then more and more expensive measures on smaller and smaller sets. We illustrate the algorithm on the case of a timbre similarity algorithm, which compares gaussian mixture models using a Monte Carlo approximation of the Kullback-Leibler distance, where p is the number of points drawn from the distributions. We describe several Monte Carlo algorithmic variants, which improve the convergence speed of the approximation. On this problem, the algorithm performs more than 30 times faster than the naive approach."
Nancy Bertin;Alain de Cheveigné,Scalable Metadata and Quick Retrieval of Audio Signals.,2005,https://doi.org/10.5281/zenodo.1417079,"Nancy Bertin, Equipe Audition CNRS UMR 8581 - ENS (DEC);Alain de Cheveign´e, Equipe Audition CNRS UMR 8581 - ENS (DEC)","Audio search algorithms have become faster and more accurate, allowing for efficient searching within large databases of audio. However, the size of the metadata used for these algorithms also increases exponentially, leading to increased storage costs and search times. This paper introduces the concept of scalable metadata, which adjusts to the increasing size of data and metadata. The authors argue that scalability is beneficial for hierarchical structures that enable fast search and demonstrate this by adapting a state-of-the-art search algorithm to a scalable indexing structure. Scalability allows search algorithms to adapt to database size increases without sacrificing performance."
Charles L. Parker,Applications of Binary Classification and Adaptive Boosting to the Query-By-Humming Problem.,2005,https://doi.org/10.5281/zenodo.1416482,"Charles Parker, Oregon State University","In the ""query-by-humming"" problem, the goal is to retrieve a specific song from a target set based on a sung query. Recent evaluations have shown that the state-of-the-art algorithm is a dynamic programming-based interval matching technique. In this paper, we propose an algorithm that combines techniques from artificial intelligence to outperform the current state-of-the-art with only a negligible increase in running time."
Ming Li;Ronan Sleep,Genre Classification via an LZ78-Based String Kernel.,2005,https://doi.org/10.5281/zenodo.1415162,"Ming Li, School of Computing Sciences, University of East Anglia;Ronan Sleep, School of Computing Sciences, University of East Anglia","We develop the notion of normalized information distance (NID) [7] into a kernel distance suitable for use with a Support Vector Machine classifier, and demonstrate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is efficient to compute, yet generates an accuracy which compares well with recent works."
Arthur Flexer;Elias Pampalk;Gerhard Widmer,Novelty Detection Based on Spectral Similarity of Songs.,2005,https://doi.org/10.5281/zenodo.1416504,"Arthur Flexer, Institute of Medical Cybernetics and Artiﬁcial Intelligence, Center for Brain Research, Medical University of Vienna;Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Gerhard Widmer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Department of Computational Perception, Johannes Kepler University","We are introducing novelty detection, i.e. the automatic identification of new or unknown data not covered by the training data, to the field of music information retrieval. Two methods for novelty detection - one based solely on the similarity information and one also utilizing genre label information - are evaluated within the context of genre classification based on spectral similarity. Both are shown to perform equally well."
Richard Stenzel;Thomas Kamps,Improving Content-Based Similarity Measures by Training a Collaborative Model.,2005,https://doi.org/10.5281/zenodo.1416090,"Richard Stenzel, Fraunhofer IPSI;Thomas Kamps, Fraunhofer IPSI","We observed that for multimedia data – especially music - collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with >250,000,000 collaborative data points crawled from the web and >190,000 songs annotated with content-based sound feature sets. A song mentioned in a playlist is regarded as one collaborative data point. In this paper we present a novel approach to bridging the performance gap between collaborative and content-based similarity measures. In the initial training phase a model vector for each song is computed, based on collaborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using explicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale e-commerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vectors. These predicted model vectors are finally used to compute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that offer a collaborative similarity recommender as service to their customers."
Fabio Vignoli;Steffen Pauws,A Music Retrieval System Based on User Driven Similarity and Its Evaluation.,2005,https://doi.org/10.5281/zenodo.1418359,"Fabio Vignoli, Philips Research Laboratories;Steffen Pauws, Philips Research Laboratories","Large music collections require new ways to let users interact with their music. The concept of finding ‘similar’ songs, albums, or artists provides handles to users for easy navigation and instant retrieval. This paper presents the realization and user evaluation of a music retrieval music that sorts songs on the basis of similarity to a given seed song. Similarity is based on a user-weighted combination of timbre, genre, tempo, year, and mood. A conclusive user evaluation assessed the usability of the system in comparison to two control systems in which the user control of defining the similarity measure was diminished."
David Meredith 0001;Geraint A. Wiggins,Comparing Pitch Spelling Algorithms.,2005,https://doi.org/10.5281/zenodo.1416366,"David Meredith, Centre for Cognition, Computation and Culture, Department of Computing, Goldsmiths’ College, University of London;Geraint A. Wiggins, Centre for Cognition, Computation and Culture, Department of Computing, Goldsmiths’ College, University of London","A pitch spelling algorithm predicts the pitch names of the notes in a musical passage when given the onset-time, MIDI note number and possibly the duration and voice of each note. Various versions of the algorithms of Longuet-Higgins, Cambouropoulos, Temperley and Sleator, Chew and Chen, and Meredith were run on a corpus containing 195972 notes, equally divided between eight classical and baroque composers. The standard deviation of the accuracies achieved by each algorithm over the eight composers was used as a measure of its style dependence (SD). Meredith’s ps1303 was the most accurate algorithm, spelling 99.43% of the notes correctly (SD = 0.54). The best version of Chew and Chen’s algorithm was the least dependent on style (SD = 0.35) and spelt 99.15% of the notes correctly. A new version of Cambouropoulos’s algorithm, combining features of all three versions described by Cambouropoulos himself, also spelt 99.15% of the notes correctly (SD = 0.47). The best version of Temperley and Sleator’s algorithm spelt 97.79% of the notes correctly, but nearly 70% of its errors were due to a single sudden enharmonic change. Longuet-Higgins’s algorithm spelt 98.21% of the notes correctly (SD = 1.79) but only when it processed the music a voice at a time."
Meinard Müller;Frank Kurth;Michael Clausen,Audio Matching via Chroma-Based Statistical Features.,2005,https://doi.org/10.5281/zenodo.1416800,"Meinard M¨uller, Universit¨at Bonn, Institut f¨ur Informatik III;Frank Kurth, Universit¨at Bonn, Institut f¨ur Informatik III;Michael Clausen, Universit¨at Bonn, Institut f¨ur Informatik III","In this paper, we describe an efficient method for audio matching which performs effectively for a wide range of classical music. The basic goal of audio matching can be described as follows: consider an audio database containing several CD recordings for one and the same piece of music interpreted by various musicians. Then, given a short query audio clip of one interpretation, the goal is to automatically retrieve the corresponding excerpts from the other interpretations. To solve this problem, we introduce a new type of chroma-based audio feature that strongly correlates to the harmonic progression of the audio signal. Our feature shows a high degree of robustness to variations in parameters such as dynamics, timbre, articulation, and local tempo deviations. As another contribution, we describe a robust matching procedure, which allows to handle global tempo variations. Finally, we give a detailed account on our experiments, which have been carried out on a database of more than 110 hours of audio comprising a wide range of classical music."
Ching-Hua Chuan;Elaine Chew,Fuzzy Analysis in Pitch-Class Determination for Polyphonic Audio Key Finding.,2005,https://doi.org/10.5281/zenodo.1417297,"Ching-Hua Chuan, Department of Computer Science, University of Southern California;Elaine Chew, Epstein Dep of Industrial & Systems Eng, University of Southern California","This paper presents a fuzzy analysis technique for pitch class determination that improves the accuracy of key finding from audio information. Errors in audio key finding, typically incorrect assignments of closely related keys, commonly result from imprecise pitch class determination and biases introduced by the quality of the sound. Our technique is motivated by hypotheses on the sources of audio key finding errors, and uses fuzzy analysis to reduce the errors caused by noisy detection of lower pitches, and to refine the biased raw frequency data, in order to extract more correct pitch classes. We compare the proposed system to two others, an earlier one employing only peak detection from FFT results, and another providing direct key finding from MIDI. All three used the same key finding algorithm (Chew’s Spiral Array CEG algorithm) and the same 410 classical music pieces (ranging from Baroque to Contemporary). Considering only the first 15 seconds of music in each piece, the proposed fuzzy analysis technique outperforms the peak detection method by 12.18% on average, matches the performance of direct key finding from MIDI 41.73% of the time, and achieves an overall maximum correct rate of 75.25% (compared to 80.34% for MIDI key finding)."
Juan Pablo Bello;Jeremy Pickens,A Robust Mid-Level Representation for Harmonic Content in Music Signals.,2005,https://doi.org/10.5281/zenodo.1417431,"Juan P. Bello and Jeremy Pickens, Centre for Digital Music, Queen Mary, University of London","When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research."
Jean-François Paiement;Douglas Eck;Samy Bengio,A Probabilistic Model for Chord Progressions.,2005,https://doi.org/10.5281/zenodo.1416922,"Jean-François Paiement, IDIAP Research Institute;Douglas Eck, University of Montreal;Samy Bengio, IDIAP Research Institute","Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progressions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies."
J. Stephen Downie;Kris West;Andreas F. Ehmann;Emmanuel Vincent,The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview.,2005,https://doi.org/10.5281/zenodo.1416044,"J. Stephen Downie, GSLIS, University of Illinois at Urbana-Champaign;Kris West, School of Computing Sciences, University of East Anglia;Andreas Ehmann, Electrical Engineering, University of Illinois at Urbana-Champaign;Emmanuel Vincent, Electronic Engineering, Queen Mary University of London",This paper is an extended abstract which provides a brief preliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper concludes with a listing of targets items to be undertaken before MIREX 2006 to ensure the ongoing success of the MIREX framework.
Slim Essid;Gaël Richard;Bertrand David,Inferring Efficient Hierarchical Taxonomies for MIR Tasks: Application to Musical Instruments.,2005,https://doi.org/10.5281/zenodo.1416268,"Slim ESSID, GET-T´el´ecom Paris, CNRS LTCI;Ga¨el RICHARD, GET-T´el´ecom Paris, CNRS LTCI;Bertrand DAVID, GET-T´el´ecom Paris, CNRS LTCI","A number of approaches for automatic audio classification are based on hierarchical taxonomies since it is acknowledged that improved performance can be thereby obtained. In this paper, we propose a new strategy to automatically acquire hierarchical taxonomies, using machine learning methods, which are expected to maximize the performance of subsequent classification. It is shown that the optimal hierarchical taxonomy of musical instruments (in the sense of inter-class distances) does not follow the traditional and more intuitive instrument classification into instrument families."
Hiromasa Fujihara;Tetsuro Kitahara;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Singer Identification Based on Accompaniment Sound Reduction and Reliable Frame Selection.,2005,https://doi.org/10.5281/zenodo.1418285,"Hiromasa Fujihara, Dept. of Intelligence Science and Technology;Tetsuro Kitahara, Dept. of Intelligence Science and Technology;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Kazunori Komatani, Dept. of Intelligence Science and Technology;Tetsuya Ogata, Dept. of Intelligence Science and Technology;Hiroshi G. Okuno, Dept. of Intelligence Science and Technology","This paper describes a method for automatic singer identification from polyphonic musical audio signals including sounds of various instruments. The main problem in automatically identifying singers is the negative influences caused by accompaniment sounds. To solve this problem, the authors developed two methods: accompaniment sound reduction and reliable frame selection. Experimental results showed that their method was able to reduce the influences of accompaniment sounds and achieved an accuracy of 95%."
Shankar Vembu;Stephan Baumann 0001,Separation of Vocals from Polyphonic Audio Recordings .,2005,https://doi.org/10.5281/zenodo.1414852,"Shankar Vembu, German Research Centre for AI;Stephan Baumann, German Research Centre for AI","Source separation techniques like independent component analysis and non-negative matrix factorization are commonly used for separating individual tracks in music samples. However, these techniques struggle to separate non-stationary sources like speech or vocals. In this paper, the authors propose solutions for extracting vocal tracks from polyphonic audio recordings. They also present techniques for identifying vocal sections in a music sample and designing a classifier for vocal-nonvocal segmentation. Additionally, they describe an application for extracting the melody from the separated vocal track. The experimental work suggests that the quality of vocal source separation is not sufficient for further melody analysis, and further investigation is needed to improve the quality of vocal source separation."
Norman Casagrande;Douglas Eck;Balázs Kégl,Frame-Level Audio Feature Extraction Using AdaBoost.,2005,https://doi.org/10.5281/zenodo.1414718,"Norman Casagrande, University of Montreal;Douglas Eck, University of Montreal;Bal´azs K´egl, University of Montreal","In this paper we adapt an AdaBoost-based image processing algorithm to the task of predicting whether an audio signal contains speech or music. We derive a frame-level discriminator that is both fast and accurate. Using a simple FFT and no built-in prior knowledge of signal structure we obtain an accuracy of 88% on frames sampled at 20ms intervals. When we smooth the output of the classifier with the output of the previous 40 frames our forecast rate rises to 93% on the Scheirer-Slaney (Scheirer and Slaney, 1997) database. To demonstrate the efficiency and effectiveness of the model, we have implemented it as a graphical real-time plugin to the popular Winamp audio player."
Petri Toiviainen;Tuomas Eerola,Classification of Musical Metre with Autocorrelation and Discriminant Functions.,2005,https://doi.org/10.5281/zenodo.1416040,"Petri Toiviainen, Department of Music, University of Jyväskylä, Finland;Tuomas Eerola, Department of Music, University of Jyväskylä, Finland","The performance of autocorrelation-based metre induction was tested with two large collections of folk melodies, consisting of approximately 13,000 melodies in MIDI file format, for which the correct metres were available. The analysis included a number of melodic accents assumed to contribute to metric structure. The performance was measured by the proportion of melodies whose metre was correctly classified by Multiple Discriminant Analysis. Overall, the method predicted notated metre with an accuracy of 75% for classification into nine categories of metre. The most frequent confusions were made within the groups of duple and triple/compound metres, whereas confusions across these groups where significantly less frequent. In addition to note onset locations and note durations, Thomassen's melodic accent was found to be an important predictor of notated metre."
Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo,ATTA: Automatic Time-Span Tree Analyzer Based on Extended GTTM.,2005,https://doi.org/10.5281/zenodo.1415572,"Masatoshi Hamanaka, PRESTO, Japan Science and Technology Agency;Keiji Hirata, NTT Communication Science Laboratories;Satoshi Tojo, Japan Advanced Institute of Science and Technology","This paper describes a music analyzing system called the automatic time-span tree analyzer (ATTA), which we have developed. The ATTA derives a time-span tree that assigns a hierarchy of 'structural importance' to the notes of a piece of music based on the generative theory of tonal music (GTTM). Although the time-span tree has been applied with music summarization and collaborative music creation systems, these systems use time-span trees manually analyzed by experts in musicology. Previous systems based on GTTM cannot acquire a time-span tree without manual application of most of the rules, because GTTM does not resolve much of the ambiguity that exists with the application of the rules. To solve this problem, we propose a novel computational model of the GTTM that re-formalizes the rules with computer implementation. The main advantage of our approach is that we can introduce adjustable parameters, which enables us to assign priority to the rules. Our analyzer automatically acquires time-span trees by configuring the parameters that cover 26 rules out of 36 GTTM rules for constructing a time-span tree. Experimental results showed that after these parameters were tuned, our method outperformed a baseline performance. We hope to distribute the time-span tree as the content for various musical tasks, such as searching and arranging music."
Roger B. Dannenberg,"Toward Automated Holistic Beat Tracking, Music Analysis and Understanding.",2005,https://doi.org/10.5281/zenodo.1415246,"Roger B. Dannenberg, School of Computer Science Carnegie Mellon University","Most music processing focuses on analyzing specific features or elements such as pitch, beat location, tempo, or genre. However, music is interconnected at many levels, and the interplay of melody, harmony, and rhythm are important in perception. This paper presents a step towards more holistic music analysis by using music structure to constrain a beat tracking program. By incorporating structural information, the beat tracker shows a significant improvement. The implications of this work for other music analysis problems are discussed."
Kristoffer Jensen;Jieping Xu;Martin Zachariasen,Rhythm-Based Segmentation of Popular Chinese Music.,2005,https://doi.org/10.5281/zenodo.1418117,"Kristoffer Jensen, Department of Medialogy, University of Aalborg Esbjerg;Jieping Xu, School of Information, Renmin University;Martin Zachariasen, Department of Computer Science, University of Copenhagen","We present a new method to segment popular music based on rhythm. By computing a shortest path based on the self-similarity matrix calculated from a model of rhythm, segmenting boundaries are found along the diagonal of the matrix. The cost of a new segment is optimized by matching manual and automatic segment boundaries. We compile a small song database of 21 randomly selected popular Chinese songs which come from Chinese Mainland, Taiwan and Hong Kong. The segmenting results on the small corpus show that 78% manual segmentation points are detected and 74% automatic segmentation points are correct. Automatic segmentation achieved 100% correct detection for 5 songs. The results are very encouraging."
Frank Kurth;Meinard Müller;David Damm;Christian Fremerey;Andreas Ribbrock;Michael Clausen,Syncplayer - An Advanced System for Multimodal Music Access.,2005,https://doi.org/10.5281/zenodo.1416496,"Frank Kurth, Universit¨at Bonn;Meinard M¨uller, Universit¨at Bonn;David Damm, Universit¨at Bonn;Christian Fremerey, Universit¨at Bonn;Andreas Ribbrock, Universit¨at Bonn;Michael Clausen, Universit¨at Bonn","In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The recording is then identified by the SyncPlayer server, a process which is performed entirely content-based. Subsequently, the server delivers music-related data like scores or lyrics to the client, which are then displayed synchronously with audio playback using a multimodal visualization plug-in. In addition to visualization, the system provides functionality for content-based music retrieval and semi-manual content annotation. To the best of our knowledge, our system is moreover the first to systematically exploit automatically generated synchronization data for content-based symbolic browsing in high quality audio recordings. SyncPlayer has already proved to be a valuable tool for evaluating algorithms in MIR research on a larger scale. In this paper, we describe the technical background of the SyncPlayer framework in detail. We also give an overview of the underlying MIR techniques of audio matching, music synchronization, and text-based retrieval that are incorporated in the current version of the system."
Eric J. Isaacson,What You See Is What You Get: on Visualizing Music.,2005,https://doi.org/10.5281/zenodo.1415992,"Eric Isaacson, Indiana University School of Music","Though music is fundamentally an aural phenomenon, we often communicate about music through visual means. The paper examines a number of visualization techniques developed for music, focusing especially on those developed for music analysis by specialists in the field, but also looking at some less successful approaches. It is hoped that, by presenting them in this way, those in the MIR community will develop a greater awareness of the kinds of musical problems music scholars are concerned with, and might lend a hand toward addressing them."
Fabian Mörchen;Alfred Ultsch;Mario Nöcker;Christian Stamm,Databionic Visualization of Music Collections According to Perceptual Distance.,2005,https://doi.org/10.5281/zenodo.1417967,"Fabian Mörchen, Data Bionics Research Group;Alfred Ultsch, Data Bionics Research Group;Mario Nöcker, Data Bionics Research Group;Christian Stamm, Data Bionics Research Group","We describe the MusicMiner system for organizing large collections of music with databionic mining techniques. Low level audio features are extracted from the raw audio data on short time windows during which the sound is assumed to be stationary. Static and temporal statistics were consistently and systematically used for aggregation of low level features to form high level features. A supervised feature selection targeted to model perceptual distance between different sounding music lead to a small set of non-redundant sound features. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map, displaying local sound differences based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song."
Masataka Goto;Takayuki Goto,"Musicream: New Music Playback Interface for Streaming, Sticking, Sorting, and Recalling Musical Pieces.",2005,https://doi.org/10.5281/zenodo.1415842,"Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Takayuki Goto, National Institute of Advanced Industrial Science and Technology (AIST)","This paper describes a novel music playback interface, called Musicream, which lets a user unexpectedly come across various musical pieces similar to those liked by the user. With most previous “query-by-example” interfaces used for similarity-based searching, for the same query and music collection a user will always receive the same list of musical pieces ranked by their similarity and opportunities to encounter unfamiliar musical pieces in the collection are limited. Musicream facilitates active, flexible, and unexpected encounters with musical pieces by providing four functions: the music-disc streaming function which creates a flow of many musical-piece entities (discs) from a (huge) music collection, the similarity-based sticking function which allows a user to easily pick out and listen to similar pieces from the flow, the meta-playlist function which can generate a playlist of playlists (ordered lists of pieces) while editing them with a high degree of freedom, and the time-machine function which automatically records all Musicream activities and allows a user to visit and retrieve a past state as if using a time machine. In our experiments, these functions were used seamlessly to achieve active and creative querying and browsing of music collections, confirming the effectiveness of Musicream."
Jean-Julien Aucouturier;François Pachet,Ringomatic: A Real-Time Interactive Drummer Using Constraint-Satisfaction and Drum Sound Descriptors.,2005,https://doi.org/10.5281/zenodo.1416532,"Jean-Julien Aucouturier, SONY CSL Paris;Franc¸ois Pachet, SONY CSL Paris","We describe a real-time musical agent that generates an audio drum-track by concatenating audio segments automatically extracted from pre-existing musical files. The drum-track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata automatically extracted from the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We report on several drum track audio descriptors designed for the system. We also describe a basic mechanism for controlling the tradeoff between the agent’s autonomy and reactivity, which we illustrate with experiments made in the context of a virtual duet between the system and a human pianist."
Samer A. Abdallah;Katy C. Noland;Mark B. Sandler;Michael A. Casey;Christophe Rhodes,Theory and Evaluation of a Bayesian Music Structure Extractor.,2005,https://doi.org/10.5281/zenodo.1416018,"Samer Abdallah, Centre for Digital Music;Katy Noland, Centre for Digital Music;Mark Sandler, Centre for Digital Music;Michael Casey, Centre for Cognition, Computation and Culture;Christophe Rhodes, Centre for Cognition, Computation and Culture","We introduce a new model for extracting classiﬁed structural segments, such as intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify signal frames on the basis of their audio properties and then to agglomerate contiguous runs of similarly classiﬁed frames into texturally homogenous (or ‘self-similar’) segments which inherit the classiﬁcaton of their constituent frames. Our work extends previous work on automatic structure extraction by addressing the classiﬁcation problem using using an unsupervised Bayesian clustering model, the parameters of which are estimated using a variant of the expectation maximisation (EM) algorithm which includes deterministic annealing to help avoid local optima. The model identiﬁes and classiﬁes all the segments in a song, not just the chorus or longest segment. We discuss the theory, implementation, and evaluation of the model, and test its performance against a ground truth of human judgements. Using an analogue of a precision-recall graph for segment boundaries, our results indicate an optimal trade-off point at approximately 80% precision for 80% recall."
Xavier Amatriain;Jordi Massaguer;David García;Ismael Mosquera,The CLAM Annotator: A Cross-Platform Audio Descriptors Editing Tool.,2005,https://doi.org/10.5281/zenodo.1416908,"Xavier Amatriain, CREATE;Jordi Massaguer, Universitat Pompeu Fabra;David Garcia, Universitat Pompeu Fabra;Ismael Mosquera, Universitat Pompeu Fabra","This paper presents the CLAM Annotator tool. This application has been developed in the context of the CLAM framework and can be used to manually edit any previously computed audio descriptors. The application offers a convenient GUI that allows to edit low-level frame descriptors, global descriptors of any kind and segmentation marks. It is designed in such a way that the interface adapts itself to a user-defined schema, offering possibilities to a large range of applications."
Tim Bell;David Blizzard;Richard D. Green;David Bainbridge 0001,Design of a Digital Music Stand.,2005,https://doi.org/10.5281/zenodo.1416290,,
Stuart Bray;George Tzanetakis,Distributed Audio Feature Extraction for Music.,2005,https://doi.org/10.5281/zenodo.1417563,"Stuart Bray, Computer Science Department, University of Victoria;George Tzanetakis, Computer Science Department (also in Music), University of Victoria","One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area where computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented."
John Ashley Burgoyne;Lawrence K. Saul,Learning Harmonic Relationships in Digital Audio with Dirichlet-Based Hidden Markov Models.,2005,https://doi.org/10.5281/zenodo.1414870,"J. Ashley Burgoyne, Department of Computer and Information Science, University of Pennsylvania;Lawrence K. Saul, Department of Computer and Information Science, University of Pennsylvania","Harmonic analysis is a standard musicological tool for understanding many pieces of Western classical music and making comparisons among them. This paper presents an approach to teach machines to analyze music in the same way that human music students do, by listening to musical recordings and performances. The approach uses a corpus of recorded Mozart symphonies and transforms the audio files into a series of normalized pitch class profile (PCP) vectors. Simplified rules of tonal harmony are encoded in a transition matrix, and a hidden Markov model (HMM) is used to train Dirichlet distributions for major and minor keys on the PCP vectors. The system successfully tracks chords and keys and shows promise for real-time implementation."
Giordano Ribeiro de Eulalio Cabral;François Pachet;Jean-Pierre Briot,Automatic X Traditional Descriptor Extraction: the Case of Chord Recognition.,2005,https://doi.org/10.5281/zenodo.1415702,"Giordano Cabral, LIP6 – Paris 6;François Pachet, Sony CSL Paris;Jean-Pierre Briot, LIP6 – Paris 6","Audio descriptor extraction is the activity of finding mathematical models which describe properties of the sound, requiring signal processing skills. The scientific literature presents a vast collection of descriptors (e.g. energy, tempo, tonality) each one representing a significant effort of research in finding an appropriate descriptor for a particular application. The Extractor Discovery System (EDS) is a recent approach for the discovery of such descriptors, which aim is to extract them automatically. This system can be useful for both non experts – who can let the system work fully automatically – and experts – who can start the system with an initial solution expecting it to enhance their results. Nevertheless, EDS still needs to be massively tested. We consider that its comparison with the results of problems already studied would be very useful to validate it as an effective tool. This work intends to perform the first part of this validation, comparing the results from classic approaches with EDS results when operated by a completely naïve user building a guitar chord recognizer."
Margaret Cahill;Donncha Ó Maidín,Melodic Similarity Algorithms -- Using Similarity Ratings for Development and Early Evaluation.,2005,https://doi.org/10.5281/zenodo.1415642,"Margaret Cahill, Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland;Donncha Ó Maidín, Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","This paper focuses on gathering similarity ratings for use in the construction, optimization and evaluation of melodic similarity algorithms. The approach involves conducting listening experiments to gather these ratings for a piece in Theme and Variation form."
Domenico Cantone;Salvatore Cristofaro;Simone Faro,"On Tuning the (\delta, \alpha)-Sequential-Sampling Algorithm for \delta-Approximate Matching with Alpha-Bounded Gaps in Musical Sequences.",2005,https://doi.org/10.5281/zenodo.1417791,"Domenico Cantone, Universit`a di Catania, Dipartimento di Matematica e Informatica;Salvatore Cristofaro, Universit`a di Catania, Dipartimento di Matematica e Informatica;Simone Faro, Universit`a di Catania, Dipartimento di Matematica e Informatica","We present a very efficient variant of the (δ, α)-SEQUENTIAL-SAMPLING algorithm, recently introduced by the authors, for the δ-approximate string matching problem with α-bounded gaps, which often arises in many questions on musical information retrieval and musical analysis. Though it retains the same worst-case O(mn)-time and O(mα)-space complexity of its progenitor to compute the number of distinct δ-approximate α-gapped occurrences of a pattern of length m at each position in a text of length n, our new variant achieves an average O(n)-time complexity in practical cases. Extensive experimentations indicate that our algorithm is more efficient than existing solutions for the same problem, especially in the case of long patterns."
Domenico Cantone;Salvatore Cristofaro;Simone Faro,"Solving the (\delta, \alpha)-Approximate Matching Problem Under Transposition Invariance in Musical Sequences.",2005,https://doi.org/10.5281/zenodo.1416892,"Domenico Cantone, Universit`a di Catania, Dipartimento di Matematica e Informatica;Salvatore Cristofaro, Universit`a di Catania, Dipartimento di Matematica e Informatica;Simone Faro, Universit`a di Catania, Dipartimento di Matematica e Informatica","The δ-approximate matching problem arises in many questions concerning musical information retrieval and musical analysis. In the case in which gaps are not allowed between consecutive pitches of the melody, transposition invariance is automatically taken care of, provided that the musical melodies are encoded using the pitch interval encoding. However, in the case in which non-null gaps are allowed between consecutive pitches of the melodies, transposition invariance is not dealt with properly by the algorithms present in literature. In this paper, we propose two slightly different variants of the approximate matching problem under transposition invariance and for each of them provide an algorithm, obtained by adapting an efficient algorithm for the δ-approximate matching problem with α-bounded gaps."
Òscar Celma;Miquel Ramírez;Perfecto Herrera,Foafing the Music: A Music Recommendation System based on RSS Feeds and User Preferences.,2005,https://doi.org/10.5281/zenodo.1414800,"Oscar Celma, Music Technology Group, Universitat Pompeu Fabra, Barcelona, SPAIN;Miquel Ram´ırez, Music Technology Group, Universitat Pompeu Fabra, Barcelona, SPAIN;Perfecto Herrera, Music Technology Group, Universitat Pompeu Fabra, Barcelona, SPAIN","In this paper we give an overview of the Foaﬁng the Music system. The system uses the Friend of a Friend (FOAF) and Rich Site Summary (RSS) vocabularies for recommending music to a user, depending on her musical tastes. Music information (new album releases, related artists’ news and available audio) is gathered from thousands of RSS feeds —an XML format for syndicating Web content. On the other hand, FOAF documents are used to deﬁne user preferences. The presented system provides music discovery by means of: user proﬁling —deﬁned in the user’s FOAF description—, context-based information —extracted from music related RSS feeds— and content-based de- scriptions —extracted from the audio itself."
Wei Chai;Barry Vercoe,Detection of Key Change in Classical Piano Music.,2005,https://doi.org/10.5281/zenodo.1415538,"Wei Chai, MIT Media Laboratory;Barry Vercoe, MIT Media Laboratory","Tonality is an important aspect of musical structure. Detecting key of music is one of the major tasks in tonal analysis and will benefit semantic segmentation of music for indexing and searching. This paper presents an HMM-based approach for segmenting musical signals based on key change and identifying the key of each segment. Classical piano music was used in the experiment. The performance, evaluated by three proposed measures (recall, precision and label accuracy), demonstrates the promise of the method."
Sally Jo Cunningham;J. Stephen Downie;David Bainbridge 0001,"""The Pain, the Pain"": Modelling Music Information Behavior and the Songs We Hate.",2005,https://doi.org/10.5281/zenodo.1417209,"Sally Jo Cunningham, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand;J. Stephen Downie, GSLIS, University of Illinois at Urbana-Champaign;David Bainbridge, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","The paper presents a grounded theory analysis of 395 user responses to the survey question, “What is the worst song ever?” Important factors uncovered include: lyric quality, the “earworm” effect, voice quality, the influence of associated music videos, over-exposure, perceptions of pretentiousness, and associations with unpleasant personal experiences."
Christophe Dalitz;Thomas Karsten,Using the Gamera Framework for Building a Lute Tablature Recognition System.,2005,https://doi.org/10.5281/zenodo.1416378,"Christoph Dalitz, Niederrhein University of Applied Sciences;Thomas Karsten, Niederrhein University of Applied Sciences","In this article we describe an optical recognition system for historic lute tablature prints that we have built with the aid of the Gamera toolkit for document analysis and recognition. We give recognition rates for various historic sources and show that our system works quite well on printed tablature sources using movable types. For engraved and manuscript sources, we discuss some principal current limitations of our system and Gamera."
Sven Degroeve;Koen Tanghe;Bernard De Baets;Marc Leman;Jean-Pierre Martens,A Simulated Annealing Optimization of Audio Features for Drum Classification.,2005,https://doi.org/10.5281/zenodo.1417311,"Sven Degroeve, Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium;Koen Tanghe, Department of Musicology (IPEM), Ghent University, Belgium;Bernard De Baets, Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium;Marc Leman, Department of Musicology (IPEM), Ghent University, Belgium;Jean-Pierre Martens, Department of Electronics and Information Systems (ELIS), Ghent University, Belgium",Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments.
Ruth Dhanaraj;Beth Logan,Automatic Prediction of Hit Songs.,2005,https://doi.org/10.5281/zenodo.1417571,"Ruth Dhanaraj, Hewlett Packard Labs;Beth Logan, Hewlett Packard Labs","We explore the automatic analysis of music to identify likely hit songs. We extract both acoustic and lyric information from each song and separate hits from non-hits using standard classifiers, specifically Support Vector Machines and boosting classifiers. Our features are based on global sounds learnt in an unsupervised fashion from acoustic data or global topics learnt from a lyrics database. Experiments on a corpus of 1700 songs demonstrate performance that is much better than random. The lyric-based features are slightly more useful than the acoustic features in correctly identifying hit songs. Concatenating the two features does not produce significant improvements. Analysis of the lyric-based features shows that the absence of certain semantic information indicates that a song is more likely to be a hit."
Simon Dixon;Gerhard Widmer,MATCH: A Music Alignment Tool Chest.,2005,https://doi.org/10.5281/zenodo.1416952,"Simon Dixon, Austrian Research Institute for Artiﬁcial Intelligence;Gerhard Widmer, Department of Computational Perception, Johannes Kepler University Linz","We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasizes note onsets in the alignment process. In tests with Classical and Romantic piano music, the average alignment error was 41ms (median 20ms), with only 2 out of 683 test cases failing to align. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed."
Peter Jan O. Doets;Reginald L. Lagendijk,Extracting Quality Parameters for Compressed Audio from Fingerprints.,2005,https://doi.org/10.5281/zenodo.1416080,"P.J.O. Doets, Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology;R.L. Lagendijk, Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology","An audio fingerprint is a compact yet very robust representation of the perceptually relevant parts of audio content. It can be used to identify audio, even when severely distorted. Audio compression causes small changes in the fingerprint. We aim to exploit these small fingerprint differences due to compression to assess the perceptual quality of the compressed audio file. Analysis shows that for uncorrelated signals the Bit Error Rate (BER) is approximately inversely proportional to the square root of the Signal-to-Noise Ratio (SNR) of the signal. Experiments using real music confirm this relation. Further experiments show how the various local spectral characteristics cause a large variation in the behavior of the fingerprint difference as a function of SNR or the bitrate set for compression."
Douglas Eck;Norman Casagrande,Finding Meter in Music Using An Autocorrelation Phase Matrix and Shannon Entropy.,2005,https://doi.org/10.5281/zenodo.1415650,"Douglas Eck, University of Montreal;Norman Casagrande, University of Montreal","This paper introduces a novel way to detect metrical structure in music. We introduce a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting autocorrelation phase matrix is useful for several tasks involving metrical structure. First we can use the matrix to enhance standard autocorrelation by calculating the Shannon entropy at each lag. This approach yields improved results for autocorrelation-based tempo induction. Second, we can efficiently search the matrix for combinations of lags that suggest particular metrical hierarchies. This approach yields a good model for predicting the meter of a piece of music. Finally, we can use the phase information in the matrix to align a candidate meter with music, making it possible to perform beat induction with an autocorrelation-based model. We present results for several meter prediction and tempo induction datasets, demonstrating that the approach is competitive with models designed specifically for these tasks. We also present preliminary beat induction results on a small set of artificial patterns."
Rebecca Fiebrink;Cory McKay;Ichiro Fujinaga,Combining D2K and JGAP for Efficient Feature Weighting for Classification Tasks in Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1415754,"Rebecca Fiebrink, McGill University;Cory McKay, McGill University;Ichiro Fujinaga, McGill University","Music classification is an important aspect of music information retrieval research. Feature weighting is a tool that can improve the performance of classifiers, but it is often underutilized due to the time it takes to achieve optimal results. Genetic algorithms offer a solution at a reduced calculation time, but they can still be costly. This paper investigates the advantages of implementing genetic algorithms in a parallel computing environment to make feature weighting more affordable for researchers in music information retrieval."
David Gerhard,Pitch Track Target Deviation in Natural Singing.,2005,https://doi.org/10.5281/zenodo.1418115,"David Gerhard, Department of Computer Science, Department of Music, University of Regina","Unlike ﬁxed-pitch instruments such as the piano, human singing can stray from a target pitch by as much as a semitone while still being perceived as a single ﬁxed note. This paper presents a study of the difference between tar- get pitch and actualized pitch in natural singing. A set of 50 subjects singing the same melody and lyric is used to compare utterance styles. An algorithm for alignment of idealized template pitch tracks to measured frequency tracks is presented. Speciﬁc examples are discussed, and generalizations are made with respect to the types of devi- ations typical in human singing. Demographics, including the skill of the singer, are presented and discussed in the context of the pitch track deviation from the ideal."
Rob van Gulik;Fabio Vignoli,Visual Playlist Generation on the Artist Map.,2005,https://doi.org/10.5281/zenodo.1415206,"Rob van Gulik, Institute of Information and Computing Sciences, Utrecht University;Fabio Vignoli, Philips Research Laboratories","This paper describes a visual playlist creation method based on a previously designed visualization technique for large music collections. The method gives users high-level control over the contents of a playlist as well as the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly portable. Future work includes an extensive user evaluation to compare the described method with alternative techniques and to measure its qualities, such as the perceived ease of use and perceived usefulness."
Peyman Heydarian;Joshua D. Reiss,The Persian Music and the Santur Instrument.,2005,https://doi.org/10.5281/zenodo.1415048,"Peyman Heydarian, Centre For Digital Music, Queen Mary, University of London;Joshua D. Reiss, Centre For Digital Music, Queen Mary, University of London","Persian music has had a profound effect on various Eastern musical cultures, and also influenced Southern European and Northern African music. The Santur, a hammered dulcimer, is one of the most important instruments in Persia. In this paper, Persian music and the Santur instrument are explained and analyzed. Techniques for fundamental frequency detection are applied to data acquired from the Santur and results are reported."
Helge Homburg;Ingo Mierswa;Bülent Möller;Katharina Morik;Michael Wurst,A Benchmark Dataset for Audio Classification and Clustering.,2005,https://doi.org/10.5281/zenodo.1417065,"Helge Homburg, University of Dortmund, AI Unit;Ingo Mierswa, University of Dortmund, AI Unit;B¨ulent M¨oller, University of Dortmund, AI Unit;Katharina Morik, University of Dortmund, AI Unit;Michael Wurst, University of Dortmund, AI Unit","We present a freely available benchmark dataset for audio classification and clustering. This dataset consists of 10 seconds samples of 1886 songs obtained from the Garageband site. Beside the audio clips themselves, textual metadata is provided for the individual songs. The songs are classified into 9 genres. In addition to the genre information, our dataset also consists of 24 hierarchical cluster models created manually by a group of users. This enables a user-centric evaluation of audio classification and clustering algorithms and gives researchers the opportunity to test the performance of their methods on heterogeneous data. We first give a motivation for assembling our benchmark dataset. Then we describe the dataset and its elements in more detail. Finally, we present some initial results using a set of audio features generated by a feature construction approach."
Toru Hosoya;Motoyuki Suzuki;Akinori Ito;Shozo Makino,Lyrics Recognition from a Singing Voice Based on Finite State Automaton for Music Information Retrieval.,2005,https://doi.org/10.5281/zenodo.1417855,"Toru Hosoya, Motoyuki Suzuki, Akinori Ito and Shozo Makino, Graduate School of Engineering, Tohoku University","Recently, several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user’s singing voice. All of these systems use only the melody information for retrieval. Although the lyrics information is useful for retrieval, there have been few attempts to exploit lyrics in the user’s input. In order to develop a MIR system that uses lyrics and melody information, lyrics recognition is needed. Lyrics recognition from a singing voice is achieved by similar technology to that of speech recognition. The difference between lyrics recognition and general speech recognition is that the input lyrics are a part of the lyrics of songs in a database. To exploit linguistic constraints maximally, we described the recognition grammar using a finite state automaton (FSA) that accepts only lyrics in the database. In addition, we carried out a “singing voice adaptation” using a speaker adaptation technique. In our experimental results, about 86% retrieval accuracy was obtained."
Xiao Hu 0001;J. Stephen Downie;Kris West;Andreas F. Ehmann,Mining Music Reviews: Promising Preliminary Results.,2005,https://doi.org/10.5281/zenodo.1417067,"Xiao Hu, GSLIS, University of Illinois at Urbana-Champaign;J. Stephen Downie, GSLIS, University of Illinois at Urbana-Champaign;Kris West, School of Computing Sciences, University of East Anglia;Andreas Ehmann, Electrical Engineering, University of Illinois at Urbana-Champaign","In this paper we present a system for the automatic mining of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genre of the music reviewed and to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically mine arbitrary bodies of text, such as weblogs (blogs) for musically relevant information."
Özgür Izmirli,Tonal Similarity from Audio Using a Template Based Attractor Model.,2005,https://doi.org/10.5281/zenodo.1416688,"Özgür İzmirli, Center for Arts and Technology Connecticut College","A model that calculates similarity of tonal evolution among pieces in an audio database is presented. The model employs a template based key finding algorithm. This algorithm is used in a sliding window fashion to obtain a sequence of tonal center estimates that delineate the trajectory of tonal evolution in tonal space. A chroma based representation is used to capture tonality information. Templates are formed from instrument sounds weighted according to pitch distribution profiles. For each window in the input audio, the chroma based representation is interpreted with respect to the precalculated templates that serve as attractor points in tonal space. This leads to a discretization in both time and tonal space making the output representation compact. Local and global variations in tempo are accounted for using dynamic time warping that employs a special type of music theoretical distance measure. Evaluation is given in two stages. The first is evaluation of the key finding model to assess its performance in key finding for raw audio input. The second is based on cross validation testing for pieces that have multiple performances in the database to determine the success of recall by distance."
Jyh-Shing Roger Jang;Chao-Ling Hsu;Hong-Ru Lee,Continuous HMM and Its Enhancement for Singing/Humming Query Retrieval.,2005,https://doi.org/10.5281/zenodo.1414842,"Jyh-Shing Roger Jang, Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University;Chao-Ling Hsu, Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University;Hong-Ru Lee, Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University","The use of HMM (Hidden Markov Models) for speech recognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by singing/humming, has seldom been reported, partly due to the difference in acoustic characteristics between speech and singing/humming inputs. This paper will derive the formula of CHMM training for frame-based MRAI. In particular, we shall propose enhancement to CHMM and demonstrate that with the enhancement scheme, CHMM can compare favourably with DTW in both efficiency and effectiveness."
Phillip B. Kirlin;Paul E. Utgoff,VOISE: Learning to Segregate Voices in Explicit and Implicit Polyphony.,2005,https://doi.org/10.5281/zenodo.1417225,"Phillip B. Kirlin and Paul E. Utgoff, Department of Computer Science, University of Massachusetts Amherst","Finding multiple occurrences of themes and patterns in music can be hampered due to polyphonic textures. This is caused by the complexity of music that weaves multiple independent lines of music together. We present and demonstrate a system, VoiSe, that is capable of isolating individual voices in both explicit and implicit polyphonic music. VoiSe is designed to work on a symbolic representation of a music score, and consists of two components: a same-voice predicate implemented as a learned decision tree, and a hard-coded voice numbering algorithm."
Tetsuro Kitahara;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,"Instrument Identification in Polyphonic Music: Feature Weighting with Mixed Sounds, Pitch-Dependent Timbre Modeling, and Use of Musical Context.",2005,https://doi.org/10.5281/zenodo.1415724,"Tetsuro Kitahara, Dept. of Intelligence Science and Technology;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST);Kazunori Komatani, Dept. of Intelligence Science and Technology;Tetsuya Ogata, Dept. of Intelligence Science and Technology;Hiroshi G. Okuno, Dept. of Intelligence Science and Technology","This paper addresses the problem of identifying musical instruments in polyphonic music. Only a few studies have dealt with this issue, and there are three main challenges: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. To address these challenges, the authors propose a method that extracts feature vectors from both isolated sounds and sound mixtures, weights the features based on their robustness, models the pitch dependency using an F0-dependent multivariate normal distribution, and calculates the a priori probability of each note based on the a posteriori probabilities of temporally neighboring notes. Experimental results show improved recognition rates for trio and duo music."
Peter Knees;Markus Schedl;Gerhard Widmer,Multiple Lyrics Alignment: Automatic Retrieval of Song Lyrics.,2005,https://doi.org/10.5281/zenodo.1415140,"Peter Knees, Department of Computational Perception, Johannes Kepler University Linz;Markus Schedl, Department of Computational Perception, Johannes Kepler University Linz;Gerhard Widmer, Department of Computational Perception, Johannes Kepler University Linz","We present an approach to automatically retrieve and extract lyrics of arbitrary songs from the Internet. It is intended to provide easy and convenient access to lyrics for users, as well as a basis for further research based on lyrics, e.g. semantic analysis. Due to the fact that many lyrics found on the web suffer from individual errors like typos, we make use of multiple versions from different sources to eliminate mistakes. This is accomplished by Multiple Sequence Alignment. The different sites are aligned and examined for matching sequences of words, finding those parts on the pages that are likely to contain the lyrics. This provides a means to find the most probable version of lyrics, i.e. a version with highest consensus among different sources."
Catherine Lai;Beinan Li;Ichiro Fujinaga,Preservation Digitization of David Edelberg's Handel LP Collection: A Pilot Project.,2005,https://doi.org/10.5281/zenodo.1416616,"Catherine Lai, Music Technology, Faculty of Music, McGill University;Beinan Li, Music Technology, Faculty of Music, McGill University;Ichiro Fujinaga, Music Technology, Faculty of Music, McGill University",This paper describes the digitization process for building an online collection of LPs and the procedure for creating the ground-truth data essential for developing an automated metadata and content capturing system.
Aristomenis S. Lampropoulos;Paraskevi S. Lampropoulou;George A. Tsihrintzis,Musical Genre Classification Enhanced by Improved Source Separation Technique.,2005,https://doi.org/10.5281/zenodo.1416292,"Aristomenis S. Lampropoulos, University of Piraeus;Paraskevi S. Lampropoulou, University of Piraeus;George A. Tsihrintzis, University of Piraeus","We present a system for musical genre classification based on audio features extracted from signals which correspond to distinct musical instrument sources. For the separation of the musical sources, we propose an innovative technique in which the convolutive sparse coding algorithm is applied to several portions of the audio signal. The system is evaluated and its performance is assessed."
Wei Liang;Shuwu Zhang;Bo Xu 0002,A Hierarchical Approach for Audio Stream Segmentation and Classification.,2005,https://doi.org/10.5281/zenodo.1415166,"Wei Liang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Shuwu Zhang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Bo Xu, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","This paper describes a hierarchical approach for fast audio stream segmentation and classification. With this approach, the audio stream is firstly segmented into audio clips by MBCR (Multiple sub-Bands spectrum Centroid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips into six pre-defined categories in terms of discriminative background sounds, which is pure speech, pure music, song, speech with music, speech with noise and silence. The experiments on real TV program recordings showed that this approach has higher accuracy and recall rate for audio classification with a fast speed under noise environments."
Wei Liang;Shuwu Zhang;Bo Xu 0002,A Histogram Algorithm for Fast Audio Retrieval.,2005,https://doi.org/10.5281/zenodo.1415812,"Wei Liang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Shuwu Zhang, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China;Bo Xu, Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China",This paper describes a fast audio detection method for specific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio features based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detection in TV program showed that the algorithm can achieve a very high overall precision and recall rate both about 97% with very fast search time about 1/40 on real time.
Dominik Lübbers,SoniXplorer: Combining Visualization and Auralization for Content-Based Exploration of Music Collections.,2005,https://doi.org/10.5281/zenodo.1418021,"Dominik L¨ubbers, RWTH Aachen University","Music can be described best by music. However, current research in the design of user interfaces for the exploration of music collections has mainly focused on visualization aspects ignoring possible benefits from spatialized music playback. We describe our first development steps towards two novel user-interface designs: The Sonic Radar arranges a fixed number of prototypes resulting from a content-based clustering process in a circle around the user’s standpoint. To derive an auralization of the scene, we introduce the concept of an aural focus of perception that adapts well-known principles from the visual domain. The Sonic SOM is based on Kohonen’s Self-Organizing Map. It helps the user in understanding the structure of his music collection by positioning titles on a two-dimensional grid according to their high-dimensional similarity. We show how our auralization concept can be adapted to extend this visualization technique and thereby support multimodal navigation."
Michael I. Mandel;Dan Ellis,Song-Level Features and Support Vector Machines for Music Classification.,2005,https://doi.org/10.5281/zenodo.1415024,"Michael I. Mandel and Daniel P.W. Ellis, LabROSA, Dept. of Elec. Eng., Columbia University, NY NY USA","Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplar-based classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums."
Daniel McEnnis;Cory McKay;Ichiro Fujinaga;Philippe Depalle,jAudio: An Feature Extraction Library.,2005,https://doi.org/10.5281/zenodo.1416648,"Daniel McEnnis, Faculty of Music, McGill University;Cory McKay, Faculty of Music, McGill University;Ichiro Fujinaga, Faculty of Music, McGill University;Philippe Depalle, Faculty of Music, McGill University","jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straightforward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio files as input. In the GUI, users select the features that they wish to have extracted—letting jAudio take care of all dependency problems—and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML file or an ARFF file depending on the user’s preference."
Anders Meng;John Shawe-Taylor,An Investigation of Feature Models for Music Genre Classification Using the Support Vector Classifier.,2005,https://doi.org/10.5281/zenodo.1416052,"Anders Meng, Informatics and Mathematical Modelling - B321, Technical University of Denmark;John Shawe-Taylor, University of Southampton","In music genre classification, most automatic systems focus on short time features derived from 10-50ms, even though the decision time for genre classification is typically several seconds. This study investigates two models, the multivariate Gaussian model and the multivariate autoregressive model, for modeling short time features. The study also explores how these models can be integrated over a segment of short time features into a kernel for application in a support vector machine. Two kernels with this property, the convolution kernel and product probability kernel, are considered. The accuracy of the best performing model on an 11 genre music setup was approximately 44%, compared to a human performance of approximately 52% on the same dataset."
Annamaria Mesaros;Jaakko Astola,The Mel-Frequency Cepstral Coefficients in the Context of Singer Identification.,2005,https://doi.org/10.5281/zenodo.1417539,"Annamaria Mesaros, Technical University of Cluj Napoca;Jaakko Astola, Institute of Signal Processing, Tampere University of Technology","The singing voice is the oldest and most complex musical instrument. A familiar singer’s voice is easily recognizable for humans, even when hearing a song for the first time. On the other hand, for automatic identification this is a difficult task among sound source identification applications. The signal processing techniques aim to extract features that are related to identity characteristics. The research presented in this paper considers 32 Mel-Frequency Cepstral Coefficients in two subsets: the low order MFCCs characterizing the vocal tract resonances and the high order MFCCs related to the glottal wave shape. We explore possibilities to identify and discriminate singers using the two sets. Based on the results we can affirm that both subsets have their contribution in defining the identity of the voice, but the high order subset is more robust to changes in singing style."
J. Enrique Muñoz Expósito;Sebastian García Galán;Nicolás Ruiz-Reyes;Pedro Vera-Candeas;F. Rivas-Peña,Speech/Music Discrimination Using a Single Warped LPC-Based Feature.,2005,https://doi.org/10.5281/zenodo.1417711,"J.E. Mu˜noz-Exp´osito, Electronics and Telecommunication Engineering Department, University of Ja´en;S. Garcia-Gal´an, Electronics and Telecommunication Engineering Department, University of Ja´en;N. Ruiz-Reyes, Electronics and Telecommunication Engineering Department, University of Ja´en;P. Vera-Candeas, Electronics and Telecommunication Engineering Department, University of Ja´en;F. Rivas-Pe˜na, Electronics and Telecommunication Engineering Department, University of Ja´en","Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in Tzanetakis and Cook (2002) is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for real-time multimedia applications."
Robert Neumayer;Michael Dittenbach;Andreas Rauber,"PlaySOM and PocketSOMPlayer, Alternative Interfaces to Large Music Collections.",2005,https://doi.org/10.5281/zenodo.1414818,"Robert Neumayer, Vienna University of Technology;Michael Dittenbach, eCommerce Competence Center;Andreas Rauber, Vienna University of Technology","With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. In this paper we present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information."
Giovanna Neve;Nicola Orio,Experiments on Segmentation Techniques for Music Documents Indexing.,2005,https://doi.org/10.5281/zenodo.1416996,"Nicola Orio, Department of Information Engineering;Giovanna Neve, Department of Information Engineering","This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units, which can be used as content descriptors of music documents. Four approaches have been implemented and compared on a test collection of real documents and queries, showing their impact on index term size and on retrieval effectiveness. From the results, simple but extensive approaches seem to give better performances than more sophisticated segmentation algorithms."
Elias Pampalk;Arthur Flexer;Gerhard Widmer,Improvements of Audio-Based Music Similarity and Genre Classificaton.,2005,https://doi.org/10.5281/zenodo.1418083,"Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Arthur Flexer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Gerhard Widmer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI)","Audio-based music similarity measures can be applied to automatically generate playlists or recommendations. In this paper, spectral similarity is combined with complementary information from fluctuation patterns, including two new descriptors derived thereof. The performance is evaluated in a series of experiments on four music collections, based on genre classification. The main findings are that improvements are substantial on two of the four collections, but simple audio statistics have limitations. Evaluating similarity through genre classification is biased by the music collection used, and in cross-validation, no pieces from the same artist should be in both the training and test set."
Elias Pampalk;Tim Pohle;Gerhard Widmer,Dynamic Playlist Generation Based on Skipping Behavior.,2005,https://doi.org/10.5281/zenodo.1414932,"Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Tim Pohle, Austrian Research Institute for Artiﬁcial Intelligence (OFAI);Gerhard Widmer, Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Department of Computational Perception, Johannes Kepler University","Common approaches to creating playlists are to randomly shuffle a collection (e.g. iPod shuffle) or manually select songs. In this paper, we present and evaluate heuristics to adapt playlists automatically given a song to start with (seed song) and immediate user feedback. Instead of rich metadata, we use audio-based similarity. The user gives feedback by pressing a skip button if the user dislikes the current song. Songs similar to skipped songs are removed, while songs similar to accepted ones are added to the playlist. We evaluate the heuristics with hypothetical use cases. For each use case, we assume a specific user behavior (e.g. the user always skips songs by a particular artist). Our results show that using audio similarity and simple heuristics, it is possible to drastically reduce the number of necessary skips."
Steffen Pauws;Sander van de Wijdeven,User Evaluation of a New Interactive Playlist Generation Concept.,2005,https://doi.org/10.5281/zenodo.1415180,"Steffen Pauws, Philips Research;Sander van de Wijdeven, Philips Research","Selecting the ‘right’ songs and putting them in the ‘right’ order are key to a great music listening or dance experience. ‘SatisFly’ is an interactive playlist generation system in which the user can tell what kind of songs should be contained in what order in the playlist, while she navigates through the music collection. The system uses constraint satisfaction to generate a playlist that meets all user wishes. In a user evaluation, it was found that users created high-quality playlists in a swift way and with little effort using the system, while still having complete control on their music choices. The novel interactive way of creating a playlist, while browsing through the music collection, was highly appreciated. Ease of navigation through a music collection is still an issue that needs further attention."
Geoffroy Peeters,Rhythm Classification Using Spectral Rhythm Patterns.,2005,https://doi.org/10.5281/zenodo.1417495,"Geoffroy Peeters, IRCAM - Sound Analysis/Synthesis Team","In this paper, we study the use of spectral patterns to represent the characteristics of the rhythm of an audio signal. Three feature sets derived from the amplitude of the Discrete Fourier Transform, the Auto-Correlation Function, and the product of the DFT and a Frequency-Mapped ACF are evaluated for their ability to represent the rhythm characteristics of an audio item through a classification task. We show that using such simple spectral representations allows obtaining results comparable to the state of the art."
Jeremy Pickens,Classifier Combination for Capturing Musical Variation.,2005,https://doi.org/10.5281/zenodo.1418219,"Jeremy Pickens, Department of Computer Science, King’s College London","At its heart, music information retrieval is characterized by the need to find the similarity between pieces of music. However, “similar” does not mean “the same”. Therefore, techniques for approximate matching are crucial to the development of good music information retrieval systems. Yet as one increases the level of approximation, one finds not only additional similar, relevant music, but also a larger number of not-as-similar, non-relevant music. The purpose of this work is to show that if two different retrieval systems do approximate matching in different manners, and both give decent results, they can be combined to give results better than either system individually. One need not sacrifice accuracy for the sake of flexibility."
Aggelos Pikrakis;Sergios Theodoridis,A Novel HMM Approach to Melody Spotting in Raw Audio Recordings.,2005,https://doi.org/10.5281/zenodo.1414886,"Aggelos Pikrakis, University of Athens;Sergios Theodoridis, University of Athens","This paper presents a melody spotting system based on Variable Duration Hidden Markov Models (VDHMM’s), capable of locating monophonic melodies in a database of raw audio recordings. The system treats the melody as a sequence of note durations and music intervals and constructs a VDHMM based on this pattern prototype. The probabilities of the VDHMM are determined according to rules that account for note duration flexibility and structural deviations from the prototype pattern. The system has been successfully tested with cello recordings in Western Classical music and Greek traditional multi-instrument recordings."
Christopher Raphael,A Graphical Model for Recognizing Sung Melodies.,2005,https://doi.org/10.5281/zenodo.1417052,"Christopher Raphael, School of Informatics, Indiana Univ.","A method is presented for automatic transcription of sung melodic fragments to score-like representation, including metric values and pitch. A joint model for pitch, rhythm, segmentation, and tempo is defined for a sung fragment. We then discuss the identification of the globally optimal musical transcription, given the observed audio data. A post process estimates the location of the tonic, so the transcription can be presented into the key of C. Experimental results are presented for a small test collection."
Craig Stuart Sapp,Online Database of Scores in the Humdrum File Format.,2005,https://doi.org/10.5281/zenodo.1417281,"Craig Stuart Sapp, Stanford University","KernScores, an online library of musical data currently consisting of over 5 million notes, has been created to assist projects dealing with the computational analysis of musical scores. The online scores are in a format suitable for processing with the Humdrum Toolkit for Music Research, but the website also provides automatic translations into several other popular data formats for digital musical scores."
Nicolas Scaringella;Giorgio Zoia,On the Modeling of Time Information for Automatic Genre Recognition Systems in Audio Signals.,2005,https://doi.org/10.5281/zenodo.1416064,"Nicolas Scaringella, Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne, EPFL, Lausanne, CH-1015 Switzerland;Giorgio Zoia, Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne, EPFL, Lausanne, CH-1015 Switzerland","The creation of huge databases coming from both restoration of existing analogue archives and new content is demanding fast and more and more reliable tools for content analysis and description, to be used for searches, content queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music catalogues, libraries and shops. Despite their use musical genres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the same pattern recognition architecture: extracting features from chunks of audio signal and classifying features independently. In this paper, we focus instead on the low-level temporal relationships between chunks when classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music segments to consolidate classification consistency by reducing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classification results are reported for a database of 1400 songs evenly distributed over 7 genres."
Elliot Sinyor;Cory McKay;Rebecca Fiebrink;Daniel McEnnis;Ichiro Fujinaga,Beatbox Classification Using ACE.,2005,https://doi.org/10.5281/zenodo.1414920,"Elliot Sinyor, McGill University;Cory McKay, McGill University;Rebecca Fiebrink, McGill University;Daniel McEnnis, McGill University;Ichiro Fujinaga, McGill University","This paper describes the use of the Autonomous Classification Engine (ACE) to classify beatboxing (vocal percussion) sounds. A set of unvoiced percussion sounds belonging to five classes (bass drum, open hihat, closed hihat and two types of snare drum) were recorded and manually segmented. ACE was used to compare various classification techniques, both with and without feature selection. The best result was 95.55% accuracy using AdaBoost with C4.5 decision trees."
Robert Young Walser,Herding Folksongs.,2005,https://doi.org/10.5281/zenodo.1415640,"Robert Young Walser, The James Madison Carpenter Project, Elphinstone Institute, University of Aberdeen","Cataloging a large, multi-media collection of traditional song and drama in preparation for online presentation highlights issues of song identity and access in the context of contemporary digitized archives. In the James Madison Carpenter collection a particular folksong sung by a particular individual may exist in multiple manifestations: typed song text, sound recording(s), and/or manuscript music notation. While controlled vocabulary references such as Child and Roud numbers provide a degree of identification, such narrative- and text-centric tools are only partly effective in differentiating folkloric materials. Additional means are needed for identifying and controlling folk materials which are distinguished by other aspects of the song such as melody or non-narrative text. The Carpenter project team’s experience with Encoded Archival Description (EAD) illustrates the value of this platform-independent, widely recognized standard and suggests opportunities for further developments particularly suited to locating and retrieving folk music materials."
Kris West;Stephen Cox,Finding An Optimal Segmentation for Audio Genre Classification.,2005,https://doi.org/10.5281/zenodo.1416746,"Kris West, School of Computing Sciences, University of East Anglia;Stephen Cox, School of Computing Sciences, University of East Anglia","In the automatic classification of music, different segmentations of the audio signal have been used to calculate features. This study evaluates these different segmentations and introduces the use of an onset detection based segmentation, which outperforms other segmentation schemes in terms of classification accuracy and model size."
Tillman Weyde;Christian Datzko,Efficient Melody Retrieval with Motif Contour Classes.,2005,https://doi.org/10.5281/zenodo.1414738,"Tillman Weyde, City University;Christian Datzko, University of Osnabrück","This paper describes the use of motif contour classes for efficient retrieval of melodies from music collections. Instead of extracting incipits or themes, complete monophonic pieces are indexed for their motifs, using classes of motif contours. Similarity relations between these classes can be used for a very efficient search. This can serve as a first level search, which can be refined by using more computationally intensive comparisons on its results. The model introduced has been implemented and tested using the MUSITECH framework. We present empirical and analytical results on the retrieval quality, the complexity, and quality/efficiency trade-off."
Wen Xue;Mark Sandler,A Partial Searching Algorithm and Its Application for Polyphonic Music Transcription.,2005,https://doi.org/10.5281/zenodo.1415740,"Xue Wen, Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London;Mark Sandler, Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London","This paper proposes an algorithm for studying spectral contents of pitched sounds in real-world recordings. We assume that the 2nd-order difference, w.r.t. partial index, of a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. Given a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be associated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the algorithm to make it work with reverberant background, such as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level."
Yi Yu 0001;Chiemi Watanabe;Kazuki Joe,Towards a Fast and Efficient Match Algorithm for Content-Based Music Retrieval on Acoustic Data.,2005,https://doi.org/10.5281/zenodo.1415874,"Yi YU, Graduate school of Humanity and Science, Nara Women’s University;Chiemi WATANABE, Graduate school of Humanity and Science, Nara Women’s University;Kazuki JOE, Graduate school of Humanity and Science, Nara Women’s University","In this paper we present a fast and efficient match algorithm, which consists of two key techniques: Spectral Correlation Based Feature Merge(SCBFM) and Two-Step Retrieval(TSR). SCBFM can remove the redundant information. In consequence, the resulting feature sequence has a smaller size, requiring less storage and computation. In addition, most of the tempo variation is removed; thus a much simpler sequence match method can be adopted. Also, TSR relies on the characteristics of Mel-Frequency Cepstral Coefficient(MFCC), where the precise match in the second step depends on the first step to filter out most of the dissimilar references with only the low order MFCC feature. As a result, the whole retrieval speed can be further improved. The experimental evaluation verifies that SCBFM-TSR yields more meaningful results in comparatively short time. The experiment results are analyzed with a theoretical approach that seeks to find the relation between Spectral Correlation(SC) threshold and storage, computation."
Samer M. Abdallah;Mark D. Plumbley,Polyphonic transcription by non-negative sparse coding of power spectra.,2004,https://doi.org/10.5281/zenodo.1415072,"Samer A. Abdallah, Centre for Digital Music, Queen Mary, University of London;Mark D. Plumbley, Centre for Digital Music, Queen Mary, University of London","We present a system for adaptive spectral basis decomposition that learns to identify independent spectral features given a sequence of short-term Fourier spectra. When applied to recordings of polyphonic piano music, the individual notes are identified as salient features, and hence each short-term spectrum is decomposed into a sum of note spectra; the resulting encoding can be used as a basis for polyphonic transcription. The system is based on a probabilistic model equivalent to a form of noisy independent component analysis (ICA) or sparse coding with non-negativity constraints. We introduce a novel modification to this model that recognizes that a short-term Fourier spectrum can be thought of as a noisy realization of the power spectral density of an underlying Gaussian process, where the noise is essentially multiplicative and non-Gaussian. Results are presented for an analysis of a live recording of polyphonic piano music."
Norman H. Adams;Mark A. Bartsch;Jonah Shifrin;Gregory H. Wakefield,Time Series Alignment for Music Information Retrieval.,2004,https://doi.org/10.5281/zenodo.1415694,"Norman H. Adams, EECS Dept., University of Michigan;Mark A. Bartsch, EECS Dept., University of Michigan;Jonah B. Shifrin, EECS Dept., University of Michigan;Gregory H. Wakefield, EECS Dept., University of Michigan","Time series representations are common in MIR applications such as query-by-humming, where a sung query might be represented by a series of 'notes' for database retrieval. While such a transcription into a sequence of (pitch, duration) pairs is convenient and musically intuitive, there is no evidence that it is an optimal representation. The present work explores three time series representations for sung queries: a sequence of notes, a 'smooth' pitch contour, and a novel sequence of pitch histograms. Dynamic alignment procedures are described for the three representations. Multiple continuity constraints are explored and a modified dynamic alignment procedure is described for the histogram representation. We measure the performance of the three representations using a collection of naturally sung queries applied to a target database of varying size. The results show that the note representation lends itself to rapid retrieval whereas the contour representation lends itself to robust performance. The histogram representation yields performance nearly as robust as the contour representation, but with computational complexity similar to the note representation."
Philippe Aigrain,Whose future is it?,2004,https://doi.org/10.5281/zenodo.1416404,"Philippe Aigrain, Sopinspace, Society for Public Information Spaces",""""""
Miguel A. Alonso;Gaël Richard;Bertrand David,Tempo And Beat Estimation Of Musical Signals.,2004,https://doi.org/10.5281/zenodo.1415784,"Miguel Alonso, ENST-GET;Bertrand David, ENST-GET;Ga¨el Richard, ENST-GET","Tempo estimation is fundamental in automatic music processing and in many multimedia applications. This paper presents an automatic tempo tracking system that processes audio recordings and determines the beats per minute and temporal beat location. The concept of spectral energy flux is defined and leads to an efficient note onset detector. The algorithm involves three stages: a frontend analysis that efficiently extracts onsets, a periodicity detection block, and the temporal estimation of beat locations. The performance of the proposed method is evaluated using a large database of 489 excerpts from several musical genres. The global recognition rate is 89.7%. Results are discussed and compared to other tempo estimation systems."
Jean-Julien Aucouturier;François Pachet,Tools and Architecture for the Evaluation of Similarity Measures : Case Study of Timbre Similarity.,2004,https://doi.org/10.5281/zenodo.1416562,"Jean-Julien Aucouturier, SONY CSL Paris;Francois Pachet, SONY CSL Paris","The systematic testing of the very many parameters and algorithmic variants involved in the design of high-level music descriptors at large, and similarity measure in particular, is a daunting task, which requires the building of a general architecture which is nearly as complex as a full-fledge Music Browsing system. In this paper, we report on experiments done in an attempt to improve the performance of the music similarity measure described in [2], using the Cuidado Music Browser ([8]). We do not principally report on the actual results of the evaluation, but rather on the methodology and the various tools that were built to support such a task. We show that many non-technical browsing features are useful at various stages of the evaluation process, and in turn that some of the tools developed for the expert user can be reinjected into the Music Browser, and benefit the non-technical user."
Jean-Julien Aucouturier;François Pachet;Peter Hanappe,From Sound Sampling To Song Sampling.,2004,https://doi.org/10.5281/zenodo.1416028,"Jean-Julien Aucouturier, SONY CSL Paris;Francois Pachet, SONY CSL Paris;Peter Hanappe, SONY CSL Paris","This paper proposes to use the techniques of Music Information Retrieval in the context of Music Interaction. We describe a system, the SongSampler, inspired by the technology of audio sampling, which automatically samples a song to produce an instrument (typically using a MIDI keyboard) that plays sounds found in the original audio file. Playing with such an instrument creates an original situation in which listeners play their own music with the sounds of their favourite tunes, in a constant interaction with a music database. The paper describes the main technical issues at stake concerning the integration of music information retrieval in an interactive instrument, and reports on preliminary experiments."
David Bainbridge 0001;Sally Jo Cunningham;J. Stephen Downie,GREENSTONE as a Music Digital Library Toolkit.,2004,https://doi.org/10.5281/zenodo.1417573,"David Bainbridge, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand;Sally Jo Cunningham, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, IL, USA","Greenstone is an open source digital library system that has been used in over 60 countries for humanitarian aid and research purposes. This article summarizes Greenstone's uses with music documents, including incorporating musical formats into the system and providing search and browsing capabilities. The article also discusses the use of plugins for importing music documents, such as symbolic notation, raw audio, and sheet music. Additionally, it mentions the use of MELDEX for extending the searching paradigm and the use of collaging as a visualization technique."
David Bainbridge 0001;Sally Jo Cunningham;J. Stephen Downie,Visual Collaging Of Music In A Digital Library.,2004,https://doi.org/10.5281/zenodo.1415832,"David Bainbridge, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand;Sally Jo Cunningham, Dept. of Computer Science, University of Waikato, Hamilton, New Zealand;J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, IL, USA","This article explores the role visual browsing can play within a digital music library. The context to the work is provided through a review of related techniques drawn from the fields of digital libraries and human computer interaction. Implemented within the open source digital library toolkit Greenstone, a prototype system is described that combines images located through textual metadata with a visualization technique known as collaging to provide a leisurely, undirected interaction with a music collection. Emphasis in the article is given to the augmentations of the basic technique to work in the musical domain."
Roberto Basili 0001;Alfredo Serafini;Armando Stellato,Classification of musical genre: a machine learning approach.,2004,https://doi.org/10.5281/zenodo.1416754,"Roberto Basili, University of Rome Tor Vergata;Alfredo Seraﬁni, University of Rome Tor Vergata;Armando Stellato, University of Rome Tor Vergata","In this paper, the authors investigate the impact of machine learning algorithms on the development of automatic music classification models for capturing genre distinctions. They create a collection of examples for widely recognized genres and evaluate the performances of different learning algorithms using features derived from MIDI transcriptions. The study focuses on symbolic musical aspects to obtain as much information as possible about dynamically changing genres without noise from irrelevant audio content. The authors also mention ongoing work on investigating audio properties for genre classification."
Stephan Baumann 0001;Tim Pohle;Shankar Vembu,Towards a Socio-cultural Compatibility of MIR Systems.,2004,https://doi.org/10.5281/zenodo.1417733,"Stephan Baumann, German Research Center for Artificial Intelligence;Tim Pohle, German Research Center for Artificial Intelligence;Vembu Shankar, Technical University of Hamburg","Future MIR systems will be of great use and pleasure for potential users. If researchers have a clear picture about their “customers” in mind they can aim at building and evaluating their systems exactly inside the different socio-cultural environments of such music listeners. Since music is in most cases embedded into a socio-cultural process we propose especially to evaluate MIR applications outside the lab during daily activities. For this purpose we designed a mobile music recommendation system relying on a trimodal music similarity metric, which allows for subjective on-the-fly adjustments of recommendations. It offers online access to large-scale metadata repositories as well as an audio database containing 1000 songs. We did first small-scale evaluations of this approach and came to interesting results regarding the perception of song similarity concerning the relations between sound, cultural issues and lyrics. Our paper will also give insights to the three different underlying approaches for song similarity computation (sound, cultural issues, lyrics), focusing in detail on a novel clustering of album reviews as found at online music retailers."
Paul Brossier;Juan Pablo Bello;Mark D. Plumbley,Fast labelling of notes in music signals.,2004,https://doi.org/10.5281/zenodo.1416132,"Paul M. Brossier, Queen Mary College, University of London, Centre for Digital Music;Juan P. Bello, Queen Mary College, University of London, Centre for Digital Music;Mark D. Plumbley, Queen Mary College, University of London, Centre for Digital Music","We present a new system for the estimation of note attributes from a live monophonic music source, within a short time delay and without any previous knowledge of the signal. The labelling is based on the temporal segmentation and the successive estimation of the fundamental frequency of the current note object. The setup, implemented around a small C library, is directed at the robust note segmentation of a variety of audio signals. A system for evaluation of performances is also presented. The further extension to polyphonic signals is considered, as well as design concerns such as portability and integration in other software environments."
Pedro Cano;Markus Koppenberger,The emergence of complex network patterns in music networks.,2004,https://doi.org/10.5281/zenodo.1417663,"Pedro Cano and Markus Koppenberger, Music Technology Group, Institut de l’Audiovisual, Universitat Pompeu Fabra","Viewing biological, social or technological systems as networks formed by nodes and connections between them can help better understand them. We study the topology of several music networks, namely citation in allmusic.com and co-occurrence of artists in playlists. The analysis uncovers the emergence of complex network phenomena in music information networks built considering artists as nodes and its relations as links. The properties provide some hints on searchability and possible optimizations in the design of music recommendation systems. It may also provide a deeper understanding on the similarity measures that can be derived from existing music knowledge sources."
Michael A. Casey;Tim Crawford,Automatic Location And Measurement Of Score-based Gestures In Audio Recordings.,2004,https://doi.org/10.5281/zenodo.1415184,"Michael Casey, Centre for Cognition, Computation and Culture, Goldsmiths College, University of London;Tim Crawford, Centre for Cognition, Computation and Culture, Goldsmiths College, University of London","This paper reports on our ﬁrst experiments in using the feature extraction tools of the MPEG-7 international standard for multimedia content description on a novel problem, the automatic identiﬁcation and analysis of score-based performance features in audio recordings of music. Our test material consists of recordings of two pieces of 17th- and 18th-century lute music in which our aim is to recognise and isolate performance features such as trills and chord-spreadings. Using the audio tools from the MPEG-7 standard facilitates interoperability and allows us to share both score and audio metadata. As well as using low-level audio MIR techniques within this MPEG-7 context, the work has potential importance as an ’ornamentation ﬁlter’ for MIR systems. It may also form a useful component in methods for instrumental performer identiﬁcation."
Òscar Celma,Architecture for an MPEG-7 Web Browser.,2004,https://doi.org/10.5281/zenodo.1417229,"Oscar Celma, Music Technology Group, Universitat Pompeu Fabra",The MPEG-7 standard provides description mechanisms and taxonomy management for multimedia documents. There are several approaches to design a multimedia database system using MPEG-7 descriptors. We discuss two of them: relational databases and native XML databases. We have implemented a search and retrieval application for MPEG-7 descriptions based on the latter.
Fernando William Cruz;Edilson Ferneda;Márcio da Costa P. Brandão;Evandro de Barros Costa;Hyggo Oliveira de Almeida;Murilo Bastos da Cunha;Rafael de Sousa;João Denicol;Carlos da Silva,A Brazilian Popular Music Oriented Digital Library For Musical Harmony E-Learning.,2004,https://doi.org/10.5281/zenodo.1417042,"Fernando W. Cruz, Catholic University of Brasília;Edilson Ferneda, Catholic University of Brasília;Márcio Brandão, University of Brasília;Evandro de B. Costa, Federal University of Alagoas;Hyggo O. de Almeida, Federal University of Campina Grande;Murilo B. da Cunha, University of Brasília;Rafael T. de Sousa Jr., University of Brasília;João Ricardo E. Denicol, Catholic University of Brasília;Carlos Alan P. da Silva, Federal University of Campina Grande","This poster presents a digital library proposal conceived for people interested in acquiring knowledge about Brazilian popular music harmony, particularly in Choro. This Brazilian musical style is a complex popular music form based on improvisation, although it contains classical music elements such as the counterpoint. We are proposing two ways of accessing the music virtual library content: a guided navigation mode, in which users interact with a cooperative Web-based learning system; and a free navigation mode, in which users can make their own queries, both through browsers or client applications."
Roger B. Dannenberg;Ning Hu,Understanding Search Performance in Query-by-Humming Systems.,2004,https://doi.org/10.5281/zenodo.1416900,"Roger B. Dannenberg, School of Computer Science, Carnegie Mellon University;Ning Hu, School of Computer Science, Carnegie Mellon University","Previous work in Query-by-Humming systems has left open many questions. Although a variety of techniques have been explored, there has been relatively little work to compare them under controlled conditions, especially with “real” audio queries from human subjects. Previous work comparing note-interval matching, melodic contour matching, and HMM-based matching is extended with comparisons to the Phillips CubyHum algorithm and various n-gram search algorithms. We also explore the sensitivity of note-interval dynamic programming searches to different parameters and consider two-stage searches combining a fast n-gram search with a more precise but slower dynamic programming algorithm."
Laurent Daudet;Gaël Richard;Pierre Leveau,Methodology and Tools for the evaluation of automatic onset detection algorithms in music.,2004,https://doi.org/10.5281/zenodo.1417247,"Pierre LEVEAU, Laboratoire d’Acoustique Musicale;Laurent DAUDET, Laboratoire d’Acoustique Musicale;Ga¨el RICHARD, GET - ENST (T´el´ecom Paris)","This paper addresses the problem of the performance evaluation of algorithms for the automatic detection of note onsets in music signals. Our experiments show that creating a database of reference files with reliable human-annotated onset times is a complex task, since its subjective part cannot be neglected. This work provides a methodology to construct such a database. With the use of a carefully designed software tool, called SOL (Sound Onset Labellizer), we can obtain a set of reference onset times that are cross-validated amongst different expert listeners. We show that the mean error of annotated times across test subjects is very much signal-dependent. This value can be used, when evaluating automatic labelling, as an indication of the relevant tolerance window. The SOL annotation software is to be released freely for research purposes. Our test library, 17 short sequences containing about 750 onsets, comes from copyright-free music or from the public RWC database. The corresponding validated onset labels are also freely distributed, and are intended to form the starting point for the definition of a reliable benchmark."
Matthew E. P. Davies;Mark D. Plumbley,Causal Tempo Tracking of Audio.,2004,https://doi.org/10.5281/zenodo.1417873,"Matthew E. P. Davies, Centre for Digital Music, Queen Mary University of London;Mark D. Plumbley, Centre for Digital Music, Queen Mary University of London","We introduce a causal approach to tempo tracking for musical audio signals. Our system is designed towards an eventual real-time implementation; requiring minimal high-level knowledge of the musical audio. The tempo tracking system is divided into two sections: an onset analysis stage, used to derive a rhythmically meaningful representation from the input audio, followed by a beat matching algorithm using auto- and cross-correlative methods to generate short term predictions of future beats in the audio. The algorithm is evaluated over a range of musical styles by comparing the predicted output to beats tapped by a musician. An investigation is also presented into three rhythmically complex beat tracking problems, where the tempo is not constant. Preliminary results demonstrate good accuracy for this type of system."
Simon Dixon;Fabien Gouyon;Gerhard Widmer,Towards Characterisation of Music via Rhythmic Patterns.,2004,https://doi.org/10.5281/zenodo.1416220,"Simon Dixon, Austrian Research Institute for AI;Fabien Gouyon, Universitat Pompeu Fabra;Gerhard Widmer, Medical University of Vienna Medical Cybernetics and AI","A central problem in music information retrieval is finding suitable representations which enable efficient and accurate computation of musical similarity and identity. Low-level audio features are ideal for calculating identity, but are of limited use for similarity measures, as many aspects of music can only be captured by considering high-level features. We present a new method of characterizing music by typical bar-length rhythmic patterns which are automatically extracted from the audio signal, and demonstrate the usefulness of this representation by its application in a genre classification task. Recent work has shown the importance of tempo and periodicity features for genre recognition, and we extend this research by employing the extracted temporal patterns as features. Standard classification algorithms are utilized to discriminate 8 classes of Standard and Latin ballroom dance music (698 pieces). Although pattern extraction is error-prone, and patterns are not always unique to a genre, classification by rhythmic pattern alone achieves up to 50% correctness (baseline 16%), and by combining with other features, a classification rate of 96% is obtained."
Shyamala Doraisamy;Stefan M. Rüger,A Polyphonic Music Retrieval System Using N-Grams.,2004,https://doi.org/10.5281/zenodo.1417961,"Shyamala Doraisamy, University Putra Malaysia;Stefan R¨uger, Imperial College London","This paper describes the development of a polyphonic music retrieval system with the n-gram approach. Musical n-grams are constructed from polyphonic musical performances in MIDI using the pitch and rhythm dimensions of music. These are encoded using text characters enabling the musical words generated to be indexed with existing text search engines. The Lemur Toolkit was adapted for the development of a demonstrator system on a collection of around 10,000 polyphonic MIDI performances. The indexing, search and retrieval with musical n-grams and this toolkit have been extensively evaluated through a series of experimental work over the past three years, published elsewhere. We discuss how the system works internally and describe our proposal for enhancements to Lemur towards the indexing of ‘overlaying’ as opposed to indexing a ‘bag of terms’. This includes enhancements to the parser for a ‘polyphonic musical word indexer’ to incorporate within document position information when indexing adjacent and concurrent musical words. For retrieval of these ‘overlaying’ musical words, a new proximity-based operator and a ranking function is proposed."
Michael Droettboom;Ichiro Fujinaga,Micro-level groundtruthing environment for OMR.,2004,https://doi.org/10.5281/zenodo.1417217,"Michael Droettboom, Digital Knowledge Center, Sheridan Libraries, The Johns Hopkins University;Ichiro Fujinaga, CIRMMT, Faculty of Music, McGill University","A simple framework for evaluating OMR at the symbol level is presented. While a true evaluation of an OMR system requires a high-level analysis, the automation of which is a largely unsolved problem, many high-level errors are correlated to these more tractably-analyzed lower-level errors."
Tuomas Eerola;Petri Toiviainen,MIR In Matlab: The MIDI Toolbox.,2004,https://doi.org/10.5281/zenodo.1416234,"Tuomas Eerola, Department of Music, University of Jyväskylä, Finland;Petri Toiviainen, Department of Music, University of Jyväskylä, Finland","The MIDI Toolbox is a compilation of functions for analyzing and visualizing MIDI files in the Matlab computing environment. In this article, the basic issues of the Toolbox are summarized and demonstrated with examples ranging from melodic contour, similarity, key-finding, meter-finding to segmentation. The Toolbox is based on symbolic musical data but signal processing methods are applied to cover such aspects of musical behaviour as geometric representations and short-term memory. Besides simple manipulation and filtering functions, the toolbox contains cognitively inspired analytic techniques that are suitable for context-dependent musical analysis, a prerequisite for many music information retrieval applications."
Jana Eggink;Guy J. Brown,Extracting Melody Lines From Complex Audio.,2004,https://doi.org/10.5281/zenodo.1418003,"Jana Eggink, Department of Computer Science, University of Sheffield, UK;Guy J. Brown, Department of Computer Science, University of Sheffield, UK","We propose a system which extracts the melody line played by a solo instrument from complex audio. At every time frame multiple fundamental frequency (F0) hypotheses are generated, and later processing uses various knowledge sources to choose the most likely succession of F0s. Knowledge sources include an instrument recognition module and temporal knowledge about tone durations and interval transitions, which are integrated in a probabilistic search. The proposed system improved the number of frames with correct F0 estimates by 14% compared to a baseline system which simply uses the strongest F0 at every point in time. The number of spurious tones was reduced to nearly a third compared to the baseline system, resulting in significantly smoother melody lines."
Dan Ellis;John Arroyo,Eigenrhythms: Drum pattern basis sets for classification and generation.,2004,https://doi.org/10.5281/zenodo.1415948,"Daniel P.W. Ellis, LabROSA, Dept. of Elec. Eng., Columbia University, NY NY USA;John Arroyo, LabROSA, Dept. of Elec. Eng., Columbia University, NY NY USA","We took a collection of 100 drum beats from popular music tracks and estimated the measure length and downbeat position of each one. Using these values, we normalized each pattern to form an ensemble of aligned drum patterns. Principal Component Analysis on this data set results in a set of basis ‘patterns’ that can be combined to give approximations and interpolations of all the examples. We use this low-dimension representation of the drum patterns as a space for classification and visualization, and discuss its application to generating continua of rhythms. Our classification results were very modest – about 20% correct on a 10-way genre classification task – but we show that the projection into principal component space reveals aspects of the rhythm that are largely orthogonal to genre but are still perceptually relevant."
Slim Essid;Gaël Richard;Bertrand David,Musical instrument recognition based on class pairwise feature selection.,2004,https://doi.org/10.5281/zenodo.1418253,"Slim ESSID, GET-ENST (T´el´ecom Paris);Ga¨el RICHARD, GET-ENST (T´el´ecom Paris);Bertrand DAVID, GET-ENST (T´el´ecom Paris)","In this work, musical instrument recognition is considered on solo music from real world performance. A large sound database is used that consists of musical phrases excerpted from commercial recordings with different instrument instances, different players, and varying recording conditions. The proposed recognition scheme exploits class pairwise feature selection based on inertia ratio maximization. Moreover, new signal processing features based on octave band energy measures are introduced that prove to be useful. Classification is performed using Gaussian Mixture Models in a one vs one fashion in association with a data rescaling procedure as pre-processing. Experimental results show that substantial improvement in recognition success is thus achieved."
Klaus Frieler,Beat and meter extraction using gaussified onsets.,2004,https://doi.org/10.5281/zenodo.1417851,"Klaus Frieler, University of Hamburg","Rhythm, beat and meter are key concepts of music in general. Many efforts had been made in the last years to automatically extract beat and meter from a piece of music given either in audio or symbolical representation (see e.g. [11] for an overview). In this paper we propose a new method for extracting beat, meter and phase information from a list of unquantized onset times. The procedure relies on a novel method called ’Gaussification’ and adopts correlation techniques combined with findings from music psychology for parameter settings."
Emilia Gómez;Perfecto Herrera,Estimating The Tonality Of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies.,2004,https://doi.org/10.5281/zenodo.1418007,"Emilia Gómez, Music Technology Group, Institut Universitari de l’Audiovisual, Universitat Pompeu Fabra;Perfecto Herrera, Music Technology Group, Institut Universitari de l’Audiovisual, Universitat Pompeu Fabra","In this paper, we evaluate two methods for key estimation from polyphonic audio recordings. Our goal is to compare a strategy using a cognition-inspired model and several machine learning techniques to find a model for tonality determination of polyphonic music from audio files. Both approaches have as an input a vector of values related to the intensity of each of the pitch classes of a chromatic scale. In this study, both methods are explained and evaluated in a large database of audio recordings of classical pieces."
Masataka Goto;Katunobu Itou;Koji Kitayama;Tetsunori Kobayashi,Speech-Recognition Interfaces for Music Information Retrieval: 'Speech Completion' and 'Speech Spotter'.,2004,https://doi.org/10.5281/zenodo.1417339,"Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Katunobu Itou, Nagoya University, Japan;Koji Kitayama, Waseda University, Japan;Tetsunori Kobayashi, Waseda University, Japan","This paper describes music information retrieval (MIR) systems featuring automatic speech recognition. Although various interfaces for MIR have been proposed, speech-recognition interfaces suitable for retrieving musical pieces have not been studied. We propose two different speech-recognition interfaces for MIR, speech completion and speech spotter, and describe two MIR-based hands-free jukebox systems that enable a user to retrieve and play back a musical piece by saying its title or the artist’s name. The first is a music-retrieval system with the speech-completion interface that is suitable for music stores and car-driving situations. When a user can remember only part of the name of a musical piece or an artist and utters only a remembered fragment, the system helps the user recall and enter the name by completing the fragment. The second is a background-music playback system with the speech-spotter interface that can enrich human-human conversation. When a user is talking to another person, the system allows the user to enter voice commands for music-playback control by spotting a special voice-command utterance in face-to-face or telephone conversations. Our experimental results from use of these systems have demonstrated the effectiveness of the speech-completion and speech-spotter interfaces."
Fabien Gouyon;Simon Dixon,Dance music classification: A tempo-based approach.,2004,https://doi.org/10.5281/zenodo.1416636,"Fabien Gouyon, Universitat Pompeu Fabra;Simon Dixon, Austrian Research Institute for AI","Recent research has shown the importance of tempo in dance music classification. In this study, we propose a domain-specific learning methodology that uses the computed tempo to select an expert classifier specialized in its own tempo range. This approach reduces the all-class learning task to a set of two- and three-class learning tasks. The current results show around 70% classification accuracy for 8 ballroom dance music classes."
Maarten Grachten;Josep Lluís Arcos;Ramón López de Mántaras,Melodic Similarity: Looking for a Good Abstraction Level.,2004,https://doi.org/10.5281/zenodo.1417403,"Maarten Grachten, IIIA-CSIC - Artiﬁcial Intelligence Research Institute;Josep-Llu´ıs Arcos, IIIA-CSIC - Artiﬁcial Intelligence Research Institute;Ramon L´opez de M´antaras, CSIC - Spanish Council for Scientiﬁc Research","Computing melodic similarity is a very general problem with diverse musical applications ranging from music analysis to content-based retrieval. Choosing the appropriate level of representation is a crucial issue and depends on the type of application. Our research interest concerns the development of a CBR system for expressive music processing. In that context, a well chosen distance measure for melodies is a crucial issue. In this paper we propose a new melodic similarity measure based on the I/R model for melodic structure and compare it with other existing measures. The experimentation shows that the proposed measure provides a good compromise between discriminatory power and ability to recognize phrases from the same song."
Matthias Gruhne;Christian Uhle;Christian Dittmar;Markus Cremer,Extraction of Drum Patterns and their Description within the MPEG-7 High-Level-Framework.,2004,https://doi.org/10.5281/zenodo.1414888,"Matthias Gruhne, Fraunhofer IDMT;Christian Uhle, Fraunhofer IDMT;Christian Dittmar, Fraunhofer IDMT;Markus Cremer, Fraunhofer IDMT","ABSTRACT  A number of metadata standards have been published in recent years due to the increasing availability of multimedia content and the resulting issue of sorting and retrieving this content. One of the most recent efforts for a well defined metadata description is the ISO/IEC MPEG-7 standard, which takes a very broad approach towards the definition of metadata. Herein, not merely hand annotated textual information can be transported and stored, but also more signal specific data that can in most cases be automatically retrieved from the multimedia content itself. In this publication an algorithm for the automated transcription of rhythmic (percussive) accompaniment in modern day popular music is described. However, the emphasis here is not a precise transcription, but on capturing the “rhythmic gist” of the piece of music in order to allow a more abstract comparison of musical pieces by their dominant rhythmic patterns. A small-scale evaluation of the algorithm is presented along with an example representation of the thus gained semantically meaningful metadata using description methods currently discussed within MPEG-7."
AnYuan Guo;Hava T. Siegelmann,Time-Warped Longest Common Subsequence Algorithm for Music Retrieval.,2004,https://doi.org/10.5281/zenodo.1417165,"AnYuan Guo, University of Massachusetts, Amherst;Hava Siegelmann, University of Massachusetts, Amherst","Recent advances in music information retrieval have enabled users to query a database by singing or humming into a microphone. The queries are often inaccurate versions of the original songs due to singing errors and errors introduced in the music transcription process. In this paper, we present the Time-Warped Longest Common Subsequence algorithm (T-WLCS), which deals with singing errors involving rhythmic distortions. The algorithm is employed on song retrieval tasks, where its performance is compared to the longest common subsequence algorithm."
Jin Ha Lee;J. Stephen Downie,"Survey Of Music Information Needs, Uses, And Seeking Behaviours: Preliminary Findings.",2004,https://doi.org/10.5281/zenodo.1417637,"Jin Ha Lee, University of Illinois at Urbana-Champaign Graduate School of Library and Information Science;J. Stephen Downie, University of Illinois at Urbana-Champaign Graduate School of Library and Information Science","User studies focusing upon real-life music information needs, uses and seeking behaviours are still very scarce in the music information retrieval (MIR) and music digital library (MDL) fields. We are conducting a multi-group survey in an attempt to acquire information that can help eradicate false assumptions in designing MIR systems. Our goal is to provide an empirical basis for MIR/MDL system development. In this paper, we present our preliminary findings and analyses based on the 427 user responses we have received to date. Two major themes have been uncovered thus far that could have a significant influence the future development of successful MIR/MDL systems. First, people display “public information-seeking” behaviours by making use of collective knowledge and/or opinions of others about music such as reviews, ratings, recommendations, etc. in their music information-seeking. Second, respondents expressed needs for contextual metadata in addition to traditional bibliographic metadata."
Pierre Hanna;Nicolas Louis;Myriam Desainte-Catherine;Jenny Benois-Pineau,Audio Features for Noisy Sound Segmentation.,2004,https://doi.org/10.5281/zenodo.1415214,"Pierre Hanna, SCRIME - LaBRI, Universit´e de Bordeaux 1, F-33405 Talence Cedex, France;Nicolas Louis, SCRIME - LaBRI, Universit´e de Bordeaux 1, F-33405 Talence Cedex, France;Myriam Desainte-Catherine, SCRIME - LaBRI, Universit´e de Bordeaux 1, F-33405 Talence Cedex, France;Jenny Benois-Pineau, SCRIME - LaBRI, Universit´e de Bordeaux 1, F-33405 Talence Cedex, France","Automatic audio classification usually considers sounds as music, speech, silence or noise, but works about the noise class are rare. In this paper, we present a new audio feature sets that lead to the definition of four classes: colored, pseudo-periodic, impulsive and sinusoids within noises. This classification relies on works about the perception of noises. This audio feature set is experimented for noisy sound segmentation. Noise-to-noise transitions are characterized by means of statistical decision model based on Bayesian framework. This statistical method has been trained and experimented both on synthetic and real audio corpus. Using proposed feature set increases the discriminant power of Bayesian decision approach compared to a usual feature set."
David Hirst,An Analytical Methodology for Acousmatic Music.,2004,https://doi.org/10.5281/zenodo.1415656,"David Hirst, Teaching, Learning and Research Support Dept, University of Melbourne","This paper presents a procedure for the analysis of acousmatic music which was derived from the synthesis of top-down (knowledge driven) and bottom-up (data-driven) cognitive psychological views. The procedure is also a synthesis of research on primitive auditory scene analysis, combined with the research on acoustic, semantic, and syntactic factors in the perception of everyday environmental sounds. The procedure can be summarized as consisting of a number of steps: Segregation of sonic objects; Horizontal integration and/or segregation; Vertical integration and/or segregation; Assimilation and meaning."
Robyn Holmes;Marie-Louise Ayres,MusicAustralia: towards a national music information infrastructure.,2004,https://doi.org/10.5281/zenodo.1418097,"Robyn Holmes, National Library of Australia;Marie-Louise Ayres, National Library of Australia","MusicAustralia is a national music discovery service developed by the National Library of Australia and ScreenSound Australia. The service aims to provide seamless access to music and music information resources from custodians across all cultural sectors. The paper describes the development of the service, including its architecture and content base. The challenges of achieving advanced services in an environment where Music Information Retrieval research is relatively undeveloped are also discussed."
Jia-Lien Hsu;Arbee L. P. Chen;Hung-Chen Chen,Finding Approximate Repeating Patterns from Sequence Data.,2004,https://doi.org/10.5281/zenodo.1415530,"Jia-Lien Hsu, Department of Computer Science and Information Engineering, Fu Jen Catholic University, Taiwan, R.O.C.;Arbee L.P. Chen, Department of Computer Science, National Chengchi University, Taipei, Taiwan, R.O.C.;Hung-Chen Chen, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, R.O.C.","In this paper, an application of feature extraction from music data is first introduced to motivate our research of finding approximate repeating patterns from sequence data. An approximate repeating pattern is defined as a sequence of symbols which appears more than once under certain approximation types in a data sequence. By using the ‘cut’ and ‘pattern_join’ operators, we develop a level-wise approach to solve the problem of finding approximate repeating patterns."
Akinori Ito;Sung-Phil Heo;Motoyuki Suzuki;Shozo Makino,Comparison Of Features For DP-Matching Based Query-by-Humming System.,2004,https://doi.org/10.5281/zenodo.1416178,"Akinori Ito, Graduate School of Engineering, Tohoku University;Sung-Phil Heo, Korea Telecom Research & Development Group;Motoyuki Suzuki, Graduate School of Engineering, Tohoku University;Shozo Makino, Graduate School of Engineering, Tohoku University","In this paper, the authors compare three different similarity measures for a DP-matching based query-by-humming music retrieval system. They formulate a DP matching-based algorithm using the similarity between the deltaPitch of an input humming and that of a song in the database. The three similarities they introduce are distance-based similarity, quantization-based similarity, and fuzzy quantization-based similarity. They compare these similarities through experiments and find that the distance-based similarity gives the best recall rate. They also examine the combination of distance-based and fuzzy-quantization-based similarities and find that the recall rate is improved by the combination."
Peter Jan O. Doets;Reginald L. Lagendijk,Stochastic Model of a Robust Audio Fingerprinting System.,2004,https://doi.org/10.5281/zenodo.1416490,"P.J.O. Doets, Dept. of Mediamatics, Information and Communication Theory group, Delft University of Technology;R.L. Lagendijk, Dept. of Mediamatics, Information and Communication Theory group, Delft University of Technology","An audio fingerprint is a compact representation of the perceptually relevant parts of audio content. A suitable audio fingerprint can be used to identify audio files, even if they are severely degraded due to compression or other types of signal processing operations. We plan to use a fingerprint not only to identify the song but also to assess the perceptual quality of the compressed content. In order to develop such a fingerprinting scheme, a model is needed to assess the behavior of a fingerprint subject to compression. In this paper, we present the initial outlines of a model for an existing robust fingerprinting system to develop a more theoretical foundation. The model describes the stochastic behavior of the system when the input signal is a stationary (stochastic) signal. In this paper, the input is assumed to be white noise. Initial theoretical results are reported and validated with experimental data."
Tristan Jehan,Perceptual Segment Clustering For Music Description And Time-axis Redundancy Cancellation.,2004,https://doi.org/10.5281/zenodo.1416854,"Tristan Jehan, Massachusetts Institute of Technology","Repeating sounds and patterns are widely exploited throughout music. However, although analysis and music information retrieval applications are often concerned with processing speed and music description, they typically discard the benefits of sound redundancy cancellation. We propose a perceptually grounded model for describing music as a sequence of labeled sound segments, for reducing data complexity, and for compressing audio."
Steve Jones;Sally Jo Cunningham;Matt Jones 0001,Organizing digital music for use: an examination of personal music collections.,2004,https://doi.org/10.5281/zenodo.1416298,"Sally Jo Cunningham, Department of Computer Science, University of Waikato;Matt Jones, Department of Computer Science, University of Waikato;Steve Jones, Department of Computer Science, University of Waikato","Current research on music information retrieval and music digital libraries focuses on providing access to huge, public music collections. In this paper, we consider a different, but related, problem: supporting an individual in maintaining and using a personal music collection. We analyze organization and access techniques used to manage personal music collections (primarily CDs and MP3 files), and from these behaviors, suggest user behaviors that should be supported in a personal music digital library (that is, a digital library of an individual’s personal music collection)."
Jürgen Kilian;Holger H. Hoos,MusicBLAST - Gapped Sequence Alignment for MIR.,2004,https://doi.org/10.5281/zenodo.1416984,"J¨urgen Kilian, Darmstadt University of Technology;Holger H. Hoos, University of British Columbia","We propose an algorithm, MusicBLAST, for approximate pattern search/matching on symbolic musical data. MusicBLAST is based on the BLAST algorithm, one of the most commonly used algorithms for similarity search on biological sequence data [1, 2]. MusicBLAST can be used in combination with an arbitrary similarity measure (e.g., melodic, rhythmic or combined) and retrieves multiple occurrences of a given search pattern and its variations. Different from many other pattern matching techniques, it can find incomplete and imperfect occurrences of a given pattern, and produces a significance measure for the accuracy and quality of its results. Like BLAST — and different from many musical pattern matching approaches — MusicBLAST retrieves heuristically optimized bi-directional alignments searching iteratively in forward and backward direction by starting at a dedicated seed note position of a performance."
Peter Knees;Elias Pampalk;Gerhard Widmer,Artist Classification with Web-Based Data.,2004,https://doi.org/10.5281/zenodo.1417189,"Peter Knees, Austrian Research Institute for Artificial Intelligence;Elias Pampalk, Austrian Research Institute for Artificial Intelligence;Gerhard Widmer, Austrian Research Institute for Artificial Intelligence, Department of Medical Cybernetics and Artificial Intelligence, Medical University of Vienna","Manifold approaches exist for organization of music by genre and/or style. In this paper we propose the use of text categorization techniques to classify artists present on the Internet. In particular, we retrieve and analyze webpages ranked by search engines to describe artists in terms of word occurrences on related pages. To classify artists we primarily use support vector machines. We present 3 experiments in which we address the following issues. First, we study the performance of our approach compared to previous work. Second, we investigate how daily fluctuations in the Internet affect our approach. Third, on a set of 224 artists from 14 genres we study (a) how many artists are necessary to define the concept of a genre, (b) which search engines perform best, (c) how to formulate search queries best, (d) which overall performance we can expect for classification, and finally (e) how our approach is suited as a similarity measure for artists."
Ian Knopke,"Sound, Music and Textual Associations on the World Wide Web.",2004,https://doi.org/10.5281/zenodo.1416144,"Ian Knopke, McGill University","Sound files on the World Wide Web are accessed from web pages. To date, this relationship has not been explored extensively in the MIR literature. This paper details a series of experiments designed to measure the similarity between the public text visible on a web page and the linked sound files, the name of which is normally unseen by the user. A collection of web pages was retrieved from the web using a specially-constructed crawler. Sound file information and associated text were parsed from the pages and analyzed for similarity using common IR techniques such as TFIDF cosine measures. The results are intended to be used in the improvement of a web crawler for audio and music, as well as for MIR purposes in general."
Arvindh Krishnaswamy,Melodic Atoms for Transcribing Carnatic Music.,2004,https://doi.org/10.5281/zenodo.1417859,"Arvindh Krishnaswamy, Center for Computer Research in Music and Acoustics","We had introduced a set of 2D melodic units to transcribe Carnatic music previously, and we now provide some illustrative examples using real pitch tracks to further our discussion on this topic."
Pedro Kröger,CsoundXML: a meta-language in XML for sound synthesis.,2004,https://doi.org/10.5281/zenodo.1415652,"Pedro Kr¨oger, Federal University at Bahia, Brazil","ABSTRACT The software sound synthesis is closely related to the Music N programs started with Music I in 1957. Although Music N has many advantages such as unit generators and a flexible score language, it presents a few problems like limitations on instrument reuse, inflexibility of use of parameters, lack of a built-in graphical interface, and usually only one paradigm for scores. Some solutions concentrate in new from-scratch Music N implementations, while others focus in building user tools like pre-processors and graphical utilities. Nevertheless, new implementations in general focus in specific groups of problems leaving others unsolved. The user tools solve only one problem with no connection with others. In this paper we investigate the problem of creating a meta-language for sound synthesis. This constitutes an elegant solution for the above cited problems, without the need of a yet new acoustic compiler implementation, allowing a tight integration which is difficult to obtain with the present user tools."
Frank Kurth;Meinard Müller;Andreas Ribbrock;Tido Röder;David Damm;Christian Fremerey,A Prototypical Service for Real-Time Access to Local Context-Based Music Information.,2004,https://doi.org/10.5281/zenodo.1414926,"Frank Kurth, University of Bonn, Germany;Meinard M¨uller, University of Bonn, Germany;Andreas Ribbrock, University of Bonn, Germany;Tido R¨oder, University of Bonn, Germany;David Damm, University of Bonn, Germany;Christian Fremerey, University of Bonn, Germany","In this contribution, we propose a generic service for real-time access to context-based music information such as lyrics or score data. The service involves a client application that plays back an audio recording and connects to a server to identify the audio piece and current playback position. The server then delivers position-specific information on the audio piece to the client, which synchronously displays the information during playback. We demonstrate the use of this service using music information retrieval techniques and present two application scenarios."
Mika Kuuskankare;Mikael Laurson,Expressive Notation Package - an Overview.,2004,https://doi.org/10.5281/zenodo.1415084,"Mika Kuuskankare, Sibelius Academy;Mikael Laurson, Sibelius Academy",The purpose of this paper is to give the reader a concise overview of Expressive Notation Package 2.0 (henceforward ENP). ENP is a music notation program that belongs to a family of music and sound related software packages developed at Sibelius Academy in Finland. ENP has been used in various research projects during the past several years.
Olivier Lartillot,A multi-parametric and redundancy-filtering approach to pattern identification.,2004,https://doi.org/10.5281/zenodo.1416426,"Olivier Lartillot, University of Jyväskylä","This paper presents the principles of a new approach aimed at automatically discovering motivic patterns in monodies. It is shown that, for the results to agree with the listener’s understanding, computer modelling needs to follow as closely as possible the strategies undertaken during the listening process. Motivic patterns, which may progressively follow different musical dimensions, are discovered through an adaptive incremental identification in a multi-dimensional parametric space. The combinatorial redundancy that would logically result from the model is carefully limited with the help of particular heuristics. In particular, a notion of specificity relation between pattern descriptions is defined, unifying suffix relation – between patterns – and inclusion relation – between the multi-parametric descriptions of patterns. This enables to discard redundant patterns, whose descriptions are less specific than other patterns and whose occurrences are included in the occurrences of the more specific patterns. Resulting analyzes come close to the structures actually perceived by the listener."
Tin Lay Nwe;Ye Wang,Automatic Detection Of Vocal Segments In Popular Songs.,2004,https://doi.org/10.5281/zenodo.1417846,"Tin Lay Nwe, I2R, Singapore;Ye Wang, School of Computing, National University of Singapore","This paper presents a technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. The proposed technique uses acoustic features which are suitable to distinguish vocal and non-vocal signals. We employ the Hidden Markov Model (HMM) classifier for vocal and non-vocal classification. In contrast to conventional HMM training methods which employ one model for each class, we create an HMM model space (multi-model HMMs) for segmentation with improved accuracy. In addition, we employ an automatic bootstrapping process which adapts the test song’s own models for better classification accuracy. Experimental evaluations conducted on a database of 20 popular music songs show the validity of the proposed approach."
Micheline Lesaffre;Marc Leman;Bernard De Baets;Jean-Pierre Martens,Methodological Considerations Concerning Manual Annotation Of Musical Audio In Function Of Algorithm Development.,2004,https://doi.org/10.5281/zenodo.1414874,"Micheline Lesaffre, IPEM: Department of Musicology, Ghent University;Marc Leman, IPEM: Department of Musicology, Ghent University;Bernard De Baets, Department of Applied Mathematics, Biometrics and Process Control, Ghent University;Jean-Pierre Martens, Department of Electronics and Information Systems (ELIS), Ghent University","In research on musical audio-mining, annotated music databases are needed which allow the development of computational tools that extract from the musical audio-stream the kind of high-level content that users can deal with in Music Information Retrieval (MIR) contexts. The notion of musical content, and therefore the notion of annotation, is ill-defined, however, both in the syntactic and semantic sense. As a consequence, annotation has been approached from a variety of perspectives (but mainly linguistic-symbolic oriented), and a general methodology is lacking. This paper is a step towards the definition of a general framework for manual annotation of musical audio in function of a computational approach to musical audio-mining that is based on algorithms that learn from annotated data."
Ming Li;Ronan Sleep,Improving Melody Classification by Discriminant Feature Extraction and Fusion.,2004,https://doi.org/10.5281/zenodo.1416728,"Ming Li, School of Computing Sciences, University of East Anglia;Ronan Sleep, School of Computing Sciences, University of East Anglia","We propose a general approach to discriminant feature extraction and fusion, built on an optimal feature transformation for discriminant analysis [6]. Our experiments indicate that our approach can dramatically reduce the dimensionality of original feature space whilst improving its discriminant power. Our feature fusion method can be carried out in the reduced lower-dimensional subspace, resulting in a further improvement in accuracy. Our experiments concern the classification of music styles based only on the pitch sequence derived from monophonic melodies."
Beth Logan,Music Recommendation from Song Sets.,2004,https://doi.org/10.5281/zenodo.1416658,"Beth Logan, Hewlett Packard Labs","We motivate the problem of music recommendation based solely on acoustics from groups of related songs or ‘song sets’. We propose four solutions which can be used with any acoustic-based similarity measure. The ﬁrst builds a model for each song set and recommends new songs according to their distance from this model. The next three approaches recommend songs according to the average, median and minimum distance to songs in the song set. For a similarity measure based on K-means models of MFCC features, experiments on a database of 18647 songs indicated that the minimum distance technique is the most effective, returning a valid recommendation as one of the top 5 32.5% of the time. The approach based on the median distance was the next best, returning a valid recommendation as one of the top 5 29.5% of the time."
Maurício A. Loureiro;Hugo Bastos de Paula;Hani C. Yehia,Timbre Classification Of A Single Musical Instrument.,2004,https://doi.org/10.5281/zenodo.1416320,"Mauricio A. Loureiro, School of Music Grad. Program in Electrical Engineering Dept. of Electronic Engineering CEFALA – Center for Research on Speech, Acoustics Language and Music UFMG - Federal University of Minas Gerais - Brazil;Hugo B. de Paula, School of Music Grad. Program in Electrical Engineering Dept. of Electronic Engineering CEFALA – Center for Research on Speech, Acoustics Language and Music UFMG - Federal University of Minas Gerais - Brazil;Hani C. Yehia, School of Music Grad. Program in Electrical Engineering Dept. of Electronic Engineering CEFALA – Center for Research on Speech, Acoustics Language and Music UFMG - Federal University of Minas Gerais - Brazil","In order to map the spectral characteristics of the large variety of sounds a musical instrument may produce, different notes were performed and sampled in several intensity levels across the whole extension of a clarinet. Amplitude and frequency time-varying curves of partials were measured by Discrete Fourier Transform. A limited set of orthogonal spectral bases was derived by Principal Component Analysis techniques. These bases defined spectral sub-spaces capable of representing all tested sounds and of grouping them according to the distance metrics of the representation. A clustering algorithm was used to infer timbre classes. Preliminary tests with resynthesized sounds with normalized pitch showed a strong relation between the perceived timbre and the cluster label to which the notes were assigned. Self-Organizing Maps lead to results similar to those obtained by PCA representation and K-means clustering algorithm."
Anna Lubiw;Luke Tanur,Pattern Matching in Polyphonic Music as a Weighted Geometric Translation Problem.,2004,https://doi.org/10.5281/zenodo.1417969,"Anna Lubiw, University of Waterloo School of Computer Science;Luke Tanur, University of Waterloo School of Computer Science","We consider the music pattern matching problem—to ﬁnd occurrences of a small fragment of music called the “pattern” in a larger body of music called the “score”—as a problem of translating a set of horizontal line segments in the plane to ﬁnd the best match in a larger set of horizontal line segments. Our contribution is that we use fairly general weight functions to measure the quality of a match, thus enabling approximate pattern matching. We give an algorithm with running time O(nm log m), where n is the size of the score and m is the size of the pattern. We show that the problem, in this geometric formulation, is unlikely to have a significantly faster algorithm because it is at least as hard as a basic problem called 3-SUM that is conjectured to have no subquadratic algorithm. We present some examples to show the potential of this method for finding minor variations of a theme, and for finding polyphonic musical patterns in a polyphonic score."
Matija Marolt,Gaussian Mixture Models For Extraction Of Melodic Lines From Audio Recordings.,2004,https://doi.org/10.5281/zenodo.1416576,"Matija Marolt, Faculty of Computer and Information Science, University of Ljubljana, Slovenia","The presented study deals with extraction of melodic line(s) from polyphonic audio recordings. We base our work on the use of expectation maximization algorithm, which is employed in a two-step procedure that finds melodic lines in audio signals. In the first step, EM is used to find regions in the signal with strong and stable pitch (melodic fragments). In the second step, these fragments are grouped into clusters according to their properties (pitch, loudness...). The obtained clusters represent distinct melodic lines. Gaussian Mixture Models, trained with EM are used for clustering. The paper presents the entire process in more detail and gives some initial results."
Cory McKay;Ichiro Fujinaga,Automatic Genre Classification Using Large High-Level Musical Feature Sets.,2004,https://doi.org/10.5281/zenodo.1416158,"Cory McKay, McGill University Faculty of Music;Ichiro Fujinaga, McGill University Faculty of Music","This paper presents a system that extracts 109 musical features from symbolic recordings (MIDI, in this case) and uses them to classify the recordings by genre. The features used here are based on instrumentation, texture, rhythm, dynamics, pitch statistics, melody and chords. The classification is performed hierarchically using different sets of features at different levels of the hierarchy. Which features are used at each level, and their relative weightings, are determined using genetic algorithms. Classification is performed using a novel ensemble of feedforward neural networks and k-nearest neighbour classifiers. Arguments are presented emphasizing the importance of using high-level musical features, something that has been largely neglected in automatic classification systems to date in favour of low-level features. The effect on classification performance of varying the number of candidate features is examined in order to empirically demonstrate the importance of using a large variety of musically meaningful features. Two differently sized hierarchies are used in order to test the performance of the system under different conditions. Very encouraging classification success rates of 98% for root genres and 90% for leaf genres are obtained for a hierarchical taxonomy consisting of 9 leaf genres."
Martin F. McKinney;Dirk Moelants,Extracting the perceptual tempo from music.,2004,https://doi.org/10.5281/zenodo.1415146,"Martin F. McKinney, Philips Research Laboratories;Dirk Moelants, IPEM-Department of Musicology, Ghent University",The study presented here outlines a procedure for measuring and quantitatively representing the perceptual tempo of a musical excerpt. We also present a method for applying such measures of perceptual tempo to the design of automatic tempo-trackers in order to more accurately represent the perceived beat in music.
R. Mitchell;Irfan A. Essa,Feature Weighting for Segmentation.,2004,https://doi.org/10.5281/zenodo.1414976,"R. Mitchell Parry, Georgia Institute of Technology College of Computing / GVU Center;Irfan Essa, Georgia Institute of Technology College of Computing / GVU Center","This paper proposes the use of feature weights to reveal the hierarchical nature of music audio. Feature weighting has been exploited in machine learning, but has not been applied to music audio segmentation. We describe both a global and a local approach to automatic feature weighting. The global approach assigns a single weighting to all features in a song. The local approach uses the local separability directly. Both approaches reveal structure that is obscured by standard features, and emphasize segments of a particular size."
Daniel Müllensiefen;Klaus Frieler,Optimizing Measures Of Melodic Similarity For The Exploration Of A Large Folk Song Database.,2004,https://doi.org/10.5281/zenodo.1418031,"Daniel Müllensiefen, University of Hamburg Department of Systematic Musicology;Klaus Frieler, University of Hamburg Department of Systematic Musicology","This investigation aims at finding an optimal way of measuring the similarity of melodies. The applicability for an automated analysis and classification was tested on a folk song collection from Luxembourg that had been thoroughly analysed by an expert ethnomusicologist. Firstly a systematization of the currently available approaches to similarity measurements of melodies was done. About 50 similarity measures were implemented which differ in the way of transforming musical data and in the computational algorithms. Three listener experiments were conducted to compare the performance of the different measures to human experts’ ratings. Then an optimized model was obtained by using linear regression, which combines the output of several measures representing different musical dimensions. The performance of this optimized measure was compared with the classification work of a human ethnomusicologist on a collection of 577 Luxembourg folksongs."
Meinard Müller;Frank Kurth;Tido Röder,Towards an Efficient Algorithm for Automatic Score-to-Audio Synchronization.,2004,https://doi.org/10.5281/zenodo.1416302,"Meinard M¨uller, Universit¨at Bonn, Institut f¨ur Informatik III;Frank Kurth, Universit¨at Bonn, Institut f¨ur Informatik III;Tido R¨oder, Universit¨at Bonn, Institut f¨ur Informatik III","In the last few years, several algorithms for the automatic alignment of audio and score data corresponding to the same piece of music have been proposed. Among the major drawbacks to these approaches are the long running times as well as the large memory requirements. In this paper we present an algorithm, which solves the synchronization problem accurately and efficiently for complex, polyphonic piano music. In a first step, we extract from the audio data stream a set of highly expressive features encoding note onset candidates separately for all pitches. This makes computations efficient since only a small number of such features is sufficient to solve the synchronization task. Based on a suitable matching model, the best match between the score and the feature parameters is computed by dynamic programming (DP). To further cut down the computational cost in the synchronization process, we introduce the concept of anchor matches, matches which can be easily established. Then the DP-based technique is locally applied between adjacent anchor matches. Evaluation results have been obtained on complex polyphonic piano pieces including Chopin’s Etudes Op. 10."
Tomoyasu Nakano;Jun Ogata;Masataka Goto;Yuzuru Hiraga,A Drum Pattern Retrieval Method by Voice Percussion.,2004,https://doi.org/10.5281/zenodo.1417569,"Tomoyasu Nakano, Graduate School of Library, Information and Media Studies, University of Tsukuba, Japan;Jun Ogata, ;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Yuzuru Hiraga, ","This paper presents a method for voice percussion recognition and its application to drum pattern retrieval. Recognition of voice percussion requires an approach that is different from existing methods due to individual differences in vocal characteristics and verbal expressions. The proposed method uses onomatopoeia as the internal representation of drum sounds and combines voice percussion recognition with the retrieval of intended drum patterns. In a recognition experiment, the method achieved a recognition rate of 91.0% for the highest-tuned setting."
Andrew Nesbit;Lloyd Hollenberg;Anthony Senyard,Towards Automatic Transcription of Australian Aboriginal Music.,2004,https://doi.org/10.5281/zenodo.1416874,"Andrew Nesbit, School of Physics, University of Melbourne, Australia;Lloyd Hollenberg, School of Physics, University of Melbourne, Australia;Anthony Senyard, Dept of Computer Science and Software Engineering, University of Melbourne, Australia;The ﬁrst author, Department of Electronic Engineering, Queen Mary, University of London","We describe a system designed for automatic extraction and segmentation of didjeridu and clapsticks from certain styles of traditional Aboriginal Australian music. For didjeridu, we locate the start of notes using a complex-domain note onset detection algorithm, and use the detected onsets as cues for determining the harmonic series of sinusoids belonging to the didjeridu. The harmonic series is hypothesised, based on prior knowledge of the fundamental frequency of the didjeridu, and the most likely hypothesis is assumed. For clapsticks, we use independent subspace analysis to split the signal into harmonic and percussive components, followed by classification of the independent components. Finally, we identify areas in which the system can be enhanced to improve accuracy and also to extract a wider range of musically-relevant features. These include algorithms such as high frequency content techniques, and also computing the morphology of the didjeridu."
Giovanna Neve;Nicola Orio,Indexing and Retrieval of Music Documents through Pattern Analysis and Data Fusion Techniques.,2004,https://doi.org/10.5281/zenodo.1416372,"Giovanna Neve, University of Padova;Nicola Orio, University of Padova","One of the challenges of music information retrieval is the automatic extraction of effective content descriptors of music documents, which can be used at indexing and at retrieval time to match queries with documents. In this paper it is proposed to index music documents with frequent musical patterns. A musical pattern is a sequence of features in the score that is repeated at least twice: features can regard perceptually relevant characteristics, such as rhythm, pitch, or both. Data fusion techniques are applied to merge the results obtained using different features. A set of experimental tests has been carried out on retrieval effectiveness, robustness to query errors, and dependency on query length on a collection of Beatles’ songs using a set of queries. The proposed approach gave good results, both using single features and, in particular, merging the rank lists obtained by different features with a data fusion approach."
François Pachet;Aymeric Zils,Automatic extraction of music descriptors from acoustic signals.,2004,https://doi.org/10.5281/zenodo.1416106,"Pachet François, Sony CSL Paris;Zils Aymeric, Sony CSL Paris","High-Level music descriptors are key ingredients for music information retrieval systems. Although there is a long tradition in extracting information from acoustic signals, the field of music information extraction is largely heuristic in nature. We present here a heuristic-based generic approach for extracting automatically high-level music descriptors from acoustic signals. This approach is based on Genetic Programming, used to build relevant features as functions of mathematical and signal processing operators. The search of relevant features is guided by specialized heuristics that embody knowledge about the signal processing functions built by the system. Signal processing patterns are used in order to control the general processing methods. In addition, rewriting rules are introduced to simplify overly complex expressions, and a caching system further reduces the computing cost of each cycle. Finally, the features build by the system are combined into an optimized machine learning descriptor model, and an executable program is generated to compute the model on any audio signal. In this paper, we describe the overall system and compare its results against traditional approaches in musical feature extraction à la Mpeg7."
Elias Pampalk,A Matlab Toolbox to Compute Music Similarity from Audio.,2004,https://doi.org/10.5281/zenodo.1418077,"Elias Pampalk, Austrian Research Institute for Artiﬁcial Intelligence","A Matlab toolbox implementing music similarity measures for audio is presented. The implemented measures focus on aspects related to timbre and periodicities in the signal. This paper gives an overview of the implemented functions. In particular, the basics of the similarity measures are reviewed and some visualizations are discussed."
Bryan Pardo,Tempo Tracking with a Single Oscillator.,2004,https://doi.org/10.5281/zenodo.1415090,"Bryan Pardo, Northwestern University Department of Computer Science","I describe a simple on-line tempo tracker, based on phase and period locking a single oscillator to performance event timings. The tracker parameters are optimized on a corpus of solo piano performances by twelve musicians. The tracker is then tested on a second corpus of performances, played by the same twelve musicians. The performance of this tracker is compared to previously published results for a tempo tracker based on combining a tempogram and Kalman filter."
Steffen Pauws,Musical key extraction from audio.,2004,https://doi.org/10.5281/zenodo.1416326,"Steffen Pauws, Philips Research Laboratories Eindhoven","The realisation and evaluation of a musical key extraction algorithm that works directly on raw audio data is presented. Its implementation is based on models of human auditory perception and music cognition. It is straightforward and has minimal computing requirements. First, it computes a chromagram from non-overlapping 100 msecs time frames of audio; a chromagram represents the likelihood of the chroma occurrences in the audio. This chromagram is correlated with Krumhansl’s key profiles that represent the perceived stability of each chroma within the context of a particular musical key. The key profile that has maximum correlation with the computed chromagram is taken as the most likely key. An evaluation with 237 CD recordings of classical piano sonatas indicated a classification accuracy of 75.1%. By considering the exact, relative, dominant, sub-dominant and parallel keys as similar keys, the accuracy is even 94.1%."
Jose Pedro;Vadim Tarasov;Eloi Batlle;Enric Guaus;Jaume Masip,Industrial audio fingerprinting distributed system with CORBA and Web Services.,2004,https://doi.org/10.5281/zenodo.1414792,"Jose P. G. Mahedero, Audiovisual Institute, Universitat Pompeu Fabra;Vadim Tarasov, Audiovisual Institute, Universitat Pompeu Fabra;Eloi Batlle, Audiovisual Institute, Universitat Pompeu Fabra;Enric Guaus, Audiovisual Institute, Universitat Pompeu Fabra;Jaume Masip, Audiovisual Institute, Universitat Pompeu Fabra","With digital technologies, music content providers face serious challenges to protect their rights. Due to the widespread nature of music sources, it is very difficult to centralize the audio management. Audio fingerprinting allows the identification of audio content regardless of the audio format and without the need of additional metadata. Monitoring the audio being broadcasted by the TV and radio stations of a country requires the design and implementation of a scalable, robust and modular framework. We have chosen CORBA as distributed environment. The whole functionality needs to be decoupled from clients. To do so, Web services have been deployed. The audio identification core uses a Hidden Markov Model-based audio fingerprinting technology. The paper discusses the design and implementation issues of a complete distributing system that automatically monitors audio content, specifically music and commercials. Today, a working prototype of such a system already exists, and is dedicated to monitoring several radio and TV stations in Spain."
Anna Pienimäki;Kjell Lemström,Clustering Symbolic Music Using Paradigmatic and Surface Level Analyses.,2004,https://doi.org/10.5281/zenodo.1417447,"Anna Pienim¨aki, University of Helsinki;Kjell Lemstr¨om, University of Helsinki","In this paper, we describe a novel automatic cluster analysis method for symbolic music. The method contains both a surface level and a paradigmatic level analysing block and works in two phases. In the first phase, each music document of a collection is analysed separately: They are first divided into phrases that are consequently fed on a harmonic analyser. The paradigmatic structure of a given music document is achieved comparing both the melodic and the harmonic similarities among its phrases. In the second phase, the collection of music documents is clustered on the ground of their paradigmatic structures and surface levels. Our experimental results show that the novel method finds some interesting, underlying similarities that cannot be found using only surface level analysis."
Aggelos Pikrakis;Iasonas Antonopoulos;Sergios Theodoridis,Music meter and tempo tracking from raw polyphonic audio.,2004,https://doi.org/10.5281/zenodo.1416348,"Aggelos Pikrakis, Department of Informatics and Telecommunications, University of Athens, Greece;Iasonas Antonopoulos, Department of Informatics and Telecommunications, University of Athens, Greece;Sergios Theodoridis, Department of Informatics and Telecommunications, University of Athens, Greece","This paper presents a method for the extraction of music meter and tempo from raw polyphonic audio recordings, assuming that music meter remains constant throughout the recording. Although this assumption can be restrictive for certain musical genres, it is acceptable for a large corpus of folklore eastern music styles, including Greek traditional dance music. Our approach is based on the self-similarity analysis of the audio recording and does not assume the presence of percussive instruments. Its novelty lies in the fact that music meter and tempo are jointly determined. The method has been applied to a variety of musical genres, in the context of Greek traditional music where music meter can be 2/4, 3/4, 4/4, 5/4, 7/8, 9/8, 12/8 and tempo ranges from 40bpm to 330bpm. Experiments have, so far, demonstrated the efficiency of our method (music meter and tempo were successfully extracted for over 95% of the recordings)."
Christopher Raphael,A Hybrid Graphical Model for Aligning Polyphonic Audio with Musical Scores.,2004,https://doi.org/10.5281/zenodo.1416500,"Christopher Raphael, School of Informatics, Indiana University, Bloomington","We present a new method for establishing an alignment between a polyphonic musical score and a corresponding sampled audio performance. The method uses a graphical model containing both discrete variables, corresponding to score position, as well as a continuous latent tempo process. We use a simple data model based only on the pitch content of the audio signal. The data interpretation is defined to be the most likely configuration of the hidden variables, given the data, and we develop computational methodology for this task using a variant of dynamic programming involving parametrically represented continuous variables. Experiments are presented on a 55-minute hand-marked orchestral test set."
Christopher Raphael,Demonstration of 'Music Plus One'--- a System for Orchestral Musical Accompanimen.,2004,https://doi.org/10.5281/zenodo.1417899,"Christopher Raphael, Indiana University, Bloomington School of Informatics","DEMONSTRATION OF “MUSIC PLUS ONE” — A SYSTEM FOR ORCHESTRAL MUSICAL ACCOMPANIMENT  Past work on musical accompaniment systems has focused mostly on generating MIDI or other sparsely-parameterized accompaniments for live musicians. We present here a system that generates a complete orchestral accompaniment that follows a live player and learns and assimilates the player’s interpretation through a series of rehearsals. Unlike our previous efforts, this system creates the accompaniment by synthesizing, in real time, an orchestral accompaniment using an actual audio recording. Our system contains three separate modules called “Listen,” “Predict,” and “Synthesize” which perform tasks analogous to the human’s hearing of the soloist, anticipating the future trajectory, and actual playing of the accompaniment."
Josh Reiss;Mark B. Sandler,Audio Issues In MIR Evaluation.,2004,https://doi.org/10.5281/zenodo.1415840,"Josh Reiss, Dept. of Electronic Engineering, Queen Mary, University of London;Mark Sandler, Dept. of Electronic Engineering, Queen Mary, University of London","Several projects are underway to create music testbeds for the music analysis and music information retrieval (MIR) communities. The issue of audio file formats has become important, especially with plans to unify testbeds into a distributed grid. This document discusses the various audio formats, their advantages and disadvantages, and provides guidelines and recommendations for the construction of an MIR evaluation testbed."
Vegard Sandvold;Fabien Gouyon;Perfecto Herrera,Drum sound classification in polyphonic audio recordings using localized sound models.,2004,https://doi.org/10.5281/zenodo.1415808,"Vegard Sandvold, University of Oslo;Fabien Gouyon, Universitat Pompeu Fabra;Perfecto Herrera, Universitat Pompeu Fabra","This paper deals with automatic percussion classification in polyphonic audio recordings, focusing on kick, snare, and cymbal sounds. The authors present a feature-based sound modeling approach that combines general knowledge about the sound characteristics of percussion instrument families with on-the-fly acquired knowledge of recording-specific sounds. This approach achieves high classification accuracy with simple sound models, with an average accuracy around 20% higher than with general models alone."
Ryan Scherle;Donald Byrd,The Anatomy of a Bibliographic Search System for Music.,2004,https://doi.org/10.5281/zenodo.1417789,"Ryan Scherle and Donald Byrd, Indiana University Digital Library Program",Traditional library catalog systems have limitations when it comes to finding musical information. The Variations2 search system is designed specifically to aid users in searching for music by leveraging a rich set of bibliographic data records that express relationships between creators of music and their creations. This paper describes the design and implementation of the system that makes these searches possible.
Shai Shalev-Shwartz;Joseph Keshet;Yoram Singer,Learning to Align Polyphonic Music.,2004,https://doi.org/10.5281/zenodo.1416546,"Shai Shalev-Shwartz, School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israel;Joseph Keshet, School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israel;Yoram Singer, School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israel","We describe an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acoustic counterpart. Our method employs a supervised learning approach using a training set of aligned symbolic and acoustic representations. The alignment function is based on mapping the input acoustic-symbolic representation along with the target alignment into an abstract vector-space. We use our method for aligning MIDI and MP3 representations of polyphonic recordings of piano music and compare it to a generative method based on hidden Markov models. In all of our experiments, the discriminative method outperforms the HMM-based method."
Prarthana Shrestha;Ton Kalker,Audio Fingerprinting In Peer-to-peer Networks.,2004,https://doi.org/10.5281/zenodo.1417457,"Prarthana Shrestha, Faculty of Electrical Engineering, Eindhoven University of Technology;Ton Kalker, Faculty of Electrical Engineering, Eindhoven University of Technology","Despite the immense potential of Peer-to-Peer (P2P) networks in facilitating collaborative applications, they have become largely known as a free haven for pirated music swapping. In this paper, we present an approach wherein the collective computational power of the P2P networks is exploited to combat the problem of unauthorized music file sharing. We propose a distributed system based on audio fingerprinting, that makes it possible to recognize the music content present in the network. When the contents are identified, the network can take special measures against the use or sharing of unauthorized music. This proposed system is self-adapting, and robust. The foregoing properties make the system particularly suitable for use in dynamic and heterogeneous environment of P2P networks."
Jane Singer,Creating a nested melodic representation: competition and cooperation among bottom-up and top-down Gestalt principles.,2004,https://doi.org/10.5281/zenodo.1417965,"Jane Singer, The Hebrew University of Jerusalem","A set of principles governing how we group notes into meaningful groups has been widely accepted in the literature. However, existing theories have not succeeded in achieving a comprehensive and verifiable representation of melody due to multiple competing segmenting factors. This paper proposes a model that incorporates widely accepted principles of segmentation and establishes preferences among competing rules to create a few preferred representations for approximately 1,000 monophonic folksongs. The model includes both bottom-up rules of proximity and similarity, as well as top-down rules that detect periodicity and patterns among groupings."
J. Stephen Downie;Joe Futrelle;David K. Tcheng,"The International Music Information Retrieval Systems Evaluation Laboratory: Governance, Access and Security.",2004,https://doi.org/10.5281/zenodo.1415120,"J. Stephen Downie, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign;Joe Futrelle, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign;David Tcheng, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign","The IMIRSEL (International Music Information Retrieval Systems Evaluation Laboratory) project provides a platform for evaluating Music Information Retrieval (MIR) and Music Digital Library (MDL) techniques. It brings together large corpora, computational resources, and the necessary rights management and technical infrastructure to support MIR/MDL research. The standardized research collection being deployed represents a diverse corpus of musical examples, hosted in a secure environment for evaluating MIR/MDL algorithms. IMIRSEL serves as a community resource for researchers who may not have access to content rights and computational resources for large-scale evaluations."
Josh Stoddard;Christopher Raphael;Paul E. Utgoff,Well-Tempered Spelling: A Key Invariant Pitch Spelling Algorithm.,2004,https://doi.org/10.5281/zenodo.1414762,"Joshua Stoddard, U. Mass., Amherst Math & Stat Department;Christopher Raphael, U. Mass., Amherst Math & Stat Department;Paul E. Utgoff, U. Mass., Amherst Comp. Sci. Department","In this paper is described a data-driven algorithm for the functionally correct spelling of MIDI pitch values in terms of Western musical notation. Input is in the form of MIDI files containing accurate pitch and rhythmic information with corresponding ground-truth spelling information for training and evaluation. The algorithm recovers harmonic information from the MIDI data and spells pitches according to their relation to the local tonic. The algorithm achieved 94.98% accuracy on the pitches that required accidentals in the local key and 99.686% overall. Voice-leading resolution was found to be the best feature of those used to infer the correct spelling. Also, this paper outlines great potential for improvement under this model."
Craig Stuart Sapp;Yi-Wen Liu;Eleanor Selfridge-Field,Search Effectiveness Measures for Symbolic Music Queries in Very Large Databases.,2004,https://doi.org/10.5281/zenodo.1417913,"Craig Stuart Sapp, CCRMA and CCARH, Dept. of Music, Stanford University;Yi-Wen Liu, CCRMA, Dept. of Electrical Engineering, Stanford University;Eleanor Selfridge-Field, CCARH, Depts. of Music, Symbolic Systems, Stanford University","In the interest of establishing robust benchmarks for search efficiency, we conducted a series of tests on symbolic databases of musical incipits and themes taken from several diverse repertories. The results we report differ from existing studies in four respects: (1) the data quantity is much larger (c. 100,000 entries); (2) the levels of melodic and rhythmic precision are more refined; (3) anchored and unanchored searches were differentiated; and (4) results from joint pitch-and-rhythm searches were compared with those for pitch-only searches. The search results were evaluated using a theoretical approach which seeks to rank the number of symbols required to achieve ""sufficient uniqueness"". How far into a melody must a search go in order to find an item which is unmatched by any other of the available items? How much does the answer depend on the specificity of the query? How much does anchoring the query matter? How much does the result depend on the nature of the repertoire? We offer experimental results for these questions."
Iman S. H. Suyoto;Alexandra L. Uitdenbogerd,Exploring Microtonal Matching.,2004,https://doi.org/10.5281/zenodo.1416656,"Iman S. H. Suyoto, School of Computer Science and Information Technology, RMIT University;Alexandra L. Uitdenbogerd, School of Computer Science and Information Technology, RMIT University","Most research into music information retrieval thus far has only examined music from the western tradition. However, music of other origins often conforms to different tuning systems. Therefore there are problems both in representing this music as well as finding matches to queries from these diverse tuning systems. We discuss the issues associated with microtonal music retrieval and present some preliminary results from an experiment in applying scoring matrices to microtonal matching."
Sara Taheri-Panah;Andrew MacFarlane 0001,Music Information Retrieval systems: why do individuals use them and what are their needs?.,2004,https://doi.org/10.5281/zenodo.1416334,"S. Taheri-Panah, Centre for Interactive Systems Research, Department of Information Science, City University, London;A. MacFarlane, Centre for Interactive Systems Research, Department of Information Science, City University, London","To date there has been very little research conducted on the behaviour of music information retrieval (MIR) users, in spite of the immense popularity of free music retrieval systems available on the Internet. In this study we examine the issue of music seeking behaviour through the examination of users life style effect of three different age groups using questionnaires. It was found that lifestyles had a significant impact on users need for music and hence their music seeking behaviour. The importance of social networks in music information seeking was reinforced in this study. An experiment was conducted with three different types of search on the Kazaa MIR system and the participants interviewed in order to collect data. Users found the Kazaa system intuitive and easy to use. Searchers used both song titles and lyrics for finding relevant music items. The insights provided by this study can be of assistance in the development of user focused Internet MIR systems."
Haruto Takeda;Takuya Nishimoto;Shigeki Sagayama,Rhythm and Tempo Recognition of Music Performance from a Probabilistic Approach.,2004,https://doi.org/10.5281/zenodo.1418209,"Haruto Takeda, Graduate School of Information Science and Technology, The University of Tokyo;Takuya Nishimoto, Graduate School of Information Science and Technology, The University of Tokyo;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo","This paper concerns both rhythm recognition and tempo analysis of expressive music performance based on a probabilistic approach. In rhythm recognition, the modern continuous speech recognition technique is applied to find the most likely intended note sequence from the given sequence of fluctuating note durations in the performance. Combining stochastic models of note durations deviating from the nominal lengths and a probabilistic grammar representing possible sequences of notes, the problem is formulated as a maximum a posteriori estimation that can be implemented using efficient search based on the Viterbi algorithm. With this, significant improvements compared with conventional “quantization” techniques were found. Tempo analysis is performed by fitting the observed tempo with parametric tempo curves in order to extract tempo dynamics and characteristics of performance to use. Tempo-change timings and parameter values in tempo curve models are estimated through the segmental k-means algorithm. Experimental results of rhythm recognition and tempo analysis applied to classical and popular music performances are also demonstrated."
Richard Terrat,Pregroup Grammars for Chords.,2004,https://doi.org/10.5281/zenodo.1416702,"Richard G. TERRAT, LIRMM/CNRS;, IRCAM","Pregroups had been conceived as an algebraic tool to recognize grammatically well-formed sentences in natural languages [3]. Here we wish to use pregroups to recognize well-formed chords of pitches, for a given definition of those chords. We show how a judicious choice of basic and simple types allows a context-free grammatical description. Then we use the robustness property to extend the set of well-formed chords in a simple way. Finally we argue in favor of an utilization of pregroups grammars for the recognition and classification of chord sequences."
Adam R. Tindale;Ajay Kapur;George Tzanetakis;Ichiro Fujinaga,Retrieval of percussion gestures using timbre classification techniques.,2004,https://doi.org/10.5281/zenodo.1416612,"Adam Tindale, Music Technology;Ajay Kapur, Electrical Engineering;George Tzanetakis, Computer Science;Ichiro Fujinaga, Music Technology","Musicians are able to recognise the subtle differences in timbre produced by different playing techniques on an instrument, yet there has been little research into achieving this with a computer. This paper will demonstrate an automatic system that can successfully recognise different timbres produced by different performance techniques and classify them using signal processing and classification tools. Success rates over 90% are achieved when classifying snare drum timbres produced by different playing techniques."
Marc Torrens;Patrick Hertzog;Josep Lluís Arcos,Visualizing and Exploring Personal Music Libraries.,2004,https://doi.org/10.5281/zenodo.1414746,"Marc Torrens, MusicStrands Inc.;Patrick Hertzog, AI Lab., EPFL;Josep-Llu´ıs Arcos, IIIA, CSIC","Nowadays, music fans are beginning to massively use mobile digital music players and dedicated software to organize and play large collections of music. In this context, users deal with huge music libraries containing thousands of tracks. Such a huge volume of music easily overwhelms users when selecting the music to listen or when organizing their collections. Music player software with visualizations based on textual lists and organizing features such as smart playlists are not really enough for helping users to efficiently manage their libraries. Thus, we propose new graphical visualizations and their associated features to allow users to better organize their personal music libraries and therefore also to ease selection later on."
Godfried T. Toussaint,A Comparison of Rhythmic Similarity Measures.,2004,https://doi.org/10.5281/zenodo.1416812,"Godfried T. Toussaint, McGill University School of Computer Science","Traditionally, rhythmic similarity measures are compared according to how well rhythms may be recognized with them, how efficiently they can be retrieved from a database, or how well they model human perception and cognition. In contrast, here similarity measures are compared on the basis of how much insight they provide about the structural inter-relationships that exist within families of rhythms, when phylogenetic trees and graphs are computed from the distance matrices determined by these similarity measures. Phylogenetic analyses yield insight into the evolution of rhythms and may uncover interesting ancestral rhythms."
Ken'ichi Toyoda;Kenzi Noike;Haruhiro Katayose,Utility System For Constructing Database Of Performance Deviations.,2004,https://doi.org/10.5281/zenodo.1417953,"Ken’ichi Toyoda, Kwansei Gakuin University;Kenzi Noike, Kwansei Gakuin University;Haruhiro Katayose, Kwansei Gakuin University","Demand for music databases is increasing for the studies of musicology and music informatics. Our goal is to construct databases that contain deviations of tempo, and dynamics, start-timing, and duration of each note. This paper describes a procedure based on hybrid use of DP Matching and HMM that efficiently extracts deviations from MIDI-formatted expressive human performances. The algorithm of quantizing the start-timing of the notes has been successfully tested on a database of ten expressive piano performances. It gives an accuracy of 92.9% when one note per bar is given as the guide. This paper also introduces tools provided so that the public can make use of our database on the web."
Wei-Ho Tsai;Hsin-Min Wang,Towards Automatic Identification Of Singing Language In Popular Music Recordings.,2004,https://doi.org/10.5281/zenodo.1417511,"Wei-Ho Tsai and Hsin-Min Wang, Institute of Information Science, Academia Sinica","The automatic analysis of singing from music is an important and challenging issue within the research target of content-based retrieval of music information. As part of this research target, this study presents a first attempt to automatically identify the language sung in a music recording. It is assumed that each language has its own set of constraints that specify which of the basic linguistic events present in a singing process are allowed to follow another. The acoustic structure of individual languages may, thus, be characterized by statistically modeling those constraints. To this end, the proposed method employs vector clustering to convert a singing signal from its spectrum-based feature representation into a sequence of smaller basic phonological units. The dynamic characteristics of the sequence are then analyzed by using bigram language models. Since the vector clustering is performed in an unsupervised manner, the resulting system does not use sophisticated linguistic knowledge and, thus, is easily portable to new language sets. In addition, to eliminate the interference of background music, we leverage the statistical estimation of a piece’s music background so that the vector clustering is relevant to the solo singing voices in the accompanied signals."
Rainer Typke;Frans Wiering;Remco C. Veltkamp,A search method for notated polyphonic music with pitch and tempo fluctuations.,2004,https://doi.org/10.5281/zenodo.1417947,"Rainer Typke, Utrecht University;Frans Wiering, Utrecht University;Remco C. Veltkamp, Utrecht University","We compare two methods of measuring melodic similarity for symbolically represented polyphonic music. Both exploit advantages of transportation distances such as continuity and partial matching in the pitch dimension. By segmenting queries and database documents, one of them also offers partial matching in the time dimension. This method can find short queries in long database documents and is more robust against pitch and tempo fluctuations in the queries or database documents than the method that uses transportation distances alone. We compare the use of transportation distances with and without segmentation for the RISM A/II collection and find that segmentation improves recall and precision. With everything else being equal, the segmented search found 80 out of 114 relevant documents, while the method relying solely on transportation distances found only 60."
George Tzanetakis;Ajay Kapur;Manj Benning,Query-by-Beat-Boxing: Music Retrieval For The DJ.,2004,https://doi.org/10.5281/zenodo.1418033,"Ajay Kapur, University of Victoria;Manj Benning, University of Victoria;George Tzanetakis, University of Victoria","BeatBoxing is a type of vocal percussion, where musicians use their lips, cheeks, and throat to create different beats. In this paper, we explore the use of BeatBoxing as a query mechanism for music information retrieval and more specifically the retrieval of drum loops. A classification system that automatically identifies the individual beat boxing sounds and can map them to corresponding drum sounds has been developed. In addition, the tempo of BeatBoxing is automatically detected and used to dynamically browse a database of music. We also describe some experiments in extracting structural representations of rhythm and their use for style classification of drum loops."
Fabio Vignoli,Digital Music Interaction Concepts: A User Study.,2004,https://doi.org/10.5281/zenodo.1414994,"Fabio Vignoli, Philips Research Laboratories","The popularity of digital music has increased rapidly, leading to the need for new user interfaces to access and retrieve music. This paper presents the results of user tests aimed at investigating how music listeners organize and access their digital music collections. The study focuses on developing novel interaction concepts based on the similarity of music items. The goal is to create an interaction concept that takes advantage of the digital nature of music and facilitates access and retrieval from large personal collections."
Fabio Vignoli;Rob van Gulik;Huub van de Wetering,"Mapping Music In The Palm Of Your Hand, Explore And Discover Your Collection.",2004,https://doi.org/10.5281/zenodo.1416960,"Rob van Gulik, Technische Universiteit Eindhoven;Fabio Vignoli, Philips Research Laboratories;Huub van de Wetering, Technische Universiteit Eindhoven","The trends of miniaturization and increasing storage capabilities for portable music players made it possible to carry increasingly more music on small portable devices, but it also introduced negative consequences for the user interface and navigation. Finding music in large collections can be hard if one does not know exactly what to look for. In this paper a novel user interface to browse and navigate through music on small devices is proposed, together with the enabling algorithms. The goal of this interface is to enable the users to explore and discover their entire collection and to support non-specific searches. To this end, a new way to visualize and navigate through music is introduced: the artist map. The artist map is designed to provide an overview of an entire music collection, or a subset thereof, by clearly visualizing the similarity between artists, computed from the music itself. Contextual information (e.g. mood, genre) is added by coloring and by attribute magnets. The artist map is implemented by a graph-drawing algorithm, which uses an improved energy model. The proposed algorithm and interface have been implemented in a prototype and will be tested with ‘real’ users."
Emmanuel Vincent;Xavier Rodet,Instrument identification in solo and ensemble music using Independent Subspace Analysis.,2004,https://doi.org/10.5281/zenodo.1416524,"Emmanuel Vincent, IRCAM, Analysis-Synthesis Group;Xavier Rodet, IRCAM, Analysis-Synthesis Group","We investigate the use of Independent Subspace Analysis (ISA) for instrument identification in musical recordings. We represent short-term log-power spectra of possibly polyphonic music as weighted non-linear combinations of typical note spectra plus background noise. These typical note spectra are learnt either on databases containing isolated notes or on solo recordings from different instruments. We show that this model has some theoretical advantages over methods based on Gaussian Mixture Models (GMM) or on linear ISA. Preliminary experiments with five instruments and test excerpts taken from commercial CDs give promising results. The performance on clean solo excerpts is comparable with existing methods and shows limited degradation under reverberant conditions. Applied to a difficult duo excerpt, the model is also able to identify the right pair of instruments and to provide an approximate transcription of the notes played by each instrument."
Kristopher West;Stephen Cox,Features and classifiers for the automatic classification of musical audio signals.,2004,https://doi.org/10.5281/zenodo.1418025,"Kris West, School of Computing Sciences, University of East Anglia;Stephen Cox, School of Computing Sciences, University of East Anglia","Several factors affecting the automatic classification of musical audio signals are examined. Classification is performed on short audio frames and results are reported as ""bag of frames"" accuracies, where the audio is segmented into 23ms analysis frames and a majority vote is taken to decide the final classification. The effect of different parameterisations of the audio signal is examined. The effect of the inclusion of information on the temporal variation of these features is examined and finally, the performance of several different classifiers trained on the data is compared. A new classifier is introduced, based on the unsupervised construction of decision trees and either linear discriminant analysis or a pair of single Gaussian classifiers. The classification results show that the topology of the new classifier gives it a significant advantage over other classifiers, by allowing the classifier to model much more complex distributions within the data than Gaussian schemes do."
Tillman Weyde,The Influence of Pitch on Melodic Segmentation.,2004,https://doi.org/10.5281/zenodo.1415556,"Tillman Weyde, University of Osnabr¨uck","Melodic segmentation is an important topic for music information retrieval, as it divides melodies into musically relevant units. Previous theories have emphasized the role of pitch in melodic segmentation, assuming that large changes in pitch mark segment boundaries. However, there has been a lack of empirical studies on this topic. This study investigates the influence of inter-onset-intervals (IOI), intensity accents, pitch intervals, and pitch interval direction changes on melodic segmentation. The results show a significant influence only for IOIs and intensity, but not for pitch intervals or changes in interval direction. The validity of the results and possible explanations are discussed, and directions for further investigations are outlined."
Brian Whitman;Dan Ellis,Automatic Record Reviews.,2004,https://doi.org/10.5281/zenodo.1416646,"Brian Whitman, MIT Media Lab Music Mind and Machine Group;Daniel P.W. Ellis, LabROSA Columbia University Electrical Engineering","Record reviews provide a unique and focused source of linguistic data that can be related to musical recordings, to provide a basis for computational music understanding systems with applications in similarity, recommendation and classification. We analyze a large testbed of music and a corpus of reviews for each work to uncover patterns and develop mechanisms for removing reviewer bias and extraneous non-musical discussion. By building upon work in grounding free text against audio signals we invent an “automatic record review” system that labels new music audio with maximal semantic value for future retrieval tasks. In effect, we grow an unbiased music editor trained from the consensus of the online reviews we have gathered."
Gavin Wood;Simon O'Keefe,A Case Study of Distributed Music Audio Analysis Using the Geddei Processing Framework.,2004,https://doi.org/10.5281/zenodo.1414810,"Gavin Wood, Department of Computer Science;Simon O’Keefe, University of York","Audio signal processing and refinement is an important part of a content-based music information retrieval system. As the repertoire of techniques becomes more varied, there are greater requirements of computation power. Distributed storage techniques have become widespread and almost invisible with the advent of file-sharing systems, online digital music stores, and online storage services. This paper provides a brief description of a software framework that can process audio in a scalable and distributed manner: Geddei. The paper then takes an interesting and relevant signal analysis task often used for music information retrieval and implements it under the Geddei framework. The ease of use is discussed and various measurements taken of Geddei, both in comparison to itself under different circumstances, and 'reference code' that was used in a previous study. We discuss the problems with the distribution of the task with Geddei and offer some possible solutions."
Otto Wüst;Òscar Celma,An MPEG-7 Database System and Application for Content-Based Management and Retrieval of Music.,2004,https://doi.org/10.5281/zenodo.1418375,"Otto Wust, Music Technology Group, Universitat Pompeu Fabra;Oscar Celma, Music Technology Group, Universitat Pompeu Fabra","Computer users are gaining access to and are starting to accumulate moderately large collections of multimedia ﬁles, in particular of audio content, and therefore demand new applications and systems capable of effectively retriev-ing and manipulating these multimedia objects. Content-based retrieval of multimedia ﬁles is typically based on searching within a feature space, deﬁned as a collection of parameters that have been extracted from the content and which describe it in a relevant way for that particular retrieval application. The MPEG-7 standard offers tools to model these metadata in an interoperable and extensi-ble way, and can therefore be considered as a framework for building content-based audio retrieval systems."
Dan Yang 0002;Won-Sook Lee,Disambiguating Music Emotion Using Software Agents.,2004,https://doi.org/10.5281/zenodo.1415272,"Dan Yang, University of Ottawa;WonSook Lee, University of Ottawa","Annotating music poses a cognitive load on listeners and this potentially interferes with the emotions being reported. One solution is to let software agents learn to make the annotator’s task easier and more efficient. Emo is a music annotation prototype that combines inputs from both human and software agents to better study human listening. A compositional theory of musical meaning provides the overall heuristics for the annotation process, with the listener drawing upon different influences such as acoustics, lyrics and cultural metadata to focus on a specific musical mood. Software agents track the way these choices are made from the influences available. A functional theory of human emotion provides the basis for introducing necessary bias into the machine learning agents. Conflicting positive and negative emotions can be separated on the basis of their different function (reward-approach and threat-avoidance) or dysfunction (psychotic). Negative emotions have strong ambiguity and these are the focus of the experiment. The results of mining psychological features of lyrics are promising, recognisable in terms of common sense ideas of emotion and in terms of accuracy. Further ideas for deploying agents in this model of music annotation are presented."
Kazuyoshi Yoshii;Masataka Goto;Hiroshi G. Okuno,Automatic Drum Sound Description for Real-World Music Using Template Adaptation and Matching Methods.,2004,https://doi.org/10.5281/zenodo.1415958,"Kazuyoshi Yoshii, Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Hiroshi G. Okuno, Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Japan","This paper presents an automatic description system of drum sounds for real-world musical audio signals. The system can represent onset times and names of drums using drum descriptors defined in the context of MPEG-7. To solve the problem of varying acoustic features of drum sounds, the authors propose new template-adaptation and template-matching methods. Experimental results showed that the accuracy of identifying bass and snare drums in popular music was about 90%. The paper also defines drum descriptors in the MPEG-7 format and demonstrates an example of the automatic drum sound description for a piece of popular music."
Takuya Yoshioka;Tetsuro Kitahara;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno,Automatic Chord Transcription with Concurrent Recognition of Chord Symbols and Boundaries.,2004,https://doi.org/10.5281/zenodo.1415068,"Takuya Yoshioka, Graduate School of Informatics, Kyoto University;Tetsuro Kitahara, Graduate School of Informatics, Kyoto University;Kazunori Komatani, Graduate School of Informatics, Kyoto University;Tetsuya Ogata, Graduate School of Informatics, Kyoto University;Hiroshi G. Okuno, Graduate School of Informatics, Kyoto University","This paper describes a method that recognizes musical chords from real-world audio signals in compact-disc recordings. The automatic recognition of musical chords is necessary for music information retrieval (MIR) systems, since the chord sequences of musical pieces capture the characteristics of their accompaniments. None of the previous methods can accurately recognize musical chords from complex audio signals that contain vocal and drum sounds. The main problem is that the chord-boundary-detection and chord-symbol-identification processes are inseparable because of their mutual dependency. In order to solve this mutual dependency problem, our method generates hypotheses about tuples of chord symbols and chord boundaries, and outputs the most plausible one as the recognition result. The certainty of a hypothesis is evaluated based on three cues: acoustic features, chord progression patterns, and bass sounds. Experimental results show that our method successfully recognized chords in seven popular music songs; the average accuracy of the results was around 77%."
Mark Zadel;Ichiro Fujinaga,Web Services for Music Information Retrieval.,2004,https://doi.org/10.5281/zenodo.1417069,"Mark Zadel, Faculty of Music, McGill University;Ichiro Fujinaga, Faculty of Music, McGill University","In the emerging world of networked and distributed digital libraries, the Web services framework will be a key to facilitating simple inter-application communication between them. Yet, despite the popularity of Web services in the business sector and their seemingly obvious applicability to the digital library domain, and to MIR in particular, the adoption of these new protocols has not been widespread. To demonstrate the tremendous potential of Web services for MIR, this paper presents an application using the Google and Amazon.com databases to generate clusters of related musical artists based on cultural metadata. The use of cultural metadata to determine artist relatedness is valuable and interesting because it captures emergent popular opinion about music. Starting from an initial seed artist, Amazon Listmania! lists are traversed to find potentially related artists. Google is used to determine which of these candidates are in fact related by assessing the co-occurrence of the two artists’ names on Internet web pages. A list of artists related to the seed is returned once a given number of artists is found. The positive results generated by the system illustrate the use of Web services for exploiting the vast amount of untapped data that are available today and highlight their importance for the future, when even more musical data will become available."
Rohit M A;Amitrajit Bhattacharjee;Preeti Rao,Four-way Classification of Tabla Strokes with Models Adapted from Automatic Drum Transcription,2021,https://doi.org/10.5281/zenodo.5624489,"Rohit M A, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Amitrajit Bhattacharjee, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Preeti Rao, Department of Electrical Engineering, Indian Institute of Technology Bombay, India","Motivated by musicological applications of the four-way categorization of tabla strokes, we consider automatic classification methods that are potentially robust to instrument differences. We present a new, diverse tabla dataset suitably annotated for the task. The acoustic correspondence between the tabla stroke categories and the common popular Western drum types motivates us to adapt models and methods from automatic drum transcription. We start by exploring the use of transfer learning on a state-of-the-art pre-trained multiclass CNN drums model. This is compared with 1-way models trained separately for each tabla stroke class. We find that the 1-way models provide the best mean f-score while the drums pre-trained and tabla-adapted 3-way models generalize better for the most scarce target class. To improve model robustness further, we investigate both drums and tabla-specific data augmentation strategies."
Taketo Akama,A Contextual Latent Space Model: Subsequence Modulation in Melodic Sequence,2021,https://doi.org/10.5281/zenodo.5624425,"Taketo Akama, Sony Computer Science Laboratories, Tokyo, Japan","Some generative models for sequences such as music and text allow us to edit only subsequences, given surrounding context sequences, which plays an important part in steering generation interactively. However, editing subsequences mainly involves randomly resampling subsequences from a possible generation space. We propose a contextual latent space model (CLSM) in order for users to be able to explore subsequence generation with a sense of direction in the generation space, e.g., interpolation, as well as exploring variations—semantically similar possible subsequences. A context-informed prior and decoder constitute the generative model of CLSM, and a context position-informed encoder is the inference model. In experiments, we use a monophonic symbolic music dataset, demonstrating that our contextual latent space is smoother in interpolation than baselines, and the quality of generated samples is superior to baseline models. The generation examples are available online."
María Alfaro-Contreras;David Rizo;Jose M. Inesta;Jorge Calvo-Zaragoza,OMR-assisted transcription: a case study with early prints,2021,https://doi.org/10.5281/zenodo.5625663,"María Alfaro-Contreras, Department of Software and Computing Systems, University of Alicante, Spain;David Rizo, Department of Software and Computing Systems, University of Alicante, Spain;Jose M. Iñesta, Department of Software and Computing Systems, University of Alicante, Spain;Jorge Calvo-Zaragoza, Department of Software and Computing Systems, University of Alicante, Spain","Most of the musical heritage is only available as physical documents, given that the engraving process was carried out by handwriting or typesetting until the end of the 20th century. Their mere availability as scanned images does not enable tasks such as indexing or editing unless they are transcribed into a structured digital format. Given the cost and time required for manual transcription, Optical Music Recognition (OMR) presents itself as a promising alternative. Quite often, OMR systems show acceptable but not perfect performance, which eventually leaves them out of the transcription process. On the assumption that OMR systems might always make some errors, it is essential that the user corrects the output. This paper contributes to a better understanding of how music transcription is improved by the assistance of OMR systems that include the end-user in the recognition process. For that, we have measured the transcription time of a printed early music work under two scenarios: a manual one and a state-of-the-art OMR-assisted one, with several alternatives each. Our results demonstrate that using OMR remarkably reduces users' effort, even when its performance is far optimal, compared to the fully manual option."
Stefan A Baumann,Deeper Convolutional Neural Networks and Broad Augmentation Policies Improve Performance in Musical Key Estimation,2021,https://doi.org/10.5281/zenodo.5624477,"Stefan Andreas Baumann, ","In recent years, complex convolutional neural network architectures such as the Inception architecture have been shown to offer significant improvements over previous architectures in image classification. So far, little work has been done applying these architectures to music information retrieval tasks, with most models still relying on sequential neural network architectures. In this paper, we adapt the Inception architecture to the specific needs of harmonic music analysis and use it to create a model (InceptionKeyNet) for the task of key estimation. We then show that the resulting model can significantly outperform state-of-the-art single-task models when trained on the same datasets. Additionally, we evaluate a broad range of augmentation methods and find that extending augmentation policies to include a more diverse set of methods further improves accuracy. Finally, we train both the proposed and state-of-the-art single-task models on differently sized training datasets and different augmentation policies and compare the differences in generalization performance."
Axel Berndt,The Music Performance Markup Format and Ecosystem,2021,https://doi.org/10.5281/zenodo.5624429,"Axel Berndt, Center of Music and Film Informatics, Ostwestfalen-Lippe University of Applied Sciences and Arts, Detmold University of Music","Music Performance Markup (MPM) is a new XML format that offers a model-based, systematic approach for describing and analysing musical performances. Its foundation is a set of mathematical models that capture the characteristics of performance features such as tempo, rubato, dynamics, articulations, and metrical accentuations. After a brief introduction to MPM, this paper will put the focus on the infrastructure of documentations, software tools and ongoing development activities around the format."
Louis Bigo;David Regnier;Nicolas Martin,Identification of rhythm guitar sections in symbolic tablatures,2021,https://doi.org/10.5281/zenodo.5624513,"David Régnier, Univ. Lille, CNRS, Centrale Lille;Nicolas Martin, Arobas Music, Lille, France;Louis Bigo, Univ. Lille, CNRS, Centrale Lille","Sections of guitar parts in pop/rock songs are commonly described by functional terms including for example rhythm guitar, lead guitar, solo or riff. At a low level, these terms generally involve textural properties, for example whether the guitar tends to play chords or single notes. At a higher level, they indicate the function the guitar is playing relative to other instruments of the ensemble, for example whether the guitar is accompanying in background, or if it is intended to play a part in the foreground. Automatic labelling of instrumental function has various potential applications including the creation of consistent datasets dedicated to the training of generative models that focus on a particular function. In this paper, we propose a computational method to identify rhythm guitar sections in symbolic tablatures. We define rhythm guitar as sections that aim at making the listener perceive the chord progression that characterizes the harmony part of the song. A set of 31 high level features is proposed to predict if a bar in a tablature should be labeled as rhythm guitar or not. These features are used by an LSTM classifier which yields to a F1 score of 0.95 on a dataset of 102 guitar tablatures with manual function annotations. Manual annotations and computed feature vectors are publicly released."
Charles Brazier;Gerhard Widmer,On-Line Audio-to-Lyrics Alignment Based on a Reference Performance,2021,https://doi.org/10.5281/zenodo.5625665,"Charles Brazier, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria","Audio-to-lyrics alignment has become an increasingly active research task in MIR, supported by the emergence of several open-source datasets of audio recordings with word-level lyrics annotations. However, there are still a number of open problems, such as a lack of robustness in the face of severe duration mismatches between audio and lyrics representation; a certain degree of language-specificity caused by acoustic differences across languages; and the fact that most successful methods in the field are not suited to work in real-time. Real-time lyrics alignment (tracking) would have many useful applications, such as fully automated subtitle display in live concerts and opera. In this work, we describe the first real-time-capable audio-to-lyrics alignment pipeline that is able to robustly track the lyrics of different languages, without additional language information. The proposed model predicts, for each audio frame, a probability vector over (European) phoneme classes, using a very small temporal context, and aligns this vector with a phoneme posteriogram matrix computed beforehand from another recording of the same work, which serves as a reference and a proxy to the written-out lyrics. We evaluate our system's tracking accuracy on the challenging genre of classical opera. Finally, robustness to out-of-training languages is demonstrated in an experiment on Jingju (Beijing opera)."
Aaron Carter-Enyi;Gilad Rabinovitch;Nathaniel Condit-Schultz,Visualizing Intertextual Form with Arc Diagrams: Contour and Schema-based Methods,2021,https://doi.org/10.5281/zenodo.5624583,"Aaron Carter-Enyi, Morehouse College;Gilad Rabinovitch, Florida State University;Nathaniel Condit-Schultz, Georgia Institute of Technology","The visualizations in Wattenberg's Shape of Song (2001) were based on pitch-string matching, but there are many other equivalence classes and similarity relations proposed by music research. This paper applies recent algorithms by Carter-Enyi (2016) and Carter-Enyi and Rabinovitch (2021) with the intention of making arc diagrams more effective for research and teaching. We first draw on Barber's intertextual analysis of Yoruba Oriki, in which tone language texts are circulated through various performances (Barber 1984). Intertextuality is exemplified through a 2018 composition by Nigerian composer Ayo Oluranti, then extended to Dizzy Gillespie's solo in his recording of ""Blue Moon"" (ca. 1952). Example visualizations are produced through an open-source implementation, ATAVizM, which brings together contour theory (Quinn 1997), schema theory (Gjerdingen 2007), and edit distance (Orpen and Huron 1992). Applications to the music of Bach and Mozart demonstrate that an African-centered analytical methodology has utility for music research at large. Computational music research can benefit from analytical approaches that draw upon humanistic theory and are applicable to a variety of musics."
Francisco J. Castellanos;Antonio-Javier Gallego;Jorge Calvo-Zaragoza,Unsupervised Domain Adaptation for Document Analysis of Music Score Images,2021,https://doi.org/10.5281/zenodo.5624455,"Francisco J. Castellanos, Department of Software and Computing Systems, University of Alicante, Spain;Antonio-Javier Gallego, Department of Software and Computing Systems, University of Alicante, Spain;Jorge Calvo-Zaragoza, Department of Software and Computing Systems, University of Alicante, Spain","<p>Document analysis is a key step within the typical Optical Music Recognition workflow. It processes an input image to obtain its layered version by extracting the different sources of information. Recently, this task has been formulated as a supervised learning problem, specifically by means of Convolutional Neural Networks due to their high performance and generalization capability. However, the requirement of training data for each new type of document still represents an important drawback. This issue can be palliated through Domain Adaptation (DA), which is the field that aims to adapt the knowledge learned with an annotated collection of data to other domains for which labels are not available. In this work, we combine a DA strategy based on adversarial training with Selectional Auto-Encoders to define an unsupervised framework for document analysis. Our experiments show a remarkable improvement for the layers that depict particular features at each domain, whereas layers that depict common features (such as staff lines) are barely affected by the adaptation process. In the best-case scenario, our method achieves an average relative improvement of around 44%, thereby representing a promising solution to unsupervised document analysis.</p>"
Rodrigo Castellon;Chris Donahue;Percy Liang,Codified audio language modeling learns useful representations for music information retrieval,2021,https://doi.org/10.5281/zenodo.5624605,"Rodrigo Castellon, Stanford University;Chris Donahue, Stanford University;Percy Liang, Stanford University","We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically, we explore representations from Jukebox (Dhariwal et al. 2020): a music generation system containing a language model trained on codified audio from 1M songs. To determine if Jukebox's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, emotion recognition, and key detection. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging, suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR."
Chin-Jui Chang;Chun-Yi Lee;Yi-Hsuan Yang,Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding,2021,https://doi.org/10.5281/zenodo.5624625,"Chin-Jui Chang, Research Center for IT Innovation, Academia Sinica;Chun-Yi Lee, Department of Computer Science, National Tsing Hua University;Yi-Hsuan Yang, Yating Music Team, Taiwan AI Labs","This paper proposes a new self-attention based model for music score infilling, i.e., to generate a polyphonic music sequence that fills in the gap between given past and future contexts. While existing approaches can only fill in a short segment with a fixed number of notes, or a fixed time span between the past and future contexts, our model can infill a variable number of notes (up to 128) for different time spans. We achieve so with three major technical contributions. First, we adapt XLNet, an autoregressive model originally proposed for unsupervised model pre-training, to music score infilling. Second, we propose a new, musically specialized positional encoding called relative bar encoding that better informs the model of notes' position within the past and future context. Third, to capitalize relative bar encoding, we perform look-ahead onset prediction to predict the onset of a note one time step before predicting the other attributes of the note. We compare our proposed model with two strong baselines and show that our model is superior in both objective and subjective analyses."
Yi-Wei Chen;Hung-Shin Lee;Yen-Hsing Chen;Hsin-Min Wang,SurpriseNet: Melody Harmonization Conditioning on User-controlled Surprise Contours,2021,https://doi.org/10.5281/zenodo.5624423,"Yi-Wei Chen, Institute of Information Science, Academia Sinica, Taiwan;Hung-Shin Lee, Institute of Information Science, Academia Sinica, Taiwan;Yen-Hsing Chen, Institute of Information Science, Academia Sinica, Taiwan;Hsin-Min Wang, Institute of Information Science, Academia Sinica, Taiwan","The surprisingness of a song is an essential and seemingly subjective factor in determining whether the listener likes it. With the help of information theory, it can be described as the transition probability of a music sequence modeled as a Markov chain. In this study, we introduce the concept of deriving entropy variations over time, so that the surprise contour of each chord sequence can be extracted. Based on this, we propose a user-controllable framework that uses a conditional variational autoencoder (CVAE) to harmonize the melody based on the given chord surprise indication. Through explicit conditions, the model can randomly generate various and harmonic chord progressions for a melody, and the Spearman's correlation and p-value significance show that the resulting chord progressions match the given surprise contour quite well. The vanilla CVAE model was evaluated in a basic melody harmonization task (no surprise control) in terms of six objective metrics. The results of experiments on the Hooktheory Lead Sheet Dataset show that our model achieves performance comparable to the state-of-the-art melody harmonization model."
Vincent K.M. Cheung;Hsuan-Kai Kao;Li Su,Semi-supervised violin fingering generation using variational autoencoders,2021,https://doi.org/10.5281/zenodo.5624441,"Vincent K.M. Cheung, Institute of Information Science, Academia Sinica, Taiwan;Hsuan-Kai Kao, Institute of Information Science, Academia Sinica, Taiwan;Li Su, Institute of Information Science, Academia Sinica, Taiwan","There are many ways to play the same note with the fingerboard hand on string instruments such as the violin. Musicians can flexibly adapt their string choice, hand position, and finger placement to maximise expressivity and playability when sounding each note. Violin fingerings therefore serve as important guides in ensuring effective performance, especially for inexperienced players. However, fingering annotations are often missing or only partially available on violin sheet music. Here, we propose a model based on the variational autoencoder that generates violin fingering patterns using only pitch and timing information found on the score. Our model leverages limited existing fingering data with the possibility to learn in a semi-supervised manner. Results indicate that fingering annotations generated by our model successfully imitate the style and preferences of a human performer. We further show its significantly improved performance with semi-supervised learning, and demonstrate our model's ability to match the state-of-the-art in violin fingering pattern generation when trained on only half the amount of labelled data."
Keunwoo Choi;Yuxuan Wang,"Listen, Read, and Identify: Multimodal Singing Language Identification of Music",2021,https://doi.org/10.5281/zenodo.5624555,"Keunwoo Choi, ByteDance;Yuxuan Wang, ByteDance","We propose a multimodal singing language classification model that uses both audio content and textual metadata. LRID-Net, the proposed model, takes an audio signal and a language probability vector estimated from the metadata and outputs the probabilities of the target languages. Optionally, LRID-Net is facilitated with modality dropouts to handle a missing modality. In the experiment, we trained several LRID-Nets with varying modality dropout configuration and tested them with various combinations of input modalities. The experiment results demonstrate that using multimodal input improves performance. The results also suggest that adopting modality dropout does not degrade the performance of the model when there are full modality inputs while enabling the model to handle missing modality cases to some extent."
Shreyan Chowdhury;Gerhard Widmer,On Perceived Emotion in Expressive Piano Performance: Further Experimental Evidence for the Relevance of Mid-level Perceptual Features,2021,https://doi.org/10.5281/zenodo.5624499,"Shreyan Chowdhury, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria; LIT AI Lab, Linz Institute of Technology, Austria","Despite recent advances in audio content-based music emotion recognition, a question that remains to be explored is whether an algorithm can reliably discern emotional or expressive qualities between different performances of the same piece. In the present work, we analyze several sets of features on their effectiveness in predicting arousal and valence of six different performances (by six famous pianists) of Bach's Well-Tempered Clavier Book 1. These features include low-level acoustic features, score-based features, features extracted using a pre-trained emotion model, and Mid-level perceptual features. We compare their predictive power by evaluating them on several experiments designed to test performance-wise or piece-wise variations of emotion. We find that Mid-level features show significant contribution in performance-wise variation of both arousal and valence - even better than the pre-trained emotion model. Our findings add to the evidence of Mid-level perceptual features being an important representation of musical attributes for several tasks - specifically, in this case, for capturing the expressive aspects of music that manifest as perceived emotion of a musical performance."
Bas Cornelissen;Willem Zuidema;John Ashley Burgoyne,Cosine Contours: a Multipurpose Representation for Melodies,2021,https://doi.org/10.5281/zenodo.5624531,"Bas Cornelissen, Institute for Logic, Language and Computation, University of Amsterdam;Willem Zuidema, Institute for Logic, Language and Computation, University of Amsterdam;John Ashley Burgoyne, Institute for Logic, Language and Computation, University of Amsterdam","Melodic contour is central to our ability to perceive and produce music. We propose to represent melodic contours as a combination of cosine functions, using the discrete cosine transform. The motivation for this approach is twofold: (1) it approximates a maximally informative contour representation (capturing most of the variation in as few dimensions as possible), but (2) it is nevertheless independent of the specifics of the data sets for which it is used. We consider the relation with principal component analysis, which only meets the first of these requirements. Theoretically, the principal components of a repertoire of random walks are known to be cosines. We find, empirically, that the principal components of melodies also closely approximate cosines in multiple musical traditions. We demonstrate the usefulness of the proposed representation by analyzing contours at three levels (complete songs, melodic phrases and melodic motifs) across multiple traditions in three small case studies."
Shuqi Dai;Zeyu Jin;Celso Gomes;Roger Dannenberg,Controllable deep melody generation via hierarchical music structure representation,2021,https://doi.org/10.5281/zenodo.5625667,"Shuqi Dai, Carnegie Mellon University;Zeyu Jin, Adobe Inc.;Celso Gomes, Adobe Inc.;Roger B. Dannenberg, Carnegie Mellon University","Recent advances in deep learning have expanded possibilities to generate music, but generating a customizable full piece of music with consistent long-term structure remains a challenge. This paper introduces MusicFrameworks, a hierarchical music structure representation and a multi-step generative process to create a full-length melody guided by long-term repetitive structure, chord, melodic contour, and rhythm constraints. We first organize the full melody with section and phrase-level structure. To generate melody in each phrase, we generate rhythm and basic melody using two separate transformer-based networks, and then generate the melody conditioned on the basic melody, rhythm and chords in an auto-regressive manner. By factoring music generation into sub-problems, our approach allows simpler models and requires less data. To customize or add variety, one can alter chords, basic melody, and rhythm structure in the music frameworks, letting our networks generate the melody accordingly. Additionally, we introduce new features to encode musical positional information, rhythm patterns, and melodic contours based on musical domain knowledge. A listening test reveals that melodies generated by our method are rated as good as or better than human-composed music in the POP909 dataset about half the time."
Emir Demirel;Sven Ahlbäck;Simon Dixon,MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription,2021,https://doi.org/10.5281/zenodo.5624643,"Emir Demirel, Queen Mary University of London;Sven Ahlb¨ack, Doremir Music Research AB;Simon Dixon, Queen Mary University of London","This paper makes several contributions to automatic lyrics transcription (ALT) research. Our main contribution is a novel variant of the Multistreaming Time-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which processes the temporal information using multiple streams in parallel with varying resolutions keeping the network more compact, and thus with a faster inference and an improved recognition rate than having identical TDNN streams. In addition, two novel preprocessing steps prior to training the acoustic model are proposed. First, we suggest using recordings from both monophonic and polyphonic domains during training the acoustic model. Second, we tag monophonic and polyphonic recordings with distinct labels for discriminating non-vocal silence and music instances during alignment. Moreover, we present a new test set with a considerably larger size and a higher musical variability compared to the existing datasets used in ALT literature, while maintaining the gender balance of the singers. Our best performing model sets the state-of-the-art in lyrics transcription by a large margin. For reproducibility, we publicly share the identifiers to retrieve the data used in this paper."
Hao-Wen Dong;Chris Donahue;Taylor Berg-Kirkpatrick;Julian Mcauley,Towards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music,2021,https://doi.org/10.5281/zenodo.5624447,"Hao-Wen Dong, University of California San Diego;Chris Donahue, Stanford University;Taylor Berg-Kirkpatrick, University of California San Diego;Julian McAuley, University of California San Diego","Modern keyboards allow a musician to play multiple instruments at the same time by assigning zones—fixed pitch ranges of the keyboard—to different instruments. In this paper, we aim to further extend this idea and examine the feasibility of automatic instrumentation—dynamically assigning instruments to notes in solo music during performance. In addition to the online, real-time-capable setting for performative use cases, automatic instrumentation can also find applications in assistive composing tools in an offline setting. Due to the lack of paired data of original solo music and their full arrangements, we approach automatic instrumentation by learning to separate parts (e.g., voices, instruments and tracks) from their mixture in symbolic multitrack music, assuming that the mixture is to be played on a keyboard. We frame the task of part separation as a sequential multi-class classification problem and adopt machine learning to map sequences of notes into sequences of part labels. To examine the effectiveness of our proposed models, we conduct a comprehensive empirical evaluation over four diverse datasets of different genres and ensembles—Bach chorales, string quartets, game music and pop music. Our experiments show that the proposed models outperform various baselines. We also demonstrate the potential for our proposed models to produce alternative convincing instrumentations for an existing arrangement by separating its mixture into parts. All source code and audio samples can be found at https://salu133445.github.io/arranger/ ."
Sachinda Edirisooriya;Hao-Wen Dong;Julian Mcauley;Taylor Berg-Kirkpatrick,An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition,2021,https://doi.org/10.5281/zenodo.5625669,"Sachinda Edirisooriya, University of California San Diego;Hao-Wen Dong, University of California San Diego;Julian McAuley, University of California San Diego;Taylor Berg-Kirkpatrick, University of California San Diego","Previous work has shown that neural architectures are able to perform optical music recognition (OMR) on monophonic and homophonic music with high accuracy. However, piano and orchestral scores frequently exhibit polyphonic passages, which add a second dimension to the task. Monophonic and homophonic music can be described as homorhythmic, or having a single musical rhythm. Polyphonic music, on the other hand, can be seen as having multiple rhythmic sequences, or voices, concurrently. We first introduce a workflow for creating large-scale polyphonic datasets suitable for end-to-end recognition from sheet music publicly available on the MuseScore forum. We then propose two novel formulations for end-to-end polyphonic OMR---one treating the problem as a type of multi-task binary classification, and the other treating it as multi-sequence detection. Building upon the encoder-decoder architecture and an image encoder proposed in past work on end-to-end OMR, we propose two novel decoder models---FlagDecoder and RNNDecoder---that correspond to the two formulations. Finally, we compare the empirical performance of these end-to-end approaches to polyphonic OMR and observe a new state-of-the-art performance with our multi-sequence detection decoder, RNNDecoder."
Anders Elowsson;Olivier Lartillot,A Hardanger Fiddle Dataset with Performances Spanning Emotional Expressions and Annotations Aligned using Image Registration,2021,https://doi.org/10.5281/zenodo.5624587,"Anders Elowsson, RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion; University of Oslo;Olivier Lartillot, RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion; University of Oslo","This paper presents a Hardanger fiddle dataset ""HF1"" with polyphonic performances spanning five different emotional expressions: normal, angry, sad, happy, and tender. The performances thus cover the four quadrants of the activity/valence-space. The onsets and offsets, together with an associated pitch, were human-annotated for each note in each performance by the fiddle players themselves. First, they annotated the normal version. These annotations were then transferred to the expressive performances using music alignment and finally human-verified. Two separate music alignment methods based on image registration were developed for this purpose; a B-spline implementation that produces a continuous temporal transformation curve and a Demons algorithm that produces displacement matrices for time and pitch that also account for local timing variations across the pitch range. Both methods start from an ""Onsetgram"" of onset salience across pitch and time and perform the alignment task accurately. Various settings of the Demons algorithm were further evaluated in an ablation study. The final dataset is around 43 minutes long and consists of 19 734 notes of Hardanger fiddle music, recorded in stereo. The dataset and source code are available online. The dataset will be used in MIR research for tasks involving polyphonic transcription, score alignment, beat tracking, downbeat tracking, tempo estimation, and classification of emotional expressions."
Jeffrey Ens;Philippe Pasquier,Building the MetaMIDI Dataset: Linking Symbolic and Audio Musical Data,2021,https://doi.org/10.5281/zenodo.5624567,"Jeff Ens, Simon Fraser University;Philippe Pasquier, Simon Fraser University","We introduce the MetaMIDI Dataset (MMD), a large scale collection of 436,631 MIDI files and metadata. MMD, contains artist and title metadata for 221,504 MIDI files, and genre metadata for 143,868 MIDI files, collected during the web-scraping process. MIDI files in MMD, were matched against a collection of 32,000,000 30-second audio clips retrieved from Spotify, resulting in over 10,796,557 audio-MIDI matches. In addition, we linked 600,142 Spotify tracks with 1,094,901 MusicBrainz recordings to produce a set of 168,032 MIDI files that are matched to the MusicBrainz database. We also provide a set of 53,496 MIDI files using audio-MIDI matches where the derived metadata on Spotify is a fuzzy match to the web-scraped metadata. These links augment many files in the dataset with the extensive metadata available via the Spotify API and the MusicBrainz database. We anticipate that this collection of data will be of great use to MIR researchers addressing a variety of research topics."
Christoph Finkensiep;Martin A Rohrmeier,Modeling and Inferring Proto-Voice Structure in Free Polyphony,2021,https://doi.org/10.5281/zenodo.5624431,"Christoph Finkensiep, École Polytechnique Fédérale de Lausanne;Martin Rohrmeier, École Polytechnique Fédérale de Lausanne","Voice leading is considered to play an important role in the structure of Western tonal music.  However, the explicit voice assignment of a piece (if present at all)  generally does not reflect all phenomena related to voice leading.  Instead, voice-leading phenomena can occur in free textures (e.g., in most keyboard music),  or cut across the explicitly notated voices  (e.g., through implicit polyphony within a single voice).  This paper presents a model of proto-voices,  voice-like structures that encode sequential and vertical relations between notes  without the need to assume explicit voices.  Proto-voices are constructed by recursive combination of primitive structural operations,  such as insertion of neighbor or passing notes,  or horizontalization of simultaneous notes.  Together, these operations give rise to a grammar-like hierarchical system  that can be used to infer the structural fabric of a piece using a chart parsing algorithm.  Such a model can serve as a foundation  for defining higher-level latent entities (such as harmonies or voice-leading schemata),  explicitly linking them to their realizations on the musical surface."
Francesco Foscarin;Nicolas Audebert;Raphael Fournier-S'Niehotta,PKSpell: Data-Driven Pitch Spelling and Key Signature Estimation,2021,https://doi.org/10.5281/zenodo.5624435,"Francesco Foscarin, CEDRIC (EA4629), CNAM Paris, HESAM Université, France;Nicolas Audebert, CEDRIC (EA4629), CNAM Paris, HESAM Université, France;Raphaël Fournier S’niehotta, CEDRIC (EA4629), CNAM Paris, HESAM Université, France","We present PKSpell: a data-driven approach for the joint estimation of pitch spelling and key signatures from MIDI files. Both elements are fundamental for the production of a full-fledged musical score and facilitate many MIR tasks such as harmonic analysis, section identification, melodic similarity, and search in a digital music library.    We design a deep recurrent neural network model that only requires information readily available in all kinds of MIDI files, including performances, or other symbolic encodings. We release a model trained on the ASAP dataset. Our system can be used with these pre-trained parameters and is easy to integrate into a MIR pipeline. We also propose a data augmentation procedure that helps re-training on small datasets.    PKSpell achieves strong key signature estimation performance on a challenging dataset. Most importantly, this model establishes a new state-of-the-art performance on the MuseData pitch spelling dataset without retraining."
Dave Foster;Simon Dixon,Filosax: A Dataset of Annotated Jazz Saxophone Recordings,2021,https://doi.org/10.5281/zenodo.5625643,"Dave Foster, Queen Mary University of London;Simon Dixon, Queen Mary University of London, Centre For Digital Music","The Filosax dataset is a large collection of specially commissioned recordings of jazz saxophonists playing with commercially available backing tracks. Five participants each recorded themselves playing the melody, interpreting a transcribed solo and improvising on 48 tracks, giving a total of around 24 hours of audio data. The solos are annotated both as individual note events with physical timing, and as sheet music with a metrical interpretation of the timing. In this paper, we outline the criteria used for choosing and sourcing the repertoire, the recording process and the semi-automatic transcription pipeline. We demonstrate the use of the dataset to analyse musical phenomena such as swing timing and dynamics of typical musical figures, as well as for training a source activity detection system and predicting expressive characteristics. Other potential applications include the modelling of jazz improvisation, performer identification, automatic music transcription, source separation and music generation."
Giovanni Gabbolini;Derek Bridge,An interpretable music similarity measure based on path interestingness,2021,https://doi.org/10.5281/zenodo.5624649,"Giovanni Gabbolini, Insight Centre for Data Analytics, School of Computer Science & IT, University College Cork, Ireland;Derek Bridge, Insight Centre for Data Analytics, School of Computer Science & IT, University College Cork, Ireland","We introduce a novel and interpretable path-based music similarity measure. Our similarity measure assumes that items, such as songs and artists, and information about those items are represented in a knowledge graph. We find paths in the graph between a seed and a target item; we score those paths based on their interestingness; and we aggregate those scores to determine the similarity between the seed and the target. A distinguishing feature of our similarity measure is its interpretability. In particular, we can translate the most interesting paths into natural language, so that the causes of the similarity judgements can be readily understood by humans. We compare the accuracy of our similarity measure with other competitive path-based similarity baselines in two experimental settings and with four datasets.  The results highlight the validity of our approach to music similarity, and demonstrate that path interestingness scores can be the basis of an accurate and interpretable similarity measure."
Hugo F Flores Garcia;Aldo Aguilar;Ethan Manilow;Bryan Pardo,Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition,2021,https://doi.org/10.5281/zenodo.5624615,"Hugo Flores Garcia, Interactive Audio Lab, Northwestern University;Aldo Aguilar, Interactive Audio Lab, Northwestern University;Ethan Manilow, Interactive Audio Lab, Northwestern University;Bryan Pardo, Interactive Audio Lab, Northwestern University","Deep learning work on musical instrument recognition has generally focused on instrument classes for which we have abundant data. In this work, we exploit hierarchical relationships between instruments in a few-shot learning setup to enable classification of a wider set of musical instruments, given a few examples at inference. We apply a hierarchical loss function to the training of prototypical networks, combined with a method to aggregate prototypes hierarchically, mirroring the structure of a predefined musical instrument hierarchy. These extensions require no changes to the network architecture and new levels can be easily added or removed. Compared to a non-hierarchical few-shot baseline, our method leads to a significant increase in classification accuracy and significant decrease in mistake severity on instrument classes unseen in training."
Mark R H Gotham;Rainer Kleinertz;Christof Weiss;Meinard Müller;Stephanie Klauk,What if the 'When' Implies the 'What'?: Human harmonic analysis datasets clarify the relative role of the separate steps in automatic tonal analysis,2021,https://doi.org/10.5281/zenodo.5676067,"Mark Gotham, Institut für Musik und Musikwissenschaft, Technische Universität Dortmund, Germany;Rainer Kleinertz, Institut für Musikwissenschaft, Saarland University, Germany;Christof Weiß, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany;Stephanie Klauk, ","This paper uses the emerging provision of human harmonic analyses to assess how reliably we can map from knowing only when chords and keys change to a full identification of what those chords and keys are. We do this with a simple implementation of pitch class profile matching methods, partly to provide a benchmark score against which to judge the performance of less readily interpretable machine learning systems, many of which explicitly separate these when and what tasks and provide performance evaluation for these separate stages. Additionally, as this 'oracle'-style, 'perfect' segmentation information will not usually be available in practice, we test the sensitivity of these methods to slight modifications in the position of segment boundaries by introducing deliberate errors. This study examines several corpora. The focus on is symbolic data, though we include one audio dataset for comparison. The code and corpora (of symbolic scores and analyses) are available within: https://github.com/MarkGotham/When-in-Rome"
Juan S. Gómez-Cañón;Estefania Cano;Yi-Hsuan Yang;Perfecto Herrera;Emilia Gomez,Let's agree to disagree: Consensus Entropy Active Learning for Personalized Music Emotion Recognition,2021,https://doi.org/10.5281/zenodo.5624399,"Juan Sebastián Gómez-Cañón, Music Technology Group, Universitat Pompeu Fabra, Spain;Estefanía Cano, Songquito UG, Erlangen, Germany;Yi-Hsuan Yang, Academia Sinica, Taiwan;Perfecto Herrera, Music Technology Group, Universitat Pompeu Fabra, Spain;Emilia Gómez, Joint Research Centre, European Commission, Seville, Spain","Previous research in music emotion recognition (MER) has tackled the inherent problem of subjectivity through the design of personalized models -- models which predict the emotions that a particular user would perceive from music.   Personalized models are trained in a supervised manner, and are tested exclusively with the annotations provided by a specific user.   While past research has focused on model adaptation or reducing the amount of annotations required from a given user, we propose a novel methodology based on uncertainty sampling and query-by-committee methods, adopting prior knowledge from the agreement of human annotations as an oracle for active learning.   We assume that our disagreements define our personal opinions and should be considered for personalization.   We use the DEAM dataset, the current benchmark dataset for MER, to pre-train our models.   We then use the AMG1608 dataset, the largest MER dataset containing multiple annotations per musical excerpt, to re-train diverse machine learning models using active learning and evaluate personalization.   Our results suggest that our methodology can be beneficial to produce personalized classification algorithms, which exhibit different results depending on the algorithms' complexity."
Curtis Hawthorne;Ian Simon;Rigel Swavely;Ethan Manilow;Jesse Engel,Sequence-to-Sequence Piano Transcription with Transformers,2021,https://doi.org/10.5281/zenodo.5624461,"Curtis Hawthorne, Google Research;Ian Simon, Google Research;Rigel Swavely, Google Research;Ethan Manilow, Google Research;Jesse Engel, Google Research","Automatic Music Transcription has seen significant progress in recent years by training custom deep neural networks on large datasets. However, these models have required extensive domain-specific design of network architectures, input/output representations, and complex decoding schemes. In this work, we show that equivalent performance can be achieved using a generic encoder-decoder Transformer with standard decoding methods. We demonstrate that the model can learn to translate spectrogram inputs directly to MIDI-like output events for several transcription tasks. This sequence-to-sequence approach simplifies transcription by jointly modeling audio features and language-like output dependencies, thus removing the need for task-specific architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labeling rather than custom model design."
Ben Hayes;Charalampos Saitis;Gyorgy Fazekas,Neural Waveshaping Synthesis,2021,https://doi.org/10.5281/zenodo.5624613,"Ben Hayes, Centre for Digital Music, Queen Mary University of London;Charalampos Saitis, Centre for Digital Music, Queen Mary University of London;George Fazekas, Centre for Digital Music, Queen Mary University of London","We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools."
Johannes Hentschel;Fabian C. Moss;Markus Neuwirth;Martin A Rohrmeier,A semi-automated workflow paradigm for the distributed creation and curation of expert annotations,2021,https://doi.org/10.5281/zenodo.5624417,"Johannes Hentschel, École Polytechnique Fédérale de Lausanne, Switzerland;Fabian C. Moss, École Polytechnique Fédérale de Lausanne, Switzerland;Markus Neuwirth, Anton Bruckner University Linz, Austria;Martin Rohrmeier, École Polytechnique Fédérale de Lausanne, Switzerland","The creation and curation of labeled datasets can be an arduous, expensive, and time-consuming task. We introduce a workflow paradigm for remote consensus-building between expert annotators, while considerably reducing the associated administrative overhead through automation. Most music annotation tasks rely heavily on human interpretation and therefore defy the concept of an objective and indisputable ground truth. Thus, our paradigm invites and documents inter-annotator controversy based on a transparent set of analytical criteria, and aims at putting forth the consensual solutions emerging from such deliberations. The workflow that we suggest traces the entire genesis of annotation data, including the relevant discussions between annotators, reviewers, and curators. It adopts a well-proven pattern from collaborative software development, namely distributed version control, and allows for the automation of repetitive maintenance tasks, such as validity checks, message dispatch, or updates of meta- and paradata. To demonstrate the workflow's effectiveness, we introduce one possible implementation through GitHub Actions and showcase its success in creating cadence, phrase, and harmony annotations for a corpus of 36 trio sonatas by Arcangelo Corelli. Both code and annotated scores are freely available and the implementation can be readily used in and adapted for other MIR projects."
Mojtaba Heydari;Frank Cwitkowitz;Zhiyao Duan,"BeatNet: CRNN and Particle Filtering for Online Joint Beat, Downbeat and Meter Tracking",2021,https://doi.org/10.5281/zenodo.5624577,"Mojtaba Heydari, Department of Electrical and Computer Engineering, University of Rochester;Frank Cwitkowitz, Department of Electrical and Computer Engineering, University of Rochester;Zhiyao Duan, Department of Electrical and Computer Engineering, University of Rochester","The online estimation of rhythmic information, such as beat positions, downbeat positions, and meter, is critical for many real-time music applications. Musical rhythm comprises complex hierarchical relationships across time, rendering its analysis intrinsically challenging and at times subjective. Furthermore, systems which attempt to estimate rhythmic information in real-time must be causal and must produce estimates quickly and efficiently. In this work, we introduce an online system for joint beat, downbeat, and meter tracking, which utilizes causal convolutional and recurrent layers, followed by a pair of sequential Monte Carlo particle filters applied during inference. The proposed system does not need to be primed with a time signature in order to perform downbeat tracking, and is instead able to estimate meter and adjust the predictions over time. Additionally, we propose an information gate strategy to significantly decrease the computational cost of particle filtering during the inference step, making the system much faster than previous sampling-based methods. Experiments on the GTZAN dataset, which is unseen during training, show that the system outperforms various online beat and downbeat tracking systems and achieves comparable performance to a baseline offline joint method."
Yuki Hiramatsu;Eita Nakamura;Kazuyoshi Yoshii,Joint Estimation of Note Values and Voices for Audio-to-Score Piano Transcription,2021,https://doi.org/10.5281/zenodo.5624411,"Yuki Hiramatsu, Graduate School of Informatics, Kyoto University, Japan;Eita Nakamura, Graduate School of Informatics, Kyoto University, Japan;Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, Japan","This paper describes an essential improvement of a state-of-the-art automatic piano transcription (APT) system that can transcribe a human-readable symbolic musical score from a piano recording. Whereas estimation of the pitches and onset times of musical notes has been improved drastically thanks to the recent advances of deep learning, estimation of note values and voice labels, which is a crucial component of the APT system, still remains a challenging task. A previous study has revealed that (i) the pitches and onset times of notes are useful but the performed note durations are less informative for estimating the note values and that (ii) the note values and voices have mutual dependency. We thus propose a bidirectional long short-term memory network that jointly estimates note values and voice labels from note pitches and onset times estimated in advance. To improve the robustness against tempo errors, extra notes, and missing notes included in the input data, we investigate data augmentation. The experimental results show the efficacy of multi-task learning and data augmentation, and the proposed method achieved better accuracies than existing methods."
Yo-Wei Hsiao;Li Su,Learning note-to-note affinity for voice segregation and melody line identification of symbolic music data,2021,https://doi.org/10.5281/zenodo.5624479,"Yo-Wei Hsiao, Institute of Information Science, Academia Sinica, Taiwan;Li Su, Institute of Information Science, Academia Sinica, Taiwan","Voice segregation, melody line identification and other tasks of identifying the horizontal elements of music have been developed independently, although their purposes are similar. In this paper, we propose a unified framework to solve the voice segregation and melody line identification tasks of symbolic music data. To achieve this, a neural network model is trained to learn note-to-note affinity values directly from their contextual notes, in order to represent a music piece as a weighted undirected graph, with the affinity values being the edge weights. Individual voices or streams are then obtained with spectral clustering over the learned graph. Conditioned on minimal prior knowledge, the framework can achieve state-of-the-art performance on both tasks, and further demonstrates strong advantages on simulated real-world symbolic music data with missing notes and asynchronous chord notes."
Jui-Yang Hsu;Li Su,VOCANO: A note transcription framework for singing voice in polyphonic music,2021,https://doi.org/10.5281/zenodo.5624383,"Jui-Yang Hsu, Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan;Li Su, Institute of Information Science, Academia Sinica, Taipei, Taiwan","High variability of singing voice and insufficiency of note event annotation present a huge bottleneck in singing voice transcription (SVT). In this paper, we present VOCANO, an open-source VOCAl NOte transcription framework built upon robust neural networks with multi-task and semi-supervised learning. Based on a state-of-the-art SVT method, we further consider virtual adversarial training (VAT), a semi-supervised learning (SSL) method for SVT on both clean and accompanied singing voice data, the latter being pre-processed using the singing voice separation (SVS) technique. The proposed framework outperforms the state of the arts on public benchmarks over a wide variety of evaluation metrics. The effects of the types of training models and the sizes of the unlabeled datasets on the performance of SVT are also discussed."
Rujing Huang;Bob L. T. Sturm;Andre Holzapfel,De-centering the West: East Asian Philosophies and the Ethics of Applying Artificial Intelligence to Music,2021,https://doi.org/10.5281/zenodo.5624543,"Rujing Huang, Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Stockholm;Bob L. T. Sturm, Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Stockholm;Andre Holzapfel, Division of Media Technology and Interaction Design, KTH Royal Institute of Technology, Stockholm","Questions about the ethical dimensions of artificial intelligence (AI) become more pressing as its applications multiply. While there is a growing literature calling attention to the ethics of AI in general, sector-specific and culturally sensitive approaches remain under-explored. We thus initiate an effort to establish a framework of ethical guidelines for music AI in the context of East Asia, a region whose rapid technological advances are playing a leading role in contemporary geopolitical competition. We draw a connection between technological ethics and non-Western philosophies such as Confucianism, Buddhism, Shintoism, and Daoism. We emphasize interrelations between AI and traditional cultural heritage and values. Drawing on the IEEE Principles of Ethically Aligned Design, we map its proposed ethical principles to East Asian contexts and their respective music ecosystem. In this process of establishing a culturally situated understanding of AI ethics, we see that the seemingly universal concepts of ""human rights"", ""well-being"", and potential ""misuse"" are ultimately fluid and need to be carefully examined in specific cultural contexts."
Tun Min Hung;Bo-Yu Chen;Yen Tung Yeh;Yi-Hsuan Yang,A Benchmarking Initiative for Audio-domain Music Generation using the FreeSound Loop Dataset,2021,https://doi.org/10.5281/zenodo.5624409,"Tun-Min Hung1,2, Academia Sinica;Bo-Yu Chen1, Academia Sinica;Yen-Tung Yeh1,2, Academia Sinica;Yi-Hsuan Yang1,3, Academia Sinica;, Taiwan AI Labs","This paper proposes a new benchmark task for generating musical passages in the audio domain by using the drum loops from the FreeSound Loop Dataset, which are publicly re-distributable. Moreover, we use a larger collection of drum loops from Looperman to establish four model-based objective metrics for evaluation, releasing these metrics as a library for quantifying and facilitating the progress of musical audio generation. Under this evaluation framework, we benchmark the performance of three recent deep generative adversarial network (GAN) models we customize to generate loops, including StyleGAN, StyleGAN2, and UNAGAN. We also report a subjective evaluation of these models. Our evaluation shows that the one based on StyleGAN2 performs the best in both objective and subjective metrics."
Hsiao-Tzu Hung;Joann Ching;Seungheon Doh;Nabin Kim;Juhan Nam;Yi-Hsuan Yang,EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation,2021,https://doi.org/10.5281/zenodo.5624519,"Hsiao-Tzu Hung1,2, Academia Sinica, Taiwan;Joann Ching1, Academia Sinica, Taiwan;Seungheon Doh3, Graduate School of Culture Technology, KAIST, South Korea;Nabin Kim4, Georgia Institute of Technology, United States;Juhan Nam3, Graduate School of Culture Technology, KAIST, South Korea;Yi-Hsuan Yang1, Academia Sinica, Taiwan","<p>While there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced &#39;yee-m&ograve;-pi-uh&#39;) dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the methodology for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks.</p>"
Kevin Ji;Daniel Yang;Timothy Tsai,Piano Sheet Music Identification Using Marketplace Fingerprinting,2021,https://doi.org/10.5281/zenodo.5624375,"Kevin Ji, Harvey Mudd College;Daniel Yang, Harvey Mudd College;TJ Tsai, Harvey Mudd College","This paper studies the problem of identifying piano sheet music based on a cell phone image of all or part of a physical page. We re-examine current best practices for large-scale sheet music retrieval through an economics perspective. In our analogy, the runtime search is like a consumer shopping in a store. The items on the shelves correspond to fingerprints, and purchasing an item corresponds to doing a fingerprint lookup in the database. From this perspective, we show that previous approaches are extremely inefficient marketplaces in which the consumer has very few choices and adopts an irrational buying strategy. The main contribution of this work is to propose a novel fingerprinting scheme called marketplace fingerprinting. This approach redesigns the system to be an efficient marketplace in which the consumer has many options and adopts a rational buying strategy that explicitly considers the cost and expected utility of each item. We also show that deciding which fingerprints to include in the database poses a type of minimax problem in which the store and the consumer have competing interests. On experiments using all solo piano sheet music images in IMSLP as a searchable database, we show that marketplace fingerprinting substantially outperforms previous approaches and achieves a mean reciprocal rank of 0.905 with sub-second average runtime."
Keunhyoung Kim;Jongpil Lee;Sangeun Kum;Juhan Nam,Learning a cross-domain embedding space of vocal and mixed audio with a structure-preserving triplet loss,2021,https://doi.org/10.5281/zenodo.5625674,"Keunhyoung Luke Kim, Neutune Research, Seoul, South Korea;Jongpil Lee, Neutune Research, Seoul, South Korea;Sangeun Kum, Neutune Research, Seoul, South Korea;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea","Recent advances of music source separation have achieved high quality of vocal isolation from mix audio. This has paved the way for various applications in the area of music informational retrieval (MIR). In this paper, we propose a method to learn a cross-domain embedding space between isolated vocal and mixed audio for vocal-centric MIR tasks, leveraging a pre-trained music source separation model. Learning the cross-domain embedding was previously attempted with a triplet-based similarity model where vocal and mixed audio are encoded by two different convolutional neural networks. We improve the approach with a structure-preserving triplet loss that exploits not only cross-domain similarity between vocal and mixed audio but also intra-domain similarity within vocal tracks or mix tracks. We learn vocal embedding using a large-scaled dataset and evaluate it in singer identification and query-by-singer tasks. In addition, we use the vocal embedding for vocal-based music tagging and artist classification in transfer learning settings. We show that the proposed model significantly improves the previous cross-domain embedding model, particularly when the two embedding spaces from isolated vocals and mixed audio are concatenated."
Qiuqiang Kong;Yin Cao;Haohe Liu;Keunwoo Choi;Yuxuan Wang,Decoupling Magnitude and Phase Estimation with Deep ResUNet for Music Source Separation,2021,https://doi.org/10.5281/zenodo.5624475,"Qiuqiang Kong, ByteDance;Yin Cao, University of Surrey;Haohe Liu, ByteDance;Keunwoo Choi, ByteDance;Yuxuan Wang, ByteDance","Deep neural network based methods have been successfully applied to music source separation. They typically learn a mapping from a mixture spectrogram to a set of source spectrograms, all with magnitudes only. This approach has several limitations: 1) its incorrect phase reconstruction degrades the performance, 2) it limits the magnitude of masks between 0 and 1 while we observe that 22% of time-frequency bins have ideal ratio mask values of over~1 in a popular dataset, MUSDB18, 3) its potential on very deep architectures is under-explored. Our proposed system is designed to overcome these. First, we propose to estimate phases by estimating complex ideal ratio masks (cIRMs) where we decouple the estimation of cIRMs into magnitude and phase estimations. Second, we extend the separation method to effectively allow the magnitude of the mask to be larger than~1. Finally, we propose a residual UNet architecture with up to 143 layers. Our proposed system achieves a state-of-the-art MSS result on the MUSDB18 dataset, especially, a SDR of 8.98 dB on vocals, outperforming the previous best performance of 7.24 dB. The source code is available at: https://github.com/bytedance/music_source_separation."
Filip Korzeniowski;Sergio Oramas;Fabien Gouyon,Artist Similarity Using Graph Neural Networks,2021,https://doi.org/10.5281/zenodo.5625676,"Filip Korzeniowski, Pandora Media LLC., Oakland, California, USA;Sergio Oramas, Pandora Media LLC., Oakland, California, USA;Fabien Gouyon, Pandora Media LLC., Oakland, California, USA","Artist similarity plays an important role in organizing, understanding, and subsequently, facilitating discovery in large collections of music. In this paper, we present a hybrid approach to computing similarity between artists using graph neural networks trained with triplet loss. The novelty of using a graph neural network architecture is to combine the topology of a graph of artist connections with content features to embed artists into a vector space that encodes similarity.  To evaluate the proposed method, we compile the new OLGA dataset, which contains artist similarities from AllMusic, together with content features from AcousticBrainz. With 17,673 artists, this is the largest academic artist similarity dataset that includes content-based features to date.  Moreover, we also showcase the scalability of our approach by experimenting with a much larger proprietary dataset.  Results show the superiority of the proposed approach over current state-of-the-art methods for music similarity.  Finally, we hope that the OLGA dataset will facilitate research on data-driven models for artist similarity."
Jin Ha Lee;Arpita Bhattacharya;Ria Antony;Nicole Santero;Anh Le,"""Finding Home"": Understanding How Music Supports Listeners' Mental Health through a Case Study of BTS",2021,https://doi.org/10.5281/zenodo.5624569,"Jin Ha Lee, University of Washington;Arpita Bhattacharya, University of California, Irvine;Ria Antony, University of Washington;Nicole Santero, University of Nevada, Las Vegas;Anh Le, University of Washington","The positive impact of music on people's mental health and wellbeing has been well researched in music psychology, but there is a dearth of research exploring the implications of these benefits for the design of commercial music services (CMS). In this paper, we investigate how popular music can support the listener's mental health through a case study of fans of the music group BTS, with a goal of understanding how they perceive and describe the way music is influencing their mental health. We aim to derive specific design implications for CMS to facilitate such support for fans' mental health and wellbeing. Through an online survey of 1190 responses, we identify and discuss the patterns of seven different mood regulations along with major themes on fans' lived experiences of how BTS's music (1) provides comfort, (2) catalyzes self-growth, and (3) facilitates coping. We conclude the study with discussion of four specific suggestions for CMS features incorporating (1) visual elements, (2) non-music media, (3) user-generated content for collective sense-making, and (4) metadata related to mood and lyrical content that can facilitate the mental health support provided by popular music."
Harin Lee;Frank Höger;Marc Schönwiesner;Minsu Park;Nori Jacoby,Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood Detection Algorithms,2021,https://doi.org/10.5281/zenodo.5625680,"Harin Lee, Max Planck Institute for Human Cognitive and Brain Sciences;Frank Höger, Max Planck Institute for Empirical Aesthetics;Marc Schönwiesner, Leipzig University;Minsu Park, New York University Abu Dhabi;Nori Jacoby, Max Planck Institute for Empirical Aesthetics","Do people from different cultural backgrounds perceive the mood in music the same way? How closely do human ratings across different cultures approximate automatic mood detection algorithms that are often trained on corpora of predominantly Western popular music? Analyzing 166 participants' responses from Brazil, South Korea, and the US, we examined the similarity between the ratings of nine categories of perceived moods in music and estimated their alignment with four popular mood detection algorithms. We created a dataset of 360 recent pop songs drawn from major music charts of the countries and constructed semantically identical mood descriptors across English, Korean, and Portuguese languages. Multiple participants from the three countries rated their familiarity, preference, and perceived moods for a given song. Ratings were highly similar within and across cultures for basic mood attributes such as sad, cheerful, and energetic. However, we found significant cross-cultural differences for more complex characteristics such as dreamy and love. To our surprise, the results of mood detection algorithms were uniformly correlated across human ratings from all three countries and did not show a detectable bias towards any particular culture. Our study thus suggests that the mood detection algorithms can be considered as an objective measure at least within the popular music context."
Jordan Lenchitz,Reconsidering quantization in MIR,2021,https://doi.org/10.5281/zenodo.5624645,"Jordan Lenchitz, College of Music, Florida State University","This paper presents a critique of the ubiquity of boilerplate quantizations in MIR research relative to the paucity of engagement with their methodological implications. The wide-ranging consequences of reflexivity on the future of scholarly inquiry combined with the near-universal contemporary recognition of the need to broaden the scope of MIR research invite and merit critical attention. To that end, focusing primarily on twelve-tone equal-tempered pitch and dyadic rhythm models, we explore the practical, cultural, perceptual, historical, and epistemological consequences of these pervasive quantizations. We analyze several case studies of meaningful and successful past research that balanced practicality with methodological validity in order to posit several best practices for both future intercultural studies and research centered on more narrowly constructed corpora. We conclude with a discussion of the dangers of solutionism on the one hand and the self-fulfilling prophecies of status quoism on the other as well as an emphasis on the need for intellectual honesty in metatheoretical discourse."
Liwei Lin;Gus Xia;Qiuqiang Kong;Junyan Jiang,"A unified model for zero-shot music source separation, transcription and synthesis",2021,https://doi.org/10.5281/zenodo.5624623,"Liwei Lin, Music X Lab, New York University Shanghai;Qiuqiang Kong, ByteDance, Shanghai, China;Junyan Jiang, Music X Lab, New York University Shanghai;Gus Xia, Music X Lab, New York University Shanghai","<p>We propose a unified model for three inter-related tasks: 1) to separate&nbsp;individual sound sources from a mixed music audio, 2) to transcribe&nbsp;each sound source to MIDI notes, and 3) to synthesize&nbsp;new pieces based on the timbre of separated sources. The model is inspired by the fact that when humans listen to music, our minds can not only separate the sounds of different instruments, but also at the same time perceive high-level representations such as score and timbre. To mirror such capability computationally, we designed a pitch-timbre disentanglement module based on a popular encoder-decoder neural architecture for source separation. The key inductive biases are vector-quantization for pitch representation and pitch-transformation invariant for timbre representation. In addition, we adopted a query-by-example method to achieve zero-shot&nbsp;learning, i.e., the model is capable of doing source separation, transcription, and synthesis for unseen&nbsp;instruments. The current design focuses on audio mixtures of two monophonic instruments. Experimental results show that our model outperforms existing multi-task baselines, and the transcribed score serves as a powerful auxiliary for separation tasks.</p>"
Carlos Lordelo;Emmanouil Benetos;Simon Dixon;Sven Ahlbäck,Pitch-Informed Instrument Assignment using a Deep Convolutional Network with Multiple Kernel Shapes,2021,https://doi.org/10.5281/zenodo.5625682,"Carlos Lordelo, Queen Mary University of London, UK;Emmanouil Benetos, Queen Mary University of London, UK;Simon Dixon, Queen Mary University of London, UK;Sven Ahlbäck, Doremir Music Research AB, Sweden","This paper proposes a deep convolutional neural network for performing note-level instrument assignment. Given a polyphonic multi-instrumental music signal along with its ground truth or predicted notes, the objective is to assign an instrumental source for each note. This problem is addressed as a pitch-informed classification task where each note is analysed individually. We also propose to utilise several kernel shapes in the convolutional layers in order to facilitate learning of timbre-discriminative feature maps. Experiments on the MusicNet dataset using 7 instrument classes show that our approach is able to achieve an average F-score of 0.904 when the original multi-pitch annotations are used as the pitch information for the system, and that it also excels if the note information is provided using third-party multi-pitch estimation algorithms. We also include ablation studies investigating the effects of the use of multiple kernel shapes and comparing different input representations for the audio and the note-related information."
Wei-Tsung Lu;Ju-Chiang Wang;Minz Won;Keunwoo Choi;Xuchen Song,SpecTNT: a Time-Frequency Transformer for Music Audio,2021,https://doi.org/10.5281/zenodo.5624503,"Wei-Tsung Lu, ByteDance, Mountain View, California, United States;Ju-Chiang Wang, ByteDance, Mountain View, California, United States;Minz Won, ByteDance, Mountain View, California, United States;Keunwoo Choi, ByteDance, Mountain View, California, United States;Xuchen Song, ByteDance, Mountain View, California, United States","Transformers have drawn attention in the MIR field for their remarkable performance shown in natural language processing and computer vision. However, prior works in the audio processing domain mostly use Transformer as a temporal feature aggregator that acts similar to RNNs. In this paper, we propose SpecTNT, a Transformer-based architecture to model both spectral and temporal sequences of an input time-frequency representation. Specifically, we introduce a novel variant of the Transformer-in-Transformer (TNT) architecture. In each SpecTNT block, a spectral Transformer extracts frequency-related features into the frequency class token (FCT) for each frame. Later, the FCTs are linearly projected and added to the temporal embeddings (TEs), which aggregate useful information from the FCTs. Then, a temporal Transformer processes the TEs to exchange information across the time axis. By stacking the SpecTNT blocks, we build the SpecTNT model to learn the representation for music signals. In experiments, SpecTNT demonstrates state-of-the-art performance in music tagging and vocal melody extraction, and shows competitive performance for chord recognition. The effectiveness of SpecTNT and other design choices are further examined through ablation studies."
Néstor Nápoles López;Mark R H Gotham;Ichiro Fujinaga,AugmentedNet: A Roman Numeral Analysis Network with Synthetic Training Examples and Additional Tonal Tasks,2021,https://doi.org/10.5281/zenodo.5624533,"Néstor Nápoles López, McGill University, CIRMMT;Mark Gotham, Universität des Saarlandes;Ichiro Fujinaga, McGill University, CIRMMT","AugmentedNet is a new convolutional recurrent neural network for predicting Roman numeral labels. The network architecture is characterized by a separate convolutional block for bass and chromagram inputs. This layout is further enhanced by using synthetic training examples for data augmentation, and a greater number of tonal tasks to solve simultaneously via multitask learning. This paper reports the improved performance achieved by combining these ideas. The additional tonal tasks strengthen the shared representation learned through multitask learning. The synthetic examples, in turn, complement key transposition, which is often the only technique used for data augmentation in similar problems related to tonal music. The name ""AugmentedNet"" speaks to the increased number of both training examples and tonal tasks. We report on tests across six relevant and publicly available datasets: ABC, BPS, HaydnSun, TAVERN, When-in-Rome, and WTC. In our tests, our model outperforms recent methods of functional harmony, such as other convolutional neural networks and Transformer-based models. Finally, we show a new method for reconstructing the full Roman numeral label, based on common Roman numeral classes, which leads to better results compared to previous methods."
Vincenzo Madaghiele;Pasquale Lisena;Raphael Troncy,MINGUS: Melodic Improvisation Neural Generator Using Seq2Seq,2021,https://doi.org/10.5281/zenodo.5625684,"Vincenzo Madaghiele, EURECOM, Sophia Antipolis, France;Pasquale Lisena, EURECOM, Sophia Antipolis, France;Raphaël Troncy, EURECOM, Sophia Antipolis, France","Sequence to Sequence (Seq2Seq) approaches have shown good performances in automatic music generation. We introduce MINGUS, a Transformer-based Seq2Seq architecture for modelling and generating monophonic jazz melodic lines.  MINGUS relies on two dedicated embedding models (respectively for pitch and duration) and exploits in prediction features such as chords (current and following), bass line, position inside the measure.   The obtained results are comparable with the state of the art of music generation with neural models, with particularly good performances on jazz music."
"Lizé Masclef, Ninon;Andrea Vaglio;Manuel Moussallam",User-centered evaluation of lyrics-to-audio alignment,2021,https://doi.org/10.5281/zenodo.5625688,"Ninon Lizé Masclef, Deezer Research;Andrea Vaglio, LTCI, Télécom Paris, Institut Polytechnique de Paris;Manuel Moussallam, Deezer Research","<p>The growing interest for Human-centered MIR motivates the development of perceptually-grounded evaluation metrics. Despite remarkable progress of lyrics-to-audio alignment systems in recent years, one thing remaining unresolved is whether the metrics employed to assess their performance are perceptually grounded. Even if a tolerance window for errors was fixed at 0.3s for the MIREX challenge, no experiment was conducted to confer psychological validity to this threshold. Following an interdisciplinary approach, fueled by psychology and musicology insights, we consider the lyrics-to-audio alignment evaluation from a user-centered perspective. In this paper, we call into question the perceptual robustness of the most used metric to evaluate this task. We investigate the perception of audio and lyrics synchrony through two realistic experimental settings inspired from karaoke, and discuss implications for evaluation metrics. The most striking features of these results are the asymmetrical perceptual thresholds of synchrony perception between lyrics and audio, as well as the influence of rhythmic factors on them.</p>"
Naotake Masuda;Daisuke Saito,Synthesizer Sound Matching with Differentiable DSP,2021,https://doi.org/10.5281/zenodo.5624609,"Naotake Masuda, The University of Tokyo;Daisuke Saito, The University of Tokyo","While synthesizers have become commonplace in music production, many users find it difficult to control the parameters of a synthesizer to create the intended sound. In order to assist the user, the sound matching task aims to estimate synthesis parameters that produce a sound closest to the query sound. Recently, neural networks have been employed for this task. These neural networks are trained on paired data of synthesis parameters and the corresponding output sound, optimizing a loss of synthesis parameters. However, synthesis parameters are only indirectly correlated with the audio output. Another problem is that query made by the user usually consists of real-world sounds, different from the synthesizer output used during training.   In this paper, we propose a novel approach to the problem of synthesizer sound matching by implementing a basic subtractive synthesizer using differentiable DSP modules. This synthesizer has interpretable controls and is similar to those used in music production. We can then train an estimator network by directly optimizing the spectral similarity of the synthesized output. Furthermore, we can train the network on real-world sounds whose ground-truth synthesis parameters are unavailable. We pre-train the network with parameter loss and fine-tune the model with spectral loss using real-world sounds. We show that the proposed method finds better matches compared to baseline models."
Andrew Mcleod;Martin A Rohrmeier,A Modular System for the Harmonic Analysis of Musical Scores using a Large Vocabulary,2021,https://doi.org/10.5281/zenodo.5655391,"Andrew McLeod, EPFL;Martin Rohrmeier, EPFL","The harmonic analysis of a musical composition is a fundamental step towards understanding its structure. Central to this analysis is the labeling of segments of a piece with chord symbols and local key information. In this work, we propose a modular system for performing such a harmonic analysis, incorporating spelled pitches (i.e., not treating enharmonically equivalent pitches as identical) and using a very large vocabulary of 1540 chords (each with a root, type, and inversion) and 70 keys (with a tonic and mode), leading to a full harmonic characterization similar to Roman numeral analysis. Our system's modular design allows each of its components to model an aspect of harmony at an appropriate level of granularity, and also aids in both flexibility and interpretability. We show that our system improves upon a state-of-the-art model for the task, both on a previously available corpus consisting mostly of pieces from the Classical and Romantic eras of Western music, as well as on a much larger corpus spanning a wider range from the 16th through the 20th centuries."
Gianluca Micchi;Katerina Kosta;Gabriele Medeot;Pierre Chanquion,A deep learning method for enforcing coherence in Automatic Chord Recognition,2021,https://doi.org/10.5281/zenodo.5624539,"Gianluca Micchi, ;Katerina Kosta, ;Gabriele Medeot, ;Pierre Chanquion, ;ByteDance, ","Deep learning approaches to automatic chord recognition and functional harmonic analysis of symbolic music have improved the state of the art, but they still face a common problem: how to deal with a vast chord vocabulary. The naive approach of writing one output class for each possible chord is hindered by the combinatorial explosion of the output size (~10 million classes). We can reduce this complexity by several orders of magnitude by treating each label (e.g. key or chord quality) independently. However this has been shown to lead to incoherent output labels. To solve this issue we introduce a modified Neural Autoregressive Distribution Estimation (NADE) as the last layer of a Convolutional Recurrent Neural Network. The NADE layer ensures that labels related to the same chord are dependently predicted, therefore enforcing coherence. The experiments showcase the advantage of the new model both in automatic chord recognition and functional harmonic analysis compared to the model that does not include NADE as well as State of the Art models."
Martin A Miguel;Diego Fernandez Slezak,Modeling beat uncertainty as a 2D distribution of period and phase: a MIR task proposal,2021,https://doi.org/10.5281/zenodo.5624639,"Martin A. Miguel1, 2, Universidad de Buenos Aires. Facultad de Ciencias Exactas y Naturales. Departamento de Computación. Buenos Aires, Argentina.;Diego Fernández Slezak1, 2, CONICET-Universidad de Buenos Aires. Instituto de Investigación en Ciencias de la Computación (ICC). Buenos Aires, Argentina.","This work proposes modeling the beat percept as a 2d probability distribution and its inference from musical stimulus as a new MIR task. We present a methodology for collecting a 2d beat distribution of period and phase from free beat-tapping data from multiple participants. The methodology allows capturing beat-tapping variability both within (e.g.: mid-track beat change) and between annotators (e.g.: participants tap at different phases). The data analysis methodology was tested with simulated beat tracks assessing robustness to tapping variability, mid-tapping beat change and disagreement between annotators. It was also tested on experimental tapping data where the entropy of the estimated beat distributions correlated with tapping difficulty reported by the participants. For the MIR task, we propose using optimal transport as an evaluation criterion for models that estimate the beat distribution from musical stimuli. This criterion provides better scores to beat estimations closer in phase or period to distributions obtained from data. Finally, we present baseline models for the task of estimating the beat distribution. The methodology is presented with aims to enhance the exploration of ambiguity in the beat percept. For example, it exposes if beat uncertainty is related to a pulse that is hard to produce or conflicting interpretations of the beat."
Olof Misgeld;Torbjörn L Gulz;Jūra Miniotaitė;Andre Holzapfel,A case study of deep enculturation and sensorimotor synchronization to real music,2021,https://doi.org/10.5281/zenodo.5624537,"Olof Misgeld, KMH Royal College of Music, Sweden;Torbjörn Gulz, KMH Royal College of Music, Sweden;Andre Holzapfel, KTH Royal Institute of Technology, Sweden;J¯ura Miniotait˙e, KTH Royal Institute of Technology, Sweden","Synchronization of movement to music is a behavioural capacity that separates humans from most other species. Whereas such movements have been studied using a wide range of methods, only few studies have investigated synchronisation to real music stimuli in a cross-culturally comparative setting. The present study employs beat tracking evaluation metrics and accent histograms to analyze the differences in the ways participants from two cultural groups synchronize their tapping with either familiar or unfamiliar music stimuli. Instead of choosing two apparently remote cultural groups, we selected two groups of musicians that share cultural backgrounds, but that differ regarding the music style they specialize in. The employed method to record tapping responses in audio format facilitates a fine-grained analysis of metrical accents that emerge from the responses. The identified differences between groups are related to the metrical structures inherent to the two musical styles, such as non-isochronicity of the beat, and differences between the groups document the influence of the deep enculturation of participants to their style of expertise. Besides these findings, our study sheds light on a conceptual weakness of a common beat tracking evaluation metric, when applied to human tapping instead of machine generated beat estimations."
Gautam Mittal;Jesse Engel;Curtis Hawthorne;Ian Simon,Symbolic Music Generation with Diffusion Models,2021,https://doi.org/10.5281/zenodo.5624363,"Gautam Mittal, University of California, Berkeley;Jesse Engel, Google Brain;Curtis Hawthorne, Google Brain;Ian Simon, Google Brain","Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings."
Faraaz Nadeem,Learning from Musical Feedback with Sonic the Hedgehog,2021,https://doi.org/10.5281/zenodo.5624541,"Faraaz Nadeem, Massachusetts Institute of Technology","Most videogame reinforcement learning (RL) research only deals with the video component of games, even though humans typically play while experiencing both audio and video. In this paper, we aim to bridge this gap in research, and present two main contributions. First, we provide methods for extracting, processing, visualizing, and hearing gameplay audio alongside video. Then, we show that in Sonic The Hedgehog, agents provided with both audio and video can outperform agents with access to only video by 6.6% on a joint training task, and 20.4% on a zero-shot transfer task. We conclude that game audio informs useful decision making, and that audio features are more easily transferable to unseen test levels than video features."
Javier Nistal;Stefan Lattner;Gaël Richard,DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis With GANs,2021,https://doi.org/10.5281/zenodo.5624507,"Javier Nistal, LTCI, Telecom Paris, IP Paris, France;Stefan Lattner, Sony Computer Science Laboratories (CSL), Paris, France;Gaël Richard, LTCI, Telecom Paris, IP Paris, France","Generative Adversarial Networks (GANs) have achieved excellent audio synthesis quality in the last years. However, making them operable with semantically meaningful controls remains an open challenge. An obvious approach is to control the GAN by conditioning it on metadata contained in audio datasets. Unfortunately, audio datasets often lack the desired annotations, especially in the musical domain. A way to circumvent this lack of annotations is to generate them, for example, with an automatic audio-tagging system. The output probabilities of such systems (so-called ""soft labels"") carry rich information about the characteristics of the respective audios and can be used to distill the knowledge from a teacher model into a student model. In this work, we perform knowledge distillation from a large audio tagging system into an adversarial audio synthesizer that we call DarkGAN. Results show that DarkGAN can synthesize musical audio with acceptable quality and exhibits moderate attribute control even with out-of-distribution input conditioning. We release the code and provide audio examples on the accompanying website."
Takehisa Oyama;Ryoto Ishizuka;Kazuyoshi Yoshii,Phase-Aware Joint Beat and Downbeat Estimation Based on Periodicity of Metrical Structure,2021,https://doi.org/10.5281/zenodo.5624517,"Takehisa Oyama, Graduate School of Informatics, Kyoto University;Ryoto Ishizuka, Graduate School of Informatics, Kyoto University;Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, PRESTO, Japan Science and Technology Agency (JST)","This paper describes a phase-aware joint beat and downbeat estimation method mainly intended for popular music with a periodic metrical structure and steady tempo. The conventional approach to beat estimation is to train a deep neural network (DNN) that estimates the beat presence probability at each frame. This approach, however, relies heavily on a periodicity-aware post-processing step that detects beat times from the noisy probability sequence. To mitigate this problem, we have designed a DNN that estimates the beat phase at each frame whose period is equal to the beat interval. The estimation losses computed at all frames not limited to a fewer number of beat frames can thus be effectively used for backpropagation-based supervised training, whereas a DNN has conventionally been trained such that it constantly outputs zero at all non-beat frames. The same applies to downbeat estimation. We also modify the post-processing method for the estimated phase sequence. For joint beat and downbeat detection, we investigate multi-task learning architectures that output beat and downbeat phases in this order, in reverse order, and in parallel. The experimental results demonstrate the importance of phase modeling for stable beat and downbeat estimation."
Yuto Ozaki;John M Mcbride;Emmanouil Benetos;Peter Pfordresher;Joren Six;Adam Tierney;Polina Proutskova;Emi Sakai;Haruka Kondo;Haruno Fukatsu;Shinya Fujii;Patrick E. Savage,Agreement Among Human and Automated Transcriptions of Global Songs,2021,https://doi.org/10.5281/zenodo.5624529,"Yuto Ozaki, Keio University;John McBride, Center for Soft and Living Matter, Institute for Basic Science;Emmanouil Benetos, Queen Mary University of London;Peter Q. Pfordresher, University at Buffalo;Joren Six, Ghent University;Adam T. Tierney, Birkbeck, University of London;Polina Proutskova, Queen Mary University of London;Emi Sakai, ;Haruka Kondo, Keio University;Haruno Fukatsu, Keio University;Shinya Fujii, Keio University;Patrick E. Savage, Keio University","Cross-cultural musical analysis requires standardized symbolic representation of sounds such as score notation. However, transcription into notation is usually conducted manually by ear, which is time-consuming and subjective. Our aim is to evaluate the reliability of existing methods for transcribing songs from diverse societies. We had 3 experts independently transcribe a sample of 32 excerpts of traditional monophonic songs from around the world (half a cappella, half with instrumental accompaniment). 16 songs also had pre-existing transcriptions created by 3 different experts. We compared these human transcriptions against one another and against 10 automatic music transcription algorithms. We found that human transcriptions can be sufficiently reliable (~90% agreement, κ ~.7), but current automated methods are not (&lt;60% agreement, κ &lt;.4). No automated method clearly outperformed others, in contrast to our predictions. These results suggest that improving automated methods for cross-cultural music transcription is critical for diversifying MIR."
Emilia Parada-Cabaleiro;Maximilian Schmitt;Anton Batliner;Bjorn W. Schuller;Markus Schedl,Automatic Recognition of Texture in Renaissance Music,2021,https://doi.org/10.5281/zenodo.5624443,"Emilia Parada-Cabaleiro, Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz, Austria;Maximilian Schmitt, ;Anton Batliner, ;Björn Schuller, ;Markus Schedl, Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz, Austria","Renaissance music constitutes a resource of immense richness for Western culture, as shown by its central role in digital humanities. Yet, despite the advance of computational musicology in analysing other Western repertoires, the use of computer-based methods to automatically retrieve relevant information from Renaissance music, e. g., identifying word-painting strategies such as madrigalisms, is still underdeveloped. To this end, we propose a score-based machine learning approach for the classification of texture in Italian madrigals of the 16th century. Our outcomes indicate that Low Level Descriptors, such as intervals, can successfully convey differences in High Level features, such as texture. Furthermore, our baseline results, particularly the ones from a Convolutional Neural Network, show that machine learning can be successfully used to automatically identify sections in madrigals associated with specific textures from symbolic sources."
Ashis Pati;Alexander Lerch,Is Disentanglement enough? On Latent Representations for Controllable Music Generation,2021,https://doi.org/10.5281/zenodo.5624591,"Ashis Pati, Center for Music Technology, Georgia Institute of Technology, USA;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology, USA","Improving controllability or the ability to manipulate one or more attributes of the generated data has become a topic of interest in the context of deep generative models of music. Recent attempts in this direction have relied on learning disentangled representations from data such that the underlying factors of variation are well separated. In this paper, we focus on the relationship between disentanglement and controllability by conducting a systematic study using different supervised disentanglement learning algorithms based on the Variational Auto-Encoder (VAE) architecture. Our experiments show that a high degree of disentanglement can be achieved by using different forms of supervision to train a strong discriminative encoder. However, in the absence of a strong generative decoder, disentanglement does not necessarily imply controllability. The structure of the latent space with respect to the VAE-decoder plays an important role in boosting the ability of a generative model to manipulate different attributes. To this end, we also propose methods and metrics to help evaluate the quality of a latent space with respect to the afforded degree of controllability."
Nicolás Pironio;Diego Fernandez Slezak;Martin A Miguel,Pulse clarity metrics developed from a deep learning beat tracking model,2021,https://doi.org/10.5281/zenodo.5625692,"Nicolás Pironio, Universidad de Buenos Aires. Facultad de Ciencias Exactas y Naturales. Departamento de Computación. Buenos Aires, Argentina.;Diego Fernández Slezak, Universidad de Buenos Aires. Facultad de Ciencias Exactas y Naturales. Departamento de Computación. Buenos Aires, Argentina.;Martín A. Miguel, Universidad de Buenos Aires. Facultad de Ciencias Exactas y Naturales. Departamento de Computación. Buenos Aires, Argentina.","In this paper we present novel pulse clarity metrics based on different sections of a state-of-the-art beat tracking model. Said model consists of two sections: a recurrent neural network that estimates beat probabilities for audio and a dynamic Bayesian network (DBN) that determines beat moments from the neural network's output. We obtained pulse clarity metrics by analyzing periodical behavior from neuron activation values and we interpreted the probability distribution computed by the DBN as the model's certainty. To analyze whether the inner workings of the model provide new insight into pulse clarity, we also proposed reference metrics using the output of both networks. We evaluated the pulse clarity metrics over a wide range of stimulus types such as songs and mono-tonal rhythms, obtaining comparable results to previous models. These results suggest that adapting a model from a related task is feasible for the pulse clarity problem. Additionally, results of the evaluation of pulse clarity models on multiple datasets showed that, with some variability, both ours and previous work generalized well beyond their original training datasets."
Verena Praher;Katharina Prinz;Arthur Flexer;Gerhard Widmer,"On the Veracity of Local, Model-agnostic Explanations in Audio Classification: Targeted Investigations with Adversarial Examples",2021,https://doi.org/10.5281/zenodo.5624471,"Verena Praher, Johannes Kepler University Linz, Austria;Katharina Prinz, Johannes Kepler University Linz, Austria;Arthur Flexer, Johannes Kepler University Linz, Austria;Gerhard Widmer, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria","Local explanation methods such as LIME have become popular in MIR as  tools for generating post-hoc, model-agnostic explanations of a  model's classification decisions. The basic idea is to identify  a small set of human-understandable features of the classified example that are most influential on the classifier's prediction.  These are then presented as an explanation.  Evaluation of such explanations in publications often resorts to  accepting what matches the expectation of a human without actually being able to verify if what the explanation shows is what really caused the model's prediction.  This paper reports on targeted investigations where we try to get more insight into the actual veracity of LIME's explanations in an audio classification task.  We deliberately design adversarial examples for the classifier, in a way that  gives us knowledge about which parts of the input are potentially responsible  for the model's (wrong) prediction. Asking LIME to explain the predictions for these adversaries permits us to study whether local explanations do indeed detect these regions  of interest. We also look at whether LIME is more successful in finding perturbations that are more prominent and easily noticeable for a human.   Our results suggest that LIME does not necessarily manage to identify the most relevant input features and hence it remains unclear whether explanations are useful or even misleading."
Laure Prétet;Gaël Richard;Geoffroy Peeters,"Is there a ""language of music-video clips"" ? A qualitative and quantitative study",2021,https://doi.org/10.5281/zenodo.5625696,"Laure Prétet, LTCI, Télécom Paris, Bridge.audio, Paris, France;Gaël Richard, LTCI, Télécom Paris, IP Paris, France;Geoffroy Peeters, LTCI, Télécom Paris, IP Paris, France","Recommending automatically a video given a music or a music given a video has become an important asset for the audiovisual industry - with user-generated or professional content.  While both music and video have specific temporal organizations, most current works do not consider those and only focus on globally recommending a media.  As a first step toward the improvement of these recommendation systems, we study in this paper the relationship between music and video temporal organization.  We do this for the case of official music videos, with a quantitative and a qualitative approach.  Our assumption is that the movement in the music are correlated to the ones in the video.  To validate this, we first interview a set of internationally recognized music video experts.  We then perform a large-scale analysis of official music-video clips (which we manually annotated into video genres) using MIR description tools (downbeats and functional segments estimation) and Computer Vision tools (shot detection).  Our study confirms that a ""language of music-video clips"" exists; i.e. editors favor the co-occurrence of music and video events using strategies such as anticipation.   It also highlights that the amount of co-occurrence depends on the music and video genres."
Gowriprasad R;Venkatesh V;Hema A Murthy;R Aravind;Sri Rama Murty K,Tabla Gharana Recognition from Audio music recordings of Tabla Solo performances,2021,https://doi.org/10.5281/zenodo.5624631,"Gowriprasad R, Indian Institute of Technology, Madras;Venkatesh V, Indian Institute of Technology, Hyderabad;Hema A Murthy, Indian Institute of Technology, Madras;R Aravind, Indian Institute of Technology, Madras;Sri Rama Murty K, Indian Institute of Technology, Hyderabad","Tabla is a percussion instrument in Hindustani music tradition. Tabla learning and performance in the Indian subcontinent is based on stylistic schools called gharana-s. Each gharana is characterized by its unique style of playing technique, dynamics of tabla strokes, repertoire, compositions, and improvisations. Identifying the gharana from a tabla performance is hence helpful to characterize the performance. This paper addresses the task of automatic gharana recognition from solo tabla recordings. We motivate the problem and present different facets and challenges in the task. We present a comprehensive and diverse collection of over 16 hours of tabla solo recordings for the task. We propose an approach using deep learning models that use a combination of convolutional neural networks (CNN) and long short-term memory (LSTM) networks. The CNNs are used to extract gharana discriminative features from the raw audio data. The LSTM networks are trained to classify the gharana-s by processing the sequence of extracted features from CNNs. Our experiments on gharana recognition include different lengths of audio data and comparison between various aspects of the task. An evaluation demonstrates promising results with the highest recognition accuracy of 93%."
Lindsey Reymore;Emmanuelle Beauvais-Lacasse;Bennett Smith;Stephen Mcadams,Navigating noise: Modeling perceptual correlates of noise-related semantic timbre categories with audio features,2021,https://doi.org/10.5281/zenodo.5624469,"Lindsey Reymore, Schulich School of Music, McGill University, Canada;Emmanuelle Beauvais-Lacasse, Schulich School of Music, McGill University, Canada;Bennett Smith, Schulich School of Music, McGill University, Canada;Stephen McAdams, Schulich School of Music, McGill University, Canada","Audio features such as inharmonicity, noisiness, and spectral roll-off have been identified as correlates of ""noisy"" sounds; however, such features are likely involved in the experience of multiple semantic timbre categories of varied meaning and valence. This paper examines the relationships among audio features and the semantic timbre categories raspy/grainy/rough, harsh/noisy, and airy/breathy. Participants (n = 153) rated a random subset of 52 stimuli from a set of 156 ~2-second orchestral instrument sounds from varied instrument families, registers, and playing techniques. Stimuli were rated on the three semantic categories of interest and on perceived playing effort and emotional valence. With an updated version of the Timbre Toolbox (R-2021 A), we extracted 44 summary audio features from the stimuli using spectral and harmonic representations. These features were used as input for various models built to predict mean semantic ratings (raspy/grainy/rough, harsh/noisy, airy/breathy) for each sound. Random Forest models predicting semantic ratings from audio features outperformed Partial Least-Squares Regression models, consistent with previous results suggesting non-linear methods are advantageous in timbre semantic predictions using audio features. In comparing Relative Variable Importance measures from the models among the three semantic categories, results demonstrate that although these related semantic categories are associated in part with overlapping features, they can be differentiated through individual patterns of feature relationships."
Kyle Robinson;Dan Brown,Quantitative User Perceptions of Music Recommendation List Diversity,2021,https://doi.org/10.5281/zenodo.5624535,"Kyle Robinson, David R. Cheriton School of Computer Science, University of Waterloo, Canada;Dan Brown, David R. Cheriton School of Computer Science, University of Waterloo, Canada","Diversity is known to play an important role in recommender systems. However, its relationship to users and their satisfaction is not well understood, especially in the music domain. We present a user study: 92 participants were asked to evaluate personalized recommendation lists at varying levels of diversity. Recommendations were generated by two different collaborative filtering methods, and diversified in three different ways, one of which is a simple and novel method based on genre filtering. All diversified lists were recognised by users to be more diverse, and this diversification increased overall recommendation list satisfaction. Our simple filtering approach was also successful at tailoring diversity to some users. Within the collaborative filtering framework, however, we were not able to generate enough diversity to match all user preferences. Our results highlight the need to diversify in music recommendation lists, even when it comes at the cost of ""accuracy""."
Martin A Rohrmeier;Fabian C. Moss,A Formal Model of Extended Tonal Harmony,2021,https://doi.org/10.5281/zenodo.5624635,"Martin Rohrmeier, Digital and Cognitive Musicology Lab, EPFL;Fabian C. Moss, Digital and Cognitive Musicology Lab, EPFL","Extended tonality is a central system that characterizes the music from the 19th up to the 21st century, including styles like popular music, film music or Jazz. Developing from classical major-minor tonality, the harmonic language of extended tonality forms its own set of rules and regularities, which are a result of the freer combinatoriality of chords within phrases, non-standard chord forms, the emancipation of dissonance, and the loosening of the concept of key. These phenomena posit a challenge for formal, mathematical theory building. The theoretical model proposed in this paper proceeds from Neo-Riemannian and Tonfeld theory, a systematic but informal music-theoretical framework for extended tonality. Our model brings together three fundamental components: the underlying algebraic structure of the Tonnetz, the three basic analytical categories from Tonfeld theory (octatonic and hexatonic collections as well as stacks of fifths), and harmonic syntax in terms of formal language theory. The proposed model is specified to a level of detail that lends itself for implementation and empirical investigation."
Simon Rouard;Gaëtan Hadjeres,CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis,2021,https://doi.org/10.5281/zenodo.5624403,"Simon Rouard, Sony CSL - CentraleSupélec;Gaëtan Hadjeres, Sony CSL","In this paper, we propose a novel score-base generative model for unconditional raw audio synthesis.  Our proposal builds upon the latest developments on diffusion process modeling with stochastic differential equations, which already demonstrated promising results on image generation.  We motivate novel heuristics for the choice of the diffusion processes better suited for audio generation, and consider the use of a conditional U-Net to approximate the score function. While previous approaches on diffusion models on audio were mainly designed as speech vocoders in medium resolution, our method termed CRASH (Controllable Raw Audio Synthesis with High-resolution) allows us to generate short percussive sounds in 44.1kHz in a controllable way.  Through extensive experiments, we showcase on a drum sound generation task the numerous sampling schemes offered by our method (unconditional generation, deterministic generation, inpainting, interpolation, variations, class-conditional sampling) and propose the class-mixing sampling, a novel way to generate ""hybrid"" sounds.  Our proposed method offers flexible generation capabilities with lighter and easier-to-train models than GAN-based methods."
Luke O Rowe;George Tzanetakis,Curriculum Learning for Imbalanced Classification in Large Vocabulary Automatic Chord Recognition,2021,https://doi.org/10.5281/zenodo.5624463,"Luke Rowe, University of Victoria;George Tzanetakis, University of Victoria","A problem inherent to the task of large vocabulary automatic chord recognition (ACR) is that the distribution over the chord qualities typically exhibits power-law characteristics. This intrinsic imbalance makes it difficult for ACR systems to learn the rare chord qualities in a large chord vocabulary. While recent ACR systems have exploited the hierarchical relationships that exist between chord qualities, few have attempted to exploit these relationships explicitly to improve the classification of rare chord qualities.    In this paper, we propose a convolutional Transformer model for the task of ACR trained on a dataset of 1217 tracks over a large chord vocabulary consisting of 170 chord types. In order to address the class imbalance of the chord quality distribution, we incorporate the hierarchical relationships between chord qualities into a curriculum learning training scheme that gradually learns the rare and complex chord qualities in the dataset. We show that the proposed convolutional Transformer model achieves state-of-the-art performance on traditional ACR evaluation metrics. Furthermore, we show that the proposed curriculum learning training scheme outperforms existing methods in improving the classification of rare chord qualities."
Justin Salamon;Oriol Nieto;Nicholas J. Bryan,Deep Embeddings and Section Fusion Improve Music Segmentation,2021,https://doi.org/10.5281/zenodo.5624371,"Justin Salamon, Adobe Research;Oriol Nieto, Adobe Research;Nicholas J. Bryan, Adobe Research","Music segmentation algorithms identify the structure of a music recording by automatically dividing it into sections and determining which sections repeat and when. Since the desired granularity of the sections may vary by application, multi-level segmentation produces several levels of segmentation ordered by granularity from one section (the whole song) up to N unique sections, and has proven to be a challenging MIR task. In this work we propose a multi-level segmentation method that leverages deep audio embeddings learned via other tasks. Our approach builds on an existing multi-level segmentation algorithm, replacing manually engineered features with deep embeddings learned through audio classification problems where data are abundant. Additionally, we propose a novel section fusion algorithm that leverages the multi-level segmentation to consolidate short segments at each level in a way that is consistent with the segmentations at lower levels. Through a series of experiments we show that replacing handcrafted features with deep embeddings can lead to significant improvements in multi-level music segmentation performance, and that section fusion further improves the results by cleaning up spurious short sections. We compare our approach to two strong baselines and show that it yields state-of-the-art results."
Antonia Saravanou;Federico Tomasi;Rishabh Mehrotra;Mounia Lalmas,Multi-Task Learning of Graph-based Inductive Representations of Music Content,2021,https://doi.org/10.5281/zenodo.5624379,"Antonia Saravanou, University of Athens;Federico Tomasi, Spotify;Rishabh Mehrotra, Spotify;Mounia Lalmas, Spotify","Music streaming platforms rely heavily on learning meaningful representations of tracks to surface apt recommendations to users in a number of different use cases. In this work, we consider the task of learning music track representations by leveraging three rich heterogeneous sources of information: (i) organizational information (e.g., playlist co-occurrence), (ii) content information (e.g., audio &amp; acoustics), and (iii) music stylistics (e.g., genre). We advocate for a multi-task formulation of graph representation learning, and propose MUSIG: Multi-task Sampling and Inductive learning on Graphs. MUSIG allows us to derive generalized track representations that combine the benefits offered by (i) the inductive graph based framework, which generates embeddings by sampling and aggregating features from a node's local neighborhood, as well as, (ii) multi-task training of aggregation functions, which ensures the learnt functions perform well on a number of important tasks. We present large scale empirical results for track recommendation for the playlist completion task, and compare different classes of representation learning approaches, including collaborative filtering, word2vec and node embeddings as well as, graph embedding approaches. Our results demonstrate that considering content information (i.e.,audio and acoustic features) is useful and that multi-task supervision helps learn better representations."
Pedro Pereira Sarmento;Adarsh Kumar;Cj Carr;Zack Zukowski;Mathieu Barthet;Yi-Hsuan Yang,DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models,2021,https://doi.org/10.5281/zenodo.5624597,"Pedro Sarmento, Queen Mary University of London;Adarsh Kumar, Academia Sinica;CJ Carr, ;Zack Zukowski, Dadabots;Mathieu Barthet, Queen Mary University of London;Yi-Hsuan Yang, Academia Sinica","Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered song-writing apps, and human-AI improvisation."
Harald Victor Schweiger;Emilia Parada-Cabaleiro;Markus Schedl,Does Track Sequence in User-generated Playlists Matter?,2021,https://doi.org/10.5281/zenodo.5624369,"Harald Schweiger, Institute of Computational Perception, Johannes Kepler University Linz (JKU), Austria;Emilia Parada-Cabaleiro, Human-centered AI Group, AI Lab, Linz Institute of Technology (LIT), Austria;Markus Schedl, Institute of Computational Perception, Johannes Kepler University Linz (JKU), Austria","The extent to which the sequence of tracks in music playlists matters to listeners is a disputed question, nevertheless a very important one for tasks such as music recommendation (e.g., automatic playlist generation or continuation). While several user studies already approached this question, results are largely inconsistent. In contrast, in this paper we take a data-driven approach and investigate 704,166 user-generated playlists of a major music streaming provider. In particular, we study the consistency (in terms of variance) of a variety of audio features and metadata between subsequent tracks in playlists, and we relate this variance to the corresponding variance computed on a position-independent set of tracks.  Our results show that some features vary on average up to 16% less among subsequent tracks in comparison to position-independent pairs of tracks. Furthermore, we show that even pairs of tracks that lie up to 12 positions apart in the playlist are significantly more consistent in several audio features and genres. Our findings yield a better understanding of how users create playlists and will stimulate further progress in sequential music recommenders."
Simon J Schwär;Sebastian Rosenzweig;Meinard Müller,A Differentiable Cost Measure for Intonation Processing in Polyphonic Music,2021,https://doi.org/10.5281/zenodo.5624601,"Simon Schwär, International Audio Laboratories Erlangen, Germany;Sebastian Rosenzweig, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Intonation is the process of choosing an appropriate pitch for a given note in a musical performance. Particularly in polyphonic singing, where all musicians can continuously adapt their pitch, this leads to complex interactions. To achieve an overall balanced sound, the musicians dynamically adjust their intonation considering musical, perceptual, and acoustical aspects. When adapting the intonation in a recorded performance, a sound engineer may have to individually fine-tune the pitches of all voices to account for these aspects in a similar way. In this paper, we formulate intonation adaptation as a cost minimization problem. As our main contribution, we introduce a differentiable cost measure by adapting and combining existing principles for measuring intonation. In particular, our measure consists of two terms, representing a tonal aspect (the proximity to a tonal grid) and a harmonic aspect (the perceptual dissonance between salient frequencies). We show that, combining these two aspects, our measure can be used to flexibly account for different artistic intents while allowing for robust and joint processing of multiple voices in real-time. In an experiment, we demonstrate the potential of our approach for the task of intonation adaptation of amateur choral music using recordings from a publicly available multitrack dataset."
Pavan M Seshadri;Alexander Lerch,Improving Music Performance Assessment With Contrastive Learning,2021,https://doi.org/10.5281/zenodo.5624481,"Pavan Seshadri, Center for Music Technology, Georgia Institute of Technology;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology","Several automatic approaches for objective music performance assessment (MPA) have been proposed in the past, however, existing systems are not yet capable of reliably predicting ratings with the same accuracy as professional judges. This study investigates contrastive learning as a potential method to improve existing MPA systems. Contrastive learning is a widely used technique in representation learning to learn a structured latent space capable of separately clustering multiple classes. It has been shown to produce state of the art results for image-based classification problems. We introduce a weighted contrastive loss suitable for regression tasks applied to a convolutional neural network and show that contrastive loss results in performance gains in regression tasks for MPA. Our results show that contrastive-based methods are able to match and exceed SoTA performance for MPA regression tasks by creating better class clusters within the latent space of the neural networks."
Dougal Shakespeare;Camille Roth,Tracing Affordance and Item Adoption on Music Streaming Platforms,2021,https://doi.org/10.5281/zenodo.5625698,"Dougal Shakespeare, Computational Social Science Team, Centre March Bloch, Berlin;Camille Roth, CNRS, France","Popular music streaming platforms offer users a diverse network of content exploration through a triad of affordances: organic, algorithmic and editorial access modes. Whilst offering great potential for discovery, such platform developments also pose the modern user with daily adoption decisions on two fronts: platform affordance adoption and the adoption of recommendations therein. Following a carefully constrained set of Deezer users over a 2-year observation period, our work explores factors driving user behaviour in the broad sense, by differentiating users on the basis of their temporal daily usage, adoption of the main platform affordances, and the ways in which they react to them, especially in terms of recommendation adoption. Diverging from a perspective common in studies on the effects of recommendation, we assume and confirm that users exhibit very diverse behaviours in using and adopting the platform affordances. The resulting complex and quite heteregeneous picture demonstrates that there is no blanket answer for adoption practices of both recommendation features and recommendations."
Zhengshan Shi,Computational analysis and modeling of expressive timing in Chopin's Mazurkas,2021,https://doi.org/10.5281/zenodo.5624515,"Zhengshan Shi, Center for Computer Research in Music and Acoustics, Stanford University","Performers' distortion of notated rhythms in a musical score is a significant factor in the production of convincingly expressive music interpretations. Sometimes exaggerated, and sometimes subtle, these distortions are driven by a variety of factors, including schematic features (both structural such as phrase boundaries and surface events such as recurrent rhythmic patterns), as well as relatively rare veridical events that characterize the individuality and uniqueness of a particular piece.   Performers tend to adopt similar pervasive approaches to interpreting schemas, resulting in common performance practices, while often formulating less common approaches to the interpretation of veridical events. Furthermore, some performers choose anomalous interpretations of schemas. We present a machine learning model of expressive performance of Chopin Mazurkas and a critical analysis of the output based upon statistical analyses of the musical scores and of recorded performances. We compare the timings of recorded human performances of selected Mazurkas by Frédéric Chopin with performances of the same works generated by a neural network trained with recorded human performances of the entire corpus. This paper demonstrates that while machine learning succeeds, to some degree, in expressive interpretation of schemata, convincingly capturing performance characteristics remains very much a work in progress."
Nithya Nadig Shikarpur;Asawari Keskar;Preeti Rao,Computational analysis of melodic mode switching in raga performance,2021,https://doi.org/10.5281/zenodo.5625702,"Nithya Shikarpur, Indian Institute of Technology Bombay;Asawari Keskar, Indian Institute of Technology Bombay;Preeti Rao, Indian Institute of Technology Bombay","Melodic mode shifting is a construct used occasionally by skilled artists in a raga performance to enhance it by bringing in temporarily shades of a different raga. In this work, we study a specific North Indian Khyal concert structure known as the Jasrangi jugalbandi where a male and female singer co-perform different ragas in an interactive fashion. The mode-shifted ragas with their relatively displaced assumed tonics comprise the identical set of scale intervals and therefore can be easily confused when performed together. With an annotated dataset based on available concerts by well-known artists, we present an analysis of the performance in terms of the raga characteristics as they are manifested through the interactive engagement. We analyse both the aspects of modal music forms, viz. the pitch distribution, representing tonal hierarchy, and the melodic phrases, across the sequence of singing turns by the two artists with reference to representative individual performances of the corresponding ragas."
Qingwei Song;Qiwei Sun;Dongsheng Guo;Haiyong Zheng,SinTra: Learning an inspiration model from a single multi-track music segment,2021,https://doi.org/10.5281/zenodo.5624355,"Qingwei Song, College of Electronic Engineering, Ocean University of China, China;Qiwei Sun, School of Microelectronics, Xi’an Jiaotong University, China;Dongsheng Guo, College of Electronic Engineering, Ocean University of China, China;Haiyong Zheng, College of Electronic Engineering, Ocean University of China, China","In this paper, we propose SinTra, an auto-regressive sequential generative model that can learn from a single multi-track music segment, to generate coherent, aesthetic, and variable polyphonic music of multi-instruments with an arbitrary length of bar. For this task, to ensure the relevance of generated samples and training music, we present a novel pitch-group representation. SinTra, consisting of a pyramid of Transformer-XL with a multi-scale training strategy, can learn both the musical structure and the relative positional relationship between notes of the single training music segment. Additionally, for maintaining the inter-track correlation, we use the convolution operation to process multi-track music, and when decoding, the tracks are independent to each other to prevent interference. We evaluate SinTra with both subjective study and objective metrics. The comparison results show that our framework can learn information from a single music segment more sufficiently than Music Transformer. Also the comparison between SinTra and its variant, i.e., the single-stage SinTra with the first stage only, shows that the pyramid structure can effectively suppress overly-fragmented notes."
Janne Spijkervet;John Ashley Burgoyne,Contrastive Learning of Musical Representations,2021,https://doi.org/10.5281/zenodo.5624573,"Janne Spijkervet, Institute for Logic, Language, and Computation, University of Amsterdam;John Ashley Burgoyne, Institute for Logic, Language, and Computation, University of Amsterdam","While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1% despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper."
Xiaoheng Sun;Qiqi He;Gao Yongwei;Wei Li,Musical Tempo Estimation Using a Multi-scale Network,2021,https://doi.org/10.5281/zenodo.5624391,"Xiaoheng Sun, School of Computer Science and Technology, Fudan University, Shanghai, China;Qiqi He, School of Computer Science and Technology, Fudan University, Shanghai, China;Yongwei Gao, School of Computer Science and Technology, Fudan University, Shanghai, China;Wei Li, School of Computer Science and Technology, Fudan University, Shanghai, China","Recently, some single-step systems without onset detection have shown their effectiveness in automatic musical tempo estimation. Following the success of these systems, in this paper we propose a Multi-scale Grouped Attention Network to further explore the potential of such methods. A multi-scale structure is introduced as the overall network architecture where information from different scales is aggregated to strengthen contextual feature learning. Furthermore, we propose a Grouped Attention Module as the key component of the network. The proposed module separates the input feature into several groups along the frequency axis, which makes it capable of capturing long-range dependencies from different frequency positions on the spectrogram. In comparison experiments, the results on public datasets show that the proposed model outperforms existing state-of-the-art methods on Accuracy1."
Pau Torras;Arnau Baró;Lei Kang;Alicia Fornés,On the Integration of Language Models into Sequence to Sequence Architectures for Handwritten Music Recognition,2021,https://doi.org/10.5281/zenodo.5624451,"Pau Torras, Computer Vision Center, Computer Science Department, Universitat Autònoma de Barcelona;Arnau Baró, Computer Vision Center, Computer Science Department, Universitat Autònoma de Barcelona;Lei Kang, Computer Vision Center, Computer Science Department, Universitat Autònoma de Barcelona;Alicia Fornés, Computer Vision Center, Computer Science Department, Universitat Autònoma de Barcelona","Despite the latest advances in Deep Learning, the recognition of handwritten music scores is still a challenging endeavour. Even though the recent Sequence to Sequence (Seq2Seq) architectures have demonstrated its capacity to reliably recognise handwritten text, their performance is still far from satisfactory when applied to historical handwritten scores. Indeed, the ambiguous nature of handwriting, the non-standard musical notation employed by composers of the time and the decaying state of old paper make these scores remarkably difficult to read, sometimes even by trained humans. Thus, in this work we explore the incorporation of language models into a Seq2Seq-based architecture to try to improve transcriptions where the aforementioned unclear writing produces statistically unsound mistakes, which as far as we know, has never been attempted for this field of research on this architecture. After studying various Language Model integration techniques, the experimental evaluation on historical handwritten music scores shows a significant improvement over the state of the art, showing that this is a promising research direction for dealing with such difficult manuscripts."
Kosetsu Tsukuda;Keisuke Ishida;Masahiro Hamasaki;Masataka Goto,Kiite Cafe: A Web Service for Getting Together Virtually to Listen to Music,2021,https://doi.org/10.5281/zenodo.5624491,"Kosetsu Tsukuda, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Keisuke Ishida, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masahiro Hamasaki, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","In light of the COVID-19 pandemic making it difficult for people to get together in person, this paper describes a public web service called Kiite Cafe that lets users get together virtually to listen to music. When users listen to music on Kiite Cafe, their experiences are characterized by two architectures: (i) visualization of each user's reactions, and (ii) selection of songs from users' favorite songs. These architectures enable users to feel social connection with others and the joy of introducing others to their favorite songs as if they were together in person to listen to music. In addition, the architectures provide three user experiences: (1) motivation to react to played songs, (2) the opportunity to listen to a diverse range of songs, and (3) the opportunity to contribute as curators. By analyzing the behavior logs of 1,760 Kiite Cafe users over about five months, we quantitatively show that these user experiences can generate various effects (e.g., users react to a more diverse range of songs on Kiite Cafe than when listening alone). We also discuss how our proposed architectures can continue to enrich music listening experiences with others even after the pandemic's resolution."
Kosetsu Tsukuda;Masahiro Hamasaki;Masataka Goto,Toward an Understanding of Lyrics-viewing Behavior While Listening to Music on a Smartphone,2021,https://doi.org/10.5281/zenodo.5624633,"Kosetsu Tsukuda, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masahiro Hamasaki, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","Why and how do people view lyrics? Although various lyrics-based systems have been proposed in MIR community, this fundamental question remains unexplored. Better understanding of lyrics viewing behavior would be beneficial for both researchers and music streaming platforms to improve their lyrics-based systems. Therefore, in this paper, we investigate why and how people view lyrics, especially when they listen to music on a smartphone. To answer ""why,"" we conduct a questionnaire-based online user survey involving 206 participants. To answer ""how,"" we analyze over 23 million lyrics request logs sent from the smartphone application of a music streaming service. Our analysis results suggest several reusable insights, including the following: (1) People have high demand for viewing lyrics to confirm what the artist sings, more deeply understand the lyrics, sing the song, and figure out the structure such as verse and chorus. (2) People like to view lyrics after returning home at night and before going to sleep rather than during the daytime. (3) People usually view the same lyrics repeatedly over time. Applying these insights, we also discuss application examples that could enable people to more actively view lyrics and listen to new songs, which would not only diversify and enrich people's music listening experiences but also be beneficial especially for music streaming platforms."
Andrea Vaglio;Romain Hennequin;Manuel Moussallam;Gael Richard,The Words Remain the Same: Cover Detection with Lyrics Transcription,2021,https://doi.org/10.5281/zenodo.5624395,"Andrea Vaglio, Deezer R&D;Romain Hennequin, Deezer R&D;Manuel Moussallam, Deezer R&D;Gaël Richard, LTCI, Télécom Paris, Institut Polytechnique de Paris","Cover detection has gained sustained interest in the scientific community and has recently made significant progress both in terms of scalability and accuracy. However, most approaches are based on the estimation of harmonic and melodic features and neglect lyrics information although it is an important invariant across covers. In this work, we propose a novel approach leveraging lyrics without requiring access to full texts though the use of lyrics recognition on audio. Our approach relies on the fusion of a singing voice recognition framework and a more classic tonal-based cover detection method. To the best of our knowledge, this is the first time that lyrics estimation from audio has been explicitly used for cover detection. Furthermore, we exploit efficient string matching and an approximated nearest neighbors search algorithm which lead to a scalable system which is able to operate on very large databases. Extensive experiments on the largest publicly available cover detection dataset demonstrate the validity of using lyrics information for this task."
Ziyu Wang;Gus Xia,MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation,2021,https://doi.org/10.5281/zenodo.5624387,"Ziyu Wang, Music X Lab, NYU Shanghai;Gus Xia, Music X Lab, NYU Shanghai","BERT has proven to be a powerful language model in natural language processing and established an effective pre-training &amp; fine-tuning methodology. We see that music, as a special form of language, can benefit from such methodology if we carefully handle its highly-structured and polyphonic properties. To this end, we propose MuseBERT and show that: 1) MuseBERT has detailed specification of note attributes and explicit encoding of music relations, without presuming any pre-defined sequential event order, 2) the pre-trained MuseBERT is not merely a language model, but also a controllable music generator, and 3) MuseBERT gives birth to various downstream music generation and analysis tasks with practical value. Experiment shows that the pre-trained model outperforms the baselines in terms of reconstruction likelihood and generation quality. We also demonstrate downstream applications including chord analysis, chord-conditioned texture generation, and accompaniment refinement."
Ju-Chiang Wang;Jordan B. L. Smith;Wei-Tsung Lu;Xuchen Song,Supervised Metric Learning For Music Structure Features,2021,https://doi.org/10.5281/zenodo.5624427,"Ju-Chiang Wang, ByteDance;Jordan B. L. Smith, ByteDance;Wei-Tsung Lu, ByteDance;Xuchen Song, ByteDance","Music structure analysis (MSA) methods traditionally search for musically meaningful patterns in audio: homogeneity, repetition, novelty, and segment-length regularity. Hand-crafted audio features such as MFCCs or chromagrams are often used to elicit these patterns. However, with more annotations of section labels (e.g., verse, chorus, bridge) becoming available, one can use supervised feature learning to make these patterns even clearer and improve MSA performance. To this end, we take a supervised metric learning approach: we train a deep neural network to output embeddings that are near each other for two spectrogram inputs if both have the same section type (according to an annotation), and otherwise far apart. We propose a batch sampling scheme to ensure the labels in a training pair are interpreted meaningfully. The trained model extracts features that can be used by existing MSA algorithms. In evaluations with three datasets (HarmonixSet, SALAMI, and RWC), we demonstrate that using the proposed features can improve a traditional MSA algorithm significantly in both intra- and cross-dataset scenarios."
Shiqi Wei;Gus Xia,Learning long-term music representations via hierarchical contextual constraints,2021,https://doi.org/10.5281/zenodo.5624351,"Shiqi Wei, School of Data Science, Fudan University;Gus Xia, Music X Lab, Computer Science Department, New York University Shanghai","<p>Learning symbolic music representations, especially disentangled representations with probabilistic interpretations, has been shown to benefit both music understanding and generation. However, most models are only applicable to short-term music, while learning long-term music representations remains a challenging task. We have seen several studies attempting to learn hierarchical representations directly in an end-to-end manner, but these models have not been able to achieve the desired results and the training process is not stable. In this paper, we propose a novel approach to learn long-term symbolic music representations through contextual constraints. First, we use contrastive learning to pre-train a long-term representation by constraining its difference from the short-term representation (extracted by an off-the-shelf model). Then, we fine-tune the long-term representation by a hierarchical prediction model such that a good long-term representation (e.g., an 8-bar representation) can reconstruct the corresponding short-term ones (e.g., the 2-bar representations within the 8-bar range). Experiments show that our method stabilizes the training and the fine-tuning steps. In addition, the designed contextual constraints benefit both reconstruction and disentanglement, significantly outperforming the baselines.</p>"
Christof Weiss;Johannes Zeitler;Tim Zunner;Florian Schuberth;Meinard Müller,Learning Pitch-Class Representations from Score-Audio Pairs of Classical Music,2021,https://doi.org/10.5281/zenodo.5624549,"Christof Weiß, International Audio Laboratories Erlangen, LTCI, Télécom Paris, Institut Polytechnique de Paris;Johannes Zeitler, International Audio Laboratories Erlangen;Tim Zunner, International Audio Laboratories Erlangen;Florian Schuberth, International Audio Laboratories Erlangen;Meinard Müller, International Audio Laboratories Erlangen","Chroma or pitch-class representations of audio recordings are an essential tool in music information retrieval. Traditional chroma features relying on signal processing are often influenced by timbral properties such as overtones or vibrato and, thus, only roughly correspond to the pitch classes indicated by a score. Deep learning provides a promising possibility to overcome such problems but requires large annotated datasets. Previous approaches therefore use either synthetic audio, MIDI-piano recordings, or chord annotations for training. Since these strategies have different limitations, we propose to learn transcription-like pitch-class representations using pre-synchronized score-audio pairs of classical music. We train several CNNs with musically inspired architectures and evaluate their pitch-class estimates for various instrumentations including orchestra, piano, chamber music, and singing. Moreover, we illustrate the learned features' behavior when used as input to a chord recognition system. In all our experiments, we compare cross-validation with cross-dataset evaluation. Obtaining promising results, our strategy shows how to leverage the power of deep learning for constructing robust but interpretable tonal representations."
Christof Weiss;Geoffroy Peeters,Training Deep Pitch-Class Representations With a Multi-Label CTC Loss,2021,https://doi.org/10.5281/zenodo.5624359,"Christof Weiß, LTCI, Télécom Paris, Institut Polytechnique de Paris, France;Geoffroy Peeters, LTCI, Télécom Paris, Institut Polytechnique de Paris, France","Despite the success of end-to-end approaches, chroma (or pitch-class) features remain a useful mid-level representation of music audio recordings due to their direct interpretability. Since traditional chroma variants obtained with signal processing suffer from timbral artifacts such as overtones or vibrato, they do not directly reflect the pitch classes notated in the score. For this reason, training a chroma representation using deep learning (""deep chroma"") has become an interesting strategy. Existing approaches involve the use of supervised learning with strongly aligned labels for which, however, only few datasets are available. Recently, the Connectionist Temporal Classification (CTC) loss, initially proposed for speech, has been adopted to learn monophonic (single-label) pitch-class features using weakly aligned labels based on corresponding score--audio segment pairs. To exploit this strategy for the polyphonic case, we propose the use of a multi-label variant of this CTC loss, the MCTC, and formalize this loss for the pitch-class scenario. Our experiments demonstrate that the weakly aligned approach achieves almost equivalent pitch-class estimates than training with strongly aligned annotations. We then study the sensitivity of our approach to segment duration and mismatch. Finally, we compare the learned features with other pitch-class representations and demonstrate their use for chord and local key recognition on classical music datasets."
Daniel Wolff;Remi Mignot;Axel Roebel,Audio Defect Detection in Music with Deep Networks,2021,https://doi.org/10.5281/zenodo.5624545,"Daniel Wolff, Analysis-Synthesis Team, UMR 9912 STMS - IRCAM, CNRS, Sorbonne Université, Paris, France;Rémi Mignot, Analysis-Synthesis Team, UMR 9912 STMS - IRCAM, CNRS, Sorbonne Université, Paris, France;Axel Roebel, Analysis-Synthesis Team, UMR 9912 STMS - IRCAM, CNRS, Sorbonne Université, Paris, France","With increasing amounts of music being digitally transferred from production to distribution, automatic means of determining media quality are needed. Protection mechanisms in digital audio processing tools have not eliminated the need of production entities located downstream the distribution chain to assess audio quality and detect defects inserted further upstream. Such analysis often relies on the received audio and scarce meta-data alone.     Deliberate use of artefacts such as clicks in popular music as well as more recent defects stemming from corruption in modern audio encodings call for data-centric and context-sensitive solutions for detection. We present a convolutional network architecture following end-to-end encoder-decoder configuration to develop detectors for two exemplary audio defects.    A click detector is trained and compared to a traditional signal processing method, with a discussion on context sensitivity. Additional post-processing is used for data augmentation and workflow simulation. The ability of our models to capture variance is explored in a detector for artefacts from decompression of corrupted MP3 compressed audio. For both tasks we describe the synthetic generation of artefacts for controlled detector training and evaluation. We evaluate our detectors on the large open-source Free Music Archive (FMA) and genre-specific datasets."
Minz Won;Keunwoo Choi;Xavier Serra,Semi-supervised Music Tagging Transformer,2021,https://doi.org/10.5281/zenodo.5624405,"Minz Won, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Keunwoo Choi, ByteDance, Mountain View, California, United States;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We present Music Tagging Transformer that is trained with a semi-supervised approach. The proposed model captures local acoustic characteristics in shallow convolutional layers, then temporally summarizes the sequence of the extracted features using stacked self-attention layers. Through a careful model assessment, we first show that the proposed architecture outperforms the previous state-of-the-art music tagging models that are based on convolutional neural networks under a supervised scheme.    The Music Tagging Transformer is further improved by noisy student training, a semi-supervised approach that leverages both labeled and unlabeled data combined with data augmentation. To our best knowledge, this is the first attempt to utilize the entire audio of the million song dataset."
Minz Won;Justin Salamon;Nicholas J. Bryan;Gautham Mysore;Xavier Serra,Emotion Embedding Spaces for Matching Music to Stories,2021,https://doi.org/10.5281/zenodo.5624483,"Minz Won, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Justin Salamon, Adobe Research, San Francisco, California, United States;Nicholas J. Bryan, Adobe Research, San Francisco, California, United States;Gautham J. Mysore, Adobe Research, San Francisco, California, United States;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Content creators often use music to enhance their stories, as it can be a powerful tool to convey emotion. In this paper, our goal is to help creators find music to match the emotion of their story. We focus on text-based stories that can be auralized (e.g., books), use multiple sentences as input queries, and automatically retrieve matching music. We formalize this task as a cross-modal text-to-music retrieval problem. Both the music and text domains have existing datasets with emotion labels, but mismatched emotion vocabularies prevent us from using mood or emotion annotations directly for matching. To address this challenge, we propose and investigate several emotion embedding spaces, both manually defined (e.g., valence/arousal) and data-driven (e.g., Word2Vec and metric learning) to bridge this gap. Our experiments show that by leveraging these embedding spaces, we are able to successfully bridge the gap between modalities to facilitate cross modal retrieval.   We show that our method can leverage the well established valence-arousal space, but that it can also achieve our goal via data-driven embedding spaces. By leveraging data-driven embeddings, our approach has the potential of being generalized to other retrieval tasks that require broader or completely different vocabularies."
Abudukelimu Wuerkaixi;Christodoulos Benetatos;Zhiyao Duan;Changshui Zhang,CollageNet: Fusing arbitrary melody and accompaniment into a coherent song,2021,https://doi.org/10.5281/zenodo.5624619,"Abudukelimu Wuerkaixi, Institute for Artiﬁcial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University Beijing, P.R.China;Christodoulos Benetatos, Department of Electrical and Computer Engineering, University of Rochester;Zhiyao Duan, Department of Electrical and Computer Engineering, University of Rochester;Changshui Zhang, Institute for Artiﬁcial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University Beijing, P.R.China","When writing pop or hip-hop music, musicians sometimes sample from other songs and fuse the samples into their own music. We propose a new task in the symbolic music domain that is similar to the music sampling practice and a  neural network model named CollageNet to fulfill this task. Specifically, given a piece of melody and an irrelevant accompaniment with the same length, we fuse them into harmonic two-track music after some necessary changes to the inputs. Besides, users are involved in the fusion process by providing controls to the amount of changes along several disentangled musical aspects: rhythm and pitch of the melody, and chord and texture of the accompaniment. We conduct objective and subjective experiments to demonstrate the validity of our model. Experimental results confirm that our model achieves significantly higher level of harmony than rule-based and data-driven baseline methods. Furthermore, the musicality of each of the tracks does not deteriorate after the transformation applied by CollageNet, which is also superior to the two baselines."
Kazuhiko Yamamoto,Human-in-the-Loop Adaptation for Interactive Musical Beat Tracking,2021,https://doi.org/10.5281/zenodo.5624651,"Kazuhiko Yamamoto, YAMAHA Corporation","In music information retrieval (MIR), beat-tracking is one of the most fundamental and important task. However, a perfect algorithm is difficult to achieve. In addition, there could be a no unique correct answer because what one interprets as a beat differs for each individual. To address this, we propose a novel human-in-the-loop user interface that allows the system to interactively adapt to a specific user and target music. In our system, the user does not need to correct all errors manually, but rather only a small portion of the errors. The system then adapts the internal neural network model to the target, and automatically corrects remaining errors. This is achieved by a novel adaptive runtime self-attention in which the adaptable parameters are intimately integrated as a part of the user interface. It enables both low-cost training using only a local context of the music piece, and by contrast, highly effective runtime adaptation using the global context. We show our framework dramatically reduces the user's effort of correcting beat tracking errors in our experiments."
Daniel Yang;Timothy Tsai,Composer Classification With Cross-Modal Transfer Learning and Musically-Informed Augmentation,2021,https://doi.org/10.5281/zenodo.5625645,"Daniel Yang, Harvey Mudd College;TJ Tsai, Harvey Mudd College","This paper studies composer style classification of piano sheet music, MIDI, and audio data. We expand upon previous work in three ways. First, we explore several musically motivated data augmentation schemes based on pitch-shifting and random removal of individual notes or groups of notes. We show that these augmentation schemes lead to dramatic improvements in model performance, of a magnitude that exceeds the benefit of pretraining on all solo piano sheet music images in IMSLP. Second, we describe a way to modify previous models in order to enable cross-model transfer learning, in which a model trained entirely on sheet music can be used to perform composer classification of audio or MIDI data. Third, we explore the performance of trained models in a 1-shot learning context, in which the model performs classification among a set of composers that are unseen in training. Our results indicate that models learn a representation of compositional style that generalizes beyond the set of composers used in training."
Daniel Yang;Kevin Ji;Timothy Tsai,Aligning Unsynchronized Part Recordings to a Full Mix Using Iterative Subtractive Alignment,2021,https://doi.org/10.5281/zenodo.5624563,"Daniel Yang, Harvey Mudd College;Kevin Ji, Harvey Mudd College;TJ Tsai, Harvey Mudd College","This paper explores an application that would enable a group of musicians in quarantine to produce a performance of a chamber work by recording each part in isolation in a completely unsynchronized manner, and then generating a synchronized performance by aligning, time scale modifying, and mixing the individual part recordings. We focus on the main technical challenge of aligning the individual part recordings against a reference ``full mix'' recording containing a performance of the work. We propose an iterative subtractive alignment approach, in which each part recording is aligned against the full mix recording and then subtracted from it. We also explore different feature representations and cost metrics to handle the asymmetrical nature of the part--full mix comparison. We evaluate our proposed approach on two different datasets: one that is a modification of the URMP dataset that presents an idealized setting, and another that contains a small set of piano trio data collected from musicians during the pandemic specifically for this study. Compared to a standard pairwise alignment approach, we find that the proposed approach has strong performance on the URMP dataset and mixed success on the more realistic piano trio data."
Mickael Zehren;Marco Alunno;Paolo Bientinesi,ADTOF: A large dataset of non-synthetic music for automatic drum transcription,2021,https://doi.org/10.5281/zenodo.5624527,"Mickaël Zehren, Umeå Universitet;Marco Alunno, Universidad EAFIT Medellín;Paolo Bientinesi, Umeå Universitet","The state-of-the-art methods for drum transcription in the presence of melodic instruments (DTM) are machine learning models trained in a supervised manner, which means that they rely on labeled datasets. The problem is that the available public datasets are limited either in size or in realism, and are thus suboptimal for training purposes. Indeed, the best results are currently obtained via a rather convoluted multi-step training process that involves both real and synthetic datasets. To address this issue, starting from the observation that the communities of rhythm games players provide a large amount of annotated data, we curated a new dataset of crowdsourced drum transcriptions. This dataset contains real-world music, is manually annotated, and is about two orders of magnitude larger than any other non-synthetic dataset, making it a prime candidate for training purposes. However, due to crowdsourcing, the initial annotations contain mistakes. We discuss how the quality of the dataset can be improved by automatically correcting different types of mistakes. When used to train a popular DTM model, the dataset yields a performance that matches that of the state-of-the-art for DTM, thus demonstrating the quality of the annotations."
Huan Zhang;Yiliang Jiang;Tao Jiang;Hu Peng,Learn by Referencing: Towards Deep Metric Learning for Singing Assessment,2021,https://doi.org/10.5281/zenodo.5624579,"Huan Zhang, School of Music, Carnegie Mellon University, Pittsburgh, US;Yiliang Jiang, Tencent Music Entertainment, Shenzhen, China;Tao Jiang, Tencent Music Entertainment, Shenzhen, China;Peng Hu, Tencent Music Entertainment, Shenzhen, China","The excellence of human singing is an important aspect of subjective, aesthetic perception of music. In this paper, we propose a novel approach to tackle Automatic Singing Assessment (ASA) task through deep metric learning. With the goal of retrieving the commonalities of good singing without explicitly engineering them, we force a triplet model to map perceptually pleasant-sounding singing performance closer to the reference track compared to others, and thus learning a joint embedding space with performance characteristics. Incorporating mid-level representations like spectrogram and chroma, this approach takes advantage of the feature learning ability of neural networks, while using the reference track as an important anchor. On our designed testing set that spans across various styles and techniques, our model outperforms traditional rule-based ASA systems."
Jingwei Zhao;Gus Xia,AccoMontage: Accompaniment Arrangement via Phrase Selection and Style Transfer,2021,https://doi.org/10.5281/zenodo.5625706,"Jingwei Zhao, Music X Lab, NYU Shanghai;Gus Xia, Music X Lab, NYU Shanghai","Accompaniment arrangement is a difficult music generation task involving intertwined constraints of melody, harmony, texture, and music structure. Existing models are not yet able to capture all these constraints effectively, especially for long-term music generation. To address this problem, we propose AccoMontage, an accompaniment arrangement system for whole pieces of music through unifying phrase selection and neural style transfer. We focus on generating piano accompaniments for folk/pop songs based on a lead sheet (i.e., melody with chord progression). Specifically, AccoMontage first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure learning-based approaches, AccoMontage introduces a novel hybrid pathway, in which rule-based optimization and deep learning are both leveraged to complement each other for high-quality generation. Experiments show that our model generates well-structured accompaniment with delicate texture, significantly outperforming the baselines."
Go Shibata;Ryo Nishikimi;Kazuyoshi Yoshii,Music structure analysis based on an LSTM-HSMM hybrid model,2020,https://doi.org/10.5281/zenodo.4245362,"Go Shibata, Graduate School of Informatics, Kyoto University, Japan;Ryo Nishikimi, Graduate School of Informatics, Kyoto University, Japan;Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, Japan","This paper describes a statistical music structure analysis method that splits an audio signal of popular music into musically meaningful sections at the beat level and classifies them into predefined categories such as intro, verse, and chorus, where beat times are assumed to be estimated in advance. A basic approach to this task is to train a recurrent neural network (e.g., long short-term memory (LSTM) network) that directly predicts section labels from acoustic features. This approach, however, suffers from frequent musically unnatural label switching because the homogeneity, repetitiveness, and duration regularity of musical sections are hard to represent explicitly in the network architecture. To solve this problem, we formulate a unified hidden semi-Markov model (HSMM) that represents the generative process of homogeneous mel-frequency cepstrum coefficients, repetitive chroma features, and mel spectra from section labels, where the emission probabilities of mel spectra are computed from the posterior probabilities of section labels predicted by an LSTM. Given these acoustic features, the most likely label sequence can be estimated with Viterbi decoding. The experimental results show that the proposed LSTM-HSMM hybrid model outperformed a conventional HSMM."
Keitaro Tanaka;Takayuki Nakatsuka;Ryo Nishikimi;Kazuyoshi Yoshii;Shigeo Morishima,Multi-instrument music transcription based on deep spherical clustering of spectrograms and pitchgrams,2020,https://doi.org/10.5281/zenodo.4245436,"Keitaro Tanaka, Waseda University, Japan;Takayuki Nakatsuka, Waseda University, Japan;Ryo Nishikimi, Kyoto University, Japan;Kazuyoshi Yoshii, Kyoto University, Japan;Shigeo Morishima, Waseda Research Institute for Science and Engineering, Japan","This paper describes a clustering-based music transcription method that estimates the piano rolls of arbitrary musical instrument parts from multi-instrument polyphonic music signals. If target musical pieces are always played by particular kinds of musical instruments, a way to obtain piano rolls is to compute the pitchgram (pitch saliency spectrogram) of each musical instrument by using a deep neural network (DNN). However, this approach has a critical limitation that it has no way to deal with musical pieces including undefined musical instruments. To overcome this limitation, we estimate a condensed pitchgram with an existing instrument-independent neural multi-pitch estimator and then separate the pitchgram into a specified number of musical instrument parts with a deep spherical clustering technique. To improve the performance of transcription, we propose a joint spectrogram and pitchgram clustering method based on the timbral and pitch characteristics of musical instruments. The experimental results show that the proposed method can transcribe musical pieces including unknown musical instruments as well as those containing only predefined instruments, at the state-of-the-art transcription accuracy."
Omar A Peracha,Improving polyphonic music models with feature-rich encoding,2020,https://doi.org/10.5281/zenodo.4245396,"Omar Peracha, Humtap (research conducted independently)","This paper explores sequential modelling of polyphonic music with deep neural networks. While recent breakthroughs have focussed on network architecture, we demonstrate that the representation of the sequence can make an equally significant contribution to the performance of the model as measured by validation set loss. By extracting salient features inherent to the training dataset, the model can either be conditioned on these features or trained to predict said features as extra components of the sequences being modelled. We show that training a neural network to predict a seemingly more complex sequence, with extra features included in the series being modelled, can improve overall model performance significantly. We first introduce TonicNet, a GRU-based model trained to initially predict the chord at a given time-step before then predicting the notes of each voice at that time-step, in contrast with the typical approach of predicting only the notes. We then evaluate TonicNet on the canonical JSB Chorales dataset and obtain state-of-the-art results."
Brian Manolovitz;Mitsunori Ogihara,Practical evaluation of repeated recommendations in personalized music discovery,2020,https://doi.org/10.5281/zenodo.4245516,"Brian Manolovitz, University of Miami;Mitsunori Ogihara, University of Miami","Studies have shown that repeated exposures to novel songs cause an increase in a person's memory and liking. These studies are commonly verified through self-reporting emotion-based surveys. This paper proposes the ""retention rate"" as an additional parameter for evaluation. The ""retention rate"" is one at which the listener revisits the novel items. The authors hypothesize that when a person listens to novel (i.e., both unfamiliar and interesting) pieces of music, the retention rate will be proportional to the number of times the discovery engine suggests the pieces to her, as long as they remain novel. The authors have tested the hypothesis through a six-week human-subject experiment that simulates a real-world listening environment and a follow-up survey. During the experiment period, each subject received, through Discover Weekly in Spotify, suggestions for novel songs up to three times and provided evaluation. One month after the evaluation experiment, the human-subjects answered whether they had revisited the novel songs. Through the analysis of the response and survey data, the researchers conclude that the more times a listener is exposed to a song during the discovery process, the more likely she is to return to the song."
Hendrik Schreiber;Frank Zalkow;Meinard Müller,Modeling and estimating local tempo: A case study on Chopin's Mazurkas,2020,https://doi.org/10.5281/zenodo.4245546,"Hendrik Schreiber, International Audio Laboratories Erlangen, Germany;Frank Zalkow, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Even though local tempo estimation promises musicological insights into expressive musical performances, it has never received as much attention in the music information retrieval (MIR) research community as either beat tracking or global tempo estimation. One reason for this may be the lack of a generally accepted definition. In this paper, we discuss how to model and measure local tempo in a musically meaningful way using a cross-version dataset of Frédéric Chopin's Mazurkas as a use case. In particular, we explore how tempo stability can be measured and taken into account during evaluation. Comparing existing and newly trained systems, we find that CNN-based approaches can accurately measure local tempo even for expressive classical music, if trained on the target genre. Furthermore, we show that different training–test splits have a considerable impact on accuracy for difficult segments."
Michael Krause;Frank Zalkow;Christof Weiss;Meinard Müller,Classifying leitmotifs in recordings of operas by Richard Wagner,2020,https://doi.org/10.5281/zenodo.4245472,"Michael Krause, International Audio Laboratories Erlangen, Germany;Frank Zalkow, International Audio Laboratories Erlangen, Germany;Julia Zalkow, International Audio Laboratories Erlangen, Germany;Christof Weiß, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","From the 19th century on, several composers of Western opera made use of leitmotifs (short musical ideas referring to semantic entities such as characters, places, items, or feelings) for guiding the audience through the plot and illustrating the events on stage. A prime example of this compositional technique is Richard Wagner's four-opera cycle Der Ring des Nibelungen. Across its different occurrences in the score, a leitmotif may undergo considerable musical variations. The concrete leitmotif instances in an audio recording are subject to acoustic variability. Our paper approaches the task of classifying such leitmotif instances in audio recordings. As our main contribution, we conduct a case study on a dataset covering 16 recorded performances of the Ring with annotations of ten central leitmotifs, leading to 2403 occurrences and 38448 instances in total. We build a neural network classification model and evaluate its ability to generalize across different performances and leitmotif occurrences. Our findings demonstrate the possibilities and limitations of leitmotif classification in audio recordings and pave the way towards the fully automated detection of leitmotifs in music recordings."
Timothy Tsai;Kevin Ji,Composer style classification of piano sheet music images using language model pretraining,2020,https://doi.org/10.5281/zenodo.4245398,"TJ Tsai, Harvey Mudd College;Kevin Ji, Harvey Mudd College","This paper studies composer style classification of piano sheet music images.  Previous approaches to the composer classification task have been limited by a scarcity of data.  We address this issue in two ways: (1) we recast the problem to be based on raw sheet music images rather than a symbolic music format, and (2) we propose an approach that can be trained on unlabeled data.  Our approach first converts the sheet music image into a sequence of musical ``words"" based on the bootleg feature representation, and then feeds the sequence into a text classifier.  We show that it is possible to significantly improve classifier performance by first training a language model on a set of unlabeled data, initializing the classifier with the pretrained language model weights, and then finetuning the classifier on a small amount of labeled data.  We train AWD-LSTM, GPT-2, and RoBERTa language models on all piano sheet music images in IMSLP.  We find that transformer-based architectures outperform CNN and LSTM models, and pretraining boosts classification accuracy for the GPT-2 model from 46% to 70% on a 9-way classification task.  The trained model can also be used as a feature extractor that projects piano sheet music into a feature space that characterizes compositional style."
Frank Zalkow;Meinard Müller,Using weakly aligned score–audio pairs to train deep chroma models for cross-modal music retrieval,2020,https://doi.org/10.5281/zenodo.4245400,"Frank Zalkow, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Many music information retrieval tasks involve the comparison of a symbolic score representation with an audio recording. A typical strategy is to compare score–audio pairs based on a common mid-level representation, such as chroma features. Several recent studies demonstrated the effectiveness of deep learning models that learn task-specific mid-level representations from temporally aligned training pairs. However, in practice, there is often a lack of strongly aligned training data, in particular for real-world scenarios. In our study, we use weakly aligned score–audio pairs for training, where only the beginning and end of a score excerpt is annotated in an audio recording, without aligned correspondences in between. To exploit such weakly aligned data, we employ the Connectionist Temporal Classification (CTC) loss to train a deep learning model for computing an enhanced chroma representation. We then apply this model to a cross-modal retrieval task, where we aim at finding relevant audio recordings of Western classical music, given a short monophonic musical theme in symbolic notation as a query. We present systematic experiments that show the effectiveness of the CTC-based model for this theme-based retrieval task."
Daniel Yang;Timothy Tsai,Camera-based piano sheet music identification,2020,https://doi.org/10.5281/zenodo.4245476,"Daniel Yang, Harvey Mudd College;TJ Tsai, Harvey Mudd College","This paper presents a method for large-scale retrieval of piano sheet music images.  Our work differs from previous studies on sheet music retrieval in two ways.  First, we investigate the problem at a much larger scale than previous studies, using all solo piano sheet music images in the entire IMSLP dataset as a searchable database.  Second, we use cell phone images of sheet music as our input queries, which lends itself to a practical, user-facing application.  We show that a previously proposed fingerprinting method for sheet music retrieval is far too slow for a real-time application, and we diagnose its shortcomings.  We propose a novel hashing scheme called dynamic n-gram fingerprinting that significantly reduces runtime while simultaneously boosting retrieval accuracy.  In experiments on IMSLP data, our proposed method achieves a mean reciprocal rank of 0.85 and an average runtime of 0.98 seconds per query."
Louis Spinelli;Josephine Lau;Jin Ha Lee,User perceptions underlying social music behavior,2020,https://doi.org/10.5281/zenodo.4245474,"Louis Spinelli, University of Washington;Josephine Lau, University of Washington;Jin Ha Lee, University of Washington","While prior studies investigating the social aspects of music provide a landscape of users' various social behaviors around commercial music services (CMS), there remains a lack in understanding of users' perceptions and value judgments underlying these behaviors. Specifically, there is more to learn about what influences and behaviors individual music users perceive as meaningful in social contexts. We used the Q methodology to explore which behaviors and influences are important to CMS users and why. We extracted two factors that explain the two different viewpoints shared by groups of music users, focusing on how they perceive the meaning and value of different social music behavior and interactions. From these findings, we then revise an existing social music coding dictionary and interaction model and offer new CMS design insights."
Yuchen Yuan;Sho Oishi;Charles Cronin;Daniel Müllensiefen;Quentin Atkinson;Shinya Fujii;Patrick E. Savage,Perceptual vs. automated judgements of music copyright infringement,2020,https://doi.org/10.5281/zenodo.4245364,"Yuchen Yuan, Keio University, Japan;Sho Oishi, Keio University, Japan;Charles Cronin, George Washington University Law School, USA;Daniel Müllensiefen, Goldsmiths, University of London, UK;Quentin Atkinson, University of Auckland, New Zealand;Shinya Fujii, Keio University, Japan;Patrick E. Savage, Keio University, Japan","Music copyright lawsuits often result in multimillion dollar damage awards or settlements, yet there are few objective guidelines for applying copyright law in in-fringement claims involving musical works. Recent re-search has attempted to develop objective methods based on automated similarity algorithms, but there remains almost no data on the role of perceived similarity in mu-sic copyright decisions despite its crucial role in copy-right law. We collected perceptual data from 20 partici-pants for 17 adjudicated copyright cases from the USA and Japan after editing the disputed sections to contain either full audio, melody only, or lyrics only. Due to the historical emphasis in legal opinions on melody as the key criterion for deciding infringement, we predicted that listening to melody-only versions would result in percep-tual judgements that more closely matched actual past legal decisions. Surprisingly, however, we found no sig-nificant differences between the three conditions, with participants matching past decisions in between 50-60% of cases in all three conditions. Automated algorithms designed to calculate melodic and audio similarity pro-duced comparable results: both algorithms were able to match past decisions with identical accuracy of 71% (12/17 cases). Analysis of cases that were difficult to classify suggests that melody, lyrics, and other factors sometimes interact in complex ways difficult to capture using quantitative metrics. We propose directions for fur-ther investigation of the role of similarity in music copy-right law using larger and more diverse samples of cases and enhanced methods, and adapting our perceptual ex-periment method to avoid relying for ground truth data only on court decisions (which may be subject to selec-tion bias). Our results contribute to important practical debates, such as whether jury members should be allowed to listen to full audio recordings during copyright cases."
Claire Savard;Erin H Bugbee;Melissa R McGuirl;Katherine M. Kinnaird,SuPP & MaPP: Adaptable structure-based representations for MIR tasks,2020,https://doi.org/10.5281/zenodo.4245438,"Claire Savard, Department of Physics, University of Colorado-Boulder, USA;Erin H. Bugbee, Department of Biostatistics, Brown University, USA;Melissa R. McGuirl, Division of Applied Mathematics, Brown University, USA;Katherine M. Kinnaird, Department of Computer Science and Program in Statistical & Data Sciences, Smith College, USA","Accurate and flexible representations of music data are paramount to addressing MIR tasks, yet many of the existing approaches are difficult to interpret or rigid in nature. This work introduces two new song representations for structure-based retrieval methods: Surface Pattern Preservation (SuPP), a continuous song representation, and Matrix Pattern Preservation (MaPP), SuPP's discrete counterpart. These representations come equipped with several user-defined parameters so that they are adaptable for a range of MIR tasks. Experimental results show MaPP as successful in addressing the cover song task on a set of Mazurka scores, with a mean precision of 0.965 and recall of 0.776. SuPP and MaPP also show promise in other MIR applications, such as novel-segment detection and genre classification, the latter of which demonstrates their suitability as inputs for machine learning problems."
Yaolong Ju;Sylvain Margot;Cory McKay;Luke Dahn;Ichiro Fujinaga,Automatic figured bass annotation using the new bach chorales figured bass dataset,2020,https://doi.org/10.5281/zenodo.4245512,"Yaolong Ju, Schulich School of Music, McGill University, Canada;Sylvain Margot, Schulich School of Music, McGill University, Canada;Cory McKay, Department of Liberal and Creative Arts, Marianopolis College, Canada;Luke Dahn, School of Music, The University of Utah, USA;Ichiro Fujinaga, Schulich School of Music, McGill University, Canada","This paper focuses on the computational study of figured bass, which remains an under-researched topic in MIR, likely due to a lack of machine-readable datasets. First, we introduce the Bach Chorales Figured Bass dataset (BCFB), a collection of 139 chorales composed by Johann Sebastian Bach that includes both the original music and figured bass annotations encoded in MusicXML, **kern, and MEI formats. We also present a comparative study on automatic figured bass annotation using both rule-based and machine learning approaches, which respectively achieved classification accuracies of 85.3% and 85.9% on BCFB. Finally, we discuss promising areas for MIR research involving figured bass, including automatic harmonic analysis."
Phillip B Kirlin,A corpus-based analysis of syncopated patterns in ragtime,2020,https://doi.org/10.5281/zenodo.4245514,"Phillip B. Kirlin, Department of Mathematics and Computer Science, Rhodes College","In this paper, we build on and extend a number of previous studies of rhythmic patterns that occur in ragtime music.  All of these studies have used the RAG-C dataset of approximately 11,000 symbolically-encoded ragtime pieces to identify salient rhythmic patterns in the corpus and  qualify how they are used.  Ragtime music is distinguished from other musical genres by frequent use of syncopation, and previous computational studies have confirmed a number of musicological hypotheses regarding the use of syncopated patterns in ragtime compositions.  In this work, we extend these studies to investigate further questions involving the use of syncopation.  Specifically, we introduce a new methodological framework for processing the RAG-C dataset and confirm that experiments from previous studies obtain similar results using the new methodology.  We investigate the use of the common ``short-long-short'' syncopated pattern in different time periods and present new results detailing its use by three well-known ragtime composers.  We describe how the use of other syncopated patterns has evolved over time and the different distributions of patterns that result from those changes.  Lastly, we present novel results identifying statistically significant patterns in the way composers varied the amount of syncopation in consecutive measures in compositions."
Florian Thalmann;Kazuyoshi Yoshii;Thomas Wilmering;Wiggins Geraint;Mark B. Sandler,A method for analysis of shared structure in large music collections using techniques from genetic sequencing and graph theory,2020,https://doi.org/10.5281/zenodo.4245440,"Florian Thalmann, Speech and Audio Processing Laboratory, Kyoto University;Kazuyoshi Yoshii, Speech and Audio Processing Laboratory, Kyoto University;Thomas Wilmering, Centre for Digital Music, Queen Mary University of London;Geraint A.Wiggins, Artificial Intelligence Lab, Vrije Universiteit Brussel;Mark B. Sandler, Centre for Digital Music, Queen Mary University of London","While common approaches to automatic structural analysis of music typically focus on individual audio files, our approach collates audio features of large sets of related files in order to find a shared musical temporal structure. The content of each individual file and the differences between them can then be described in relation to this shared structure. We first construct a large similarity graph of temporal segments, such as beats or bars, based on self-alignments and selected pair-wise alignments between the given input files. Part of this graph is then partitioned into groups of corresponding segments using multiple sequence alignment. This partitioned graph is searched for recurring sections which can be organized hierarchically based on their co-occurrence. We apply our approach to discover shared harmonic structure in a dataset containing a large number of different live performances of a number of songs. Our evaluation shows that using the joint information from a number of files has the advantage of evening out the noisiness or inaccuracy of the underlying feature data and leads to a robust estimate of shared musical material."
Woosung Choi;Minseok Kim;Jaehwa Chung;Daewon Lee;Soonyoung Jung,Investigating U-Nets with various intermediate blocks for spectrogram-based singing voice separation,2020,https://doi.org/10.5281/zenodo.4245404,"Woosung Choi, Department of Computer Science and Engineering, Korea University, Republic of Korea;Minseok Kim, Department of Computer Science and Engineering, Korea University, Republic of Korea;Jaehwa Chung, Department of Computer Science, Korea National Open University, Republic of Korea;Daewon Lee, Department of Computer Engineering, Seokyeong University, Republic of Korea;Soonyoung Jung, Department of Computer Science and Engineering, Korea University, Republic of Korea","Singing Voice Separation (SVS) tries to separate singing voice from a given mixed musical signal. Recently, many U-Net-based models have been proposed for the SVS task, but there were no existing works that evaluate and compare various types of intermediate blocks that can be used in the U-Net architecture. In this paper, we introduce a variety of intermediate spectrogram transformation blocks. We implement U-nets based on these blocks and train them on complex-valued spectrograms to consider both magnitude and phase. These networks are then compared on the SDR metric. When using a particular block composed of convolutional and fully-connected layers, it achieves state-of-the-art SDR on the MUSDB singing voice separation task by a large margin of 0.9 dB. Our code and models are available online."
Matevž Pesek;Lovro Suhadolnik;Peter Šavli;Matija Marolt,The rhythmic dictator: Does gamification of rhythm dictation exercises help?,2020,https://doi.org/10.5281/zenodo.4245478,"Matevž Pesek, Faculty of computer and information science, University of Ljubljana, Slovenia;Lovro Suhadolnik, Faculty of computer and information science, University of Ljubljana, Slovenia;Peter Šavli, Conservatory of Music and Ballet Ljubljana, Slovenia;Matija Marolt, Faculty of computer and information science, University of Ljubljana, Slovenia","We present the development and evaluation of a gamified rhythmic dictation application for music theory learning. The application's focus is on mobile accessibility and user experience, so it includes intuitive controls for input of rhythmic exercises, a responsive user interface, several gamification elements and a flexible exercise generator. We evaluated the rhythmic dictation application with conservatory-level music theory students through A/B testing, to assess their engagement and performance. The results show a significant impact of the application on the students' exam scores."
Florian Henkel;Rainer Kelz;Gerhard Widmer,Learning to read and follow music in complete score sheet images,2020,https://doi.org/10.5281/zenodo.4245550,"Florian Henkel, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Rainer Kelz, Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Vienna, Austria;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria","This paper addresses the task of score following in sheet music given as unprocessed images. While existing work either relies on OMR software to obtain a computer-readable score representation, or crucially relies on prepared sheet image excerpts, we propose the first system that directly performs score following in full-page, completely unprocessed sheet images.  Based on incoming audio and a given image of the score, our system directly predicts the most likely position within the page that matches the audio, outperforming current state-of-the-art image-based score followers in terms of alignment precision. We also compare our method to an OMR-based approach and empirically show that it can be a viable alternative to such a system."
Yunpeng Li;Marco Tagliasacchi;Beat Gfeller;Dominik Roblek,Learning to denoise historical music,2020,https://doi.org/10.5281/zenodo.4245480,"Yunpeng Li, Google;Beat Gfeller, Google;Marco Tagliasacchi, Google;Dominik Roblek, Google","We propose an audio-to-audio generative model that learns to denoise old music recordings. Our model internally converts its input into a time-frequency representation by means of a short-time Fourier transform (STFT), and processes the resulting complex spectrogram using a convolutional neural network. The network is trained with both reconstruction and adversarial objectives on a synthetic noisy music dataset, which is created by mixing clean music with real noise samples extracted from quiet segments of old recordings. We evaluate our method quantitatively on held-out test examples of the synthetic dataset, and qualitatively by human rating on samples of actual historical recordings. Our results show that the proposed method is effective in removing noise, while preserving the quality and details of the original music."
Christof Weiss;Stephanie Klauk;Mark R H Gotham;Meinard Müller;Rainer Kleinertz,Discourse not dualism: An interdisciplinary dialogue on sonata form in Beethoven's early piano sonatas,2020,https://doi.org/10.5281/zenodo.4245402,"Christof Weiß, International Audio Laboratories Erlangen, Germany;Stephanie Klauk, Institut für Musikwissenschaft, Saarland University, Germany;Mark Gotham, Institut für Musikwissenschaft, Saarland University, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany;Rainer Kleinertz, Institut für Musikwissenschaft, Saarland University, Germany","The computational analysis of music has traditionally seen a sharp divide between the ""audio approach"" relying on signal processing and the ""symbolic approach"" based on scores. Likewise, there has also been an unfortunate gap between any such computational endeavour and more traditional approaches as used in historical musicology. In this paper, we take a step towards ameliorating this situation through the application of a computational method for visualizing local key characteristics in audio recordings. We exploit these visualizations of diatonic scale content by discussing their musicological implications, being aware of methodological limitations as for the case of minor keys. As a proof of concept, we use this method for investigating differences between the traditional sonata-form model and selected Beethoven piano sonatas in the context of sonata theory from the end of the 18th century. We consider this scenario as an example for a rewarding dialogue between computer science and historical musicology."
Jennifer Thom;Angela Nazarian;Ruth Brillman;Henriette Cramer;Sarah Mennicken,"""Play music"": User motivations and expectations for non-specific voice queries",2020,https://doi.org/10.5281/zenodo.4245524,"Jennifer Thom, Spotify, USA;Angela Nazarian, University of California, Davis;Ruth Brillman, Spotify, USA;Henriette Cramer, Spotify, USA;Sarah Mennicken, Spotify, USA","The growing market of voice-enabled devices introduces new types of music search requests that can be more ambiguous than in typed search interfaces as voice assistants can potentially support conversational requests. However, these systems may not be able to fulfill ambiguous requests in a manner that matches the user need.  In this work, we study an example of ambiguous requests which we term as non-specific queries (NSQs), such as ""play music,"" where users ask to stream content using a single utterance that does not specify what content they want to hear.  To better understand user motivations for making NSQs, we conducted semi-structured qualitative interviews with voice users. We observed four themes that structure user perceptions of the benefits and shortcomings of making NSQs: the tradeoff between control and convenience, varying expectations for personalization, the effects of context on expectations, and learned user behaviors. We conclude with implications for how these themes can inform the interaction design of voice search systems in handling non-specific music requests in voice search systems."
Felipe V Falcão;Nazareno Andrade;Flavio Figueiredo;Diego Furtado Silva;Fabio Morais,Measuring disruption in song similarity networks,2020,https://doi.org/10.5281/zenodo.4245356,"Felipe Falcão, Universidade Federal de Campina Grande, Brazil;Nazareno Andrade, Universidade Federal de Campina Grande, Brazil;Flávio Figueiredo, Universidade Federal de Minas Gerais, Brazil;Diego Silva, Universidade Federal de São Carlos, Brazil;Fabio Morais, Universidade Federal de Campina Grande, Brazil","Investigating music with a focus on the similarity relations between songs, albums, and artists plays an important role when trying to understand trends in the history of music genres. In particular, representing these relations as a similarity network allows us to investigate the innovation presented by these entities in a multitude of points-of-view, including disruption. A disruptive object is one that creates a new stream of events, changing the traditional way of how a context usually works. The proper measurement of disruption remains as a task with large room for improvement, and these gaps are even more evident in the music domain, where the topic has not received much attention so far. This work builds on preliminary studies focused on the analysis of music disruption derived from metadata-based similarity networks, demonstrating that the raw audio can augment similarity information. We developed a case study based on a collection of a Brazilian local music tradition called Forró, that emphasizes the analytical and musicological potential of the musical disruption metric to describe and explain a genre trajectory over time."
Axel Marmoret;Jeremy Cohen;Frédéric Bimbot;Nancy Bertin,Uncovering audio patterns in music with Nonnegative Tucker Decomposition for structural segmentation,2020,https://doi.org/10.5281/zenodo.4245552,"Axel Marmoret, Univ Rennes, Inria, CNRS, IRISA, France.;Jérémy E. Cohen, Univ Rennes, Inria, CNRS, IRISA, France.;Nancy Bertin, Univ Rennes, Inria, CNRS, IRISA, France.;Frédéric Bimbot, Univ Rennes, Inria, CNRS, IRISA, France.","Recent work has proposed the use of tensor decomposition to model repetitions and to separate tracks in loop-based electronic music. The present work investigates further on the ability of Nonnegative Tucker Decompositon (NTD) to uncover musical patterns and structure in pop songs in their audio form.Exploiting the fact that NTD tends to express the content of bars as linear combinations of a few patterns, we illustrate the ability of the decomposition to capture and single out repeated motifs in the corresponding compressed space, which can be interpreted from a musical viewpoint. The resulting features also turn out to be efficient for structural segmentation, leading to experimental results on the RWC Pop data set which are potentially challenging state-of-the-art approaches that rely on extensive example-based learning schemes."
Daniel Harasim;Christoph Finkensiep;Petter Ericson;Timothy J. O'Donnell;Martin Rohrmeier,The jazz Harmony Treebank,2020,https://doi.org/10.5281/zenodo.4245406,"Daniel Harasim, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland;Christoph Finkensiep, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland;Petter Ericson, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland;Timothy J. O’Donnell, Department of Linguistics, McGill University, Canada;Martin Rohrmeier, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland","Grammatical models which represent the hierarchical structure of chord sequences have proven very useful in recent analyses of Jazz harmony. A critical resource for building and evaluating such models is a ground-truth database of syntax trees that encode hierarchical analyses of chord sequences. In this paper, we introduce the Jazz Harmony Treebank (JHT),  a dataset of hierarchical analyses of complete Jazz standards. The analyses were created and checked by experts, based on lead sheets from the open iRealPro collection. The JHT is publicly available in JavaScript Object Notation (JSON), a human-understandable and machine-readable format for structured data. We additionally discuss statistical properties of the corpus and present a simple open-source web application for the graphical creation and editing of trees which was developed during the creation of the dataset."
Kento Watanabe;Masataka Goto,A chorus-section detection method for lyrics text,2020,https://doi.org/10.5281/zenodo.4245442,"Kento Watanabe, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper addresses the novel task of detecting chorus sections in English and Japanese lyrics text. Although chorus-section detection using audio signals has been studied, whether chorus sections can be detected from text-only lyrics is an open issue. Another open issue is whether patterns of repeating lyric lines such as those appearing in chorus sections depend on language. To investigate these issues, we propose a neural network-based model for sequence labeling. It can learn phrase repetition and linguistic features to detect chorus sections in lyrics text. It is, however, difficult to train this model since there was no dataset of lyrics with chorus-section annotations as there was no prior work on this task. We therefore generate a large amount of training data with such annotations by leveraging pairs of musical audio signals and their corresponding manually time-aligned lyrics; we first automatically detect chorus sections from the audio signals and then use their temporal positions to transfer them to the line-level chorus-section annotations for the lyrics. Experimental results show that the proposed model with the generated data contributes to detecting the chorus sections, that the model trained on Japanese lyrics can detect chorus sections surprisingly well in English lyrics and that patterns of repeating lyric lines are language-independent."
Ziyu Wang;Ke Chen;Junyan Jiang;Yiyi Zhang;Maoran Xu;Shuqi Dai;Gus Xia,POP909: A pop-song dataset for music arrangement generation,2020,https://doi.org/10.5281/zenodo.4245366,"Ziyu Wang, Music X Lab, Computer Science Department, NYU Shanghai;Ke Chen, CREL, Music Department, UC San Diego;Junyan Jiang, Music X Lab, Computer Science Department, NYU Shanghai;Yiyi Zhang, Center for Data Science, New York University;Maoran Xu, Department of Statistics, University of Florida;Shuqi Dai, Computer Science Department, Carnegie Mellon University;Xianbin Gu, Music X Lab, Computer Science Department, NYU Shanghai;Gus Xia, Music X Lab, Computer Science Department, NYU Shanghai","Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms."
Tsung-Ping Chen;Satoru Fukayama;Masataka Goto;Li Su,Chord jazzification: Learning jazz interpretations of chord symbols,2020,https://doi.org/10.5281/zenodo.4245444,"Tsung-Ping Chen, Institute of Information Science, Academia Sinica, Taiwan;Satoru Fukayama, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Li Su, Institute of Information Science, Academia Sinica, Taiwan","Chord symbols, typically notating the root note and the chord quality, are extensively used yet oversimplified representation of tonal harmony and chord progressions in popular music. In spite of its convenience, the chord symbol notation only provides basic information about the chordal configuration, and leaves much room for interpretation. With such limitations, an algorithm generating merely chord symbols is usually insufficient for a wide range of music genres such as jazz. To solve this problem, we propose chord jazzification, a process to generate realistic chord configurations in jazz style. With deep learning approaches, we decompose chord jazzification into coloring and voicing. Coloring concerns the choice of color tones, while voicing concerns the configurations of chords. We also create a new dataset featuring interpretations of chord symbols in pop-jazz compositions. By conducting experiments on the new dataset, we show that 1) the two-stage process outperforms an end-to-end generation approach in modeling chord configurations, and 2) attention-based models are better at capturing the structure of chord sequences in comparison with recurrent neural networks."
Ziyu Wang;Dingsu Wang;Yixiao Zhang;Gus Xia,Learning interpretable representation for controllable polyphonic music generation,2020,https://doi.org/10.5281/zenodo.4245518,"Ziyu Wang, Music X Lab, Computer Science Department, NYU Shanghai;Dingsu Wang, Music X Lab, Computer Science Department, NYU Shanghai;Yixiao Zhang, Music X Lab, Computer Science Department, NYU Shanghai;Gus Xia, Music X Lab, Computer Science Department, NYU Shanghai","While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation."
Taketo Akama,Connective fusion: Learning transformational joining of sequences with application to melody creation,2020,https://doi.org/10.5281/zenodo.4245360,"Taketo Akama, Sony Computer Science Laboratories","We present Connective Fusion, a music generation scheme by transformational joining of two musical sequences for creative purposes. Given two shorter sequences as inputs, our model transforms each of them such that their concatenation is more coherent to form a longer sequence, while each of the transformed shorter sequences retains meaningful similarity with the corresponding input sequence. In short, our model connects and fuses two contextually unrelated sequences in a coherent way. This transformation can be applied iteratively to gradually fuse the input sequences. The style latent space is simultaneously learned, allowing users to control how the two sequences are merged. Our approach comprises two steps of unsupervised learning: a deep generative model with a latent space is learned, followed by adversarial learning of the transformation function in the latent space. We demonstrate the usefulness of our method through the task of melody creation using a symbolic music dataset."
Ziyu Wang;Yiyi Zhang;Yixiao Zhang;Junyan Jiang;Ruihan Yang;Gus Xia;Junbo Zhao,PianoTree VAE: Structured representation learning for polyphonic music,2020,https://doi.org/10.5281/zenodo.4245446,"Ziyu Wang, Music X Lab, Computer Science Department, NYU Shanghai;Yiyi Zhang, Center for Data Science, New York University;Yixiao Zhang, Music X Lab, Computer Science Department, NYU Shanghai;Junyan Jiang, Music X Lab, Computer Science Department, NYU Shanghai;Ruihan Yang, Music X Lab, Computer Science Department, NYU Shanghai;Junbo Zhao (Jake), Computer Science Department, Zhejiang University;Gus Xia, Music X Lab, Computer Science Department, NYU Shanghai","The dominant approach for music representation learning involves the deep unsupervised model family variational autoencoder (VAE). However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning. The experiments prove the validity of the PianoTree VAE via (i)-semantically meaningful latent code for polyphonic segments; (ii)-more satisfiable reconstruction aside of decent geometry learned in the latent space; (iii)-this model's benefits to the variety of the downstream music generation."
Yudhik Agrawal;Samyak Jain;Emily Carlson;Petri Toiviainen;Vinoo Alluri,Towards multimodal MIR: Predicting individual differences from music-induced movement,2020,https://doi.org/10.5281/zenodo.4245368,"Yudhik Agrawal, Cognitive Science Lab, International Institute of Information Technology, Hyderabad, India;Samyak Jain, Cognitive Science Lab, International Institute of Information Technology, Hyderabad, India;Emily Carlson, Department of Music, Art and Culture Studies, University of Jyväskylä, Finland;Petri Toiviainen, Department of Music, Art and Culture Studies, University of Jyväskylä, Finland;Vinoo Alluri, Cognitive Science Lab, International Institute of Information Technology, Hyderabad, India","As the field of Music Information Retrieval grows, it is important to take into consideration the multi-modality of music and how aspects of musical engagement such as movement and gesture might be taken into account. Bodily movement is universally associated with music and reflective of important individual features related to music preference such as personality, mood, and empathy. Future multimodal MIR systems may benefit from taking these aspects into account. The current study addresses this by identifying individual differences, specifically Big Five personality traits, and scores on the Empathy and Systemizing Quotients (EQ/SQ) from participants' free dance movements. Our model successfully explored the unseen space for personality as well as EQ, SQ, which has not previously been accomplished for the latter. R2 scores for personality, EQ, and SQ were 76.3%, 77.1%, and 86.7% respectively. As a follow-up, we investigated which bodily joints were most important in defining these traits. We discuss how further research may explore how the mapping of these traits to movement patterns can be used to build a more personalized, multi-modal recommendation system, as well as potential therapeutic applications."
Lisa Kawai;Philippe Esling;Tatsuya Harada,Attributes-aware deep music transformation,2020,https://doi.org/10.5281/zenodo.4245520,"Lisa Kawai, The University of Tokyo, Japan;Philippe Esling, IRCAM, France;Tatsuya Harada, The University of Tokyo, Japan","Recent machine learning techniques have enabled a large variety of novel music generation processes. However, most approaches do not provide any form of interpretable control over musical attributes, such as pitch and rhythm. Obtaining control over the generation process is critically important for its use in real-life creative setups. Nevertheless, this problem remains arduous, as there are no known functions nor differentiable approximations to transform symbolic music with control of musical attributes.In this work, we propose a novel method that enables attributes-aware music transformation from any set of musical annotations, without requiring complicated derivative implementation. By relying on an adversarial confusion criterion on given musical annotations, we force the latent space of a generative model to abstract from these features. Then, reintroducing these features as conditioning to the generative function, we obtain a continuous control over them. To demonstrate our approach, we rely on sets of musical attributes computed by the jSymbolic library as annotations and conduct experiments that show that our method outperforms previous methods in control. Finally, comparing correlations between attributes and the transformed results show that our method can provide explicit control over any continuous or discrete annotation."
Andrea Vaglio;Romain Hennequin;Manuel Moussallam;Gael Richard;Florence d'Alché-Buc,Multilingual lyrics-to-audio alignment,2020,https://doi.org/10.5281/zenodo.4245484,"Andrea Vaglio, Deezer R&D;Romain Hennequin, ;Manuel Moussallam, LTCI, Télécom Paris, Institut Polytechnique de Paris;Gaël Richard, LTCI, Télécom Paris, Institut Polytechnique de Paris;Florence d’Alché-Buc, LTCI, Télécom Paris, Institut Polytechnique de Paris","Lyrics-to-audio alignment methods have recently reported impressive results, opening the door to practical applications such as karaoke and within song navigation. However, most studies focus on a single language - usually English - for which annotated data are abundant. The question of their ability to generalize to other languages, especially in low (or even zero) training resource scenarios has been so far left unexplored. In this paper, we address the lyrics-to-audio alignment task in a generalized multilingual setup. More precisely, this investigation presents the first (to the best of our knowledge) attempt to create a language-independent lyrics-to-audio alignment system. Building on a RNN model trained with a CTC algorithm, we study the relevance of different intermediate representations, either character or phoneme, along with several strategies to design a training set. The evaluation is conducted on multiple languages with a varying amount of data available, from plenty to zero. Results show that learning from diverse data and using a universal phoneme set as an intermediate representation yield the best generalization performances."
Ethan Manilow;Gordon Wichern;Jonathan LeRoux,Hierarchical musical instrument separation,2020,https://doi.org/10.5281/zenodo.4245448,"Ethan Manilow, Northwestern University;Gordon Wichern, Mitsubishi Electric Research Laboratories (MERL);Jonathan Le Roux, Mitsubishi Electric Research Laboratories (MERL)","Many sounds that humans encounter are hierarchical in nature; a piano note is one of many played during a performance, which is one of many instruments in a band, which might be playing in a bar with other noises occurring. Inspired by this, we re-frame the musical source separation problem as hierarchical, combining similar instruments together at certain levels and separating them at other levels. This allows us to deconstruct the same mixture in multiple ways, depending on the appropriate level of the hierarchy for a given application. In this paper, we present various methods for hierarchical musical instrument separation, with some methods focusing on separating specific instruments (like guitars) and other methods that determine what to separate based on a user-supplied audio example. We additionally show that separating all hierarchy levels is possible even when training data is limited at fine-grained levels of the hierarchy."
Rohit M A;Vinutha T P;Preeti Rao,Structural segmentation of Dhrupad vocal bandish audio based on tempo,2020,https://doi.org/10.5281/zenodo.4245522,"Rohit M A, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Vinutha T P, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Preeti Rao, Department of Electrical Engineering, Indian Institute of Technology Bombay, India","A Dhrupad vocal concert comprises a composition section that is interspersed with improvised episodes of increased rhythmic activity involving the interaction between the vocals and the percussion. Tracking the changing rhythmic density, in relation to the underlying metric tempo of the piece, thus facilitates the detection and labeling of the improvised sections in the concert structure. This work concerns the automatic detection of the musically relevant rhythmic densities as they change in time across the bandish (composition) performance. An annotated dataset of Dhrupad bandish concert sections is presented. We implement a CNN-based system, trained to detect local tempo relationships, and follow it with temporal smoothing. We also employ audio source separation as a pre-processing step to the detection of the individual surface densities of the vocals and the percussion. This helps us obtain the complete musical description of the concert sections in terms of capturing the changing rhythmic interaction of the two performers."
Matthew Davies;Magdalena Fuentes;João Fonseca;Luis Aly;Marco Jerónimo;Filippo Bonini Baraldi,Moving in time: Computational analysis of microtiming in Maracatu de baque solto,2020,https://doi.org/10.5281/zenodo.4245554,"Matthew E. P. Davies, University of Coimbra, CISUC, DEI, Portugal;Magdalena Fuentes, CUSP, MARL, New York University, USA;João Fonseca, University of Porto, Faculty of Engineering, Portugal;Luís Aly, University of Porto, Faculty of Engineering, Portugal;Marco Jerónimo, University of Porto, Faculty of Engineering, Portugal;Filippo Bonini Baraldi, Ethnomusicology Institute (INET-md), FCSH, Universidade Nova de Lisboa, Portugal, Centre de Recherche en Ethnomusicologie (CREM-LESC), Paris Nanterre University, France","""Maracatu de baque solto"" is a Carnival performance combining music, poetry, and dance, occurring in the Zona da Mata Norte region of Pernambuco (Northeast Brazil). Maracatu percussive music is strongly repetitive, and is played as loud and as fast as possible. Both from an MIR and ethnomusicological perspective this makes a complex musical scene to analyse and interpret. In this paper we focus on the extraction of microtiming profiles towards the longer term goal of understanding how rhythmic performance in Maracatu is used to promote health and well-being. To conduct this analysis we use a set of recordings acquired with contact microphones which minimise the interference between performers. Our analysis reveals that the microtiming profiles differ substantially from those observed in more widely studied South American music. In particular, we highlight the presence of dynamic microtiming profiles as well as the importance of the choice of time-keeper instrument, which dictates how the performances can be understood. Throughout this work, we emphasize the importance of a multidisciplinary approach in which MIR, audio engineering, and ethnomusicology must interact to provide meaningful insight about this music."
Elena V. Epure;Guillaume Salha;Romain Hennequin,Multilingual music genre embeddings for effective cross-lingual music item annotation,2020,https://doi.org/10.5281/zenodo.4245556,"Elena V. Epure, Deezer Research;Guillaume Salha, Deezer Research;Romain Hennequin, Deezer Research","Annotating music items with music genres is crucial for music recommendation and information retrieval, yet challenging given that music genres are subjective concepts. Recently, in order to explicitly consider this subjectivity, the annotation of music items was modeled as a translation task: predict for a music item its music genres within a target vocabulary or taxonomy (tag system) from a set of music genre tags originating from other tag systems. However, without a parallel corpus, previous solutions could not handle tag systems in other languages, being limited to the English-language only. Here, by learning multilingual music genre embeddings, we enable cross-lingual music genre translation without relying on a parallel corpus. First, we apply compositionality functions on pre-trained word embeddings to represent multi-word tags. Second, we adapt the tag representations to the music domain by leveraging multilingual music genres graphs with a modified retrofitting algorithm. Experiments show that our method: 1) is effective in translating music genres across tag systems in multiple languages (English, French and Spanish); 2) outperforms the previous baseline in an English-language multi-source translation task."
Robert Lieck;Martin Rohrmeier,Modelling hierarchical key structure with pitch scapes,2020,https://doi.org/10.5281/zenodo.4245558,"Robert Lieck, Digital and Cognitive Musicology Lab, EPFL, Switzerland;Martin Rohrmeier, Digital and Cognitive Musicology Lab, EPFL, Switzerland","Musical form and syntax in Western classical music are hierarchically organised on different timescales. One of the most important features of this structure is the organisation of modulations between different keys throughout a piece. Music theoretical research has established taxonomies of prototypical modulation plans for different modes and musical forms. However, these prototypes still require empirical validation based on quantitative statistical methods and cannot be retrieved automatically so far.In this paper, we present a novel method to infer prototypical modulation plans from musical corpora. A modulation plan is formalised as a transposition-invariant probabilistic model over the underlying pitch class distributions based on a hierarchical pitch scape representation. Prototypical modulation plans can be learned in an unsupervised manner by training a mixture model (similar to a Gaussian mixture model) on the data, so that different prototypes appear as distinct clusters.We evaluate our approach by performing hierarchical clustering on a corpus of more than 150 Baroque pieces, with the extracted clusters showing excellent agreement with the most common prototypes postulated in music theory. Our method bears a great potential for modelling, analysis and discovery of hierarchical key structure and prototypes in corpora across a broad range of musical styles. An accompanying library is available at: github.com/robert-lieck/pitchscapes."
Christoph Finkensiep;Ken Déguernel;Markus Neuwirth;Martin Rohrmeier,Voice-leading schema recognition using rhythm and pitch features,2020,https://doi.org/10.5281/zenodo.4245482,"Christoph Finkensiep, École Polytechnique Fédérale de Lausanne;Ken Déguernel, École Polytechnique Fédérale de Lausanne;Markus Neuwirth, Anton Bruckner University Linz;Martin Rohrmeier, École Polytechnique Fédérale de Lausanne","Musical schemata constitute important structural building blocks used across historical styles and periods.They consist of two or more melodic lines that are combined to form specific successions of intervals. This paper tackles the problem of recognizing voice-leading schemata in polyphonic music.Since schema types and subtypes can be realized in a wide variety of ways on the musical surface,finding schemata in an automated fashion is a challenging task.To perform schema inference we employ a skipgram model that computes schema candidates, which are then classified using a binary classifier on musical features related to pitch and rhythm.This model is evaluated on a novel dataset of schema annotations in Mozart's piano sonatas produced byexpert annotators, which is published alongside this paper.The features are chosen to encode music-theoretically predicted properties of schema instances.We assess the relevance of each feature for the classification task, thus contributing to the theoretical understanding of complex musical objects."
Ayush Patwari;Nicholas Kong;Jun Wang;Ullas Gargi;Michele Covell,Semantically meaningful attributes from co-listen embeddings for playlist exploration and expansion,2020,https://doi.org/10.5281/zenodo.4245486,"Ayush Patwari, YouTube Music;Nicholas Kong, YouTube Music;Jun Wang, YouTube Music;Ullas Gargi, YouTube Music;Michele Covell, Google Research;Aren Jansen, Google Research","Audio embeddings of musical similarity are often used for music recommendations and autoplay discovery. These embeddings are typically learned using co-listen data to train a deep neural network, to provide consistent tripletloss distances. Instead of directly using these co-listen–based embeddings, we explore making recommendations based on a second, smaller embedding space of human-intelligible musical attributes. To do this, we use the co-listen–based audio embeddings as inputs to small attribute classifiers, trained on a small hand-labeled dataset. These classifiers map from the original embedding space to a new interpretable attribute coordinate system that provides a more useful distance measure for downstream applications. The attributes and attribute embeddings allow us to provide a search interface and more intelligible recommendations for music curators. We examine the relative performance of these two embedding spaces (the co-listen–audio embedding and the attribute embedding) for the mathematical separation of thematic playlists. We also report on the usefulness of recommendations from the attribute-embedding space to human curators for automatically extending thematic playlists."
Bruno Di Giorgi;Matthias Mauch;Mark Levy,Downbeat tracking with tempo invariant convolutional neural networks,2020,https://doi.org/10.5281/zenodo.4245408,"Bruno Di Giorgi, Apple Inc.;Matthias Mauch, Apple Inc.;Mark Levy, Apple Inc.","The human ability to track musical downbeats is robust to changes in tempo, and it extends to tempi never previously encountered.  We propose a deterministic time-warping operation that enables this skill in a convolutional neural network (CNN) by allowing the network to learn rhythmic patterns independently of tempo.  Unlike conventional deep learning approaches, which learn rhythmic patterns at the tempi present in the training dataset, the patterns learned in our model are tempo-invariant, leading to better tempo generalisation and more efficient usage of the network capacity.We test the generalisation property on a synthetic dataset created by rendering the Groove MIDI Dataset using FluidSynth, split into a training set containing the original performances and a test set containing tempo-scaled versions rendered with different SoundFonts (test-time augmentation).The proposed model generalises nearly perfectly to unseen tempi (F-measure of 0.89 on both training and test sets), whereas a comparable conventional CNN achieves similar accuracy only for the training set (0.89) and drops to 0.54 on the test set.The generalisation advantage of the proposed model extends to real music, as shown by results on the GTZAN and Ballroom datasets."
Francesco Foscarin;Andrew McLeod;Philippe Rigaux;Florent Jacquemard;Masahiko Sakai,ASAP: A dataset of aligned scores and performances for piano transcription,2020,https://doi.org/10.5281/zenodo.4245490,"Francesco Foscarin, CNAM, Paris, France;Andrew McLeod, EPFL, Lausanne, Switzerland;Philippe Rigaux, CNAM, Paris, France;Florent Jacquemard, INRIA, Paris, France;Masahiko Sakai, Nagoya University, Nagoya, Japan","In this paper we present Aligned Scores and Performances (ASAP): a new dataset of 222 digital musical scores aligned with 1068 performances (more than 92 hours) of Western classical piano music.The scores are provided as paired MusicXML files and quantized MIDI files, and the performances as paired MIDI files and partially as audio recordings.Scores and performances are aligned with downbeat, beat, time signature, and key signature annotations. ASAP has been obtained thanks to a new annotation workflow that combines score analysis andalignment algorithms, with the goal of reducing the time for manual annotation. The dataset itself is, to our knowledge, the largest that includes an alignment of music scores to MIDI and audio performance data. As such, it is a useful resource for a wide variety of MIR applications, from those that target the complete audio-to-score Automatic Music Transcription task, to others that target more specific aspects (e.g., key signature estimation and beat or downbeat tracking from both MIDI and audio representations)."
Aayush Surana;Yash Goyal;Manish Shrivastava;Suvi H Saarikallio;Vinoo Alluri,Tag2Risk: Harnessing social music tags for characterizing depression risk,2020,https://doi.org/10.5281/zenodo.4245450,"Aayush Surana, International Institute of Information Technology, Hyderabad, India;Yash Goyal, International Institute of Information Technology, Hyderabad, India;Manish Shrivastava, International Institute of Information Technology, Hyderabad, India;Suvi Saarikallio, Department of Music, Art and Culture Studies, University of Jyväskylä, Finland;Vinoo Alluri, International Institute of Information Technology, Hyderabad, India","Musical preferences have been considered a mirror of the self. In this age of Big Data, online music streaming services allow us to capture ecologically valid music listening behavior and provide a rich source of information to identify several user-specific aspects. Studies have shown musical engagement to be an indirect representation of internal states including internalized symptomatology and depression. The current study aims at unearthing patterns and trends in the individuals at risk for depression as it manifests in naturally occurring music listening behavior. Mental-well being scores, musical engagement measures, and listening histories of Last.fm users (N=541) were acquired. Social tags associated with each listener's most popular tracks were analyzed to unearth the mood/emotions and genres associated with the users. Results revealed that social tags prevalent in the users at risk for depression were predominantly related to emotions depicting Sadness associated with genre tags representing neo-psychedelic-, avant garde-, dream-pop. This study will open up avenues for an MIR-based approach to characterizing and predicting risk for depression which can be helpful in early detection and additionally provide bases for designing music recommendations accordingly."
Gabriel Meseguer Brocal;Geoffroy Peeters,Content based singing voice source separation via strong conditioning using aligned phonemes,2020,https://doi.org/10.5281/zenodo.4245560,"Gabriel Meseguer-Brocal, STMS UMR9912, Ircam/CNRS/SU, Paris;Geoffroy Peeters, LTCI, Institut Polytechnique de Paris","Informed source separation has recently gained renewed interest with the introduction of neural networks and the availability of large multitrack datasets containing both the mixture and the separated sources.These approaches use prior information about the target source to improve separation.Historically, Music Information Retrieval ({MIR}) researchers have focused primarily on score-informed source separation, but more recent approaches explore lyrics-informed source separation.However, because of the lack of multitrack datasets with time-aligned lyrics, models use weak conditioning with the non-aligned lyrics.In this paper, we present a multimodal multitrack dataset with lyrics aligned in time at the word level with phonetic information as well as explore strong conditioning using the aligned phonemes.Our model follows a {U-Net} architecture and takes as input both the magnitude spectrogram of a musical mixture and a matrix with aligned phoneme information.The phoneme matrix is embedded to obtain the parameters that control Feature-wise Linear Modulation ({FiLM}) layers.These layers condition the {U-Net} feature maps to adapt the separation process to the presence of different phonemes via affine transformations.We show that phoneme conditioning can be successfully applied to improve singing voice source separation."
Shunit Haviv Hakimi;Nadav Bhonker;Ran El-Yaniv,BebopNet: Deep neural models for personalized jazz improvisations,2020,https://doi.org/10.5281/zenodo.4245562,"Shunit Haviv Hakimi, Technion – Israel Institute of Technology;Nadav Bhonker, Technion – Israel Institute of Technology;Ran El-Yaniv, Technion – Israel Institute of Technology","A major bottleneck in the evaluation of music generation is that music appreciation is a highly subjective matter. When considering an average appreciation as an evaluation metric, user studies can be helpful. The challenge of generating personalized content, however, has been examined only rarely in the literature. In this paper, we address generation of personalized music and propose a novel pipeline for music generation that learns and optimizes user-specific musical taste. We focus on the task of symbol-based, monophonic, harmony-constrained jazz improvisations. Our personalization pipeline begins with BebopNet, a music language model trained on a corpus of jazz improvisations by Bebop giants. BebopNet is able to generate improvisations based on any given chord progression. We then assemble a personalized dataset, labeled by a specific user, and train a user-specific metric that reflects this user's unique musical taste. Finally, we employ a personalized variant of beam-search with BebopNet to optimize the generated jazz improvisations for that user. We present an extensive empirical study in which we apply this pipeline to extract individual models as implicitly defined by several human listeners. Our approach enables an objective examination of subjective personalized models whose performance is quantifiable. The results indicate that it is possible to model and optimize personal jazz preferences and offer a foundation for future research in personalized generation of art. We also briefly discuss opportunities, challenges, and questions that arise from our work, including issues related to creativity."
Arianne N. van Nieuwenhuijsen;John Ashley Burgoyne;Frans Wiering;Mick Sneekes,A simple method for user-driven music thumbnailing,2020,https://doi.org/10.5281/zenodo.4245410,"Arianne N. van Nieuwenhuijsen, Utrecht University, The Netherlands;John Ashley Burgoyne, University of Amsterdam, The Netherlands;Frans Wiering, Utrecht University, The Netherlands;Mick Sneekes, Utrecht University, The Netherlands","More and more music is becoming available digitally, increasing the need to navigate through large numbers of audio tracks easily. One approach for improving the browsing experience is music thumbnailing: the procedure of finding a continuous fragment that can represent the whole musical piece. This paper proposes a human-centred approach to creating thumbnails based on listeners' perception, directly asking listeners to identify the most characteristic fragment. We carried out a user study to assign representativeness scores to multiple fragments from a selection of popular music tracks. To strengthen the results, we performed a replication of the same user study with new participants and a different set of music. Thereafter, we used audio features, the segmentation algorithm, and participants' overall familiarity with the songs to predict representativeness scores. The results suggest that neither segmentation nor familiarity have a significant impact on users' thumbnail preferences: even segments with starting points that pay no regard to song structure can be suitable thumbnails. Three high-level audio characteristics, however, do impact the perceived representativeness of a fragment: Raw Intensity, Melodic Conventionality, and Conventionally of Intensity. Based on these findings, we propose a new, easy-to-apply method for music thumbnailing."
Jada E Watson,Programming inequality: Gender representation on Canadian country radio (2005-2019),2020,https://doi.org/10.5281/zenodo.4245452,"Jada Watson, School of Music, University of Ottawa","In May 2015, a consultant for country radio revealed a decades' long practice of limiting space for songs by female artists. He encouraged program directors to avoid playing songs by women back-to-back and advocated for programming their songs at 13-15% of station playlists. His words sparked debate within the industry and drew attention to growing inequalities on radio and within the genre. The majority of these discussions have centered on US country radio, with limited attention to the growing imbalance on the format in Canada. While country format radio in both countries subscribe to a practice of gender-based programming, Canadian program directors are governed by the federal Broadcasting Act, which regulates dissemination of Canadian content. Using metadata extracted from one of the main radio monitoring services – Mediabase, this paper examines gender-related trends on Canadian country format radio between 2005 and 2019. Through data-driven analysis of Mediabase's weekly re-ports, this paper shows declining representation of songs by women on Canadian country radio and addresses the impact of Canadian content regulations on this process."
Matan Gover;Philippe Depalle,Score-informed source separation of choral music,2020,https://doi.org/10.5281/zenodo.4245412,"Matan Gover, Schulich School of Music, McGill University;Philippe Depalle, Schulich School of Music, McGill University","Choral music recordings are a particularly challenging target for source separation due to the choral blend and the inherent acoustical complexity of the 'choral timbre'. Due to the scarcity of publicly available multi-track choir recordings, we create a dataset of synthesized Bach chorales. We apply data augmentation to alter the chorales so that they more faithfully represent music from a broader range of choral genres. For separation we employ Wave-U-Net, a time-domain convolutional neural network (CNN) originally proposed for vocals and accompaniment separation. We show that Wave-U-Net outperforms a baseline implemented using score-informed NMF (non-negative matrix factorization). We introduce score-informed Wave-U-Net to incorporate the musical score into the separation process. We experiment with different score conditioning methods and show that conditioning on the score leads to improved separation results. We propose a 'score-guided' model variant in which separation is guided by the score alone, bypassing the need to specify the identity of the extracted source. Finally, we evaluate our models (trained on synthetic data only) on real choir recordings and find that in the absence of a large training set of real recordings, NMF still performs better than Wave-U-Net in this setting. To our knowledge, this paper is the first to study source separation of choral music."
Cynthia C. S. Liem;Chris Mostert,Can't trust the feeling? How open data reveals unexpected behavior of high-level music descriptors,2020,https://doi.org/10.5281/zenodo.4245414,"Cynthia C. S. Liem, Delft University of Technology;Chris Mostert, Delft University of Technology","Copyright restrictions prevent the widespread sharing of commercial music audio. Therefore, the availability of resharable pre-computed music audio features has become critical. In line with this, the AcousticBrainz platform offers a dynamically growing, open and community-contributed large-scale resource of locally computed low-level and high-level music descriptors. Beyond enabling research reuse, the availability of such an open resource allows for renewed reflection on the music descriptors we have at hand: while they were validated to perform successfully under lab conditions, they now are being run 'in the wild'. Their response to these more ecological conditions can shed light on the degree to which they truly had construct validity. In this work, we seek to gain further understanding into this, by analyzing high-level classifier-based music descriptor output in AcousticBrainz. While no hard ground truth is available on what the true value of these descriptors should be, some oracle information can still be derived, relying on semantic redundancies between several descriptors, and multiple feature submissions being available for the same recording. We report on multiple unexpected patterns found in the data, indicating that the descriptor values should not be taken as absolute truth, and hinting at directions for more comprehensive descriptor testing that are overlooked in common machine learning evaluation and quality assurance setups."
Mengyi Shan;Timothy Tsai,Improved handling of repeats and jumps in audio-sheet image synchronization,2020,https://doi.org/10.5281/zenodo.4245358,"Mengyi Shan, Harvey Mudd College;TJ Tsai, Harvey Mudd College","This paper studies the problem of automatically generating Youtube piano score following videos given an audio recording and raw sheet music images.  Whereas previous works focus on synthetic sheet music where the data has been cleaned and preprocessed, we instead focus on developing a system that can cope with the messiness of raw, unprocessed sheet music PDFs from IMSLP.  We investigate how well existing systems cope with real scanned sheet music, filler pages and unrelated pieces or movements, and discontinuities due to jumps and repeats.  We find that a significant bottleneck in system performance is handling jumps and repeats correctly.  In particular, we find that a previously proposed Jump DTW algorithm does not perform robustly when jump locations are unknown a priori.  We propose a novel alignment algorithm called Hierarchical DTW that can handle jumps and repeats even when jump locations are not known.  It first performs alignment at the feature level on each sheet music line, and then performs a second alignment at the segment level.  By operating at the segment level, it is able to encode domain knowledge about how likely a particular jump is.  Through carefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we show that Hierarachical DTW significantly outperforms Jump DTW in handling various types of jumps."
Shahan Nercessian,Zero-shot singing voice conversion,2020,https://doi.org/10.5281/zenodo.4245370,"Shahan Nercessian, iZotope, Inc.","In this paper, we propose the use of speaker embedding networks to perform zero-shot singing voice conversion, and suggest two architectures for its realization.  The use of speaker embedding networks not only enables the capability to adapt to new voices on-the-fly, but also allows for model training on unlabeled data.  This not only facilitates the collection of suitable singing voice data, but also allows networks to be pretrained on large speech corpora before being refined on singing voice datasets, improving network generalization.  We illustrate the effectiveness of the proposed zero-shot singing voice conversion algorithms by both qualitative and quantitative means."
Fabrizio Pedersoli;Masataka Goto,Dance beat tracking from visual information alone,2020,https://doi.org/10.5281/zenodo.4245456,"Fabrizio Pedersoli, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","We propose and explore the novel task of dance beat tracking, which can be regarded as a fundamental topic in the Dance Information Retrieval (DIR) research field. Dance beat tracking aims at detecting musical beats from a dance video by using its visual information without using its audio information (i.e., dance music). The visual analysis of dances is important to achieve general machine understanding of dances, not limited to dance music. As a sub-area of Music Information Retrieval (MIR) research, DIR also shares similar goals with MIR and needs to extract various high-level semantics from dance videos. While audio-based beat tracking has been thoroughly studied in MIR, there has not been visual-based beat tracking for dance videos.We approach dance beat tracking as a time series classification problem and conduct several experiments using a Temporal Convolutional Neural Network (TCN) using the AIST Dance Video Database. We evaluate the proposed solution considering different data splits based on either ""dancer"" or ""music"". Moreover, we propose a periodicity-based loss that considerably improves the overall beat tracking performance according to several evaluation metrics."
Ke Chen;Cheng-i Wang;Taylor Berg-Kirkpatrick;Shlomo Dubnov,Music SketchNet: Controllable music generation via factorized representations of pitch and rhythm,2020,https://doi.org/10.5281/zenodo.4245372,"Ke Chen, CREL, Music Department;Cheng-i Wang, Smule, Inc;Taylor Berg-Kirkpatrick, UC San Diego;Shlomo Dubnov, CREL, Music Department","Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete monophonic musical pieces, conditioned on surrounding context, and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architectures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process."
Jin Ha Lee;Anh Thu Nguyen,How music fans shape commercial music services: A case study of BTS and ARMY,2020,https://doi.org/10.5281/zenodo.4245564,"Jin Ha Lee, University of Washington;Anh Thu Nguyen, University of Washington","Much of the existing research on user aspects in the music information retrieval field tends to focus on general user needs or behavior related to music information seeking, music listening and sharing, or other use of commercial music services. However, we have a limited understanding of the personal and social contexts of music fans who enthusiastically support musicians and are often avid users of commercial music services. In this study, we aim to better understand the contextual complexities surrounding music fans through a case study of the group BTS and its fan community, ARMY. In particular, we are interested in discovering factors that influence the interactions of music fans with music services, especially in the current environment where the prevalence of social media and other tools/technologies influences musical enjoyment. Through virtual ethnography and content analysis, we identified four factors that affect music fans' interactions with commercial music services: 1) perception of music genres, 2) participatory fandom, 3) desire for agency and transparency, and 4) importance of non-musical factors. The discussion of each aspect is followed by design implications for commercial music services to consider."
Avriel C Epps-Darling;Henriette Cramer;Romain Takeo Bouyer,Artist gender representation in music streaming,2020,https://doi.org/10.5281/zenodo.4245416,"Avriel Epps-Darling, Harvard University;Romain Takeo Bouyer, Spotify;Henriette Cramer, Spotify","This study examines gender representation in current music streaming, utilizing one of the world's largest streaming services. First, we found listeners generally stream fewer female or mixed-gender creator groups than male artists, with differences per genre. Second, while still relatively low, we found that recommendation-based streaming has a slightly higher proportion of female creators than ""organic"" listening (i.e., tracks that are not recommended by editors or algorithms). Third, we examined streaming data from 200,000 US users to determine the proportion of female artists in organic and recommended streams over a 28-day period and the relationship between recommended streams and users' future organic listening. The proportion of female artists in recommended streaming appears predictive of the proportion of female artists in organic streaming; these effects are moderated by gender and age. Fourth, this study also samples creators across different popularity levels, seeing more female and multi-gender groups at lower levels than in the middle tiers. However, solo female artists are better represented again in the superstars category, suggesting influence of selected superstars and genres. We conclude by discussing potential avenues in algorithmic auditing."
Filip Korzeniowski;Oriol Nieto;Matthew McCallum;Minz Won;Sergio Oramas;Erik Schmidt,Mood classification using listening data,2020,https://doi.org/10.5281/zenodo.4245488,"Filip Korzeniowski, Pandora Media LLC., Oakland, California, USA;Oriol Nieto, Pandora Media LLC., Oakland, California, USA;Matthew C. McCallum, Pandora Media LLC., Oakland, California, USA;Minz Won, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Sergio Oramas, Pandora Media LLC., Oakland, California, USA;Erik M. Schmidt, Netﬂix Inc., Los Gatos, California, USA","The mood of a song is a highly relevant feature for exploration and recommendation in large collections of music. These collections tend to require automatic methods for predicting such moods. In this work, we show that listening-based features outperform content-based ones when classifying moods: embeddings obtained through matrix factorization of listening data appear to be more informative of a track mood than embeddings based on its audio content. To demonstrate this, we compile a subset of the Million Song Dataset, totalling 67k tracks, with expert annotations of 188 different moods collected from AllMusic. Our results on this novel dataset not only expose the limitations of current audio-based models, but also aim to foster further reproducible research on this timely topic"
Chang-Bin Jeon;Hyeong-Seok Choi;Kyogu Lee,Exploring aligned lyrics-informed singing voice separation,2020,https://doi.org/10.5281/zenodo.4245526,"Chang-Bin Jeon, Department of Intelligence and Information;Hyeong-Seok Choi, Department of Intelligence and Information;Kyogu Lee, Music and Audio Research Group (MARG), Center for Superintelligence, Seoul National University","In this paper, we propose a method of utilizing aligned lyrics as additional information to improve the performance of singing voice separation. We have combined the highway network-based lyrics encoder into Open-unmix separation network and show that the model trained with the aligned lyrics indeed results in a better performance than the model that was not informed. The question now remains whether the increase of performance is actually due to the phonetic contents that lie in the informed aligned lyrics or not. To this end, we investigated the source of performance increase in multifaceted ways by observing the change of performance when incorrect lyrics were given to the model. Experiment results show that the model can use not only just vocal activity information but also the phonetic contents from the aligned lyrics."
Estefania Cano;Fernando Mora Ángel;Gustavo Adolfo López Gil;José Ricardo Zapata;Antonio Escamilla;Juan Fernando Alzate Londoño;Moises Betancur Pelaez,Sesquialtera in the Colombian bambuco: Perception and estimation of beat and meter,2020,https://doi.org/10.5281/zenodo.4245454,"Estefanía Cano, Fraunhofer IDMT, Germany;Fernando Mora-Ángel, Universidad de Antioquia, Colombia;Gustavo A. López Gil, Universidad de Antioquia, Colombia;José R. Zapata, Universidad Pontiﬁcia Bolivariana , Colombia;Antonio Escamilla, Universidad Pontiﬁcia Bolivariana , Colombia;Juan F. Alzate, Universidad de Antioquia, Colombia;Moisés Betancur, Universidad de Antioquia, Colombia","The bambuco, one of the national rhythms of Colombia, is characterized by the presence of sesquialteras or the superposition of rhythmic elements from two meters. In this work, we analyze sesquialteras in bambucos from two perspectives. First, we analyze the perception of beat and meter by asking 10 Colombian musicians to perform beat annotations in a dataset of bambucos. Results show  great diversity in the annotations: a total of five different meters or meter combinations were found in the annotations, with each bambuco in the study being annotated in at least two different meters. Second, we perform a beat tracking analysis in a dataset of bambucos with two state-of-the-art algorithms.  Given that the algorithms used in the analysis were designed to deal with the rhythmic regularity of a single meter, it is not surprising that tracking performance is not very high (~42% mean F-measure). However, a deeper analysis of  the onset detection functions used for beat tracking, indicate that there is enough information on the signal level to characterize the bi-metric behavior of bambucos. With this in mind, we highlight possibilities for computational analysis of rhythm in bambucos."
Yu-Fen Huang;Jeng-I Liang;I-CHIEH WEI;Li Su,Joint analysis of mode and playing technique in guqin performance with machine learning,2020,https://doi.org/10.5281/zenodo.4245378,"Yu-Fen Huang, Institute of Information Science, Academia Sinica, Taiwan;Jeng-I Liang, Department of Traditional Music, Taipei National University of the Arts, Taiwan;I-Chieh Wei, Institute of Information Science, Academia Sinica, Taiwan;Li Su, Institute of Information Science, Academia Sinica, Taiwan","Music is hierarchically structured, in which the global attributes (e.g., the determined tonal structure, musical form) dominate the distribution of local elements (e.g., pitch, playing technique arrangement). Existing methods for instrumental playing technique detection mostly focus on the local features extracted from audio. However, we argue that structural information is critical for both global and local tasks, particularly considering the characteristics of Guqin music. Incorporating mode and playing technique analysis, this study demonstrates that the structural relationship between notes is crucial for detecting mode, and such information also provides extra guidance for the playing technique detection in local-level. In this study, a new dataset is compiled for Guqin performance analysis. The mode detection is achieved by pattern matching, and the predicted results are conjoined with audio features to be inputted into a neural network for playing technique detection. Advanced techniques are developed to optimize the extracted pitch contour from the audio. It is manifest in the results that the global and local features are inter-connected in Guqin music. Our analysis identifies key components affecting the recognition of mode and playing technique, and challenging cases resulting from unique properties in Guqin audio signal are discussed for further research."
Yucong Jiang,Score following with hidden tempo using a switching state-space model,2020,https://doi.org/10.5281/zenodo.4245528,"Yucong Jiang, University of Richmond;Christopher Raphael, Indiana University Bloomington","A score-following program traces the notes in a musical score during a performance. This capability is essential to many meaningful applications that synchronize audio with a score in an on-line fashion. Existing algorithms often stumble on certain difficult cases, one of which is piano music. This paper presents a new method to tackle such cases. The method treats tempo as a variable rather than a constant (with constraints), allowing the program to adapt to live performance variations. This is first expressed by a Kalman filter model at the note level, and then by an almost equivalent switching state-space model at the audio frame level. The latter contains both discrete and continuous hidden variables, and is computationally intractable. We show how certain reasonable approximations make the computation manageable. This new method is tested on a dataset of 50 piano excerpts. Compared with a previously established state-of-the-art algorithm, the new method shows more stable and accurate results: it reduces fatal score-following errors, and improves accuracy from 65.0% to 69.1%."
Sangeun Kum;Jing-Hua Lin;Li Su;Juhan Nam,Semi-supervised learning using teacher-student models for vocal melody extraction,2020,https://doi.org/10.5281/zenodo.4245374,"Sangeun Kum, Graduate School of Culture Technology, KAIST, South Korea;Jing-Hua Lin, Institute of Information Science, Academia Sinica, Taiwan;Li Su, Institute of Information Science, Academia Sinica, Taiwan;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea","The lack of labeled data is a major obstacle in many music information retrieval tasks such as melody extraction, where labeling is extremely laborious or costly. Semi-supervised learning (SSL) provides a solution to alleviate the issue by leveraging a large amount of unlabeled data. In this paper, we propose an SSL method using teacher-student models for vocal melody extraction. The teacher model is pre-trained with labeled data and guides the student model to make identical predictions given unlabeled input in a self-training setting. We examine three setups of teacher-student models with different data augmentation schemes and loss functions. Also, considering the scarcity of labeled data in the test phase, we artificially generate large-scale testing data with pitch labels from unlabeled data using an analysis-synthesis method. The results show that the SSL method significantly increases the performance against supervised learning only and the improvement depends on the teacher-student models, the size of unlabeled data, the number of self-training iterations, and other training details. We also find that it is essential to ensure that the unlabeled audio has vocal parts. Finally, we show that the proposed SSL method allows a simple convolutional recurrent neural network model to achieve performance comparable to state-of-the-arts."
Yin-Jyun Luo;Kin Wai Cheuk;Tomoyasu Nakano;Masataka Goto;Dorien Herremans,Unsupervised disentanglement of pitch and timbre for isolated musical instrument sounds,2020,https://doi.org/10.5281/zenodo.4245532,"Yin-Jyun Luo, Singapore University of Technology and Design (SUTD), Singapore;Kin Wai Cheuk, Singapore University of Technology and Design (SUTD), Singapore;Tomoyasu Nakano, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Dorien Herremans, Singapore University of Technology and Design (SUTD), Singapore","Disentangling factors of variation aims to uncover latent variables that underlie the process of data generation. In this paper, we propose a framework that achieves unsupervised pitch and timbre disentanglement for isolated musical instrument sounds without relying on data annotations or pre-trained neural networks. Our framework, based on variational auto-encoders, takes as input a spectral frame, and encodes pitch and timbre as categorical and continuous variables, respectively. The input is then reconstructed by combining those variables. Under an unsupervised training setting, a major challenge is that encoders are tasked to capture factors of interest with distinct latent representations, without access to the corresponding ground-truth labels. We therefore introduce auxiliary tasks and objectives which leverage pitch shifting as a strategy to create surrogate labels, thereby encouraging the disentanglement of pitch and timbre. Through an ablation study we analyze the impact of the proposed objectives. The evaluation shows the efficacy of the proposed framework for learning disentangled representations, and verifies its applicability to unsupervised pitch classification and conditional spectral synthesis."
Chitralekha Gupta;Lin Huang;Haizhou Li,Automatic rank-ordering of singing vocals with twin-neural network,2020,https://doi.org/10.5281/zenodo.4245458,"Chitralekha Gupta, Department of Electrical and Computer Engineering, National University of Singapore, Singapore;Lin Huang, Department of Electrical and Computer Engineering, National University of Singapore, Singapore;Haizhou Li, Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Machine Listening Lab, University of Bremen, Germany","When making judgements, humans are known to be better at choosing a preferred option amongst a small number of options, rather than giving an absolute ranking of all the options. This preference-based judgment rank-ordering method is called Best-Worst Scaling (BWS). Inspired by this concept, we propose a preference-based framework to generate a relative rank-ordering of singing vocals, and therefore, singers. We adopt a twin-neural network (Siamese) that learns to choose a preferred candidate in terms of singing quality between two inputs. With a few such pairwise comparisons, this method generates a relative rank-order of a complete list of singers. Additionally, we incorporate a knowledge-based musically-relevant pitch histogram representation, as a conditioning vector, to provide explicit musical information to the network. The experiments show that this method is able to reliably evaluate singing quality and rank-order singing vocals, independent of the song or the singer. The results suggest that the twin-neural network learns the underlying discerning properties relevant to singing quality, instead of being specific to the content of a song or singer."
Cheng-Zhi Anna Huang;Hendrik Vincent Koops;Ed Newton-Rex;Monica Dinculescu;Carrie Cai,AI song contest: Human-AI co-creation in songwriting,2020,https://doi.org/10.5281/zenodo.4245530,"Cheng-Zhi Anna Huang, Google Brain;Hendrik Vincent Koops, RTL Netherlands;Ed Newton-Rex, ByteDance;Monica Dinculescu, Google Brain;Carrie J. Cai, Google Brain","Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression."
Kosetsu Tsukuda;Masataka Goto,Analysis of song/artist latent features and its application for song search,2020,https://doi.org/10.5281/zenodo.4245538,"Kosetsu Tsukuda, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","For recommending songs to a user, one effective approach is to represent artists and songs with latent vectors and predict the user's preference toward the songs. Although the latent vectors represent the characteristics of artists and songs well, they have typically been used only for computing the preference score. In this paper, we discuss how we can leverage these vectors for realizing applications that enable users to search for songs from new perspectives. To this end, by embedding song/artist vectors into the same feature space, we first propose two concepts of artist-song relationships: overall similarity and prominent affinity. Overall similarity is the degree to which the characteristics of a song are similar overall to the characteristics of the artist; while prominent affinity is the degree to which a song prominently represents the characteristics of the artist. By using Last.fm play logs for two years, we analyze the characteristics of the concepts. Moreover, based on the analysis results, we propose three applications for song search. Through case studies, we demonstrate that our proposed applications are beneficial for searching for songs according to the users' various search intents."
Andrew McLeod;James Owers;Kazuyoshi Yoshii,The MIDI degradation toolkit: Symbolic music augmentation and correction,2020,https://doi.org/10.5281/zenodo.4245566,"Andrew McLeod, EPFL;James Owers, University of Edinburgh;Kazuyoshi Yoshii, Kyoto University","In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a ``degraded'' version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difficulty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK's degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community."
Hao-Wen Dong;Ke Chen;Julian McAuley;Taylor Berg-Kirkpatrick,MusPy: A toolkit for symbolic music generation,2020,https://doi.org/10.5281/zenodo.4245380,"Hao-Wen Dong, University of California San Diego;Ke Chen, University of California San Diego;Julian McAuley, University of California San Diego;Taylor Berg-Kirkpatrick, University of California San Diego","In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy ."
Philippe Esling;Théis Bazin;Adrien Bitton;Tristan J. J. Carsault;Ninon Devis,Ultra-light deep MIR by trimming lottery tickets,2020,https://doi.org/10.5281/zenodo.4245492,"Philippe Esling, IRCAM - Sorbonne Université, CNRS UMR 9912 - 1, place Igor Stravinsky, Paris, France;Theis Bazin, IRCAM - Sorbonne Université, CNRS UMR 9912 - 1, place Igor Stravinsky, Paris, France;Adrien Bitton, IRCAM - Sorbonne Université, CNRS UMR 9912 - 1, place Igor Stravinsky, Paris, France;Tristan Carsault, IRCAM - Sorbonne Université, CNRS UMR 9912 - 1, place Igor Stravinsky, Paris, France;Ninon Devis, IRCAM - Sorbonne Université, CNRS UMR 9912 - 1, place Igor Stravinsky, Paris, France","Current state-of-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all discriminative tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success. In this paper, we address this issue by developing a new approach based on the recent lottery ticket hypothesis. We modify the original lottery approach to allow for explicitly removing parameters, through structured trimming of entire units, instead of simply masking individual weights. This allows to obtain models which are effectively lighter in terms of size, memory and number of operations.We show that our proposal allows to remove up to 95% of the models parameters without loss of accuracy, leading to ultra-light deep MIR models. We confirm the surprising result that, at smaller compression ratios (removing up to 90% of the network), lighter models consistently outperform their heavier counterpart. We exhibit these results on a large array of MIR tasks including audio classification, pitch recognition, chord extraction, drum transcription and onset estimation. These resulting ultra-light deep models for MIR can run on CPU, and can even fit on embedded devices with minimal degradation of accuracy."
Gabriel Meseguer Brocal;Rachel Bittner;Simon Durand;Brian Brost,Data cleansing with contrastive learning for vocal note event annotations,2020,https://doi.org/10.5281/zenodo.4245420,"Gabriel Meseguer-Brocal, STMS UMR9912, Ircam/CNRS/SU, Paris;Rachel Bittner, Spotify, USA;Simon Durand, Spotify, USA;Brian Brost, Spotify, USA","Data cleansing is a well studied strategy for cleaning erroneous labels in datasets, which has not yet been widely adopted in Music Information Retrieval.Previously proposed data cleansing models do not consider structured (e.g. time varying) labels, such as those common to music data.We propose a novel data cleansing model for time-varying, structured labels which exploits the local structure of the labels, and demonstrate its usefulness for vocal note event annotations in music.Our model is trained in a contrastive learning manner by automatically contrasting likely correct labels pairs against local deformations of them.We demonstrate that the accuracy of a transcription model improves greatly when trained using our proposed data cleaning strategy compared with the accuracy when trained using the original dataset.Additionally we use our model to estimate the annotation error rates in the DALI dataset, and highlight other potential uses for this type of model."
Juan S. Gómez-Cañón;Estefania Cano;Perfecto Herrera;Emilia Gomez,Joyful for you and tender for us: the influence of individual characteristics and language on emotion labeling and classification,2020,https://doi.org/10.5281/zenodo.4245568,"Juan Sebastián Gómez-Cañón, Music Technology Group, Universitat Pompeu Fabra, Spain;Estefanía Cano, Songquito UG, Erlangen, Germany;Perfecto Herrera, Music Technology Group, Universitat Pompeu Fabra, Spain;Emilia Gómez, European Commission, Joint Research Centre, Seville, Spain","Tagging a musical excerpt with an emotion label may result in a vague and ambivalent exercise. This subjectivity entangles several high-level music description tasks when the computational models built to address them produce predictions on the basis of a ""ground truth"". In this study, we investigate the relationship between emotions perceived in pop and rock music (mainly in Euro-American styles) and personal characteristics from the listener, using language as a key feature. Our goal is to understand the influence of lyrics comprehension on music emotion perception and use this knowledge to improve Music Emotion Recognition (MER) models. We systematically analyze over 30K annotations of 22 musical fragments to assess the impact of individual differences on agreement, as defined by Krippendorff's coefficient. We employ personal characteristics to form group-based annotations by assembling ratings with respect to listeners' familiarity, preference, lyrics comprehension, and music sophistication. Finally, we study our group-based annotations in a two-fold approach: (1) assessing the similarity within annotations using manifold learning algorithms and unsupervised clustering, and (2) analyzing their performance by training classification models with diverse ""ground truths"". Our results suggest that a) applying a broader categorization of taxonomies and b) using multi-label, group-based annotations based on language, can be beneficial for MER models."
Francisco J. Castellanos;Jorge Calvo-Zaragoza;Jose M. Inesta,A neural approach for full-page optical music recognition of mensural documents,2020,https://doi.org/10.5281/zenodo.4245494,"Francisco J. Castellanos, Department of Software and Computing Systems, University of Alicante, Spain;Jorge Calvo-Zaragoza, Department of Software and Computing Systems, University of Alicante, Spain;Jose M. Inesta, Department of Software and Computing Systems, University of Alicante, Spain","The digitization of the content within musical manuscripts allows the possibility of preserving, disseminating, and exploiting that cultural heritage. The automation of this process has been object of study for a long time in the field of Optical Music Recognition (OMR), with a wide variety of proposed solutions. Currently, there is a tendency to use machine learning strategies based on neural networks because of their high performance and flexibility to adapt to different scenarios by changing only the training data. However, most of the recent literature addresses only specific parts of the traditional OMR workflow such as music object detection or symbol classification. In this paper, we progress one step further by proposing a full-page OMR system for Mensural notation scores that consists of simply two processes, which are enough to extract the symbolic music information from a full page. More precisely, our pipeline uses Selectional Auto-Encoders to extract single staff regions, combined with end-to-end staff-level recognition based on Convolutional Recurrent Neural Networks for retrieving the music notation. The results confirm the adequacy of our method, reporting a successful behavior on two Mensural collections (Capitan and Seils datasets) with a straightforward implementation."
Fred Bruford;Olivier Lartillot;SKoT McDonald;Mark B. Sandler,Multidimensional similarity modelling of complex drum loops using the GrooveToolbox,2020,https://doi.org/10.5281/zenodo.4245418,"Fred Bruford, Centre for Digital Music, Queen Mary University, United Kingdom;Olivier Lartillot, RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, University of Oslo, Norway;SKoT McDonald, inMusic Brands inc, United States;Mark Sandler, Centre for Digital Music, Queen Mary University, United Kingdom","The GrooveToolbox is a new Python library implementing numerous algorithms, both novel and pre-existing, for the analysis of symbolic drum loops, including rhythm features, similarity metrics and microtiming features. As part of the GrooveToolbox we introduce two new metrics of rhythm similarity and four new features for describing the perceptually salient properties of microtiming deviations in drum loops. Based on a two-part perceptual evaluation, we show these four new microtiming features can each correlate to similarity perception, and be used along with rhythm similarity metrics to improve personalized similarity models for complex drum loops. A new measure of structural rhythmic similarity is also shown to correlate more strongly to similarity perception of drum loops than the more commonly used Hamming distance. These results point to the potential application of the GrooveToolbox and its new features in drum loop analysis for intelligent music production tools. The GrooveToolbox may be found at:  https://github.com/fredbru/GrooveToolbox"
André Ofner;Sebastian Stober,Modeling perception with hierarchical prediction: Auditory segmentation with deep predictive coding locates candidate evoked potentials in EEG,2020,https://doi.org/10.5281/zenodo.4245496,"André Ofner and Sebastian Stober, Otto von Guericke University, Magdeburg, Germany","The human response to music combines low-level expectations that are driven by the perceptual characteristics of audio with high-level expectations from the context and the listener's expertise. This paper discusses surprisal based music representation learning with a hierarchical predictive neural network. In order to inspect the cognitive validity of the network's predictions along their time-scales, we use the network's prediction error to segment electroencephalograms (EEG) based on the audio signal. Using the NMED-T dataset on passive natural music listening we explore the automatic segmentation of audio and EEG into events using the suggested model. By averaging only the EEG signal at predicted locations, we were able to visualize auditory evoked potentials connected to local and global musical structures. This indicates the potential of unsupervised predictive learning with deep neural networks as means to retrieve musical structure from audio and as a basis to uncover the corresponding cognitive processes in the human brain."
HAO HAO TAN;Dorien Herremans,Music FaderNets: Controllable music generation based on high-Level features via low-level feature modelling,2020,https://doi.org/10.5281/zenodo.4245376,"Hao Hao Tan, Singapore University of Technology and Design;Dorien Herremans, Singapore University of Technology and Design","High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate ""sliding faders"" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the ""faders"" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test."
Sebastian Böck;Matthew Davies,"Deconstruct, analyse, reconstruct: How to improve tempo, beat, and downbeat estimation",2020,https://doi.org/10.5281/zenodo.4245498,"Sebastian Böck, enliteAI, Vienna, Austria;Matthew E. P. Davies, University of Coimbra, CISUC, DEI","In this paper, we undertake a critical assessment of a state-of-the-art deep neural network approach for computational rhythm analysis. Our methodology is to deconstruct this approach, analyse its constituent parts, and then reconstruct it. To this end, we devise a novel multi-task approach for the simultaneous estimation of tempo, beat, and downbeat. In particular, we seek to embed more explicit musical knowledge into the design decisions in building the network. We additionally reflect this outlook when training the network, and include a simple data augmentation strategy to increase the network's exposure to a wider range of tempi, and hence beat and downbeat information. Via an in-depth comparative evaluation, we present state-of-the-art results over all three tasks, with performance increases of up to 6% points over existing systems."
Bo-Yu Chen;Jordan B. L. Smith;Yi-Hsuan Yang,Neural loop combiner: Neural network models for assessing the compatibility of loops,2020,https://doi.org/10.5281/zenodo.4245462,"Bo-Yu Chen, Academia Sinica, Taiwan;Jordan B. L. Smith, TikTok, London, UK;Yi-Hsuan Yang, Academia Sinica, Taiwan","Music producers who use loops may have access to thousands in loop libraries, but finding ones that are compatible is a time-consuming process; we hope to reduce this burden with automation. State-of-the-art systems for estimating compatibility, such as AutoMashUpper, are mostly rule-based and could be improved on with machine learning. To train a model, we need a large set of loops with ground truth compatibility values. No such dataset exists, so we extract loops from existing music to obtain positive examples of compatible loops, and propose and compare various strategies for choosing negative examples. For reproducibility, we curate data from the Free Music Archive. Using this data, we investigate two types of model architectures for estimating the compatibility of loops: one based on a Siamese network, and the other a pure convolutional neural network (CNN). We conducted a user study in which participants rated the quality of the combinations suggested by each model, and found the CNN to outperform the Siamese network. Both model-based approaches outperformed the rule-based one. We have opened source the code for building the models and the dataset."
Peter Van Kranenburg (Utrecht University;Meertens Institute)*,Rule mining for local boundary detection in melodies,2020,https://doi.org/10.5281/zenodo.4245422,"Peter van Kranenburg, Meertens Instituut, Utrecht University","The task of melodic segmentation is a long-standing MIR task that has not yet been solved. In this paper, a rule mining algorithm is employed to find rule sets that classify notes within their local context as phrase boundaries. Both the discovered rule set and a Random Forest Classifier trained on the same data set outperform previous methods on the task of melodic segmentation of melodies from the Essen Folk Song Collection, the Meertens Tune Collections, and the set of Bach Chorales. By inspecting the rules, some important clues are revealed about what constitutes a melodic phrase boundary, notably a prevalence of rhythm features over pitch features."
Jaehun Kim;Andrew M. Demetriou;Sandy Manolios;M. Stella Tavella;Cynthia C. S. Liem,"""Butter lyrics over hominy grit"": Comparing audio and psychology-based text features in MIR tasks",2020,https://doi.org/10.5281/zenodo.4245574,"Jaehun Kim, Delft University of Technology, Netherlands;Andrew M. Demetriou, Delft University of Technology, Netherlands;Sandy Manolios, ;M. Stella Tavella, Musixmatch, Bologna, Italy;Cynthia C. S. Liem, Delft University of Technology, Netherlands","Psychology research has shown that song lyrics are a rich source of data, yet they are often overlooked in the field of MIR compared to audio. In this paper, we provide an initial assessment of the usefulness of features drawn from lyrics for various fields, such as MIR and Music Psychology. To do so, we assess the performance of lyric-based text features on 3 MIR tasks, in comparison to audio features. Specifically, we draw sets of text features from the field of Natural Language Processing and Psychology. Further, we estimate their effect on performance while statistically controlling for the effect of audio features, by using a hierarchical regression statistical model. Lyric-based features show a small but statistically significant effect, that anticipates further research. Implications and directions for future studies are discussed."
Bas Cornelissen;Willem Zuidema;John Ashley Burgoyne,Mode classification and natural units in plainchant,2020,https://doi.org/10.5281/zenodo.4245572,"Bas Cornelissen, Institute for Logic, Language and Computation, University of Amsterdam;Willem Zuidema, Institute for Logic, Language and Computation, University of Amsterdam;John Ashley Burgoyne, Institute for Logic, Language and Computation, University of Amsterdam","Many musics across the world are structured around multiple modes, which hold a middle ground between scales and melodies. We study whether we can classify mode in a corpus of 20,865 medieval plainchant melodies from the Cantus database. We revisit the traditional 'textbook' classification approach (using the final, the range and initial note) as well as the only prior computational study we are aware of, which uses pitch profiles. Both approaches work well, but largely reduce modes to scales and ignore their melodic character. Our main contribution is a model that reaches 93–95% F1 score on mode classification, compared to 86–90% using traditional pitch-based musicological methods. Importantly, it reaches 81–83% even when we discard all absolute pitch information and reduce a melody to its contour. The model uses tf–idf vectors and strongly depends on the choice of units: i.e., how the melody is segmented. If we borrow the syllable or word structure from the lyrics, the model outperforms all of our baselines. This suggests that, like language, music is made up of 'natural' units, in our case between the level of notes and complete phrases, a finding that may well be useful in other musics."
Jacob deGroot-Maggetti;Timothy R de Reuse;Laurent Feisthauer;Samuel Howes;Yaolong Ju;Suzuka Kokubu;Sylvain Margot;Néstor Nápoles López;Finn Upham,Data quality matters: Iterative corrections on a corpus of Mendelssohn string quartets and implications for MIR analysis,2020,https://doi.org/10.5281/zenodo.4245460,"Jacob deGroot-Maggetti, Schulich School of Music, McGill University, Canada;Timothy de Reuse, Schulich School of Music, McGill University, Canada;Laurent Feisthauer, Department of Information, University of Lille, France;Samuel Howes, Schulich School of Music, McGill University, Canada;Yaolong Ju, Schulich School of Music, McGill University, Canada;Suzuka Kokubu, Schulich School of Music, McGill University, Canada;Sylvain Margot, Schulich School of Music, McGill University, Canada;Néstor Nápoles López, Schulich School of Music, McGill University, Canada;Finn Upham, Schulich School of Music, McGill University, Canada","In this paper, we describe a workflow of successive corrections on Optical Music Recognition (OMR) generated MusicXML files and their respective outputs under Music Information Retrieval tasks. The original OMR-generated files of six Mendelssohn String Quartets were initially corrected by individual members of this interdisciplinary group, then reviewed by others to further standardize the quality and music analysis priorities of the team. Four MIR tasks are applied to each round of corrections on this collection: cadence detection, chord labeling, key finding, and monophonic pattern discovery.We measure changes in the outputs of these four MIR tasks from one round of correction to the next in order to evaluate the impact of corrections. Results show that expert revision is more beneficial to some MIR tasks than to others. The resulting corpus of curated MusicXML files is available as an open-source repository under a Creative Commons Attribution 4.0 International License for further MIR research."
Luca Angioloni;Valentijn Borghuis;Lorenzo Brusci;Paolo Frasconi,"CONLON: A pseudo-song generator based on a new pianoroll, Wasserstein autoencoders, and optimal interpolations",2020,https://doi.org/10.5281/zenodo.4245576,"Luca Angioloni, DINFO, Università di Firenze, Italy;Tijn Borghuis, Eindhoven University of Technology, The Netherlands;Lorenzo Brusci, Musi-co, Eindhoven, The Netherlands;Paolo Frasconi, DINFO, Università di Firenze, Italy","We introduce CONLON, a pattern-based MIDI generation method that employs a new lossless pianoroll-like data description in which velocities and durations are stored in separate channels. CONLON uses Wasserstein autoencoders as the underlying generative model. Its generation strategy is similar to interpolation, where MIDI pseudo-songs are obtained by concatenating patterns decoded from smooth trajectories in the embedding space, but aims to produce a smooth result in the pattern space by computing optimal trajectories as the solution of a widest-path problem. A set of surveys enrolling 69 professional musicians shows that our system, when trained on datasets of carefully selected and coherent patterns, is able to produce pseudo-songs that are musically consistent and potentially useful for professional musicians. Additional materials can be found at https://paolo-f.github.io/CONLON/ ."
Guillaume Doras;Furkan Yesiler;Joan Serra;Emilia Gomez;Geoffroy Peeters,Combining musical features for cover detection,2020,https://doi.org/10.5281/zenodo.4245424,"Guillaume Doras, Sacem, France Ircam, CNRS, Sorbonne Université, STMS Lab, France;Furkan Yesiler, ;Joan Serrà, Dolby Laboratories, Spain;Emilia Gómez, Music Technology Group, Universitat Pompeu Fabra, Spain European Commission, Joint Research Centre, Spain;Geoffroy Peeters, Telecom Paris, LTCI, France","Recent works have addressed the automatic cover detection problem from a metric learning perspective. They employ different input representations, aiming to exploit melodic or harmonic characteristics of songs and yield promising performances. In this work, we propose a comparative study of these different representations and show that systems combining melodic and harmonic features drastically outperform those relying on a single input representation. We illustrate how these features complement each other with both quantitative and qualitative analyses. We finally investigate various fusion schemes and propose methods yielding state-of-the-art performances on two publicly-available large datasets."
Antonio Ramires;Frederic Font;Dmitry Bogdanov;Jordan B. L. Smith;Yi-Hsuan Yang;Joann Ching;Bo-Yu Chen;Yueh-Kao Wu;Hsu Wei-Han;Xavier Serra,The freesound loop dataset and annotation tool,2020,https://doi.org/10.5281/zenodo.4245430,"António Ramires, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Frederic Font, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Dmitry Bogdanov, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Jordan B. L. Smith, TikTok, Lodon, United Kingdom;Yi-Hsuan Yang, Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan;Joann Ching, Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan;Bo-Yu Chen, Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan;Yueh-Kao Wu, Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan;Hsu Wei-Han, Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Music loops are essential ingredients in electronic music production, and there is a high demand for pre-recorded loops in a variety of styles. Several commercial and community databases have been created to meet this demand, but most of them are not suitable for research due to their strict licensing. In this paper, we present the Freesound Loop Dataset (FSLD), a new large-scale dataset of music loops annotated by experts. The loops originate from Freesound, a community database of audio recordings released under Creative Commons licenses, so the audio in our dataset may be redistributed. The annotations include instrument, meter, key and genre tags. We describe the methodology used to assemble and annotate the data, and report on the distribution of tags in the data and inter-annotator agreement. We also present to the community an online loop annotator tool that we developed. To illustrate the usefulness of FSLD, we present short case studies on using it to estimate tempo and key, generate new loops, and evaluate a loop separation algorithm. We anticipate that the community will find yet more uses for the data, in applications from automatic loop characterisation to algorithmic composition."
Furkan Yesiler;Joan Serra;Emilia Gomez,Less is more: Faster and better music version identification with embedding distillation,2020,https://doi.org/10.5281/zenodo.4245570,"Furkan Yesiler, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Joan Serrà, Dolby Laboratories, Barcelona, Spain;Emilia Gómez, European Commission, Joint Research Centre, Seville, Spain","Version identification systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made significant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99% smaller embeddings that, moreover, yield up to a 3% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop."
Karim M. Ibrahim;Elena V. Epure;Geoffroy Peeters;Gael Richard,Should we consider the users in contextual music auto-tagging models?,2020,https://doi.org/10.5281/zenodo.4245426,"Karim M. Ibrahim, LTCI, Télécom Paris, Institut Polytechnique de Paris;Elena V. Epure, Deezer Research;Geoffroy Peeters, LTCI, Télécom Paris, Institut Polytechnique de Paris;Gaël Richard, LTCI, Télécom Paris, Institut Polytechnique de Paris","Music tags are commonly used to describe and categorize music. Various auto-tagging models and datasets have been proposed for the automatic music annotation with tags. However, the past approaches often neglect the fact that many of these tags largely depend on the user, especially the tags related to the context of music listening. In this paper, we address this problem by proposing a user-aware music auto-tagging system and evaluation protocol. Specifically, we use both the audio content and user information extracted from the user listening history to predict contextual tags for a given user/track pair. We propose a new dataset of music tracks annotated with contextual tags per user. We compare our model to the traditional audio-based model and study the influence of user embeddings on the classification quality. Our work shows that explicitly modeling the user listening history into the automatic tagging process could lead to more accurate estimation of contextual tags."
Derek S Cheng;Thorsten Joachims;Douglas Turnbull,Exploring acoustic similarity for novel music recommendation,2020,https://doi.org/10.5281/zenodo.4245500,"Derek Cheng, Cornell University;Thorsten Joachims, Cornell University;Douglas Turnbull, Ithaca College","Most commercial music services rely on collaborative filtering to recommend artists and songs. While this method is effective for popular artists with large fanbases, it can present difficulties for recommending novel, lesser known artists due to a relative lack of user preference data. In this paper, we therefore seek to understand how content-based approaches can be used to more effectively recommend songs from these lesser known artists. Specifically, we conduct a user study to answer three questions. Firstly, do most users agree which songs are most acoustically similar? Secondly, is acoustic similarity a good proxy for how an individual might construct a playlist or recommend music to a friend? Thirdly, if so, can we find acoustic features that are related to human judgments of acoustic similarity? To answer these questions, our study asked 117 test subjects to compare two unknown candidate songs relative to a third known reference song. Our findings show that 1) judgments about acoustic similarity are fairly consistent, 2) acoustic similarity is highly correlated with playlist selection and recommendation, but not necessarily personal preference, and 3) we identify a subset of acoustic features from the Spotify Web API that is particularly predictive of human similarity judgments."
Wayne Chi;Prachi Kumar;Suri Yaddanapudi;Suresh Rahul;Umut Isik,Generating music with a self-correcting non-chronological autoregressive model,2020,https://doi.org/10.5281/zenodo.4245578,"Wayne Chi, Amazon Web Services;Prachi Kumar, Amazon Web Services;Suri Yaddanapudi, Amazon Web Services;Rahul Suresh, Amazon Web Services;Umut Isik, Amazon Web Services","We describe a novel approach for generating music using a self-correcting, non-chronological, autoregressive model. We represent music as a sequence of edit events, each of which denotes either the addition or removal of a note---even a note previously generated by the model. During inference, we generate one edit event at a time using direct ancestral sampling. Our approach allows the model to fix previous mistakes such as incorrectly sampled notes and prevent accumulation of errors which autoregressive models are prone to have. Another benefit of our approach is a finer degree of control during human and AI collaboration as our approach is notewise online. We show through quantitative metrics and human survey evaluation that our approach generates better results than orderless NADE and Gibbs sampling approaches."
Helena Cuesta;Brian McFee;Emilia Gomez,Multiple F0 estimation in vocal ensembles using convolutional neural networks,2020,https://doi.org/10.5281/zenodo.4245434,"Helena Cuesta, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Brian McFee, Music and Audio Research Lab & Center for Data Science, New York University, USA;Emilia Gómez, Music Technology Group, Universitat Pompeu Fabra, Barcelona","This paper addresses the extraction of multiple F0 values from polyphonic and a cappella vocal performances using convolutional neural networks (CNNs). We address the major challenges of ensemble singing, i.e., all melodic sources are vocals and singers sing in harmony. We build upon an existing architecture to produce a pitch salience function of the input signal, where the harmonic constant-Q transform (HCQT) and its associated phase differentials are used as an input representation. The pitch salience function is subsequently thresholded to obtain a multiple F0 estimation output. For training, we build a dataset that comprises several multi-track datasets of vocal quartets with F0 annotations.This work proposes and evaluates a set of CNNs for this task in diverse scenarios and data configurations, including recordings with additional reverb. Our models outperform a state-of-the-art method intended for the same music genre when evaluated with an increased F0 resolution, as well as a general-purpose method for multi-F0 estimation. We conclude with a discussion on future research directions."
Javier Nistal;Stefan Lattner;Gaël Richard,DrumGAN: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks,2020,https://doi.org/10.5281/zenodo.4245504,"Javier Nistal, Sony CSL;Stefan Lattner, Sony CSL;Gaël Richard, LTCI, Télécom Paris, Institut Polytechnique de Paris","Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments."
Ioannis Petros Samiotis;Sihang Qiu;Andrea Mauri;Cynthia C. S. Liem;Christoph Lofi;Alessandro Bozzon,Microtask crowdsourcing for music score transcriptions: An experiment with error detection,2020,https://doi.org/10.5281/zenodo.4245580,"Ioannis Petros Samiotis, Delft University of Technology, Netherlands;Sihang Qiu, Delft University of Technology, Netherlands;Andrea Mauri, Delft University of Technology, Netherlands;Cynthia C. S. Liem, Delft University of Technology, Netherlands;Christoph Loﬁ, Delft University of Technology, Netherlands;Alessandro Bozzon, Delft University of Technology, Netherlands","Human annotation is still an essential part of modern transcription workflows for digitizing music scores, either as a standalone approach where a single expert annotator transcribes a complete score, or for supporting an automated Optical Music Recognition (OMR) system. Research on human computation has shown the effectiveness of crowdsourcing for scaling out human work by defining a large number of microtasks which can easily be distributed and executed. However, microtask design for music transcription is a research area that remains unaddressed. This paper focuses on the design of a crowdsourcing task to detect errors in a score transcription which can be deployed in either automated or human-driven transcription workflows. We conduct an experiment where we study two design parameters: 1) the size of the score to be annotated and 2) the modality in which it is presented in the user interface. We analyze the performance and reliability of non-specialised crowdworkers on Amazon Mechanical Turk with respect to these design parameters, differentiated by worker experience and types of transcription errors. Results are encouraging, and pave the way for scalable and efficient crowd-assisted music transcription systems."
Pritish Chandna;Helena Cuesta;Emilia Gomez,A deep learning based analysis-synthesis framework for unison singing,2020,https://doi.org/10.5281/zenodo.4245502,"Pritish Chandna, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Helena Cuesta, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Emilia Gómez, Music Technology Group, Universitat Pompeu Fabra, Barcelona","Unison singing is the name given to an ensemble of singers simultaneously singing the same melody and lyrics. While each individual singer in a unison sings the same principle melody, there are slight timing and pitch deviations between the singers, which, along with the ensemble of timbres, give the listener a perceived sense of ""unison"". In this paper, we present a study of unison singing in the context of choirs; utilising some recently proposed deep-learning based methodologies, we analyse the fundamental frequency (F0) distribution of the individual singers in recordings of unison mixtures. Based on the analysis, we propose a system for synthesising a unison signal from an a cappella input and a single voice prototype representative of a unison mixture. We use subjective listening test to evaluate perceptual factors of our proposed system for synthesis, including quality, adherence to the melody as well the degree of perceived unison."
Albin Correya;Dmitry Bogdanov;Luis Joglar-Ongay;Xavier Serra,Essentia.js: A JavaScript library for music and audio analysis on the web,2020,https://doi.org/10.5281/zenodo.4245510,"Albin Correya, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Dmitry Bogdanov, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Luis Joglar-Ongay, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Open-source software libraries for audio/music analysis and feature extraction have a significant impact on the development of Audio Signal Processing and Music Information Retrieval (MIR) systems. Despite the abundance of such tools on the native computing platforms, there is a lack of an extensive and easy-to-use reference library for audio feature extraction on the Web. In this paper, we present Essentia.js, an open-source JavaScript (JS) library for audio and music analysis on both web clients and JS-based servers. Along with the Web Audio API, it can be used for efficient and robust real-time audio feature extraction on the web browsers. Essentia.js is modular, lightweight, and easy-to-use, deploy, maintain, and integrate into the existing plethora of JS libraries and Web technologies. It is powered by a WebAssembly back-end of the Essentia C++ library, which facilitates a JS interface to a wide range of low-level and high-level audio features. It also provides a higher-level JS API and add-on MIR utility modules along with extensive documentation, usage examples, and tutorials. We benchmark the proposed library on two popular web browsers, Node.js engine, and Android devices, comparing it to the native performance of Essentia and Meyda JS library."
Gabriel Oliveira;Mariana Santos;Danilo B Seufitelli;Anisio Lacerda;Mirella M Moro,Detecting collaboration profiles in success-based music genre networks,2020,https://doi.org/10.5281/zenodo.4245534,"Gabriel P. Oliveira, Universidade Federal de Minas Gerais, Brazil;Mariana O. Silva, Universidade Federal de Minas Gerais, Brazil;Danilo B. Seuﬁtelli, Universidade Federal de Minas Gerais, Brazil;Anisio Lacerda, Universidade Federal de Minas Gerais, Brazil;Mirella M. Moro, Universidade Federal de Minas Gerais, Brazil","We analyze and identify collaboration profiles in success-based music genre networks. Such networks are built upon data recently collected from both global and regional Spotify weekly charts.  Overall, our findings reveal an increase in the number of distinct successful genres from high-potential markets, pointing out that local repertoire is more important than ever on building the global music ecosystem. We also detect collaboration patterns mapped into four different profiles: Solid, Regular, Bridge and Emerging, wherein the two first depict higher average success. These findings indicate  great opportunities for the music industry by revealing the driving power of inter-genre collaborations within regional and global markets."
Darius Petermann;Pritish Chandna;Helena Cuesta;Jordi Bonada;Emilia Gomez,Deep learning based source separation applied to choir ensembles,2020,https://doi.org/10.5281/zenodo.4245536,"Darius Petermann, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Pritish Chandna, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Helena Cuesta, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Jordi Bonada, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Emilia Gómez, European Commission, Joint Research Centre, Seville","Choral singing is a widely practiced form of ensemble singing wherein a group of people sing simultaneously in polyphonic harmony. The most commonly practiced setting for choir ensembles consists of four parts; Soprano, Alto, Tenor and Bass (SATB), each with its own range of fundamental frequencies (F0s). The task of source separation for this choral setting entails separating the SATB mixture into the constituent parts. Source separation for musical mixtures is well studied and many deep learning based methodologies have been proposed for the same. However, most of the research has been focused on a typical case which consists in separating vocal, percussion and bass sources from a mixture, each of which has a distinct spectral structure. In contrast, the simultaneous and harmonic nature of ensemble singing leads to high structural similarity and overlap between the spectral components of the sources in a choral mixture, making source separation for choirs a harder task than the typical case. This, along with the lack of an appropriate consolidated dataset has led to a dearth of research in the field so far. In this paper we first assess how well some of the recently developed methodologies for musical source separation perform for the case of SATB choirs. We then propose a novel domain-specific adaptation for conditioning the recently proposed U-Net architecture for musical source separation using the fundamental frequency contour of each of the singing groups and demonstrate that our proposed approach surpasses results from domain-agnostic architectures."
Carlos Eduardo Cancino-Chacón;Silvan Peter;Shreyan Chowdhury;Anna Aljanaki;Gerhard Widmer,On the characterization of expressive performance in classical music: First results of the con espressione game,2020,https://doi.org/10.5281/zenodo.4245506,"Carlos Cancino-Chacón, Austrian Research Institute for Artiﬁcial Intelligence, Vienna, Austria;Silvan Peter, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Shreyan Chowdhury, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Anna Aljanaki, Institute of Computer Science, University of Tartu, Estonia;Gerhard Widmer, Austrian Research Institute for Artiﬁcial Intelligence, Vienna, Austria","A piece of music can be expressively performed, or interpreted, in a variety of ways. With the help of an online questionnaire, the Con Espressione Game, we collected some 1,500 descriptions of expressive character relating to 45 performances of 9 excerpts from classical piano pieces, played by different famous pianists. More specifically, listeners were asked to describe, using freely chosen words (preferably: adjectives), how they perceive the expressive character of the different performances. In this paper, we offer a first account of this new data resource for expressive performance research, and provide an exploratory analysis, addressing three main questions: (1) how similarly do different listeners describe a performance of a piece? (2) what are the main dimensions (or axes) for expressive character emerging from this?; and (3) how do measurable parameters of a performance (e.g., tempo, dynamics) and mid- and high-level features that can be predicted by machine learning models (e.g., articulation, arousal) relate to these expressive dimensions? The dataset that we publish along with this paper was enriched by adding hand-corrected score-to-performance alignments, as well as descriptive audio features such as tempo and dynamics curves."
Yu Wang;Justin Salamon;Mark Cartwright;Nicholas J. Bryan;Juan P Bello,Few-shot drum transcription in polyphonic music,2020,https://doi.org/10.5281/zenodo.4245384,"Yu Wang, Music and Audio Research Lab, New York University, USA;Justin Salamon, Adobe Research, San Francisco, USA;Mark Cartwright, Music and Audio Research Lab, New York University, USA;Nicholas J. Bryan, Adobe Research, San Francisco, USA;Juan Pablo Bello, Music and Audio Research Lab, New York University, USA","Data-driven approaches to automatic drum transcription (ADT) are often limited to a predefined, small vocabulary of percussion instrument classes. Such models cannot recognize out-of-vocabulary classes nor are they able to adapt to finer-grained vocabularies. In this work, we address open vocabulary ADT by introducing few-shot learning to the task. We train a Prototypical Network on a synthetic dataset and evaluate the model on multiple real-world ADT datasets with polyphonic accompaniment. We show that, given just a handful of selected examples at inference time, we can match and in some cases outperform a state-of-the-art supervised ADT approach under a fixed vocabulary setting. At the same time, we show that our model can successfully generalize to finer-grained or extended vocabularies unseen during training, a scenario where supervised approaches cannot operate at all. We provide a detailed analysis of our experimental results, including a breakdown of performance by sound class and by polyphony."
Ashis Pati;Siddharth Kumar Gururani;Alexander Lerch,dMelodies: A music dataset for disentanglement learning,2020,https://doi.org/10.5281/zenodo.4245382,"Ashis Pati, Center for Music Technology, Georgia Institute of Technology, USA;Siddharth Gururani, Center for Music Technology, Georgia Institute of Technology, USA;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology, USA","Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (? 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised dis- entanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset."
Jongpil Lee;Nicholas J. Bryan;Justin Salamon;Zeyu Jin;Juhan Nam,Metric learning vs classification for disentangled music representation learning,2020,https://doi.org/10.5281/zenodo.4245468,"Jongpil Lee, Graduate School of Culture Technology, KAIST, Daejeon, South Korea;Nicholas J. Bryan, Adobe Research, San Francisco, CA, USA;Justin Salamon, Adobe Research, San Francisco, CA, USA;Zeyu Jin, Adobe Research, San Francisco, CA, USA;Juhan Nam, Graduate School of Culture Technology, KAIST, Daejeon, South Korea","Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classification, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning, classification, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classification, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We find that classification-based models are generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging."
Jiawen Huang;Yun-Ning Hung;Ashis Pati;Siddharth Kumar Gururani;Alexander Lerch,Score-informed networks for music performance assessment,2020,https://doi.org/10.5281/zenodo.4245582,"Jiawen Huang, Center for Music Technology, Georgia Institute of Technology, USA;Yun-Ning Hung, Center for Music Technology, Georgia Institute of Technology, USA;Ashis Pati, Center for Music Technology, Georgia Institute of Technology, USA;Siddharth Gururani, Center for Music Technology, Georgia Institute of Technology, USA;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology, USA","The assessment of music performances in most cases takes into account the underlying musical score being performed. While there have been several automatic approaches for objective music performance assessment (MPA) based on extracted features from both the performance audio and the score, deep neural network-based methods incorporating score information into MPA models have not yet been investigated. In this paper, we introduce three different models capable of score-informed performance assessment. These are (i) a convolutional neural network that utilizes a simple time-series input comprising of aligned pitch contours and score, (ii) a joint embedding model which learns a joint latent space for pitch contours and scores, and (iii) a distance matrix-based convolutional neural network which utilizes patterns in the distance matrix between pitch contours and musical score to predict assessment ratings. Our results provide insights into the suitability of different architectures and input representations and demonstrate the benefits of score-informed models as compared to score-independent models."
Jacopo de Berardinis;Angelo Cangelosi;Eduardo Coutinho,The multiple voices of musical emotions: Source separation for improving music emotion recognition models and their interpretability,2020,https://doi.org/10.5281/zenodo.4245428,"Jacopo de Berardinis, Machine Learning and Robotics Group, University of Manchester;Angelo Cangelosi, Machine Learning and Robotics Group, University of Manchester;Eduardo Coutinho, Applied Music Research Lab, University of Liverpool","Despite the manifold developments in music emotion recognition and related areas, estimating the emotional impact of music still poses many challenges. These are often associated to the complexity of the acoustic codes to emotion and the lack of large amounts of data with robust golden standards. In this paper, we propose a new computational model (EmoMucs) that considers the role of different musical voices in the prediction of the emotions induced by music. We combine source separation algorithms for breaking up music signals into independent song elements (vocals, bass, drums, other) and end-to-end state-of-the-art machine learning techniques for feature extraction and emotion modelling (valence and arousal regression). Through a series of computational experiments on a benchmark dataset using source-specialised models trained independently and different fusion strategies, we demonstrate that EmoMucs outperforms state-of-the-art approaches with the advantage of providing insights into the relative contribution of different musical elements to the emotions perceived by listeners."
Michael M Michelashvili;Lior Wolf,Hierarchical timbre-painting and articulation generation,2020,https://doi.org/10.5281/zenodo.4245584,"Michael Michelashvili, Tel Aviv University;Lior Wolf, Tel Aviv University","We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting."
Kyle Robinson;Dan Brown;Markus Schedl,User insights on diversity in music recommendation lists,2020,https://doi.org/10.5281/zenodo.4245464,"Kyle Robinson, David R. Cheriton School of Computer Science, University of Waterloo, Canada;Dan Brown, David R. Cheriton School of Computer Science, University of Waterloo, Canada;Markus Schedl, Institute of Computational Perception, Johannes Kepler University Linz, Austria","While many researchers have proposed various ways of quantifying recommendation list diversity, these approaches have had little input from users on their own perceptions and preferences in seeking diversity. Through an exploratory user study, we provide a better understanding of how users view the concept of diversity in music recommendations, and how they might optimise levels of intra-list diversity themselves. In our study, 17 participants interacted with and rated the suggestions from two different recommendation systems. One provided static top-7 collaborative filtering recommendations, and the other provided an interactive slider to re-rank these recommendations based on a continuous diversity scale. We also asked participants a series of free-form questions on music discovery and diversity in semi-structured interviews. User-preferred levels of diversity varied widely both within and between subjects. Although most users agreed that diversity is beneficial in music discovery, they also noted a risk of dissatisfaction from too much diversity. A key finding is that preference for diversification was often linked to user mood. Participants also expressed a clear distinction between diversity within existing preferences, and outside of existing preferences. These ideas of inner and outer diversity are not well defined within the bounds of current diversity metrics, and we discuss their implications."
Polina Proutskova;Anja Volk;Peyman Heidarian;Gyorgy Fazekas,From music ontology towards ethno-music-ontology,2020,https://doi.org/10.5281/zenodo.4245586,"Polina Proutskova, Center for Digital Music, Queen Mary University of London, UK;Anja Volk, Department of Information and Computing Sciences, Utrecht University, Netherlands;Peyman Heidarian, Department of Computer Science The University of Waikato, NZ;György Fazekas, Center for Digital Music, Queen Mary University of London, UK","This paper presents exploratory work investigating the suitability of the Music Ontology - the most widely used formal specification of the music domain - for modelling non-Western musical traditions. Four contrasting case studies from a variety of musical cultures are analysed: Dutch folk song research, reconstructive performance of rural Russian traditions, contemporary performance and composition of Persian classical music, and recreational use of a personal world music collection. We propose semantic models describing the respective do- mains and examine the applications of the Music Ontology for these case studies: which concepts can be successfully reused, where they need adjustments, and which parts of the reality in these case studies are not covered by the Mu- sic Ontology. The variety of traditions, contexts and modelling goals covered by our case studies sheds light on the generality of the Music Ontology and on the limits of generalisation ""for all musics"" that could be aspired for on the Semantic Web."
Antonio R. Parmezan;Diego Furtado Silva;Gustavo Batista,A combination of local approaches for hierarchical music genre classification,2020,https://doi.org/10.5281/zenodo.4245540,"Antonio R. S. Parmezan, Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo, São Carlos, Brazil;Diego Furtado Silva, Departamento de Computação – Universidade Federal de São Carlos, São Carlos, Brazil;Gustavo E. A. P. A. Batista, School of Computer Science and Engineering – University of New South Wales, Sydney, Australia","Labeling a music recording according to its genre is an intuitive and familiar way to describe its content. Music genres are valuable information, especially for music organization, personalized listening experience, and playlist generation. Automatically classifying music genres is a challenging endeavor due to the inherent ambiguity and subjectivity. Most efforts on music genre classification consider the complete independence between labels. However, music genres typically respect a hierarchical structure based on the influences or origins of each style. Conversely, many of the methods available for hierarchical classification are based on assumptions about the class hierarchy, such as the need for multiple children in each hierarchy's node, which may limit their use in music applications. Also, the local classifier per node approach that would be the most suitable for this scenario is costly regarding time and memory. In this paper, we present two local hierarchical classification approaches and show how to combine them to obtain a single one that is more robust and faithful to the music genre classification scenario. We evaluate our proposal in a music dataset hierarchically labeled with 120 music genres. As shown, compared to state-of-the-art approaches, our approach has a lower computational cost and can achieve competitive performances."
Jason Smith;Erin Truesdell;Jason Freeman;Brian Magerko;Kristy Boyer;Tom Mcklin,Modeling music and code knowledge to support a co-creative AI agent for education,2020,https://doi.org/10.5281/zenodo.4245386,"Jason Smith, Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, USA;Erin J.K. Truesdell, Expressive Machinery Lab, Georgia Institute of Technology, Atlanta, GA, USA;Jason Freeman, Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, USA;Brian Magerko, Expressive Machinery Lab, Georgia Institute of Technology, Atlanta, GA, USA;Kristy Elizabeth Boyer, Computer & Information Science & Engineering, University of Florida, Gainesville, FL, USA;Tom McKlin, The Findings Group, Decatur, Georgia, USA","EarSketch is an online environment for learning introductory computing concepts through code-driven, sample-based music production. This paper details the design and implementation of a module to perform code and music analyses on projects on the EarSketch platform. This analysis module combines inputs in the form of symbolic metadata, audio feature analysis, and user code to produce comprehensive models of user projects. The module performs a detailed analysis of the abstract syntax tree of a user's code to model use of computational concepts. It uses music information retrieval (MIR) and symbolic metadata to analyze users' musical design choices. These analyses produce a model containing users' coding and musical decisions, as well as qualities of the algorithmic music created by those decisions. The models produced by this module will support future development of CAI, a Co-creative Artificial Intelligence. CAI is designed to collaborate with learners and promote increased competency and engagement with topics in the EarSketch curriculum. Our module combines code analysis and MIR to further the educational goals of CAI and EarSketch and to explore the application of multimodal analysis tools to education."
Yun-Ning Hung;Alexander Lerch,Multitask learning for instrument activation aware music source separation,2020,https://doi.org/10.5281/zenodo.4245548,"Yun-Ning Hung, Center for Music Technology, Georgia Institute of Technology;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology","Music source separation is a core task in music information retrieval which has seen a dramatic improvement in the past years. Nevertheless, most of the existing systems focus exclusively on the problem of source separation itself and ignore the utilization of other~---possibly related---~MIR tasks which could lead to additional quality gains. In this work, we propose a novel multitask structure to investigate using instrument activation information to improve source separation performance. Furthermore, we investigate our system on six independent instruments, a more realistic scenario than the three instruments included in the widely-used MUSDB dataset, by leveraging a combination of the MedleyDB and Mixing Secrets datasets. The results show that our proposed multitask model outperforms the baseline Open-Unmix model on the mixture of Mixing Secrets and MedleyDB dataset while maintaining comparable performance on the MUSDB dataset."
Shih-Lun Wu;Yi-Hsuan Yang,The jazz transformer on the front line: Exploring the shortcomings of AI-composed music through quantitative measures,2020,https://doi.org/10.5281/zenodo.4245390,"Shih-Lun Wu, National Taiwan University;Yi-Hsuan Yang, Taiwan AI Labs, Academia Sinica","This paper presents the Jazz Transformer, a generative model that utilizes a neural sequence model called the Transformer-XL for modeling lead sheets of Jazz music. Moreover, the model endeavors to incorporate structural events present in the Weimar Jazz Database (WJazzD) for inducing structures in the generated music. While we are able to reduce the training loss to a low value, our listening test suggests however a clear gap between the average ratings of the generated and real compositions. We therefore go one step further and conduct a series of computational analysis of the generated compositions from different perspectives. This includes analyzing the statistics of the pitch class, grooving, and chord progression, assessing the structureness of the music with the help of the fitness scape plot, and evaluating the model's understanding of Jazz music through a MIREX-like continuation prediction task. Our work presents in an analytical manner why machine-generated music to date still falls short of the artwork of humanity, and sets some goals for future work on automatic composition to further pursue."
Taegyun Kwon;Dasaem Jeong;Juhan Nam,Polyphonic piano transcription using autoregressive multi-state note model,2020,https://doi.org/10.5281/zenodo.4245466,"Taegyun Kwon, Graduate School of Culture Technology, KAIST, South Korea;Dasaem Jeong, Graduate School of Culture Technology, KAIST, South Korea;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea","Recent advances in polyphonic piano transcription have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multiple loss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order is learned by an auto-regressive connection within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters."
Christopher J Tralie;Elizabeth Dempsey,"Exact, parallelizable dynamic time warping alignment with linear memory",2020,https://doi.org/10.5281/zenodo.4245470,"Christopher J. Tralie, Ursinus College;Elizabeth Dempsey, Ursinus College","Audio alignment is a fundamental preprocessing step in many MIR pipelines. For two audio clips with M and N frames, respectively, the most popular approach, dynamic time warping (DTW), has O(MN) requirements in both memory and computation, which is prohibitive for frame-level alignments at reasonable rates. To address this, a variety of memory efficient algorithms exist to approximate the optimal alignment under the DTW cost. To our knowledge, however, no exact algorithms exist that are guaranteed to break the quadratic memory barrier.  In this work, we present a divide and conquer algorithm that computes the exact globally optimal DTW alignment using O(M+N) memory. Its runtime is still O(MN), trading off memory for a 2x increase in computation.  However, the algorithm can be parallelized up to a factor of min(M, N) with the same memory constraints, so it can still run more efficiently than the textbook version with an adequate GPU. We use our algorithm to compute exact alignments on a collection of orchestral music, which we use as ground truth to benchmark the alignment accuracy of several popular approximate alignment schemes at scales that were not previously possible."
Yu-Hua Chen;Yu-Siang Huang;Wen-Yi Hsiao;Yi-Hsuan Yang,Automatic composition of guitar tabs by transformers and groove modeling,2020,https://doi.org/10.5281/zenodo.4245542,"Yu-Hua Chen, Taiwan AI Labs, Taiwan, Academia Sinica, Taiwan, National Taiwan University, Taiwan;Yu-Hsiang Huang, Taiwan AI Labs, Taiwan;Wen-Yi Hsiao, Taiwan AI Labs, Taiwan;Yi-Hsuan Yang, Taiwan AI Labs, Taiwan, Academia Sinica, Taiwan","Recent years have witnessed great progress in using deep learning algorithms to learn to compose music in the form of a MIDI file.  However, whether such algorithms apply equally well to compose guitar tabs, which are quite different from MIDIs, remain relatively unexplored. To address this, we build a model for composing fingerstyle guitar tabs with a neural sequence model architecture called the Transformer-XL. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful fingering (i.e., string-fret combinations), which is important for tabs but not for MIDIs. Second, whether it generates compositions with coherent rhythmic grooving, which is crucial for fingerstyle guitar music. And, finally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence for the promise of deep learning for guitar tab composition, and suggests areas for future study."
Taejun Kim;Minsuk Choi;Evan Sacks;Yi-Hsuan Yang;Juhan Nam,A computational analysis of real-world DJ mixes using mix-to-track subsequence alignment,2020,https://doi.org/10.5281/zenodo.4245544,"Taejun Kim, Graduate School of Culture Technology, KAIST, South Korea;Minsuk Choi, Graduate School of Culture Technology, KAIST, South Korea;Evan Sacks, 1001Tracklists, United States;Yi-Hsuan Yang, Research Center for IT Innovation, Academia Sincia, Taiwan;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea","A DJ mix is a sequence of music tracks concatenated seamlessly, typically rendered for audiences in a live setting by a DJ on stage. As a DJ mix is produced in a studio or the live version is recorded for music streaming services, computational methods to analyze DJ mixes, for example, extracting track information or understanding DJ techniques, have drawn research interests. Many of previous works are, however, limited to identifying individual tracks in a mix or segmenting it, and the sizes of the datasets are usually small. In this paper, we provide an in-depth analysis of DJ music by aligning a mix to its original music tracks. We set up the subsequence alignment such that the audio features are less sensitive to the tempo or key change of the original track in a mix. This approach provides temporally tight mix-to-track matching from which we can obtain cue-points, transition length, mix segmentation, and musical changes in DJ performance. Using 1,557 mixes from 1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the proposed analysis and show a wide range of statistics, which may elucidate the creative process of DJ music making."
Sanga Chaki;Pranjal Doshi;Sourangshu Bhattacharya;Prof. Priyadarshi Patnaik,Explaining perceived emotion predictions in music: An attentive approach,2020,https://doi.org/10.5281/zenodo.4245388,"Sanga Chaki, Advanced Technology Development Centre, IIT Kharagpur, India;Pranjal Doshi, Department of Computer Science and Engineering, IIT Kharagpur, India;Sourangshu Bhattacharya, Department of Computer Science and Engineering, IIT Kharagpur, India;Priyadarshi Patnaik, Department of Humanities and Social Sciences, IIT Kharagpur, India","Dynamic prediction of perceived emotions of music is a challenging problem with interesting applications. Utilization of relevant context in audio sequence is essential for effective prediction. Existing methods have used LSTMs with modest success. In this work we describe three attentive LSTM based approaches for dynamic emotion prediction from music clips. We validate our models through extensive experimentation on standard dataset annotated with arousal-valence values in continuous time, and choose the best performer. We find that the LSTM based attention models perform better than the state of the art transformers for the dynamic emotion prediction task, both in terms of R2 and Kendall-Tau metrics. We explore individual smaller feature sets in search of a more effective one and to understand how different features contribute to perceived emotion. The spectral features are found to perform at par with the generic ComPare feature set [1]. Through attention map analysis we visualize how attention is distributed over music clips' frames for emotion prediction. It is observed that the models attend to frames which contribute to changes in reported arousal-valence values and chroma to produce better emotion predictions, effectively capturing long-term dependencies."
Olivier Lartillot;Fred Bruford,Bistate reduction and comparison of drum patterns,2020,https://doi.org/10.5281/zenodo.4245432,"Olivier Lartillot, RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, University of Oslo;Fred Bruford, Centre for Digital Music, Queen Mary University, United Kingdom","This paper develops the hypothesis that symbolic drum patterns can be represented in a reduced form as a simple oscillation between two states, a Low state (commonly associated with kick drum events) and a High state (often associated with either snare drum or high hat). Both an onset time and an accent time is associated to each state. The systematic inference of the reduced form is formalized. This enables the specification of a rhythmic structural similarity measure on drum patterns, where reduced patterns are compared through alignment. The two-state representation allows a low computational cost alignment, once the complex topological formalization is fully taken into account. A comparison with the Hamming distance, as well as similarity ratings collected from listeners on a drum loop dataset, indicates that the bistate reduction enables to convey subtle aspects that goes beyond surface-level comparison of rhythmic textures."
Meijun Liu;Eva Zangerle;Xiao Hu;Alessandro Melchiorre;Markus Schedl,"Pandemics, music, and collective sentiment: Evidence from the outbreak of COVID-19",2020,https://doi.org/10.5281/zenodo.4245394,"Meijun Liu, The University of Hong Kong, Hong Kong SAR, China;Eva Zangerle, University of Innsbruck, Innsbruck, Austria;Xiao Hu, The University of Hong Kong, Hong Kong SAR, China;Alessandro Melchiorre, Johannes Kepler University Linz, Austria;Markus Schedl, Johannes Kepler University Linz, Austria","The COVID-19 pandemic causes a massive global health crisis and produces substantial economic and social distress, which in turn may cause stress and anxiety among people. Real-world events play a key role in shaping collective sentiment in a society. As people listen to music daily everywhere in the world, the sentiment of music being listened to can reflect the mood of the listeners and serve as a measure of collective sentiment. However, the exact relationship between real-world events and the sentiment of music being listened to is not clear. Driven by this research gap, we use the unexpected outbreak of COVID-19 as a natural experiment to explore how users' sentiment of music being listened to evolves before and during the outbreak of the pandemic. We employ causal inference approaches on an extended version of the LFM-1b dataset of listening events shared on Last.fm, to examine the impact of the pandemic on the sentiment of music listened to by users in different countries. We find that, after the first COVID-19 case in a country was confirmed, the sentiment of artists users listened to becomes more negative. This negative effect is pronounced for males while females' music emotion is less influenced by the outbreak of the COVID-19 pandemic. We further find a negative association between the number of new weekly COVID-19 cases and users' music sentiment. Our results provide empirical evidence that public sentiment can be monitored based on collective music listening behaviors, which can contribute to research in related disciplines."
Martin Rohrmeier,Towards a formalization of musical rhythm,2020,https://doi.org/10.5281/zenodo.4245508,"Martin Rohrmeier, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne","Temporality lies at the very heart of music, and the play with rhythmic and metrical structures constitutes a major device across musical styles and genres. Rhythmic and metrical structure are closely intertwined, particularly in the tonal idiom. While there have been many approaches for modeling musical tempo, beat and meter and their inference, musical rhythm and its complexity have been comparably less explored and formally modeled. The model formulates a generative grammar of symbolic rhythmic musical structure and its internal recursive substructure. The approach characterizes rhythmic groups in alignment with meter in terms of the recursive subdivision of temporal units, as well as dependencies established by recursive operations such as preparation and different kinds of shifting (such as anticipation and delay). The model is formulated in terms of an abstract context-free grammar and applies for monophonic rhythms and harmonic rhythm."
Yixiao Zhang;Junyan Jiang;Gus Xia;Simon Dixon,Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model,2022,https://doi.org/10.5281/zenodo.7316584,"Yixiao Zhang, Centre for Digital Music, Queen Mary University of London;Junyan Jiang, Music X Lab, NYU Shanghai;Gus Xia, Music X Lab, NYU Shanghai;Simon Dixon, Centre for Digital Music, Queen Mary University of London","Lyric interpretations can help people understand songs and their lyrics quickly, and can also make it easier to manage, retrieve and discover songs efficiently from the growing mass of music archives. In this paper we propose BART-fusion, a novel model for generating lyrics interpretations from lyrics and music audio that combines a large-scale pre-trained language model with an audio encoder. We employ a cross-modal attention module to incorporate the audio representation into the lyrics representation to help the pre-trained language model understand the song from an audio perspective, while preserving the language model's original generative performance. We also release the Song Interpretation Dataset, a new large-scale dataset for training and evaluating our model. Experimental results show that the additional audio information helps our model to understand words and music better, and to generate precise and fluent interpretations. An additional experiment on cross-modal music retrieval shows that interpretations generated by BART-fusion can also help people retrieve music more accurately than with the original BART."
Tsung-Ping Chen;Li Su,Toward postprocessing-free neural networks for joint beat and downbeat estimation,2022,https://doi.org/10.5281/zenodo.7342730,"Tsung-Ping Chen, Institute of Information Science, Academia Sinica, Taiwan;Li Su, Institute of Information Science, Academia Sinica, Taiwan","Recent deep learning-based models for estimating beats and downbeats are mainly composed of three successive stages---feature extraction, sequence modeling, and post processing. While such a framework is prevalent in the scenario of sequence labeling tasks and yields promising results in beat and downbeat estimations, it also indicates a shortage of the employed neural networks, given that the post-processing usually provides a notable performance gain over the previous stage. Moreover, the assumption often made for the post-processing is not suitable for many musical pieces. In this work, we attempt to improve the performance of joint beat and downbeat estimation without incorporating the post-processing stage. By inspecting a state-of-the-art approach, we propose reformulations regarding the network architecture and the loss function. We evaluate our model on various music data and show that the proposed methods are capable of improving the baseline approach without the aid of a post-processing stage."
Matan Gover;Oded Zewi,Music Translation: Generating Piano Arrangements in Different Playing Levels,2022,https://doi.org/10.5281/zenodo.7316588,"Matan Gover, Simply;Oded Zewi, Simply","We present a novel task of playing level conversion: generating a music arrangement in a target difficulty level, given another arrangement of the same musical piece in a different level. For this task, we create a parallel dataset of piano arrangements in two strictly well-defined playing levels, annotated at individual phrase resolution, taken from the song catalog of a piano learning app.In a series of experiments, we train models that successfully modify the playing level while preserving the musical 'essence'. We further show, via an ablation study, the contributions of specific data representation and augmentation techniques to the model's performance.In order to evaluate the performance of our models, we conduct a human evaluation study with expert musicians. The evaluation shows that our best model creates arrangements that are almost as good as ground truth examples. Additionally, we propose MuTE, an automated evaluation metric for music translation tasks, and show that it correlates with human ratings."
Ian Simon;Joshua Gardner;Curtis Hawthorne;Ethan Manilow;Jesse Engel,Scaling Polyphonic Transcription with Mixtures of Monophonic Transcriptions,2022,https://doi.org/10.5281/zenodo.7316590,"Ian Simon, Google Research, Brain Team;Josh Gardner, ;Curtis Hawthorne, ;Ethan Manilow, ;Jesse Engel, Google Research, Brain Team","Automatic Music Transcription (AMT), in particular the problem of automatically extracting notes from audio, has seen much recent progress via the training of neural network models on musical audio recordings paired with aligned ground-truth note labels.  However, progress is currently limited by the difficulty of obtaining such note labels for natural audio recordings at scale.  In this paper, we take advantage of the fact that for monophonic music, the transcription problem is much easier and largely solved via modern pitch-tracking methods.  Specifically, we show that we are able to combine recordings of real monophonic music (and their transcriptions) into artificial and musically-incoherent mixtures, greatly increasing the scale of labeled training data.  By pretraining on these mixtures, we can use a larger neural network model and significantly improve upon the state of the art in multi-instrument polyphonic transcription.  We demonstrate this improvement across a variety of datasets and in a ``zero-shot'' setting where the model has not been trained on any data from the evaluation domain."
Anup Singh;Kris Demuynck;Vipul Arora,Attention-based audio embeddings for query-by-example,2022,https://doi.org/10.5281/zenodo.7316592,"Anup Singh, IDLab, Department of Electronics and Information Systems, imec - Ghent University, Belgium;Kris Demuynck, IDLab, Department of Electronics and Information Systems, imec - Ghent University, Belgium;Vipul Arora, Department of Electrical Engineering, Indian Institute of Technology Kanpur, India","An ideal audio retrieval system efficiently and robustly recognizes a short query snippet from an extensive database. However, the performance of well-known audio fingerprinting systems falls short at high signal distortion levels. This paper presents an audio retrieval system that generates noise and reverberation robust audio fingerprints using the contrastive learning framework. Using these fingerprints, the method performs a comprehensive search to identify the query audio and precisely estimate its timestamp in the reference audio. Our framework involves training a CNN to maximize the similarity between pairs of embeddings extracted from clean audio and its corresponding distorted and time-shifted version. We employ a channel-wise spectral-temporal attention mechanism to capture salient time indices and spectral bands in the CNN features. The attention mechanism enables the CNN to better discriminate the audio by giving more weight to the salient spectral-temporal patches in the signal. Experimental results indicate that our system is efficient in computation and memory usage while being more accurate, particularly at higher distortion levels, than competing state-of-the-art systems and scalable to a larger database."
Otso Björklund,SIATEC-C: Computationally efficient repeated pattern discovery in polyphonic music,2022,https://doi.org/10.5281/zenodo.7316594,"Otso Björklund, University of Helsinki","The use of point-set representations of music enable repeated pattern discoveryto be performed on polyphonic music. The discovery of patterns containing polyphony is also enabled by the use of point-set representations. The SIA and SIATEC algorithms discover repeated patterns in point-sets bycomputing maximal translatable patterns and their translational equivalence classes.While the algorithms are relatively efficient, their application to larger piecesof music is not viable due to quadratic space complexity.This paper introcudes a novel algorithm, SIATEC-C, for repeated pattern discovery in point-set representations of music. The algorithm discovers repeated patterns and finds all of their occurrences, whilerunning with subquadratic space complexity. The algorithm can also provide significant runningtime improvements over the comparable SIATEC algorithm.The computational performance of the algorithm is compared with SIATEC. The accuracy of the algorithmis also evaluated on the JKU-PDD data set."
Marcel A Vélez Vásquez;John Ashley Burgoyne,Tailed U-Net: Multi-Scale Music Representation Learning,2022,https://doi.org/10.5281/zenodo.7316596,"Marcel A. Vélez Vásquez, Music Cognition Group · Institute for Logic, Language, and Computation · University of Amsterdam;John Ashley Burgoyne, Music Cognition Group · Institute for Logic, Language, and Computation · University of Amsterdam","Self-supervised learning has steadily been gaining traction in recent years. In music information retrieval (MIR), one promising recent application of self-supervised learning is the CLMR framework (contrastive learning of musical representations). CLMR has shown good performance, achieving results on par with state-of-the-art end-to-end classification models, but it is strictly an encoding framework. It suffers the characteristic limitation of any encoder that it cannot explicitly combine multi-timescale information, whereas a characteristic feature of human audio perception is that we tend to perceive all frequencies simultaneously. To this end, we propose a generalization of CLMR that learns to extract and explicitly combine representations across different frequency resolutions, which we coin the tailed U-Net (TUNe). TUNe architectures combine multi-timescale information during a decoding phase, similar to U-Net architectures used in computer vision and source separation, but have a tail added to reduce sample-level information to a smaller pre-defined number of representation dimensions. The size of the decoding phase is a hyperparameter, and in the case of a zero-layer decoding phase, TUNe reduces to CLMR. The best TUNe architectures, however, require less training time to match CLMR performance, have superior transfer learning performance, and are competitive with state-of-the-art models even at dramatically reduced dimensionalities."
Da-Yi Wu;Wen-Yi Hsiao;Fu-Rong Yang;Oscar D Friedman;Warren Jackson;Scott Bruzenak;Yi-Wen Liu;Yi-Hsuan Yang,DDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation,2022,https://doi.org/10.5281/zenodo.7316600,"Da-Yi Wu, Academia Sinica;Wen-Yi Hsiao, Taiwan AI Labs;Fu-Rong Yang, National Tsing Hua Univ.;Oscar Friedman, 470 Music Group;Warren Jackson, PARC;Scott Bruzenak, 470 Music Group;Yi-Wen Liu, National Tsing Hua Univ.;Yi-Hsuan Yang, Academia Sinica, Taiwan AI Labs","A vocoder is a conditional audio generation model that converts acoustic features such as mel-spectrograms into waveforms. Taking inspiration from Differentiable Digital Signal Processing (DDSP), we propose a new vocoder named SawSing for singing voices. SawSing synthesizes the harmonic part of singing voices by filtering a sawtooth source signal with a linear time-variant finite impulse response filter whose coefficients are estimated from the input mel-spectrogram by a neural network. As this approach enforces phase continuity, SawSing can generate singing voices without the phase-discontinuity glitch of many existing vocoders. Moreover, the source-filter assumption provides an inductive bias that allows SawSing to be trained on a small amount of data. Our evaluation shows that SawSing converges much faster and outperforms state-of-the-art generative adversarial network- and diffusion-based vocoders in a resource-limited scenario with only 3 training recordings and a 3-hour training time."
Elio Quinton,Equivariant self-supervision for musical tempo estimation,2022,https://doi.org/10.5281/zenodo.7316602,"Elio Quinton, Universal Music Group","Self-supervised methods have emerged as a promising avenue for representation learning in the recent years since they alleviate the need for labeled datasets, which are scarce and expensive to acquire. Contrastive methods are a popular choice for self-supervision in the audio domain, and typically provide a learning signal by forcing the model to be invariant to some transformations of the input. These methods, however, require measures such as negative sampling or some form of regularisation to be taken to prevent the model from collapsing on trivial solutions. In this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data. We derive a simple loss function that prevents the network from collapsing on a trivial solution during training, without requiring any form of regularisation or negative sampling.Our experiments show that it is possible to learn meaningful representations for tempo estimation by solely relying on equivariant self-supervision, achieving performance comparable with supervised methods on several benchmarks. As an added benefit, our method only requires moderate compute resources and therefore remains accessible to a wide research community."
Yuqiang Li;Shengchen Li;George Fazekas,How Music features and Musical Data Representations Affect Objective Evaluation of Music Composition: A Review of CSMT Data Challenge 2020,2022,https://doi.org/10.5281/zenodo.7316604,"Yuqiang Li, Xi’an Jiaotong-Liverpool University;Shengchen Li, Xi’an Jiaotong-Liverpool University;George Fazekas, Queen Mary University of London","Tools and methodologies for distinguishing computer-generated melodies from human-composed melodies have a broad range of applications from detecting copyright infringement through the evaluation of generative music systems to facilitating transparent and explainable AI. This paper reviews a data challenge on distinguishing computer-generated melodies from human-composed melodies held in association with the Conference on Sound and Music Technology (CSMT) in 2020. An investigation of the submitted systems and the results are presented first. Besides the structure of the proposed models, the paper investigates two important factors that were identified as contributors to good model performance: the specific music features and the music representation used. Through an analysis of the submissions, important melody-related music features have been identified. Encoding or representation of the music in the context of neural network modes has also been found to significantly impact system performance through an experiment where the top-ranked system was re-implemented with different input representations for comparison purposes. Besides demonstrating the feasibility of developing an objective music composition evaluation system, the investigation presented in this paper also reveals some important limitations of current music composition systems opening opportunities for future work in the community."
Eunjin Choi;Yoonjin Chung;Seolhee Lee;Jongik Jeon;Taegyun Kwon;Juhan Nam,YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations,2022,https://doi.org/10.5281/zenodo.7316606,"Eunjin Choi, Graduate School of Culture Technology, KAIST, South Korea;Yoonjin Chung, Graduate School of AI, KAIST, South Korea;Seolhee Lee, Graduate School of Culture Technology, KAIST, South Korea;JongIk Jeon, Department of Industrial Design, KAIST, South Korea;Taegyun Kwon, Graduate School of Culture Technology, KAIST, South Korea;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea","Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB."
Anil Venkatesh;Viren Sachdev,Detecting Symmetries of All Cardinalities With Application to Musical 12-Tone Rows,2022,https://doi.org/10.5281/zenodo.7316608,"Anil Venkatesh, Department of Mathematics and Computer Science, Adelphi University;Viren Sachdev, Department of Mathematics and Computer Science, Adelphi University","Popularized by Arnold Schoenberg in the mid-20th century, the method of twelve-tone composition produces musical compositions based on one or more orderings of the equal-tempered chromatic scale. The work of twelve-tone composers is famously challenging to traditional Western tonal and structural sensibilities; even so, group theoretic approaches have determined that 10% of certain composers' works contain a highly unusual classical symmetry of music. We extend this result by revealing many symmetries that were previously undetected in the works of Schoenberg, Webern, and Berg. Our approach is computational rather than group theoretic, scanning each composition for symmetries of many different cardinalities. Thus, we capture partial symmetries that would be overlooked by more formal means. Moreover, our methods are applicable beyond the narrow scope of twelve-tone composition. We achieve our results by first extending the group-theoretic notion of symmetry to encompass shorter motives that may be repeated and reprised in a given composition, and then comparing the incidence of these symmetries between the work of composers and the space of all possible 12-tone rows. We present four candidate hierarchies of symmetry and show that in each model, between 75% and 95% of actual compositions contained high levels of internal symmetry."
Jaehun Kim;Cynthia C. S. Liem,The power of deep without going deep? A study of HDPGMM music representation learning,2022,https://doi.org/10.5281/zenodo.7316610,"Jaehun Kim, Delft University of Technology;Cynthia C. S. Liem, Delft University of Technology","In the previous decade, Deep Learning (DL) has proven to be one of the most effective machine learning methods to tackle a wide range of Music Information Retrieval (MIR) tasks. It offers highly expressive learning capacity that can fit any music representation needed for MIR-relevant downstream tasks. However, it has been criticized for sacrificing interpretability. On the other hand, the Bayesian nonparametric (BN) approach promises similar positive properties as DL, such as high flexibility, while being robust to overfitting and preserving interpretability. Therefore, the primary motivation of this work is to explore the potential of Bayesian nonparametric models in comparison to DL models for music representation learning. More specifically, we assess the music representation learned from the Hierarchical Dirichlet Process Gaussian Mixture Model (HDPGMM), an infinite mixture model based on the Bayesian nonparametric approach, to MIR tasks, including classification, auto-tagging, and recommendation. The experimental result suggests that the HDPGMM music representation can outperform DL representations in certain scenarios, and overall comparable."
Daiki Naruse;Tomoyuki Takahata;Yusuke Mukuta;Tatsuya Harada,Pop Music Generation with Controllable Phrase Lengths,2022,https://doi.org/10.5281/zenodo.7316612,"Daiki Naruse, The University of Tokyo, Japan;Tomoyuki Takahata, The University of Tokyo, Japan;Yusuke Mukuta, The University of Tokyo, Japan, RIKEN, Japan;Tatsuya Harada, The University of Tokyo, Japan, RIKEN, Japan","Research on music generation using deep learning has attracted more attention; in particular, Transformer-based models have succeeded in generating coherent musical pieces. Recently, an increasing number of studies have focused on phrases that are smaller musical units, and several studies have addressed phrase-level control. In this study, we propose a method for sequentially generating a piece that enables the control of each phrase length and, consequently, the length of the entire piece. We added PHRASE and a new event, BAR COUNTDOWN, which indicates the number of bars remaining in the phrase, to the existing event-based music representations. To reflect user input indicating the phrase lengths of the piece being generated, we used an autoregressive generation model that adds these two events to the generated event-token sequence based on the user input and uses it as input for the next time step. Subjective listening tests revealed that the pieces generated by our methods possessed designated phrase lengths and ended naturally at the determined length."
Yen-Tung Yeh;Yi-Hsuan Yang;Bo-Yu Chen,Exploiting Pre-trained Feature Networks for Generative Adversarial Networks in Audio-domain Loop Generation,2022,https://doi.org/10.5281/zenodo.7316614,"Yen-Tung Yeh, Academia Sinica, National Taiwan University;Bo-Yu Chen, Academia Sinica, National Taiwan University;Yi-Hsuan Yang, Academia Sinica, Taiwan AI Labs","While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation."
Daiyu Zhang;Ju-Chiang Wang;Katerina Kosta;Jordan B. L. Smith;Shicen Zhou,Modeling the rhythm from lyrics for melody generation of pop songs,2022,https://doi.org/10.5281/zenodo.7316616,"Daiyu Zhang, ByteDance;Ju-Chiang Wang, ByteDance;Katerina Kosta, ByteDance;Jordan B. L. Smith, ByteDance;Shicen Zhou, ByteDance","Creating a pop song melody according to pre-written lyrics is a typical practice for composers. A computational model of how lyrics are set as melodies is important for automatic composition systems, but an end-to-end lyric-to-melody model would require enormous amounts of paired training data. To mitigate the data constraints, we adopt a two-stage approach, dividing the task into lyric-to-rhythm and rhythm-to-melody modules. However, the lyric-to-rhythm task is still challenging due to its multimodality. In this paper, we propose a novel lyric-to-rhythm framework that includes part-of-speech tags to achieve better text-setting, and a Transformer architecture designed to model long-term syllable-to-note associations. For the rhythm-to-melody task, we adapt a proven chord-conditioned melody Transformer, which has achieved state-of-the-art results. Experiments for Chinese lyric-to-melody generation show that the proposed framework is able to model key characteristics of rhythm and pitch distributions in the dataset, and in a subjective evaluation, the melodies generated by our system were rated as similar to or better than those of a state-of-the-art alternative."
Simeon Rau;Frank Heyen;Stefan Wagner;Michael Sedlmair,Visualization for AI-Assisted Composing,2022,https://doi.org/10.5281/zenodo.7316618,"Simeon Rau, VISUS, University of Stuttgart, Germany;Frank Heyen, VISUS, University of Stuttgart, Germany;Stefan Wagner, ISTE, University of Stuttgart, Germany;Michael Sedlmair, VISUS, University of Stuttgart, Germany","We propose a visual approach for interactive, AI-assisted composition that serves as a compromise between fully automatic and fully manual composition. Instead of generating a whole piece, the AI takes on the role of an assistant that generates short melodies for the composer to choose from and adapt. In an iterative process, the composer queries the AI for continuations or alternative fill-ins, chooses a suggestion, and adds it to the piece. As listening to many suggestions would take time, we explore different ways to visualize them, to allow the composer to focus on the most interesting-looking melodies. We also present the results of a qualitative evaluation with five composers."
Ellie Bean Abrams;Eva Muñoz Vidal;Claire Pelofi;Pablo Ripollés,Retrieving musical information from neural data: how cognitive features enrich acoustic ones,2022,https://doi.org/10.5281/zenodo.7343078,"Ellie Bean Abrams, Music and Audio Research Laboratory, New York University;Eva Muñoz Vidal, Music and Audio Research Laboratory, New York University;Claire Pelofi, Music and Audio Research Laboratory, New York University;Pablo Ripollés, Music and Audio Research Laboratory, New York University","Various features – from low-level acoustics, to higher-level statistical regularities, to memory associations – contribute to the experience of musical enjoyment and pleasure. Recent work suggests that musical surprisal, that is, the unexpectedness of a musical event given its context, may directly predict listeners' experiences of pleasure and enjoyment during music listening. Understanding how surprisal shapes listeners' preferences for certain musical pieces has implications for music recommender systems, which are typically content- (both acoustic or semantic) or metadata-based. Here we test a recently developed computational algorithm, called Dynamic-Regularity Extraction (D-REX), that uses Bayesian inference to predict the surprisal that humans experience while listening to music. We demonstrate that the brain tracks musical surprisal as modeled by D-REX by conducting a decoding analysis on the neural signal (collected through magnetoencephalography) of participants listening to music. Thus, we demonstrate the validity of a computational model of musical surprisal, which may remarkably inform the next generation of recommender systems. In addition, we present an open-source neural dataset which will be available for future research to foster approaches combining MIR with cognitive neuroscience, an approach we believe will be a key strategy in characterizing people's reactions to music."
Jingwei Zhao;Gus Xia;Ye Wang,Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention,2022,https://doi.org/10.5281/zenodo.7316622,"Jingwei Zhao2,4, NUS;Gus Xia3,5, NYU Shanghai;Ye Wang1,2,4, NUS","We propose Beat Transformer, a novel Transformer encoder architecture for joint beat and downbeat tracking. Different from previous models that track beats solely based on the spectrogram of an audio mixture, our model deals with demixed spectrograms with multiple instrument channels. This is inspired by the fact that humans perceive metrical structures from richer musical contexts, such as chord progression and instrumentation. To this end, we develop a Transformer model with both time-wise attention and instrument-wise attention to capture deep-buried metrical cues. Moreover, our model adopts a novel dilated self-attention mechanism, which achieves powerful hierarchical modelling with only linear complexity. Experiments demonstrate a significant improvement in demixed beat tracking over the non-demixed version. Also, Beat Transformer achieves up to 4% point improvement in downbeat tracking accuracy over the TCN architectures. We further discover an interpretable attention pattern that mirrors our understanding of hierarchical metrical structures."
Seungyeon Rhyu;Sarah Kim;Kyogu Lee,Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning,2022,https://doi.org/10.5281/zenodo.7342916,"Seungyeon Rhyu, Music and Audio Research Group (MARG), Seoul National University, South Korea;Sarah Kim, Krust Universe, South Korea;Kyogu Lee, Music and Audio Research Group (MARG), Seoul National University, South Korea","We propose a system for rendering a symbolic piano performance with flexible musical expression. It is necessary to actively control musical expression for creating a new music performance that conveys various emotions or nuances. However, previous approaches were limited to following the composer's guidelines of musical expression or dealing with only a part of the musical attributes. We aim to disentangle the entire musical expression and structural attribute of piano performance using a conditional VAE framework. It stochastically generates expressive parameters from latent representations and given note structures. In addition, we employ self-supervised approaches that force the latent variables to represent target attributes. Finally, we leverage a two-step encoder and decoder that learn hierarchical dependency to enhance the naturalness of the output. Experimental results show that our system can stably generate performance parameters relevant to the given musical scores, learn disentangled representations, and control musical attributes independently of each other."
Karim M. Ibrahim;Elena V. Epure;Geoffroy Peeters;Gaël Richard,Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts,2022,https://doi.org/10.5281/zenodo.7342816,"Karim M. Ibrahim, LTCI, Télécom Paris, Institut Polytechnique de Paris;Elena V. Epure, Deezer Research;Geoffroy Peeters, LTCI, Télécom Paris, Institut Polytechnique de Paris;Gaël Richard, LTCI, Télécom Paris, Institut Polytechnique de Paris","As music has become more available especially on music streaming platforms, people have started to have distinct preferences to fit to their varying listening situations, also known as context. Hence, there has been a growing interest in considering the user's situation when recommending music to users. Previous works have proposed personalized auto-taggers to infer situation-related tags from music content and user's global listening preferences. However, in a practical music retrieval system, these context-aware auto-tagger could be only used by assuming that the context class is explicitly provided by the user. In this work, for designing a fully automatised music retrieval system, we propose to disambiguate the user's listening information from stream data. Namely, we propose a system which can generate a situational playlist for a user at a certain time first by leveraging personalized music auto-taggers, and second by automatically inferring the user's situation from stream data (e.g. device, network) and user's general profile information (e.g. age). Experiments show that such a personalized context-aware music retrieval system is feasible, but the performance suffers in the case of new users, new tracks or when the number of context classes increases."
Yueh-Kao Wu;Ching-Yu Chiu;Yi-Hsuan Yang,Jukedrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE,2022,https://doi.org/10.5281/zenodo.7316628,"Yueh-Kao Wu, Academia Sinica;Ching-Yu Chiu, National Cheng Kung University;Yi-Hsuan Yang, Taiwan AI Labs","This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio- domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio."
Junyan Jiang;Daniel Chin;Yixiao Zhang;Gus Xia,Learning Hierarchical Metrical Structure Beyond Measures,2022,https://doi.org/10.5281/zenodo.7316630,"Junyan Jiang, Music X Lab, NYU Shanghai;Daniel Chin, Music X Lab, NYU Shanghai;Yixiao Zhang, Centre for Digital Music, QMUL;Gus Xia, Music X Lab, NYU Shanghai","Music contains hierarchical structures beyond beats and measures. While hierarchical structure annotations are helpful for music information retrieval and computer musicology, such annotations are scarce in current digital music databases. In this paper, we explore a data-driven approach to automatically extract hierarchical metrical structures from scores. We propose a new model with a Temporal Convolutional Network-Conditional Random Field (TCN-CRF) architecture. Given a symbolic music score, our model takes in an arbitrary number of voices in a beat-quantized form, and predicts a 4-level hierarchical metrical structure from downbeat-level to section-level. We also annotate a dataset using RWC-POP MIDI files to facilitate training and evaluation. We show by experiments that the proposed method performs better than the rule-based approach under different orchestration settings. We also perform some simple musicological analysis on the model predictions. All demos, datasets and pre-trained models are publicly available on Github."
Francisco C. F. Almeida;Gilberto Bernardes;Christof Weiss,Mid-level Harmonic Audio Features for Musical Style Classification,2022,https://doi.org/10.5281/zenodo.7316632,"Francisco Almeida, Univ. Porto, Faculty of Engineering & INESC TEC;Gilberto Bernardes, Univ. Porto, Faculty of Engineering & INESC TEC;Christof Weiû, International Audio Laboratories Erlangen","The extraction of harmonic information from musical audio is fundamental for several music information retrieval tasks. In this paper, we propose novel harmonic audio features based on the perceptually-inspired tonal interval vector space, computed as the Fourier transform of chroma vectors. Our contribution includes mid-level features for musical dissonance, chromaticity, dyadicity, triadicity, diminished quality, diatonicity, and whole-toneness. Moreover, we quantify the perceptual relationship between short- and long-term harmonic structures, tonal dispersion, harmonic changes, and complexity. Beyond the computation on fixed-size windows, we propose a context-sensitive harmonic segmentation approach. We assess the robustness of the new harmonic features in style classification tasks regarding classical music periods and composers. Our results align with, slightly outperforming, existing features and suggest that other musical properties than those in state-of-the-art literature are partially captured. We discuss the features regarding their musical interpretation and compare the different feature groups regarding their effectiveness for discriminating classical music periods and composers."
Johannes Imort;Giorgio Fabbro;Marco A Martinez Ramirez;Stefan Uhlich;Yuichiro Koyama;Yuki Mitsufuji,Distortion Audio Effects: Learning How to Recover the Clean Signal,2022,https://doi.org/10.5281/zenodo.7316634,"Johannes Imort, RWTH Aachen University, Germany;Giorgio Fabbro, Sony Europe B.V., Stuttgart, Germany;Marco A. Martínez-Ramírez, Sony Group Corporation, Tokyo, Japan;Stefan Uhlich, Sony Europe B.V., Stuttgart, Germany;Yuichiro Koyama, Sony Group Corporation, Tokyo, Japan;Yuki Mitsufuji, Sony Group Corporation, Tokyo, Japan","Given the recent advances in music source separation and automatic mixing, removing audio effects in music tracks is a meaningful step toward developing an automated remixing system. This paper focuses on removing distortion audio effects applied to guitar tracks in music production. We explore whether effect removal can be solved by neural networks designed for source separation and audio effect modeling.Our approach proves particularly effective for effects that mix the processed and clean signals. The models achieve better quality and significantly faster inference compared to state-of-the-art solutions based on sparse optimization. We demonstrate that the models are suitable not only for declipping but also for other types of distortion effects. By discussing the results, we stress the usefulness of multiple evaluation metrics to assess different aspects of reconstruction in distortion effect removal."
Antonio Ríos-Vila;Jose M. Inesta;Jorge Calvo-Zaragoza,End-to-End Full-Page Optical Music Recognition for Mensural Notation,2022,https://doi.org/10.5281/zenodo.7342678,"Antonio Ríos-Vila, University of Alicante, Spain;José M. Iñesta, University of Alicante, Spain;Jorge Calvo-Zaragoza, University of Alicante, Spain","Optical Music Recognition (OMR) systems typically consider workflows that include several steps, such as staff detection, symbol recognition, and semantic reconstruction. However, fine-tuning these systems is costly due to the specific data labeling process that has to be performed to train models for each of these steps. In this paper, we present the first segmentation-free full-page OMR system that receives a page image and directly outputs the transcription in a single step. This model requires only the annotations of full score pages, which greatly alleviates the task of manual labeling. The model has been tested with early music written in mensural notation, for which the presented approach is especially beneficial. Results show that this methodology provides a solution with promising results and establishes a new line of research for holistic transcription of music score pages."
Bruno Di Giorgi;Mark Levy;Richard Sharp,Mel Spectrogram Inversion with Stable Pitch,2022,https://doi.org/10.5281/zenodo.7316638,"Bruno Di Giorgi, Apple;Mark Levy, Apple;Richard Sharp, Apple","Vocoders are models capable of transforming a low-dimensional spectral representation of an audio signal, typically the mel spectrogram, to a waveform. Modern speech generation pipelines use a vocoder as their final component. Recent vocoder models developed for speech achieve high degree of realism, such that it is natural to wonder how they would perform on music signals. Compared to speech, the heterogeneity and structure of the musical sound texture offers new challenges. In this work we focus on one specific artifact that some vocoder models designed for speech tend to exhibit when applied to music: the perceived instability of pitch when synthesizing sustained notes. We argue that the characteristic sound of this artifact is due to the lack of horizontal phase coherence, which is often the result of using a time-domain target space with a model that is invariant to time-shifts, such as a convolutional neural network. We propose a new vocoder model that is specifically designed for music. Key to improving the pitch stability is the choice of a shift-invariant target space that consists of the magnitude spectrum and the phase gradient. We discuss the reasons that inspired us to re-formulate the vocoder task, outline a working example, and evaluate it on musical signals. Our method results in 60% and 10% improved reconstruction of sustained notes and chords with respect to existing models, using a novel harmonic error metric."
Xingjian Du;Huidong Liang;Yuan Wan;Yuheng Lin;Ke Chen;Bilei Zhu;Zejun Ma,Latent feature augmentation for chorus detection,2022,https://doi.org/10.5281/zenodo.7316640,"Xingjian Du, ByteDance AI Lab, Shanghai, China;Huidong Liang, ByteDance AI Lab, Shanghai, China;Yuan Wan, ByteDance AI Lab, Shanghai, China;Yuheng Lin, ByteDance AI Lab, Shanghai, China;Ke Chen, University of California San Diego, San Diego, United States;Bilei Zhu, ByteDance AI Lab, Shanghai, China;Zejun Ma, ByteDance AI Lab, Shanghai, China","In this paper, we introduce LA-Chorus, a chorus detection model based on latent feature augmentation and ResNet FPN architecture. Our contributions in LA-Chorus are three-fold. Firstly, we propose a method for implicitly augmenting chorus data in the latent space during the train7 ing stage. Compared to augmentations on audio surfaces such as time stretching and pitch shifting, latent augmentations indicate changes at a higher level in original audio, thereby increasing the diversity and sufficiency in training. Second, we apply Feature Pyramid Network (FPN) to generate additional embeddings from low dimension to high dimension, consequently achieving a multi-scale training paradigm. Lastly, we release Di-Chorus, a new open-source dataset of diverse genres and languages for the community of music structure analysis. In conjunction with other public datasets, we conduct comprehensive ex18 periments to evaluate the performance of LA-Chorus compared to other state-of-the-art models, which demonstrate the out-performance of LA-Chorus and the effectiveness of proposed latent feature augmentation."
Li Yi;Haochen Hu;Jingwei Zhao;Gus Xia,AccoMontage2: A Complete Harmonization and Accompaniment Arrangement System,2022,https://doi.org/10.5281/zenodo.7316642,"Li Yi, Music X Lab, NYU Shanghai;Haochen Hu, Music X Lab, NYU Shanghai;Jingwei Zhao, Institute of Data Science, NUS;Gus Xia, Music X Lab, NYU Shanghai","We propose AccoMontage2, a system capable of doing full-length song harmonization and accompaniment arrangement based on a lead melody. Following AccoMontage, this study focuses on generating piano arrangements for popular/folk songs and it carries on the generalized template-based retrieval method. The novelties of this study are twofold. First, we invent a harmonization module (which AccoMontage does not have). This module generates structured and coherent full-length chord progression by optimizing and balancing three loss terms: a micro-level loss for note-wise dissonance, a meso-level loss for phrase-template matching, and a macro-level loss for full piece coherency. Second, we develop a graphical user interface which allows users to select different styles of chord progression and piano texture. Currently, chord progression styles include Pop, R&amp;B, and Dark, while piano texture styles include several levels of voicing density and rhythmic complexity. Experimental results show that both our harmonization and arrangement results significantly outperform the baselines. Lastly, we release AccoMontage2 as an online application as well as the organized chord progression templates as a public dataset."
Matthew C Mccallum;Filip Korzeniowski;Sergio Oramas;Fabien Gouyon;Andreas Ehmann,Supervised and Unsupervised Learning of Audio Representations for Music Understanding,2022,https://doi.org/10.5281/zenodo.7316644,"Matthew C. McCallum, SiriusXM, USA;Filip Korzeniowski, ;Sergio Oramas, ;Fabien Gouyon, ;Andreas F. Ehmann, ","In this work, we provide a broad comparative analysis of strategies for pre-training audio understanding models for several tasks in the music domain, including labelling of genre, era, origin, mood, instrumentation, key, pitch, vocal characteristics, tempo and sonority. Specifically, we explore how the domain of pre-training datasets (music or generic audio) and the pre-training methodology (supervised or unsupervised) affects the adequacy of the resulting audio embeddings for downstream tasks.We show that models trained via supervised learning on large-scale expert-annotated music datasets achieve state-of-the-art performance in a wide range of music labelling tasks, each with novel content and vocabularies. This can be done in an efficient manner with models containing less than 100 million parameters that require no fine-tuning or reparameterization for downstream tasks, making this approach practical for industry-scale audio catalogs.Within the class of unsupervised learning strategies, we show that the domain of the training dataset can significantly impact the performance of representations learned by the model. We find that restricting the domain of the pre-training dataset to music allows for training with smaller batch sizes while achieving state-of-the-art in unsupervised learning---and in some cases, supervised learning---for music understanding.We also corroborate that, while achieving state-of-the-art performance on many tasks, supervised learning can cause models to specialize to the supervised information provided, somewhat compromising a model's generality."
Rishabh A Dahale;Vaibhav Vinayak Talwadker;Preeti Rao;Prateek Verma,Generating Coherent Drum Accompaniment with Fills and Improvisations,2022,https://doi.org/10.5281/zenodo.7316646,"Rishabh Dahale, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Vaibhav Talwadker, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Preeti Rao, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Prateek Verma, Stanford University","Creating a complex work of art like music necessitates profound creativity. With recent advancements in Deep Learning and powerful models such as Transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite fills and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with fills/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments – Piano, Guitar, Bass and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to find that improvisation is largely absent, attributed possibly to its relatively low representation in the training data. We propose a novelty function that represents the extent of improvisation in a specific bar relative to its neighbors. We train a model to detect improvisation positions from the melodic accompaniment tracks. Finally, we use a novel BERT inspired in-filling architecture, to learn the structure of both the drums and melody to in-fill elements of improvised music."
Alia Morsi;Xavier Serra,Bottlenecks and solutions for audio to score alignment research,2022,https://doi.org/10.5281/zenodo.7343047,"Alia Morsi, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","<p>Although audio to score alignment is a classic Music Information Retrieval problem, it has not been defined uniquely with the scope of musical scenarios representing its core. The absence of a unified vision makes it difficult to pinpoint its state-of-the-art and determine directions for improvement. To get past this bottleneck, it is necessary to consolidate datasets and evaluation methodologies to allow comprehensive benchmarking. In our review of prior work, we demonstrate the extent of variation in problem scope, datasets, and evaluation practices across audio to score alignment research. To circumvent the high cost of creating large-scale datasets with various instruments, styles, performance conditions, and musician proficiency from scratch, the research community could generate ground truth approximations from non-audio to score alignment datasets which include a temporal mapping between a music score and its corresponding audio. We show a methodology for adapting the Aligned Scores and Performances dataset, created originally for beat tracking and music transcription. We filter the dataset semi- automatically by applying a set of Dynamic Time Warping based Audio to Score Alignment methods using out-of-the-box Chroma and Constant-Q Transform extraction algorithms, suitable for the characteristics of the piano performances of the dataset. We use the results to discuss the limitations of the generated ground truths and data adaptation method. While the adapted dataset does not provide the necessary diversity for solving the initial problem, we conclude with ideas for expansion, and identify future directions for curating more comprehensive datasets through data adaptation, or synthesis.</p>"
Martin Clayton;Preeti Rao;Nithya Shikarpur;Sujoy Roychowdhury;Jin Li,Raga Classification From Vocal Performances Using Multimodal Analysis,2022,https://doi.org/10.5281/zenodo.7316650,"Martin Clayton, Department of Music, Durham University, United Kingdom;Preeti Rao, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Nithya Shikarpur, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Sujoy Roychowdhury, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Jin Li, Department of Music, Durham University, United Kingdom","<p>Work on musical gesture and embodied cognition suggests a rich complementarity between audio and movement information in musical performance. Pose estimation algorithms now make it possible (in contrast to Motion Capture) to collect rich movement information from unconstrained performances of indefinite length. Vocal performances of Indian art music offer the opportunity to carry out multimodal analysis using this information, combing musician&#39;s body movements (i.e. pose and gesture data) with audio features. In this work we investigate raga identification from 12 s excerpts from a dataset of 3 singers and 9 ragas using the combination of audio and visual representations that are each semantically salient on their own. While gesture based classification is relatively weak by itself, we show that combining latent representations from the pre-trained unimodal networks can surpass the already high performance obtained by audio features.</p>"
Oleg Lesota;Emilia Parada-Cabaleiro;Stefan Brandl;Elisabeth Lex;Navid Rekabsaz;Markus Schedl,Traces of Globalization in Online Music Consumption Patterns and Results of Recommendation Algorithms,2022,https://doi.org/10.5281/zenodo.7316652,"Oleg Lesota, Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz, Austria;Emilia Parada-Cabaleiro, Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz, Austria;Stefan Brandl, Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz, Austria;Elisabeth Lex, Graz University of Technology, Austria;Navid Rekabsaz, Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz, Austria;Markus Schedl, Multimedia Mining and Search Group, Institute of Computational Perception, JKU Linz, Austria","<p>Music streaming platforms allow users to enjoy music from all over the globe.Such opportunity speeds up cultural exchange between different countries, a process often associated with globalization. While such an exchange could lead to more diverse music consumption, empirical evidence on its influence on online music consumption is limited. Besides, the extent to which music recommender systems foster exchange or amplify globalization in music remains an understudied problem.In this paper, we present findings from an empirical study to detect traces of globalization in domestic vs. foreign online music consumption. Besides, we investigate if popular recommendation algorithms, specifically ItemKNN and NeuMF, are prone to amplifying globalization processes. Our experiments on Last.fm listening data show nuanced patterns of globalization in music consumption. We observe a strong position of US music in all considered countries. In countries such as Sweden, Great Britain, or Brazil, US music shows various levels of coexistence with domestic music. We find that Finland is least influenced by US music, while greatly consuming and &#39;exporting&#39; domestic music. With respect to recommendation algorithms, ItemKNN tends to recommend domestic music to users of many countries, while NeuMF contributes to accelerating globalization and shifting balance towards dominance of US music on the market.</p>"
Kongmeng Liew;Vipul Mishra;Yangyang Zhou;Elena V. Epure;Romain Hennequin;Shoko Wakamiya;Eiji Aramaki,Network Analyses for Cross-Cultural Music Popularity,2022,https://doi.org/10.5281/zenodo.7316654,"Kongmeng Liew, Graduate School of Science and Technology, Nara Institute of Science and Technology, Japan;Vipul Mishra, Graduate School of Science and Technology, Nara Institute of Science and Technology, Japan;Yangyang Zhou, Graduate School of Science and Technology, Nara Institute of Science and Technology, Japan;Elena V. Epure, Deezer Research, Paris, France;Romain Hennequin, Deezer Research, Paris, France;Shoko Wakamiya, Graduate School of Science and Technology, Nara Institute of Science and Technology, Japan;Eiji Aramaki, Graduate School of Science and Technology, Nara Institute of Science and Technology, Japan","Anglo-American popular culture has been said to be intricately connected to global popular culture, both shaping and being shaped by popular trends worldwide, yet few research has examined this issue empirically. Our research quantitatively maps the extent of these cultural influences in popular music consumption, by using network analyses to explore cross-cultural popularity in music from 30 countries corresponding to 6 cultural regions (N = 4863 unique songs over six timepoints from 2019-2021). Using Top100 charts from these countries, we constructed a network based on the co-occurrence of songs in charts, and used eigencentrality as an indicator of cross-cultural song popularity. We then compared the country-of-origin of the artists, arousal music features, and socioeconomic indicators. Songs from artists with Anglo-American backgrounds tended to have higher eigencentrality overall, and mixed effects regressions showed that eigencentrality was negatively associated with danceability, and positively associated with spectral energy, and the migrant population of the country (of the charts). Next, using community detection, we observed 11 separate 'communities' in the network. Most communities appeared to be limited by region/culture, but Anglo-American music seemed disproportionally able to transcend cultural boundaries far beyond their geographical borders. We also discuss implications pertaining to cultural hegemony, and the effectiveness of our method in estimating cross-cultural popularity."
Polykarpos Polykarpidis;Dionysios Kalofonos;Dimitrios Balageorgos;Christina Anagnostopoulou,Three related corpora in Middle Byzantine music notation and a preliminary comparative analysis,2022,https://doi.org/10.5281/zenodo.7316656,"Polykarpos Polykarpidis, Department of Music Studies, National and Kapodistrian University of Athens, Greece;Dionysios Kalofonos, Independent researcher, Manchester, UK;Dimitrios Balageorgos, Department of Music Studies, National and Kapodistrian University of Athens, Greece;Christina Anagnostopoulou, Department of Music Studies, National and Kapodistrian University of Athens, Greece","The Middle Byzantine notation (MBn) is used to capture the plainchant melodies of eastern Orthodox Christian music from the middle of the 12th century until 1814. In the context of this research, we study the evolution of a subgenre of Byzantine music known as Heirmologic. We present three Heirmologic corpora spanning the periods before, during and after the 16th century. We discuss the challenges we faced during the digitisation process, and the steps we took to overcome them. For the analysis of the three corpora, we apply the three methods, namely notational texture, melodic arch similarity, and Jensen-Shannon distances of Markovian models, the second of which is novel and inspired by the idea of melodic arches. Through these methods, we aim at highlighting the differences of the corpora in order to obtain an outline of the evolution of the subgenre. We observe that the post 16th century Heirmologic pieces are more similar to the 16th century ones, while there is a greater difference with the pre 16th century pieces. This indicates that the 16th century constitutes a turning point in the melodic features of the Heirmologic subgenre."
Dichucheng Li;Yulun Wu;Qinyu Li;Jiahao Zhao;Yi Yu;Fan Xia;Wei Li,Playing Technique Detection by Fusing Note Onset Information in Guzheng Performance,2022,https://doi.org/10.5281/zenodo.7316658,"Dichucheng Li, School of Computer Science and Technology, Fudan University, Shanghai, China;Yulun Wu, School of Computer Science and Technology, Fudan University, Shanghai, China;Qinyu Li, College of Experimental Art, Sichuan Conservatory of Music, Sichuan, China;Jiahao Zhao, School of Computer Science and Technology, Fudan University, Shanghai, China;Yi Yu, National Institute of Informatics (NII), Tokyo, Japan;Fan Xia, College of Experimental Art, Sichuan Conservatory of Music, Sichuan, China;Wei Li, School of Computer Science and Technology, Fudan University, Shanghai, China","The Guzheng is a kind of traditional Chinese instruments with diverse playing techniques. Instrument playing techniques (IPT) play an important role in musical performance. However, most of the existing works for IPT detection show low efficiency for variable-length audio and provide no assurance in the generalization as they rely on a single sound bank for training and testing. In this study, we propose an end-to-end Guzheng playing technique detection system using Fully Convolutional Networks that can be applied to variable-length audio. Because each Guzheng playing technique is applied to a note, a dedicated onset detector is trained to divide an audio into several notes and its predictions are fused with frame-wise IPT predictions. During fusion, we add the IPT predictions frame by frame inside each note and get the IPT with the highest probability within each note as the final output of that note. We create a new dataset named GZ_IsoTech from multiple sound banks and real-world recordings for Guzheng performance analysis. Our approach achieves 87.97% in frame-level accuracy and 80.76% in note-level F1-score, outperforming existing works by a large margin, which indicates the effectiveness of our proposed method in IPT detection."
Babak Nikzat;Rafael Caro Repetto,KDC: an open corpus for computational research of dastgāhi music,2022,https://doi.org/10.5281/zenodo.7316660,"Babak Nikzat, Kunstuniversität Graz;Rafael Caro Repetto, Kunstuniversität Graz","Iranian dastgāhi music is considered as the classical repertory of contemporary Iran. In the 19th century, the melodic modes that developed during its long history were grouped in categories, each of them known as dastgāh. The dastgāhi system presents unique features, that have been object of musicological study since its inception. However, computational methods for its research are still scarce, due in good part to the lack of open, well curated corpora. The aim of the KUG Dastgāhi Corpus (KDC) is to contribute to the development of computational corpus driven research for this tradition. KDC is created following the FAIR principles, and in close collaboration with performers and scholars, who contribute to it with annotations and qualitative evaluations. Besides presenting the first version of KDC, in this paper we explore the possibilities that Iranian dastgāhi music offers to computational research. In order to test the performance of state-of-the-art technologies applied to this music tradition, we present preliminary results for several analytical tasks, and discuss thei opportunities and limitations learnt in the process."
Ke Nie,Inaccurate Prediction or Genre Evolution? Rethinking Genre Classification,2022,https://doi.org/10.5281/zenodo.7342846,"Ke Nie, University of California, San Diego","The existing MIR research on genre classification primarily focuses on how to classify a song into the ""correct"" genre while downplaying the fact that genres mutate over time and in response to social change in terms of their musical properties. Songs claiming the same genre can sound very different if they are released years apart, and genres may revive musical traditions from the past. In this paper, I show that the performance of genre classifiers fluctuates as genres evolve. Unsatisfactory performance of the classifiers may not indicate algorithmic flaws but rather the change of genre characteristics. I demonstrate this by studying the case of Chinese Hip-Hop music. Specifically, I collected and analyzed 69,427 songs from four genres (Hip-Hop, Pop, Rock, and Folk) released on a Chinese music platform between 2009 and 2019. Using classifiers trained from the songs in different year cohorts to predict the genre of all the songs, I show how genre classifiers can be used to detect the stylistic shift in Hip-Hop that happened during this period. The paper thus offers a novel, sociological perspective on contending with the much-challenged idea of improving genre classification accuracy for its own sake. However, instead of questioning the effort, I argue that MIR research on genre classification can be helpful for studying genre as a social construct and cultural phenomenon if the pursuit of prediction performance and the cultural meaning of inaccurate prediction are carefully balanced."
Thomas Nuttall;Genís Plaja-Roglans;Lara Pearson;Xavier Serra,In Search of Sañcāras: Tradition-informed Repeated Melodic Pattern Recognition in Carnatic Music,2022,https://doi.org/10.5281/zenodo.7316666,"Thomas Nuttall, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Genís Plaja-Roglans, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Lara Pearson, Max Planck Institute for Empirical Aesthetics, Frankfurt am Main, Germany;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Carnatic Music is a South Indian art and devotional music practice in which melodic patterns (motifs and phrases), known as sañcāras, play a crucial structural and expressive role. We demonstrate how the combination of transposition invariant features learnt by a Complex Autoencoder (CAE) and predominant pitch tracks extracted using a Frequency-Temporal Attention Network (FTA-Net) can be used to annotate and group regions of variable-length, repeated, melodic patterns in audio recordings of multiple Carnatic Music performances. These models are trained on novel/expert-curated datasets of hundreds of Carnatic audio recordings and the extraction process tailored to account for the unique characteristics of sañcāras in Carnatic Music. Experimental results show that the proposed method is able to identify 54% of all sañcaras annotated by a professional Carnatic vocalist. Code to reproduce and interact with these results is available online."
Zhaowen Wang;Mingjin Che;Yue Yang;Wen Wu Meng;Qinyu Li;Fan Xia;Wei Li,Automatic Chinese National Pentatonic Modes Recognition Using Convolutional Neural Network,2022,https://doi.org/10.5281/zenodo.7316670,"Zhaowen Wang, Department of Music AI and Information Technology, Central Conservatory of Music, China;Mingjin Che, College of Experimental Art, Sichuan Conservatory of Music, China;Yue Yang, Department of Music AI and Information Technology, Central Conservatory of Music, China;Wenwu Meng, College of Experimental Art, Sichuan Conservatory of Music, China;Qinyu Li, College of Experimental Art, Sichuan Conservatory of Music, China;Fan Xia, College of Experimental Art, Sichuan Conservatory of Music, China;Wei Li, School of Computer Science and Technology, Fudan University, China","Chinese national pentatonic modes, with five tones of Gong, Shang, Jue, Zhi and Yu as the core, play an essential role in traditional Chinese music culture. After the early twentieth century, with the development of new Chinese music, the ancient Chinese theory of scales gradually developed into a new pentatonic modes theory under the influence of western music. In this paper, we briefly introduce our self-built CNPM (Chinese National Pentatonic Modes) Dataset, then design residual convolutional neural network models to identify which TongGong system the mode belongs, the pitch of tonic, the mode pattern and the mode type from audio signals, in combination with musical domain knowledge. We use both single-task and multi-task models with three strategies for identification, and compare them with a simple template-based baseline method. In experiments, we use seven accuracy metrics to evaluate the models. The results on identifying both the tonic pitch and the pattern of mode correctly achieve an average accuracy of 69.65%. As an initial research on automatic Chinese national pentatonic modes recognition, this work will contribute to the development of multicultural music information retrieval, computational ethnomusicology and five-tone music therapy."
David Gillman;Atalay Kutlay;Uday Goyat,Teach Yourself Georgian Folk Songs Dataset: A Annotated Corpus Of Traditional Vocal Polyphony,2022,https://doi.org/10.5281/zenodo.7316672,"David Gillman, New College of Florida;Uday Goyat, Georgia Institute of Technology;Atalay Kutlay, New College of Florida","New datasets of non-Western traditional music contribute to the development of knowledge in MIR and allow computational techniques to inform ethnomusicology. We present an annotated dataset of traditional vocal polyphony from two regions of the Republic of Georgia with disparate musical characteristics. The audio for each song consists of four polyphonic recordings of one performance from different microphones. We present a process and workflow that we use to annotate the dataset, which takes advantage of the salience of individual voices in each recording. The process results in an $f_0$ estimate for each vocal part."
Lucas S Maia;Martín Rocamora;Luiz W P Biscainho;Magdalena Fuentes,Adapting meter tracking models to Latin American music,2022,https://doi.org/10.5281/zenodo.7316673,"Lucas S. Maia, PEE/COPPE, Federal University of Rio de Janeiro, Brazil;Martín Rocamora, FING, Universidad de la República, Uruguay;Luiz W. P. Biscainho, PEE/COPPE, Federal University of Rio de Janeiro, Brazil;Magdalena Fuentes, MARL-IDM, New York University, United States","Beat and downbeat tracking models have improved significantly in recent years with the introduction of deep learning methods. However, despite these improvements, several challenges remain. Particularly, the adaptation of available models to underrepresented music traditions in MIR is usually synonymous with collecting and annotating large amounts of data, which is impractical and time-consuming. Transfer learning, data augmentation, and fine-tuning techniques have been used quite successfully in related tasks and are known to alleviate this bottleneck. Furthermore, when studying these music traditions, models are not required to generalize to multiple mainstream music genres but to perform well in more constrained, homogeneous conditions. In this work, we investigate simple yet effective strategies to adapt beat and downbeat tracking models to two different Latin American music traditions and analyze the feasibility of these adaptations in real-world applications concerning the data and computational requirements. Contrary to common belief, our findings show it is possible to achieve good performance by spending just a few minutes annotating a portion of the data and training a model in a standard CPU machine, with the precise amount of resources needed depending on the task and the complexity of the dataset."
Kaustuv Kanti Ganguli;Sertan Şentürk;Carlos Guedes,Critiquing Task- versus Goal-oriented Approaches: A Case for Makam Recognition,2022,https://doi.org/10.5281/zenodo.7316676,"Kaustuv Kanti Ganguli, College of Interdisciplinary Studies, Zayed University, UAE;Sertan ¸Sentürk, Independent Researcher, UK;Carlos Guedes, Music and Sound Cultures Lab, New York University Abu Dhabi, UAE","Computational Musicology and Music Information Retrieval (MIR) address the core musical question under study from a different perspective, often a combination of top-down vs. bottom-up approaches. However, the evaluation metrics for MIR tend to capture the model accuracy in terms of the goal. For instance, mode (melodic framework) recognition is implemented with a goal to evaluate and compare melodic analysis approaches, but it is worth investigating if at all it lends itself as one befitting proxy task. In this work, we aim to review whether the model actually learns the task it is intended for. This is particularly relevant in non-Eurogenetic music repertoires where the grammatical rules are rather prescriptive. We employ methodologies that combine domain-knowledge and data-driven optimizations as a possible way for a comprehensive understanding of these relationships. This is tested on Makam which is one of the understudied corpora in MIR. We evaluate an array of feature-engineering methods on the largest mode recognition dataset curated for Ottoman-Turkish makam music, composed of 1000 recordings in 50 makams. We adapted the time-delayed melody surfaces (TDMS) feature, which in combination with support vector machine (SVM) classifier yields 77.2% recognition accuracy, comparable to the current state-of-the-art. We also address (ethno)musicology-driven tasks with a view to gathering deeper insights into this music, such as tuning, intonation, and melodic similarity. We aim to propose avenues to extend the study to makam characterization over the mere goal of recognizing the mode, to better understand the (dis)similarity space and other plausible musically interesting facets."
Charilaos Papaioannou;Ioannis Valiantzas;Theodore Giannakopoulos;Maximos Kaliakatsos-Papakostas;Alexandros Potamianos,A Dataset for Greek Traditional and Folk Music: Lyra,2022,https://doi.org/10.5281/zenodo.7316678,"Charilaos Papaioannou, School of ECE, National Technical University of Athens, Greece;Ioannis Valiantzas, Department of Music Studies, National and Kapodistrian University Of Athens, Greece;Theodoros Giannakopoulos, National Center for Scientific Research - Demokritos, Greece;Maximos Kaliakatsos-Papakostas, Athena RC, Greece;Alexandros Potamianos, School of ECE, National Technical University of Athens, Greece","Studying under-represented music traditions under the MIR scope is crucial, not only for developing novel analysis tools, but also for unveiling musical functions that might prove useful in studying world musics. This paper presents a dataset for Greek Traditional and Folk music that includes 1570 pieces, summing in around 80 hours of data. The dataset incorporates YouTube timestamped links for retrieving audio and video, along with rich metadata information with regards to instrumentation, geography and genre, among others. The content has been collected from a Greek documentary series that is available online, where academics present music traditions of Greece with live music and dance performance during the show, along with discussions about social, cultural and musicological aspects of the presented music. Therefore, this procedure has resulted in a significant wealth of descriptions regarding a variety of aspects, such as musical genre, places of origin and musical instruments. In addition, the audio recordings were performed under strict production-level specifications, in terms of recording equipment, leading to very clean and homogeneous audio content. In this work, apart from presenting the dataset in detail, we propose a baseline deep-learning classification approach to recognize the involved musicological attributes. The dataset, the baseline classification methods and the models are provided in public repositories. Future directions for further refining the dataset are also discussed."
Yuya Yamamoto;Juhan Nam;Hiroko Terasawa,Analysis and detection of singing techniques in repertoires of J-POP solo singers,2022,https://doi.org/10.5281/zenodo.7316680,"Yuya Yamamoto, Doctoral Program in Informatics, University of Tsukuba, Japan;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea;Hiroko Terasawa, Doctoral Program in Informatics, University of Tsukuba, Japan","In this paper, we focus on singing techniques within the scope of music information retrieval research. We investigate how singers use singing techniques using real-world recordings of famous solo singers in Japanese popular music songs (J-POP). First, we built a new dataset of singing techniques. The dataset consists of 168 commercial J-POP songs, and each song is annotated using various singing techniques with timestamps and vocal pitch contours. We also present descriptive statistics of singing techniques on the dataset to clarify what and how often singing techniques appear. We further explored the difficulty of the automatic detection of singing techniques using previously proposed machine learning techniques. In the detection, we also investigate the effectiveness of auxiliary information (i.e., pitch and distribution of label duration), not only providing the baseline. The best result achieves 40.4% at macro-average F-measure on nine-way multi-class detection. We provide the annotation of the dataset and its detail on the appendix website."
Lele Liu;Qiuqiang Kong;Veronica Morfi;Emmanouil Benetos,Performance MIDI-to-score conversion by neural beat tracking,2022,https://doi.org/10.5281/zenodo.7316682,"Lele Liu, Centre for Digital Music, Queen Mary University of London, UK;Qiuqiang Kong, ByteDance Shanghai, China;Veronica Morfi, Centre for Digital Music, Queen Mary University of London, UK;Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London, UK","Rhythm quantisation is an essential part of converting performance MIDI recordings into musical scores. Previous works on rhythm quantisation are limited to the use of probabilistic or statistical methods. In this paper, we propose a MIDI-to-score quantisation method using a convolutional-recurrent neural network (CRNN) trained on MIDI note sequences to predict whether notes are on beats. Then, we expand the CRNN model to predict the quantised times for all beat and non-beat notes. Furthermore, we enable the model to predict the key signatures, time signatures, and hand parts of all notes. Our proposed performance MIDI-to-score system achieves significantly better performance compared to commercial software evaluated on the MV2H metric. We release the toolbox for converting performance MIDI into MIDI scores at: https://github.com/cheriell/PM2S ."
Sangjun Han;Hyeongrae Ihm;Moontae Lee;Woohyung Lim,Symbolic Music Loop Generation with Neural Discrete Representations,2022,https://doi.org/10.5281/zenodo.7342755,"Sangjun Han, LG AI Research;Hyeongrae Ihm, LG AI Research;Moontae Lee, LG AI Research, University of Illinois at Chicago;Woohyung Lim, LG AI Research","Since most of music has repetitive structures from motifs to phrases, repeating musical ideas can be a basic operation for music composition. The basic block that we focus on is conceptualized as loops which are essential ingredients of music. Furthermore, meaningful note patterns can be formed in a finite space, so it is sufficient to represent them with combinations of discrete symbols as done in other domains. In this work, we propose symbolic music loop generation via learning discrete representations. We first extract loops from MIDI datasets using a loop detector and then learn an autoregressive model trained by discrete latent codes of the extracted loops. We show that our model outperforms well-known music generative models in terms of both fidelity and diversity, evaluating on random space. Our code and supplementary materials are available at https://github.com/sjhan91/Loop_VQVAE_Official."
Marco A Martinez Ramirez;Weihsiang Liao;Chihiro Nagashima;Giorgio Fabbro;Stefan Uhlich;Yuki Mitsufuji,Automatic music mixing with deep learning and out-of-domain data,2022,https://doi.org/10.5281/zenodo.7316688,"Marco A. Martínez-Ramírez, Sony Group Corporation, Tokyo, Japan;Wei-Hsiang Liao, Sony Group Corporation, Tokyo, Japan;Giorgio Fabbro, Sony Europe B.V., Stuttgart, Germany;Stefan Uhlich, Sony Group Corporation, Tokyo, Japan;Chihiro Nagashima, Sony Group Corporation, Tokyo, Japan;Yuki Mitsufuji, Sony Group Corporation, Tokyo, Japan","Music mixing traditionally involves recording instruments in the form of clean, individual tracks and blending them into a final mixture using audio effects and expert knowledge (e.g., a mixing engineer). The automation of music production tasks has become an emerging field in recent years, where rule-based methods and machine learning approaches have been explored. Nevertheless, the lack of dry or clean instrument recordings limits the performance of such models, which is still far from professional human-made mixes. We explore whether we can use out-of-domain data such as wet or processed multitrack music recordings and repurpose it to train supervised deep learning models that can bridge the current gap in automatic mixing quality. To achieve this we propose a novel data preprocessing method that allows the models to perform automatic music mixing. We also redesigned a listening test method for evaluating music mixing systems. We validate our results through such subjective tests using highly experienced mixing engineers as participants."
Mahshid Alinoori;Vassilios Tzerpos,Music-STAR: a Style Translation system for Audio-based Re-instrumentation,2022,https://doi.org/10.5281/zenodo.7316690,"Mahshid Alinoori, Department of Electrical Engineering and Computer Science, York University, Canada;Vassilios Tzerpos, Department of Electrical Engineering and Computer Science, York University, Canada","Music style translation aims to generate variations of existing pieces of music by altering the style-related characteristics of the original piece while content, such as the melody, remains unchanged. These alterations could involve timbre translation, re-harmonization, or music rearrangement. Previous studies have achieved promising results utilizing time-frequency and symbolic music representations. Music style translation on raw audio has also been investigated and applied to single-instrument pieces. Although processing raw audio is more challenging, it provides richer information about timbres, dynamics, and articulations.In this paper, we introduce Music-STAR, the first audio-based translation system that translates the existing instruments in a piece into a set of target instruments without using source separation. To conduct our experiments, we also present an audio dataset that contains two-track pieces performed by two instrument sets alongside their stems. We carry out subjective and objective evaluations to compare Music-STAR with a variety of baseline methods and demonstrate its superiority."
Darius Afchar;Romain Hennequin;Vincent Guigue,Learning Unsupervised Hierarchies of Audio Concepts,2022,https://doi.org/10.5281/zenodo.7316692,"Darius Afchar, Deezer Research, France;Romain Hennequin, Deezer Research, France;Vincent Guigue, MLIA, ISIR - Sorbonne Université - CNRS, France","Music signals are difficult to interpret from their low-level features, perhaps even more than images: e.g. highlighting part of a spectrogram or an image is often insufficient to convey high-level ideas that are genuinely relevant to humans. In computer vision, concept learning was therein proposed to adjust explanations to the right abstraction level (e.g. detect clinical concepts from radiographs). These methods have yet to be used for MIR.In this paper, we adapt concept learning to the realm of music, with its particularities. For instance, music concepts are typically non-independent and of mixed nature (e.g. genre, instruments, mood), unlike previous work that assumed disentangled concepts.We propose a method to learn numerous music concepts from audio and then automatically hierarchise them to expose their mutual relationships. We conduct experiments on datasets of playlists from a music streaming service, serving as a few annotated examples for diverse concepts. Evaluations show that the mined hierarchies are aligned with both ground-truth hierarchies of concepts -- when available -- and with proxy sources of concept similarity in the general case."
Massimo Quadrana;Antoine Larreche-Mouly;Matthias Mauch,Multi-objective Hyper-parameter Optimization of Behavioral Song Embeddings,2022,https://doi.org/10.5281/zenodo.7316694,"Massimo Quadrana, Apple;Antoine Larreche-Mouly, Apple;Matthias Mauch, Apple","Song embeddings are a key component of most music recommendation engines. In this work, we study the hyper-parameter optimization of behavioral song embeddings based on Word2Vec on a selection of downstream tasks, namely next-song recommendation, false neighbor rejection, and artist and genre clustering. We present new optimization objectives and metrics to monitor the effects of hyper-parameter optimization. We show that single-objective optimization can cause side effects on the non optimized metrics and propose a simple multi-objective optimization to mitigate these effects.We find that next-song recommendation quality of Word2Vec is anti-correlated with song popularity, and we show how song embedding optimization can balance performance across different popularity levels.We then show potential positive downstream effects on the task of play prediction.Finally, we provide useful insights on the effects of training dataset scale by testing hyper-parameter optimization on an industry-scale dataset."
Huan Zhang;Jingjing Tang;Syed Rm Rafee;Simon Dixon;George Fazekas;Geraint A. Wiggins,ATEPP: A Dataset of Automatically Transcribed Expressive Piano Performance,2022,https://doi.org/10.5281/zenodo.7342764,"Huan Zhang, School of Electronic Engineering and Computer Science, Queen Mary University of London;Jingjing Tang, School of Electronic Engineering and Computer Science, Queen Mary University of London;Syed Rifat Mahmud Rafee, School of Electronic Engineering and Computer Science, Queen Mary University of London;Simon Dixon, School of Electronic Engineering and Computer Science, Queen Mary University of London;György Fazekas, School of Electronic Engineering and Computer Science, Queen Mary University of London","Computational models of expressive piano performance rely on attributes like tempo, timing, dynamics and pedalling. Despite some promising models for performance assessment and performance rendering, results are limited by the scale, breadth and uniformity of existing datasets. In this paper, we present ATEPP, a dataset that contains 1000 hours of performances of standard piano repertoire by 49 world-renowned pianists, organized and aligned by compositions and movements for comparative studies. Scores in MusicXML format are also available for around half of the tracks. We first evaluate and verify the use of transcribed MIDI for representing expressive performance with a listening evaluation that involves recent transcription models. Then, the process of sourcing and curating the dataset is outlined, including composition entity resolution and a pipeline for audio matching and solo filtering. Finally, we conduct baseline experiments for performer identification and performance rendering on our datasets, demonstrating its potential in generalizing expressive features of individual performing style."
Chen Zhang;Jiaxing Yu;Luchin Chang;Xu Tan;Jiawei Chen;Tao Qin;Kejun Zhang,PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription,2022,https://doi.org/10.5281/zenodo.7316698,"Chen Zhang, Department of Computer Science, Zhejiang University, China;Jiaxing Yu, Department of Computer Science, Zhejiang University, China;LuChin Chang, Department of Computer Science, Zhejiang University, China;Xu Tan, Microsoft Research Asia;Jiawei Chen, South China University of Technology;Tao Qin, Microsoft Research Asia;Kejun Zhang, Department of Computer Science, Zhejiang University, China","Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of the paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT."
Chitralekha Gupta;Yize Wei;Zequn Gong;Purnima Kamath;Zhuoyao Li;Lonce Wyse,Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures,2022,https://doi.org/10.5281/zenodo.7342809,"Chitralekha Gupta, National University of Singapore;Yize Wei, National University of Singapore;Zequn Gong, National University of Singapore;Purnima Kamath, National University of Singapore;Zhuoyao Li, National University of Singapore;Lonce Wyse, National University of Singapore","Standard evaluation metrics such as the Inception score and Fréchet Audio Distance provide a general audio quality distance metric between the synthesized audio and reference clean audio. However, the sensitivity of these metrics to variations in the statistical parameters that define an audio texture is not well studied. In this work, we provide a systematic study of the sensitivity of some of the existing audio quality evaluation metrics to parameter variations in audio textures. Furthermore, we also study three more potentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram matrix based distance, (b) an Accumulated Gram metric using a summarized version of the Gram matrices, and (c) a cochlear-model based statistical features metric. These metrics use deep features that summarize the statistics of any given audio texture, thus being inherently sensitive to variations in the statistical parameters that define an audio texture. We study and evaluate the sensitivity of existing standard metrics as well as Gram matrix and cochlear-model based metrics in response to control-parameter variations for audio textures across a wide range of texture and parameter types, and validate with subjective evaluation. We find that each of the metrics is sensitive to different sets of texture-parameter types. This is the first step towards investigating objective metrics for assessing parameter sensitivity in audio textures."
Igor Vatolkin;Cory Mckay,Stability of Symbolic Feature Group Importance in the Context of Multi-Modal Music Classification,2022,https://doi.org/10.5281/zenodo.7316702,"Igor Vatolkin, TU Dortmund University;Cory McKay, Marianopolis College","Multi-modal music classification creates supervised models trained on features from different sources (modalities): the audio signal, the score, lyrics, album covers, expert tags, etc. A concept of ""multi-group feature importance"" not only helps to measure the individual relevance of features of a feature type under investigation (such as the instruments present in a piece), but also serves to quantify the potential for further improving classification by adding features from other feature types or extracted from different kinds of sources, based on a multi-objective analysis of feature sets after evolutionary feature selection. In this study, we investigate the stability of feature group importance when different classification methods and different measures of classification quality are applied. Since musical scores are particularly helpful in deriving semantically meaningful, robust genre characteristics, we focus on the feature groups analyzed by the jSymbolic feature extraction software, which describe properties associated with instrumentation, basic pitch statistics, melody, chords, tempo, and other rhythmic aspects. These symbolic features are analyzed in the context of musical information drawn from five other modalities, and experiments are conducted involving two datasets, one small and one large. The results show that, although some feature groups can remain similarly important compared to others, differences can also be evident in various applications, and can depend on the particular classifier and evaluation measure being used. Insights drawn from this type of analysis can potentially be helpful in effectively matching specific features or feature groups to particular classifiers and evaluation measures in future feature-based MIR research."
Franca Bittner;Marcel Gonzalez;Maike L Richter;Hanna Lukashevich;Jakob Abeßer,Multi-pitch Estimation meets Microphone Mismatch: Applicability of Domain Adaptation,2022,https://doi.org/10.5281/zenodo.7342840,"Franca Bittner, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany;Marcel Gonzalez, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany;Maike Richter, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany;Hanna Lukashevich, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany;Jakob Abeûer, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany","The performance of machine learning (ML) models is known to be affected by discrepancies between training (source) and real-world (target) data distributions. This problem is referred to as domain shift and is commonly approached using domain adaptation (DA) methods. As one relevant scenario, automatic piano transcription algorithms in music learning applications potentially suffer from domain shift since pianos are recorded in different acoustic conditions using various devices. Yet, most currently available datasets for piano transcription only cover ideal recording situations with high-quality microphones. Consequently, a transcription model trained on these datasets will face a mismatch between source and target data in real-world scenarios. To address this issue, we employ a recently proposed dataset which includes annotated piano recordings covering typical real-life recording settings for a piano learning application on mobile devices. We first quantify the influence of the domain shift on the performance of a deep learning-based piano multi-pitch estimation (MPE) algorithm. Then, we employ and evaluate four unsupervised DA methods to reduce domain shift. Our results show that the studied MPE model is surprisingly robust to domain shift in microphone mismatch scenarios and the DA methods do not notably improve the transcription performance."
Chris Donahue;John Thickstun;Percy Liang,Melody transcription via generative pre-training,2022,https://doi.org/10.5281/zenodo.7316706,"Chris Donahue, Stanford University;John Thickstun, Stanford University;Percy Liang, Stanford University","Despite the central role that melody plays in music perception, it remains an open challenge in MIR to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in *melody transcription* is building methods which can handle broad audio containing any number of instrument ensembles and musical styles---existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data---we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build a system capable of transcribing human-readable lead sheets directly from music audio."
Yigitcan Özer;Meinard Müller,Source Separation of Piano Concertos with Test-Time Adaptation,2022,https://doi.org/10.5281/zenodo.7316708,"Yigitcan Özer, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Music source separation (MSS) aims at decomposing a music recording into constituent sources, such as a lead instrument and the accompaniment. Despite the difficulties in MSS due to the high correlation of musical sources in time and frequency, deep neural networks (DNNs) have led to substantial improvements to accomplish this task. For training supervised machine learning models such as DNNs, isolated sources are required. In the case of popular music, one can exploit open-source datasets which involve multitrack recordings of vocals, bass, and drums. For western classical music, however, isolated sources are generally not available. In this article, we consider the case of piano concertos, which are composed for a pianist typically accompanied by an orchestra. The lack of multitrack recordings makes training supervised machine learning models for the separation of piano and orchestra challenging. To overcome this problem, we generate artificial training material by randomly mixing sections of the solo piano repertoire (e.g., piano sonatas) and orchestral pieces without piano (e.g., symphonies) to train state-of-the-art DNN models for MSS. As our main contribution, we propose a test-time adaptation (TTA) procedure, which exploits random mixtures of the piano-only and orchestra-only parts in the test data to further improve the separation quality."
Martha E Thomae Elias;Julie Cumming;Ichiro Fujinaga,Counterpoint Error-Detection Tools for Optical Music Recognition of Renaissance Polyphonic Music,2022,https://doi.org/10.5281/zenodo.7316710,"Martha E. Thomae, McGill University;Julie E. Cumming, McGill University;Ichiro Fujinaga, McGill University","This paper discusses part of a larger project to preserve and increase access to Guatemalan music sources written in mensural notation by using a digitization and music information retrieval (MIR) workflow to obtain both digital images and symbolic scores with editorial corrections. The workflow involves MIR tools such as optical music recognition (OMR), automatic voice alignment for mensural notation, editorial correction software, and computational counterpoint error detection. In this paper, we evaluate whether the use of automatic counterpoint error-detection tools makes the correction process more efficient. The results confirm that marking illegal dissonances in the score following the rules of Renaissance counterpoint indeed makes the process of editorial correction of scribal errors in Renaissance music more efficient by reducing the time taken and improving the accuracy of such corrections. Moreover, marking the illegal dissonances in the score also allowed us to catch OMR errors that had passed through undetected at a previous stage of the workflow."
Louis Couturier;Louis Bigo;Florence Leve,A Dataset of Symbolic Texture Annotations in Mozart Piano Sonatas,2022,https://doi.org/10.5281/zenodo.7316712,"Louis Couturier, MIS, Université de Picardie Jules Verne, Amiens, France;Louis Bigo, CRIStAL, UMR 9189 CNRS, Université de Lille, France;Florence Levé, MIS, Université de Picardie Jules Verne, Amiens, France","Musical scores are generally analyzed under different aspects, notably melody, harmony, rhythm, but also through their texture, although this last concept is arguably more delicate to formalize. Symbolic texture depicts how sounding components are organized in the score. It outlines the density of elements, their heterogeneity, role and interactions. In this paper, we release a set of manual annotations for each bar of 9 movements among early piano sonatas by W. A. Mozart, totaling 1164 labels that follow a syntax dedicated to piano score texture. A quantitative analysis of the annotations highlights some characteristic textural features in the corpus. In addition, we present and release the implementation of low-level descriptors of symbolic texture. These descriptors can be correlated with texture annotations and used in different machine-learning tasks. Along with provided data, they offer promising applications in computer assisted music analysis and composition."
Nazif Can Tamer;Pedro Ramoneda;Xavier Serra,Violin Etudes: A Comprehensive Dataset for f0 Estimation and Performance Analysis,2022,https://doi.org/10.5281/zenodo.7316714,"Nazif Can Tamer, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Pedro Ramoneda, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona","Violin performance analysis requires accurate and robust f0 estimates to give feedback on the playing accuracy. Despite the recent advancements in data-driven f0 estimators, their application to performance analysis remains a challenge due to style-specific and dataset-induced biases. In this paper, we address this problem by introducing Violin Etudes, a 27.8-hours violin performance dataset constructed with domain knowledge in instrument pedagogy and a novel automatic f0-labeling paradigm. Experimental results on unseen datasets show that the CREPE f0 estimator trained on Violin Etudes outperforms the widely-used pre-trained version trained on multiple manually-labeled datasets. Further preliminary findings suggest that (i) existing data-driven f0 estimators may overfit to equal temperament, and (ii) iterative re-labeling regularized by our novel Constrained Harmonic Resynthesis method can simultaneously enhance datasets and f0 estimators. Our dataset curation methodology is easily scalable to other instruments owing to the quantity of pedagogical data online. It also supports a range of MIR research directions thanks to the performance difficulty labels from educational institutions."
Nikita Srivatsan;Taylor Berg-Kirkpatrick,Checklist Models for Improved Output Fluency in Piano Fingering Prediction,2022,https://doi.org/10.5281/zenodo.7316716,"Nikita Srivatsan, Carnegie Mellon University;Taylor Berg-Kirkpatrick, UC San Diego","In this work we present a new approach for the task of predicting fingerings for piano music. While prior neural approaches have often treated this as a sequence tagging problem with independent predictions, we put forward a checklist system, trained via reinforcement learning, that maintains a representation of recent predictions in addition to a hidden state, allowing it to learn soft constraints on output structure. We also demonstrate that by modifying input representations --- which in prior work using neural models have often taken the form of one-hot encodings over individual keys on the piano --- to encode relative position on the keyboard to the prior note instead, we can achieve much better performance. Additionally, we reassess the use of raw per-note labeling precision as an evaluation metric, noting that it does not adequately measure the fluency, i.e. human playability, of a model's output. To this end, we compare methods across several statistics which track the frequency of adjacent finger predictions that while independently reasonable would be physically challenging to perform in sequence, and implement a reinforcement learning strategy to minimize these as part of our training loss. Finally through human expert evaluation, we demonstrate significant gains in performability directly attributable to improvements with respect to these metrics."
Jaidev Shriram;Makarand Tapaswi;Vinoo Alluri,Sonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations,2022,https://doi.org/10.5281/zenodo.7372149,"Jaidev Shriram, International Institute of Information Technology, Hyderabad;Makarand Tapaswi, International Institute of Information Technology, Hyderabad;Vinoo Alluri, International Institute of Information Technology, Hyderabad","Reading, much like music listening, is an immersive experience that transports readers while taking them on an emotional journey. Listening to complementary music has the potential to amplify the reading experience, especially when the music is stylistically cohesive and emotionally relevant. In this paper, we propose the first fully automatic method to build a dense soundtrack for books, which can play high-quality instrumental music for the entirety of the reading duration. Our work employs a unique text processing and music weaving pipeline that determines the context and emotional composition of scenes in a chapter. This allows our method to identify and play relevant excerpts from the soundtrack of the book's movie adaptation. By relying on the movie composer's craftsmanship, our book soundtracks include expert-made motifs and other scene-specific musical characteristics. We validate the design decisions of our approach through a perceptual study. Our readers note that the book soundtrack greatly enhanced their reading experience, due to high immersiveness granted via uninterrupted and style-consistent music, and a heightened emotional state attained via high precision emotion and scene context recognition."
Marco Pasini;Jan Schlüter,Musika! Fast Infinite Waveform Music Generation,2022,https://doi.org/10.5281/zenodo.7316720,"Marco Pasini, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Jan Schlüter, Institute of Computational Perception, Johannes Kepler University Linz, Austria","Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours."
Jiafeng Liu;Yuanliang Dong;Zehua Cheng;Xinran Zhang;Xiaobing Li;Feng Yu;Maosong Sun,Symphony Generation with Permutation Invariant Language Model,2022,https://doi.org/10.5281/zenodo.7316722,"Jiafeng Liu, Department of Music AI and Music Information Technology, Central Conservatory of Music;Yuanliang Dong, Department of Music AI and Music Information Technology, Central Conservatory of Music;Zehua Cheng, Department of Computer Science, University of Oxford;Xinran Zhang, Department of Music AI and Music Information Technology, Central Conservatory of Music;Xiaobing Li, Department of Music AI and Music Information Technology, Central Conservatory of Music;Feng Yu, Department of Music AI and Music Information Technology, Central Conservatory of Music;Maosong Sun, Department of Music AI and Music Information Technology, Central Conservatory of Music, Department of Computer Science and Technology , Tsinghua University","In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation."
Qingqing Huang;Aren Jansen;Joonseok Lee;Ravi Ganti;Judith Yue Li;Daniel P W Ellis,MuLan: A Joint Embedding of Music Audio and Natural Language,2022,https://doi.org/10.5281/zenodo.7316724,"Qingqing Huang, Google Research, Seoul National University;Aren Jansen, Google Research, Seoul National University;Joonseok Lee, Google Research, Seoul National University;Ravi Ganti, Google Research, Seoul National University;Judith Yue Li, Google Research, Seoul National University;Daniel P. W. Ellis, Google Research, Seoul National University","Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications."
Peiling Lu;Xu Tan;Botao Yu;Tao Qin;Sheng Zhao;Tie-Yan Liu,MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks,2022,https://doi.org/10.5281/zenodo.7342745,"Peiling Lu, Microsoft Research Asia, Beijing, China;Xu Tan, Microsoft Research Asia, Beijing, China;Botao Yu, Nanjing University, Nanjing, China;Tao Qin, Microsoft Research Asia, Beijing, China;Sheng Zhao, Microsoft Azure Speech, Beijing, China;Tie-Yan Liu, Microsoft Research Asia, Beijing, China","Human usually composes music by organizing elements according to the musical form to express music ideas. However, for neural network-based music generation, it is difficult to do so due to the lack of labelled data on musical form. In this paper, we develop MeloForm, a system that generates melody with musical form using expert systems and neural networks. Specifically, 1) we design an expert system to generate a melody by developing musical elements from motifs to phrases then to sections with repetitions and variations according to pre-given musical form; 2) considering the generated melody is lack of musical richness, we design a Transformer based refinement model to improve the melody without changing its musical form. MeloForm enjoys the advantages of precise musical form control by expert systems and musical richness learning via neural models. Both subjective and objective experimental evaluations demonstrate that MeloForm generates melodies with precise musical form control with 97.79% accuracy, and outperforms baseline systems in terms of subjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure, thematic, richness and overall quality, without any labelled musical form data. Besides, MeloForm can support various kinds of forms, such as verse and chorus form, rondo form, variational form, sonata form, etc."
Chang-Bin Jeon;Kyogu Lee,Towards robust music source separation on loud commercial music,2022,https://doi.org/10.5281/zenodo.7342777,"Chang-Bin Jeon, Seoul National University;Kyogu Lee, Seoul National University","Nowadays, commercial music has extreme loudness and heavily compressed dynamic range compared to the past. Yet, in music source separation, these characteristics have not been thoroughly considered, resulting in the domain mismatch between the laboratory and the real world. In this paper, we confirmed that this domain mismatch negatively affect the performance of the music source separation networks. To this end, we first created the out-of-domain evaluation datasets, musdb-L and XL, by mimicking the music mastering process. Then, we quantitatively verify that the performance of the state-of-the-art algorithms significantly deteriorated in our datasets. Lastly, we proposed LimitAug data augmentation method to reduce the domain mismatch, which utilizes an online limiter during the training data sampling process. We confirmed that it not only alleviates the performance degradation on our out-of-domain datasets, but also results in higher performance on in-domain data."
Michael Zhou;Andrew Mcgraw;Douglas R Turnbull,Towards Quantifying the Strength of Music Scenes Using Live Event Data,2022,https://doi.org/10.5281/zenodo.7316730,"Michael Zhou, Columbia University;Andrew McGraw, University of Richmond;Douglas R. Turnbull, Ithaca College","There are many benefits for a community when there is a vibrant local music scene (e.g., increased mental &amp; physical well-being, increased economic activity) and there are many factors that contribute to an environment in which a live music scene can thrive (e.g., available performance spaces, helpful government policies). In this paper, we explore using an estimate of the live music event rate (LMER) as a rough indicator to measure the strength of a local music scene. We define LMER as the number of music shows per 100,000 people per year and then explore how this indicator is (or is not) correlated with 28 other socioeconomic indicators. To do this, we analyze a set of 308,051 music events from 2019 across 1,139 cities in the United States. Our findings reveal that factors related to transportation (e.g., high walkability), population (high density), economics (high employment rate), age (high proportion of individuals age 20-29), and education (bachelor's degree or higher) are strongly correlated with having a high number of live music events. Conversely, we did not find statistically significant evidence that other indica- tors (e.g., racial diversity) are correlated."
Morgan Buisson;Brian Mcfee;Slim Essid;Hélène C.  Crayencour Crayencour,Learning Multi-Level Representations for Hierarchical Music Structure Analysis.,2022,https://doi.org/10.5281/zenodo.7343060,"Morgan Buisson, LTCI, Télécom Paris, Institut Polytechnique de Paris, France;Brian McFee, Music and Audio Research Laboratory, New York University, USA;Slim Essid, LTCI, Télécom Paris, Institut Polytechnique de Paris, France;Hélène C. Crayencour, L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France","Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, we explore unsupervised learning of such representations using a contrastive approach operating at different time-scales. We evaluate the proposed system on flat and multi-level music segmentation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity."
Curtis Hawthorne;Ian Simon;Adam Roberts;Neil Zeghidour;Joshua Gardner;Ethan Manilow;Jesse Engel,Multi-instrument Music Synthesis with Spectrogram Diffusion,2022,https://doi.org/10.5281/zenodo.7316734,"Curtis Hawthorne, Google Research, Brain Team;Ian Simon, Google Research, Brain Team;Adam Roberts, Google Research, Brain Team;Neil Zeghidour, Google Research, Brain Team;Josh Gardner, ;Ethan Manilow, ","An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fréchet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes."
Franco Caspe;Andrew Mcpherson;Mark Sandler,DDX7: Differentiable FM Synthesis of Musical Instrument Sounds,2022,https://doi.org/10.5281/zenodo.7343063,"Franco Caspe, Centre for Digital Music, Queen Mary University of London, UK;Andrew McPherson, Centre for Digital Music, Queen Mary University of London, UK;Mark Sandler, Centre for Digital Music, Queen Mary University of London, UK","FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source. On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks."
Mojtaba Heydari;Zhiyao Duan,Singing beat tracking with Self-supervised front-end and linear transformers,2022,https://doi.org/10.5281/zenodo.7316738,"Mojtaba Heydari, Department of Electrical and Computer Engineering, University of Rochester;Zhiyao Duan, Department of Electrical and Computer Engineering, University of Rochester","Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction. Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features."
Saurjya Sarkar;Emmanouil Benetos;Mark Sandler,EnsembleSet: a new high quality synthesised dataset for chamber ensemble separation,2022,https://doi.org/10.5281/zenodo.7316740,"Saurjya Sarkar, Centre for Digital Music, Queen Mary University of London, UK;Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London, UK;Mark Sandler, Centre for Digital Music, Queen Mary University of London, UK","Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articulation mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets."
Tengyu Deng;Eita Nakamura;Kazuyoshi Yoshii,End-to-End Lyrics Transcription Informed by Pitch and Onset Estimation,2022,https://doi.org/10.5281/zenodo.7316742,"Tengyu Deng, Graduate School of Informatics, Kyoto University, Japan;Eita Nakamura, Graduate School of Informatics, Kyoto University, Japan;Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, Japan, PRESTO, Japan Science and Technology Agency (JST), Japan","This paper presents an automatic lyrics transcription (ALT) method for music recordings that leverages the framewise semitone-level sung pitches estimated in a multi-task learning framework. Compared to automatic speech recognition (ASR), ALT is challenging due to the insufficiency of training data and the variation and contamination of acoustic features caused by singing expressions and accompaniment sounds. The domain adaptation approach has thus recently been taken for updating an ASR model pre-trained from sufficient speech data. In the naive application of the end-to-end approach to ALT, the internal audio-to-lyrics alignment often fails due to the time-stretching nature of singing features. To stabilize the alignment, we make use of the semi-synchronous relationships between notes and characters. Specifically, a convolutional recurrent neural network (CRNN) is used for estimating the semitone-level pitches with note onset times while eliminating the intra- and inter-note pitch variations. This estimate helps an end-to-end ALT model based on connectionist temporal classification (CTC) learn correct audio-to-character alignment and mapping, where the ALT model is trained jointly with the pitch and onset estimation model. The experimental results show the usefulness of the pitch and onset information in ALT."
Ilaria Manco;Emmanouil Benetos;Elio Quinton;George Fazekas,Contrastive Audio-Language Learning for Music,2022,https://doi.org/10.5281/zenodo.7316744,"Ilaria Manco, School of EECS, Queen Mary University of London, London, U.K;Emmanouil Benetos, School of EECS, Queen Mary University of London, London, U.K;Elio Quinton, Music & Audio Machine Learning Lab, Universal Music Group, London, U.K;György Fazekas, School of EECS, Queen Mary University of London, London, U.K","As one of the most intuitive interfaces known to humans, natural language has the potential to mediate many tasks that involve human-computer interaction, especially in application-focused fields like Music Information Retrieval. In this work, we explore cross-modal learning in an attempt to bridge audio and language in the music domain. To this end, we propose MusCALL, a framework for Music Contrastive Audio-Language Learning. Our approach consists of a dual-encoder architecture that learns the alignment between pairs of music audio and descriptive sentences, producing multimodal embeddings that can be used for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to this property, MusCALL can be transferred to virtually any task that can be cast as text-based retrieval. Our experiments show that our method performs significantly better than the baselines at retrieving audio that matches a textual description and, conversely, text that matches an audio query. We also demonstrate that the multimodal alignment capability of our model can be successfully extended to the zero-shot transfer scenario for genre classification and auto-tagging on two public datasets."
Dmitry Bogdanov;Xavier Lizarraga-Seijas;Pablo Alonso-Jiménez;Xavier Serra,MusAV: A dataset of relative arousal-valence annotations for validation of audio models,2022,https://doi.org/10.5281/zenodo.7316746,"Dmitry Bogdanov, Music Technology Group, Universitat Pompeu Fabra, Spain;Xavier Lizarraga-Seijas, Music Technology Group, Universitat Pompeu Fabra, Spain;Pablo Alonso-Jiménez, Music Technology Group, Universitat Pompeu Fabra, Spain;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Spain","We present MusAV, a new public benchmark dataset for comparative validation of arousal and valence (AV) regression models for audio-based music emotion recognition. To gather the ground truth, we rely on relative judgments instead of absolute values to simplify the manual annotation process and improve its consistency. We build MusAV by gathering comparative annotations of arousal and valence on pairs of tracks, using track audio previews and metadata from the Spotify API. The resulting dataset contains 2,092 track previews covering 1,404 genres, with pairwise relative AV judgments by 20 annotators and various subsets of the ground truth based on different levels of annotation agreement. We demonstrate the use of the dataset in an example study evaluating nine models for AV regression that we train based on state-of-the-art audio embeddings and three existing datasets of absolute AV annotations. The results on MusAV offer a view of the performance of the models complementary to the metrics obtained during training and provide insights into the impact of the considered datasets and embeddings on the generalization abilities of the models."
Shuqi Dai;Huiran Yu;Roger B Dannenberg,What is missing in deep music generation? A study of repetition and structure in popular music,2022,https://doi.org/10.5281/zenodo.7316748,"Shuqi Dai, Carnegie Mellon University;Huiran Yu, Carnegie Mellon University;Roger B. Dannenberg, Carnegie Mellon University","Structure is one of the most essential aspects of music, and music structure is commonly indicated through repetition. However, the nature of repetition and structure in music is still not well understood, especially in the context of music generation, and much remains to be explored with Music Information Retrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and American) illustrate important music construction principles: (1) structure exists at multiple hierarchical levels, (2) songs use repetition and limited vocabulary so that individual songs do not follow general statistics of song collections, (3) structure interacts with rhythm, melody, harmony, and predictability, and (4) over the course of a song, repetition is not random, but follows a general trend as revealed by cross-entropy. These and other findings offer challenges as well as opportunities for deep-learning music generation and suggest new formal music criteria and evaluation methods. Music from recent music generation systems is analyzed and compared to human-composed music in our datasets, often revealing striking differences from a structural perspective."
Angelo Cesar Mendes Da Silva;Diego F Silva;Ricardo Marcondes Marcacini,Heterogeneous Graph Neural Network for Music Emotion Recognition,2022,https://doi.org/10.5281/zenodo.7316750,"Angelo Cesar Mendes da Silva, Universidade de São Paulo, Brazil;Diego Furtado Silva, Universidade Federal de São Carlos, Brazil;Ricardo Marcondes Marcacini, Universidade de São Paulo, Brazil","Music emotion recognition has been a growing field of research motivated by the wealth of information that these labels express. Recognition of emotions highlights music's social and psychological functions, extending traditional applications such as style recognition or content similarity. Once musical data are intrinsically multi-modal, exploring this characteristic is usually beneficial. However, building a structure that incorporates different modalities in a unique space to represent the songs is challenging. Integrating information from related instances by learning heterogeneous graph-based representations has achieved state-of-the-art results in multiple tasks. This paper proposes structuring musical features over a heterogeneous network and learning a multi-modal representation using Graph Convolutional Networks with features extracted from audio and lyrics as inputs to handle the music emotion recognition tasks. We show that the proposed learning approach resulted in a representation with greater power to discriminate emotion labels. Moreover, our heterogeneous graph neural network classifier outperforms related works for music emotion recognition."
Mathilde Abrassart;Guillaume Doras,"And what if two musical versions don't share melody, harmony, rhythm, or lyrics ?",2022,https://doi.org/10.5281/zenodo.7316752,"Mathilde Abrassart, Ircam Amplify;Guillaume Doras, Ircam, Sorbonne Université, CNRS, STMS Lab","Version identification (VI) has seen substantial progress over the past few years. On the one hand, the introduction of the metric learning paradigm has favored the emergence of scalable yet accurate VI systems. On the other hand, using features focusing on specific aspects of musical pieces, such as melody, harmony, or lyrics, yielded interpretable and promising performances. In this work, we build upon these recent advances and propose a metric learning-based system systematically leveraging four dimensions commonly admitted to convey musical similarity between versions: melodic line, harmonic structure, rhythmic patterns, and lyrics. We describe our deliberately simple model architecture, and we show in particular that an approximated representation of the lyrics is an efficient proxy to discriminate between versions and non-versions. We then describe how these features complement each other and yield new state-of-the-art performances on two publicly available datasets. We finally suggest that a VI system using a combination of melodic, harmonic, rhythmic and lyrics features could theoretically reach the optimal performances obtainable on these datasets."
Genís Plaja-Roglans;Marius Miron;Xavier Serra,A diffusion-inspired training strategy for singing voice extraction in the waveform domain,2022,https://doi.org/10.5281/zenodo.7316754,"Genís Plaja-Roglans, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Marius Miron, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Notable progress in music source separation has been achieved using multi-branch networks that operate on both temporal and spectral domains. However, such networks tend to be complex and heavy-weighted. In this work, we tackle the task of singing voice extraction from polyphonic music signals in an end-to-end manner using an approach inspired by the training procedure of denoising diffusion models. We perform unconditional signal modelling to gradually convert an input mixture signal to the corresponding singing voice or accompaniment. We use fewer parameters than the state-of-the-art models while operating on the waveform domain, bypassing phase-related problems. More concisely, we train a non-causal WaveNet using a diffusion-inspired strategy improving the said network for singing voice extraction and obtaining performance comparable to the end-to-end state-of-the-art on MUSDB18. We further report results on a non-MUSDB-overlapping version of MedleyDB and the multi-track audio of the Saraga Carnatic dataset showing good generalization, and run perceptual tests of our approach. Code, models, and audio examples are made available."
Romain Loiseau;Baptiste Bouvier;Yann Teytaut;Elliot Vincent;Mathieu Aubry;Loic Landrieu,A Model You Can Hear: Audio Identification with Playable Prototypes,2022,https://doi.org/10.5281/zenodo.7316756,"Romain Loiseau, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France;Baptiste Bouvier, ;Yann Teytaut, ;Elliot Vincent, INRIA and DIENS (ENS-PSL, CNRS, INRIA);Mathieu Aubry, LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France;Loic Landrieu, LASTIG, Univ Gustave Eiffel, IGN, ENSG","Machine learning techniques have proved useful for classifying and analyzing audio content. However, recent methods typically rely on abstract and high-dimensional representations that are difficult to interpret. Inspired by transformation-invariant approaches developed for image and 3D data, we propose an audio identification model based on learnable spectral prototypes. Equipped with dedicated transformation networks, these prototypes can be used to cluster and classify input audio samples from large collections of sounds. Our model can be trained with or without supervision and reaches state-of-the-art results for speaker and instrument identification, while remaining easily interpretable. The code is available at: https://github.com/romainloiseau/a-model-you-can-hear"
Marcos Acosta;Irmak Bukey;T J Tsai,An Exploration of Generating Sheet Music Images,2022,https://doi.org/10.5281/zenodo.7316758,"Marcos Acosta, Harvey Mudd College;Irmak Bukey, Pomona College;TJ Tsai, Harvey Mudd College","Many previous works in recent years have explored various forms of music generation. These works have focused on generating either raw audio waveforms or symbolic music. In this work, we explore the feasibility of generating sheet music images, which is often the primary form in which musical compositions are notated for other musicians. Using the PrIMuS dataset as a testbed, we explore five different sequence-based approaches for generating lines of sheet music: generating sequences of (a) pixel columns, (b) image patches, (c) visual word tokens, (d) semantic tokens, and (e) XML-based tags. We show sample generated images, discuss the practical challenges and problems with each approach, and give our recommendation on the most promising paths to explore in the future."
Weixing Wei;Peilin Li;Yi Yu;Wei Li,HPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano Transcription,2022,https://doi.org/10.5281/zenodo.7316762,"Weixing Wei, School of Computer Science and Technology, Fudan University , China;Peilin Li, School of Computer Science and Technology, Fudan University , China;Yi Yu, Digital Content and Media Sciences Research Division, National Institute of Informatics (NII), Japan;Wei Li, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, China","While neural network models are making significant progress in piano transcription, they are becoming more resource-consuming due to requiring larger model size and more computing power. In this paper, we attempt to apply more prior about piano to reduce model size and improve the transcription performance. The sound of a piano note contains various overtones, and the pitch of a key does not change over time. To make full use of such latent information, we propose HPPNet that using the Harmonic Dilated Convolution to capture the harmonic structures and the Frequency Grouped Recurrent Neural Network to model the pitch-invariance over time. Experimental results on the MAESTRO dataset show that our piano transcription system achieves state-of-the-art performance both in frame and note scores (frame F1 93.15%, note F1 97.18%). Moreover, the model size is much smaller than the previous state-of-the-art deep learning models."
Pedro L T Neves;José Fornari;João B Florindo,Generating music with sentiment using Transformer-GANs,2022,https://doi.org/10.5281/zenodo.7342704,"Pedro L. T. Neves, State University of Campinas;Jose Fornari, State University of Campinas;João B. Florindo, State University of Campinas","The field of Automatic Music Generation has seen significant progress thanks to the advent of Deep Learning. However, most of these results have been produced by unconditional models, which lack the ability to interact with their users, not allowing them to guide the generative process in meaningful and practical ways. Moreover, synthesizing music that remains coherent across longer timescales while still capturing the local aspects that make it sound ``realistic'' or human-like is still challenging. This is due to the large computational requirements needed to work with long sequences of data, and also to limitations imposed by the training schemes that are often employed. In this paper, we propose a generative model of symbolic music conditioned by data retrieved from human sentiment. The model is a Transformer-GAN trained with labels that correspond to different configurations of the valence and arousal dimensions that quantitatively represent human affective states. We try to tackle both of the problems above by employing an efficient linear version of Attention and using a Discriminator both as a tool to improve the overall quality of the generated music and its ability to follow the conditioning signals."
Ke Chen;Hao-Wen Dong;Yi Luo;Julian Mcauley;Taylor Berg-Kirkpatrick;Miller Puckette;Shlomo Dubnov,Improving Choral Music Separation through Expressive Synthesized Data from Sampled Instruments,2022,https://doi.org/10.5281/zenodo.7316766,"Ke Chen, UC San Diego, USA;Hao-Wen Dong, UC San Diego, USA;Yi Luo, Tencent AI Lab, China;Julian McAuley, UC San Diego, USA;Taylor Berg-Kirkpatrick, UC San Diego, USA;Miller Puckette, UC San Diego, USA;Shlomo Dubnov, UC San Diego, USA","Choral music separation refers to the task of extracting tracks of voice parts (e.g., soprano, alto, tenor, and bass) from mixed audio. The lack of datasets has impeded research on this topic as previous work has only been able to train and evaluate models on a few minutes of choral music data due to copyright issues and dataset collection difficulties. In this paper, we investigate the use of synthesized training data for the source separation task on real choral music. We make three contributions: first, we provide an automated pipeline for synthesizing choral music data from sampled instrument plugins within controllable options for instrument expressiveness. This produces an 8.2-hour-long choral music dataset from the JSB Chorales Dataset and one can easily synthesize additional data. Second, we conduct an experiment to evaluate multiple separation models on available choral music separation datasets from previous work. To the best of our knowledge, this is the first experiment to comprehensively evaluate choral music separation. Third, experiments demonstrate that the synthesized choral data is of sufficient quality to improve the model's performance on real choral music datasets. This provides additional experimental statistics and data support for the choral music separation study."
Kyungyun Lee;Gladys Hitt;Emily Terada;Jin Ha Lee,Ethics of Singing Voice Synthesis: Perceptions of Users and Developers,2022,https://doi.org/10.5281/zenodo.7316768,"Kyungyun Lee, Gaudio Lab, Seoul, Korea;Gladys Hitt, University of Washington, Seattle, USA;Emily Terada, University of Washington, Seattle, USA;Jin Ha Lee, University of Washington, Seattle, USA","Singing Voice Synthesis (SVS) has recently garnered much attention as its quality has improved vastly with the use of artificial intelligence (AI), creating many opportunities for supporting music creators and listeners. Recently, there have been growing concerns about ethical issues related to AI development in general, and to AI-based SVS development specifically. Many questions remain unexplored about how to ethically develop and use such technology. In this paper, we investigate the perception of ethical issues related to SVS from the perspectives of two different groups: the general public and developers. We collected 3,075 user comments from YouTube videos showcasing various uses of SVS as part of a mainstream variety show. Additionally, we interviewed six researchers developing SVS technology. Through thematic analysis, we identify and discuss three different aspects related to ethical issues in SVS development, highlighting the similarities and differences between the perspectives of the general public and developers: (1) Use scenarios, (2) Attitudes towards development, and (3) Meaning of Creativity, and (4) Concerns about human rights, intellectual property (IP) and legal issues."
Takuya Takahashi;Mathieu Barthet,Emotion-driven Harmonisation And Tempo Arrangement of Melodies Using Transfer Learning,2022,https://doi.org/10.5281/zenodo.7316770,"Takuya Takahashi, Centre for Digital Music, Queen Mary University of London;Mathieu Barthet, Centre for Digital Music, Queen Mary University of London","We propose and assess deep learning models for harmonic and tempo arrangement generation given melodies and emotional constraints. A dataset of 4000 symbolic scores and emotion labels was gathered by expanding the HTPD3 dataset with mood tags from last.fm and allmusic.com. We explore how bi-directional LSTM and Transformer encoder architectures can learn relationships between symbolic melodies, chord progressions, tempo, and expressed emotions, with and without a transfer learning strategy leveraging symbolic music data without emotion labels. Three emotion annotation summarisation methods based on the Arousal/Valence (AV) representation are compared: Emotion Average, Emotion Surface, and Emotion Category. 20 participants (average age: 30.2, 7 females and 13 males from Japan) rated how well generated accompaniments matched melodies (musical coherence) as well as perceived emotions for 75 arrangements corresponding to combinations of models and emotion summarisation methods. Musical coherence and match between target and perceived emotions were highest when melodies were encoded using a BLSTM model with transfer learning. The proposed method generates emotion-driven harmonic/tempo arrangements in a fast way, a keen advantage compared to state of the art. Applications of this work include AI-based composition assistant and live interactive music systems for entertainment such as video games."
Yigitcan Özer;Matej Ištvánek;Vlora Arifi-Müller;Meinard Müller,Using Activation Functions for Improving Measure-Level Audio Synchronization,2022,https://doi.org/10.5281/zenodo.7342932,"Yigitcan Özer, International Audio Laboratories Erlangen, Germany;Matˇej Ištvánek, Brno University of Technology, Brno, Czech Republic;Vlora Ariﬁ-Müller, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Audio synchronization aims at aligning multiple recordings of the same piece of music. Traditional synchronization approaches are often based on dynamic time warping using chroma features as an input representation. Previous work has shown how one can integrate onset cues into this pipeline for improving the alignment's temporal accuracy. Furthermore, recent work based on deep neural networks has led to significant improvements for learning onset, beat, and downbeat activation functions. However, for music with soft onsets and abrupt tempo changes, these functions may be unreliable, leading to unstable results. As the main contribution of this paper, we introduce a combined approach that integrates activation functions into the synchronization pipeline. We show that this approach improves the temporal accuracy thanks to the activation cues while inheriting the robustness of the traditional synchronization approach. Conducting experiments based on string quartet recordings, we evaluate our combined approach where we transfer measure annotations from a reference recording to a target recording."
Katerina Kosta;Wei Tsung Lu;Gabriele Medeot;Pierre Chanquion,A deep learning method for melody extraction from a polyphonic symbolic music representation,2022,https://doi.org/10.5281/zenodo.7402960,"Katerina Kosta, ByteDance;Wei Tsung Lu, ByteDance;Gabriele Medeot, ByteDance;Pierre Chanquion, ByteDance","The task of identifying melodic lines in a polyphonic piece is a known active research topic in the symbolic and audio domain. Its importance has attracted the interest of researchers focusing on Music Information Retrieval and musicological applications and achieving high results is a common goal in industrial applications. The distinction of a melody in a written score can be a challenging task, however improvements have been reported in recent years using deep learning methods. In this paper, we present a lightweight deep bidirectional LSTM model for identifying the most salient melodic line of a music piece using handcrafted features without requiring the input score to be separated into multiple parts. We evaluate our model to measure the effectiveness of several data augmentation techniques and to compare performance to other state-of-the-art models. We also identify the features importance and evaluate their incremental contribution on the model performance using evaluation metrics. Results on the POP909 dataset show that our model approximates or outperforms current state of the art models trained on the same dataset, based on different implemented metrics and observations."
Peter Knees;Bruce Ferwerda;Andreas Rauber;Sebastian Strumbelj;Annabel Resch;Laurenz Tomandl;Valentin Bauer;Fung Yee Tang;Josip Bobinac;Amila Ceranic;Riad Dizdar,A Reproducibility Study on User-centric MIR Research and Why it is Important,2022,https://doi.org/10.5281/zenodo.7316776,"Peter Knees, Faculty of Informatics, TU Wien, Austria;Bruce Ferwerda, Department of Computer Science and Informatics, Jönköping University, Sweden;Andreas Rauber, Faculty of Informatics, TU Wien, Austria;Sebastian Strumbelj, Faculty of Informatics, TU Wien, Austria;Annabel Resch, Faculty of Informatics, TU Wien, Austria;Laurenz Tomandl, Faculty of Informatics, TU Wien, Austria;Valentin Bauer, Faculty of Informatics, TU Wien, Austria;Fung Yee Tang, Faculty of Informatics, TU Wien, Austria;Josip Bobinac, Faculty of Informatics, TU Wien, Austria;Amila Ceranic, Faculty of Informatics, TU Wien, Austria;Riad Dizdar, Faculty of Informatics, TU Wien, Austria","Reproducibility of results is a central pillar of scientific work. In music information retrieval research, this is widely acknowledged and practiced by the community by re-implementing algorithms and re-validating machine learning experiments. In this paper, we argue for an increased need to also reproduce the results and findings of user studies, including qualitative work, especially since these often lay the foundations and serve as justification for choices taken in algorithmic design and optimization criteria. As an example, we attempt to reproduce the study by Kim et al. presented in the RecSys (2020) paper ''Do Channels Matter? Illuminating Interpersonal Influence on Music Recommendations.'' By repeating this study on how interpersonal relationships can affect a user's assessment of music recommendations on a new sample of n=142 participants, we can largely confirm and support the validity of the original results. At the same time, we extend the analysis and also observe differences with regards to adoption rates between different channels as well as different factors that influences the adoption rate. From this specific reproducibility study, we conclude that potential cultural differences should be accounted for more explicitly in future studies and that systems development should be more explicitly connected to its intended target audience."
Noah Schaffer;Boaz Cogan;Ethan Manilow;Max Morrison;Prem Seetharaman;Bryan Pardo,Music Separation Enhancement with Generative Modeling,2022,https://doi.org/10.5281/zenodo.7316778,"Noah Schaffer, Interactive Audio Lab, Northwestern University, Evanston IL, USA;Boaz Cogan, Interactive Audio Lab, Northwestern University, Evanston IL, USA;Ethan Manilow, Interactive Audio Lab, Northwestern University, Evanston IL, USA;Max Morrison, Interactive Audio Lab, Northwestern University, Evanston IL, USA;Prem Seetharaman, Descript, Inc.;Bryan Pardo, Interactive Audio Lab, Northwestern University, Evanston IL, USA","Despite phenomenal progress in recent years, state-of-the-art music separation systems produce source estimates with significant perceptual shortcomings, such as adding extraneous noise or removing harmonics. We propose a post-processing model (the Make it Sound Good (MSG) post-processor) to enhance the output of music source separation systems. We apply our post-processing model to state-of-the-art waveform-based and spectrogram-based music source separators, including a separator unseen by MSG during training. Our analysis of the errors produced by source separators shows that waveform models tend to introduce more high-frequency noise, while spectrogram models tend to lose transients and high frequency content. We introduce objective measures to quantify both kinds of errors and show MSG improves the source reconstruction of both kinds of errors. Crowdsourced subjective evaluations demonstrate that human listeners prefer source estimates of bass and drums that have been post-processed by MSG."
Stefan Lattner,SampleMatch: Drum Sample Retrieval by Musical Context,2022,https://doi.org/10.5281/zenodo.7316780,"Stefan Lattner, Sony Computer Science Laboratories (CSL), Paris, France","Modern digital music production typically involves combining numerous acoustic elements to compile a piece of music. Important types of such elements are drum samples, which determine the characteristics of the percussive components of the piece. Artists must use their aesthetic judgement to assess whether a given drum sample fits the current musical context. However, selecting drum samples from a potentially large library is tedious and may interrupt the creative flow. In this work, we explore the automatic drum sample retrieval based on aesthetic principles learned from data. As a result, artists can rank the samples in their library by fit to some musical context at different stages of the production process (i.e., by fit to incomplete song mixtures). To this end, we use contrastive learning to maximize the score of drum samples originating from the same song as the mixture. We conduct a listening test to determine whether the human ratings match the automatic scoring function. We also perform objective quantitative analyses to evaluate the efficacy of our approach."
Timothy De Reuse;Ichiro Fujinaga,"A Transformer-Based ""Spellchecker"" for Detecting Errors in OMR Output",2022,https://doi.org/10.5281/zenodo.7316782,"Timothy de Reuse, Centre for Interdisciplinary Research in Music Media and Technology, McGill University;Ichiro Fujinaga, Centre for Interdisciplinary Research in Music Media and Technology, McGill University","The outputs of Optical Music Recognition (OMR) systems require time-consuming human correction. Given that most of the errors induced by OMR processes appear non-musical to humans, we propose that the time to correct errors may be reduced by marking all symbols on a score that are musically unlikely, allowing the human to focus their attention accordingly. Using a dataset of Romantic string quartets, we train a variant of the Transformer network architecture on the task of classifying each symbol of an optically-recognized musical piece in symbolic format as correct or erroneous, based on whether a manual correction of the piece would require an insertion, deletion, or replacement of a symbol at that location. Since we have a limited amount of data with real OMR errors, we employ extensive data augmentation to add errors into training data in a way that mimics how OMR would modify the score. Our best-performing models achieve 99% recall and 50% precision on this error-detection task."
Vjosa Preniqi;Kyriaki Kalimeri;Charalampos Saitis,"""More than words"": Linking Music Preferences and Moral Values through Lyrics",2022,https://doi.org/10.5281/zenodo.7343071,"Vjosa Preniqi1, Centre for Digital Music, Queen Mary University of London, London UK;Kyriaki Kalimeri2, ISI Foundation, Turin, Italy;Charalampos Saitis1, Centre for Digital Music, Queen Mary University of London, London UK","This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song's overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression ap- proaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (.20 ≤ r ≤ .30) than values of empathy and equality (.08 ≤ r ≤ .11), while basic demographic variables only account for a small part in the models' explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements."
Jui-Te Wu;Jun-You Wang;Jyh-Shing Roger Jang;Li Su,A unified model for zero-shot singing voice conversion and synthesis,2022,https://doi.org/10.5281/zenodo.7316786,"Jui-Te Wu, NTU-AS Data Science Degree Program, National Taiwan University, Taiwan;Jun-You Wang, Department of Computer Science and Information Engineering, National Taiwan University, Taiwan;Jyh-Shing Roger Jang, NTU-AS Data Science Degree Program, National Taiwan University, Taiwan;Li Su, Institute of Information Science, Academia Sinica, Taiwan","Recent advances in deep learning not only facilitate the implementation of zero-shot singing voice synthesis (SVS) and singing voice conversion (SVC) tasks but also provide the opportunity to unify these two tasks into one generalized model. In this paper, we propose such a model that generate the singing voice of any target singer from any source singing content in either text or audio format. The model incorporates self-supervised joint training of the phonetic encoder and the acoustic encoder, with an audio-to-phoneme alignment process in each training step, such that these encoders map the audio and text data respectively into a shared, temporally aligned, and singer agnostic latent space. The target singer's latent representations encoded at different granularity levels are all trained to match the source latent representations sequentially with the attention mechanisms in the decoding stage. This enables the model to generate unseen target singer's voice with fine-grained resolution from either text or audio sources. Both objective and subjective experiments confirmed that the proposed model is competitive with the state-of-the-art SVC and SVS methods."
Stewart Greenhill;Majid Abdolshah;Vuong Le;Sunil Gupta;Svetha Venkatesh,Semantic Control of Generative Musical Attributes,2022,https://doi.org/10.5281/zenodo.7316788,"Stewart Greenhill, Applied Artiﬁcial Intelligence Institute, Deakin University, Australia;Majid Abdolshah, Applied Artiﬁcial Intelligence Institute, Deakin University, Australia;Vuong Le, Applied Artiﬁcial Intelligence Institute, Deakin University, Australia;Sunil Gupta, Applied Artiﬁcial Intelligence Institute, Deakin University, Australia;Svetha Venkatesh, Applied Artiﬁcial Intelligence Institute, Deakin University, Australia","Deep generative neural networks have been successful in tasks such as composing novel music and rendering expressive performance. Controllability is essential for building creative tools from such models. Recent work in this area has focused on disentangled latent space representations, but this is only part of the solution. Efficient control of semantic attributes must handle non-linearities and holes that occur in latent spaces, whilst minimising unwanted changes to other attributes. This paper introduces SeNT-Gen, a neural traversal algorithm that uses a secondary neural network to model the complex relationships between latent codes and musical attributes. This enables precise editing of semantic attributes that adapts to context. We demonstrate the method using the dMelodies dataset, and show strong performance for several VAE models."
Pablo Alonso-Jiménez;Xavier Serra;Dmitry Bogdanov,Music Representation Learning Based on Editorial Metadata from Discogs,2022,https://doi.org/10.5281/zenodo.7316790,"Pablo Alonso-Jiménez, Music Technology Group, Universitat Pompeu Fabra;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra;Dmitry Bogdanov, Music Technology Group, Universitat Pompeu Fabra","This paper revisits the idea of music representation learning supervised by editorial metadata, contributing to the state of the art in two ways. First, we exploit the public editorial metadata available on Discogs, an extensive community-maintained music database containing information about artists, releases, and record labels. Second, we use a contrastive learning setup based on COLA, different from previous systems based on triplet loss. We train models targeting several associations derived from the metadata and experiment with stacked combinations of learned representations, evaluating them on standard music classification tasks. Additionally, we consider learning all the associations jointly in a multi-task setup. We show that it is possible to improve the performance of current self-supervised models by using inexpensive metadata commonly available in music collections, producing representations comparable to those learned on classification setups. We find that the resulting representations based on editorial metadata outperform a system trained with music style tags available in the same large-scale dataset, which motivates further research using this type of supervision. Additionally, we give insights on how to preprocess Discogs metadata to build training objectives and provide public pre-trained models."
Chih-Pin Tan;Alvin W Y Su;Yi-Hsuan Yang,Melody Infilling with User-Provided Structural Context,2022,https://doi.org/10.5281/zenodo.7316792,"Chih-Pin Tan, National Cheng Kung University;Alvin W.Y. Su, National Cheng Kung University;Yi-Hsuan Yang, Academia Sinica","This paper proposes a novel Transformer-based model for music score infilling, to generate a music passage that fills in the gap between given past and future contexts. While existing infilling approaches can generate a passage that connects smoothly locally with the given contexts, they do not take into account the musical form or structure of the music and may therefore generate overly smooth results. To address this issue, we propose a structure-aware conditioning approach that employs a novel attention-selecting module to supply user-provided structure-related information to the Transformer for infilling. With both objective and subjective evaluations, we show that the proposed model can harness the structural information effectively and generate melodies in the style of pop of higher quality than the two existing structure-agnostic infilling models."
Xichu Ma;Xiao Liu;Bowen Zhang;Ye Wang,Robust Melody Track Identification in Symbolic Music,2022,https://doi.org/10.5281/zenodo.7342531,"Xichu Ma, National University of Singapore;Xiao Liu, National University of Singapore;Bowen Zhang, National University of Singapore;Ye Wang, National University of Singapore","<p>Melody tracks are worthy of special attention in the field of symbolic music information retrieval (MIR) because they contribute more towards music perception than many other musical components. However, many existing symbolic MIR systems neglect melody track identification (MTI) and are thus less effective. Existing MTI methods are also not robust and perform poorly on MIDI files representing music of unusual genres, arrangements, or formats. To address this problem, we propose a CNN-Transformer-based MTI model designed to robustly identify a single melody track for a given MIDI file. As this process can take a sizable amount of time for long songs, we also use a sparse Transformer to speed up attention computation. Our experiments show that our proposed model outperforms state-of-the-art (SOTA) algorithms in accuracy and can also benefit downstream MIR tasks.</p>"
Florian Thalmann;Eita Nakamura;Kazuyoshi Yoshii,Tracking the Evolution of a Band's Live Performances over Decades,2022,https://doi.org/10.5281/zenodo.7342596,"Florian Thalmann, Graduate School of Informatics, Kyoto University, Japan;Eita Nakamura, Graduate School of Informatics, Kyoto University, Japan;Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, Japan","Evolutionary studies have become a dominant thread in the analysis of large audio collections. Such corpora usually consist of musical pieces by various composers or bands and the studies usually focus on identifying general historical trends in harmonic content or music production techniques. In this paper we present a comparable study that examines the music of a single band whose publicly available live recordings span three decades. We first discuss the opportunities and challenges faced when working with single-artist and live-music datasets and introduce solutions for audio feature validation and outlier detection. We then investigate how individual songs vary over time and identify general performance trends using a new approach based on relative feature values, which improves accuracy for features with a large variance. Finally, we validate our findings by juxtaposing them with descriptions posted in online forums by experienced listeners of the band's large following."
Ashvala Vinay;Alexander Lerch,Evaluating Generative Audio Systems and Their Metrics,2022,https://doi.org/10.5281/zenodo.7343083,"Ashvala Vinay, Center for Music Technology, Georgia Institute of Technology;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology","Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems."
Alison B Ma;Alexander Lerch,Representation Learning for the Automatic Indexing of Sound Effects Libraries,2022,https://doi.org/10.5281/zenodo.7316800,"Alison B. Ma, Music Informatics Group, Georgia Institute of Technology;Alexander Lerch, Music Informatics Group, Georgia Institute of Technology","Labeling and maintaining a commercial sound effects library is a time-consuming task exacerbated by databases that continually grow in size and undergo taxonomy updates. Moreover, sound search and taxonomy creation are complicated by non-uniform metadata, an unrelenting problem even with the introduction of a new industry standard, the Universal Category System. To address these problems and overcome dataset-dependent limitations that inhibit the successful training of deep learning models, we pursue representation learning to train generalized embeddings that can be used for a wide variety of sound effects libraries and are a taxonomy-agnostic representation of sound. We show that a task-specific but dataset-independent representation can successfully address data issues such as class imbalance, inconsistent class labels, and insufficient dataset size, outperforming established representations such as OpenL3. Detailed experimental results show the impact of metric learning approaches and different cross-dataset training methods on representational effectiveness."
Francesco Foscarin;Katharina Hoedt;Verena Praher;Arthur Flexer;Gerhard Widmer,"Concept-Based Techniques for ""Musicologist-Friendly"" Explanations in Deep Music Classifiers",2022,https://doi.org/10.5281/zenodo.7316804,"Francesco Foscarin, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Katharina Hoedt, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Verena Praher, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Arthur Flexer, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria","Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations."
Maximilian Mayerl;Stefan Brandl;Günther Specht;Markus Schedl;Eva Zangerle,Verse versus Chorus: Structure-aware Feature Extraction for Lyrics-based Genre Recognition,2022,https://doi.org/10.5281/zenodo.7316806,"Maximilian Mayerl, Department of Computer Science, Leopold-Franzens-Universität Innsbruck, Austria;Stefan Brandl, ;Günther Specht, Department of Computer Science, Leopold-Franzens-Universität Innsbruck, Austria;Markus Schedl, ;Eva Zangerle, Department of Computer Science, Leopold-Franzens-Universität Innsbruck, Austria","The aim of lyrics-based genre recognition is to automatically determine the genre of a given song based on its lyrics. Previous approaches for this task have commonly used textual features extracted from the entirety of a song's lyrics, neglecting the inherent structure of lyrics consisting of, for instance, verses and choruses. Therefore, we pose the hypothesis that features extracted from different parts of the lyrics can have significantly different predictive power. To test this hypothesis, we perform a series of experiments to determine whether models trained on features taken from verses and choruses perform differently for genre recognition. Our experiments indeed confirm our hypothesis, showing that generally, using features extracted from verses leads to higher performance than features extracted from choruses. Digging deeper, we found that this is especially true for pop and rap songs. Rock songs show the opposite effect, with features extracted from choruses performing better than those taken from verses."
Longshen Ou;Xiangming Gu;Ye Wang,Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription,2022,https://doi.org/10.5281/zenodo.7342796,"Longshen Ou, School of Computing, National University of Singapore;Xiangming Gu, School of Computing, National University of Singapore;Ye Wang, School of Computing, National University of Singapore","Automatic speech recognition (ASR) has progressed significantly in recent years due to the emergence of large-scale datasets and the self-supervised learning (SSL) paradigm. However, as its counterpart problem in the singing domain, the development of automatic lyric transcription (ALT) suffers from limited data and degraded intelligibility of sung lyrics. To fill in the performance gap between ALT and ASR, we attempt to exploit the similarities between speech and singing. In this work, we propose a transfer-learning-based ALT solution that takes advantage of these similarities by adapting wav2vec 2.0, an SSL ASR model, to the singing domain. We maximize the effectiveness of transfer learning by exploring the influence of different transfer starting points. We further enhance the performance by extending the original CTC model to a hybrid CTC/attention model. Our method surpasses previous approaches by a large margin on various ALT benchmark datasets. Further experiments show that, with even a tiny proportion of training data, our method still achieves competitive performance."
Daniel Szelogowski;Lopamudra Mukherjee;Benjamin Whitcomb,A Novel Dataset and Deep Learning Benchmark for Classical Music Form Recognition and Analysis,2022,https://doi.org/10.5281/zenodo.7316810,"Daniel Szelogowski, University of Wisconsin Ð Whitewater;Lopamudra Mukherjee, University of Wisconsin Ð Whitewater;Benjamin Whitcomb, University of Wisconsin Ð Whitewater","Automated computational analysis schemes for Western classical music analysis based on form and hierarchical structure have not received much attention in the literature so far. One reason, of course, is the paucity of labeled datasets — which, if available, could be used to train machine learning approaches. Dataset curation cannot be crowdsourced; one needs trained musicians to devote sizable effort to carry out such annotations. Further, such an analysis is not simple for beginners; obtaining labeled data that can capture the nuances of a musician's reasoning acquired over years of practice is fraught with challenges. To this end, we provide a system for computational analysis of classical music, both for machine learning and music researchers. First, we introduce a labeled dataset containing 200 classical music pieces annotated by form and phrases. Then, by leveraging this dataset, we show that deep learning-based methods can be used to learn Form Classification as well as Phrase Analysis and Classification, for which few (if any) results have been reported yet. Taken together, we provide the community with a unique dataset as well as a toolkit needed to analyze classical music structure, which can be used or extended to drive applications in both commercial and educational settings."
Guillem Cortès;Alex Ciurana;Emilio Molina;Marius Miron;Owen Meyers;Joren Six;Xavier Serra,BAF: An audio fingerprinting dataset for broadcast monitoring,2022,https://doi.org/10.5281/zenodo.7372162,"Guillem Cortès, BMAT Licensing S.L., Barcelona;Alex Ciurana, MTG, Universitat Pompeu Fabra, Barcelona;Emilio Molina, Epidemic Sound, Stockholm;Marius Miron, IPEM, Ghent University, Ghent;Owen Meyers, ;Joren Six, ;Xavier Serra, MTG, Universitat Pompeu Fabra, Barcelona","Audio Fingerprinting (AFP) is a well-studied problem in music information retrieval for various use-cases e.g. content-based copy detection, DJ-set monitoring, and music excerpt identification. However, AFP for continuous broadcast monitoring (e.g. for TV &amp; Radio), where music is often in the background, has not received much attention despite its importance to the music industry. In this paper (1) we present BAF, the first public dataset for music monitoring in broadcast. It contains 74 hours of production music from Epidemic Sound and 57 hours of TV audio recordings. Furthermore, BAF provides cross-annotations with exact matching timestamps between Epidemic tracks and TV recordings. Approximately, 80% of the total annotated time is background music. (2) We benchmark BAF with public state-of-the-art AFP systems, together with our proposed baseline PeakFP: a simple, non-scalable AFP algorithm based on spectral peak matching. In this benchmark, none of the algorithms obtain a F1-score above 47%, pointing out that further research is needed to reach the AFP performance levels in other studied use cases. The dataset, baseline, and benchmark framework are open and available for research."
Emmanouil Karystinaios;Gerhard Widmer,Cadence Detection in Symbolic Classical Music using Graph Neural Networks.,2022,https://doi.org/10.5281/zenodo.7316814,"Emmanouil Karystinaios, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria","Cadences are complex structures that have been driving music from the beginning of contrapuntal polyphony until today. Detecting such structures is vital for numerous MIR tasks such as musicological analysis, key detection, music segmentation, and others. However, automatic cadence detection remains a challenging task mainly because it involves a combination of high-level musical elements like harmony, voice leading, and rhythm. In this work, we present a graph representation of symbolic scores as an intermediate means to solve the cadence detection task. We approach cadence detection as an imbalanced node classification problem using a Graph Convolutional Network. We obtain results that are at least on par with the state of the art, and we present a model capable of making predictions at multiple levels of granularity, from individual notes to beats, thanks to the fine-grained, note-by-note representation. Moreover, our experiments suggest that graph convolution is able to learn non-local features that assist in cadence detection, freeing us from the need of having to devise specialized features that encode non-local context. We argue that this general approach to modeling musical scores and classification tasks has a number of potential advantages, beyond the specific recognition task presented here."
Jingwei Zhao;Gus Xia;Ye Wang,Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation,2022,https://doi.org/10.5281/zenodo.7316816,"Jingwei Zhao2,4, NUS Graduate School;Gus Xia3,5, MBZUAI;Ye Wang1,2,4, NUS","The variational auto-encoder has become a leading framework for symbolic music generation, and a popular research direction is to study how to effectively control the generation process. A straightforward way is to control a model using different conditions during inference. However, in music practice, conditions are usually sequential (rather than simple categorical labels), involving rich information that overlaps with the learned representation. Consequently, the decoder gets confused about whether to listen to the latent representation or the condition, and sometimes just ignores the condition. To solve this problem, we leverage domain adversarial training to disentangle the representation from condition cues for better control. Specifically, we propose a condition corruption objective that uses the representation to denoise a corrupted condition. Minimized by a discriminator and maximized by the VAE encoder, this objective adversarially induces a condition-invariant representation. In this paper, we focus on the task of melody harmonization to illustrate our idea, while our methodology can be generalized to other controllable generative tasks. Demos and experiments show that our methodology facilitates not only condition-invariant representation learning but also higher-quality controllability compared to baselines."
Yang Qu;Yutian Qin;Lecheng Chao;Hangkai Qian;Ziyu Wang;Gus Xia,Modeling perceptual loudness of piano tone: theory and applications,2022,https://doi.org/10.5281/zenodo.7343094,"Yang Qu, Music X Lab, NYU Shanghai, City University of Hong Kong;Yutian Qin, Music X Lab, NYU Shanghai, New York University;Lecheng Chao, Music X Lab, NYU Shanghai;Hangkai Qian, Music X Lab, NYU Shanghai;Ziyu Wang, Music X Lab, NYU Shanghai, Mohamed Bin Zayed University of Artificial Intelligence;Gus Xia, Music X Lab, NYU Shanghai, Mohamed Bin Zayed University of Artificial Intelligence","The relationship between perceptual loudness and physical attributes of sound is an important subject in both computer music and psychoacoustics. Early studies of ""equal-loudness contour"" can trace back to the 1920s and the measured loudness with respect to intensity and frequency has been revised many times since then. However, most studies merely focus on synthesized sound, and the induced theories on natural tones with complex timbre has rarely been justified. To this end, we investigate both theory and applications of natural-tone loudness perception in this paper via modeling piano tone. The theory part contains: 1) an accurate measurement of piano-tone equal-loudness contour of pitches, and 2) a machine-learning model capable of inferring loudness purely based on spectral features trained on human subject measurements. As for the application, we apply our theory to piano control transfer, in which we adjust the MIDI velocities on two different player pianos (in different acoustic environments) to achieve the same perceptual effect. Experiments show that both of our theoretical loudness modeling and the corresponding performance control transfer algorithm significantly outperform their baselines."
Maximilian Damböck;Richard Vogl;Peter Knees,On the Impact and Interplay of Input Representations and Network Architectures for Automatic Music Tagging,2022,https://doi.org/10.5281/zenodo.7343091,"Maximilian Damböck, Faculty of Informatics, TU Wien, Austria;Richard Vogl, Faculty of Informatics, TU Wien, Austria;Peter Knees, Faculty of Informatics, TU Wien, AT, School of Music, Georgia Tech, USA","Automatic music tagging systems have once more gained relevance over the last years, not least through their use in applications such as music recommender systems. State-of-the-art systems are based on a variant of convolutional neural networks (CNNs) and use some type of time-frequency audio representation as input, in a fitting combination to predict semantic tags available through expert or crowd-based annotation. In this work we systematically compare five widely used audio input representations (STFT, CQT, Mel spectrograms, MFCCs, and raw audio waveform) using five established convolutional neural network architectures (MusicCNN, VGG16, ResNet, a Squeeze and Excitation Network (SeNet), as well as a newly proposed MusicCNN variant using dilated convolutions) for the task of music tag prediction. Performance of all factor combinations are measured on two distinct tagging datasets, namely MagnaTagATune and MTG Jamendo. A two-way ANOVA shows that both input representation and model architecture significantly impact the classification results. Despite differently sized input representations and practical impact on model training, we find that using STFT as input representations provides the best results overall and on specific tag categories (genre, instrument, mood), while other representations show less consistent behavior in these regards. Furthermore, the proposed dilated convolutional architecture shows significant performance improvements for all input representations except raw waveform."
Shreyas Nadkarni;Sujoy Roychowdhury;Preeti Rao;Martin Clayton,Exploring the Correspondence of Melodic Contour With Gesture in Raga Alap Singing,2023,https://doi.org/10.5281/zenodo.10265213,"Shreyas Nadkarni, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Sujoy Roychowdhury, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Preeti Rao, Department of Electrical Engineering, Indian Institute of Technology Bombay, India;Martin Clayton, Department of Music, Durham University, United Kingdom","Musicology research suggests a correspondence between manual gesture and melodic contour in raga performance. Computational tools such as pose estimation from video and time series pattern matching potentially facilitate larger-scale studies of gesture and audio correspondence. We present a dataset of audiovisual recordings of Hindustani vocal music comprising 9 ragas sung by 11 expert performers. With the automatic segmentation of the audiovisual time series based on analyses of the extracted F0 contour, we study whether melodic similarity implies gesture similarity. Our results indicate that specific representations of gesture kinematics can predict high-level melodic features such as held notes and raga-characteristic motifs significantly better than chance."
Miguel Perez;Holger Kirchhoff;Xavier Serra,TriAD: Capturing Harmonics With 3D Convolutions,2023,https://doi.org/10.5281/zenodo.10265215,"Miguel Perez, Huawei, Munich Research Center;Holger Kirchhoff, ;Xavier Serra, MTG, Universitat Pompeu Fabra","Thanks to advancements in deep learning (DL), automatic music transcription (AMT) systems recently outperformed previous ones fully based on manual feature design. Many of these highly capable DL models, however, are computationally expensive. Researchers are moving towards smaller models capable of maintaining state-of-the-art (SOTA) results by embedding musical knowledge in the network architecture. Existing approaches employ convolutional blocks specifically designed to capture the harmonic structure. These approaches, however, require either large kernels or multiple kernels, with each kernel aiming to capture a different harmonic. We present TriAD, a convolutional block that achieves an unequally distanced dilation over the frequency axis. This allows our method to capture multiple harmonics with a single yet small kernel. We compare TriAD with other methods of capturing harmonics, and we observe that our approach maintains SOTA results while reducing the number of parameters required. We also conduct an ablation study showing that our proposed method effectively relies on harmonic information."
Fabio Morreale;Megha Sharma;I-Chieh Wei,Data Collection in Music Generation Training Sets: A Critical Analysis,2023,https://doi.org/10.5281/zenodo.10265217,"Fabio Morreale, University of Auckland;Megha Sharma, University of Tokyo;I-Chieh Wei, University of Auckland","The practices of data collection in training sets for Automatic Music Generation (AMG) tasks are opaque and overlooked. In this paper, we aimed to identify these practices and surface the values they embed. We systematically identified all datasets used to train AMG models presented at the last ten editions of ISMIR. For each dataset, we checked how it was populated and the extent to which musicians wittingly contributed to its creation.\ Almost half of the datasets (42.6%) were indiscriminately populated by accumulating music data available online without seeking any sort of permission. We discuss the ideologies that underlie this practice and propose a number of suggestions AMG dataset creators might follow. Overall, this paper contributes to the emerging  self-critical corpus of work of the ISMIR community, reflecting on the ethical considerations and the social responsibility of our work."
Bob L. T. Sturm;Arthur Flexer,A Review of Validity and Its Relationship to Music Information Research,2023,https://doi.org/10.5281/zenodo.10265219,"Bob L. T. Sturm, Division of Speech, Music and Hearing, KTH Stockholm, Sweden;Arthur Flexer, Institute of Computational Perception, Johannes Kepler University Linz, Austria","Validity is the truth of an inference made from evidence and is a central concern in scientific work. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has been so far. Puzzling MIR phenomena like adversarial attacks, horses, and performance glass ceilings become less mysterious through the lens of validity. In this paper, we review the subject of validity as presented in a key reference of causal inference: Shadish et al., Experimental and Quasi-experimental Designs for Generalised Causal Inference [1]. We discuss the four types of validity and threats to each one. We consider them in relationship to MIR experiments grounded with a practical demonstration using a typical MIR experiment."
Gowriprasad R;Srikrishnan Sridharan;R Aravind;Hema A. Murthy,Segmentation and Analysis of Taniavartanam in Carnatic Music Concerts,2023,https://doi.org/10.5281/zenodo.10265221,"Gowriprasad R, Indian Institute of Technology Madras;Srikrishnan S, Carnatic Percussionist;R Aravind, Indian Institute of Technology Madras;Hema A Murthy, Indian Institute of Technology Madras","In Carnatic music concerts, taniavartanam is a solo percussion segment that showcases intricate and elaborate extempore rhythmic evolution through a series of homogeneous sections with shared rhythmic characteristics. While taniavartanam segments have been segmented from concerts earlier, no effort has been made to analyze these percussion segments. This paper attempts to further segment the taniavartanam portion into musically meaningful segments. A taniavartanam segment consists of an abhipraya, where artists show their prowess at extempore enunciation of percussion stroke segments, followed by an optional korapu, where each artist challenges the other, and concluding with mohra and korvai, each with its own nuances. This work helps obtain a comprehensive musical description of the taniavartanam in Carnatic concerts. However, analysis is complicated owing to a plethora of tala and nade. The segmentation of a taniavartanam section can be used for further analysis, such as stroke sequence recognition, and help find relations between different learning schools. The study uses 12 hours of taniavartanam segments consisting of four tala-s and five nade-s for analysis and achieves 0.85 F1-score in the segmentation task."
Changhong Wang;Gaël Richard;Brian McFee,Transfer Learning and Bias Correction With Pre-Trained Audio Embeddings,2023,https://doi.org/10.5281/zenodo.10265223,"Changhong Wang, LTCI, Télécom Paris, Institut Polytechnique de Paris, France;Gaël Richard, LTCI, Télécom Paris, Institut Polytechnique de Paris, France;Brian McFee, Music and Audio Research Laboratory, New York University, USA","Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applications in MIR have sufficient quantities of training data, it is becoming increasingly common to transfer models across domains. This approach allows representations derived for one task to be applied to another, and can result in high accuracy with less stringent training data requirements for the downstream task. However, the properties of pre-trained audio embeddings are not fully understood. Specifically, and unlike traditionally engineered features, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition. We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets."
Michèle Duguay;Kate Mancey;Johanna Devaney,Collaborative Song Dataset (CoSoD): An Annotated Dataset of Multi-Artist Collaborations in Popular Music,2023,https://doi.org/10.5281/zenodo.10265225,"Michèle Duguay, Department of Music, Harvard University;Kate Mancey, Department of Music, Harvard University;Johanna Devaney, Brooklyn College and Graduate Center, City University of New York","The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010–2019 Billboard ""Hot 100"" year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, panning, and gender of the performers), and relevant metadata. CoSoD complements other popular music datasets by focusing exclusively on musical collaborations between independent acts. In addition to facilitating the study of song form and vocal production, CoSoD allows for the in-depth study of gender as it relates to various timbral, pitch, and formal parameters in musical collaborations. In this paper, we detail the contents of the dataset and outline the annotation process. We also present an experiment using CoSoD that examines how the use of reverberation, layering, and panning are related to the gender of the artist. In this experiment, we find that men's voices are on average treated with less reverberation and occupy a more narrow position in the stereo mix than women's voices."
Michele Newman;Lidia Morris;Jin Ha Lee,Human-AI Music Creation: Understanding the Perceptions and Experiences of Music Creators for Ethical and Productive Collaboration,2023,https://doi.org/10.5281/zenodo.10265227,"Michele Newman, University of Washington;Lidia Morris, University of Washington;Jin Ha Lee, University of Washington","Recently, there has been a surge in Artificial Intelligence (AI) tools that allow creators to develop melodies, harmonies, lyrics, and mixes with the touch of a button. The reception of and discussion on the use of these tools - and more broadly, any AI-based art creation tools - tend to be polarizing, with opinions ranging from enthusiasm about their potential to fear about how these tools will impact the livelihood and creativity of human creators. However, a more desirable future path is most likely somewhere in between these two polar opposites where productive and ethical human-AI collaboration could happen through the use of these tools. To explore this possibility, we first need to improve our understanding of how music creators perceive and utilize these types of tools in their creative process. We conducted case studies of a range of music creators to better understand their perception and usage of AI-based music creation tools. Through a thematic analysis of these cases, we identify the opportunities and challenges related to the use of AI for music creation from the perspective of the musicians and discuss the design implications for AI music tools."
Nathan Fradet;Nicolas Gutowski;Fabien Chhel;Jean-Pierre Briot,Impact of Time and Note Duration Tokenizations on Deep Learning Symbolic Music Modeling,2023,https://doi.org/10.5281/zenodo.10265229,"Nathan Fradet, Sorbonne University, CNRS, LIP6, F-75005 Paris;Nicolas Gutowski, University of Angers, LERIA, 49000 Angers, France;Fabien Chhel, University of Angers, LERIA, 49000 Angers, France;Jean-Pierre Briot, Sorbonne University, CNRS, LIP6, F-75005 Paris","Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways, and recent research has focused on developing more efficient methods. However, the key differences between these methods are often unclear, and few studies have compared them. In this work, we analyze the current common tokenization methods and experiment with time and note duration representations. We compare the performance of these two impactful criteria on several tasks, including composer classification, emotion classification, music generation, and sequence representation. We demonstrate that explicit information leads to better results depending on the task."
Max Johnson;Mark R. H. Gotham,Musical Micro-Timing for Live Coding,2023,https://doi.org/10.5281/zenodo.10265231,"Max Johnson, Department of Computer Science and Technology, University of Cambridge;Mark Gotham, Department of Computer Science, Durham University","Micro-timing is an essential part of human music-making, yet it is absent from most computer music systems. Partly to address this gap, we present a novel system for generating music with style-specific micro-timing within the Sonic Pi live coding language. We use a probabilistic approach to control the exact timing according to patterns discovered in new analyses of existing micro-timing data (jembe drumming and Viennese waltz). This implementation also required the introduction of musical metre into Sonic Pi. The new metre and micro-timing systems are inherently flexible, and thus open to a wide range of creative possibilities including (but not limited to): creating new micro-timing profiles for additional styles; expanded definitions of metre; and the free mixing of one micro-timing style with the musical content of another. The code is freely available as a Sonic Pi plug-in and released open source at https://github.com/MaxTheComputerer/sonicpi-metre."
Francisco J. Castellanos;Antonio Javier Gallego;Ichiro Fujinaga,A Few-Shot Neural Approach for Layout Analysis of Music Score Images,2023,https://doi.org/10.5281/zenodo.10265233,"Francisco J. Castellanos, University Institute for Computing Research, University of Alicante, Spain;Antonio Javier Gallego, University Institute for Computing Research, University of Alicante, Spain;Ichiro Fujinaga, Schulich School of Music, McGill University, Montreal, Canada","Optical Music Recognition (OMR) is a well-established research field focused on the task of reading musical notation from images of music scores. In the standard OMR workflow, layout analysis is a critical component for identifying relevant parts of the image, such as staff lines, text, or notes. State-of-the-art approaches to this task are based on machine learning, which entails having to label a training corpus, an error-prone, laborious, and expensive task that must be performed by experts. In this paper, we propose a novel few-shot strategy for building robust models by utilizing only partial annotations, therefore requiring minimal human effort. Specifically, we introduce a masking layer and an oversampling technique to train models using a small set of annotated patches from the training images. Our proposal enables achieving high performance even with scarce training data, as demonstrated by experiments on four benchmark datasets. The results indicate that this approach achieves performance values comparable to models trained with a fully annotated corpus, but, in this case, requiring the annotation of only between 20% and 39% of this data."
Behzad Haki;Błażej Kotowski;Cheuk Lun Isaac Lee;Sergi Jordà,TapTamDrum: A Dataset for Dualized Drum Patterns,2023,https://doi.org/10.5281/zenodo.10265237,"Behzad Haki, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Bła˙zej Kotowski, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Cheuk Lun Isaac Lee, ;Sergi Jordà, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Drummers spend extensive time practicing rudiments to develop technique, speed, coordination, and phrasing. These rudiments are often practiced on ""silent"" practice pads using only the hands. Additionally, many percussive instruments across cultures are played exclusively with the hands. Building on these concepts and inspired by Einstein's probably apocryphal quote, ""Make everything as simple as possible, but not simpler,"" we hypothesize that a dual-voice reduction could serve as a natural and meaningful compressed representation of multi-voiced drum patterns. This representation would retain more information than its corresponding monotonic representation while maintaining relative simplicity for tasks such as rhythm analysis and generation. To validate this potential representation, we investigate whether experienced drummers can consistently represent and reproduce the rhythmic essence of a given drum pattern using only their two hands. We present TapTamDrum: a novel dataset of repeated dualizations from four experienced drummers, along with preliminary analysis and tools for further exploration of the data."
Andrea Martelloni;Andrew P. McPherson;Mathieu Barthet,Real-Time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar,2023,https://doi.org/10.5281/zenodo.10265236,"Andrea Martelloni, Queen Mary University of London;Andrew P McPherson, Imperial College;Mathieu Barthet, Queen Mary University of London","Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distributions corresponding to different taxonomic classes. Results indicate that the networks are strong classifiers especially in a simplified 2-class recognition task, and the VAEs yield improved class separation compared to CNNs as evidenced by increased KL-Divergence across distributions. We argue that the VAE embedding quality could support control intimacy and rich interaction when the latent space's parameters are used to control an external synthesis engine. Further design challenges around generalisation to different datasets have been identified."
Hiromu Yakura;Masataka Goto,IteraTTA: An Interface for Exploring Both Text Prompts and Audio Priors in Generating Music With Text-to-Audio Models,2023,https://doi.org/10.5281/zenodo.10265239,"Hiromu Yakura, University of Tsukuba;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST)","Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness."
Mirco Pezzoli;Raffaele Malvermi;Fabio Antonacci;Augusto Sarti,Similarity Evaluation of Violin Directivity Patterns for Musical Instrument Retrieval,2023,https://doi.org/10.5281/zenodo.10265243,"Mirco Pezzoli, Dipartimento di Elettronica, Informazione e Bioingegneria - Politecnico di Milano, Milan, Italy;Raffaele Malvermi, Dipartimento di Elettronica, Informazione e Bioingegneria - Politecnico di Milano, Milan, Italy;Fabio Antonacci, Dipartimento di Elettronica, Informazione e Bioingegneria - Politecnico di Milano, Milan, Italy;Augusto Sarti, Dipartimento di Elettronica, Informazione e Bioingegneria - Politecnico di Milano, Milan, Italy","The directivity of a musical instrument is a function that describes the spatial characteristics of its sound radiation. The majority of the available literature focuses on measuring directivity patterns, with analysis mainly limited to visual inspections. Recently, some similarity metrics for directivity patterns have been introduced, yet their application has not being fully addressed. In this work, we introduce the problem of musical instrument retrieval based on the directivity pattern features. We aim to exploit the available similarity metrics for directivity patterns in order to determine distances between instruments. We apply the methodology to a data set of violin directivities, including historical and modern high-quality instruments. Results show that the methodology facilitates the comparison of musical instruments and the navigation of databases of directivity patterns."
George Sioros,Polyrhythmic Modelling of Non-Isochronous and Microtiming Patterns,2023,https://doi.org/10.5281/zenodo.10265245,"George Sioros, University of Plymouth","Computational models and analyses of musical rhythms are predominantly based on the subdivision of durations down to a common isochronous pulse, which plays a fundamental structural role in the organization of their durational patterns. Meter, the most widespread example of such a temporal scheme, consists of several hierarchically organized pulses. Deviations from isochrony found in musical patterns are considered to form an expressive, micro level of organization that is distinct from the structural macro-organization of the basic pulse. However, polyrhythmic structures, such as those found in music from West Africa or the African diaspora, challenge both the hierarchical subdivision of durations and the structural isochrony of the above models. Here we present a model that integrates the macro- and micro-organization of rhythms by generating non-isochronous girds from isochronous pulses within a polyrhythmic structure. Observed micro-timing patterns may then be generated from structural non-isochronous grids, rather than being understood as expressive deviations from isochrony. We examine the basic mathematical properties of the model and show that meter can be generated as a special case. Finally, we demonstrate the model in the analysis of micro-timing patterns observed in Brazilian samba performances."
Shangda Wu;Dingyao Yu;Xu Tan;Maosong Sun,CLaMP: Contrastive Language-Music Pre-Training for Cross-Modal Symbolic Music Information Retrieval,2023,https://doi.org/10.5281/zenodo.10265247,"Shangda Wu, Central Conservatory of Music, China;Dingyao Yu, Microsoft Research Asia;Xu Tan, Microsoft Research Asia;Maosong Sun, Central Conservatory of Music, China, Tsinghua University, China","We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description. In comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP demonstrated comparable or superior performance on score-oriented datasets. Our models and code are available at https://github.com/microsoft/muzic/tree/main/clamp."
Luca Marinelli;György Fazekas;Charalampos Saitis,Gender-Coded Sound: Analysing the Gendering of Music in Toy Commercials via Multi-Task Learning,2023,https://doi.org/10.5281/zenodo.10265249,"Luca Marinelli, C4DM, Queen Mary University of London, UK;György Fazekas, C4DM, Queen Mary University of London, UK;Charalampos Saitis, C4DM, Queen Mary University of London, UK","Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from examining music in isolation, while studies of the gendering of music within multimodal communicative events are sparse. In this paper, we outline a method to automatically analyse how music in TV advertising aimed at children may be deliberately used to reinforce traditional gender roles. Our dataset of 606 commercials included music-focused mid-level perceptual features, multimodal aesthetic emotions, and content analytical items. Despite its limited size, and because of the extreme gender polarisation inherent in toy advertisements, we obtained noteworthy results by leveraging multi-task transfer learning on our densely annotated dataset. The models were trained to categorise commercials based on their intended target audience, specifically distinguishing between masculine, feminine, and mixed audiences. Additionally, to provide explainability for the classification in gender targets, the models were jointly trained to perform regressions on emotion ratings across six scales, and on mid-level musical perceptual attributes across twelve scales. Standing in the context of MIR, computational social studies and critical analysis, this study may benefit not only music scholars but also advertisers, policymakers, and broadcasters."
Li-Yang Tseng;Tzu-Ling Lin;Hong-Han Shuai;Jen-Wei Huang;Wen-Whei Chang,A Dataset and Baselines for Measuring and Predicting the Music Piece Memorability,2023,https://doi.org/10.5281/zenodo.10265251,"Li-Yang Tseng, National Yang Ming Chiao Tung University, Hsinchu, Taiwan;Tzu-Ling Lin, National Yang Ming Chiao Tung University, Hsinchu, Taiwan;Hong-Han Shuai, National Yang Ming Chiao Tung University, Hsinchu, Taiwan;Jen-Wei Huang, National Yang Ming Chiao Tung University, Hsinchu, Taiwan;Wen-Whei Chang, National Yang Ming Chiao Tung University, Hsinchu, Taiwan","Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspired by this phenomenon, we focus on measuring and predicting music memorability. To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure. We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs. To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods. Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible. Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music. As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research."
Carlos Peñarrubia;Carlos Garrido-Munoz;Jose J. Valero-Mas;Jorge Calvo-Zaragoza,Efficient Notation Assembly in Optical Music Recognition,2023,https://doi.org/10.5281/zenodo.10265253,"Carlos Penarrubia, U. I. for Computing Research, University of Alicante, Spain;Carlos Garrido-Munoz, U. I. for Computing Research, University of Alicante, Spain;Jose J. Valero-Mas, Music Technology Group, Universitat Pompeu Fabra, Spain;Jorge Calvo-Zaragoza, U. I. for Computing Research, University of Alicante, Spain","Optical Music Recognition (OMR) is the field of research that studies how to computationally read music notation from written documents. Thanks to recent advances in computer vision and deep learning, there are successful approaches that can locate the music-notation elements from a given music score image. Once detected, these elements must be related to each other to reconstruct the musical notation itself, in the so-called notation assembly stage. However, despite its relevance in the eventual success of the OMR, this stage has been barely addressed in the literature. This work presents a set of neural approaches to perform this assembly stage. Taking into account the number of possible syntactic relationships in a music score, we give special importance to the efficiency of the process in order to obtain useful models in practice. Our experiments, using the MUSCIMA++ handwritten sheet music dataset, show that the considered approaches are capable of outperforming the existing state of the art in terms of efficiency with limited (or no) performance degradation. We believe that the conclusions of this work provide novel insights into the notation assembly step, while indicating clues on how to approach the previous stages of the OMR and improve the overall performance."
Yuting Yang;Zeyu Jin;Connelly Barnes;Adam Finkelstein,White Box Search Over Audio Synthesizer Parameters,2023,https://doi.org/10.5281/zenodo.10265255,"Yuting Yang, Princeton University;Zeyu Jin, Adobe Research;Connelly Barnes, Adobe Research;Adam Finkelstein, Princeton University","Synthesizer parameter inference searches for a set of patch connections and parameters to generate audio that best matches a given target sound. Such optimization tasks benefit from access to accurate gradients. However, typical audio synths incorporate components with discontinuities – such as sawtooth or square waveforms, or a categorical search over discrete parameters like a choice among such waveforms – that thwart conventional automatic differentiation (AD). AD libraries in frameworks like TensorFlow and PyTorch typically ignore discontinuities, providing incorrect gradients at such locations. Thus, SOTA parameter inference methods avoid differentiating the synth directly, and resort to workarounds such as genetic search or neural proxies. Instead, we adapt and extend recent computer graphics methods for differentiable rendering to directly differentiate the synth as a white box program, and thereby optimize its parameters using gradient descent. We evaluate our framework using a generic FM synth with ADSR, noise, and IIR filters, adapting its parameters to match a variety of target audio clips. Our method outperforms baselines in both quantitative and qualitative evaluations."
Vincent K. M. Cheung;Lana Okuma;Kazuhisa Shibata;Kosetsu Tsukuda;Masataka Goto;Shinichi Furuya,"Decoding Drums, Instrumentals, Vocals, and Mixed Sources in Music Using Human Brain Activity With fMRI",2023,https://doi.org/10.5281/zenodo.10265257,"Vincent K.M. Cheung, Sony Computer Science Laboratories, Tokyo, Japan;Lana Okuma, RIKEN Center for Brain Science, Japan;Kazuhisa Shibata, RIKEN Center for Brain Science, Japan;Kosetsu Tsukuda, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Shinichi Furuya, Sony Computer Science Laboratories, Tokyo, Japan","Brain decoding allows the read-out of stimulus and mental content from neural activity, and has been utilised in various neural-driven classification tasks related to the music information retrieval community. However, even the relatively simple task of instrument classification has only been demonstrated for single- or few-note stimuli when decoding from neural data recorded using functional magnetic resonance imaging (fMRI). Here, we show that drums, instrumentals, vocals, and mixed sources of naturalistic musical stimuli can be decoded from single-trial spatial patterns of auditory cortex activation as recorded using fMRI. Comparing classification based on convolutional neural networks (CNN), random forests (RF), and support vector machines (SVM) further revealed similar neural encoding of vocals and mixed sources, despite vocals being most easily identifiable. These results highlight the prominence of vocal information during music perception, and illustrate the potential of using neural representations towards evaluating music source separation performance and informing future algorithm design."
Liyue Zhang;Xinyu Yang;Yichi Zhang;Jing Luo,Dual Attention-Based Multi-Scale Feature Fusion Approach for Dynamic Music Emotion Recognition,2023,https://doi.org/10.5281/zenodo.10265259,"Liyue Zhang, The School of Software, Xi’an Jiaotong University , China;Xinyu Yang, The School of Computer Science and Technology, Xi’an Jiaotong University , China;Yichi Zhang, The School of Computer Science and Technology, Xi’an Jiaotong University , China;Jing Luo, The School of Computer Science and Technology, Xi’an Jiaotong University , China","Music Emotion Recognition (MER) refers to automatically extracting emotional information from music and predicting its perceived emotions, and it has social and psychological applications. This paper proposes a Dual Attention-based Multi-scale Feature Fusion (DAMFF) method and a newly developed dataset named MER1101 for Dynamic Music Emotion Recognition (DMER). Specifically, multi-scale features are first extracted from the log Mel-spectrogram by multiple parallel convolutional blocks. Then, a Dual Attention Feature Fusion (DAFF) module is utilized to achieve multi-scale context fusion and capture emotion-critical features in both spatial and channel dimensions. Finally, a BiLSTM-based sequence learning model is employed for dynamic music emotion prediction. To enrich existing music emotion datasets, we developed a high-quality dataset, MER1101, which has a balanced emotional distribution, covering over 10 genres, at least four languages, and more than a thousand song snippets. We demonstrate the effectiveness of our proposed DAMFF approach on both the developed MER1101 dataset, as well as on the established DEAM2015 dataset. Compared with other models, our model achieves a higher Consistency Correlation Coefficient (CCC), and has strong predictive power in arousal with comparable results in valence."
Keisuke Toyama;Taketo Akama;Yukara Ikemiya;Yuhta Takida;Wei-Hsiang Liao;Yuki Mitsufuji,Automatic Piano Transcription With Hierarchical Frequency-Time Transformer,2023,https://doi.org/10.5281/zenodo.10265261,"Keisuke Toyama, Sony Group Corporation, Japan;Taketo Akama, Sony Computer Science Laboratories, Japan;Yukara Ikemiya, Sony Group Corporation, Japan;Yuhta Takida, Sony Group Corporation, Japan;Wei-Hsiang Liao, Sony Group Corporation, Japan;Yuki Mitsufuji, Sony Group Corporation, Japan","Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations."
Nazif Can Tamer;Yigitcan Özer;Meinard Müller;Xavier Serra,High-Resolution Violin Transcription Using Weak Labels,2023,https://doi.org/10.5281/zenodo.10265263,"Nazif Can Tamer, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Yigitcan Özer, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","A descriptive transcription of a violin performance requires detecting not only the notes but also the fine-grained pitch variations, such as vibrato. Most existing deep learning methods for music transcription do not capture these variations and often need frame-level annotations, which are scarce for the violin. In this paper, we propose a novel method for high-resolution violin transcription that can leverage piece-level weak labels for training. Our conformer-based model works on the raw audio waveform and transcribes violin notes and their corresponding pitch deviations with 5.8 ms frame resolution and 10-cent frequency resolution. We demonstrate that our method (1) outperforms generic systems in the proxy tasks of violin transcription and pitch estimation, and (2) can automatically generate new training labels by aligning its feature representations with unseen scores. We share our model along with 34 hours of score-aligned solo violin performance dataset, notably including the 24 Paganini Caprices."
Lejun Min;Junyan Jiang;Gus Xia;Jingwei Zhao,Polyffusion: A Diffusion Model for Polyphonic Score Generation With Internal and External Controls,2023,https://doi.org/10.5281/zenodo.10265265,"Lejun Min, Music X Lab, Computer Science Department, NYU Shanghai;Junyan Jiang, Music X Lab, Computer Science Department, NYU Shanghai;Gus Xia, Music X Lab, Computer Science Department, NYU Shanghai;Jingwei Zhao, Institute of Data Science, NUS","We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls."
Claire Arthur;Nathaniel Condit-Schultz,The Coordinated Corpus of Popular Musics (CoCoPops): A Meta-Corpus of Melodic and Harmonic Transcriptions,2023,https://doi.org/10.5281/zenodo.10265267,"Claire Arthur, Georgia Institute of Technology School of Music;Nathaniel Condit-Schultz, Georgia Institute of Technology School of Music","This paper introduces a new corpus, CoCoPops: The Coordinated Corpus of Popular Musics. The corpus can be considered a ""meta corpus"" in that it both extends and combines two existing corpora—the widely-used McGill Bill- board corpus the and RS200 corpus. Both the McGill Billboard corpus and the RS200 contain expert harmonic annotations using different encoding schemes and each represent harmony in fundamentally different ways: Billboard using a root-quality representation and the RS200 using Roman numerals. By combining these corpora into a unified format, using the well-known **kern and**harm representations, we aim to facilitate research in computational musicology, which is frequently burdened by corpora spread across multiple encoding formats. The format will also facilitate cross-corpus comparison with the large body of existing works in **kern format. For a 100-song subset of the CoCoPops-Billboard collection, we also provide participant ratings of continuous valence and arousal ratings, along with the RMS (Root Mean Square) signal level and associated timestamps. In this paper we describe the corpus and the procedures used to create it."
Anja Volk;Tinka Veldhuis;Katrien Foubert;Jos De Backer,Towards Computational Music Analysis for Music Therapy,2023,https://doi.org/10.5281/zenodo.10265269,"Anja Volk, Department of Information and Computing Sciences, Utrecht University, the Netherlands;Tinka Veldhuis, Department of Information and Computing Sciences, Utrecht University, the Netherlands;Katrien Foubert, Faculty of Medicine, KU Leuven, LUCA School of Arts, Belgium;Jos de Backer, Faculty of Medicine, KU Leuven, LUCA School of Arts, Belgium","The research field of music therapy has witnessed a rising interest in recent years to develop and employ computational methods to support therapists in their daily practice. While Music Information Retrieval (MIR) research has identified the area of health and well-being as a promising application field for MIR methods to support health professionals, collaborations with experts in this field are as of today sparse. This paper provides an overview of potential applications of computational music analysis as developed in MIR for the field of active music therapy. We elaborate on the music therapy method of improvisation, with a particular focus on introducing therapeutic concepts that relate to musical structures. We identify application scenarios for analysing musical structures in improvisations, introduce existing analysis methods of therapists, and discuss the potential of MIR methods to support these analyses. Upon identifying a current gap between high-level concepts of therapists and low-level features from existing computational methods, the paper concludes further steps towards developing computational approaches to music analysis for music therapy in an interdisciplinary collaboration."
Luca Comanducci;Fabio Antonacci;Augusto Sarti,Timbre Transfer Using Image-to-Image Denoising Diffusion Implicit Models,2023,https://doi.org/10.5281/zenodo.10265271,"Luca Comanducci, Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy;Fabio Antonacci, Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy;Augusto Sarti, Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy","Timbre transfer techniques aim at converting the sound of a musical piece generated by one instrument into the same one as if it was played by another instrument, while maintaining as much as possible the content in terms of musical characteristics such as melody and dynamics. Following their recent breakthroughs in deep learning-based generation, we apply Denoising Diffusion Models (DDMs) to perform timbre transfer. Specifically, we apply the recently proposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate the sampling procedure.  Inspired by the recent application of DDMs to image translation problems we formulate the timbre transfer task similarly, by first converting the audio tracks into log mel spectrograms and by conditioning the generation of the desired timbre spectrogram through the input timbre spectrogram.   We perform both one-to-one and many-to-many timbre transfer, by converting audio waveforms containing only single instruments and multiple instruments, respectively. We compare the proposed technique with existing state-of-the-art methods both through listening tests and objective measures in order to demonstrate the effectiveness of the proposed model."
Neha Rajagopalan;Blair Kaneshiro,Correlation of EEG Responses Reflects Structural Similarity of Choruses in Popular Music,2023,https://doi.org/10.5281/zenodo.10265273,"Neha Rajagopalan, Stanford University;Blair Kaneshiro, Stanford University","Music structure analysis is a core topic in Music Information Retrieval and could be advanced through the inclusion of new data modalities. In this study we consider neural correlates of music structure processing using popular music - specifically choruses of Bollywood songs - and the {NMED-H} electroencephalographic (EEG) dataset. Motivated by recent findings that listeners' EEG responses correlate when hearing a shared music stimulus, we investigate whether responses correlate not only within single choruses but across pairs of chorus instances as well. We find statistically significant correlations within and across several chorus instances, suggesting that brain responses synchronize across structurally matched music segments even if they are not contextually or acoustically identical. Correlations were only occasionally higher within than across choruses. Our findings advance the state of the art of naturalistic music neuroscience, while also highlighting a novel approach for further studies of music structure analysis and audio understanding more broadly."
Mark R. H. Gotham,Chromatic Chords in Theory and Practice,2023,https://doi.org/10.5281/zenodo.10265275,"Mark R. H. Gotham, Durham University","""Chromatic harmony"" is seen as a fundamental part of (extended) tonal music in the Western classical tradition (c.1700–1900). It routinely features in core curricula. Yet even in this globalised and data-driven age, 1) there are significant gaps between how different national ""schools"" identify important chords and progressions, label them, and shape the corresponding curricula; 2) even many common terms lack robust definition; and 3) empirical evidence rarely features, even in in discussions about ""typical"", ""representative"" practice. This paper addresses those three considerations by: 1) comparing English- and German-speaking traditions as an example of this divergence; 2) proposing a framework for defining common terms where that is lacking; and 3) surveying the actual usage of these chromatic chord categories using a computational corpus study of human harmonic analyses."
Yo-Wei Hsiao;Tzu-Yun Hung;Tsung-Ping Chen;Li Su,BPS-Motif: A Dataset for Repeated Pattern Discovery of Polyphonic Symbolic Music,2023,https://doi.org/10.5281/zenodo.10265277,"Yo-Wei Hsiao, Academia Sinica, Taiwan;Tzu-Yun Hung, Academia Sinica, Taiwan;Tsung-Ping Chen, Academia Sinica, Taiwan;Li Su, Academia Sinica, Taiwan","Intra-opus repeated pattern discovery in polyphonic symbolic music data has  challenges in both algorithm design and data annotation. To solve these challenges, we propose BPS-motif, a new symbolic music dataset containing the note-level annotation of motives and occurrences in Beethoven's piano sonatas. The size of the proposed dataset is larger than previous symbolic datasets for repeated pattern discovery. We report the process of dataset annotation, specifically a peer review process and discussion phase to improve the annotation quality. Finally, we propose a motif discovery method which is shown outperforming baseline methods on repeated pattern discovery."
Michael Krause;Sebastian Strahl;Meinard Müller,Weakly Supervised Multi-Pitch Estimation Using Cross-Version Alignment,2023,https://doi.org/10.5281/zenodo.10265279,"Michael Krause, International Audio Laboratories Erlangen, Germany;Sebastian Strahl, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Multi-pitch estimation (MPE), the task of detecting active pitches within a polyphonic music recording, has garnered significant research interest in recent years. Most state-of-the-art approaches for MPE are based on deep networks trained using pitch annotations as targets. The success of current methods is therefore limited by the difficulty of obtaining large amounts of accurate annotations. In this paper, we propose a novel technique for learning MPE without any pitch annotations at all. Our approach exploits multiple recorded versions of a musical piece as surrogate targets. Given one version of a piece as input, we train a network to minimize the distance between its output and time-frequency representations of other versions of that piece.  Since all versions are based on the same musical score, we hypothesize that the learned output corresponds to pitch estimates. To further ensure that this hypothesis holds, we incorporate domain knowledge about overtones and noise levels into the network. Overall, our method replaces strong pitch annotations with weaker and easier-to-obtain cross-version targets. In our experiments, we show that our proposed approach yields viable multi-pitch estimates and outperforms two baselines."
Patricia Hu;Gerhard Widmer,The Batik-Plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations,2023,https://doi.org/10.5281/zenodo.10265283,"Patricia Hu, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria; LIT AI Lab, Linz Institute of Technology, Austria","We present the Batik plays Mozart Corpus, a piano performance dataset combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored Bösendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can further be connected to the musicological annotations (harmony, cadences, phrases) on these scores that were recently published by [1].  The result is a high-quality, high-precision corpus mapping scores and musical structure annotations to precise note-level professional performance information. As the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects.  In the paper, we outline the curation process of the alignment and conduct two exploratory experiments to demonstrate its usefulness in analyzing expressive performance.  [1] Hentschel, J., Neuwirth, M., &amp; Rohrmeier, M. (2021). The Annotated Mozart Sonatas: Score, Harmony, and Cadence. Transactions of the International Society for Music Information Retrieval (TISMIR), Vol. 4, No. 1, pp. 67-80."
Joan Serrà;Davide Scaini;Santiago Pascual;Daniel Arteaga;Jordi Pons;Jeroen Breebaart;Giulio Cengarle,Mono-to-Stereo Through Parametric Stereo Generation,2023,https://doi.org/10.5281/zenodo.10265285,"Joan Serrà, Dolby Laboratories;Davide Scaini, Dolby Laboratories;Santiago Pascual, Dolby Laboratories;Daniel Arteaga, Dolby Laboratories;Jordi Pons, Dolby Laboratories;Jeroen Breebaart, Dolby Laboratories;Giulio Cengarle, Dolby Laboratories","Generating a stereophonic presentation from a monophonic audio signal is a challenging open task, especially if the goal is to obtain a realistic spatial imaging with a specific panning of sound elements. In this work, we propose to convert mono to stereo by means of predicting parametric stereo (PS) parameters using both nearest neighbor and deep network approaches. In combination with PS, we also propose to model the task with generative approaches, allowing to synthesize multiple and equally-plausible stereo renditions from the same mono signal. To achieve this, we consider both autoregressive and masked token modelling approaches. We provide evidence that the proposed PS-based models outperform a competitive classical decorrelation baseline and that, within a PS prediction framework, modern generative models outshine equivalent non-generative counterparts. Overall, our work positions both PS and generative modelling as strong and appealing methodologies for mono-to-stereo upmixing. A discussion of the limitations of these approaches is also provided."
Charilaos Papaioannou;Emmanouil Benetos;Alexandros Potamianos,From West to East: Who Can Understand the Music of the Others Better?,2023,https://doi.org/10.5281/zenodo.10265287,"Charilaos Papaioannou, School of ECE, National Technical University of Athens, Greece;Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London, UK;Alexandros Potamianos, School of ECE, National Technical University of Athens, Greece","Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository."
Juan C. Martinez-Sevilla;Adrián Roselló;David Rizo;Jorge Calvo-Zaragoza,On the Performance of Optical Music Recognition in the Absence of Specific Training Data,2023,https://doi.org/10.5281/zenodo.10265289,"Juan C. Martinez-Sevilla, U. I. for Computing Research, University of Alicante, Spain;Adrian Rosello, U. I. for Computing Research, University of Alicante, Spain;David Rizo, U. I. for Computing Research, University of Alicante, Spain;Jorge Calvo-Zaragoza, U. I. for Computing Research, University of Alicante, Spain","Optical Music Recognition (OMR) has become a popular technology to retrieve information present in musical scores in conjunction with the increasing improvement of Deep Learning techniques, which represent the state-of-the-art in the field. However, its effectiveness is limited to cases where the target collection is similar in musical context and graphical appearance to the available training examples. To address this limitation, researchers have resorted to labeling examples for specific neural models, which is time-consuming and raises questions about usability. In this study, we propose a holistic and comprehensive study for dealing with new music collections in OMR, including extensive experiments to identify key aspects to have in mind that lead to better performance ratios. We resort to collections written in Mensural notation as specific use case, comprising 5 different corpora of training domains and up to 15 test collections. Our experiments report many interesting insights that will be important to create a manual of best practices when dealing with new collections in OMR systems."
Martin E. Malandro,Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling,2023,https://doi.org/10.5281/zenodo.10265291,"Martin E. Malandro, Sam Houston State University","We introduce Composer's Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of measures from a MIDI file, and we train a T5-like model to accomplish this task. Composer's Assistant consists of this model together with scripts that enable interaction with the model in REAPER. We conduct objective and subjective tests of our model. We release our complete system, consisting of source code, pretrained models, and REAPER scripts. Our models were trained only on permissively-licensed MIDI files."
Ethan Lustig;David Temperley,"The FAV Corpus: An Audio Dataset of Favorite Pieces and Excerpts, With Formal Analyses and Music Theory Descriptors",2023,https://doi.org/10.5281/zenodo.10265293,"Ethan Lustig, Eastman School of Music, University of Rochester;David Temperley, Eastman School of Music, University of Rochester","We introduce a novel audio corpus, the FAV Corpus, of over 400 favorite musical excerpts and pieces, formal analyses, and free-response comments. In a survey, 140 American university students (mostly music majors) were asked to provide three of their favorite 15-second musical excerpts, from any genre or time period. For each selection, respondents were asked: ""Why do you love the excerpt? Try to be as specific and detailed as possible (music theory terms are encouraged but not required)."" Classical selections were dominated by a very small number of composers, while the pop and jazz artists were diverse. A thematic coding of the respondents' comments found that the most common themes were melody (34.2% of comments), harmony (27.2%), and sonic factors: texture (27.6%), instrumentation (24.3%), and timbre (12.5%). (Rhythm (19.5%) and meter (4.6%) were less present in the comments.) The comments cite simplicity three times more than complexity, and energy gain 14 times more than energy decrease, suggesting that people's favorite excerpts involve simple moments of energy gain or ""build-up"". The complete FAV Corpus is publicly available online at EthanLustig.com/FavCorpus. We will discuss future possibilities for the corpus, including potential directions in the spaces of machine learning and music recommendation."
Le Zhuo;Ruibin Yuan;Jiahao Pan;Yinghao Ma;Yizhi Li;Ge Zhang;Si Liu;Roger B. Dannenberg;Jie Fu;Chenghua Lin;Emmanouil Benetos;Wenhu Chen;Wei Xue;Yike Guo,LyricWhiz: Robust Multilingual Zero-Shot Lyrics Transcription by Whispering to ChatGPT,2023,https://doi.org/10.5281/zenodo.10265295,"Le Zhuo, Beihang University;Ruibin Yuan, Beijing Academy of Artificial Intelligence;Jiahao Pan, ;Yinghao Ma, ;Yizhi Li, ;Ge Zhang, ;Si Liu, Carnegie Mellon University;Roger Dannenberg, Carnegie Mellon University;Jie Fu, Beijing Academy of Artificial Intelligence;Chenghua Lin, ;Emmanouil Benetos, Queen Mary University of London;Wenhu Chen, University of Waterloo;Wei Xue, Hong Kong University of Science and Technology;Yike Guo, Hong Kong University of Science and Technology","We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the ""ear"" by transcribing the audio, while GPT-4 serves as the ""brain,"" acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copy-right license, based on MTG-Jamendo, and offer a human- annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task."
Alia Morsi;Kana Tatsumi;Akira Maezawa;Takuya Fujishima;Xavier Serra,Sounds Out of Pläce? Score-Independent Detection of Conspicuous Mistakes in Piano Performances,2023,https://doi.org/10.5281/zenodo.10265297,"Alia Morsi, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Kana Tatsumi, Nagoya Institute of Technology, Nagoya, Japan;Akira Maezawa, Yamaha Corporation, Hamamatsu, Japan;Takuya Fujishima, Yamaha Corporation, Hamamatsu, Japan;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In piano performance, some mistakes stand out to listeners, whereas others may go unnoticed. Former research concluded that the salience of mistakes depended on factors including their contextual appropriateness and a listener's degree of familiarity to what is being performed. A conspicuous error is considered to be an area where there is something obviously wrong with the performance, which a listener can detect regardless of their degree of knowledge of what is being performed. Analogously, this paper attempts to build a score-independent conspicuous error detector for standard piano repertoire of beginner to intermediate students. We gather three qualitatively different piano playing MIDI data: (1) 103 sight-reading sessions for beginning and intermediate adult pianists with formal music training, (2) 245 performances by presumably late-beginner to early-advanced pianists on a digital piano, and (3) 50 etude performances by an advanced pianist. The data was annotated at the regions considered to contain conspicuous mistakes. Then, we use a Temporal Convolutional Network to detect the sites of such mistakes from the piano roll. We investigate the use of two pre-training methods to overcome data scarcity: (1) synthetic data with procedurally-generated mistakes, and (2) training a part of the model as a piano roll auto-encoder. Experimental evaluation shows that the TCN performs at an F-measure of 0.78 without pretraining for sight-reading data, but the proposed pretraining steps improve the F-measure on performance and etude data, approaching the agreement between human raters on conspicuous error labels. Importantly, we report on the lessons learned from this pilot study, and what should be addressed to continue this research direction."
Hugo Flores García;Prem Seetharaman;Rithesh Kumar;Bryan Pardo,VampNet: Music Generation via Masked Acoustic Token Modeling,2023,https://doi.org/10.5281/zenodo.10265299,"Hugo Flores García, Descript Inc., Northwestern University;Prem Seetharaman, Descript Inc.;Rithesh Kumar, Descript Inc.;Bryan Pardo, Northwestern University","We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation.  We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online."
Yucong Jiang,Expert and Novice Evaluations of Piano Performances: Criteria for Computer-Aided Feedback,2023,https://doi.org/10.5281/zenodo.10265301,"Yucong Jiang, University of Richmond","Learning an instrument can be rewarding, but is unavoidably a huge undertaking. Receiving constructive feedback on one's playing is crucial for improvement. However, personal feedback from an expert instructor is seldom available on demand. The goal motivating this project is to build software that will provide comparably useful feedback to beginners, in order to supplement feedback from human instructors. To lay the groundwork for that, in this paper we investigate performance assessment criteria from both quantitative and qualitative perspectives. We gathered 83 piano performances from 21 players. Each recording was evaluated by both expert piano instructors and novice players. This dataset is unique in that the novice evaluators are also players, and that both quantitative and qualitative evaluations are collected. Our analysis of the evaluations indicates that the kind of specific, concrete piano techniques that are most elusive to novice evaluators are precisely the kind of characteristics that can be detected, measured, and visualized for learners by a well-designed software tool."
Andres Ferraro;Jaehun Kim;Sergio Oramas;Andreas Ehmann;Fabien Gouyon,Contrastive Learning for Cross-Modal Artist Retrieval,2023,https://doi.org/10.5281/zenodo.10265303,"Andres Ferraro, Pandora-SiriusXM, Oakland;Jaehun Kim, ;Sergio Oramas, ;Andreas Ehmann, ;Fabien Gouyon, ","Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally represented in several modalities, e.g., audio signals, user interaction data, or editorial data. However, data of any given modality might not be available for all items in any music dataset. In this work, we propose a method based on contrastive learning to combine embeddings from multiple modalities and explore the impact of the presence or absence of embeddings from diverse modalities in an artist similarity task. Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities, both in terms of artist retrieval accuracy and coverage. Improvements with respect to other methods are particularly significant for less popular query artists. We demonstrate our method successfully combines complementary information from diverse modalities, and is more robust to missing modality data (i.e., it better handles the retrieval of artists with different modality embeddings than the query artist's)."
Christoph Finkensiep;Matthieu Haeberle;Friedrich Eisenbrand;Markus Neuwirth;Martin Rohrmeier,Repetition-Structure Inference With Formal Prototypes,2023,https://doi.org/10.5281/zenodo.10265305,"Christoph Finkensiep, École Polytechnique Fédérale de Lausanne, Switzerland;Matthieu Haeberle, École Polytechnique Fédérale de Lausanne, Switzerland;Friedrich Eisenbrand, École Polytechnique Fédérale de Lausanne, Switzerland;Markus Neuwirth, Anton Bruckner Privatuniversität Linz, Austria;Martin Rohrmeier, École Polytechnique Fédérale de Lausanne, Switzerland","The concept of form in music encompasses a wide range of musical aspects, such as phrases and (hierarchical) segmentation, formal functions, cadences and voice-leading schemata, form templates, and repetition structure. In an effort towards a unified model of form, this paper proposes an integration of repetition structure   (i.e., which segments of a piece occur several times) and formal templates (such as AABA). While repetition structure can be modeled using context-free grammars,   most prior approaches allow for arbitrary grammar rules. Constraining the structure of the inferred rules to conform to a small set of templates (meta-rules) not only reduces the space of possible rules that need to be considered but also ensures that the resulting repetition grammar remains interpretable in the context of musical form.   The resulting formalism can be extended to cases of varied repetition and thus constitutes a building block for a larger model of form."
Peter van Kranenburg;Eoin J. Kearns,Algorithmic Harmonization of Tonal Melodies Using Weighted Pitch Context Vectors,2023,https://doi.org/10.5281/zenodo.10265307,"Peter van Kranenburg, Meertens Institute, Utrecht University;Eoin Kearns, Meertens Institute","Most melodies from the Western common practice period have a harmonic background, i.e., a succession of chords that fit the melody. In this paper we provide a novel approach to infer this harmonic background from the score notation of a melody. We first construct a pitch context vector for each note in the melody. This vector summarises the pitches that are in the preceding and following contexts of the note. Next, we use these pitch context vectors to generate a list of candidate chords for each note. The candidate chords fit the pitch context of a given note each with a computed strength. Finally, we find an optimal path through the chord candidates, employing a score function for the fitness of a given candidate chord. The algorithm chooses one chord for each note, optimizing the total score. A set of heuristics is incorporated in the score function. The system is heavily parameterised, extremely flexible, and does not need training. This creates a framework to experiment with harmonization of melodies. The output is evaluated by an expert survey, which yields convincing and positive results."
Kento Watanabe;Masataka Goto,Text-to-Lyrics Generation With Image-Based Semantics and Reduced Risk of Plagiarism,2023,https://doi.org/10.5281/zenodo.10265309,"Kento Watanabe, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper proposes a text-to-lyrics generation method, aiming to provide lyric writing support by suggesting the generated lyrics to users who struggle to find the right words to convey their message. Previous studies on lyrics generation have focused on generating lyrics based on semantic constraints such as specific keywords, lyric style, and topics. However, these methods had limitations because users could not freely input their intentions as text. Even if such intentions can be given as input text, the lyrics generated from the input tend to contain similar wording, making it difficult to inspire the user. Our method is therefore developed to generate lyrics that (1) convey a message similar to the input text and (2) contain wording different from the input text. A straightforward approach of training a text-to-lyrics encoder-decoder is not feasible since there is no text-lyric paired data for this purpose. To overcome this issue, we divide the text-to-lyrics generation process into a two-step pipeline, eliminating the need for text-lyric paired data. (a) First, we use an existing text-to-image generation technique as a text analyzer to obtain an image that captures the meaning of the input text, ignoring the wording. (b) Next, we use our proposed image-to-lyrics encoder-decoder (I2L) to generate lyrics from the obtained image while preserving its meaning. The training of this I2L model only requires pairs of ""lyrics"" and ""images generated from lyrics"", which are readily prepared. In addition, we propose for the first time a lyrics generation method that reduces the risk of plagiarism by prohibiting the generation of uncommon phrases in the training data. Experimental results show that the proposed method can generate lyrics with different phrasing while conveying a message similar to the input text."
SeungHeon Doh;Keunwoo Choi;Jongpil Lee;Juhan Nam,LP-MusicCaps: LLM-Based Pseudo Music Captioning,2023,https://doi.org/10.5281/zenodo.10265311,"SeungHeon Doh, Graduate School of Culture Technology, KAIST, South Korea;Keunwoo Choi, Gaudio Lab, Inc., South Korea;Jongpil Lee, Neutune, South Korea;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea","Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model."
Morgan Buisson;Brian McFee;Slim Essid;Helene C. Crayencour,A Repetition-Based Triplet Mining Approach for Music Segmentation,2023,https://doi.org/10.5281/zenodo.10265313,"Morgan Buisson, LTCI, Télécom Paris, Institut Polytechnique de Paris, France;Brian McFee, Music and Audio Research Laboratory, New York University, USA;Slim Essid, LTCI, Télécom Paris, Institut Polytechnique de Paris, France;Hélène C. Crayencour, L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France","Contrastive learning has recently appeared as a well-suited method to find representations of music audio signals that are suitable for structural segmentation. However, most existing unsupervised training strategies omit the notion of repetition and therefore fail at encompassing this essential aspect of music structure. This work introduces a triplet mining method which explicitly considers repeating sequences occurring inside a music track by leveraging common audio descriptors. We study its impact on the learned representations through downstream music segmentation. Because musical repetitions can be of different natures, we give further insight on the role of the audio descriptors employed at the triplet mining stage as well as the trade-off existing between the quality of the triplets mined and the quantity of unlabelled data used for training. We observe that our method requires less non-annotated data while remaining competitive against other unsupervised methods trained on a larger corpus."
Francesco Foscarin;Daniel Harasim;Gerhard Widmer,Predicting Music Hierarchies With a Graph-Based Neural Decoder,2023,https://doi.org/10.5281/zenodo.10265315,"Francesco Foscarin, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Daniel Harasim, Independent Researcher;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria","This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis. The parsing involves two steps. First, the input sequence is passed through a transformer encoder to enrich it with contextual information. Then, a classifier filters the graph of all possible dependency arcs to produce the dependency tree. One major benefit of this system is that it can be easily integrated into modern deep-learning pipelines. Moreover, since it does not rely on any particular symbolic grammar, it can consider multiple musical features simultaneously, make use of sequential context information, and produce partial results for noisy inputs. We test our approach on two datasets of musical trees -- time-span trees of monophonic note sequences and harmonic trees of jazz chord sequences -- and show that our approach outperforms previous methods."
Johannes Zeitler;Simon Deniffel;Michael Krause;Meinard Müller,Stabilizing Training With Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation With Weakly Aligned Targets,2023,https://doi.org/10.5281/zenodo.10265317,"Johannes Zeitler, International Audio Laboratories Erlangen, Germany;Simon Deniffel, International Audio Laboratories Erlangen, Germany;Michael Krause, International Audio Laboratories Erlangen, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Soft dynamic time warping (SDTW) is a differentiable loss function that allows for training neural networks from weakly aligned data. Typically, SDTW is used to iteratively compute and refine soft alignments that compensate for temporal deviations between the training data and its weakly annotated targets. One major problem is that a mismatch between the estimated soft alignments and the reference alignments in the early training stage leads to incorrect parameter updates, making the overall training procedure unstable. In this paper, we investigate such stability issues by considering the task of pitch class estimation from music recordings as an illustrative case study. In particular, we introduce and discuss three conceptually different strategies (a hyperparameter scheduling, a diagonal prior, and a sequence unfolding strategy) with the objective of stabilizing intermediate soft alignment results. Finally, we report on experiments that demonstrate the effectiveness of the strategies and discuss efficiency and implementation issues."
Danbinaerin Han;Rafael Caro Repetto;Dasaem Jeong,Finding Tori: Self-Supervised Learning for Analyzing Korean Folk Song,2023,https://doi.org/10.5281/zenodo.10265319,"Danbinaerin Han, Department of Art & Technology, Sogang University;Rafael Caro Repetto, Kunstuniversität Graz;Dasaem Jeong, Department of Art & Technology, Sogang University","In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs."
Bernardo Torres;Stefan Lattner;Gaël Richard,Singer Identity Representation Learning Using Self-Supervised Techniques,2023,https://doi.org/10.5281/zenodo.10265323,"Bernardo Torres, LTCI, Telecom Paris, Institut Polytechnique de Paris;Stefan Lattner, Sony Computer Science Laboratories Paris;Gaël Richard, LTCI, Telecom Paris, Institut Polytechnique de Paris","Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and trained models to facilitate further research on singing voice and related areas."
Yinghao Ma;Ruibin Yuan;Yizhi Li;Ge Zhang;Chenghua Lin;Xingran Chen;Anton Ragni;Hanzhi Yin;Emmanouil Benetos;Norbert Gyenge;Ruibo Liu;Gus Xia;Roger B. Dannenberg;Yike Guo;Jie Fu,On the Effectiveness of Speech Self-Supervised Learning for Music,2023,https://doi.org/10.5281/zenodo.10265321,"Yinghao Ma, Queen Mary University of London;Ruibin Yuan, Beijing Academy of Artificial Intelligence;Yizhi Li, University of Sheffield;Ge Zhang, University of Michigan Ann Arbor;Xingran Chen, ;Hanzhi Yin, Carnegie Mellon University;Chenghua Lin, University of Sheffield;Emmanouil Benetos, Queen Mary University of London;Anton Ragni, University of Sheffield;Norbert Gyenge, University of Sheffield;Ruibo Liu, Dartmouth College;Gus Xia, New York University Shanghai;Roger Dannenberg, Carnegie Mellon University;Yike Guo, Hong Kong University of Science and Technology;Jie Fu, Beijing Academy of Artificial Intelligence","Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent models such as wav2vec2.0 have shown promise. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train 12 SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms."
Tian Cheng;Masataka Goto,Transformer-Based Beat Tracking With Low-Resolution Encoder and High-Resolution Decoder,2023,https://doi.org/10.5281/zenodo.10265325,"Tian Cheng, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","In this paper, we address the beat tracking task which is to predict beat times corresponding to the input audio. Due to the long sequential inputs, it is still challenging to model the global structure efficiently and to deal with the data imbalance between beats and no beats. In order to meet the above challenges, we propose a novel Transformer-based model consisting of a low-resolution encoder and a high-resolution decoder. The encoder with low temporal resolution is suited to capture global features with more balanced data. The decoder with high temporal resolution is designed to predict beat times at a desired resolution. In the decoder, the global structure is considered by the cross attention between the global features and high-dimensional features. There are two key modifications in the proposed model: (1) adding 1D convolutional layers in the encoder and (2) replacing positional embedding by the upsampled encoder features in the decoder. In the experiment, we achieved the state-of-the-art performance and showed that the decoder produced more precise and stable results."
Vanessa Nina Borsan;Mathieu Giraud;Richard Groult;Thierry Lecroq,Adding Descriptors to Melodies Improves Pattern Matching: A Study on Slovenian Folk Songs,2023,https://doi.org/10.5281/zenodo.10265329,"Vanessa Nina Borsan, Univ. Lille, CNRS, Centrale Lille UMR 9189 CRIStAL, F-59000 Lille, France;Mathieu Giraud, Univ. Lille, CNRS, Centrale Lille UMR 9189 CRIStAL, F-59000 Lille, France;Richard Groult, Univ Rouen Normandie, INSA Rouen Normandie, Université Le Havre Normandie, Normandie Univ LITIS UR 4108, F-76000 Rouen, France;Thierry Lecroq, Univ Rouen Normandie, INSA Rouen Normandie, Université Le Havre Normandie, Normandie Univ LITIS UR 4108, F-76000 Rouen, France","The objective of pattern-matching topics is to gain insights into repetitive patterns within or across various music genres and cultures. This approach aims to shed light on the recurring instances present in diverse musical traditions. The paper presents a study analyzing folk songs using symbolic music representation, including melodic sequences and musical information. By examining a corpus of 400 monophonic Slovenian tunes, we are releasing annotations of structure, contour, and implied harmony. We propose an efficient algorithm based on suffix arrays and bit-vectors to match both music content (melodic sequence) and context (descriptors). Our study reveals that certain descriptors, such as contour types and harmonic ""stability"" exhibit variations based on phrase position within a tune. Additionally, combining melody and descriptors in pattern-matching queries enhances precision for classification tasks. We emphasize the importance of the interplay between melodic sequences and music descriptors, highlighting that different pattern queries may have varying levels of detail requirements. As a result, our approach promotes flexibility in computational music analysis. Lastly, our objective is to foster the knowledge of Slovenian folk songs."
Karlijn Dinnissen;Christine Bauer,How Control and Transparency for Users Could Improve Artist Fairness in Music Recommender Systems,2023,https://doi.org/10.5281/zenodo.10265331,"Karlijn Dinnissen, Utrecht University, The Netherlands;Christine Bauer, Paris Lodron University Salzburg, Austria","As streaming services have become a main channel for music consumption, they significantly impact various stakeholders: users, artists who provide music, and other professionals working in the music industry. Therefore, it is essential to consider all stakeholders' goals and values when developing and evaluating the music recommender systems integrated into these services. One vital goal is treating artists fairly, thereby giving them a fair chance to have their music recommended and listened to, and subsequently building a fan base. Such artist fairness is often assumed to have a trade-off with user goals such as satisfaction. Using insights from two studies, this work shows the opposite: some goals from different stakeholders are complementary. Our first study, in which we interview music artists, demonstrates that they often see increased transparency and control for users as a means to also improve artist fairness. We expand with a second study asking other music industry professionals about these topics using a questionnaire. Its results indicate that transparency towards users is highly valued and should be increased."
Ahyeon Choi;Eunsik Shin;Haesun Joung;Joongseek Lee;Kyogu Lee,Towards a New Interface for Music Listening: A User Experience Study on YouTube,2023,https://doi.org/10.5281/zenodo.10265333,"Ahyeon Choi, Department of Intelligence and Information, Seoul National University;Eunsik Shin, Department of Intelligence and Information, Seoul National University;Haesun Joung, Department of Intelligence and Information, Seoul National University;Joongseek Lee, Department of Intelligence and Information, Seoul National University;Kyogu Lee, Department of Intelligence and Information, Seoul National University","In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from traditional music streaming services in that they provide a diverse range of music-related videos as well as soundtracks. However, notwithstanding the surge in the platform's utilization as a music consumption tool, there is a lack of thorough research on the phenomenon. To investigate its usability and interface satisfaction as a music listening tool, we conducted semi-structured interviews with 27 users who listen to music through YouTube more than three times a week. Our qualitative analysis found that YouTube has five main meanings for users as a music streaming service: 1) exploring musical diversity, 2) sharing unique playlists, 3) providing visual satisfaction, 4) facilitating user interaction, and 5) allowing free and easy access. We also propose wireframes of a video streaming service for better audio-visual music listening in two stages: search and listening. By these wireframes, we offer practical solutions to enhance user satisfaction with YouTube for music listening. It has implications not only for YouTube but also for other streaming services for music."
Xavier Riley;Simon Dixon,FiloBass: A Dataset and Corpus Based Study of Jazz Basslines,2023,https://doi.org/10.5281/zenodo.10265335,"Xavier Riley, Queen Mary University of London;Simon Dixon, Queen Mary University of London, Centre for Digital Music","We present FiloBass: a novel corpus of music scores and annotations which focuses on the important but often overlooked role of the double bass in jazz accompaniment. Inspired by recent works that shed light on the role of the soloist, we offer a collection of 48 manually verified transcriptions of professional jazz bassists, comprising over 50,000 note events, which are based on the backing tracks used in the FiloSax dataset. For each recording we provide audio stems, scores, performance-aligned MIDI and associated metadata for beats, downbeats, chord symbols and markers for musical form.  We then use FiloBass to enrich our understanding of jazz bass lines, by conducting a corpus-based musical analysis with a contrastive study of existing instructional methods. Together with the original FiloSax dataset, our work represents a significant step toward a fully annotated performance dataset for a jazz quartet setting. By illuminating the critical role of the bass in jazz, this work contributes to a more nuanced and comprehensive understanding of the genre."
Louis Couturier;Louis Bigo;Florence Levé,Comparing Texture in Piano Scores,2023,https://doi.org/10.5281/zenodo.10265337,"Louis Couturier, MIS, Université de Picardie Jules Verne, Amiens, France;Louis Bigo, Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France;Florence Levé, MIS, Université de Picardie Jules Verne, Amiens, France","In this paper, we propose four different approaches to quantify similarities of compositional texture in symbolically encoded piano music. A melodic contour or harmonic progression can be shaped into a wide variety of different rhythms, densities, or combinations of layers. Instead of describing these textural organizations only locally, using existing formalisms, we question how these parameters may evolve throughout a musical piece, and more specifically how much they change. Hence, we define several distance functions to compare texture between two musical bars, based either on textural labels annotated with a dedicated syntax, or on symbolic scores. We propose an evaluation methodology based on textural heterogeneity and contrasts in classical Thema and Variations using the TAVERN dataset. Finally, we illustrate use cases of these tools to analyze long-term structure, and discuss the impact of these results on the understanding of musical texture."
Johannes Hentschel;Andrew McLeod;Yannis Rammos;Martin Rohrmeier,Introducing DiMCAT for Processing and Analyzing Notated Music on a Very Large Scale,2023,https://doi.org/10.5281/zenodo.10265339,"Johannes Hentschel, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland;Andrew McLeod, Fraunhofer IDMT, Ilmenau, Germany;Yannis Rammos, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland;Martin Rohrmeier, Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland","As corpora of digital musical scores continue to grow, the need for research tools capable of manipulating such data efficiently, with an intuitive interface, and support for a diversity of file formats, becomes increasingly pressing. In response, this paper introduces the Digital Musicology Corpus Analysis Toolkit (DiMCAT), a Python library for processing large corpora of digitally encoded musical scores. Equally aimed at music-analytical corpus studies, MIR, and machine-learning research, DiMCAT performs common data transformations and analyses using dataframes. Dataframes reduce the inherent complexity of atomic score contents (e.g., notes), larger score entities (e.g., measures), and abstractions (e.g., chord symbols) into easily manipulable computational structures, whose vectorized operations scale to large quantities of musical material. The design of DiMCAT's API prioritizes computational speed and ease of use, thus aiming to cater to machine-learning practitioners and musicologists alike."
Sehun Kim;Kazuya Takeda;Tomoki Toda,Sequence-to-Sequence Network Training Methods for Automatic Guitar Transcription With Tokenized Outputs,2023,https://doi.org/10.5281/zenodo.10265341,"Sehun Kim, Nagoya University;Kazuya Takeda, Nagoya University;Tomoki Toda, Nagoya University","We propose multiple methods for effectively training a sequence-to-sequence automatic guitar transcription model which uses tokenized music representation as an output. Our proposed method mainly consists of 1) a hybrid CTC-Attention model for sequence-to-sequence automatic guitar transcription that uses tokenized music representation, and 2) two data augmentation methods for training the model. Our proposed model is a generic encoder-decoder Transformer model but adopts multi-task learning with CTC from the encoder to speed up learning alignments between the output tokens and acoustic features. Our proposed data augmentation methods scale up the amount of training data by 1) creating bar overlap when splitting an excerpt to be used for network input, and 2) by utilizing MIDI-only data to synthetically create audio-MIDI pair data. We confirmed that 1) the proposed data augmentation methods were highly effective for training generic Transformer models that generate tokenized outputs, 2) our proposed hybrid CTC-Attention model outperforms conventional methods that transcribe guitar performance with tokens, and 3) the addition of multi-task learning with CTC in our proposed model is especially effective when there is an insufficient amount of training data."
Alain Riou;Stefan Lattner;Gaëtan Hadjeres;Geoffroy Peeters,PESTO: Pitch Estimation With Self-Supervised Transposition-Equivariant Objective,2023,https://doi.org/10.5281/zenodo.10265343,"Alain Riou, LTCI, Télécom-Paris, Institut Polytechnique de Paris, France;Stefan Lattner, Sony Computer Science Laboratories - Paris, France;Gaëtan Hadjeres, Sony AI;Geoffroy Peeters, LTCI, Télécom-Paris, Institut Polytechnique de Paris, France","In this paper, we address the problem of pitch estimation using self-supervised learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset.  We use a lightweight (&lt; 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its constant-Q transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.  We evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with low-resource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation."
Vanessa Nina Borsan;Mathieu Giraud;Richard Groult,The Games We Play: Exploring the Impact of ISMIR on Musicology,2023,https://doi.org/10.5281/zenodo.10265345,"Vanessa Nina Borsan, Univ. Lille, CNRS, Centrale Lille;Mathieu Giraud, Univ. Lille, CNRS, Centrale Lille;Richard Groult, Univ Rouen Normandie, INSA Rouen Normandie, Université Le Havre Normandie, Normandie Univ","Throughout history, a consistent temporal and spatial gap has persisted between the inception of novel knowledge and technology and their subsequent adoption for extensive practical utilization. The article explores the dynamic interaction and exchange of methodologies between musicology and computational music research. It focuses on an analysis of ten years' worth of papers from the International Society for Music Information Retrieval (ISMIR) from 2012 to 2021. Over 1000 citations of ISMIR papers were reviewed, and out of these, 51 later works published in musicological venues drew from the findings of 28 ISMIR papers. Final results reveal that most contributions from ISMIR rarely make their way to musicology or humanities. Nevertheless, the paper highlights four examples of successful knowledge transfers between the fields and discusses best practices for collaborations while addressing potential causes for such disparities. In the epilogue, we address the interlaced origins of the problem as stemming from the language of new media, institutional restrictions, and the inability to engage in multidisciplinary communication."
Genís Plaja-Roglans;Marius Miron;Adithi Shankar;Xavier Serra,Carnatic Singing Voice Separation Using Cold Diffusion on Training Data With Bleeding,2023,https://doi.org/10.5281/zenodo.10265347,"Genís Plaja-Roglans, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Marius Miron, Earth Species Project;Adithi Shankar, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Supervised music source separation systems using deep learning are trained by minimizing a loss function between pairs of predicted separations and ground-truth isolated sources. However, open datasets comprising isolated sources are few, small, and restricted to a few music styles. At the same time, multi-track datasets with source bleeding are usually found larger in size, and are easier to compile. In this work, we address the task of singing voice separation when the ground-truth signals have bleeding and only the target vocals and the corresponding mixture are available. We train a cold diffusion model on the frequency domain to iteratively transform a mixture into the corresponding vocals with bleeding. Next, we build the final separation masks by clustering spectrogram bins according to their evolution along the transformation steps. We test our approach on a Carnatic music scenario for which solely datasets with bleeding exist, while current research on this repertoire commonly uses source separation models trained solely with Western commercial music. Our evaluation on a Carnatic test set shows that our system improves Spleeter on interference removal and it is competitive in terms of signal distortion. Code is open sourced"
Kosetsu Tsukuda;Tomoyasu Nakano;Masahiro Hamasaki;Masataka Goto,Unveiling the Impact of Musical Factors in Judging a Song on First Listen: Insights From a User Survey,2023,https://doi.org/10.5281/zenodo.10265351,"Kosetsu Tsukuda, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Tomoyasu Nakano, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masahiro Hamasaki, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","When a user listens to a song for the first time, what musical factors (e.g., melody, tempo, and lyrics) influence the user's decision to like or dislike the song? An answer to this question would enable researchers to more deeply understand how people interact with music. Thus, in this paper, we report the results of an online survey involving 302 participants to investigate the influence of 10 musical factors. We also evaluate how a user's personal characteristics (i.e., personality traits and musical sophistication) relate to the importance of each factor for the user. Moreover, we propose and evaluate three factor-based functions that would enable more effectively browsing songs on a music streaming service. The user survey results provide several reusable insights, including the following: (1) for most participants, the melody and singing voice are important factors in judging whether they like a song on first listen; (2) personal characteristics do influence the important factors (e.g., participants who have high openness and are sensitive to beat deviations emphasize melody); and (3) the proposed functions each have a certain level of demand because they enable users to easily find music that fits their tastes. We have released part of the survey results as publicly available data so that other researchers can reproduce the results and analyze the data from their own viewpoints."
Jan Hajič jr.;Gustavo A. Ballen;Klára Hedvika Mühlová;Hana Vlhová-Wörner,Towards Building a Phylogeny of Gregorian Chant Melodies,2023,https://doi.org/10.5281/zenodo.10340442,"Jan Hajiˇc jr., Masaryk Institute and Archive, Czech Academy of Sciences, Czechia;Gustavo A. Ballen, School of Biological and Behavioural Sciences, Queen Mary University of London, UK;Klára Hedvika Mühlová, Faculty of Arts, Masaryk University, Czechia;Hana Vlhová-Wörner, Masaryk Institute and Archive, Czech Academy of Sciences, Czechia","The historical development of medieval plainchant melodies is an intriguing musicological topic that invites computational approaches to study it at scale. Plainchant melodies can be represented as strings from a limited alphabet, hence making it technically possible to apply bioinformatic tools that are used to study the relationships of biological sequences. We show that using phylogenetic trees to study relationships of plainchant sources is not merely possible, but that it can indeed produce meaningful results. We develop a simple plainchant substitution model for Multiple Sequence Alignment, adapt a Bayesian phylogenetic tree building method, and demonstrate the promise of this approach by validating the resultant phylogenetic tree built from a set of Divine Office sources for the Christmas Vespers against musicological knowledge."
Yiwei Ding;Alexander Lerch,Audio Embeddings as Teachers for Music Classification,2023,https://doi.org/10.5281/zenodo.10265353,"Yiwei Ding, Music Informatics Group, Georgia Institute of Technology;Alexander Lerch, Music Informatics Group, Georgia Institute of Technology","Music classification has been one of the most popular tasks in the field of music information retrieval. With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks. However, the increasing model complexity makes both training and inference computationally expensive. In this paper, we integrate the ideas of transfer learning and feature-based knowledge distillation and systematically investigate using pre-trained audio embeddings as teachers to guide the training of low-complexity student networks. By regularizing the feature space of the student networks with the pre-trained embeddings, the knowledge in the teacher embeddings can be transferred to the students. We use various pre-trained audio embeddings and test the effectiveness of the method on the tasks of musical instrument classification and music auto-tagging. Results show that our method significantly improves the results in comparison to the identical model trained without the teacher's knowledge. This technique can also be combined with classical knowledge distillation approaches to further improve the model's performance."
Ilya Borovik;Vladimir Viro,ScorePerformer: Expressive Piano Performance Rendering With Fine-Grained Control,2023,https://doi.org/10.5281/zenodo.10265355,"Ilya Borovik, Skoltech, Russia;Vladimir Viro, Peachnote GmbH, Germany","We present ScorePerformer, an encoder-decoder transformer with hierarchical style encoding heads for controllable rendering of expressive piano music performances. We design a tokenized representation of symbolic score and performance music, the Score Performance Music tuple (SPMuple), and validate a novel way to encode the local performance tempo in a local note time window. Along with the encoding, we extend a transformer encoder with multi-level maximum mean discrepancy variational autoencoder style modeling heads that learn performance style at the global, bar, beat, and onset levels for fine-grained performance control. To offer an interpretation of the learned latent spaces, we introduce performance direction marking classifiers that associate vectors in the latent space with direction markings to guide performance rendering through the model. Evaluation results show the importance of the architectural design choices and demonstrate that ScorePerformer produces diverse and coherent piano performances that follow the control input."
Emmanouil Karystinaios;Gerhard Widmer,Roman Numeral Analysis With Graph Neural Networks: Onset-Wise Predictions From Note-Wise Features,2023,https://doi.org/10.5281/zenodo.10265357,"Emmanouil Karystinaios, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria","Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music.  This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score.  The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm.  Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets.  In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/manoskary/chordgnn"
Brian Regan;Desislava Hristova;Mariano Beguerisse-Díaz,Semi-Automated Music Catalog Curation Using Audio and Metadata,2023,https://doi.org/10.5281/zenodo.10265359,"Brian Regan, Spotify;Desislava Hristova, Spotify;Mariano Beguerisse-Díaz, Spotify","We present a system to assist Subject Matter Experts (SMEs) to curate large online music catalogs. The system detects releases that are incorrectly attributed to an artist discography (misattribution), when the discography of a single artist is incorrectly separated (duplication), and predicts suitable relocations of misattributed releases. We use historical discography corrections to train and evaluate our system's component models. These models combine vector representations of audio with metadata-based features, which outperform models based on audio or metadata alone. We conduct three experiments with SMEs in which our system detects misattribution in artist discographies with precision greater than 77%, duplication with precision greater than 71%, and by combining the approaches, predicts a correct relocation for misattributed releases with precision up to 45%. These results demonstrate the potential of such proactive curation systems in saving valuable human time and effort by directing attention where it is most needed."
Ioannis Petros Samiotis;Christoph Lofi;Alessandro Bozzon,Crowd's Performance on Temporal Activity Detection of Musical Instruments in Polyphonic Music,2023,https://doi.org/10.5281/zenodo.10265361,"Ioannis Petros Samiotis, Delft University of Technology;Christoph Loﬁ, Delft University of Technology;Alessandro Bozzon, Delft University of Technology","Musical instrument recognition enables applications such as instrument-based music search and audio manipulation, which are highly sought-after processes in everyday music consumption and production. Despite continuous progresses, advances in automatic musical instrument recognition is hindered by the lack of large, diverse and publicly available annotated datasets. As studies have shown, there is potential to scale up music data annotation processes through crowdsourcing. However, it is still unclear the extent to which untrained crowdworkers can effectively detect when a musical instrument is active in an audio excerpt. In this study, we explore the performance of non-experts on online crowdsourcing platforms, to detect temporal activity of instruments on audio extracts of selected genres. We study the factors that can affect their performance, while we also analyse user characteristics that could predict their performance. Our results bring further insights into the general crowd's capabilities to detect instruments."
Igor Pereira;Felipe Araújo;Filip Korzeniowski;Richard Vogl,MoisesDB: A Dataset for Source Separation Beyond 4-Stems,2023,https://doi.org/10.5281/zenodo.10265363,"Igor Pereira, Moises Systems Inc., Salt Lake City, USA.;Felipe Araújo, Moises Systems Inc., Salt Lake City, USA.;Filip Korzeniowski, Moises Systems Inc., Salt Lake City, USA.;Richard Vogl, Moises Systems Inc., Salt Lake City, USA.","In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres.  For each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems.  This will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data.  To facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB. Alongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results."
Zeng Ren;Wulfram Gerstner;Martin Rohrmeier,Music as Flow: A Formal Representation of Hierarchical Processes in Music,2023,https://doi.org/10.5281/zenodo.10265365,"Zeng Ren, EPFL;Wulfram Gerstner, EPFL;Martin Rohrmeier, EPFL","Modeling the temporal unfolding of musical events and its interpretation in terms of hierarchical relations is a common theme in music theory, cognition, and composition. To faithfully encode such relations, we need an elegant way to represent both the semantics of prolongation, where a single event is elaborated into multiple events, and process, where the connection from one event to another is elaborated into multiple connections. In existing works, trees are used to capture the former and graphs for the latter. Each such model has the potential to either encode relations between events (e.g., an event being a repetition of another), or relations between processes (e.g., two consecutive steps making up a larger skip), but not both together explicitly. To model meaningful relations between musical events and processes and combine the semantic expressiveness of trees and graphs, we propose a structured representation using  algebraic datatype (ADT) with dependent type. We demonstrate its applications towards encoding functional interpretations of harmonic progressions, and large scale organizations of key regions. This paper offers two contributions. First, we provide a novel unifying hierarchical framework for musical processes and events. Second, we provide a structured data type encoding such interpretations, which could facilitate computational approaches in music theory and generation."
Silvan David Peter,Online Symbolic Music Alignment With Offline Reinforcement Learning,2023,https://doi.org/10.5281/zenodo.10265367,"Silvan David Peter, Institute of Computational Perception, Johannes Kepler University Linz, Austria","Symbolic Music Alignment is the process of matching performed MIDI notes to corresponding score notes. In this paper, we introduce a reinforcement learning (RL)- based online symbolic music alignment technique. The RL agent — an attention-based neural network — itera- tively estimates the current score position from local score and performance contexts. For this symbolic alignment task, environment states can be sampled exhaustively and the reward is dense, rendering a formulation as a simpli- fied offline RL problem straightforward. We evaluate the trained agent in three ways. First, in its capacity to identify correct score positions for sampled test contexts; second, as the core technique of a complete algorithm for symbolic online note-wise alignment; and finally, as a real-time sym- bolic score follower. We further investigate the pitch-based score and performance representations used as the agent's inputs. To this end, we develop a second model, a two- step Dynamic Time Warping (DTW)-based offline align- ment algorithm leveraging the same input representation. The proposed model outperforms a state-of-the-art refer- ence model of offline symbolic music alignment."
Oren Barkan;Shlomi Shvartzman;Noy Uzrad;Moshe Laufer;Almog Elharar;Noam Koenigstein,Inversynth II: Sound Matching via Self-Supervised Synthesizer-Proxy and Inference-Time Finetuning,2023,https://doi.org/10.5281/zenodo.10265371,"Oren Barkan, The Open Univesity of Israel;Shlomi Shvartzman, Tel Aviv University, Israel;Noy Uzrad, Tel Aviv University, Israel;Moshe Laufer, Tel Aviv University, Israel;Almog Elharar, Tel Aviv University, Israel;Noam Koenigstein, Tel Aviv University, Israel","Synthesizers are widely used electronic musical instruments. Given an input sound, inferring the underlying synthesizer's parameters to reproduce it is a difficult task known as sound-matching. In this work, we tackle the problem of automatic sound matching, which is otherwise performed manually by professional audio experts. The novelty of our work stems from the introduction of a novel differentiable synthesizer-proxy that enables gradient-based optimization by comparing the input and reproduced audio signals. Additionally, we introduce a novel self-supervised finetuning mechanism that further refines the prediction at inference time. Both contributions lead to state-of-the-art results, outperforming previous methods across various metrics. Our code is available at: https://github.com/inversynth/InverSynth2."
Amantur Amatov;Dmitry Lamanov;Maksim Titov;Ivan Vovk;Ilya Makarov;Mikhail Kudinov,A Semi-Supervised Deep Learning Approach to Dataset Collection for Query-by-Humming Task,2023,https://doi.org/10.5281/zenodo.10265375,"Amantur Amatov, Higher School of Economics, Moscow, Russia;Dmitry Lamanov, Huawei Noah’s Ark Lab, Moscow, Russia;Maksim Titov, Huawei Noah’s Ark Lab, Moscow, Russia;Ivan Vovk, Huawei Noah’s Ark Lab, Moscow, Russia;Ilya Makarov, AI Center, NUST MISiS, Moscow, Russia;Mikhail Kudinov, Huawei Noah’s Ark Lab, Moscow, Russia","Query-by-Humming (QbH) is a task that involves finding the most relevant song based on a hummed or sung fragment. Despite recent successful commercial solutions, implementing QbH systems remains challenging due to the lack of high-quality datasets for training machine learning models. In this paper, we propose a deep learning data collection technique and introduce Covers and Hummings Aligned Dataset (CHAD), a novel dataset that contains 18 hours of short music fragments, paired with time-aligned hummed versions. To expand our dataset, we employ a semi-supervised model training pipeline that leverages the QbH task as a specialized case of cover song identification (CSI) task. Starting with a model trained on the initial dataset, we iteratively collect groups of fragments of cover versions of the same song and retrain the model on the extended data. Using this pipeline, we collect over 308 hours of additional music fragments, paired with time-aligned cover versions. The final model is successfully applied to the QbH task and achieves competitive results on benchmark datasets. Our study shows that the proposed dataset and training pipeline can effectively facilitate the implementation of QbH systems."
Keren Shao;Ke Chen;Taylor Berg-Kirkpatrick;Shlomo Dubnov,Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction,2023,https://doi.org/10.5281/zenodo.10265373,"Keren Shao, University of California San Diego;Ke Chen, University of California San Diego;Taylor Berg-Kirkpatrick, University of California San Diego;Shlomo Dubnov, University of California San Diego","In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction."
Chin-Yun Yu;György Fazekas,Singing Voice Synthesis Using Differentiable LPC and Glottal-Flow-Inspired Wavetables,2023,https://doi.org/10.5281/zenodo.10265377,"Chin-Yun Yu, Centre for Digital Music, Queen Mary University of London, UK;György Fazekas, Centre for Digital Music, Queen Mary University of London, UK","This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for singing voice synthesis (SVS) that exploits the physical characteristics of the human voice using differentiable digital signal processing. GOLF employs a glottal model as the harmonic source and IIR filters to simulate the vocal tract, resulting in an interpretable and efficient approach. We show it is competitive with state-of-the-art singing voice vocoders, requiring fewer synthesis parameters and less memory to train, and runs an order of magnitude faster for inference. Additionally, we demonstrate that GOLF can model the phase components of the human voice, which has immense potential for rendering and analysing singing voices in a differentiable manner. Our results highlight the effectiveness of incorporating the physical properties of the human voice mechanism into SVS and underscore the advantages of signal-processing-based approaches, which offer greater interpretability and efficiency in synthesis."
Qiaoyu Yang;Frank Cwitkowitz;Zhiyao Duan,Harmonic Analysis With Neural Semi-CRF,2023,https://doi.org/10.5281/zenodo.10265379,"Qiaoyu Yang, University of Rochester;Frank Cwitkowitz, University of Rochester;Zhiyao Duan, University of Rochester","Automatic harmonic analysis of symbolic music is an important and useful task for both composers and listeners. The task consists of two components: recognizing harmony labels and finding their time boundaries. Most of the previous attempts focused on the first component, while time boundaries were rarely modeled explicitly. Lack of boundary modeling in the objective function could lead to segmentation errors. In this paper, we introduce a novel approach named Harana, to jointly detect the labels and boundaries of harmonic regions using neural semi-CRF (conditional random field). In contrast to rule-based scores used in traditional semi-CRF, a neural score function is proposed to incorporate features with more representational power. To improve the robustness of the model to imperfect harmony profiles, we design an additional score component to penalize the match between the candidate harmony label and the absent notes in the music. Quantitative results from our experiments demonstrate that the proposed approach improves segmentation quality as well as frame-level accuracy compared to previous methods."
Alberto Acquilino;Ninad Puranik;Ichiro Fujinaga;Gary Scavone,A Dataset and Baseline for Automated Assessment of Timbre Quality in Trumpet Sound,2023,https://doi.org/10.5281/zenodo.10265381,"Alberto Acquilino, Department of Music Research, Schulich School of Music, McGill University;Ninad Puranik, Department of Music Research, Schulich School of Music, McGill University;Ichiro Fujinaga, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT);Gary Scavone, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)","Music Performance Analysis is based on the evaluation of performance parameters such as pitch, dynamics, timbre, tempo and timing. While timbre is the least specific parameter among these and is often only implicitly understood, prominent brass pedagogues have reported that the presence of excessive muscle tension and inefficiency in playing by a musician is reflected in the timbre quality of the sound produced. In this work, we explore the application of machine learning to automatically assess timbre quality in trumpet playing, given both its educational value and connection to performance quality. An extensive dataset consisting of more than 19,000 tones played by 110 trumpet players of different expertise has been collected. A subset of 1,481 tones from this dataset was labeled by eight professional graders on a scale of 1 to 4 based on the perceived efficiency of sound production. Statistical analysis is performed to identify the correlation among the assigned ratings by the expert graders. A Random Forest classifier is trained using the mode of the ratings and its accuracy and variability is assessed with respect to the variability in human graders as a reference. An analysis of the important discriminatory features identifies stability of spectral peaks as a critical factor in trumpet timbre quality."
Frank Heyen;Quynh Quang Ngo;Michael Sedlmair,Visual Overviews for Sheet Music Structure,2023,https://doi.org/10.5281/zenodo.10265383,"Frank Heyen, VISUS, University of Stuttgart, Germany;Quynh Quang Ngo, VISUS, University of Stuttgart, Germany;Michael Sedlmair, VISUS, University of Stuttgart, Germany","We propose different methods for alternative representation and visual augmentation of sheet music that help users gain an overview of general structure, repeating patterns, and the similarity of segments. To this end, we explored mapping the overall similarity between sections or bars to colors. For these mappings, we use dimensionality reduction or clustering to assign similar segments to similar colors and vice versa. To provide a better overview, we further designed simplified music notation representations, including hierarchical and compressed encodings. These overviews allow users to display whole pieces more compactly on a single screen without clutter and to find and navigate to distant segments more quickly. Our preliminary evaluation with guitarists and tablature shows that our design supports users in tasks such as analyzing structure, finding repetitions, and determining the similarity of specific segments to others."
Luís Carvalho;Gerhard Widmer,Passage Summarization With Recurrent Models for Audio – Sheet Music Retrieval,2023,https://doi.org/10.5281/zenodo.10265385,"Luís Carvalho, Institute of Computational Perception & LIT Artiﬁcial Intelligence Lab, Johannes Kepler University Linz, Austria;Gerhard Widmer, Institute of Computational Perception & LIT Artiﬁcial Intelligence Lab, Johannes Kepler University Linz, Austria","Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo deviations. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio - sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music. We conduct a number of experiments on synthetic and real piano data and scores, showing that our proposed recurrent method leads to more accurate retrieval in all possible configurations."
Pedro Ramoneda;Jose J. Valero-Mas;Dasaem Jeong;Xavier Serra,Predicting Performance Difficulty From Piano Sheet Music Images,2023,https://doi.org/10.5281/zenodo.10265387,"Pedro Ramoneda, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Jose J. Valero-Mas, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Dasaem Jeong, MALer Lab, Sogang University, Seoul;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona","Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the music information retrieval community has recently shown interest in this task, existing approaches mainly use machine-readable scores, leaving the broader case of sheet music images unaddressed. Based on previous works involving sheet music images, we use a mid-level representation, bootleg score, describing notehead positions relative to staff lines coupled with a transformer model. This architecture is adapted to our task by introducing a different encoding scheme that reduces the encoded sequence length to one-eighth of the original size. In terms of evaluation, we consider five datasets---more than 7500 scores with up to 9 difficulty levels---, two being mainly compiled for this work. The results obtained when pretraining the scheme on the IMSLP corpus and fine-tuning it on the considered datasets prove the proposal's validity, achieving the best-performing model with a balanced accuracy of 40.3\% and a mean square error of 1.3. Finally, we provide access to our code, data, and models for transparency and reproducibility."
Junghyun Koo;Yunkee Chae;Chang-Bin Jeon;Kyogu Lee,Self-Refining of Pseudo Labels for Music Source Separation With Noisy Labeled Data,2023,https://doi.org/10.5281/zenodo.10265389,"Junghyun Koo, Seoul National University;Yunkee Chae, Seoul National University;Chang-Bin Jeon, Seoul National University;Kyogu Lee, Seoul National University","Music source separation (MSS) faces challenges due to limited availability and potential noise in correctly labeled individual instrument tracks. In this paper, we propose an automated approach for refining mislabeled instrument tracks in a partially noisy-labeled dataset. The proposed self-refining technique with noisy-labeled dataset results in only a 1% accuracy degradation for multi-label instrument recognition compared to a classifier trained with a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data for training MSS models and shows that utilizing the refined dataset for MSS leads to comparable results to a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on self-refined datasets even outperformed those trained on datasets refined with a classifier trained on clean labels."
Marcel A. Vélez Vásquez;Mariëlle Baelemans;Jonathan Driedger;Willem Zuidema;John Ashley Burgoyne,Quantifying the Ease of Playing Song Chords on the Guitar,2023,https://doi.org/10.5281/zenodo.10265391,"Marcel A. Vélez Vásquez, ILLC, University of Amsterdam, the Netherlands;Mariëlle Baelemans, ILLC, University of Amsterdam, the Netherlands;Jonathan Driedger, Chordify, Groningen, the Netherlands;Willem Zuidema, ILLC, University of Amsterdam, the Netherlands;John Ashley Burgoyne, ILLC, University of Amsterdam, the Netherlands","Quantifying the difficulty of playing songs has recently gained traction in the MIR community. While previous work has mostly focused on piano, this paper concentrates on rhythm guitar, which is especially popular with amateur musicians and has a broad skill spectrum. This paper proposes a rubric-based 'playability' metric to formalise this spectrum. The rubric comprises seven criteria that contribute to a single playability score, representing the overall difficulty of a song. The rubric was created through interviewing and incorporating feedback from guitar teachers and experts. Additionally, we introduce the playability prediction task by adding annotations to a subset of 200 songs from the McGill Billboard dataset, labelled by a guitar expert using the proposed rubric. We use this dataset to weight each rubric criterion for maximal reliability. Finally, we create a rule-based baseline to score each rubric criterion automatically from chord annotations and timings, and compare this baseline against simple deep learning models trained on chord symbols and textual representations of guitar tablature. The rubric, dataset, and baselines lay a foundation for understanding what makes songs easy or difficult for guitar players and how we can use MIR tools to match amateurs with songs closer to their skill level."
Irmak Bükey;Jason Zhang;TJ Tsai,FlexDTW: Dynamic Time Warping With Flexible Boundary Conditions,2023,https://doi.org/10.5281/zenodo.10265393,"Irmak Bükey, Pomona College;Jason Zhang, University of Michigan;TJ Tsai, Harvey Mudd College","Alignment algorithms like DTW and subsequence DTW assume specific boundary conditions on where an alignment path can begin and end in the cost matrix.  In practice, the boundary conditions may not be known a priori or may not satisfy such strict assumptions.  This paper introduces an alignment algorithm called FlexDTW that is designed to handle a wide range of boundary conditions.  FlexDTW allows alignment paths to start anywhere on the bottom or left edge of the cost matrix (adjacent to the origin) and to end anywhere on the top or right edge.  In order to properly compare paths of very different lengths, we use a goodness measure that normalizes the cumulative path cost by the path length.  The key insight of FlexDTW is that the Manhattan length of a path can be computed by simply knowing the starting point of the path, which can be computed recursively during dynamic programming.  We artificially generate a suite of 16 benchmarks based on the Chopin Mazurka dataset in order to characterize audio alignment performance under a variety of boundary conditions.  We show that FlexDTW has consistently strong performance that is comparable or better than commonly used alignment algorithms, and it is the only system with strong performance in some boundary conditions."
Alexandre D'Hooge;Louis Bigo;Ken Déguernel,Modeling Bends in Popular Music Guitar Tablatures,2023,https://doi.org/10.5281/zenodo.10265396,"Alexandre D’Hooge, Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France;Louis Bigo, ;Ken Déguernel, ","Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends. This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard.  In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 and a limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures."
Geoffroy Peeters,Self-Similarity-Based and Novelty-Based Loss for Music Structure Analysis,2023,https://doi.org/10.5281/zenodo.10265397,"Geoffroy Peeters, LTCI, Télécom-Paris, Institut Polytechnique de Paris, France","Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity.  In this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels.  For this we jointly optimize  - a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and  - a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss.  We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA.  Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI."
Carey Bunks;Tillman Weyde;Simon Dixon;Bruno Di Giorgi,Modeling Harmonic Similarity for Jazz Using Co-occurrence Vectors and the Membrane Area,2023,https://doi.org/10.5281/zenodo.10265400,"Carey Bunks, Queen Mary University of London, UK;Tillman Weyde, City, University of London, UK;Simon Dixon, Queen Mary University of London, UK;Bruno Di Giorgi, Apple, UK","In jazz, measuring harmonic similarity is complicated by the common practice of reharmonization -- the altering or substitution of chords without fundamentally changing the piece's harmonic identity. This is analogous to natural language processing tasks where synonymous terms can be used interchangeably without significantly modifying the meaning of a text.  Our approach to modeling harmonic similarity borrows from NLP techniques, such as distributional semantics, by embedding chords into a vector space using a co-occurrence matrix.  We show that the method can robustly detect harmonic similarity between songs, even when reharmonized.  The co-occurrence matrix is computed from a corpus of symbolic jazz-chord progressions, and the result is a map from chords into vectors. A song's harmony can then be represented as a piecewise-linear path constructed from the cumulative sum of its chord vectors.  For any two songs, their harmonic similarity can be measured as the minimal surface membrane area between their vector paths.  Using a dataset of jazz contrafacts, we show that our approach reduces the median rank of matches from 318 to 18 compared to a baseline approach using pitch class vectors."
Shuqi Dai;Yuxuan Wu;Siqi Chen;Roy Huang;Roger B. Dannenberg,SingStyle111: A Multilingual Singing Dataset With Style Transfer,2023,https://doi.org/10.5281/zenodo.10265401,"Shuqi Dai, Computer Science Department, Carnegie Mellon University , USA;Yuxuan Wu, Computer Science Department, Carnegie Mellon University , USA;Siqi Chen, University of Southern California, USA;Roy Huang, Computer Science Department, Carnegie Mellon University , USA;Roger B. Dannenberg, Computer Science Department, Carnegie Mellon University , USA","There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111."
Haven Kim;Kento Watanabe;Masataka Goto;Juhan Nam,A Computational Evaluation Framework for Singable Lyric Translation,2023,https://doi.org/10.5281/zenodo.10265405,"Haven Kim, Graduate School of Culture Technology, KAIST, South Korea;Kento Watanabe, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Juhan Nam, Graduate School of Culture Technology, KAIST, South Korea","Lyric translation plays a pivotal role in amplifying the global resonance of music, bridging cultural divides, and fostering universal connections. Translating lyrics, unlike conventional translation tasks, requires a delicate balance between singability and semantics. In this paper, we present a computational framework for the quantitative evaluation of singable lyric translation, which seamlessly integrates musical, linguistic, and cultural dimensions of lyrics. Our comprehensive framework consists of four metrics that measure syllable count distance, phoneme repetition similarity, musical structure distance, and semantic similarity. To substantiate the efficacy of our framework, we collected a singable lyrics dataset, which precisely aligns English, Japanese, and Korean lyrics on a line-by-line and section-by-section basis, and conducted a comparative analysis between singable and non-singable lyrics. Our multidisciplinary approach provides insights into the key components that underlie the art of lyric translation and establishes a solid groundwork for the future of computational lyric translation assessment."
Kosetsu Tsukuda;Masahiro Hamasaki;Masataka Goto,Chorus-Playlist: Exploring the Impact of Listening to Only Choruses in a Playlist,2023,https://doi.org/10.5281/zenodo.10265403,"Kosetsu Tsukuda, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masahiro Hamasaki, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","When people listen to playlists on a music streaming service, they typically listen to each song from start to end in order. However, what if it were possible to use a function to listen to only the choruses of each song in a playlist one after another? In this paper, we call this music listening concept ""chorus-playlist,"" and we investigate its potential impact from various perspectives such as the demand and the objectives for listening to music with chorus-playlist. To this end, we conducted a questionnaire-based online user survey involving 214 participants. Our analysis results suggest reusable insights, including the following: (1) We show a high demand for listening to existing playlists with the chorus-playlist approach. We also reveal preferred options for chorus playback, such as adding crossfade transitions between choruses. (2) People listen to playlists with chorus-playlist for various objectives. For example, when they listen to their own self-made playlists, they want to boost a mood or listen to music in a specific context such as work or driving. (3) There is also a high demand for playlist creation on the premise of continuous listening to only the choruses of the songs in a playlist. The diversities of artists, genres, and moods are more important when creating such a playlist than when creating a usual playlist."
David Lewis;Elisabete Shibata;Andrew Hankinson;Johannes Kepper;Kevin R. Page;Lisa Rosendahl;Mark Saccomano;Christine Siegert,Supporting Musicological Investigations With Information Retrieval Tools: An Iterative Approach to Data Collection,2023,https://doi.org/10.5281/zenodo.10265407,"David Lewis, University of Oxford, UK;Elisabete Shibata, BeethovenHaus Bonn, Germany;Andrew Hankinson, RISM Digital Centre, Switzerland;Johannes Kepper, Paderborn University, Germany;Kevin R. Page, University of Oxford, UK;Lisa Rosendahl, BeethovenHaus Bonn, Germany;Mark Saccomano, Paderborn University, Germany;Christine Siegert, BeethovenHaus Bonn, Germany","Digital musicology research often proceeds by extending and enriching its evidence base as it progresses, rather than starting with a complete corpus of data and metadata, as a consequence of an emergent research need.  In this paper, we consider a research workflow which assumes an incremental approach to data gathering and annotation. We describe tooling which implements parts of this workflow, developed to support the study of nineteenth-century music arrangements, and evaluate the applicability of our approach through interviews with musicologists and music editors who have used the tools. We conclude by considering extensions of this approach and the wider implications for digital musicology and music information retrieval."
Federico Simonetta;Ana Llorens;Martín Serrano;Eduardo García-Portugués;Álvaro Torrente,Optimizing Feature Extraction for Symbolic Music,2023,https://doi.org/10.5281/zenodo.10265409,"Federico Simonetta, ICCMU - Instituto Complutense de Ciencias Musicales, Madrid;Ana Llorens, Universidad Complutense de Madrid, Madrid;Martín Serrano, ICCMU - Instituto Complutense de Ciencias Musicales, Madrid;Eduardo García-Portugués, Universidad Carlos III, Madrid;Álvaro Torrente, ICCMU - Instituto Complutense de Ciencias Musicales, Madrid, Universidad Complutense de Madrid, Madrid","This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development. The proposed tool also enhances classification accuracy when combined with other sets of features. We demonstrate the contribution of each set of features and the computational resources they require. Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one. To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks."
Mathias Rose Bjare;Stefan Lattner;Gerhard Widmer,Exploring Sampling Techniques for Generating Melodies With a Transformer Language Model,2023,https://doi.org/10.5281/zenodo.10265411,"Mathias Rose Bjare, Institute of Computational Perception, Johannes Kepler University Linz, Austria;Stefan Lattner, Sony Computer Science Laboratories (CSL), Paris, France;Gerhard Widmer, Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria","Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed ""typical sampling"", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances."
John Ashley Burgoyne;Janne Spijkervet;David John Baker,Measuring the Eurovision Song Contest: A Living Dataset for Real-World MIR,2023,https://doi.org/10.5281/zenodo.10265415,"John Ashley Burgoyne, University of Amsterdam;Janne Spijkervet, ByteDance;David John Baker, University of Amsterdam","Every year, several dozen, primarily European, countries, send performers to compete on live television at the Eurovision Song Contest, with the goal of entertaining an international audience of more than 150 million viewers. Each participating country is able to evaluate every other country's performance via a combination of rankings from professional jurors and telephone votes from viewers. Between fan sites and the official Song Contest organisation, a complete historical record of musical performances and country-to-country contest scores is available, back to the very first edition in 1956, and for the most recent contests, there is also information about each individual juror's rankings. In this paper, we introduce MIRoVision, a set of scripts which collates the data from these sources into a single, easy-to-use dataset, and a discrete-choice model to convert the raw contest scores into a stable, interval-scale measure of the quality of Eurovision Song Contest entries across the years. We use this model to simulate contest outcomes from previous editions and compare the results to the implied win probabilities from bookmakers at various online betting markets. We also assess how successful content-based MIR could be at predicting Eurovision outcomes, using state-of-the-art music foundation models. Given its annual recurrence, emphasis on new music and lesser-known artists, and sophisticated voting structure, the Eurovision Song Contest is an outstanding testing ground for MIR algorithms, and we hope that this paper will inspire the community to use the contest as a regular assessment of the strength of modern MIR."
Pablo Alonso-Jiménez;Xavier Serra;Dmitry Bogdanov,Efficient Supervised Training of Audio Transformers for Music Representation Learning,2023,https://doi.org/10.5281/zenodo.10265414,"Pablo Alonso-Jiménez, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona;Dmitry Bogdanov, Music Technology Group, Universitat Pompeu Fabra, Barcelona","In this work, we address music representation learning using convolution-free transformers. We build on top of existing spectrogram-based audio transformers such as AST and train our models on a supervised task using patchout training similar to PaSST. In contrast to previous works, we study how specific design decisions affect downstream music tagging tasks instead of focusing on the training task. We assess the impact of initializing the training with different existing weights, using various input audio segment lengths, using learned representations from different blocks and tokens of the transformer for downstream tasks, and applying patchout at inference to speed up feature extraction. We find that 1) initializing the audio training from ImageNet or AudioSet weights and longer input segments are beneficial both for the training and downstream tasks, 2) the best representations for the downstream tasks are located in the middle blocks of the transformer, and 3) using patchout at inference allows faster processing than our convolutional baselines while maintaining superior performance. The resulting models, MAEST, are publicly available and obtain the best performance among open models in music tagging tasks."
Michael Krause;Christof Weiß;Meinard Müller,A Cross-Version Approach to Audio Representation Learning for Orchestral Music,2023,https://doi.org/10.5281/zenodo.10265419,"Michael Krause, International Audio Laboratories Erlangen, Germany;Christof Weiß, University of Würzburg, Germany;Meinard Müller, International Audio Laboratories Erlangen, Germany","Deep learning systems have become popular for tackling a variety of music information retrieval tasks. However, these systems often require large amounts of labeled data for supervised training, which can be very costly to obtain. To alleviate this problem, recent papers on learning music audio representations employ alternative training strategies that utilize unannotated data. In this paper, we introduce a novel cross-version approach to audio representation learning that can be used with music datasets containing several versions (performances) of a musical work. Our method exploits the correspondences that exist between two versions of the same musical section. We evaluate our proposed cross-version approach qualitatively and quantitatively on complex orchestral music recordings and show that it can better capture aspects of instrumentation compared to techniques that do not use cross-version information."
Tomoyasu Nakano;Masataka Goto,"Music Source Separation With MLP Mixing of Time, Frequency, and Channel",2023,https://doi.org/10.5281/zenodo.10265417,"Tomoyasu Nakano, National Institute of Advanced Industrial Science and Technology (AIST), Japan;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper proposes a new music source separation (MSS) model based on an architecture with MLP-Mixer that leverages multilayer perceptrons (MLPs). Most of the recent MSS techniques are based on architectures with CNNs, RNNs, and attention-based transformers that take waveforms or complex spectrograms or both as inputs. For the growth of the research field, we believe it is important to study not only the current established methodologies but also diverse perspectives. Therefore, since the MLP-Mixer-based architecture has been reported to perform as well as or better than architectures with CNNs and transformers in the computer vision field despite the MLP's simple computation, we report a way to effectively apply such an architecture to MSS as a reusable insight. In this paper we propose a model called TFC-MLP, which is a variant of the MLP-Mixer architecture that preserves time-frequency positional relationships and mixes time, frequency, and channel dimensions separately, using complex spectrograms as input. The TFC-MLP was evaluated with source-to-distortion ratio (SDR) using the MUSDB18-HQ dataset. Experimental results showed that the proposed model can achieve competitive SDRs when compared with state-of-the-art MSS models."
Huan Zhang;Emmanouil Karystinaios;Simon Dixon;Gerhard Widmer;Carlos Eduardo Cancino-Chacón,Symbolic Music Representations for Classification Tasks: A Systematic Evaluation,2023,https://doi.org/10.5281/zenodo.10265421,"Huan Zhang, Queen Mary University of London, United Kingdom;Emmanouil Karystinaios, Johannes Kepler University, Austra;Simon Dixon, Queen Mary University of London, United Kingdom;Gerhard Widmer, Johannes Kepler University, Austra;Carlos Eduardo Cancino-Chacón, Johannes Kepler University, Austra","Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language-like fashion. However, symbolic music is neither an image nor a sentence intrinsically, and research in the symbolic domain is lacking a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation."
Jacopo de Berardinis;Valentina Anita Carriero;Albert Meroño-Peñuela;Andrea Poltronieri;Valentina Presutti,The Music Meta Ontology: A Flexible Semantic Model for the Interoperability of Music Metadata,2023,https://doi.org/10.5281/zenodo.10265423,"Jacopo de Berardinis, King’s College London, UK;Valentina Anita Carriero, University of Bologna, Italy;Albert Meroño-Penuela, King’s College London, UK;Andrea Poltronieri, University of Bologna, Italy;Valentina Presutti, University of Bologna, Italy","The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity of musical concepts arising from different genres, styles, and periods – standing to benefit from a lingua franca to accommodate various stakeholders (musicologists, librarians, data engineers, etc.). To initiate this transition, we introduce the Music Meta ontology, a rich and flexible semantic model to describe music metadata related to artists, compositions, performances, recordings, and links. We follow eXtreme Design methodologies and best practices for data engineering, to reflect the perspectives and the requirements of various stakeholders into the design of the model, while leveraging ontology design patterns and accounting for provenance at different levels (claims, links). After presenting the main features of Music Meta, we provide a first evaluation of the model, alignments to other schema (Music Ontology, DOREMUS, Wikidata), and support for data transformation."
Jeff Miller;Johan Pauwels;Mark Sandler,Polar Manhattan Displacement: Measuring Tonal Distances Between Chords Based on Intervallic Content,2023,https://doi.org/10.5281/zenodo.10265427,"Jeff Miller, Queen Mary University of London;Johan Pauwels, Centre for Digital Music;Mark Sandler, Queen Mary University of London","Large-scale studies of musical harmony are often hampered by lack of suitably labelled data. It would be highly advantageous if an algorithm were able to autonomously describe chords, scales, etc. in a consistent and musically informative way. In this paper, we revisit tonal interval vectors (TIVs), which reveal certain insights as to the interval and tonal nature of pitch class sets. We then describe the qualities and criteria required to comprehensively and consistently measure displacements between TIVs. Next, we present the Polar Manhattan Displacement (PMD), a compound magnitude and phase measure for describing the displacements between pitch class sets in a tonally-informed manner. We end by providing examples of how PMD can be used in automated harmonic sequence analysis over a complex chord vocabulary."
