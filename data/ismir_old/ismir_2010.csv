Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Tom Collins;Jeremy Thurlow;Robin C. Laney;Alistair Willis;Paul H. Garthwaite,A Comparative Evaluation of Algorithms for Discovering Translational Patterns in Baroque Keyboard Works.,2010,https://doi.org/10.5281/zenodo.1416070,"Tom Collins, The Open University, UK, education;Jeremy Thurlow, University of Cambridge, UK, education;Robin Laney, The Open University, UK, education;Alistair Willis, The Open University, UK, education;Paul H. Garthwaite, The Open University, UK, education","We consider the problem of intra-opus pattern discovery, that is, the task of discovering patterns of a speciﬁed type within a piece of music. A music analyst undertook this task for works by Domenico Scarlattti and Johann Sebastian Bach, forming a benchmark of ‘target’ patterns. The performance of two existing algorithms and one of our own creation, called SIACT, is evaluated by comparison with this benchmark. SIACT out-performs the existing algo- rithms with regard to recall and, more often than not, pre- cision. It is demonstrated that in all but the most care- fully selected excerpts of music, the two existing algo- rithms can be affected by what is termed the ‘problem of isolated membership’. Central to the relative success of SIACT is our intention that it should address this particu- lar problem. The paper contrasts string-based and geomet- ric approaches to pattern discovery, with an introduction to the latter. Suggestions for future work are given."
1,Verena Konz;Meinard Müller;Sebastian Ewert,A Multi-Perspective Evaluation Framework for Chord Recognition.,2010,https://doi.org/10.5281/zenodo.1415686,"Verena Konz, Saarland University, DEU, education, MPI Informatik, DEU, facility;Meinard Müller, Saarland University, DEU, education, MPI Informatik, DEU, facility;Sebastian Ewert, University of Bonn, DEU, education","The automated extraction of chord labels from audio recordings constitutes a major task in music information retrieval. To evaluate computer-based chord labeling procedures, one requires ground truth annotations for the underlying audio material. However, the manual generation of such annotations on the basis of audio recordings is tedious and time-consuming. On the other hand, trained musicians can easily derive chord labels from symbolic score data. In this paper, we bridge this gap by describing a procedure that allows for transferring annotations and chord labels from the score domain to the audio domain and vice versa. Using music synchronization techniques, the general idea is to locally warp the annotations of all given data streams onto a common time axis, which then allows for a cross-domain evaluation of the various types of chord labels. As a further contribution of this paper, we extend this principle by introducing a multi-perspective evaluation framework for simultaneously comparing chord recognition results over multiple performances of the same piece of music. The revealed inconsistencies in the results do not only indicate limitations of the employed chord labeling strategies but also deepen the understanding of the underlying music material."
2,Riccardo Miotto;Nicola Orio,A Probabilistic Approach to Merge Context and Content Information for Music Retrieval.,2010,https://doi.org/10.5281/zenodo.1415256,"Riccardo Miotto, University of Padova, ITA, education;Nicola Orio, University of Padova, ITA, education","An interesting problem in music information retrieval is how to combine the information from different sources in order to improve retrieval effectiveness. This paper introduces an approach to represent a collection of tagged songs through an hidden Markov model with the purpose to develop a system that merges in the same framework both acoustic similarity and semantic descriptions. The former provides content-based information on song similarity, the latter provides context-aware information about individual songs. Experimental results show how the proposed model leads to better performances than approaches that rank songs using both a single information source and a their linear combination."
3,Graham Grindlay;Daniel P. W. Ellis,A Probabilistic Subspace Model for Multi-instrument Polyphonic Transcription.,2010,https://doi.org/10.5281/zenodo.1416842,"Graham Grindlay, Columbia University, USA, education;Daniel P.W. Ellis, Columbia University, USA, education","In this paper we present a general probabilistic model suitable for transcribing single-channel audio recordings containing multiple polyphonic sources. Our system requires no prior knowledge of the instruments in the mixture, although it can benefit from this information if available. In contrast to many existing polyphonic transcription systems, our approach explicitly models the individual instruments and is thereby able to assign detected notes to their respective sources. We use a set of training instruments to learn a model space which is then used during transcription to constrain the properties of models fit to the target mixture. In addition, we encourage model sparsity using a simple approach related to tempering. We evaluate our method on both recorded and synthesized two-instrument mixtures, obtaining average frame-level F-measures of up to 0.60 for synthesized audio and 0.53 for recorded audio. If knowledge of the instrument types in the mixture is available, we can increase these measures to 0.68 and 0.58, respectively, by initializing the model with parameters from similar instruments."
4,Maxime Le Coz;Hélène Lachambre;Lionel Koenig;Régine André-Obrecht,A Segmentation-based Tempo Induction Method.,2010,https://doi.org/10.5281/zenodo.1416766,"Maxime Le Coz, IRIT, Universite Paul Sabatier, FRA, education;Helene Lachambre, IRIT, Universite Paul Sabatier, FRA, education;Lionel Koenig, IRIT, Universite Paul Sabatier, FRA, education;Regine Andre-Obrecht, IRIT, Universite Paul Sabatier, FRA, education","The automatized beat detection and localization have been the subject of multiple research in the ﬁeld of music infor- mation retrieval. Most of the methods are based on onset detection. We propose an alternative approach: Our method is based on the “Forward-Backward seg- mentation”: the segments may be interpreted as attacks, decays, sustains and releases of notes. We process the seg- ment boundaries as a weighted Dirac signal. Three meth- ods devived from its spectral analysis are proposed to ﬁnd a periodicity which corresponds to the tempo. The experiments are carried out on a corpus of 100 songs of the RWC database. The performances of our system on this base demonstrate a potential in the use of a “ Forward- Backward Segmentation” for temporal information retrieval in musical signals."
5,Igor Vatolkin;Wolfgang M. Theimer;Martin Botteck,AMUSE (Advanced MUSic Explorer) - A Multitool Framework for Music Data Analysis.,2010,https://doi.org/10.5281/zenodo.1414918,"Igor Vatolkin, TU Dortmund, DEU, education;Wolfgang Theimer, Research in Motion, Bochum, DEU, company","A large variety of research tools is available now for music information retrieval tasks. In this paper we present a further framework which aims to facilitate the interaction between these applications. Since the available tools are very different in target domain, range of available methods, learning efforts, installation and runtime characteristics etc., it is not easy to find software which is optimal for certain research goals. Another problematic issue is that many incompatible data formats exist, so it is not always possible to use output from one tool just as input for another one. At first we describe some of the available projects and outline our motivation starting the development of AMUSE framework for audio data analysis. Requirements and application purposes are given. The structure of our framework is introduced in detail and the information for efficient application is provided. Finally we discuss several ideas for further work."
6,Cyril Joder;Slim Essid;Gaël Richard,An Improved Hierarchical Approach for Music-to-symbolic Score Alignment.,2010,https://doi.org/10.5281/zenodo.1417883,"Cyril Joder, Institut TELECOM, TELECOM ParisTech, CNRS LTCI, education;Slim Essid, Institut TELECOM, TELECOM ParisTech, CNRS LTCI, education;Gaël Richard, Institut TELECOM, TELECOM ParisTech, CNRS LTCI, education","We present an efﬁcient approach for an off-line alignment of a symbolic score to a recording of the same piece, using a statistical model. A hidden state model is built from the score, which allows for the use of two different kinds of features, namely chroma vectors and an onset detection function (spectral ﬂux) with speciﬁc production models, in a simple manner. We propose a hierarchical pruning method for an approximate decoding of this statistical model. This strategy reduces the search space in an adaptive way, yielding a better overall efﬁciency than the tested state-of-the art method. Experiments run on a large database of 94 pop songs show that the resulting system obtains higher recognition rates than the dynamic programming algorithm (DTW), with a signiﬁcantly lower complexity, even though the rhythmic information is not used for the alignment."
7,Chung-Che Wang;Jyh-Shing Roger Jang;Wennen Wang,An Improved Query by Singing/Humming System Using Melody and Lyrics Information.,2010,https://doi.org/10.5281/zenodo.1414802,"Chung-Che Wang, Tsing Hua Univ., TWN, education;Jyh-Shing Roger Jang, Tsing Hua Univ., TWN, education;Wennen Wang, Institute for Information Industry, TWN, facility","""This paper proposes an improved query by singing/humming (QBSH) system using both melody and lyrics information for achieving better performance. Singing/humming discrimination (SHD) is first performed to distinguish singing from humming queries. For a humming query, we apply a pitch-only melody recognition method that has been used for QBSH task at MIREX with rank-1 performance. For a singing query, we combine the scores from melody recognition and lyrics recognition to take advantage of the extra lyrics information. Lyrics recognition is based on a modified tree lexicon that is commonly used in speech recognition. The performance of the overall QBSH system achieves 39.01% and 23.53% error reduction rates, respectively, for top-20 recognition under two experimental settings, indicating the feasibility of the proposed method."""
8,Andrew Hankinson;Laurent Pugin;Ichiro Fujinaga,An Interchange Format for Optical Music Recognition Applications.,2010,https://doi.org/10.5281/zenodo.1417633,"Andrew Hankinson, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, CAN, education;Laurent Pugin, RISM Switzerland & Geneva University, CHE, education;Ichiro Fujinaga, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, CAN, education","Page appearance and layout for music notation is a critical component of the overall musical information contained in a document. To capture and transfer this information, we outline an interchange format for OMR applications, the OMR Interchange Package (OIP) format, which is designed to allow layout information and page images to be preserved and transferred along with semantic musical content. We identify a number of uses for this format that can enhance digital representations of music, and introduce a novel idea for distributed optical music recognition system based on this format."
9,Dingding Wang 0001;Tao Li 0001;Mitsunori Ogihara,Are Tags Better Than Audio? The Effect of Joint Use of Tags and Audio Content Features for Artistic Style Clustering.,2010,https://doi.org/10.5281/zenodo.1417543,"Dingding Wang, Florida International University, USA, education;Tao Li, Florida International University, USA, education;Mitsunori Ogihara, University of Miami, USA, education","Social tags are receiving growing interests in informa- tion retrieval. In music information retrieval previous re- search has demonstrated that tags can assist in music clas- siﬁcation and clustering. This paper studies the problem of combining tags and audio contents for artistic style clus- tering. After studying the effectiveness of using tags and audio contents separately for clustering, this paper pro- poses a novel language model that makes use of both data sources. Experiments with various methods for combining feature sets demonstrate that tag features are more useful than audio content features for style clustering and that the proposed model can marginally improve clustering perfor- mance by combing tags and audio contents."
10,Shintaro Funasawa;Hiromi Ishizaki;Keiichiro Hoashi;Yasuhiro Takishima;Jiro Katto,Automated Music Slideshow Generation Using Web Images Based on Lyrics.,2010,https://doi.org/10.5281/zenodo.1415996,"Shintaro Funasawa, Waseda University, JPN, education;Jiro Katto, Waseda University, JPN, education;Hiromi Ishizaki, KDDI R&D Laboratories Inc., JPN, company;Keiichiro Hoashi, KDDI R&D Laboratories Inc., JPN, company;Yasuhiro Takishima, KDDI R&D Laboratories Inc., JPN, company","In this paper, we propose a system which automatically generates slideshows for music, by utilizing images retrieved from photo sharing web sites, based on query words extracted from song lyrics. The proposed system consists of two major steps: (1) query extraction from song lyrics, (2) image selection from web image search results. Moreover, in order to improve the display duration of each image in the slideshow, we adjust image transition timing by analyzing the duration of each lyric line in the input song. We have conducted subjective evaluation experiments, which prove that the proposal can generate impressive music slideshows for any input song."
11,Eric Humphrey,Automatic Characterization of Digital Music for Rhythmic Auditory Stimulation.,2010,https://doi.org/10.5281/zenodo.1418135,"Eric Humphrey, University of Miami, USA, education","A computational rhythm analysis system is proposed to characterize the suitability of musical recordings for rhythmic auditory stimulation, a neurologic music therapy technique that uses rhythm to entrain periodic physical motion. Current applications of RAS are limited by the general inability to take advantage of the enormous amount of digital music that exists today. The system aims to identify motor-rhythmic music for the entrainment of neuromuscular activity for rehabilitation and exercise, motivating the concept of musical “use-genres.” This work builds upon prior research in meter and tempo analysis to establish a representation of rhythm chroma and alternatively describe beat spectra."
12,Menno van Zaanen;Pieter Kanters,Automatic Mood Classification Using TF*IDF Based on Lyrics.,2010,https://doi.org/10.5281/zenodo.1417287,"Menno van Zaanen, Tilburg University, NLD, education;Pieter Kanters, Tilburg University, NLD, education","This paper presents the outcomes of research into using lingual parts of music in an automatic mood classification system. Using a collection of lyrics and corresponding user-tagged moods, we build classifiers that classify lyrics of songs into moods. By comparing the performance of different mood frameworks (or dimensions), we examine to what extent the linguistic part of music reveals adequate information for assigning a mood category and which aspects of mood can be classified best. Our results show that word oriented metrics provide a valuable source of information for automatic mood classification of music, based on lyrics only. Metrics such as term frequencies and tf*idf values are used to measure relevance of words to the different mood classes. These metrics are incorporated in a machine learning classifier setup. Different partitions of the mood plane are investigated and we show that there is no large difference in mood prediction based on the mood division. Predictions on the valence, tension and combinations of aspects lead to similar performance."
13,Emanuele Coviello;Luke Barrington;Antoni B. Chan;Gert R. G. Lanckriet,Automatic Music Tagging With Time Series Models.,2010,https://doi.org/10.5281/zenodo.1415274,"Emanuele Coviello, University of California, San Diego, USA, education;Luke Barrington, University of California, San Diego, USA, education;Antoni B. Chan, City University of Hong Kong, HKG, education;Gert. R. G. Lanckriet, University of California, San Diego, USA, education","State-of-the-art systems for automatic music tagging model music based on bag-of-feature representations which give little or no account of temporal dynamics, a key characteristic of the audio signal. We describe a novel approach to automatic music annotation and retrieval that captures temporal (e.g., rhythmical) aspects as well as timbral content. The proposed approach leverages a recently proposed song model that is based on a generative time series model of the musical content — the dynamic texture mixture (DTM) model — that treats fragments of audio as the output of a linear dynamical system. To model characteristic temporal dynamics and timbral content at the tag level, a novel, efﬁcient hierarchical EM algorithm for DTM (HEM-DTM) is used to summarize the common information shared by DTMs modeling individual songs associated with a tag. Experiments show learning the semantics of music beneﬁts from modeling temporal dynamics."
14,Halfdan Rump;Shigeki Miyabe;Emiru Tsunoo;Nobutaka Ono;Shigeki Sagayama,Autoregressive MFCC Models for Genre Classification Improved by Harmonic-percussion Separation.,2010,https://doi.org/10.5281/zenodo.1418239,"Halfdan Rump, The University of Tokyo, Graduate School of Information Science and Technology, JPN, education;Shigeki Miyabe, The University of Tokyo, Graduate School of Information Science and Technology, JPN, education;Emiru Tsunoo, The University of Tokyo, Graduate School of Information Science and Technology, JPN, education;Nobukata Ono, The University of Tokyo, Graduate School of Information Science and Technology, JPN, education;Shigeki Sagama, The University of Tokyo, Graduate School of Information Science and Technology, JPN, education","In this work we improve accuracy of MFCC-based genre classification by using the Harmonic-Percussion Signal Separation (HPSS) algorithm on the music signal, and then calculate the MFCCs on the separated signals. The choice of the HPSS algorithm was mainly based on the observation that the presence of harmonics causes the high MFCCs to be noisy. A multivariate autoregressive (MAR) model was trained on the improved MFCCs, and performance in the task of genre classification was evaluated. By combining features calculated on the separated signals, relative error rate reductions of 20% and 16.2% were obtained when an SVM classifier was trained on the MFCCs and MAR features respectively. Next, by analyzing the MAR features calculated on the separated signals, it was concluded that the original signal contained some information which the MAR model was capable of handling, and that the best performance was obtained when all three signals were used. Finally, by choosing the number of MFCCs from each signal type to be used in the autoregressive modelling, it was verified that the best performance was reached when the high MFCCs calculated on the harmonic signal were discarded."
15,Jakob Abeßer;Paul Bräuer;Hanna M. Lukashevich;Gerald Schuller,Bass Playing Style Detection Based on High-level Features and Pattern Similarity.,2010,https://doi.org/10.5281/zenodo.1418213,"Jakob Abeßer, Fraunhofer IDMT, DEU, facility;Paul Br¨auer, Piranha Musik & IT, DEU, company;Hanna Lukashevich, Fraunhofer IDMT, DEU, facility;Gerald Schuller, Fraunhofer IDMT, DEU, facility","In this paper, we compare two approaches for automatic classiﬁcation of bass playing styles, one based on high- level features and another one based on similarity mea- sures between bass patterns. For both approaches, we com- pare two different strategies: classiﬁcation of patterns as a whole and classiﬁcation of all measures of a pattern with a subsequent accumulation of the classiﬁcation results. Fur- thermore, we investigate the inﬂuence of potential tran- scription errors on the classiﬁcation accuracy, which tend to occur when real audio data is analyzed. We achieve best classiﬁcation accuracy values of 60.8% for the feature-based classiﬁcation and 68.5% for the classiﬁca- tion based on pattern similarity based on a taxonomy con- sisting of 8 different bass playing styles."
16,Leigh M. Smith,Beat Critic: Beat Tracking Octave Error Identification By Metrical Profile Analysis.,2010,https://doi.org/10.5281/zenodo.1417891,"Leigh M. Smith, IRCAM, facility","Computational models of beat tracking of musical au-
dio have been well explored, however, such systems often
make “octave errors”, identifying the beat period at dou-
ble or half the beat rate than that actually recorded in the
music. A method is described to detect if octave errors
have occurred in beat tracking. Following an initial beat
tracking estimation, a feature vector of metrical proﬁle sep-
arated by spectral subbands is computed. A measure of
subbeat quaver (1/8th note) alternation is used to compare
half time and double time measures against the initial beat
track estimation and indicate a likely octave error. This er-
ror estimate can then be used to re-estimate the beat rate.
The performance of the approach is evaluated against the
RWC database, showing successful identiﬁcation of octave
errors for an existing beat tracker. Using the octave error
detector together with the existing beat tracking model im-
proved beat tracking by reducing octave errors to 43% of
the previous error rate."
17,Qi Lu;Xiaoou Chen;Deshun Yang;Jun Wang,Boosting for Multi-Modal Music Emotion Classification.,2010,https://doi.org/10.5281/zenodo.1417085,"Qi Lu, Peking University, Institute of Computer Science & Technology, CHN, education;Xiaoou Chen, Peking University, Institute of Computer Science & Technology, CHN, education;Deshun Yang, Peking University, Institute of Computer Science & Technology, CHN, education;Jun Wang, Peking University, Institute of Computer Science & Technology, CHN, education","""With the explosive growth of music recordings, automatic classification of music emotion becomes one of the hot spots on research and engineering. Typical music emotion classification (MEC) approaches apply machine learning methods to train a classifier based on audio features. In addition to audio features, the MIDI and lyrics features of music also contain useful semantic information for predicting the emotion of music. In this paper we apply AdaBoost algorithm to integrate MIDI, audio and lyrics information and propose a two-layer classifying strategy called Fusion by Subtask Merging for 4-class music emotion classification. We evaluate each modality respectively using SVM, and then combine any two of the three modalities, using AdaBoost algorithm (MIDI+audio, MIDI+lyrics, audio+lyrics). Moreover, integrating this in a multimodal system (MIDI+audio+lyrics) allows an improvement in the overall performance. The experimental results show that MIDI, audio and lyrics information are complementary, and can be combined to improve a classification system."""
18,Thierry Bertin-Mahieux;Ron J. Weiss;Daniel P. W. Ellis,Clustering Beat-Chroma Patterns in a Large Music Database.,2010,https://doi.org/10.5281/zenodo.1416720,"Thierry Bertin-Mahieux, Columbia University, USA, education;Ron J. Weiss, New York University, USA, education;Daniel P. W. Ellis, Columbia University, USA, education","A musical style or genre implies a set of common con-
ventions and patterns combined and deployed in different
ways to make individual musical pieces; for instance, most
would agree that contemporary pop music is assembled
from a relatively small palette of harmonic and melodic
patterns. The purpose of this paper is to use a database
of tens of thousands of songs in combination with a com-
pact representation of melodic-harmonic content (the beat-
synchronous chromagram) and data-mining tools (cluster-
ing) to attempt to explicitly catalog this palette – at least
within the limitations of the beat-chroma representation.
We use online k-means clustering to summarize 3.7 mil-
lion 4-beat bars in a codebook of a few hundred prototypes.
By measuring how accurately such a quantized codebook
can reconstruct the original data, we can quantify the de-
gree of diversity (distortion as a function of codebook size)
and temporal structure (i.e. the advantage gained by joint
quantizing multiple frames) in this music. The most popu-
lar codewords themselves reveal the common chords used
in the music. Finally, the quantized representation of mu-
sic can be used for music retrieval tasks such as artist and
genre classiﬁcation, and identifying songs that are similar
in terms of their melodic-harmonic content."
19,Markus Schedl;Tim Pohle;Noam Koenigstein;Peter Knees,What's Hot? Estimating Country-specific Artist Popularity.,2010,https://doi.org/10.5281/zenodo.1415870,"Markus Schedl, Johannes Kepler University, Linz, Austria, education;Tim Pohle, Johannes Kepler University, Linz, Austria, education;Noam Koenigstein, Tel Aviv University, Tel Aviv, Israel, education;Peter Knees, Johannes Kepler University, Linz, Austria, education","Predicting artists that are popular in certain regions of the world is a well desired task, especially for the music indus- try. Also the cosmopolitan and cultural-aware music aﬁ- cionado is likely be interested in which music is currently “hot” in other parts of the world. We therefore propose four approaches to determine artist popularity rankings on the country-level. To this end, we mine the following data sources: page counts from Web search engines, user posts on Twitter, shared folders on the Gnutella ﬁle sharing net- work, and playcount data from last.fm. We propose meth- ods to derive artist rankings based on these four sources and perform cross-comparison of the resulting rankings via overlap scores. We further elaborate on the advantages and disadvantages of all approaches as they yield interestingly diverse results."
20,Ron J. Weiss;Juan Pablo Bello,Identifying Repeated Patterns in Music Using Sparse Convolutive Non-negative Matrix Factorization.,2010,https://doi.org/10.5281/zenodo.1415934,"Ron J. Weiss, New York University, USA, education;Juan Pablo Bello, New York University, USA, education","We describe an unsupervised, data-driven, method for automatically identifying repeated patterns in music by analyzing a feature matrix using a variant of sparse convolutive non-negative matrix factorization. We utilize sparsity constraints to automatically identify the number of patterns and their lengths, parameters that would normally need to be fixed in advance. The proposed analysis is applied to beat-synchronous chromagrams in order to concurrently extract repeated harmonic motifs and their locations within a song. Finally, we show how this analysis can be used for long-term structure segmentation, resulting in an algorithm that is competitive with other state-of-the-art segmentation algorithms based on hidden Markov models and self similarity matrices."
21,Cillian Kelly;Mikel Gainza;David Dorran;Eugene Coyle,Locating Tune Changes and Providing a Semantic Labelling of Sets of Irish Traditional Tunes.,2010,https://doi.org/10.5281/zenodo.1418303,"Cillian Kelly, DIT Kevin St., IRL, education;Mikel Gainza, DIT Kevin St., IRL, education;David Dorran, DIT Kevin St., IRL, education;Eugene Coyle, DIT Kevin St., IRL, education","An approach is presented which provides the tune change loca-
tions within a set of Irish Traditional tunes. Also provided are
semantic labels for each part of each tune within the set. A set
in Irish Traditional music is a number of individual tunes played
segue. Each of the tunes in the set are made up of structural seg-
ments called parts. Musical variation is a prominent characteris-
tic of this genre. However, a certain set of notes known as ‘set
accented tones’ are considered impervious to musical variation.
Chroma information is extracted at ‘set accented tone’ locations
within the music. The resulting chroma vectors are grouped to
represent the parts of the music. The parts are then compared
with one another to form a part similarity matrix. Unit kernels
which represent the possible structures of an Irish Traditional
tune are matched with the part similarity matrix to determine the
tune change locations and semantic part labels."
22,Matthias Mauch;Simon Dixon,Approximate Note Transcription for the Improved Identification of Difficult Chords.,2010,https://doi.org/10.5281/zenodo.1416598,"Matthias Mauch, Queen Mary University of London, Centre for Digital Music, GBR, facility;Simon Dixon, Queen Mary University of London, Centre for Digital Music, GBR, facility","""The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord proﬁles and higher-level time-series modelling have received a lot of attention, resulting in methods with an overall performance of more than 70% in the MIREX Chord Detection task 2009. Research on the front end of chord transcription algorithms has often concentrated on ﬁnding good chord templates to ﬁt the chroma features. In this paper we reverse this approach and seek to ﬁnd chroma features that are more suitable for usage in a musically-motivated model. We do so by performing a prior approximate transcription using an existing technique to solve non-negative least squares problems (NNLS). The resulting NNLS chroma features are tested by using them as an input to an existing state-of-the-art high-level model for chord transcription. We achieve very good results of 80% accuracy using the song collection and metric of the 2009 MIREX Chord Detection tasks. This is a signiﬁcant increase over the top result (74%) in MIREX 2009. The nature of some chords makes their identiﬁcation particularly susceptible to confusion between fundamental frequency and partials. We show that the recognition of these diffcult chords in particular is substantially improved by the prior approximate transcription using NNLS."""
23,Thomas Rocher;Matthias Robine;Pierre Hanna;Laurent Oudre,Concurrent Estimation of Chords and Keys from Audio.,2010,https://doi.org/10.5281/zenodo.1417485,"Thomas Rocher, LaBRI, University of Bordeaux, FRA, education;Matthias Robine, LaBRI, University of Bordeaux, FRA, education;Pierre Hanna, LaBRI, University of Bordeaux, FRA, education;Laurent Oudre, Institut TELECOM, TELECOM ParisTech, FRA, education","This paper proposes a new method for local key and chord estimation from audio signals. A harmonic content of the musical piece is ﬁrst extracted by computing a set of chroma vectors. Correlation with ﬁxed chord and key templates then selects a set of key/chord pairs for every frame. A weighted acyclic harmonic graph is then built with these pairs as vertices, and the use of a musical distance to weigh its edges. Finally, the output sequences of chords and keys are obtained by ﬁnding the best path in the graph. The proposed system allows a mutual and beneﬁcial chord and key estimation. It is evaluated on a corpus com- posed of Beatles songs for both the local key estimation and chord recognition tasks. Results show that it performs better than state-of-the art chord analysis algorithms while providing a more complete harmonic analysis."
24,Hussein Hirjee;Daniel G. Brown 0001,Solving Misheard Lyric Search Queries Using a Probabilistic Model of Speech Sounds.,2010,https://doi.org/10.5281/zenodo.1414942,"Hussein Hirjee, University of Waterloo, Cheriton School of Computer Science, CAN, education;Daniel G. Brown, University of Waterloo, Cheriton School of Computer Science, CAN, education","Music listeners often mishear the lyrics to unfamiliar songs heard from public sources, such as the radio. Since standard text search engines will ﬁnd few relevant results when they are entered as a query, these misheard lyrics require phonetic pattern matching techniques to identify the song. We introduce a probabilistic model of mishearing trained on examples of actual misheard lyrics, and develop a phoneme similarity scoring matrix based on this model. We compare this scoring method to simpler pattern matching algorithms on the task of ﬁnding the correct lyric from a collection given a misheard query. The probabilistic method signiﬁcantly outperforms all other methods, ﬁnding 5-8% more correct lyrics within the ﬁrst ﬁve hits than the previous best method."
25,Noam Koenigstein;Gert R. G. Lanckriet;Brian McFee;Yuval Shavitt,Collaborative Filtering Based on P2P Networks.,2010,https://doi.org/10.5281/zenodo.1415620,"Noam Koenigstein, School of Electrical Engineering, Tel Aviv University, ISR, education;Yuval Shavitt, School of Electrical Engineering, Tel Aviv University, ISR, education;Gert Lanckriet, Electrical and Computer Engineering, University of California, San Diego, USA, education;Brian McFee, Computer Science and Engineering, University of California, San Diego, USA, education","Peer-to-Peer (P2P) networks are used by millions of people for sharing music files. As these networks become ever more popular, they also serve as an excellent source for Music Information Retrieval (MIR) tasks. This paper reviews the latest MIR studies based on P2P data-sets, and presents a new file sharing data collection system over the Gnutella. We discuss several advantages of P2P based data-sets over some of the more “traditional” data sources, and evaluate the information quality of our data-set in comparison to other data sources (Last.fm, social tags, biography data, and MFCCs). The evaluation is based on an artists similarity task using Partial Order Embedding (POE). We show that a P2P based Collaborative Filtering dataset performs at least as well as “traditional” data-sets, yet maintains some inherent advantages such as scale, availability and additional information features such as ID3 tags and geographical location."
26,Alex Hrybyk;Youngmoo E. Kim,Combined Audio and Video Analysis for Guitar Chord Identification.,2010,https://doi.org/10.5281/zenodo.1417465,"Alex Hrybyk, Drexel University, USA, education;Youngmoo Kim, Drexel University, USA, education","This paper presents a multi-modal approach to automatically identifying guitar chords using audio and video of the performer. Chord identiﬁcation is typically performed by analyzing the audio, using a chroma based feature to extract pitch class information, then identifying the chord with the appropriate label. Even if this method proves perfectly accurate, stringed instruments add extra ambiguity as a single chord or melody may be played in different positions on the fretboard. Preserving this information is important, because it signiﬁes the original ﬁngering, and implied “easiest” way to perform the selection. This chord identiﬁcation system combines analysis of audio to determine the general chord scale (i.e. A major, G minor), and video of the guitarist to determine chord voicing (i.e. open, barred, inversion), to accurately identify the guitar chord."
27,Teppo E. Ahonen,Combining Chroma Features For Cover Version Identification.,2010,https://doi.org/10.5281/zenodo.1414766,"Teppo E. Ahonen, University of Helsinki, FIN, education","We present an approach for cover version identiﬁcation which is based on combining different discretized features derived from the chromagram vectors extracted from the audio data. For measuring similarity between features, we use a parameter-free quasi-universal similarity metric which utilizes data compression. Evaluation proves that combined feature distances increase the accuracy in cover version identiﬁcation."
28,Arthur Flexer;Dominik Schnitzer;Martin Gasser;Tim Pohle,Combining Features Reduces Hubness in Audio Similarity.,2010,https://doi.org/10.5281/zenodo.1416360,"Arthur Flexer, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility;Dominik Schnitzer, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility, Johannes Kepler University Linz, Austria, education;Martin Gasser, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility;Tim Pohle, Johannes Kepler University Linz, Austria, education","In audio based music similarity, a well known effect is the existence of hubs, i.e. songs which appear similar to many other songs without showing any meaningful perceptual similarity. We verify that this effect also exists in very large databases (> 250000 songs) and that it even gets worse with growing size of databases. By combining different aspects of audio similarity we are able to reduce the hub problem while at the same time maintaining a high overall quality of audio similarity."
29,Nick Collins,Computational Analysis of Musical Influence: A Musicological Case Study Using MIR Tools.,2010,https://doi.org/10.5281/zenodo.1416756,"Nick Collins, University of Sussex, GBR, education","Are there new insights through computational methods to the thorny problem of plotting the ﬂow of musical inﬂuence? This project, motivated by a musicological study of early synth pop, applies MIR tools as an aid to the investigator. Web scraping and web services provide one angle, sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz. Charts of inﬂuence are constructed in GraphViz combining artist similarity and dates. Content based music similarity is the second approach, based around a core collection of synth pop albums. The prospect for new musical analyses are discussed with respect to these techniques."
30,Jin Ha Lee,Crowdsourcing Music Similarity Judgments using Mechanical Turk.,2010,https://doi.org/10.5281/zenodo.1416478,"Jin Ha Lee, University of Washington, USA, education","""Collecting human judgments for music similarity evalua-
tion has always been a difficult and time consuming task. 
This paper explores the viability of Amazon Mechanical 
Turk (MTurk) for collecting human judgments for audio 
music similarity evaluation tasks. We compared the simi-
larity judgments collected from Evalutron6000 (E6K) and 
MTurk using the Music Information Retrieval Evaluation 
eXchange 2009 Audio Music Similarity and Retrieval 
task dataset. Our data show that the results are highly 
comparable, and MTurk may be a useful method for col-
lecting subjective ground truth data. Furthermore, there 
are several benefits to using MTurk over the traditional 
E6K infrastructure. We conclude that using MTurk is a 
practical alternative of music similarity when it is used 
with some precautions."""
31,Frédéric Bimbot;Olivier Le Blouch;Gabriel Sargent;Emmanuel Vincent,Decomposition Into Autonomous and Comparable Blocks: A Structural Description of Music Pieces.,2010,https://doi.org/10.5281/zenodo.1414734,"Frédéric BIMBOT, IRISA, CNRS - UMR 6074, FRA, education;Olivier LE BLOUCH, INRIA, Rennes Bretagne Atlantique, FRA, education;Gabriel SARGENT, Université de Rennes 1, FRA, education;Emmanuel VINCENT, INRIA, Rennes Bretagne Atlantique, FRA, education","""The structure of a music piece is a concept which is often 
referred to in various areas of music sciences and technolo-
gies, but for which there is no commonly agreed definition. 
This raises a methodological issue in MIR, when designing 
and evaluating automatic structure inference algorithms. It 
also strongly limits the possibility to produce consistent 
large-scale annotation datasets in a cooperative manner.  
This article proposes an approach called decomposition into 
autonomous and comparable blocks, based on principles 
inspired from structuralism and generativism. It specifies a 
methodology for producing music structure annotation by 
human listeners based on simple criteria and resorting solely 
to the listening experience of the annotator. 
We show on a development set that the proposed approach 
can provide a reasonable level of concordance across anno-
tators and we introduce a set of annotations on the RWC da-
tabase, intended to be released to the MIR community."""
32,Bruno Angeles;Cory McKay;Ichiro Fujinaga,Discovering Metadata Inconsistencies.,2010,https://doi.org/10.5281/zenodo.1415550,"Bruno Angeles, CIRMMT, Schulich School of Music, McGill University, CAN, education;Cory McKay, CIRMMT, Schulich School of Music, McGill University, CAN, education;Ichiro Fujinaga, CIRMMT, Schulich School of Music, McGill University, CAN, education","""This paper describes the use of fingerprinting-based 
querying in identifying metadata inconsistencies in music 
libraries, as well as the updates to the jMusicMeta-
Manager software in order to perform the analysis. Test 
results are presented for both the Codaich database and a 
generic library of unprocessed metadata. Statistics were 
computed in order to evaluate the differences between a 
manually-maintained 
library 
and 
an 
unprocessed 
collection when comparing metadata with values on a 
MusicBrainz server queried by fingerprinting."""
33,Darrell Conklin;Mathieu Bergeron,Discovery of Contrapuntal Patterns.,2010,https://doi.org/10.5281/zenodo.1417413,"Darrell Conklin, Universidad del País Vasco, ESP, education, IKERBASQUE, Basque Foundation for Science, ESP, facility;Mathieu Bergeron, CIRMMT, McGill University, CAN, education","This paper develops and applies a new method for the discovery of polyphonic patterns. The method supports the representation of abstract relations that are formed between notes that overlap in time without being simultaneous. Such relations are central to understanding species counterpoint. The method consists of an application of the vertical viewpoint technique, which relies on a vertical slicing of the musical score. It is applied to two-voice contrapuntal textures extracted from the Bach chorale harmonizations. Results show that the new method is powerful enough to represent and discover distinctive modules of species counterpoint, including remarkably the suspension principle of fourth species counterpoint. In addition, by focusing on two voices in particular and setting them against all other possible voice pairs, the method can elicit patterns that illustrate well the unique treatment of the voices under investigation, e.g. the inner and outer voices. The results are promising and indicate that the method is suitable for computational musicology research."
34,Alberto Pinto,Eigenvector-based Relational Motif Discovery.,2010,https://doi.org/10.5281/zenodo.1415906,"Alberto Pinto, Università degli Studi di Milano, ITA, education","The development of novel analytical tools to investigate the structure of music works is central in current music information retrieval research. In particular, music summarization aims at finding the most representative parts of a music piece (motifs) that can be exploited for an efficient music database indexing system. Here we present a novel approach for motif discovery in music pieces based on an eigenvector method. Scores are segmented into a network of bars and then ranked depending on their centrality. Bars with higher centrality are more likely to be relevant for music summarization. Results on the corpus of J.S.Bach’s 2-part Inventions demonstrate the effectiveness of the method and suggest that different musical metrics might be more suitable than others for different applications."
35,Cory McKay;John Ashley Burgoyne;Jason Hockman;Jordan B. L. Smith;Gabriel Vigliensoni;Ichiro Fujinaga,"Evaluating the Genre Classification Performance of Lyrical Features Relative to Audio, Symbolic and Cultural Features.",2010,https://doi.org/10.5281/zenodo.1415706,"Cory McKay, McGill University, CAN, education;John Ashley Burgoyne, McGill University, CAN, education;Jason Hockman, McGill University, CAN, education;Jordan B. L. Smith, McGill University, CAN, education;Gabriel Vigliensoni, McGill University, CAN, education;Ichiro Fujinaga, McGill University, CAN, education","""This paper describes experimental research investigating the genre classification utility of combining features extracted from lyrical, audio, symbolic and cultural sources of musical information. It was found that cultural features consisting of information extracted from both web searches and mined listener tags were particularly effective, with the result that classification accuracies were achieved that compare favorably with the current state of the art of musical genre classification. It was also found that features extracted from lyrics were less effective than the other feature types. Finally, it was found that, with some exceptions, combining feature types does improve classification performance. The new lyricFetcher and jLyrics software are also presented as tools that can be used as a framework for developing more effective classification methodologies based on lyrics in the future."""
36,Joachim Ganseman;Paul Scheunders;Gautham J. Mysore;Jonathan S. Abel,Evaluation of a Score-informed Source Separation System.,2010,https://doi.org/10.5281/zenodo.1416062,"Joachim Ganseman, Paul Scheunders, University of Antwerp, BEL, education;Gautham J. Mysore, Jonathan S. Abel, Stanford University, USA, education","In this work, we investigate a method for score-informed source separation using Probabilistic Latent Component Analysis (PLCA). We present extensive test results that give an indication of the performance of the method, its strengths and weaknesses. For this purpose, we created a test database that has been made available to the public, in order to encourage comparisons with alternative methods."
37,Miguel Molina-Solana;Maarten Grachten;Gerhard Widmer,Evidence for Pianist-specific Rubato Style in Chopin Nocturnes.,2010,https://doi.org/10.5281/zenodo.1417793,"Miguel Molina-Solana, University of Granada, Spain, education;Maarten Grachten, Ghent University, Belgium, education;Gerhard Widmer, Johannes Kepler Univ., Austria, education","The performance of music usually involves a great deal of interpretation by the musician. In classical music, the ﬁnal ritardando is a good example of the expressive aspect of music performance. Even though expressive timing data is expected to have a strong component that is determined by the piece itself, in this paper we investigate to what degree individual performance style has an effect on the timing of ﬁnal ritardandi. The particular approach taken here uses Friberg and Sundberg’s kinematic rubato model in order to characterize performed ritardandi. Using a machine-learning classiﬁer, we carry out a pianist identiﬁcation task to assess the suitability of the data for characterizing the individual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, when cancelling the piece-speciﬁc aspects, pianists can often be identiﬁed with accuracy above baseline. This fact suggests the existence of a performer-speciﬁc style of playing ritardandi."
38,Jason Hockman;Ichiro Fujinaga,Fast vs Slow: Learning Tempo Octaves from User Data.,2010,https://doi.org/10.5281/zenodo.1416110,"Jason A. Hockman, McGill University, CAN, education;Ichiro Fujinaga, McGill University, CAN, education","The widespread use of beat- and tempo-tracking methods in music information retrieval tasks has been marginalized due to undesirable sporadic results from these algorithms. While sensorimotor and listening studies have demonstrated the subjectivity and variability inherent to human performance of this task, MIR applications such as recommendation require more reliable output than available from present tempo estimation models. In this paper, we present a initial investigation of tempo assessment based on the simple classification of whether the music is fast or slow. Through three experiments, we provide performance results of our method across two datasets, and demonstrate its usefulness in the pursuit of a reliable global tempo estimation."
39,Scott Miller;Paul Reimer;Steven R. Ness;George Tzanetakis,"Geoshuffle: Location-Aware, Content-based Music Browsing Using Self-organizing Tag Clouds.",2010,https://doi.org/10.5281/zenodo.1417703,"Scott Miller, University of Victoria, CAN, education;Paul Reimer, University of Victoria, CAN, education;Steven Ness, University of Victoria, CAN, education;George Tzanetakis, University of Victoria, CAN, education","In the past few years the computational capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShufﬂe – a prototype system for content-based music browsing and exploration that tar- gets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning ca- pabilities based on GPS. GeoShufﬂe adds location-based and time-based context to a user’s listening preferences. Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of inter- action is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of tex- tual information that can be displayed. We propose self- organizing tag clouds, a 2D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evalute the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can im- prove the quality of music recommendation and that self- organizing tag clouds provide faster browsing and are more engaging than text-based tag clouds."
40,Christian Fremerey;Meinard Müller;Michael Clausen,Handling Repeats and Jumps in Score-performance Synchronization.,2010,https://doi.org/10.5281/zenodo.1415942,"Christian Fremerey, Bonn University, DEU, education;Meinard Müller, Saarland University, DEU, education; MPI Informatik, DEU, facility;Michael Clausen, Bonn University, DEU, education","Given a score representation and a recorded performance of the same piece of music, the task of score-performance synchronization is to temporally align musical sections such as bars speciﬁed by the score to temporal sections in the performance. Most of the previous approaches assume that the score and the performance to be synchronized globally agree with regard to the overall musical structure. In practice, however, this assumption is often violated. For example, a performer may deviate from the score by ignoring a repeat or introducing an additional repeat that is not written in the score. In this paper, we introduce a synchronization approach that can cope with such structural differences. As main technical contribution, we describe a novel variant of dynamic time warping (DTW), referred to as JumpDTW, which allows for handling jumps and repeats in the alignment. Our approach is evaluated for the practically relevant case of synchronizing score data obtained from scanned sheet music via optical music recognition to corresponding audio recordings. Our experiments based on Beethoven piano sonatas show that JumpDTW can robustly identify and handle most of the occurring jumps and repeats leading to an overall alignment accuracy of over 99% on the bar-level."
41,Jingxuan Li;Tao Li 0001;Mitsunori Ogihara,Hierarchical Co-Clustering of Artists and Tags.,2010,https://doi.org/10.5281/zenodo.1416718,"Jingxuan Li, Florida International University, USA, education;Tao Li, Florida International University, USA, education;Mitsunori Ogihara, University of Miami, USA, education","""The user-assigned tag is a growingly important research topic in MIR. Noticing that some tags are more speciﬁc versions of others, this paper studies the problem of organizing tags into a hierarchical structure by taking into account the fact that the corresponding artists are organized into a hierarchy based on genre and style. A novel clustering algorithm, Hierarchical Co-clustering Algorithm (HCC), is proposed as a solution. Unlike traditional hierarchical clustering algorithms that deal with homogeneous data only, the proposed algorithm simultaneously organizes two distinct data types into hierarchies. HCC is additionally able to receive constraints that state certain objects “must-be-together” or “should-be-together” and build clusters so as to satisfying the constraints. HCC may lead to better and deeper understandings of relationship between artists and tags assigned to them. An experiment ﬁnds that by trying to hierarchically cluster the two types of data better clusters are obtained for both. It is also shown that HCC is able to incorporate instance-level constraints on artists and/or tags to improve the clustering process."""
42,Youngmoo E. Kim;Erik M. Schmidt;Raymond Migneco;Brandon G. Morton;Patrick Richardson;Jeffrey J. Scott;Jacquelin A. Speck;Douglas Turnbull,State of the Art Report: Music Emotion Recognition: A State of the Art Review.,2010,https://doi.org/10.5281/zenodo.1417945,"Youngmoo E. Kim, Drexel University, USA, education;Erik M. Schmidt, Drexel University, USA, education;Raymond Migneco, Drexel University, USA, education;Brandon G. Morton, Drexel University, USA, education;Patrick Richardson, Drexel University, USA, education;Jeffrey Scott, Drexel University, USA, education;Jacquelin A. Speck, Drexel University, USA, education;Douglas Turnbull, Ithaca College, USA, education","This paper surveys the state of the art in automatic emo-
tion recognition in music. Music is oftentimes referred
to as a “language of emotion” [1], and it is natural for us
to categorize music in terms of its emotional associations.
Myriad features, such as harmony, timbre, interpretation,
and lyrics affect emotion, and the mood of a piece may
also change over its duration. But in developing automated
systems to organize music in terms of emotional content,
we are faced with a problem that oftentimes lacks a well-
deﬁned answer; there may be considerable disagreement
regarding the perception and interpretation of the emotions
of a song or ambiguity within the piece itself. When com-
pared to other music information retrieval tasks (e.g., genre
identiﬁcation), the identiﬁcation of musical mood is still in
its early stages, though it has received increasing attention
in recent years. In this paper we explore a wide range of
research in music emotion recognition, particularly focus-
ing on methods that use contextual text information (e.g.,
websites, tags, and lyrics) and content-based approaches,
as well as systems combining multiple feature domains."
43,Ioannis Karydis;Milos Radovanovic;Alexandros Nanopoulos;Mirjana Ivanovic,"Looking Through the ""Glass Ceiling"": A Conceptual Framework for the Problems of Spectral Similarity.",2010,https://doi.org/10.5281/zenodo.1417283,"Ioannis Karydis, Ionian University, GRC, education;Miloš Radovanović, University of Novi Sad, SRB, education;Alexandros Nanopoulos, University of Hildesheim, DEU, education;Mirjana Ivanović, University of Novi Sad, SRB, education","Spectral similarity measures have been shown to exhibit good performance in several Music Information Retrieval (MIR) applications. They are also known, however, to possess several undesirable properties, namely allowing the existence of hub songs (songs which frequently appear in nearest neighbor lists of other songs), “orphans” (songs which practically never appear), and difficulties in distinguishing the farthest from the nearest neighbor due to the concentration effect caused by high dimensionality of data space. In this paper we develop a conceptual framework that allows connecting all three undesired properties. We show that hubs and “orphans” are expected to appear in high-dimensional data spaces, and relate the cause of their appearance with the concentration property of distance / similarity measures. We verify our conclusions on real music data, examining groups of frames generated by Gaussian Mixture Models (GMMs), considering two similarity measures: Earth Mover’s Distance (EMD) in combination with Kullback-Leibler (KL) divergence, and Monte Carlo (MC) sampling. The proposed framework can be useful to MIR researchers to address problems of spectral similarity, understand their fundamental origins, and thus be able to develop more robust methods for their remedy."
44,Noam Koenigstein;Yuval Shavitt;Ela Weinsberg;Udi Weinsberg,On the Applicability of Peer-to-peer Data in Music Information Retrieval Research.,2010,https://doi.org/10.5281/zenodo.1414784,"Noam Koenigstein, School of Electrical Engineering, Tel-Aviv University, ISR, education;Yuval Shavitt, School of Electrical Engineering, Tel-Aviv University, ISR, education;Ela Weinsberg, Dept. of Industrial Engineering, Tel-Aviv University, ISR, education;Udi Weinsberg, School of Electrical Engineering, Tel-Aviv University, ISR, education","Peer-to-Peer (p2p) networks are being increasingly adopted as an invaluable resource for various music information retrieval (MIR) tasks, including music similarity, recommendation and trend prediction. However, these networks are usually extremely large and noisy, which raises doubts regarding the ability to actually extract sufﬁciently accurate information. This paper evaluates the applicability of using data originating from p2p networks for MIR research, focusing on partial crawling, inherent noise and localization of songs and search queries. These aspects are quantiﬁed using songs collected from the Gnutella p2p network. We show that the power-law nature of the network makes it relatively easy to capture an accurate view of the main-streams using relatively little effort. However, some applications, like trend prediction, mandate collection of the data from the “long tail”, hence a much more exhaustive crawl is needed. Furthermore, we present techniques for overcoming noise originating from user generated content and for ﬁltering non informative data, while minimizing information loss."
45,Thomas Lidy;Rudolf Mayer;Andreas Rauber;Pedro J. Ponce de León;Antonio Pertusa;José Manuel Iñesta Quereda,A Cartesian Ensemble of Feature Subspace Classifiers for Music Categorization.,2010,https://doi.org/10.5281/zenodo.1415598,"T. Lidy, Vienna University of Technology, AUT, education;R. Mayer, Vienna University of Technology, AUT, education;A. Rauber, Vienna University of Technology, AUT, education;P. J. Ponce de León, University of Alicante, ESP, education;A. Pertusa, University of Alicante, ESP, education;J. M. Iñesta, University of Alicante, ESP, education","We present a cartesian ensemble classification system that is based on the principle of late fusion and feature subspaces. These feature subspaces describe different aspects of the same data set. The framework is built on the Weka machine learning toolkit and able to combine arbitrary feature sets and learning schemes. In our scenario, we use it for the ensemble classification of multiple feature sets from the audio and symbolic domains. We present an extensive set of experiments in the context of music genre classification, based on numerous Music IR benchmark datasets, and evaluate a set of combination/voting rules. The results show that the approach is superior to the best choice of a single algorithm on a single feature set. Moreover, it also releases the user from making this choice explicitly."
46,Julián Urbano;Mónica Marrero;Diego Martín 0001;Juan Lloréns,Improving the Generation of Ground Truths Based on Partially Ordered Lists.,2010,https://doi.org/10.5281/zenodo.1417525,"Julián Urbano, University Carlos III of Madrid, ESP, education;Mónica Marrero, University Carlos III of Madrid, ESP, education;Diego Martín, University Carlos III of Madrid, ESP, education;Juan Lloréns, University Carlos III of Madrid, ESP, education","Ground truths based on partially ordered lists have been used for some years now to evaluate the effectiveness of Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity. However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations. In this paper we revise the methodology used to generate these ground truths and disclose some issues that need to be addressed. In particular, we focus on the arrangement and aggregation of the relevant results, and show that it is not possible to ensure lists completely consistent. We develop a measure of consistency based on Average Dynamic Recall and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method. The results of the MIREX 2005 evaluation are revisited using these alternative ground truths."
47,João Lobato Oliveira;Fabien Gouyon;Luis Gustavo Martins;Luís Paulo Reis,IBT: A Real-time Tempo and Beat Tracking System.,2010,https://doi.org/10.5281/zenodo.1416470,"João Lobato Oliveira, Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal, education, Artificial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal, education;Fabien Gouyon, Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal, education;Luis Gustavo Martins, Research Center for Science and Technology in Art (CITAR), UCP, Porto, Portugal, education;Luis Paulo Reis, Artificial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal, education","This paper describes a tempo induction and beat tracking system based on the efficient strategy (initially introduced in the BeatRoot system [Dixon S., “Automatic extraction of tempo and beat from expressive performances.” Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main reasons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consideration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform)."
48,Riccardo Miotto;Luke Barrington;Gert R. G. Lanckriet,Improving Auto-tagging by Modeling Semantic Co-occurrences.,2010,https://doi.org/10.5281/zenodo.1415590,"Riccardo Miotto, University of Padova, ITA, education;Luke Barrington, UC San Diego, USA, education;Gert Lanckriet, UC San Diego, USA, education",Automatic taggers describe music in terms of a multinomial distribution over relevant semantic concepts. This paper presents a framework for improving automatic tagging of music content by modeling contextual relationships between these semantic concepts. The framework extends existing auto-tagging methods by adding a Dirichlet mixture to model the contextual co-occurrences between semantic multinomials. Experimental results show that adding context improves automatic annotation and retrieval of music and demonstrate that the Dirichlet mixture is an appropriate model for capturing co-occurrences between semantics.
49,Jouni Paulus,Improving Markov Model Based Music Piece Structure Labelling with Acoustic Information.,2010,https://doi.org/10.5281/zenodo.1416732,"Jouni Paulus, Fraunhofer Institute for Integrated Circuits IIS, DEU, facility","This paper proposes using acoustic information in the labelling of music piece structure descriptions. Here, music piece structure means the sectional form of the piece: temporal segmentation and grouping to parts such as chorus or verse. The structure analysis methods rarely provide the parts with musically meaningful names. The proposed method labels the parts in a description. The baseline method models the sequential dependencies between musical parts with N-grams and uses them for the labelling. The acoustic model proposed in this paper is based on the assumption that the parts with the same label even in different pieces share some acoustic properties compared to other parts in the same pieces. The proposed method uses mean and standard deviation of relative loudness in a part as the feature which is then modelled with a single multivariate Gaussian distribution. The method is evaluated on three data sets of popular music pieces, and in all of them the inclusion of the acoustic model improves the labelling accuracy over the baseline method."
50,Kazuyoshi Yoshii;Masataka Goto,Infinite Latent Harmonic Allocation: A Nonparametric Bayesian Approach to Multipitch Analysis.,2010,https://doi.org/10.5281/zenodo.1414912,"Kazuyoshi Yoshii, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility","This paper presents a statistical method called Infinite Latent Harmonic Allocation (iLHA) for detecting multiple fundamental frequencies in polyphonic audio signals. Conventional methods face a crucial problem known as model selection because they assume that the observed spectra are superpositions of a certain fixed number of bases (sound sources and/or finer parts). iLHA avoids this problem by assuming that the observed spectra are superpositions of a stochastically-distributed unbounded (theoretically infinite) number of bases. Such uncertainty can be treated in a principled way by leveraging the state-of-the-art paradigm of machine-learning called Bayesian nonparametrics. To represent a set of time-sliced spectral strips, we formulated nested infinite Gaussian mixture models (GMMs) based on hierarchical and generalized Dirichlet processes. Each strip is allowed to contain an unbounded number of sound sources (GMMs), each of which is allowed to contain an unbounded number of harmonic partials (Gaussians). To train the nested infinite GMMs efficiently, we used a modern inference technique called collapsed variational Bayes (CVB). Our experiments using audio recordings of real piano and guitar performances showed that fully automated iLHA based on noninformative priors performed as well as optimally tuned conventional methods."
51,Yushen Han;Christopher Raphael,Informed Source Separation of Orchestra and Soloist.,2010,https://doi.org/10.5281/zenodo.1416750,"Yushen Han, Indiana University Bloomington, USA, education;Christopher Raphael, Indiana University Bloomington, USA, education","A novel technique of unmasking to repair the degradation in sources separated by spectrogram masking is proposed. Our approach is based on explicit knowledge of the musical audio at note level from a score-audio alignment, which we termed Informed Source Separation (ISS). Such knowledge allows the spectrogram energy to be decomposed into note-based models. We assume that a spectrogram mask for the solo is obtained and focus on the problem of repairing audio resulting from applying the mask. We evaluate the spectrogram as well as the harmonic structure of the music. We either search for unmasked (orchestra) partials of the orchestra to be transposed onto a masked (solo) region or reshape a solo partial with phase and amplitude imputed from unmasked regions. We describe a Kalman smoothing technique to decouple the phase and amplitude of a musical partial that enables the modification to the spectrogram. Audio examples from a piano concerto are available for evaluation."
52,Gabriele Barbieri;François Pachet;Mirko Degli Esposti;Pierre Roy,Is There a Relation Between the Syntax and the Fitness of an Audio Feature?.,2010,https://doi.org/10.5281/zenodo.1416014,"Gabriele Barbieri, Università di Bologna, ITA, education;Franc¸ois Pachet, Sony CSL, FRA, company;Mirko Degli Esposti, Università di Bologna, ITA, education;Pierre Roy, Sony CSL, FRA, company","Feature generation has been proposed recently to generate feature sets automatically, as opposed to human-designed feature sets. This technique has shown promising results in many areas of supervised classiﬁcation, in particular in the audio domain. However, feature generation is usually performed blindly, with genetic algorithms. As a result search performance is poor, thereby limiting its practical use. We propose a method to increase the search perfor- mance of feature generation systems. We focus on ana- lytical features, i.e. features determined by their syntax. Our method consists in ﬁrst extracting statistical proper- ties of the feature space called spin patterns, by analogy with statistical physics. We show that spin patterns carry information about the topology of the feature space. We exploit these spin patterns to guide a simulated annealing algorithm speciﬁcally designed for feature generation. We evaluate our approach on three audio classiﬁcation prob- lems, and show that it increases performance by an order of magnitude. More generally this work is a ﬁrst step in using tools from statistical physics for the supervised clas- siﬁcation of complex audio signals."
53,Dominik Schnitzer;Arthur Flexer;Gerhard Widmer;Martin Gasser,Islands of Gaussians: The Self Organizing Map and Gaussian Music Similarity Features.,2010,https://doi.org/10.5281/zenodo.1415248,"Dominik Schnitzer, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility, Department of Computational Perception, Johannes Kepler University, Linz, Austria, education;Arthur Flexer, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility, Department of Computational Perception, Johannes Kepler University, Linz, Austria, education;Gerhard Widmer, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility, Department of Computational Perception, Johannes Kepler University, Linz, Austria, education;Martin Gasser, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility","Multivariate Gaussians are of special interest in the MIR field of automatic music recommendation. They are used as the de facto standard representation of music timbre to compute music similarity. However, standard algorithms for clustering and visualization are usually not designed to handle Gaussian distributions and their attached metrics (e.g. the Kullback-Leibler divergence). Hence to use these features the algorithms generally handle them indirectly by first mapping them to a vector space, for example by deriving a feature vector representation from a similarity matrix. This paper uses the symmetrized Kullback-Leibler centroid of Gaussians to show how to avoid the vectorization detour for the Self Organizing Maps (SOM) data visualization algorithm. We propose an approach so that the algorithm can directly and naturally work on Gaussian music similarity features to compute maps of music collections. We show that by using our approach we can create SOMs which (1) better preserve the original similarity topology and (2) are far less complex to compute, as the often costly vectorization step is eliminated."
54,Matija Marolt;Marieke Lefeber,It's Time for a Song - Transcribing Recordings of Bell-playing Clocks.,2010,https://doi.org/10.5281/zenodo.1416704,"Matija Marolt, University of Ljubljana, SVN, education;Marieke Lefeber, Meertens Instituut and Museum Speelklok, NLD, facility","""The paper presents an algorithm for automatic transcription of recordings of bell-playing clocks. Bell-playing clocks are clocks containing a hidden bell-playing mechanism that is periodically activated to play a melody. Clocks from the eighteenth century give us unique insight into the musical taste of their owners, so we are interested in studying their repertoire and performances - thus the need for automatic transcription. In the paper, we first present an analysis of acoustical properties of bells found in bell-playing clocks. We propose a model that describes positions of bell partials and an algorithm that discovers the number of bells and positions of their partials in a given recording. To transcribe a recording, we developed a probabilistic method that maximizes the joint probability of a note sequence given the recording and positions of bell partials. Finally, we evaluate our algorithms on a set of recordings of bell-playing clocks."""
55,Philippe Hamel;Douglas Eck,Learning Features from Music Audio with Deep Belief Networks.,2010,https://doi.org/10.5281/zenodo.1414970,"Philippe Hamel, Université de Montréal, CAN, education, CIRMMT, CAN, facility;Douglas Eck, Université de Montréal, CAN, education, CIRMMT, CAN, facility","Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically extract relevant features from audio for a given task. The feature extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the audio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. In particular, we learned the features to solve the task of genre recognition. The learned features perform significantly better than MFCCs. Moreover, we obtain a classification accuracy of 84.3% on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classifiers using frame-based features. We also applied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features."
56,Brian McFee;Luke Barrington;Gert R. G. Lanckriet,Learning Similarity from Collaborative Filters.,2010,https://doi.org/10.5281/zenodo.1416198,"Brian McFee, University of California, San Diego, USA, education;Luke Barrington, University of California, San Diego, USA, education;Gert Lanckriet, University of California, San Diego, USA, education","Collaborative ﬁltering methods (CF) exploit the wisdom of crowds to capture deeply structured similarities in musical objects, such as songs, artists or albums. When CF is available, it frequently outperforms content-based methods in recommendation tasks. However, songs in the so-called “long tail” cannot reap the beneﬁts of collaborative ﬁltering, and practitioners must rely on content-based methods. We propose a method for improving content-based recommendation in the long tail by learning an optimized similarity function from a sample of collaborative ﬁltering data. Our experimental results demonstrate substantial improvements in accuracy by learning optimal similarity functions."
57,Joaquín Mora;Francisco Gómez 0001;Emilia Gómez;Francisco Escobar-Borrego;José Miguel Díaz-Báñez,Characterization and Similarity in A Cappella Flamenco Cantes.,2010,https://doi.org/10.5281/zenodo.1415242,"Joaquín Mora, University of Seville, ESP, education;Francisco Gómez, Polytechnic University of Madrid, ESP, education;Emilia Gómez, Universitat Pompeu Fabra, ESP, education;Francisco Escobar-Borrego, University of Seville, ESP, education;José Miguel Díaz-Báñez, University of Seville, ESP, education","""This paper intends to research on the link between musical similarity and style and sub-style (variant) classification in the context of flamenco a cappella singing styles. Given the limitation of standard computational models for melodic characterization and similarity computation in this particular context, we have proposed a specific set of melodic features adapted to flamenco singing styles. In order to evaluate them, we have gathered a collection of music recordings from the most representative singers and have manually extracted those proposed features. Based on those features, we have defined a similarity measure between two performances and have validated their usefulness in differentiating several styles and variants. The main conclusion of this work is the need to incorporate specific musical features to the design of similarity measures for flamenco music so that flamenco-adapted MIR systems can be developed."""
58,Seokhwan Jo;Chang D. Yoo,Melody Extraction from Polyphonic Audio Based on Particle Filter.,2010,https://doi.org/10.5281/zenodo.1414978,"Seokhwan Jo, Korea Advanced Institute of Science Technology, KOR, education;Chang D. Yoo, Korea Advanced Institute of Science Technology, KOR, education","This paper considers a particle ﬁlter based algorithm to extract melody from a polyphonic audio in the short-time Fourier transforms (STFT) domain. The extraction is focused on overcoming the difﬁculties due to harmonic / percussive sound interferences, possibility of octave mismatch, and dynamic variation in melody. The main idea of the algorithm is to consider probabilistic relations between melody and polyphonic audio. Melody is assumed to follow a Markov process, and the framed segments of polyphonic audio are assumed to be conditionally independent given the parameters that represent the melody. The melody parameters are estimated using sequential importance sampling (SIS) which is a conventional particle ﬁlter method. In this paper, the likelihood and state transition are deﬁned to overcome the aforementioned difﬁculties. The SIS algorithm relies on sequential importance density, and this density is designed using multiple pitches which are estimated by a simple multi-pitch extraction algorithm. Experimental results show that the considered algorithm outperforms other famous melody extraction algorithms in terms of the raw pitch accuracy (RPA) and the raw chroma accuracy (RCA)."
59,Stanislaw Andrzej Raczynski;Emmanuel Vincent;Frédéric Bimbot;Shigeki Sagayama,Multiple Pitch Transcription using DBN-based Musicological Models.,2010,https://doi.org/10.5281/zenodo.1415198,"Stanisław A. Raczyński, The University of Tokyo, JPN, education;Emmanuel Vincent, INRIA Rennes, Britagne Atlantique, FRA, facility;Frédéric Bimbot, INRIA Rennes, Britagne Atlantique, FRA, facility;Shigeki Sagayama, The University of Tokyo, JPN, education","We propose a novel approach to solve the problem of estimating pitches of notes present in an audio signal. We have developed a probabilistically rigorous model that takes into account temporal dependencies between musical notes and between the underlying chords, as well as the instantaneous dependencies between chords, notes and the observed note saliences. We investigated its modeling ability by measuring the cross-entropy with symbolic (MIDI) data and then proceed to observe the model's performance in multiple pitch estimation of audio data."
60,Noor Azilah Draman;Campbell Wilson;Sea Ling,Modified Ais-based Classifier for Music Genre Classification.,2010,https://doi.org/10.5281/zenodo.1417437,"Noor Azilah Draman, Monash University, AUS, education;Campbell Wilson, Monash University, AUS, education;Sea Ling, Monash University, AUS, education","Automating human capabilities for classifying different genre of songs is a difficult task. This has led to various studies that focused on finding solutions to solve this problem. Analyzing music contents (often referred as con- tent-based analysis) is one of many ways to identify and group similar songs together. Various music contents, for example beat, pitch, timbral and many others were used and analyzed to represent the music. To be able to mani- pulate these content representations for recognition: fea- ture extraction and classification are two major focuses of investigation in this area. Though various classification techniques proposed so far, we are introducing yet anoth- er one. The objective of this paper is to introduce a possi- ble new technique in the Artificial Immune System (AIS) domain called a modified immune classifier (MIC) for music genre classification. MIC is the newest version of Negative Selection Algorithm (NSA) where it stresses the self and non-self cells recognition and a complementary process for generating detectors. The discussion will de- tail out the MIC procedures applied and the modified part in solving the classification problem. At the end, the re- sults of proposed framework will be presented, discussed and directions for future work are given."
61,Kazuma Murao;Masahiro Nakano;Yu Kitano;Nobutaka Ono;Shigeki Sagayama,Monophonic Instrument Sound Segregation by Clustering NMF Components Based on Basis Similarity and Gain Disjointness.,2010,https://doi.org/10.5281/zenodo.1417113,"Kazuma Murao, The University of Tokyo, JPN, education;Masahiro Nakano, The University of Tokyo, JPN, education;Yu Kitano, The University of Tokyo, JPN, education;Nobutaka Ono, The University of Tokyo, JPN, education;Shigeki Sagayama, The University of Tokyo, JPN, education","This paper discusses a method for monophonic instrument sound separation based on nonnegative matrix factorization (NMF). In general, it is not easy to classify NMF components into each instrument. By contrast, monophonic instrument sound gives us an important clue to classify them, because no more than one sound would be activated simultaneously. Our approach is to classify NMF components into each instrument based on basis spectrum vector similarity and temporal activity disjointness. Our clustering employs a hierarchical clustering algorithm: group average method (GAM). The efﬁciency of our approach is evaluated by some experiments."
62,Parag Chordia;Avinash Sastry;Trishul Mallikarjuna;Aaron Albin,Multiple Viewpoints Modeling of Tabla Sequences.,2010,https://doi.org/10.5281/zenodo.1416540,"Parag Chordia, Georgia Tech, Center for Music Technology, USA, facility;Avinash Sastry, Georgia Tech, Center for Music Technology, USA, facility;Trishul Malikarjuna, Georgia Tech, Center for Music Technology, USA, facility;Aaron Albin, Georgia Tech, Center for Music Technology, USA, facility","We describe a system that attempts to predict the con-
tinuation of a symbolically encoded tabla composition at
each time step using a variable-length n-gram model. Us-
ing cross-entropy as a measure of model ﬁt, the best model
attained an entropy rate of 0.780 in a cross-validation ex-
periment, showing that symbolic tabla compositions can
be effectively encoded using such a model. The choice of
smoothing algorithm, which determines how information
from different-order models is combined, is found to be an
important factor in the models performance. We extend the
basic n-gram model by adding viewpoints, other streams
of information that can be used to improve predictive per-
formance. First, we show that adding a short-term model,
built on the current composition and not the entire corpus,
leads to substantial improvements. Additional experiments
were conducted with derived types, representations derived
from the basic data type (stroke names), and cross-types,
which model dependencies between parameters, such as
duration and stroke name. For this database, such exten-
sions improved performance only marginally, although this
may have been due to the low entropy rate attained by the
basic model."
63,Kaichun K. Chang;Jyh-Shing Roger Jang;Costas S. Iliopoulos,Music Genre Classification via Compressive Sampling.,2010,https://doi.org/10.5281/zenodo.1418289,"Kaichun K. Chang, King’s College London, GBR, education;Jyh-Shing Roger Jang, National Tsing Hua University, TWN, education;Costas S. Iliopoulos, King’s College London, GBR, education","Compressive sampling (CS) is a new research topic in
signal processing that has piqued the interest of a wide
range of researchers in different ﬁelds recently. In this pa-
per, we present a CS-based classiﬁer for music genre clas-
siﬁcation, with two sets of features, including short-time
and long-time features of audio music. The proposed clas-
siﬁer generates a compact signature to achieve a signiﬁcant
reduction in the dimensionality of the audio music signals.
The experimental results demonstrate that the computation
time of the CS-based classiﬁer is only about 20% of SVM
on GTZAN dataset, with an accuracy of 92.7%. Several
experiments were conducted in this study to illustrate the
feasibility and robustness of the proposed methods as com-
pared to other approaches."
64,Yannis Panagakis;Constantine Kotropoulos;Gonzalo R. Arce,Sparse Multi-label Linear Embedding Within Nonnegative Tensor Factorization Applied to Music Tagging.,2010,https://doi.org/10.5281/zenodo.1417036,"Yannis Panagakis, Aristotle University of Thessaloniki, GRC, education;Constantine Kotropoulos, Aristotle University of Thessaloniki, GRC, education;Gonzalo R. Arce, University of Delaware, USA, education","A novel framework for music tagging is proposed. First,
each music recording is represented by bio-inspired audi-
tory temporal modulations. Then, a multilinear subspace
learning algorithm based on sparse label coding is devel-
oped to effectively harness the multi-label information for
dimensionality reduction. The proposed algorithm is re-
ferred to as Sparse Multi-label Linear Embedding Non-
negative Tensor Factorization, whose convergence to a sta-
tionary point is guaranteed. Finally, a recently proposed
method is employed to propagate the multiple labels of
training auditory temporal modulations to auditory tem-
poral modulations extracted from a test music recording
by means of the sparse ℓ1 reconstruction coefﬁcients. The
overall framework, that is described here, outperforms both
humans and state-of-the-art computer audition systems in
the music tagging task, when applied to the CAL500 dataset."
65,Michael I. Mandel;Douglas Eck;Yoshua Bengio,Learning Tags that Vary Within a Song.,2010,https://doi.org/10.5281/zenodo.1416130,"Michael I Mandel, LISA Lab, Université de Montréal, CAN, education;Douglas Eck, LISA Lab, Université de Montréal, CAN, education;Yoshua Bengio, LISA Lab, Université de Montréal, CAN, education","This paper examines the relationship between human generated tags describing different parts of the same song. These tags were collected using Amazon’s Mechanical Turk service. We ﬁnd that the agreement between different people’s tags decreases as the distance between the parts of a song that they heard increases. To model these tags and these relationships, we describe a conditional restricted Boltzmann machine. Using this model to ﬁll in tags that should probably be present given a context of other tags, we train automatic tag classiﬁers (autotaggers) that outperform those trained on the original data."
66,Jun Wang;Xiaoou Chen;Yajie Hu;Tao Feng,Predicting High-level Music Semantics Using Social Tags via Ontology-based Reasoning.,2010,https://doi.org/10.5281/zenodo.1417785,"Jun Wang, Institute of Computer Science and Technology, Peking University, CHN, education;Xiaoou Chen, Institute of Computer Science and Technology, Peking University, CHN, education;Yajie Hu, Institute of Computer Science and Technology, Peking University, CHN, education;Tao Feng, Institute of Computer Science and Technology, Peking University, CHN, education","""High-level semantics such as “mood” and “usage” are very useful in music retrieval and recommendation but they are normally hard to acquire. Can we predict them from a cloud of social tags? We propose a semantic identification and reasoning method: Given a music taxonomy system, we map it to an ontology’s terminology, map its finite set of terms to the ontology’s assertional axioms, and then map tags to the closest conceptual level of the referenced terms in WordNet to enrich the knowledge base, then we predict richer high-level semantic information with a set of reasoning rules. We find this method predicts mood annotations for music with higher accuracy, as well as giving richer semantic association information, than alternative SVM-based methods do."""
67,Özgür Izmirli;Roger B. Dannenberg,Understanding Features and Distance Functions for Music Sequence Alignment.,2010,https://doi.org/10.5281/zenodo.1418353,"Özgür İzmirli, Connecticut College, USA, education;Roger B. Dannenberg, Carnegie Mellon University, USA, education","We investigate the problem of matching symbolic representations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representations that optimize the classification of “matching” vs. “non-matching” frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representation but they also reveal interesting projection structures that differ distinctly from the traditional chromagram."
68,Bernhard Niedermayer;Gerhard Widmer,A Multi-pass Algorithm for Accurate Audio-to-Score Alignment.,2010,https://doi.org/10.5281/zenodo.1415910,"Bernhard Niedermayer, Johannes Kepler University Linz, AUT, education;Gerhard Widmer, Johannes Kepler University Linz, AUT, education, Austrian Research Institute for Artificial Intelligence, AUT, facility","Most current audio-to-score alignment algorithms work on the level of score time frames; i.e., they cannot differentiate between several notes occurring at the same discrete time within the score. This level of accuracy is sufficient for a variety of applications. However, for those that deal with, for example, musical expression analysis such micro-timings might also be of interest. Therefore, we propose a method that estimates the onset times of individual notes in a post-processing step. Based on the initial alignment and a feature obtained by matrix factorization, those notes for which the confidence in the alignment is high are chosen as anchor notes. The remaining notes in between are revised, taking into account the additional information about these anchors and the temporal relations given by the score. We show that this method clearly outperforms a reference method that uses the same features but does not differentiate between anchor and non-anchor notes."
69,Robert Macrae;Simon Dixon,Accurate Real-time Windowed Time Warping.,2010,https://doi.org/10.5281/zenodo.1416156,"Robert Macrae, Queen Mary University of London, GBR, education;Simon Dixon, Queen Mary University of London, GBR, education","Dynamic Time Warping (DTW) is used to ﬁnd alignments between two related streams of information and can be used to link data, recognise patterns or ﬁnd similarities. Typically, DTW requires the complete series of both input streams in advance and has quadratic time and space requirements. As such DTW is unsuitable for real-time applications and is inefﬁcient for aligning long sequences. We present Windowed Time Warping (WTW), a variation on DTW that, by dividing the path into a series of DTW windows and making use of path cost estimation, achieves alignments with an accuracy and efﬁciency superior to other leading modiﬁcations and with the capability of synchronising in real-time. We demonstrate this method in a score following application. Evaluation of the WTW score following system found 97.0% of audio note onsets were correctly aligned within 2000 ms of the known time. Results also show reductions in execution times over state-of-the-art efﬁcient DTW modiﬁcations."
70,Florian Kaiser;Thomas Sikora,Music Structure Discovery in Popular Music using Non-negative Matrix Factorization.,2010,https://doi.org/10.5281/zenodo.1418085,"Florian Kaiser, Technische Universität Berlin, DEU, education;Thomas Sikora, Technische Universität Berlin, DEU, education","""We introduce a method for the automatic extraction of musical structures in popular music. The proposed algorithm uses non-negative matrix factorization to segment regions of acoustically similar frames in a self-similarity matrix of the audio data. We show that over the dimensions of the NMF decomposition, structural parts can easily be modeled. Based on that observation, we introduce a clustering algorithm that can explain the structure of the whole music piece. The preliminary evaluation we report in the paper shows very encouraging results."""
71,Steven K. Tjoa;K. J. Ray Liu,Musical Instrument Recognition using Biologically Inspired Filtering of Temporal Dictionary Atoms.,2010,https://doi.org/10.5281/zenodo.1416166,"Steven K. Tjoa, University of Maryland, USA, education;K. J. Ray Liu, University of Maryland, USA, education","Most musical instrument recognition systems rely entirely upon spectral information instead of temporal information. In this paper, we test the hypothesis that temporal informa- tion can improve upon the accuracy achievable by the state of the art in instrument recognition. Unlike existing tem- poral classiﬁcation methods which use traditional features such as temporal moments, we extract novel features from temporal atoms generated by nonnegative matrix factoriza- tion by using a multiresolution gamma ﬁlterbank. Among isolated sounds taken from twenty-four instrument classes, the proposed system can achieve 92.3% accuracy, thus im- proving upon the state of the art."
72,Benoît Mathieu;Slim Essid;Thomas Fillon;Jacques Prado;Gaël Richard,"YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software.",2010,https://doi.org/10.5281/zenodo.1418321,"Benoit Mathieu, Institut Telecom, Telecom ParisTech, CNRS/LTCI, education;Slim Essid, Institut Telecom, Telecom ParisTech, CNRS/LTCI, education;Thomas Fillon, Institut Telecom, Telecom ParisTech, CNRS/LTCI, education;Jacques Prado, Institut Telecom, Telecom ParisTech, CNRS/LTCI, education;Ga¨el Richard, Institut Telecom, Telecom ParisTech, CNRS/LTCI, education","Music Information Retrieval systems are commonly built on a feature extraction stage. For applications involving automatic classiﬁcation (e.g. speech/music discrimination, music genre or mood recognition, ...), traditional approaches will consider a large set of audio features to be extracted on a large dataset. In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efﬁcient feature extraction. In this paper, a new audio feature extraction software, YAAFE 1 , is presented and compared to widely used libraries. The main advantage of YAAFE is a signiﬁcantly lower complexity due to the appropriate exploitation of redundancy in the feature calculation. YAAFE remains easy to conﬁgure and each feature can be parameterized independently. Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License."
73,Markus Schedl,On the Use of Microblogging Posts for Similarity Estimation and Artist Labeling.,2010,https://doi.org/10.5281/zenodo.1418215,"Markus Schedl, Johannes Kepler University, AUT, education","Microblogging services, such as Twitter, have risen enormously in popularity during the past years. Despite their popularity, such services have never been analyzed for MIR purposes, to the best of our knowledge. We hence present first investigations of the usability of music artist-related microblogging posts to perform artist labeling and similarity estimation tasks. To this end, we look into different text-based indexing models and term weighting measures. Two artist collections are used for evaluation, and the different methods are evaluated against data from last.fm. We show that microblogging posts are a valuable source for musical meta-data."
74,Andre Holzapfel;Yannis Stylianou,Parataxis: Morphological Similarity in Traditional Music.,2010,https://doi.org/10.5281/zenodo.1416896,"Andre Holzapfel, Institute of Computer Science, FORTH, GRC, facility, University of Crete, GRC, education;Yannis Stylianou, Institute of Computer Science, FORTH, GRC, facility, University of Crete, GRC, education","In this paper an automatic system for the detection of similar phrases in music of the Eastern Mediterranean is proposed. This music follows a specific structure, which is referred to as parataxis. The proposed system can be applied to audio signals of complex mixtures that contain the lead melody together with instrumental accompaniment. It is shown that including a lead melody estimation into a state-of-the-art system for cover song detection leads to promising results on a dataset of transcribed traditional dances from the island of Crete in Greece. Furthermore, a general framework that includes also rhythmic aspects is proposed. The proposed method represents a simple framework for the support of ethnomusicological studies on related forms of traditional music."
75,Aline K. Honingh;Rens Bod,Pitch Class Set Categories as Analysis Tools for Degrees of Tonality.,2010,https://doi.org/10.5281/zenodo.1417533,"Aline Honingh, Institute for Logic, Language and Computation, University of Amsterdam, NLD, education;Rens Bod, Institute for Logic, Language and Computation, University of Amsterdam, NLD, education","""This is an explorative paper in which we present a new method for music analysis based on pitch class set categories. It has been shown before that pitch class sets can be divided into six different categories. Each category inherits a typical character which can “tell” something about the music in which it appears. In this paper we explore the possibilities of using pitch class set categories for 1) classification in major/minor mode, 2) classification in tonal/atonal music, 3) determination of a degree of tonality, and 4) determination of a composer’s period."""
76,Erik M. Schmidt;Youngmoo E. Kim,Prediction of Time-varying Musical Mood Distributions from Audio.,2010,https://doi.org/10.5281/zenodo.1416238,"Erik M. Schmidt, Drexel University, USA, education;Youngmoo E. Kim, Drexel University, USA, education","The appeal of music lies in its ability to express emotions, and it is natural for us to organize music in terms of emo- tional associations. But the ambiguities of emotions make the determination of a single, unequivocal response label for the mood of a piece of music unrealistic. We address this lack of speciﬁcity by modeling human response labels to music in the arousal-valence (A-V) representation of af- fect as a stochastic distribution. Based upon our collected data, we present and evaluate methods using multiple sets of acoustic features to estimate these mood distributions parametrically using multivariate regression. Furthermore, since the emotional content of music often varies within a song, we explore the estimation of these A-V distributions in a time-varying context, demonstrating the ability of our system to track changes on a short-time basis."
77,Ching-Hua Chuan;Elaine Chew,Quantifying the Benefits of Using an Interactive Decision Support Tool for Creating Musical Accompaniment in a Particular Style.,2010,https://doi.org/10.5281/zenodo.1417337,"Ching-Hua Chuan, University of North Florida, USA, education;Elaine Chew, University of Southern California, USA, education","""We present a human-centered experiment designed to measure the degree of support for creating musical accompaniment provided by an interactive composition decision-support system. We create an interactive system with visual and audio cues to assist users in the choosing of chords to craft an accompaniment in a desired style. We propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quantitative measures of musical distance – percentage correct and closely related chords, and average neo-Riemannian distance – compare the user-created accompaniment with the original, with and without decision support. Numbers of backward edits, unique chords explored, and repeated chord choices during composition help quantify composition behavior. We present experimental data from musicians and non-musicians. We observe that decision support reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as repeated chord choices, and the gap between musicians’ and non-musicians’ work, without significantly limiting the range of users’ choices."""
78,Akira Maezawa;Masataka Goto;Hiroshi G. Okuno,Query-by-conducting: An Interface to Retrieve Classical-music Interpretations by Real-time Tempo Input.,2010,https://doi.org/10.5281/zenodo.1416614,"Akira Maezawa, Graduate School of Informatics, Kyoto University, JPN, education;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility;Hiroshi G. Okuno, Graduate School of Informatics, Kyoto University, JPN, education","This paper presents an interface for ﬁnding interpretations of a user-speciﬁed music, Query-by-Conducting. In classical music, there are many interpretations to a particular piece, and ﬁnding “the” interpretation that matches the listener’s taste allows a listener to further enjoy the piece. The critical issue in ﬁnding such an interpretation is the way or interface to allow the listener to listen through different interpretations. Our interface allows a user, by swinging a conducting hardware interface, to conduct the desired global tempo along the playback of a piece, at any time in the piece. The real-time conducting input by the user dynamically switches the interpretation being played back to the one closest to how the user is currently conducting. At the end of the piece, our interface ranks each interpretation according to how close the tempo of each interpretation was to the user input. At the core of our interface is an automated tempo estimation method based on audio-score alignment. We improve tempo estimation by requiring the audio-score alignment of different interpretations to be consistent with each other. We evaluate the tempo estimation method using a solo, chamber, and orchestral repertoire. The proposed tempo estimation decreases the error by as much as 0.94 times the original error."
79,Michael O. Jewell;Christophe Rhodes;Mark d'Inverno,Querying Improvised Music: Do You Sound Like Yourself?.,2010,https://doi.org/10.5281/zenodo.1417227,"Michael O. Jewell, Goldsmiths, University of London, GBR, education;Christophe Rhodes, Goldsmiths, University of London, GBR, education;Mark d’Inverno, Goldsmiths, University of London, GBR, education","Improvisers are often keen to assess how their performance practice stands up to an ideal: whether that ideal is of technical accuracy or instant composition of material meeting complex harmonic constraints at speed. This paper reports on the development of an interface for querying and navigating a collection of recorded material for the purpose of presenting information on musical similarity, and the application of this interface to the investigation of a set of recordings by jazz performers. We investigate the retrieval performance of our tool, and in analysing the ‘hits’ and particularly the ‘misses’, provide information suggesting a change in one of the authors’ improvisation style."
80,Arnaud Dessein;Arshia Cont;Guillaume Lemaitre,Real-time Polyphonic Music Transcription with Non-negative Matrix Factorization and Beta-divergence.,2010,https://doi.org/10.5281/zenodo.1414824,"Arnaud Dessein, IRCAM – CNRS UMR 9912, France, facility;Arshia Cont, IRCAM – CNRS UMR 9912, France, facility;Guillaume Lemaitre, IRCAM – CNRS UMR 9912, France, facility","In this paper, we investigate the problem of real-time polyphonic music transcription by employing non-negative matrix factorization techniques and the β-divergence as a cost function. We consider real-world setups where the music signal arrives incrementally to the system and is transcribed as it unfolds in time. The proposed transcription system is addressed with a modiﬁed non-negative matrix factorization scheme, called non-negative decomposition, where the incoming signal is projected onto a ﬁxed basis of templates learned off-line prior to the decomposition. We discuss the use of non-negative matrix factorization with the β-divergence to achieve the real-time decomposition. The proposed system is evaluated on the speciﬁc task of piano music transcription and the results show that it can outperform several state-of-the-art off-line approaches."
81,Tim Crawford;Matthias Mauch;Christophe Rhodes,Recognising Classical Works in Historical Recordings.,2010,https://doi.org/10.5281/zenodo.1417429,"Tim Crawford, Goldsmiths, University of London, Centre for Cognition, Computation and Culture, GBR, education;Matthias Mauch, Queen Mary, University of London, Centre for Digital Music, GBR, education;Christophe Rhodes, Goldsmiths, University of London, Department of Computing, GBR, education","""In collections of recordings of classical music, it is normal to find multiple performances, usually by different artists, of the same pieces of music. While there may be differences in many dimensions of musical similarity, such as timbre, pitch or structural detail, the underlying musical content is essentially and recognizably the same. The degree of divergence is generally less than that found between ‘cover songs’ in the domain of popular music, and much less than in typical performances of jazz standards. MIR methods, based around variants of the chroma representation, can be useful in tasks such as work identification especially where disco/bibliographical metadata is absent or incomplete as well as for access, curation and management of collections. We describe some initial experiments in work-recognition on a test-collection comprising c. 2000 digital transfers of historical recordings, and show that the use of NNLS chroma, a new, musically-informed chroma feature, dramatically improves recognition."""
82,Alan Marsden,Recognition of Variations Using Automatic Schenkerian Reduction.,2010,https://doi.org/10.5281/zenodo.1418179,"Alan Marsden, Lancaster Institute for the Contemporary Arts, Lancaster University, UK, education","Experiments on techniques to automatically recognise whether or not an extract of music is a variation of a given theme are reported, using a test corpus derived from ten of Mozart‘s sets of variations for piano. Methods which examine the notes of the ‗surface‘ are compared with methods which make use of an automatically derived quasi-Schenkerian reduction of the theme and the extract in question. The maximum average F-measure achieved was 0.87. Unexpectedly, this was for a method of matching based on the surface alone, and in general the results for matches based on the surface were marginally better than those based on reduction, though the small number of possible test queries means that this result cannot be regarded as conclusive. Other inferences on which factors seem to be important in recognising variations are discussed. Possibilities for improved recognition of matching using reduction are outlined."
83,James Bergstra;Michael I. Mandel;Douglas Eck,Scalable Genre and Tag Prediction with Spectral Covariance.,2010,https://doi.org/10.5281/zenodo.1416942,"James Bergstra, University of Montreal, CAN, education;Michael Mandel, University of Montreal, CAN, education;Douglas Eck, University of Montreal, CAN, education","Cepstral analysis is effective in separating source from ﬁl- ter in vocal and monophonic [pitched] recordings, but is it a good general-purpose framework for working with mu- sic audio? We evaluate covariance in spectral features as an alternative to means and variances in cepstral features (par- ticularly MFCCs) as summaries of frame-level features. We ﬁnd that spectral covariance is more effective than mean, variance, and covariance statistics of MFCCs for genre and social tag prediction. Support for our model comes from strong and state-of-the-art performance on the GTZAN genre dataset, MajorMiner, and MagnaTagatune. Our clas- siﬁcation strategy based on linear classiﬁers is easy to im- plement, exhibits very little sensitivity to hyper-parameters, trains quickly (even for web-scale datasets), is fast to ap- ply, and offers competitive performance in genre and tag prediction."
84,Chun-Man Mak;Tan Lee;Suman Senapati;Yu Ting Yeung;Wang-Kong Lam,Similarity Measures for Chinese Pop Music Based on Low-level Audio Signal Attributes.,2010,https://doi.org/10.5281/zenodo.1415636,"Chun-Man Mak, The Chinese University of Hong Kong, HKG, education;Tan Lee, The Chinese University of Hong Kong, HKG, education;Suman Senapati, The Chinese University of Hong Kong, HKG, education;Yu-Ting Yeung, The Chinese University of Hong Kong, HKG, education;Wang-Kong Lam, The Chinese University of Hong Kong, HKG, education","In this article a method of computing similarity of two Chinese pop songs is presented. It is based on five attributes extracted from the audio signal. They include music instrument, singing voice style, singer gender, tempo, and degree of noisiness. We compare the computed similarity measures with similarity scores obtained with subjective listening by over 200 human subjects. The results show that rhythm and mood related attributes like tempo and degree of noisiness are most correlated to human perception of Chinese pop songs. Instrument and singing style are relatively less relevant. The results of subjective evaluation also indicate that the proposed method of similarity computation is fairly correlated with human perception."
85,Daniel Gärtner,Singing / Rap Classification of Isolated Vocal Tracks.,2010,https://doi.org/10.5281/zenodo.1417089,"Daniel G¨artner, Fraunhofer Institute for Digital Media Technology IDMT, DEU, facility","In this paper, a system for the classiﬁcation of the vo- cal characteristics in HipHop / R&B music is presented. Isolated vocal track segments, taken from acapella ver- sions of commercial recordings, are classiﬁed into classes singing and rap. A feature-set motivated by work from song / speech classiﬁcation, speech emotion recognition, and from differences that humans perceive and utilize, is presented. An SVM is used as classiﬁer, accuracies of about 90% are achieved. In addition, the features are an- alyzed according to their contribution, using the IRMFSP feature selection algorithm. In another experiment, it is shown that the features are robust against utterance-speci- ﬁc characteristics."
86,Chao-Ling Hsu;Jyh-Shing Roger Jang,Singing Pitch Extraction by Voice Vibrato / Tremolo Estimation and Instrument Partial Deletion.,2010,https://doi.org/10.5281/zenodo.1417357,"Chao-Ling Hsu, ;Jyh-Shing Roger Jang, National Tsing Hua University, TWN, education","""This paper proposes a novel and effective approach to 
extract the pitches of the singing voice from monaural 
polyphonic songs. The sinusoidal partials of the musical 
audio signals are first extracted. The Fourier transform is 
then applied to extract the vibrato/tremolo information of 
each partial. Some criteria based on this vibrato/tremolo 
information are employed to discriminate the vocal par-
tials from the music accompaniment partials. Besides, a 
singing pitch trend estimation algorithm which is able to 
find the global singing progressing tunnel is also pro-
posed. The singing pitches can then be extracted more 
robustly via these two processes. Quantitative evaluation 
shows that the proposed algorithms significantly improve 
the raw pitch accuracy of our previous approach and are 
comparable with other state of the art approaches submit-
ted to MIREX."""
87,Dominikus Baur;Bartholomäus Steinmayr;Andreas Butz,SongWords: Exploring Music Collections Through Lyrics.,2010,https://doi.org/10.5281/zenodo.1416682,"Dominikus Baur, University of Munich (LMU), DEU, education;Bartholomäus Steinmayr, University of Munich (LMU), DEU, education;Andreas Butz, University of Munich (LMU), DEU, education","""The lyrics of a song are an interesting, yet underused type of symbolic music data. We present SongWords, an ap- plication for tabletop computers that allows browsing and exploring a music collection based on its lyrics. Song- Words can present the collection in a self-organizing map or sorted along different dimensions. Songs can be ordered by lyrics, user-generated tags or alphabetically by name, which allows exploring simple correlations, e.g., between genres (such as gospel) and words (such as lord). In this paper, we discuss the design rationale and implementation of SongWords as well as a user study with personal music collections. We found that lyrics indeed enable a different access to music collections and identiﬁed some challenges for future lyrics-based interfaces."""
88,Ruben Hillewaere;Bernard Manderick;Darrell Conklin,String Quartet Classification with Monophonic Models.,2010,https://doi.org/10.5281/zenodo.1417619,"Ruben Hillewaere, Vrije Universiteit Brussel, BEL, education;Bernard Manderick, Vrije Universiteit Brussel, BEL, education;Darrell Conklin, Universidad del País Vasco, ESP, education, IKERBASQUE, Basque Foundation for Science, ESP, facility","Polyphonic music classification remains a very challenging area in the field of music information retrieval. In this study, we explore the performance of monophonic models on single parts that are extracted from the polyphony. The presented method is specifically designed for the case of voiced polyphony, but can be extended to any type of music with multiple parts. On a dataset of 207 Haydn and Mozart string quartet movements, global feature models with standard machine learning classifiers are compared with a monophonic n-gram model for the task of composer recognition. Global features emerging from feature selection are presented, and future guidelines for the research of polyphonic music are outlined."
89,Peter Knees;Markus Schedl;Tim Pohle;Klaus Seyerlehner;Gerhard Widmer,Supervised and Unsupervised Web Document Filtering Techniques to Improve Text-Based Music Retrieval.,2010,https://doi.org/10.5281/zenodo.1417173,"Peter Knees, Johannes Kepler University Linz, AUT, education;Markus Schedl, Johannes Kepler University Linz, AUT, education;Tim Pohle, Johannes Kepler University Linz, AUT, education;Klaus Seyerlehner, Johannes Kepler University Linz, AUT, education;Gerhard Widmer, Johannes Kepler University Linz, AUT, education","We aim at improving a text-based music search engine by applying different techniques to exclude misleading information from the indexing process. The idea of the original approach is to index music pieces by “contextual” information, more precisely, by all texts to be found on Web pages retrieved via a common Web search engine. This representation allows for issuing arbitrary textual queries to retrieve relevant music pieces. The goal of this work is to improve precision of the retrieved set of music pieces by filtering out Web pages that lead to irrelevant tracks. To this end we present two unsupervised and two supervised filtering approaches. Evaluation is carried out on two collections previously used in the literature. The obtained results suggest that the proposed filtering techniques can improve results significantly but are only effective when applied to large and diverse music collections with millions of Web pages associated."
90,Carolina Ramirez;Jun Ohya,Symbol Classification Approach for OMR of Square Notation Manuscripts.,2010,https://doi.org/10.5281/zenodo.1415122,"Carolina Ramirez, Waseda University, JPN, education;Jun Ohya, Waseda University, JPN, education","Researchers in the field of OMR (Optical Music Recognition) have acknowledged that the automatic transcription of medieval musical manuscripts is still an open problem [2, 3], mainly due to lack of standards in notation and the physical quality of the documents. Nonetheless, the amount of medieval musical manuscripts is so vast that the consensus seems to be that OMR can be a vital tool to help in the preserving and sharing of this information in digital format. In this paper we report our results on a preliminary approach to OMR of medieval plainchant manuscripts in square notation, at the symbol classification level, which produced good results in the recognition of eight basic symbols. Our preliminary approach consists of the pre-processing, segmentation, and classification stages."
91,Aggelos Gkiokas;Vassilios Katsouros;George Carayannis,Tempo Induction Using Filterbank Analysis and Tonal Features.,2010,https://doi.org/10.5281/zenodo.1415210,"Aggelos Gkiokas, Institute for Language and Speech Processing, facility, National Technical University of Athens, GRC, education;Vassilis Katsouros, Institute for Language and Speech Processing, facility;George Carayannis, National Technical University of Athens, GRC, education","""This paper presents an algorithm that extracts the tempo of a musical excerpt. The proposed system assumes a constant tempo and deals directly with the audio signal. A sliding window is applied to the signal and two feature classes are extracted. The first class is the log-energy of each band of a mel-scale triangular filterbank, a common feature vector used in various MIR applications. For the second class, a novel feature for the tempo induction task is presented; the strengths of the twelve western musical tones at all octaves are calculated for each audio frame, in a similar fashion with Pitch Class Profile. The time-evolving feature vectors are convolved with a bank of resonators, each resonator corresponding to a target tempo. Then the results of each feature class are combined to give the final output. The algorithm was evaluated on the popular ISMIR 2004 Tempo Induction Evaluation Exchange Dataset. Results demonstrate that the superposition of the different types of features enhance the performance of the algorithm, which is in the current state-of-the-art algorithms of the tempo induction task."""
92,Simone Sammartino;Lorenzo J. Tardón;Cristina de la Bandera;Isabel Barbancho;Ana M. Barbancho,The Standardized Variogram as a Novel Tool for Music Similarity Evaluation.,2010,https://doi.org/10.5281/zenodo.1417195,"Simone Sammartino, Universidad de Málaga, ESP, education;Lorenzo J. Tardón, Universidad de Málaga, ESP, education;Cristina de la Bandera, Universidad de Málaga, ESP, education;Isabel Barbancho, Universidad de Málaga, ESP, education;Ana M. Barbancho, Universidad de Málaga, ESP, education","Most of methods for audio similarity evaluation are based on the Mel frequency cepstral coefﬁcients, employed as main tool for the characterization of audio contents. Such approach needs some way of data compression aimed to optimize the information retrieval task and to reduce the computational costs derived from the usage of cluster analysis tools and probabilistic models. A novel approach is presented in this paper, based on the standardized variogram. This tool, inherited from Geostatistics, is applied to MFCCs matrices to reduce their size and compute compact representations of the audio contents (song signatures), aimed to evaluate audio similarity. The performance of the proposed approach is analyzed in comparison with other alternative methods and on the base of human responses."
93,Ya-Xi Chen;René Klüber,ThumbnailDJ: Visual Thumbnails of Music Content.,2010,https://doi.org/10.5281/zenodo.1418271,"Ya-Xi Chen, University of Munich, DEU, education;René Klüber, University of Munich, DEU, education","Musical perception is non-visual and people cannot describe what a song sounds like without listening to it. To facilitate music browsing and searching, we explore the automatic generation of visual thumbnails for music. Targeting an expert user groups, DJs, we developed a concept named ThumbnailDJ: Based on a metaphor of music notation, a visual thumbnail can be automatically generated for an audio file, including information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, and our concept was preferred most. Based on the results of this interview, we refined ThumbnailDJ and conducted an evaluation with DJs. The results confirmed that ThumbnailDJ can facilitate expert users browsing and searching within their music collection."
94,Rafael Ferrer;Tuomas Eerola,Timbral Qualities of Semantic Structures of Music.,2010,https://doi.org/10.5281/zenodo.1416484,"Rafael Ferrer, Finnish Centre of Excellence in Interdisciplinary Music Research, FIN, facility;Tuomas Eerola, Finnish Centre of Excellence in Interdisciplinary Music Research, FIN, facility","The rapid expansion of social media in music has provided the ﬁeld with impressive datasets that offer insights into the semantic structures underlying everyday uses and classiﬁcation of music. We hypothesize that the organization of these structures are rather directly linked with the ”qualia” of the music as sound. To explore the ways in which these structures are connected with the qualities of sounds, a semantic space was extracted from a large collection of musical tags with latent semantic and cluster analysis. The perceptual and musical properties of 19 clusters were investigated by a similarity rating task that used spliced musical excerpts representing each cluster. The resulting perceptual space denoting the clusters correlated high with selected acoustical features extracted from the stimuli. The ﬁrst dimension related to the high-frequency energy content, the second to the regularity of the spectrum, and the third to the ﬂuctuations within the spectrum. These ﬁndings imply that meaningful organization of music may be derived from low-level descriptions of the excerpts. Novel links with the functions of music embedded into the tagging information included within the social media are proposed."
95,Kjell Lemström,Towards More Robust Geometric Content-Based Music Retrieval.,2010,https://doi.org/10.5281/zenodo.1415150,"Kjell Lemström, University of Helsinki, FIN, education","This paper studies the problem of transposition and time-scale invariant (ttsi) polyphonic music retrieval in symbolically encoded music. In the setting, music is represented by sets of points in plane. We give two new algorithms. Applying a search window of size w and given a query point set, of size m, to be searched for in a database point set, of size n, our algorithm for exact ttsi occurrences runs in O(mwn log n) time; for partial occurrences we have an O(mnw2 log n) algorithm. The framework used is flexible allowing development towards even more robust geometric retrieval."
96,Bryan Duggan;Brendan O'Shea,Tunepal - Disseminating a Music Information Retrieval System to the Traditional Irish Music Community.,2010,https://doi.org/10.5281/zenodo.1416312,"Bryan Duggan, Dublin Institute of Technology, IRL, education;Brendan O’ Shea, Dublin Institute of Technology, IRL, education","In this paper we present two new query-by-playing (QBP) music information retrieval (MIR) systems aimed at musicians playing traditional Irish dance music. Firstly, a browser hosted system - tunepal.org is presented. Secondly, we present Tunepal for iPhone/iPod touch devices - a QBP system that can be used in situ in traditional music sessions. Both of these systems use a backend corpus of 13,290 tunes drawn from community sources and “standard” references. These systems have evolved from academic research to become popular tools used by musicians around the world. 16,064 queries have been logged since the systems were launched on 31 July, 2009 and 11 February, 2010 respectively to 18 May 2010. As we log data on every query made, including geocoding queries made on the iPhone, we propose that these tools may be used to follow trends in the playing of traditional music. We also present an analysis of the data we have collected on the usage of these systems."
97,Florian Eyben;Sebastian Böck;Björn W. Schuller;Alex Graves,Universal Onset Detection with Bidirectional Long Short-Term Memory Neural Networks.,2010,https://doi.org/10.5281/zenodo.1417131,"Florian Eyben, Technische Universität München, DEU, education;Sebastian Böck, Technische Universität München, DEU, education;Björn Schuller, Technische Universität München, DEU, education;Alex Graves, Technische Universität München, DEU, education","Many different onset detection methods have been proposed in recent years. However those that perform well tend to be highly specialised for certain types of music, while those that are more widely applicable give only moderate performance. In this paper we present a new onset detector with superior performance and temporal precision for all kinds of music, including complex music mixes. It is based on auditory spectral features and relative spectral differences processed by a bidirectional Long Short-Term Memory recurrent neural network, which acts as reduction function. The network is trained with a large database of onset data covering various genres and onset types. Due to the data driven nature, our approach does not require the onset detection method and its parameters to be tuned to a particular type of music. We compare results on the Bello onset data set and can conclude that our approach is on par with related results on the same set and outperforms them in most cases in terms of F1-measure. For complex music with mixed onset types, an absolute improvement of 3.6% is reported."
98,Mathieu Lagrange;Joan Serrà,Unsupervised Accuracy Improvement for Cover Song Detection Using Spectral Connectivity Network.,2010,https://doi.org/10.5281/zenodo.1416998,"Mathieu Lagrange, IRCAM-CNRS UMR 9912, FRA, facility;Joan Serr`a, Universitat Pompeu Fabra, ESP, education","This paper introduces a new method for improving the accuracy in medium scale music similarity problems. Recently, it has been shown that the raw accuracy of query by example systems can be enhanced by considering priors about the distribution of its output or the structure of the music collection being considered. The proposed approach focuses on reducing the dependency to those priors by considering an eigenvalue decomposition of the aforementioned system’s output. Experiments carried out in the framework of cover song detection show that the proposed approach has good performance for enhancing a high accuracy system. Furthermore, it maintains the accuracy level for lower performing systems."
99,Audrey Laplante,Users' Relevance Criteria in Music Retrieval in Everyday Life: An Exploratory Study.,2010,https://doi.org/10.5281/zenodo.1415578,"Audrey Laplante, École de bibliothéconomie et des sciences de l’information, Université de Montréal, Montréal, QC, Canada, education","""The paper presents the findings of a qualitative study on the way young adults make relevance inferences about music items when searching for music for recreational purposes. Data were collected through in-depth interviews and analyzed following the constant comparative method. Content analysis revealed that participants used four types of clues to make relevance inferences: bibliographic metadata (e.g., names of contributors, labels), relational metadata (e.g., genres, similar artists), associative metadata (e.g., cover arts), and recommendations/reviews. Relevance judgments were also found to be influenced by the external context (i.e., the functions music plays in one’s life) and the internal context (i.e., individual tastes and beliefs, state of mind)."""
100,Gabriel Vigliensoni;Cory McKay;Ichiro Fujinaga,Using jWebMiner 2.0 to Improve Music Classification Performance by Combining Different Types of Features Mined from the Web.,2010,https://doi.org/10.5281/zenodo.1416590,"Gabriel Vigliensoni, CIRMMT, McGill University, CAN, education;Cory McKay, CIRMMT, McGill University, CAN, education;Ichiro Fujinaga, CIRMMT, McGill University, CAN, education","""This paper presents the jWebMiner 2.0 cultural feature extraction software and describes the results of several musical genre classification experiments performed with it. jWebMiner 2.0 is an easy-to-use and open-source tool that allows users to mine the Internet in order to extract features based on both Last.fm social tags and general web search string co-occurrences extracted using the Yahoo! API. The experiments performed found that the features based on social tags were more effective at classifying music into a small (5-genre) genre ontology, but the features based on general web co-occurrences were more effective at classifying a moderate (10-genre) ontology. It was also found that combining the two types of features resulted in improved performance overall."""
101,Björn W. Schuller;Christoph Kozielski;Felix Weninger;Florian Eyben;Gerhard Rigoll,Vocalist Gender Recognition in Recorded Popular Music.,2010,https://doi.org/10.5281/zenodo.1415984,"Björn Schuller, Technische Universität München, DEU, education;Christoph Kozielski, Technische Universität München, DEU, education;Felix Weninger, Technische Universität München, DEU, education;Florian Eyben, Technische Universität München, DEU, education;Gerhard Rigoll, Technische Universität München, DEU, education","We introduce the task of vocalist gender recognition in popular music and evaluate the beneﬁt of Non-Negative Matrix Factorization based enhancement of melodic components to this aim. The underlying automatic separation of drum beats is described in detail, and the obtained signiﬁcant gain by its use is veriﬁed in extensive test-runs on a novel database of 1.5 days of MP3 coded popular songs based on transcriptions of the Karaoke-game UltraStar. As classiﬁers serve Support Vector Machines and Hidden Naive Bayes. Overall, the suggested methods lead to fully automatic recognition of the pre-dominant vocalist gender at 87.31 % accuracy on song level for artists unkown to the system in originally recorded music."
102,Xiao Hu 0001;J. Stephen Downie,When Lyrics Outperform Audio for Music Mood Classification: A Feature Analysis.,2010,https://doi.org/10.5281/zenodo.1415540,"Xiao Hu, University of Illinois at Urbana-Champaign, USA, education;J. Stephen Downie, University of Illinois at Urbana-Champaign, USA, education","This paper builds upon and extends previous work on multi-modal mood classification (i.e., combining audio and lyrics) by analyzing in-depth those feature types that have shown to provide statistically significant improvements in the classification of individual mood categories. The dataset used in this study comprises 5,296 songs (with lyrics and audio for each) divided into 18 mood categories derived from user-generated tags taken from last.fm. These 18 categories show remarkable consistency with the popular Russell’s mood model. In seven categories, lyric features significantly outperformed audio spectral features. In one category only, audio outperformed all lyric features types. A fine grained analysis of the significant lyric feature types indicates a strong and obvious semantic association between extracted terms and the categories. No such obvious semantic linkages were evident in the case where audio spectral features proved superior."
103,Jouni Paulus;Meinard Müller;Anssi Klapuri,State of the Art Report: Audio-Based Music Structure Analysis.,2010,https://doi.org/10.5281/zenodo.1417289,"Jouni Paulus, Fraunhofer Institute for Integrated Circuits IIS, DEU, facility;Meinard Müller, Saarland University, DEU, education; MPI Informatik, DEU, education;Anssi Klapuri, Queen Mary Univ. of London, GBR, education","Humans tend to organize perceived information into hierarchies and structures, a principle that also applies to music. Even musically untrained listeners unconsciously analyze and segment music with regard to various musical aspects, for example, identifying recurrent themes or detecting temporal boundaries between contrasting musical parts. This paper gives an overview of state-of-the-art methods for computational music structure analysis, where the general goal is to divide an audio recording into temporal segments corresponding to musical parts and to group these segments into musically meaningful categories. There are many different criteria for segmenting and structuring music audio. In particular, one can identify three conceptually different approaches, which we refer to as repetition-based, novelty-based, and homogeneity-based approaches. Furthermore, one has to account for different musical dimensions such as melody, harmony, rhythm, and timbre. In our state-of-the-art report, we address these different issues in the context of music structure analysis, while discussing and categorizing the most relevant and recent articles in this field."
104,Michael Scott Cuthbert;Christopher Ariza,Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data.,2010,https://doi.org/10.5281/zenodo.1416114,"Michael Scott Cuthbert, Massachusetts Institute of Technology, USA, education;Christopher Ariza, Massachusetts Institute of Technology, USA, education","Music21 is an object-oriented toolkit for analyzing, 
searching, and transforming music in symbolic (score-
based) forms. The modular approach of the project allows 
musicians and researchers to write simple scripts rapidly 
and reuse them in other projects. The toolkit aims to pro-
vide powerful software tools integrated with sophisticated 
musical knowledge to both musicians with little pro-
gramming experience (especially musicologists) and to 
programmers with only modest music theory skills. 
This paper introduces the music21 system, demon-
strating how to use it and the types of problems it is well-
suited toward advancing. We include numerous examples 
of its power and flexibility, including demonstrations of 
graphing data and generating annotated musical scores."
105,Jeffrey J. Scott;Raymond Migneco;Brandon G. Morton;Christian M. Hahn;Paul J. Diefenbach;Youngmoo E. Kim,An Audio Processing Library for MIR Application Development in Flash.,2010,https://doi.org/10.5281/zenodo.1414740,"Jeffrey Scott, Drexel University, USA, education;Raymond Migneco, Drexel University, USA, education;Brandon Morton, Drexel University, USA, education;Christian M. Hahn, Drexel University, USA, education;Paul Diefenbach, Drexel University, USA, education;Youngmoo E. Kim, Drexel University, USA, education","In recent years, the Adobe Flash platform has risen as a credible and universal platform for rapid development and deployment of interactive web-based applications. It is also the accepted standard for delivery of streaming me- dia, and many web applications related to music informa- tion retrieval, such as Pandora, Last.fm and Musicovery, are built using Flash. The limitations of Flash, however, have made it difﬁcult for music-IR researchers and de- velopers to utilize complex sound and music signal pro- cessing within their web applications. Furthermore, the real-time audio processing and synchronization required for some music-IR-related activities demands signiﬁcant computational power and specialized audio algorithms, far beyond what is possible to implement using Flash script- ing. By taking advantage of features recently added to the platform, including dynamic audio control and C cross- compilation for near-native performance, we have devel- oped the Audio-processing Library for Flash (ALF), pro- viding developers with a library of common audio pro- cessing routines and affording Flash developers a degree of sound interaction previously unavailable through web- based platforms. We present several music-IR-driven ap- plications that incorporate ALF to demonstrate its utility."
106,Peter Grosche;Meinard Müller;Craig Stuart Sapp,What Makes Beat Tracking Difficult? A Case Study on Chopin Mazurkas.,2010,https://doi.org/10.5281/zenodo.1415852,"Peter Grosche, Saarland University, DEU, education, MPI Informatik, DEU, facility;Meinard Müller, Saarland University, DEU, education, MPI Informatik, DEU, facility;Craig Stuart Sapp, Standford University, USA, education","The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat tracking approaches still have signiﬁcant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for detecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musical properties of certain beats that frequently evoke tracking errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimental results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material."
107,Charlie Inskip;Andy MacFarlane;Pauline Rafferty,"Upbeat and Quirky, With a Bit of a Build: Interpretive Repertoires in Creative Music Search.",2010,https://doi.org/10.5281/zenodo.1417617,"Charlie Inskip, City University London, GBR, education;Andy MacFarlane, City University London, GBR, education;Pauline Rafferty, University of Aberystwyth, GBR, education","Pre-existing commercial music is widely used to accom-
pany moving images in films, TV commercials and com-
puter games. This process is known as music synchronisa-
tion. Professionals are employed by rights holders and  
film makers to perform creative music searches on large 
catalogues to find appropriate pieces of music for syn-
chronisation. This paper discusses a Discourse Analysis 
of thirty interview texts related to the process. Coded ex-
amples are presented and discussed. Four interpretive re-
pertoires are identified: the Musical Repertoire, the 
Soundtrack Repertoire, the Business Repertoire and the 
Cultural Repertoire. These ways of talking about music 
are adopted by all of the community regardless of their 
interest as Music Owner or Music User. 
Music is shown to have multi-variate and sometimes 
conflicting meanings within this community which are 
dynamic and negotiated. This is related to a theoretical 
feedback model of communication and meaning making 
which proposes that Owners and Users employ their own 
and shared ways of talking and thinking about music and 
its context to determine musical meaning. The value to 
the music information retrieval community is to inform 
system design from a user information needs perspective."
108,Emmanuel Vincent;Stanislaw Andrzej Raczynski;Nobutaka Ono;Shigeki Sagayama,A Roadmap Towards Versatile MIR.,2010,https://doi.org/10.5281/zenodo.1418141,"Emmanuel Vincent, INRIA, FRA, facility;Stanisław A. Raczyński, The University of Tokyo, JPN, education;Nobutaka Ono, The University of Tokyo, JPN, education;Shigeki Sagayama, The University of Tokyo, JPN, education","""Most MIR systems are speciﬁcally designed for one application and one cultural context and suffer from the semantic gap between the data and the application. Advances in the theory of Bayesian language and information processing enable the vision of a versatile, meaningful and accurate MIR system integrating all levels of information. We propose a roadmap to collectively achieve this vision."""
109,Jacek Wolkowicz;Vlado Keselj,Predicting Development of Research in Music Based on Parallels with Natural Language Processing.,2010,https://doi.org/10.5281/zenodo.1416808,"Jacek Wołkowicz, Dalhousie University, CAN, education;Vlado Keˇselj, Dalhousie University, CAN, education","The hypothesis of the paper is that the domain of Nat-
ural Languages Processing (NLP) resembles current re-
search in music so one could beneﬁt from this by employ-
ing NLP techniques to music. In this paper the similarity
between both domains is described. The levels of NLP are
listed with pointers to respective tasks within the research
of computational music. A brief introduction to history of
NLP enables locating music research in this history. Pos-
sible directions of research in music, assuming its afﬁnity
to NLP, are introduced. Current research in generational
and statistical music modeling is compared to similar NLP
theories. The paper is concluded with guidelines for music
research and information retrieval."
