Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,J. Stephen Downie;Donald Byrd;Tim Crawford,Ten Years of ISMIR: Reflections on Challenges and Opportunities.,2009,https://doi.org/10.5281/zenodo.1415170,"J. Stephen Downie, University of Illinois at Urbana-Champaign, USA, education;Donald Byrd, Indiana University at Bloomington, USA, education;Tim Crawford, Goldsmiths College, University of London, GBR, education","The International Symposium on Music Information Retrieval (ISMIR) was born on 13 August 1999. This paper expresses the opinions of three of ISMIR’s founders as they reflect upon what has happened during its first decade. The paper provides the background context for the events that led to the establishment of ISMIR. We highlight the first ISMIR, held in Plymouth, MA in October of 2000, and use it to elucidate key trends that have influenced subsequent ISMIRs. Indicators of growth and success drawn from ISMIR publication data are presented. The role that the Music Information Retrieval Evaluation eXchange (MIREX) has played at ISMIR is examined. The factors contributing to ISMIR's growth and success are also enumerated. The paper concludes with a set of challenges and opportunities that the newly formed International Society for Music Information Retrieval should embrace to ensure the future vitality of the conference series and the ISMIR community."
1,David Bretherton;Daniel A. Smith;Monica M. C. Schraefel;Richard Polfreman;Mark Everist;Jeanice Brooks;Joe Lambert,Integrating Musicology's Heterogeneous Data Sources for Better Exploration.,2009,https://doi.org/10.5281/zenodo.1415168,"David Bretherton, University of Southampton, GBR, education;Daniel Alexander Smith, University of Southampton, GBR, education;mc schraefel, University of Southampton, GBR, education;Richard Polfreman, University of Southampton, GBR, education;Mark Everist, University of Southampton, GBR, education;Jeanice Brooks, University of Southampton, GBR, education;Joe Lambert, University of Southampton, GBR, education","Musicologists have to consult an extraordinarily heterogeneous body of primary and secondary sources during all stages of their research. Many of these sources are now available online, but the historical dispersal of material across libraries and archives has now been replaced by segregation of data and metadata into a plethora of online repositories. This segregation hinders the intelligent manipulation of metadata, and means that extracting large tranches of basic factual information or running multi-part search queries is still enormously and needlessly time consuming. To counter this barrier to research, the “musicSpace” project is experimenting with integrating access to many of musicology’s leading data sources via a modern faceted browsing interface that utilises Semantic Web and Web2.0 technologies such as RDF and AJAX. This will make previously intractable search queries tractable, enable musicologists to use their time more efficiently, and aid the discovery of potentially significant information that users did not think to look for. This paper outlines our work to date.  "
2,Kurt Jacobson;Yves Raimond;Mark B. Sandler,An Ecosystem for Transparent Music Similarity in an Open World.,2009,https://doi.org/10.5281/zenodo.1417151,"Kurt Jacobson, Queen Mary University of London, UK, education;Yves Raimond, BBC, UK, company;Mark Sandler, Queen Mary University of London, UK, education","There exist many methods for deriving music similarity associations and additional variations are likely to be seen in the future. In this work we introduce the Similarity Ontology for describing associations between items. Using a combination of RDF/OWL and N3, our ontology allows for transparency and provenance tracking in a distributed and open system. We describe a similarity ecosystem where agents assert and aggregate similarity statements on the Web of Data allowing a client application to make queries for recommendation, playlisting, or other tasks. In this ecosystem any number of similarity derivation methods can exist side-by-side, specifying similarity relationships as well as the processes used to derive these statements. The data consumer can then select which similarity statements to trust based on knowledge of the similarity derivation processes or a list of trusted assertion agents."
3,Andrew Hankinson;Laurent Pugin;Ichiro Fujinaga,Interfaces for Document Representation in Digital Music Libraries.,2009,https://doi.org/10.5281/zenodo.1414780,"Andrew Hankinson, Schulich School of Music of McGill University, CAN, education;Laurent Pugin, RISM Switzerland, CHE, facility;Ichiro Fujinaga, Schulich School of Music of McGill University, CAN, education","Musical documents, that is, documents whose primary 
content is printed music, introduce interesting design 
challenges for presentation in an online environment. 
Considerations for the unique properties of printed 
msic, as well as users’ expected levels of comfort with 
these materials, present opportunities for developing a 
viewer specifically tailored to displaying musical 
documents. 
This 
paper 
outlines 
five 
design 
considerations for a music document viewer, drawing 
examples from existing digital music libraries. We then 
present 
our 
work 
towards 
incorporating 
these 
considerations in a new digital music library system 
currently under development."
4,Rolf Inge Godøy;Alexander Refsum Jensenius,Body Movement in Music Information Retrieval.,2009,https://doi.org/10.5281/zenodo.1416668,"Rolf Inge Godøy, University of Oslo, NOR, education;Alexander Refsum Jensenius, University of Oslo, NOR, education","""We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations. There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions. The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR."""
5,Maarten Grachten;Gerhard Widmer,Who Is Who in the End? Recognizing Pianists by Their Final Ritardandi.,2009,https://doi.org/10.5281/zenodo.1417109,"Maarten Grachten, Johannes Kepler University, Linz, Austria, education;Gerhard Widmer, Austrian Research Institute for Artificial Intelligence, Vienna, Austria, facility, Johannes Kepler University, Linz, Austria, education","The performance of music usually involves a great deal of interpretation by the musician. In classical music, ﬁnal ritardandi are emblematic for the expressive aspect of music performance. In this paper we investigate to what degree individual performance style has an effect on the form of ﬁnal ritardandi. To this end we look at interonset-interval deviations from a performance norm. We deﬁne a criterion for ﬁltering out deviations that are likely to be due to measurement error. Using a machine-learning classiﬁer, we evaluate an automatic pairwise pianist identiﬁcation task as an initial assessment of the suitability of the ﬁltered data for characterizing the individual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, pianists can often be identiﬁed with accuracy signiﬁcantly above baseline."
6,Jin Ha Lee;M. Cameron Jones;J. Stephen Downie,"An Analysis of ISMIR Proceedings: Patterns of Authorship, Topic, and Citation.",2009,https://doi.org/10.5281/zenodo.1416618,"Jin Ha Lee, University of Washington, USA, education;M. Cameron Jones, University of Illinois at Urbana-Champaign, USA, education;J. Stephen Downie, University of Illinois at Urbana-Champaign, USA, education","""This paper presents analyses of peer-reviewed papers and posters published in the past nine years of ISMIR proceedings: examining publication and authorship practices, topics and titles of research, as well as the citation patterns among the ISMIR proceedings. The main objective is to provide an overview of the progress made over the past nine years in the ISMIR community and to obtain some insights into where the community should be heading in the coming years. Overall, the ISMIR community has grown considerably over the past nine years, both in the number of papers and posters published each year, as well as the number of authors contributing. Furthermore, the amount of collaboration among authors, as reflected in co-authorship, has increased. Main areas of research are revealed by an analysis of most commonly used title terms. Also, major authors and research groups are identified by analyzing the co-authorship and citation patterns in ISMIR proceedings."""
7,Maarten Grachten;Markus Schedl;Tim Pohle;Gerhard Widmer,The ISMIR Cloud: A Decade of ISMIR Conferences at Your Fingertips.,2009,https://doi.org/10.5281/zenodo.1416434,"Maarten Grachten, Johannes Kepler University, AUT, education;Markus Schedl, Johannes Kepler University, AUT, education;Tim Pohle, Johannes Kepler University, AUT, education;Gerhard Widmer, Johannes Kepler University, AUT, education","In this paper, we analyze the proceedings of the past International Symposia on Music Information Retrieval (ISMIR). We extract meaningful term sets from the accepted submissions and apply term weighting and Web-based filtering techniques to distill information about the topics covered by the papers. This enables us to visualize and interpret the change of hot ISMIR topics in the course of time. Furthermore, the performed analysis allows for assessing the cumulative ISMIR proceedings by semantic content (rather than by literal text search). To illustrate this, we introduce two prototype applications that are publicly accessible online 1 . The first allows the user to search for ISMIR publications by selecting subsets of ISMIR topics. The second provides interactive visual access to the joint content of ISMIR publications in the form of a tag cloud – the ISMIR Cloud."
8,Meinard Müller;Verena Konz;Andi Scharfstein;Sebastian Ewert;Michael Clausen,Towards Automated Extraction of Tempo Parameters from Expressive Music Recordings.,2009,https://doi.org/10.5281/zenodo.1416024,"Meinard Müller, Verena Konz, Andi Scharfstein, Saarland University and MPI Informatik, DEU, education;Sebastian Ewert, Michael Clausen, Bonn University, Computer Science, DEU, education","A performance of a piece of music heavily depends on the musician’s or conductor’s individual vision and personal interpretation of the given musical score. As basis for the analysis of artistic idiosyncrasies, one requires accurate annotations that reveal the exact timing and intensity of the various note events occurring in the performances. In the case of audio recordings, this annotation is often done manually, which is prohibitive in view of large music collections. In this paper, we present a fully automatic approach for extracting temporal information from a music recording using score-audio synchronization techniques. This information is given in the form of a tempo curve that reveals the relative tempo difference between an actual performance and some reference representation of the underlying musical piece. As shown by our experiments on harmony-based Western music, our approach allows for capturing the overall tempo flow and for certain classes of music even finer expressive tempo nuances."
9,Matija Marolt,Probabilistic Segmentation and Labeling of Ethnomusicological Field Recordings.,2009,https://doi.org/10.5281/zenodo.1415532,"Matija Marolt, University of Ljubljana, SVN, education","The paper presents a method for segmentation and labeling of ethnomusicological field recordings. Field recordings are integral documents of folk music performances and typically contain interviews with performers intertwined with actual performances. As these are live recordings of amateur folk musicians, they may contain interruptions, false starts, environmental noises or other interfering factors. Our goal was to design a robust algorithm that would approximate manual segmentation of field recordings. First, short audio fragments are classified into one of the following categories: speech, solo singing, choir singing, instrumental or bell chiming performance. Then, a set of candidate segment boundaries is obtained by observing how the energy of the signal and its content change, and finally the recording is segmented with a probabilistic model that maximizes the posterior probability of segments given a set of candidate segment boundaries with their probabilities and prior knowledge of lengths of segments belonging to different categories. Evaluation of the algorithm on a set of field recordings from the Ehtnomuse archive is presented."
10,Thibault Langlois;Gonçalo Marques,A Music Classification Method based on Timbral Features.,2009,https://doi.org/10.5281/zenodo.1417863,"Thibault Langlois, Faculdade de Ciências da Universidade de Lisboa, PRT, education;Gonçalo Marques, Instituto Superior de Engenharia de Lisboa, PRT, education","This paper describes a method for music classification based solely on the audio contents of the music signal. More specifically, the audio signal is converted into a compact symbolic representation that retains timbral characteristics and accounts for the temporal structure of a music piece. Models that capture the temporal dependencies observed in the symbolic sequences of a set of music pieces are built using a statistical language modeling approach. The proposed method is evaluated on two classification tasks (Music Genre classification and Artist Identification) using publicly available datasets. Finally, a distance measure between music pieces is derived from the method and examples of playlists generated using this distance are given. The proposed method is compared with two alternative approaches which include the use of Hidden Markov Models and a classification scheme that ignores the temporal structure of the sequences of symbols. In both cases the proposed approach outperforms the alternatives."
11,Frieder Stolzenburg,A Periodicity-based Theory for Harmony Perception and Scales.,2009,https://doi.org/10.5281/zenodo.1414904,"Frieder Stolzenburg, Hochschule Harz, Automation & Computer Sciences Department, DEU, education","Empirical results demonstrate, that human subjects rate harmonies, e.g. major and minor triads, differently with respect to their sonority. These judgements of listeners have a strong psychophysical basis. Therefore, harmony perception often is explained by the notions of dissonance and tension, computing the consonance of one or two intervals. In this paper, a theory on harmony perception based on the notion of periodicity is introduced. Mathematically, periodicity is derivable from the frequency ratios of the tones in the chord with respect to its lowest tone. The used ratios can be computed by continued fraction expansion and are psychophysically motivated by the just noticeable differences in pitch perception. The theoretical results presented here correlate well to experimental results and also explain the origin of complex chords and common musical scales."
12,Yoshiyuki Kobayashi,Automatic Generation of Musical Instrument Detector by Using Evolutionary Learning Method.,2009,https://doi.org/10.5281/zenodo.1417233,"Yoshiyuki Kobayashi, SONY Corporation, JPN, company","This paper presents a novel way of generating information extractors that obtain high-level information from recorded music such as the presence of a certain musical instrument. Our information extractor is comprised of a feature set and a discrimination or regression formula. We introduce a scheme to generate the entire information extractor given only a large amount of labeled dataset. For example, data could be waveform, and label could be the presence of musical instruments in them. We propose a very flexible description of features that allows various kinds of data other than waveform. Our proposal also includes a modified evolutionary learning method to optimize the feature set. We applied our scheme to automatically generate musical instrument detectors for mixed-down music in stereo. The experiment showed that our scheme could find a suitable set of features for the objective and could generate good detectors."
13,Andre Holzapfel;Yannis Stylianou,Rhythmic Similarity in Traditional Turkish Music.,2009,https://doi.org/10.5281/zenodo.1417861,"Andre Holzapfel, Institute of Computer Science, FORTH, GRC, facility, University of Crete, GRC, education;Yannis Stylianou, Institute of Computer Science, FORTH, GRC, facility, University of Crete, GRC, education","In this paper, the problem of automatically assigning a piece of traditional Turkish music into a class of rhythm referred to as usul is addressed. For this, an approach for rhythmic similarity measurement based on scale transforms has been evaluated on a set of MIDI data. Because this task is related to time signature estimation, the accuracy of the proposed method is evaluated and compared with a state of the art time signature estimation approach. The results indicate that the proposed method can be successfully applied to audio signals of Turkish music and that it captures relevant properties of the individual usul."
14,Emmanouil Benetos;Andre Holzapfel;Yannis Stylianou,Pitched Instrument Onset Detection based on Auditory Spectra.,2009,https://doi.org/10.5281/zenodo.1416174,"Emmanouil Benetos, Institute of Computer Science, FORTH, Greece, facility, Multimedia Informatics Lab, Computer Science Department, University of Crete, Greece, education;André Holzapfel, Institute of Computer Science, FORTH, Greece, facility, Multimedia Informatics Lab, Computer Science Department, University of Crete, Greece, education;Yannis Stylianou, Institute of Computer Science, FORTH, Greece, facility, Multimedia Informatics Lab, Computer Science Department, University of Crete, Greece, education","""In this paper, a novel method for onset detection of music signals using auditory spectra is proposed. The auditory spectrogram provides a time-frequency representation that employs a sound processing model resembling the human auditory system. Recent work on onset detection employs DFT-based features, such as the spectral ﬂux and group delay function. The spectral ﬂux and group delay are in- troduced in the auditory framework and an onset detection algorithm is proposed. Experiments are conducted on a dataset covering 11 pitched instrument types, consisting of 1829 onsets in total. Results indicate the superiority of the auditory representations over the DFT-based ones, with the auditory spectral ﬂux exhibiting an onset detection im- provement by 2% in terms of F-measure when compared to the DFT-based feature."""
15,Dominikus Baur;Tim Langer;Andreas Butz,Shades of Music: Letting Users Discover Sub-Song Similarities.,2009,https://doi.org/10.5281/zenodo.1417844,"Dominikus Baur, University of Munich (LMU), DEU, education;Tim Langer, University of Munich (LMU), DEU, education;Andreas Butz, University of Munich (LMU), DEU, education","Many interesting pieces of music violate established structures or rules of their genre on purpose. These songs can be very atypical in their interior structure and their different parts might actually allude to entirely different other songs or genres. We present a query-by-example-based user interface that shows songs related to the one currently playing. This relation is not based on overall similarity, but on the similarity between the part currently playing and parts of other songs in the collection along different dimensions (pitch, timbre, bars, beats, loudness). The similarity is initially computed automatically, but can be corrected by the user. Once a sufficient number of corrections has been made, we expect the similarity measure to reach an even higher precision. Our system thereby allows users to discover hidden similarities on the level of song sections instead of whole songs."
16,Norberto Degara-Quintela;Antonio S. Pena;Soledad Torres-Guijarro,A Comparison of Score-Level Fusion Rules for Onset Detection in Music Signals.,2009,https://doi.org/10.5281/zenodo.1415692,"Norberto Degara-Quintela, University of Vigo, ESP, education;Antonio Pena, University of Vigo, ESP, education;Soledad Torres-Guijarro, Laboratorio Oficial de Metroloxía de Galicia (LOMG), ESP, facility","Finding automatically the starting time of audio events is a difficult process. A promising approach for onset detection lies in the combination of multiple algorithms. The goal of this paper is to compare score-level fusion rules that combine signal processing algorithms in a problem of automatic detection of onsets. Previous approaches usually combine detection functions by adding these functions in the time domain. The combination methods explored in this work fuse, at score-level, the peak score information (peak time and onset probability) in order to obtain a better estimate of the probability of having an onset given the probability estimates of multiple experts. Three state-of-the-art spectral-based onset detection functions are used: a spectral flux detection function, a weighted phase deviation function, and a complex domain detection function. Both untrained and trained fusion rules will be compared using a standard data set of music excerpts."
17,Yajie Hu;Xiaoou Chen;Deshun Yang,Lyric-based Song Emotion Detection with Affective Lexicon and Fuzzy Clustering Method.,2009,https://doi.org/10.5281/zenodo.1417983,"Yajie Hu, Peking University, CHN, education;Xiaoou Chen, Peking University, CHN, education;Deshun Yang, Peking University, CHN, education","A method is proposed for detecting the emotions of Chinese song lyrics based on an affective lexicon. The lexicon is composed of words translated from ANEW and words selected by other means. For each lyric sentence, emotion units, each based on an emotion word in the lexicon, are found out, and the influences of modifiers and tenses on emotion units are taken into consideration. The emotion of a sentence is calculated from its emotion units. To figure out the prominent emotions of a lyric, a fuzzy clustering method is used to group the lyric’s sentences according to their emotions. The emotion of a cluster is worked out from that of its sentences considering the individual weight of each sentence. Clusters are weighted according to the weights and confidences of their sentences and singing speeds of sentences are considered as the adjustment of the weights of clusters. Finally, the emotion of the cluster with the highest weight is selected from the prominent emotions as the main emotion of the lyric. The performance of our approach is evaluated through an experiment of emotion classification of 500 Chinese song lyrics."
18,Klaus Seyerlehner;Peter Knees;Dominik Schnitzer;Gerhard Widmer,Browsing Music Recommendation Networks.,2009,https://doi.org/10.5281/zenodo.1416972,"Klaus Seyerlehner, Johannes Kepler University, AUT, education;Peter Knees, Johannes Kepler University, AUT, education;Dominik Schnitzer, Austrian Research Institute for AI, AUT, facility;Gerhard Widmer, Austrian Research Institute for AI, AUT, facility",Many music portals offer the possibility to explore mu- sic collections via browsing automatically generated mu- sic recommendations. In this paper we argue that such music recommender systems can be transformed into an equivalent recommendation graph. We then analyze the recommendation graph of a real-world content-based mu- sic recommender systems to ﬁnd out if users can really explore the underlying song database by following those recommendations. We ﬁnd that some songs are not rec- ommended at all and are consequently not reachable via browsing. We then take a ﬁrst attempt to modify a recom- mendation network in such a way that the resulting net- work is better suited to explore the respective music space.
19,Hiromi Ishizaki;Keiichiro Hoashi;Yasuhiro Takishima,Full-Automatic DJ Mixing System with Optimal Tempo Adjustment based on Measurement Function of User Discomfort.,2009,https://doi.org/10.5281/zenodo.1418231,"Hiromi Ishizaki, KDDI R&D Laboratories Inc., JPN, company;Keiichiro Hoashi, KDDI R&D Laboratories Inc., JPN, company;Yasuhiro Takishima, KDDI R&D Laboratories Inc., JPN, company","This paper proposes an automatic DJ mixing method that can automate the processes of real world DJs and describes a prototype for a fully automatic DJ mix-like playing system. Our goal is to achieve a fully automatic DJ mixing system that can preserve overall user comfort level during DJ mixing. In this paper, we assume that the difference between the original and adjusted songs is the main cause of user discomfort in the mixed song. In order to preserve user comfort, we define the measurement function of user discomfort based on the results of a subjective experiment. Furthermore, this paper proposes a unique tempo adjustment technique called “optimal tempo adjustment”, which is robust for any combination of tempi of songs to be mixed. In the subjective experiment, the proposed method obtained higher averages of user ratings on three evaluation items compared to the conventional method. These results indicate that our system is able to preserve user comfort."
20,David Gross-Amblard;Philippe Rigaux;Lylia Abrouk;Nadine Cullot,Fingering Watermarking in Symbolic Digital Scores.,2009,https://doi.org/10.5281/zenodo.1416454,"David Gross-Amblard, Université de Bourgogne, FRA, education;Philippe Rigaux, Université de Dauphine, Paris IX, FRA, education;Lylia Abrouk, Université de Bourgogne, FRA, education;Nadine Cullot, Université de Bourgogne, FRA, education","We propose a new watermarking method that hides the writer’s identity into symbolic musical scores featuring ﬁn-gering annotations. These annotations constitute a valu-able part of the symbolic representation, yet they can be slightly modiﬁed without altering the quality of the musi-cal information. The method applies a controlled distortion of the existing ﬁngerings so that unauthorized copies can be identiﬁed. The proposed watermarking method is robust against attacks like random ﬁngering alterations and score cropping, and its detection does not require the original ﬁngering, but only the suspect one. The method is general and applies to various ﬁngering contexts and instruments."
21,Yi-Hsuan Yang;Yu-Ching Lin;Ann Lee;Homer H. Chen,Improving Musical Concept Detection by Ordinal Regression and Context Fusion.,2009,https://doi.org/10.5281/zenodo.1416650,"Yi-Hsuan Yang, National Taiwan University, TWN, education;Yu-Ching Lin, National Taiwan University, TWN, education;Ann Lee, National Taiwan University, TWN, education;Homer Chen, National Taiwan University, TWN, education","To facilitate information retrieval of large-scale music databases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept detection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label classification methods. To further improve the detection accuracy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selection to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924."
22,Laurent Oudre;Yves Grenier;Cédric Févotte,Template-based Chord Recognition : Influence of the Chord Types.,2009,https://doi.org/10.5281/zenodo.1414884,"Laurent Oudre, Institut TELECOM, TELECOM ParisTech, CNRS LTCI, FRA, education;Yves Grenier, Institut TELECOM, TELECOM ParisTech, CNRS LTCI, FRA, education;C´edric F´evotte, CNRS LTCI, TELECOM ParisTech, FRA, education","This paper describes a fast and efficient template-based chord recognition method. We introduce three chord models taking into account one or more harmonics for the notes of the chord. The use of pre-determined chord models enables to consider several types of chords (major, minor, dominant seventh, minor seventh, augmented, diminished...). After extracting a chromagram from the signal, the detected chord over a frame is the one minimizing a measure of fit between the chromagram frame and the chord templates. Several popular measures in the probability and signal processing field are considered for our task. In order to take into account the time persistence, we perform a post-processing filtering over the recognition criteria. The transcription tool is evaluated on the 13 Beatles albums with different chord types and compared to state-of-the-art chord recognition methods. We particularly focus on the influence of the chord types considered over the performances of the system. Experimental results show that our method outperforms the state-of-the-art and more importantly is less computationally demanding than the other evaluated systems."
23,Ioannis Karydis;Alexandros Nanopoulos;Hans-Henning Gabriel;Myra Spiliopoulou,Tag-Aware Spectral Clustering of Music Items.,2009,https://doi.org/10.5281/zenodo.1417483,"Ioannis Karydis, Ionian University, GRC, education;Alexandros Nanopoulos, Hildesheim University, DEU, education;Hans-Henning Gabriel, Otto-von-Guericke-University Magdeburg, DEU, education;Myra Spiliopoulou, Otto-von-Guericke-University Magdeburg, DEU, education","Social tagging is an increasingly popular phenomenon with substantial impact on Music Information Retrieval (MIR). Tags express the personal perspectives of the user on the music items (such as songs, artists, or albums) they tagged. These personal perspectives should be taken into account in MIR tasks that assess the similarity between music items. In this paper, we propose an novel approach for clustering music items represented in social tagging systems. Its characteristic is that it determines similarity between items by preserving the 3-way relationships among the inherent dimensions of the data, i.e., users, items, and tags. Conversely to existing approaches that use reductions to 2-way relationships (between items-users or items-tags), this characteristic allows the proposed algorithm to consider the personal perspectives of tags and to improve the clustering quality. Due to the complexity of social tagging data, we focus on spectral clustering that has been proven effective in addressing complex data. However, existing spectral clustering algorithms work with 2-way relationships. To overcome this problem, we develop a novel data-modeling scheme and a tag-aware spectral clustering procedure that uses tensors (high-dimensional arrays) to store the multi-graph structures that capture the personalised aspects of similarity. Experimental results with data from Last.fm indicate the superiority of the proposed method in terms of clustering quality over conventional spectral clustering approaches that consider only 2-way relationships."
24,Christopher Santoro;Corey Cheng,Multiple F0 Estimation in the Transform Domain.,2009,https://doi.org/10.5281/zenodo.1416242,"Christopher A. Santoro, LSB Audio, USA, company;Corey I. Cheng, University of Miami, USA, education","A novel algorithm is proposed to estimate the fundamental frequencies present in polyphonic acoustic mixtures expressed in a transform domain. As an example, the algorithm operates on Modified Discrete Cosine Transform (MDCT) coefficients in order to demonstrate the utility of the method in commercially available perceptual audio codecs which use the MDCT. An auditory model is developed along with several optimizations that deal with the constraints of processing in the transform-domain, including an interpolation method, a transform-domain half-wave rectification model, tonal component estimation, and sparse convolution. Test results are separated by instrument and analyzed in detail. The proposed algorithm is shown to perform comparably to state of the art time-domain methods."
25,Zoltán Juhász,Motive Identification in 22 Folksong Corpora Using Dynamic Time Warping and Self Organizing Maps.,2009,https://doi.org/10.5281/zenodo.1416216,"Zoltán Juhász, Research Institute for Technical Physics and Materials Science, HUN, facility","A system for automatic motive identification of large folksong corpora is described in this article. The method is based on a dynamic time warping algorithm determining inherent repeating elements of the melodies and a self-organizing map that learns the most typical motive contours. Using this system, the typical motive collections of 22 cultures in Eurasia have been determined, and another great common self organising map has been trained by the unified collection of the national/areal motive collections. The analysis of the overlaps of the national-areal excitations on the common map allowed us to draw a graph of connections, which shows two main distinct groups, according to the geographical distribution."
26,Matthias Gruhne;Christian Dittmar;Daniel Gärtner,Improving Rhythmic Similarity Computation by Beat Histogram Transformations.,2009,https://doi.org/10.5281/zenodo.1417635,"Matthias Gruhne, Bach Technology AS, NOR, company;Christian Dittmar, Fraunhofer IDMT, DEU, facility;Daniel Gaertner, Fraunhofer IDMT, DEU, facility","Rhythmic descriptors are often utilized for semantic mu-
sic classiﬁcation, such as genre recognition or tempo de-
tection. Several algorithms dealing with the extraction of
rhythmic information from music signals were proposed in
literature. Most of them derive a so-called beat histogram
by auto-correlating a representation of the temporal enve-
lope of the music signal. To circumvent the problem of
tempo dependency, post-processing via higher-order statis-
tics has been reported. Tests concluded, that these statis-
tics are still tempo dependent to a certain extent.
This
paper describes a method, which transforms the original
auto-correlated envelope into a tempo-independent rhyth-
mic feature vector by multiplying the lag-axis with a stretch
factor. This factor is computed with a new correlation tech-
nique which works in the logarithmic domain. The pro-
posed method is evaluated for rhythmic similarity, consist-
ing of two tasks: One test with manually created rhythms
as proof of concept and another test using a large real-
world music archive."
27,Parag Chordia;Alex Rae,Using Source Separation to Improve Tempo Detection.,2009,https://doi.org/10.5281/zenodo.1416536,"Parag Chordia, Georgia Institute of Technology, USA, education;Alex Rae, Georgia Tech Center for Music Technology, USA, education","We describe a novel tempo estimation method based on decomposing musical audio into sources using principal latent component analysis (PLCA). The approach is motivated by the observation that in rhythmically complex music, some layers may be more rhythmically regular than the overall mix, thus facilitating tempo detection. Each excerpt was analyzed using PLCA and the resulting components were each tempo tracked using a standard autocorrelation-based algorithm. We describe several techniques for aggregating or choosing among the multiple estimates that result from this process to extract a global tempo estimate. The system was evaluated on the MIREX 2006 training database as well as a newly constructed database of rhythmically complex electronic music consisting of 27 examples (IDM DB). For these databases the algorithms improved accuracy by 10% (60% vs 50%) and 22.3% (48.2% vs. 25.9%) respectively. These preliminary results suggest that for some types of music, source-separation may lead to better tempo detection."
28,Peter Grosche;Meinard Müller,A Mid-Level Representation for Capturing Dominant Tempo and Pulse Information in Music Recordings.,2009,https://doi.org/10.5281/zenodo.1416016,"Peter Grosche, Saarland University and MPI Informatik, DEU, education;Meinard Müller, Saarland University and MPI Informatik, DEU, education","Automated beat tracking and tempo estimation from music recordings become challenging tasks in the case of non-percussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation which captures predominant local pulse information. To this end, we first derive a tempogram by performing a local spectral analysis on a previously extracted, possibly very noisy onset representation. From this, we derive for each time position the predominant tempo as well as a sinusoidal kernel that best explains the local periodic nature of the onset representation. Then, our main idea is to accumulate the local kernels over time yielding a single function that reveals the predominant local pulse (PLP). We show that this function constitutes a robust mid-level representation from which one can derive musically meaningful tempo and beat information for non-percussive music even in the presence of significant tempo fluctuations. Furthermore, our representation allows for incorporating prior knowledge on the expected tempo range to exhibit information on different pulse levels."
29,Dominik Lübbers;Matthias Jarke,Adaptive Multimodal Exploration of Music Collections.,2009,https://doi.org/10.5281/zenodo.1415518,"Dominik Lübbers, RWTH Aachen University, DEU, education; German University of Technology, OMN, education;Matthias Jarke, RWTH Aachen University, DEU, education","""Discovering music that we like rarely happens as a result of a directed search. Except for the case where we have exact meta data at hand it is hard to articulate what song is attractive to us. Therefore it is essential to develop and evaluate systems that support guided exploratory browsing of the music space. While a number of algorithms for organizing music collections according to a given similarity measure have been applied successfully, the generated structure is usually only presented visually and listening requires cumbersome skipping through the individual pieces. To close this media gap we describe an immersive multimodal exploration environment which extends the presentation of a song collection in a video-game-like virtual 3-D landscape by carefully adjusted spatialized plackback of songs. The user can freely navigate through the virtual world guided by the acoustic clues surrounding him. Observing his interaction with the environment the system furthermore learns the user’s way of structuring his collection by adapting a weighted combination of a wide range of integrated content-based, meta-data-based and collaborative similarity measures. Our evaluation proves the importance of auditory feedback for music exploration and shows that our system is capable of adjusting to different notions of similarity."""
30,Chao-Ling Hsu;Liang-Yu Chen;Jyh-Shing Roger Jang;Hsing-Ji Li,Singing Pitch Extraction from Monaural Polyphonic Songs by Contextual Audio Modeling and Singing Harmonic Enhancement.,2009,https://doi.org/10.5281/zenodo.1418009,"Chao-Ling Hsu, National Tsing Hua Univ., TWN, education;Liang-Yu Chen, National Tsing Hua Univ., TWN, education;Jyh-Shing Roger Jang, National Tsing Hua Univ., TWN, education;Hsing-Ji Li, Institute for Information Industry, TWN, facility","This paper proposes a novel approach to extract the pitches of singing voices from monaural polyphonic songs. The hidden Markov model (HMM) is adopted to model the transition between adjacent singing pitches in time, and the relationships between melody and its chord, which is implicitly represented by features extracted from the spectrum. Moreover, another set of features which represents the energy distribution of the enhanced singing harmonic structure is proposed by applying a normalized sub-harmonic summation technique. By using these two feature sets with complementary characteristics, a 2-stream HMM is constructed for singing pitch extraction. Quantitative evaluation shows that the proposed system outperforms the compared approaches for singing pitch extraction from polyphonic songs."
31,Keiichiro Hoashi;Shuhei Hamawaki;Hiromi Ishizaki;Yasuhiro Takishima;Jiro Katto,Usability Evaluation of Visualization Interfaces for Content-based Music Retrieval Systems.,2009,https://doi.org/10.5281/zenodo.1414834,"Keiichiro Hoashi, KDDI R&D Laboratories Inc., JPN, company;Shuhei Hamawaki, Graduate School of Science and Engineering, Waseda University, JPN, education;Hiromi Ishizaki, KDDI R&D Laboratories Inc., JPN, company;Yasuhiro Takishima, KDDI R&D Laboratories Inc., JPN, company;Jiro Katto, Graduate School of Science and Engineering, Waseda University, JPN, education","This research presents a formal user evaluation of a typical visualization method for content-based music information retrieval (MIR) systems, and also proposes a novel interface to improve MIR usability. Numerous interfaces to visualize content-based MIR systems have been proposed, but reports on user evaluations of such proposed GUIs are scarce. This research aims to evaluate the effectiveness of a typical 2-D visualization method for content-based MIR systems, by conducting comparative user evaluations against the traditional list-based format to present MIR results to the user. Based on the observations of the experimental results, we next propose a 3-D visualization system, which features a function to specify sub-regions of the feature space based on genre classification results, and a function which allows users to select features that are assigned to the axes of the 3-D space. Evaluation of this GUI conclude that the functions of the 3-D system can significantly improve both the efficiency and usability of MIR systems."
32,Heng-Yi Lin;Yin-Tzu Lin;Ming-Chun Tien;Ja-Ling Wu,Music Paste: Concatenating Music Clips based on Chroma and Rhythm Features.,2009,https://doi.org/10.5281/zenodo.1415758,"Heng-Yi Lin, National Taiwan University, TWN, education;Yin-Tzu Lin, National Taiwan University, TWN, education;Ming-Chun Tien, National Taiwan University, TWN, education;Ja-Ling Wu, National Taiwan University, TWN, education","In this paper, we provide a tool for automatically choosing appropriate music clips from a given audio collection and properly combining the chosen clips. To seamlessly concatenate two different music clips without causing any audible defect is really a hard nut to crack. Borrowing the idea from the musical dice game and the DJ’s strategy and considering psychoacoustics, we employ the currently available audio analysis and editing techniques to paste music sounded as pleasant as possible. Besides, we conduct subjective evaluations on the correlation between pasting methods and the auditory quality of combined clips. The experimental results show that the automatically generated music pastes are acceptable to most of the evaluators. The proposed system can be used to generate lengthened or shortened background music and dancing suite, which is useful for some audio-assisted multimedia applications."
33,Emiru Tsunoo;Nobutaka Ono;Shigeki Sagayama,Musical Bass-Line Pattern Clustering and Its Application to Audio Genre Classification.,2009,https://doi.org/10.5281/zenodo.1417141,"Emiru Tsunoo, The University of Tokyo, JPN, education;Nobutaka Ono, The University of Tokyo, JPN, education;Shigeki Sagayama, The University of Tokyo, JPN, education","This paper discusses a new approach for clustering musical bass-line patterns representing particular genres and its application to audio genre classification. Many musical genres are characterized not only by timbral information but also by distinct representative bass-line patterns. So far this kind of temporal features have not so effectively been utilized. In particular, modern music songs mostly have certain fixed bar-long bass-line patterns per genre. For instance, while frequently bass-lines in rock music have constant pitch and a uniform rhythm, in jazz music there are many characteristic movements such as walking bass. We propose a representative bass-line pattern template extraction method based on k-means clustering handling a pitch-shift problem. After extracting the fundamental bass-line pattern templates for each genre, distances from each template are calculated and used as a feature vector for supervised learning. Experimental result shows that the automatically calculated bass-line pattern information can be used for genre classification effectively and improve upon current approaches based on timbral features."
34,Joan Serrà;Massimiliano Zanin;Cyril Laurier;Mohamed Sordo,Unsupervised Detection of Cover Song Sets: Accuracy Improvement and Original Identification.,2009,https://doi.org/10.5281/zenodo.1418063,"Joan Serr`a, Universitat Pompeu Fabra, ESP, education;Massimiliano Zanin, Universidad Autónoma de Madrid, ESP, education;Cyril Laurier, Universitat Pompeu Fabra, ESP, education;Mohamed Sordo, Universitat Pompeu Fabra, ESP, education","The task of identifying cover songs has formerly been studied in terms of a prototypical query retrieval framework. However, this framework is not the only one the task allows. In this article, we revise the task of identifying cover songs to include the notion of sets (or groups) of covers. In particular, we study the application of unsupervised clustering and community detection algorithms to detect cover sets. We consider current state-of-the-art algorithms and propose new methods to achieve this goal. Our experiments show that the detection of cover sets is feasible, that it can be performed in a reasonable amount of time, that it does not require extensive parameter tuning, and that it presents certain robustness to inaccurate measurements. Furthermore, we highlight two direct outcomes that naturally arise from the proposed framework revision: increasing the accuracy of query retrieval-based systems and detecting the original song within a set of covers."
35,Matthias Mauch;Katy C. Noland;Simon Dixon,Using Musical Structure to Enhance Automatic Chord Transcription.,2009,https://doi.org/10.5281/zenodo.1414844,"Matthias Mauch, Queen Mary University of London, Centre for Digital Music, GBR, education;Katy Noland, Queen Mary University of London, Centre for Digital Music, GBR, education;Simon Dixon, Queen Mary University of London, Centre for Digital Music, GBR, education","Chord extraction from audio is a well-established music computing task, and many valid approaches have been presented in recent years that use different chord templates, smoothing techniques and musical context models. The present work shows that additional exploitation of the repetitive structure of songs can enhance chord extraction, by combining chroma information from multiple occurrences of the same segment type. To justify this claim we modify an existing chord labelling method, providing it with manual or automatic segment labels, and compare chord extraction results on a collection of 125 songs to baseline methods without segmentation information. Our method results in consistent and more readily readable chord labels and provides a statistically significant boost in label accuracy."
36,Jennifer MacRitchie;Bryony Buck;Nicholas J. Bailey,Visualising Musical Structure Through Performance Gesture.,2009,https://doi.org/10.5281/zenodo.1418205,"J.MacRitchie, University of Glasgow, GBR, education;B.Buck, University of Glasgow, GBR, education;N.J.Bailey, University of Glasgow, GBR, education","A musical performance is seen as the performer’s inter-
pretation of a musical score, illuminating the interaction
between the musical structure and implied emotive charac-
ter [1]. It has been demonstrated that performers’ physical
gestures correlate with structural and emotional aspects of
the piece they are performing and that this information can
be decoded by an audience when presented with a visual-
only performance [2].
This paper investigates the relationship between direc-
tion of physical movement and underlying musical struc-
tures. The Vicon motion capture system is used to record
3D movements made by nine university-level pianists per-
forming Chopin preludes op.28 Nos 6 and 7. The examina-
tion of several pianists provides insight into the similarity
and diﬀerences in gestures between performers and how
these relate to structure.
Principal Component Analysis (PCA) of these perfor-
mances and consequent analysis of variance reveals a rela-
tionship between extrema of the ﬁrst six signiﬁcant compo-
nents and timing of phrasing structure in Prelude 7 where
motion troughs consistently lag behind the occurence of
phrase boundaries in the audio. This relationship is then
examined for Prelude 6 which encompasses longer, ex-
panded phrases and changes in rhythm. These expanded
phrases are associated with elongated or split gestures, and
variations of the motif with changes in movement."
37,Martín Haro;Perfecto Herrera,From Low-Level to Song-Level Percussion Descriptors of Polyphonic Music.,2009,https://doi.org/10.5281/zenodo.1417201,"Martín Haro, Universitat Pompeu Fabra, Barcelona, Spain, education;Perfecto Herrera, Universitat Pompeu Fabra, Barcelona, Spain, education","""We address here the automatic description of percussive events in real-world polyphonic music. By taking a pattern recognition approach we evaluate more than 2,450 object- level features. Three binary instrument-wise support vec- tor machines (SVM) are built from a training set of more that 100 songs and 10 genres. Then, we use these bi- nary models to build a drum transcription system achieving comparable results with state of the art algorithms. Finally, we present 17 song-level percussion descriptors computed from the imperfect output of the transcription algorithm. We evaluate the usefulness of the proposed descriptors in music information retrieval (MIR) tasks like genre clas- siﬁcation, danceability estimation and Western vs. non- Western music discrimination. We conclude that the pre- sented song-level percussion descriptors provide comple- mentary information to “classic” descriptors, that can help in the previously mentioned MIR tasks."""
38,Yannis Panagakis;Constantine Kotropoulos;Gonzalo R. Arce,Music Genre Classification Using Locality Preserving Non-Negative Tensor Factorization and Sparse Representations.,2009,https://doi.org/10.5281/zenodo.1416414,"Yannis Panagakis, Aristotle University of Thessaloniki, GRC, education;Constantine Kotropoulos, Aristotle University of Thessaloniki, GRC, education;Gonzalo R. Arce, University of Delaware, USA, education","A robust music genre classification framework is proposed that combines the rich, psycho-physiologically grounded properties of auditory cortical representations of music recordings and the power of sparse representation-based classifiers. A novel multilinear subspace analysis method that incorporates the underlying geometrical structure of the cortical representations space into non-negative tensor factorization is proposed for dimensionality reduction compatible to the working principle of sparse representation-based classification. The proposed method is referred to as Locality Preserving Non-Negative Tensor Factorization (LPNTF). Dimensionality reduction is shown to play a crucial role within the classification framework under study. Music genre classification accuracy of 92.4% and 94.38% on the GTZAN and the ISMIR2004 Genre datasets is reported, respectively. Both accuracies outperform any accuracy ever reported for state of the art music genre classification algorithms applied to the aforementioned datasets."
39,Justin Salamon;Martin Rohrmeier,A Quantitative Evaluation of a Two Stage Retrieval Approach for a Melodic Query by Example System.,2009,https://doi.org/10.5281/zenodo.1416324,"Justin Salamon, Universitat Pompeu Fabra, Barcelona, Spain, education;Martin Rohrmeier, University of Cambridge, United Kingdom, education","We present a two-stage approach for retrieval in a melodic Query by Example system inspired by the BLAST algorithm used in bioinformatics for DNA matching. The first stage involves an indexing method using n-grams and reduces the number of targets to consider in the second stage. In the second stage we use a matching algorithm based on local alignment with modified cost functions which take into account musical considerations. We evaluate our system using queries made by real users utilising both short-term and long-term memory, and present a detailed study of the system’s parameters and how they affect retrieval performance and efficiency. We show that whilst similar approaches were shown to be unsuccessful for Query by Humming (where singing and transcription errors result in queries with higher error rates), in the case of our system the approach is successful in reducing the database size without decreasing retrieval performance."
40,Sten Govaerts;Erik Duval,A Web-based Approach to Determine the Origin of an Artist..,2009,https://doi.org/10.5281/zenodo.1415974,"Sten Govaerts, K.U. Leuven, BEL, education;Erik Duval, K.U. Leuven, BEL, education","One can define the origin of an artist as the geographical location where he started his career. The origin is an important metadata element, because it can help to specify subgenres, be an indicator of regional popularity and improve recommendations. In this paper, we present six methods to determine the origin, based on Web data sources: one extracts data from Last.fm, two query Freebase and three analyze biographies. We evaluate the different methods with 11275 artists. Circa 55% of the artists can be classified using biographies. The best Freebase method can classify 26% and the Last.fm based method 7%. When comparing on accuracy, the Last.fm and Freebase methods perform similarly with around 90% accuracy. For the biography-based methods we achieve 71%. To improve coverage, a final, hybrid method achieves 77% accuracy and 60% coverage. The accuracy of the continent classification is 87%. As a showcase for our classifier, we developed a mashup application that displays, among others, information about the origin of artists from radio station playlists on a map."
41,Wijnand Schepens,Chronicle: Representation of Complex Time Structures.,2009,https://doi.org/10.5281/zenodo.1417353,"Wijnand Schepens, University College Ghent, Belgium, education","Chronicle is a novel open source system for representing structured data involving time, such as music. It offers an XML-based ﬁle format, object models for internal representation in various programming languages, and software libraries and tools for reading and writing XML and for data transformations. Chronicle deﬁnes basic blocks for representing time-based information using events, a hierarchy of groups and instantiable templates. It supports two modes of timing: local timing within a group and association with other el-ements. The built-in mechanism for resolving time references can be used to implement both timescale mappings and tagging of information. Chronicle aims to be a powerful and ﬂexible foundation on which new ﬁle formats and software can be built. Chronicle focuses on structure and timing, but leaves the actual content free to choose. Thus format- or software-developers can specify their own domain-model. This makes it possible to make representations for different types of musical information (scores, performance data, ...) in different styles or cultures (CMN, non-western, contempo-rary, ...), but also for other domains like choreography, scheduling, task management, and so on. It is also ideal for structured tagging of audio and multimedia (movie subtitles, karaoke, synchronisation, ...) and for representing ”internal” data used in music algorithms. The system is organized in four levels of increasing com-plexity. Software developed for a speciﬁc level and domain will also accept lower level data, while users can choose to represent data in a higher level and use Chronicle tools to reduce the level."
42,Erik M. Schmidt;Kris West;Youngmoo E. Kim,Efficient Acoustic Feature Extraction for Music Information Retrieval Using Programmable Gate Arrays.,2009,https://doi.org/10.5281/zenodo.1416698,"Erik M. Schmidt, Drexel University, USA, education;Kris West, University of Illinois, USA, education;Youngmoo E. Kim, Drexel University, USA, education","Many of the recent advances in music information retrieval from audio signals have been data-driven, i.e., resulting from the analysis of very large data sets. Widespread performance evaluations on common data sets, such as the annual MIREX events, have also been instrumental in advancing the field. These endeavors incur a large computational cost, and could potentially benefit greatly from more rapid calculation of acoustic features. Traditional, cluster-based solutions for large-scale feature extraction are expensive and space- and power-inefficient. Using the massively parallel architecture of the field programmable gate array (FPGA), it is possible to design an application specific chip rivaling the speed of a cluster for large-scale acoustic feature computation at lower cost. Recent advances in development tools, such as the Xilinx Blockset in Simulink, allow rapid prototyping, simulation, and implementation on actual hardware. Such devices also show potential for the implementation of MIR systems on embedded devices such as cell phones and PDAs where hardware acceleration would be an absolute necessity. We present a prototype library for acoustic feature calculation for implementation on Xilinx FPGA hardware. Furthermore, using a genre classification task we compare the performance of simulated hardware features to those computed using standard methods, demonstrating a nearly negligible drop in classification performance with the potential for large reductions in computation time."
43,Pascal Ferraro;Pierre Hanna;Laurent Imbert;Thomas Izard,Accelerating Query-by-Humming on GPU.,2009,https://doi.org/10.5281/zenodo.1415798,"Pascal Ferraro, LaBRI - U. Bordeaux 1, France, education, PIMS/CNRS - U. Calgary, Canada, facility;Pierre Hanna, LaBRI - U. Bordeaux 1, France, education;Laurent Imbert, Lirmm - CNRS, France, facility, PIMS/CNRS - U. Calgary, Canada, facility;Thomas Izard, Lirmm - U. Montpellier 2, France, education","Searching for similarities in large musical databases has become a common procedure. Local alignment methods, based on dynamic programming, explore all the possible matchings between two musical pieces; and as a result return the optimal local alignment. Unfortunately these very powerful methods have a very high computational cost. The exponential growth of musical databases makes exact alignment algorithm unrealistic for searching similarities. Alternatives have been proposed in bioinformatics either by using heuristics or by developing faster implementation of exact algorithm. The main motivation of this work is to exploit the huge computational power of commonly available graphic cards to develop high performance solutions for Query-by-Humming applications. In this paper, we present a fast implementation of a local alignment method, which allows to retrieve a hummed query in a database of MIDI files, with good accuracy, in a time up to 160 times faster than other comparable systems."
44,Zafar Rafii;Bryan Pardo,Learning to Control a Reverberator Using Subjective Perceptual Descriptors.,2009,https://doi.org/10.5281/zenodo.1418035,"Zafar Rafii, Northwestern University, USA, education;Bryan Pardo, Northwestern University, USA, education","""The complexity of existing tools for mastering audio can be daunting. Moreover, many people think about sound in individualistic terms (such as “boomy”) that may not have clear mappings onto the controls of existing audio tools. We propose learning to map subjective audio descriptors, such as “boomy”, onto measures of signal properties in order to build a simple controller that manipulates an audio reverberator in terms of a chosen descriptor. For example, “make the sound less boomy”. In the learning process, a user is presented with a series of sounds altered in different ways by a reverberator and asked to rate how well each sound represents the audio concept. The system correlates these ratings with reverberator parameters to build a controller that manipulates reverberation in the user’s terms. In this paper, we focus on developing the mapping between reverberator controls, measures of qualities of reverberation and user ratings. Results on 22 subjects show the system learns quickly (under 3 minutes of training per concept), predicts users responses well (mean correlation coefficient of system predictiveness 0.75) and meets users’ expectations (average human rating of 7.4 out of 10)."""
45,Masatoshi Hamanaka;Satoshi Tojo,Interactive Gttm Analyzer.,2009,https://doi.org/10.5281/zenodo.1415094,"Masatoshi Hamanaka, University of Tsukuba, JPN, education;Satoshi Tojo, Japan Advanced Institute of Science and Technology, JPN, education","We describe an interactive analyzer for the generative theory of tonal music (GTTM). Generally, a piece of music has more than one interpretation, and dealing with such ambiguity is one of the major problems when constructing a music analysis system. To solve this problem, we propose an interactive GTTM analyzer, called an automatic time-span tree analyzer (ATTA), with a GTTM manual editor. The ATTA has adjustable parameters that enable the analyzer to generate multiple analysis results. As the ATTA cannot output all the analysis results that correspond to all the interpretations of a piece of music, we designed a GTTM manual editor, which generates all the analysis results. Experimental results showed that our interactive GTTM analyzer outperformed the GTTM manual editor without an ATTA. Since we hope to contribute to the research of music analysis, we publicize our interactive GTTM analyzer and a dataset of three hundred pairs of a score and analysis results by musicologist on our website http://music.iit.tsukuba.ac.jp/hamanaka/gttm.htm, which is the largest database of analyzed results from the GTTM to date."
46,Roger B. Dannenberg;Larry A. Wasserman,Estimating the Error Distribution of a Single Tap Sequence without Ground Truth.,2009,https://doi.org/10.5281/zenodo.1417265,"Roger B. Dannenberg, Carnegie Mellon University, USA, education;Larry Wasserman, Carnegie Mellon University, USA, education","Detecting beats, estimating tempo, aligning scores to au- dio, and detecting onsets are all interesting problems in the ﬁeld of music information retrieval. In much of this research, it is convenient to think of beats as occuring at precise time points. However, anyone who has attempted to label beats by hand soon realizes that precise annotation of music audio is not possible. A common method of beat annotation is simply to tap along with audio and record the tap times. This raises the question: How accurate are the taps? It may seem that an answer to this question would re- quire knowledge of “true” beat times. However, tap times can be characterized as a random distribution around true beat times. Multiple independent taps can be used to esti- mate not only the location of the true beat time, but also the statistical distribution of measured tap times around the true beat time. Thus, without knowledge of true beat times, and without even requiring the existence of precise beat times, we can estimate the uncertainty of tap times. This characterization of tapping can be useful for estimat- ing tempo variation and evaluating alternative annotation methods."
47,Cory McKay;John Ashley Burgoyne;Jessica Thompson 0001;Ichiro Fujinaga,"Using ACE XML 2.0 to Store and Share Feature, Instance and Class Data for Musical Classification.",2009,https://doi.org/10.5281/zenodo.1418173,"Cory McKay, CIRMMT, McGill University, CAN, education;John Ashley Burgoyne, CIRMMT, McGill University, CAN, education;Jessica Thompson, Music Technology, McGill University, CAN, education;Ichiro Fujinaga, CIRMMT, McGill University, CAN, education","This paper introduces ACE XML 2.0, a set of file formats that are designed to meet the special representational needs of research in automatic music classification. Such standardized formats are needed to facilitate the sharing and long-term storage of valuable research data. ACE XML 2.0 is designed to represent a broad range of musical information clearly using a flexible, extensible, self-contained and formally structured framework. An emphasis is placed on representing extracted feature values, feature descriptions, instance annotations, class ontologies and related metadata."
48,Pablo Cancela;Martín Rocamora;Ernesto López,An Efficient Multi-Resolution Spectral Transform for Music Analysis.,2009,https://doi.org/10.5281/zenodo.1416788,"Pablo Cancela, Universidad de la República, Instituto de Ingeniería Eléctrica, Montevideo, Uruguay, education;Martín Rocamora, Universidad de la República, Instituto de Ingeniería Eléctrica, Montevideo, Uruguay, education;Ernesto López, Universidad de la República, Instituto de Ingeniería Eléctrica, Montevideo, Uruguay, education","In this paper we focus on multi-resolution spectral analysis algorithms for music signals based on the FFT. Two previously devised efficient algorithms (efficient constant-Q transform [1] and multiresolution FFT [2]) are reviewed and compared with a new proposal based on the IIR filtering of the FFT. Apart from its simplicity, the proposed method shows to be a good compromise between design flexibility and reduced computational effort. Additionally, it was used as a part of an effective melody extraction algorithm."
49,Mert Bay;Andreas F. Ehmann;J. Stephen Downie,Evaluation of Multiple-F0 Estimation and Tracking Systems.,2009,https://doi.org/10.5281/zenodo.1418241,"Mert Bay, University of Illinois at Urbana-Champaign, USA, education;Andreas F. Ehmann, University of Illinois at Urbana-Champaign, USA, education;J. Stephen Downie, University of Illinois at Urbana-Champaign, USA, education","""Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music in- formation retrieval systems. This paper presents the sys- tematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music. The eval- uations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008. The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed."""
50,Ferdinand Fuhrmann;Martín Haro;Perfecto Herrera,"Scalability, Generality and Temporal Aspects in Automatic Recognition of Predominant Musical Instruments in Polyphonic Music.",2009,https://doi.org/10.5281/zenodo.1416394,"Ferdinand Fuhrmann, Universitat Pompeu Fabra, ESP, education;Martín Haro, Universitat Pompeu Fabra, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, ESP, education","In this paper we present an approach towards the classiﬁcation of pitched and unpitched instruments in polyphonic audio. In particular, the presented study accounts for three aspects currently lacking in literature: model scalability to polyphonic data, model generalisation in respect to the number of instruments, and incorporation of perceptual information. Therefore, our goal is a unifying recognition framework which enables the extraction of the main instruments’ information. The applied methodology consists of training classiﬁers with audio descriptors, using extensive datasets to model the instruments sufﬁciently. All data consist of real world music, including categories of 11 pitched and 3 percussive instruments. We designed our descriptors by temporal integration of the raw feature values, which are directly extracted from the polyphonic data. Moreover, to evaluate the applicability of modelling temporal aspects in polyphonic audio, we studied the performance of different encodings of the temporal information. Along with accuracies of 63% and 78% for the pitched and percussive classiﬁcation task, results show both the importance of temporal encoding as well as strong limitations of modelling it accurately."
51,Toni Heittola;Anssi Klapuri;Tuomas Virtanen,Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation.,2009,https://doi.org/10.5281/zenodo.1417377,"Toni Heittola, Tampere University of Technology, FIN, education;Anssi Klapuri, Tampere University of Technology, FIN, education;Tuomas Virtanen, Tampere University of Technology, FIN, education","""This paper proposes a novel approach to musical instrument recognition in polyphonic audio signals by using a source-ﬁlter model and an augmented non-negative matrix factorization algorithm for sound separation. The mixture signal is decomposed into a sum of spectral bases modeled as a product of excitations and ﬁlters. The excitations are restricted to harmonic spectra and their fundamental fre- quencies are estimated in advance using a multipitch estimator, whereas the ﬁlters are restricted to have smooth fre- quency responses by modeling them as a sum of elemen- tary functions on the Mel-frequency scale. The pitch and timbre information are used in organizing individual notes into sound sources. In the recognition, Mel-frequency cep- stral coefﬁcients are used to represent the coarse shape of the power spectrum of sound sources and Gaussian mix- ture models are used to model instrument-conditional den- sities of the extracted features. The method is evaluated with polyphonic signals, randomly generated from 19 in- strument classes. The recognition rate for signals having six note polyphony reaches 59%."""
52,Zhiyao Duan;Jinyu Han;Bryan Pardo,Harmonically Informed Multi-Pitch Tracking.,2009,https://doi.org/10.5281/zenodo.1418045,"Zhiyao Duan, Northwestern University, USA, education;Jinyu Han, Northwestern University, USA, education;Bryan Pardo, Northwestern University, USA, education","This paper presents a novel system for multi-pitch tracking, i.e. estimate the pitch trajectory of each monophonic source in a mixture of harmonic sounds. The system consists of two stages: multi-pitch estimation and pitch trajectory formation. In the first stage, we propose a new approach based on modeling spectral peaks and non-peak regions to estimate pitches and polyphony in each single frame. In the second stage, we view the pitch trajectory formation problem as a constrained clustering problem of pitch estimates in all the frames. Constraints are imposed on some pairs of pitch estimates, according to time and frequency proximity. In clustering, harmonic structure is employed as the feature. The proposed system is tested on 10 recorded four-part J. S. Bach chorales. Both multi-pitch estimation and tracking results are very promising. In addition, for multi-pitch estimation, the proposed system is shown to outperform a state-of-the-art multi-pitch estimation approach."
53,Kazuyoshi Yoshii;Masataka Goto,Continuous pLSI and Smoothing Techniques for Hybrid Music Recommendation.,2009,https://doi.org/10.5281/zenodo.1415204,"Kazuyoshi Yoshii, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility","This paper presents an extended probabilistic latent semantic indexing (pLSI) for hybrid music recommendation that deals with rating data provided by users and with content-based data extracted from audio signals. The original pLSI can be applied to collaborative filtering by treating users and items as discrete random variables that follow multinomial distributions. In hybrid recommendation, it is necessary to deal with musical contents that are usually represented as continuous vectorial values. To do this, we propose a continuous pLSI that incorporates Gaussian mixture models. This extension, however, causes a severe local optima problem because it increases the number of parameters drastically. This is considered to be a major factor generating “hubs,” which are items that are inappropriately recommended to almost all users. To solve this problem, we tested three smoothing techniques: multinomial smoothing, Gaussian parameter tying, and artist-based item clustering. The experimental results revealed that although the first method improved nothing, the others significantly improved the recommendation accuracy and reduced the hubness. This indicates that it is important to appropriately limit the model complexity to use the pLSI in practical."
54,François Maillet;Douglas Eck;Guillaume Desjardins;Paul Lamere,Steerable Playlist Generation by Learning Song Similarity from Radio Station Playlists.,2009,https://doi.org/10.5281/zenodo.1416280,"François Maillet, Université de Montréal, CIRMMT, CAN, education;Douglas Eck, Université de Montréal, CIRMMT, CAN, education;Guillaume Desjardins, Université de Montréal, CIRMMT, CAN, education;Paul Lamere, The Echo Nest, USA, company","This paper presents an approach to generating steerable playlists. We ﬁrst demonstrate a method for learning song transition probabilities from audio features extracted from songs played in professional radio station playlists. We then show that by using this learnt similarity function as a prior, we are able to generate steerable playlists by choosing the next song to play not simply based on that prior, but on a tag cloud that the user is able to manipulate to express the high-level characteristics of the music he wishes to listen to."
55,Klaas Bosteels;Elias Pampalk;Etienne E. Kerre,Evaluating and Analysing Dynamic Playlist Generation Heuristics Using Radio Logs and Fuzzy Set Theory.,2009,https://doi.org/10.5281/zenodo.1417675,"Klaas Bosteels, Ghent University, BEL, education;Elias Pampalk, Last.fm Ltd., GBR, company;Etienne E. Kerre, Ghent University, BEL, education","""In this paper, we analyse and evaluate several heuristics for adding songs to a dynamically generated playlist. We explain how radio logs can be used for evaluating such heuristics, and show that formalizing the heuristics using fuzzy set theory simpliﬁes the analysis. More concretely, we verify previous results by means of a large scale eval- uation based on 1.26 million listening patterns extracted from radio logs, and explain why some heuristics perform better than others by analysing their formal deﬁnitions and conducting additional evaluations."""
56,Luke Barrington;Reid Oda;Gert R. G. Lanckriet,Smarter than Genius? Human Evaluation of Music Recommender Systems.,2009,https://doi.org/10.5281/zenodo.1417803,"Luke Barrington, University of California, San Diego, USA, education;Reid Oda, University of California, San Diego, USA, education;Gert Lanckriet, University of California, San Diego, USA, education","Genius is a popular commercial music recommender system that is based on collaborative ﬁltering of huge amounts of user data. To understand the aspects of music similarity that collaborative ﬁltering can capture, we compare Genius to two canonical music recommender systems: one based purely on artist similarity, the other purely on similarity of acoustic content. We evaluate this comparison with a user study of 185 subjects. Overall, Genius produces the best recommendations. We demonstrate that collaborative ﬁltering can actually capture similarities between the acoustic content of songs. However, when evaluators can see the names of the recommended songs and artists, we ﬁnd that artist similarity can account for the performance of Genius. A system that combines these musical cues could generate music recommendations that are as good as Genius, even when collaborative ﬁltering data is unavailable."
57,Fei Wang 0001;Xin Wang 0013;Bo Shao;Tao Li 0001;Mitsunori Ogihara,Tag Integrated Multi-Label Music Style Classification with Hypergraph.,2009,https://doi.org/10.5281/zenodo.1416962,"Fei Wang, Florida International University, USA, education;Xin Wang, Florida International University, USA, education;Bo Shao, Florida International University, USA, education;Tao Li, Florida International University, USA, education;Mitsunori Ogihara, University of Miami, USA, education","Automatic music style classification is an important, but challenging problem in music information retrieval. It has a number of applications, such as indexing of and searching in musical databases. Traditional music style classification approaches usually assume that each piece of music has a unique style and they make use of the music contents to construct a classifier for classifying each piece into its unique style. However, in reality, a piece may match more than one, even several different styles. Also, in this modern Web 2.0 era, it is easy to get a hold of additional, indirect information (e.g., music tags) about music. This paper proposes a multi-label music style classification approach, called Hypergraph integrated Support Vector Machine (HiSVM), which can integrate both music contents and music tags for automatic music style classification. Experimental results based on a real world data set are presented to demonstrate the effectiveness of the method."
58,Matthew D. Hoffman;David M. Blei;Perry R. Cook,Easy As CBA: A Simple Probabilistic Model for Tagging Music.,2009,https://doi.org/10.5281/zenodo.1417347,"Matthew D. Hoffman, Princeton University, USA, education;David M. Blei, Princeton University, USA, education;Perry R. Cook, Princeton University, USA, education","""Many songs in large music databases are not labeled with semantic tags that could help users sort out the songs they want to listen to from those they do not. If the words that apply to a song can be predicted from audio, then those predictions can be used both to automatically annotate a song with tags, allowing users to get a sense of what qualities characterize a song at a glance. Automatic tag prediction can also drive retrieval by allowing users to search for the songs most strongly characterized by a particular word. We present a probabilistic model that learns to predict the probability that a word applies to a song from audio. Our model is simple to implement, fast to train, predicts tags for new songs quickly, and achieves state-of-the-art performance on annotation and retrieval tasks."""
59,Joon Hee Kim;Brian Tomasik;Douglas Turnbull,Using Artist Similarity to Propagate Semantic Information.,2009,https://doi.org/10.5281/zenodo.1416510,"Joon Hee Kim, Swarthmore College, USA, education;Brian Tomasik, Swarthmore College, USA, education;Douglas Turnbull, Swarthmore College, USA, education","Tags are useful text-based labels that encode semantic information about music (instrumentation, genres, emotions, geographic origins). While there are a number of ways to collect and generate tags, there is generally a data sparsity problem in which very few songs and artists have been accurately annotated with a sufficiently large set of relevant tags. We explore the idea of tag propagation to help alleviate the data sparsity problem. Tag propagation, originally proposed by Sordo et al., involves annotating a novel artist with tags that have been frequently associated with other similar artists. In this paper, we explore four approaches for computing artists similarity based on different sources of music information (user preference data, social tags, web documents, and audio content). We compare these approaches in terms of their ability to accurately propagate three different types of tags (genres, acoustic descriptors, social tags). We find that the approach based on collaborative filtering performs best. This is somewhat surprising considering that it is the only approach that is not explicitly based on notions of semantic similarity. We also find that tag propagation based on content-based music analysis results in relatively poor performance."
60,Cyril Laurier;Mohamed Sordo;Joan Serrà;Perfecto Herrera,Music Mood Representations from Social Tags.,2009,https://doi.org/10.5281/zenodo.1415600,"Cyril Laurier, Universitat Pompeu Fabra, ESP, education;Mohamed Sordo, Universitat Pompeu Fabra, ESP, education;Joan Serr`a, Universitat Pompeu Fabra, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, ESP, education","""This paper presents ﬁndings about mood representations. We aim to analyze how do people tag music by mood, to create representations based on this data and to study the agreement between experts and a large community. For this purpose, we create a semantic mood space from last.fm tags using Latent Semantic Analysis. With an unsuper- vised clustering approach, we derive from this space an ideal categorical representation. We compare our commu- nity based semantic space with expert representations from Hevner and the clusters from the MIREX Audio Mood Classiﬁcation task. Using dimensional reduction with a Self-Organizing Map, we obtain a 2D representation that we compare with the dimensional model from Russell. We present as well a tree diagram of the mood tags obtained with a hierarchical clustering approach. All these results show a consistency between the community and the ex- perts as well as some limitations of current expert models. This study demonstrates a particular relevancy of the basic emotions model with four mood clusters that can be sum- marized as: happy, sad, angry and tender. This outcome can help to create better ground truth and to provide more realistic mood classiﬁcation algorithms. Furthermore, this method can be applied to other types of representations to build better computational models."""
61,Edith Law;Kris West;Michael I. Mandel;Mert Bay;J. Stephen Downie,Evaluation of Algorithms Using Games: The Case of Music Tagging.,2009,https://doi.org/10.5281/zenodo.1417647,"Edith Law, CMU, USA, education;Kris West, IMIRSEL/UIUC, USA, education;Michael Mandel, Columbia University, USA, education;Mert Bay, IMIRSEL/UIUC, USA, education;J. Stephen Downie, IMIRSEL/UIUC, USA, education","Search by keyword is an extremely popular method for retrieving music. To support this, novel algorithms that automatically tag music are being developed. The conventional way to evaluate audio tagging algorithms is to compute measures of agreement between the output and the ground truth set. In this work, we introduce a new method for evaluating audio tagging algorithms on a large scale by collecting set-level judgments from players of a human computation game called TagATune. We present the design and preliminary results of an experiment comparing five algorithms using this new evaluation metric, and contrast the results with those obtained by applying several conventional agreement-based evaluation metrics."
62,Tatsuya Kako;Yasunori Ohishi;Hirokazu Kameoka;Kunio Kashino;Kazuya Takeda,Automatic Identification for Singing Style based on Sung Melodic Contour Characterized in Phase Plane.,2009,https://doi.org/10.5281/zenodo.1417415,"Tatsuya Kako, Graduate School of Information Science, Nagoya University, JPN, education;Yasunori Ohishi, NTT Communication Science Laboratories, NTT Corporation, JPN, company;Hirokazu Kameoka, NTT Communication Science Laboratories, NTT Corporation, JPN, company;Kunio Kashino, NTT Communication Science Laboratories, NTT Corporation, JPN, company;Kazuya Takeda, Graduate School of Information Science, Nagoya University, JPN, education","A stochastic representation of singing styles is proposed. The dynamic property of melodic contour, i.e., fundamental frequency (F0) sequence, is assumed to be the main cue for singing styles because it can characterize such typical ornamentations as vibrato . F0 signal trajectories in the phase plane are used as the basic representation. By fitting Gaussian mixture models to the observed F0 trajectories in the phase plane, a parametric representation is obtained by a set of GMM parameters. The effectiveness of our proposed method is confirmed through experimental evaluation where 94.1% accuracy for singer-class discrimination was obtained."
63,Philippe Hamel;Sean Wood;Douglas Eck,Automatic Identification of Instrument Classes in Polyphonic and Poly-Instrument Audio.,2009,https://doi.org/10.5281/zenodo.1415092,"Philippe Hamel, Université de Montréal, CAN, education;Sean Wood, Université de Montréal, CAN, education;Douglas Eck, Université de Montréal, CAN, education","We present and compare several models for automatic identification of instrument classes in polyphonic and poly-instrument audio. The goal is to be able to identify which categories of instrument (Strings, Woodwind, Guitar, Piano, etc.) are present in a given audio example. We use a machine learning approach to solve this task. We constructed a system to generate a large database of musically relevant poly-instrument audio. Our database is generated from hundreds of instruments classified in 7 categories. Musical audio examples are generated by mixing multi-track MIDI files with thousands of instrument combinations. We compare three different classifiers : a Support Vector Machine (SVM), a Multilayer Perceptron (MLP) and a Deep Belief Network (DBN). We show that the DBN tends to outperform both the SVM and the MLP in most cases."
64,Brian Tomasik;Joon Hee Kim;Margaret Ladlow;Malcolm Augat;Derek Tingle;Rich Wicentowski;Douglas Turnbull,Using Regression to Combine Data Sources for Semantic Music Discovery.,2009,https://doi.org/10.5281/zenodo.1415558,"Brian Tomasik, Swarthmore College, USA, education;Joon Hee Kim, Swarthmore College, USA, education;Margaret Ladlow, Swarthmore College, USA, education;Malcolm Augat, Swarthmore College, USA, education;Derek Tingle, Swarthmore College, USA, education;Richard Wicentowski, Swarthmore College, USA, education;Douglas Turnbull, Swarthmore College, USA, education","In the process of automatically annotating songs with descriptive labels, multiple types of input information can be used. These include keyword appearances in web documents, acoustic features of the song’s audio content, and similarity with other tagged songs. Given these individual data sources, we explore the question of how to aggregate them. We find that fixed-combination approaches like sum and max perform well but that trained linear regression models work better. Retrieval performance improves with more data sources. On the other hand, for large numbers of training songs, Bayesian hierarchical models that aim to share information across individual tag regressions offer no advantage."
65,Xiao Hu 0001;J. Stephen Downie;Andreas F. Ehmann,Lyric Text Mining in Music Mood Classification.,2009,https://doi.org/10.5281/zenodo.1416790,"Xiao Hu, University of Illinois at Urbana-Champaign, USA, education;J. Stephen Downie, University of Illinois at Urbana-Champaign, USA, education;Andreas F. Ehmann, University of Illinois at Urbana-Champaign, USA, education","""This research examines the role lyric text can play in improving audio music mood classification. A new method is proposed to build a large ground truth set of 5,585 songs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relatively complete set of lyric features and representation models were investigated. The best performing lyric feature set was also compared to a leading audio-based system. In combining lyric and audio sources, hybrid feature sets built with three different feature selection methods were also examined. The results show patterns at odds with findings in previous studies: audio features do not always outperform lyrics features, and combining lyrics and audio features can improve performance in many mood categories, but not all of them."""
66,Xin Xu;Masaki Naito;Tsuneo Kato;Hisashi Kawai,Robust and Fast Lyric Search based on Phonetic Confusion Matrix.,2009,https://doi.org/10.5281/zenodo.1418227,"Xin Xu, KDDI R&D Laboratories, Inc., JPN, company;Masaki Naito, KDDI R&D Laboratories, Inc., JPN, company;Tsuneo Kato, KDDI R&D Laboratories, Inc., JPN, company;Hisashi Kawai, National Institute of Information and Communications Technology, JPN, facility","This paper proposes a robust and fast lyric search method for music information retrieval. Current lyric search systems by normal text retrieval techniques are severely deteriorated in the case that the queries of lyric phrases contain incorrect parts due to mishearing and misremembering. To solve this problem, the authors apply acoustic distance, which is computed based on a confusion matrix of an ASR experiment, into DP-based phonetic string matching. The experimental results show that the search accuracy is increased by more than 40% compared with the normal text retrieval method; and by 2% ∼4% compared with the conventional phonetic string matching method. Considering the high computation complexity of DP matching, the authors propose a novel two-pass search strategy to shorten the processing time. By pre-selecting the probable candidates by a rapid index-based search for the first pass and executing a DP-based search among these candidates during the second pass, the proposed method reduces processing time by 85.8% and keeps search accuracy at the same level as that of a complete search by DP matching with all lyrics."
67,Phillip B. Kirlin,Using Harmonic and Melodic Analyses to Automate the Initial Stages of Schenkerian Analysis.,2009,https://doi.org/10.5281/zenodo.1416654,"Phillip B. Kirlin, University of Massachusetts Amherst, USA, education","Structural music analysis is used to reveal the inner workings of a musical composition by recursively applying reductions to the music, resulting in a series of successively more abstract views of the composition. Schenkerian analysis is the most well-developed type of structural analysis, and while there is a wide body of research on the theory, there is no well-deﬁned algorithm to perform such an analysis. A automated algorithm for Schenkerian analysis would be extremely useful to music scholars and researchers studying music from a computational standpoint. The ﬁrst major step in producing a Schenkerian analysis involves selecting notes from the composition in question for the primary soprano and bass parts of the analysis. We present an algorithm for this that uses harmonic and melodic analyses to accomplish this task."
68,James B. Maxwell;Philippe Pasquier;Arne Eigenfeldt,Hierarchical Sequential Memory for Music: A Cognitive Model.,2009,https://doi.org/10.5281/zenodo.1414850,"James B. Maxwell, Simon Fraser University, CAN, education;Philippe Pasquier, Simon Fraser University, CAN, education;Arne Eigenfeldt, Simon Fraser University, CAN, education","""We propose a new machine-learning framework called the Hierarchical Sequential Memory for Music, or HSMM. The HSMM is an adaptation of the Hierarchical Temporal Memory (HTM) framework, designed to make it better suited to musical applications. The HSMM is an online learner, capable of recognition, generation, continuation, and completion of musical structures."""
69,Jessica Thompson 0001;Cory McKay;John Ashley Burgoyne;Ichiro Fujinaga,Additions and Improvements in the ACE 2.0 Music Classifier.,2009,https://doi.org/10.5281/zenodo.1416048,"Jessica Thompson, McGill University, CAN, education;Cory McKay, McGill University, CAN, education;John Ashley Burgoyne, McGill University, CAN, education;Ichiro Fujinaga, McGill University, CAN, education","""This paper presents additions and improvements to the Autonomous Classification Engine (ACE), a framework for using and optimizing classifiers. Given a set of feature values, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimen-sionality-reduction techniques in order to arrive at a con-figuration that is well-suited to a given problem. Changes and additions have been made to ACE in order to in-crease its functionality as well as to make it easier to use and incorporate into other software frameworks. Details are provided on ACE’s remodeled class structure and associated API, the improved command line and graphi-cal user interfaces, a new ACE XML 2.0 ZIP file format and expanded statistical reporting associated with cross validation. The resulting improved processing and meth-ods of operation are also discussed."""
70,Diane Hu;Lawrence K. Saul,A Probabilistic Topic Model for Unsupervised Learning of Musical Key-Profiles.,2009,https://doi.org/10.5281/zenodo.1415160,"Diane J. Hu, University of California, San Diego, USA, education;Lawrence K. Saul, University of California, San Diego, USA, education","We describe a probabilistic model for learning musical key-proﬁles from symbolic ﬁles of polyphonic, classical mu-sic. Our model is based on Latent Dirichlet Allocation (LDA), a statistical approach for discovering hidden topics in large corpora of text. In our adaptation of LDA, sym-bolic music ﬁles play the role of text documents, groups of musical notes play the role of words, and musical key-proﬁles play the role of topics. The topics are discovered as signiﬁcant, recurring distributions over twelve neutral pitch-classes. Though discovered automatically, these dis-tributions closely resemble the traditional key-proﬁles used to indicate the stability and importance of neutral pitch-classes in the major and minor keys of western music. Un-like earlier approaches based on human judgement, our model learns key-proﬁles in an unsupervised manner, in-ferring them automatically from a large musical corpus that contains no key annotations. We show how these learned key-proﬁles can be used to determine the key of a musical piece and track its harmonic modulations. We also show how the model’s inferences can be used to compare musi-cal pieces based on their harmonic structure."
71,Dan Tidhar;György Fazekas;Sefki Kolozali;Mark B. Sandler,Publishing Music Similarity Features on the Semantic Web.,2009,https://doi.org/10.5281/zenodo.1417071,"Dan Tidhar, Queen Mary, University of London, GBR, education;György Fazekas, Queen Mary, University of London, GBR, education;Sefki Kolozali, Queen Mary, University of London, GBR, education;Mark Sandler, Queen Mary, University of London, GBR, education","""We describe the process of collecting, organising and publishing a large set of music similarity features produced by the SoundBite [10] playlist generator tool. These data can be a valuable asset in the development and evaluation of new Music Information Retrieval algorithms. They can also be used in Web-based music search and retrieval applications. For this reason, we make a database of features available on the Semantic Web via a SPARQL end-point, which can be used in Linked Data services. We provide examples of using the data in a research tool, as well as in a simple web application which responds to audio queries and finds a set of similar tracks in our database."""
72,Jakob Abeßer;Hanna M. Lukashevich;Christian Dittmar;Gerald Schuller,Genre Classification Using Bass-Related High-Level Features and Playing Styles.,2009,https://doi.org/10.5281/zenodo.1417697,"Jakob Abeßer, Fraunhofer IDMT, DEU, facility;Hanna Lukashevich, Fraunhofer IDMT, DEU, facility;Christian Dittmar, Fraunhofer IDMT, DEU, facility;Gerald Schuller, Fraunhofer IDMT, DEU, facility","Considering its mediation role between the poles of rhythm, harmony, and melody, the bass plays a crucial role in most music genres. This paper introduces a novel set of transcription-based high-level features that characterize the bass and its interaction with other participating instruments. Furthermore, a new method to model and automatically retrieve different genre-specific bass playing styles is presented. A genre classification task is used as benchmark to compare common machine learning algorithms based on the presented high-level features with a classification algorithm solely based on detected bass playing styles."
73,Hanna M. Lukashevich;Jakob Abeßer;Christian Dittmar;Holger Großmann,From Multi-Labeling to Multi-Domain-Labeling: A Novel Two-Dimensional Approach to Music Genre Classification.,2009,https://doi.org/10.5281/zenodo.1417535,"Hanna Lukashevich, Fraunhofer Institute for Digital Media Technologies, DEU, facility;Jakob Abeßer, Fraunhofer Institute for Digital Media Technologies, DEU, facility;Christian Dittmar, Fraunhofer Institute for Digital Media Technologies, DEU, facility;Holger Grossmann, Fraunhofer Institute for Digital Media Technologies, DEU, facility","""In this publication we describe a novel two-dimensional approach for automatic music genre classiﬁcation. Although the subject poses a well studied task in Music Information Retrieval, some fundamental issues of genre classiﬁcation have not been covered so far. Especially many modern genres are inﬂuenced by manifold musical styles. Most of all, this holds true for the broad category “World Music”, which comprises many different regional styles and a mutual mix up thereof. A common approach to tackle this issue in manual categorization is to assign mul- tiple genre labels to a single recording. However, for com- monly used automatic classiﬁcation algorithms, multi- labeling poses a problem due to its ambiguities. Thus, we propose to break down multi-label genre annotations into single-label annotations within given time segments and musical domains. A corresponding multi-stage evalu- ation based on a representative set of items from a global music taxonomy is performed and discussed accordingly. Therefore, we conduct 3 different experiments that cover multi-labeling, multi-labeling with time segmentation and the proposed multi-domain labeling."""
74,Dimitri Diakopoulos;Owen Vallis;Jordan Hochenbaum;Jim W. Murphy;Ajay Kapur,21st Century Electronica: MIR Techniques for Classification and Performance.,2009,https://doi.org/10.5281/zenodo.1416278,"Dimitri Diakopoulos, California Institute of the Arts, USA, education;Owen Vallis, California Institute of the Arts, USA, education; New Zealand School of Music, NZL, education;Jordan Hochenbaum, California Institute of the Arts, USA, education; New Zealand School of Music, NZL, education;Jim Murphy, California Institute of the Arts, USA, education;Ajay Kapur, New Zealand School of Music, NZL, education","The performance of electronica by Disc Jockys (DJs) 
presents a unique opportunity to develop interactions be-
tween performer and music. Through recent research in 
the MIR field, new tools for expanding DJ performance 
are emerging. The use of spectral, loudness, and temporal 
descriptors for the classification of electronica is ex-
plored. Our research also introduces the use of a multi-
touch interface to drive a performance-oriented DJ appli-
cation utilizing the feature set. Furthermore, we present 
that a multi-touch surface provides an extensible and col-
laborative interface for browsing and manipulating MIR-
related data in real time."
75,Eric Nichols;Dan Morris;Sumit Basu;Christopher Raphael,Relationships Between Lyrics and Melody in Popular Music.,2009,https://doi.org/10.5281/zenodo.1417321,"Eric Nichols, Indiana University, USA, education;Dan Morris, Microsoft Research, USA, company;Sumit Basu, Microsoft Research, USA, company;Christopher Raphael, Indiana University, USA, education","Composers of popular music weave lyrics, melody, and 
instrumentation together to create a consistent and com-
pelling emotional scene. The relationships among these 
elements are critical to musical communication, and un-
derstanding the statistics behind these relationships can 
contribute to numerous problems in music information 
retrieval and creativity support. In this paper, we present 
the results of an observational study on a large symbolic 
database of popular music; our results identify several 
patterns in the relationship between lyrics and melody.  "
76,Makoto P. Kato,RhythMiXearch: Searching for Unknown Music by Mixing Known Music.,2009,https://doi.org/10.5281/zenodo.1416542,"Makoto P. Kato, Kyoto University, JPN, education","We present a novel method for searching for unknown music. RhythMiXearch is a music search system we developed that can accept two music inputs and mix those inputs to search for music that could reasonably be a result of the mixture. This approach expands the ability of Query-by-Example and allows greater flexibility for users in finding unknown music. Each music piece stored by our system is characterized by text data written by users, i.e., review data. We used Latent Dirichlet Allocation (LDA) to capture semantics from the reviews that were then used to characterize the music by Hevner’s eight impression categories. RhythMiXearch mixes two music inputs in accordance with a probabilistic mixture model and finds music that is the most likely product of the mixture. Our experimental results indicate that the proposed method is comparable to human in searching for music by multiple examples."
77,Benjamin Martin 0001;Matthias Robine;Pierre Hanna,Musical Structure Retrieval by Aligning Self-Similarity Matrices.,2009,https://doi.org/10.5281/zenodo.1416502,"Benjamin Martin, Matthias Robine and Pierre Hanna, LaBRI - University of Bordeaux, FRA, education","""We propose a new retrieval system based on musical structure using symbolic structural queries. The aim is to compare musical form in audio files without extracting explicitly the underlying audio structure. From a given or arbitrary segmentation, an audio file is segmented. Irrespective of the audio feature choice, we then compute a self-similarity matrix whose coefficients correspond to the estimation of the similarity between entire parts, obtained by local alignment. Finally, we compute a binary matrix from the symbolic structural query and compare it to the audio segmented matrix, which provides a structural similarity score. We perform experiments using large databases of audio files, and prove robustness to possible imprecisions in the structural query."""
78,Dirk Moelants;Olmo Cornelis;Marc Leman,Exploring African Tone Scales.,2009,https://doi.org/10.5281/zenodo.1416338,"Dirk Moelants, Ghent University, BEL, education;Olmo Cornelis, University College Ghent, BEL, education;Marc Leman, Ghent University, BEL, education","Key-finding is a central topic in Western music analysis and development of MIR tools. However, most approaches rely on the Western 12-tone scale, which is not universally used. African music does not follow a fixed tone scale. In order to classify and study African tone scales, we developed a system in which the pitch is first analyzed on a continuous scale. Peak analysis is then applied on these data to extract the actual scale used. This system has been applied to a selection of African music, it allows us to look for similarities using cross-correlation. Thus it provides an interesting tool for query-by-example and database management in collections of ethnic music which can not be simply classified according to keys. Next to this the data can be used for ethnomusicological research. The study of the intervals used in this collection, e.g., gives us evidence for Western influence, with recent recordings having a tendency to use more regular intervals."
79,Nicola Montecchio;Nicola Orio,A Discrete Filter Bank Approach to Audio to Score Matching for Polyphonic Music.,2009,https://doi.org/10.5281/zenodo.1418093,"Nicola Montecchio, University of Padova, ITA, education;Nicola Orio, University of Padova, ITA, education","This paper presents a system for tracking the position of a polyphonic music performance in a symbolic score, possibly in real time. The system, based on Hidden Markov Models, is brieﬂy presented, focusing on speciﬁc aspects such as observation modeling based on discrete ﬁlterbanks, in contrast with traditional FFT-based approaches, and describing the approaches to decoding. Experimental results are provided to assess the validity of the presented model. Proof-of-concept applications are shown, which effectively employ the described approach beyond the traditional automatic accompaniment system."
80,Eric Battenberg;David Wessel,Accelerating Non-Negative Matrix Factorization for Audio Source Separation on Multi-Core and Many-Core Architectures.,2009,https://doi.org/10.5281/zenodo.1417020,"Eric Battenberg, University of California, Berkeley, USA, education;David Wessel, University of California, Berkeley, USA, education","Non-negative matrix factorization (NMF) has been successfully used in audio source separation and parts-based analysis; however, iterative NMF algorithms are computationally intensive, and therefore, time to convergence is very slow on typical personal computers. In this paper, we describe high performance parallel implementations of NMF developed using OpenMP for shared-memory multi-core systems and CUDA for many-core graphics processors. For 20 seconds of audio, we decrease running time from 18.5 seconds to 2.6 seconds using OpenMP and 0.6 seconds using CUDA. These performance increases allow source separation to be carried out on entire songs in a number of seconds, a process which was previously impractical with respect to time. We give insight into how such significant speed gains were made and encourage the development and use of parallel music information retrieval software."
81,Peter van Kranenburg;Anja Volk;Frans Wiering;Remco C. Veltkamp,Musical Models for Melody Alignment.,2009,https://doi.org/10.5281/zenodo.1415608,"Peter van Kranenburg, Utrecht University, NLD, education;Anja Volk, Utrecht University, NLD, education;Frans Wiering, Utrecht University, NLD, education;Remco C. Veltkamp, Utrecht University, NLD, education","In this paper we show that the modeling of musical knowledge within alignment algorithms results in a successful similarity approach to melodies. The score of the alignment of two melodies is taken as a measure of similarity. We introduce a number of scoring functions that model the inﬂuence of different musical parameters. The evaluation of their retrieval performance on a well-annotated set of 360 folk-song melodies with various kinds of melodic variation, shows that a combination of pitch, rhythm and segmentation-based scoring functions performs best, with a mean average precision of 0.83."
82,Brian McFee;Gert R. G. Lanckriet,Heterogeneous Embedding for Subjective Artist Similarity.,2009,https://doi.org/10.5281/zenodo.1416284,"Brian McFee, University of California, San Diego, USA, education;Gert Lanckriet, University of California, San Diego, USA, education","""We describe an artist recommendation system which integrates several heterogeneous data sources to form a holistic similarity space. Using social, semantic, and acoustic features, we learn a low-dimensional feature transformation which is optimized to reproduce human-derived measurements of subjective similarity between artists. By producing low-dimensional representations of artists, our system is suitable for visualization and recommendation tasks."""
83,Masahiro Niitsuma;Tsutomu Fujinami;Yo Tomita,The Intersection of Computational Analysis and Music Manuscripts: A New Model for Bach Source Studies of the 21st Century.,2009,https://doi.org/10.5281/zenodo.1417935,"Masahiro Niitsuma, School of Muisc and Sonic Arts, Queen’s University, Belfast, GBR, education;Tsutomu Fujinami, School of Knowledge Science, Japan Advanced Institute of Science and Technology (JAIST), JPN, education;Yo Tomita, School of Muisc and Sonic Arts, Queen’s University, Belfast, GBR, education","This paper addresses the intersection of computational analysis and musicological source studies. In musicology, scholars often find themselves in the situation where their methodologies are inadequate to achieve their goals. Their problems appear to be twofold: (1) the lack of scientific objectivity and (2) the over-reliance on new source discoveries. We propose three stages to resolve these problems, a preliminary result of which is shown. The successful outcome of this work will have a huge impact not only on musicology but also on a wide range of subjects."
84,Tim Pohle;Dominik Schnitzer;Markus Schedl;Peter Knees;Gerhard Widmer,On Rhythm and General Music Similarity.,2009,https://doi.org/10.5281/zenodo.1418229,"Tim Pohle, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility;Dominik Schnitzer, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility;Markus Schedl, Johannes Kepler University, AUT, education;Peter Knees, Johannes Kepler University, AUT, education;Gerhard Widmer, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility","The contribution of this paper is threefold: First, we propose modiﬁcations to Fluctuation Patterns [14]. The resulting descriptors are evaluated in the task of rhythm similarity computation on the “Ballroom Dancers” collection. Second, we show that by combining these rhythmic de- scriptors with a timbral component, results for rhythm sim- ilarity computation are improved beyond the level obtained when using the rhythm descriptor component alone. Third, we present one “uniﬁed” algorithm with ﬁxed parameter set. This algorithm is evaluated on three differ- ent music collections. We conclude from these evaluations that the computed similarities reﬂect relevant aspects both of rhythm similarity and of general music similarity. The performance can be improved by tuning parameters of the “uniﬁed” algorithm to the speciﬁc task (rhythm similarity / general music similarity) and the speciﬁc collection, re- spectively."
85,Juan Pablo Bello,Grouping Recorded Music by Structural Similarity.,2009,https://doi.org/10.5281/zenodo.1414876,"Juan Pablo Bello, Music and Audio Research Lab (MARL), New York University, USA, education","""This paper introduces a method for the organization of recorded music according to structural similarity. It uses the Normalized Compression Distance (NCD) to measure the pairwise similarity between songs, represented using beat-synchronous self-similarity matrices. The approach is evaluated on its ability to cluster a collection into groups of performances of the same musical work. Tests are aimed at finding the combination of system parameters that improve clustering, and at highlighting the benefits and shortcomings of the proposed method. Results show that structural similarities can be well characterized by this approach, given consistency in beat tracking and overall song structure."""
86,Dominik Schnitzer;Arthur Flexer;Gerhard Widmer,A Filter-and-Refine Indexing Method for Fast Similarity Search in Millions of Music Tracks.,2009,https://doi.org/10.5281/zenodo.1417831,"Dominik Schnitzer, Austrian Research Institute for Artificial Intelligence, AUT, facility;Arthur Flexer, Austrian Research Institute for Artificial Intelligence, AUT, facility;Gerhard Widmer, Johannes Kepler University, AUT, education",We present a filter-and-refine method to speed up acoustic audio similarity queries which use the Kullback-Leibler divergence as similarity measure. The proposed method rescales the divergence and uses a modified FastMap [1] implementation to accelerate nearest-neighbor queries. The search for similar music pieces is accelerated by a factor of 10−30 compared to a linear scan but still offers high recall values (relative to a linear scan) of 95 − 99%. We show how the proposed method can be used to query several million songs for their acoustic neighbors very fast while producing almost the same results that a linear scan over the whole database would return. We present a working prototype implementation which is able to process similarity queries on a 2.5 million songs collection in about half a second on a standard CPU.
87,Nicola Orio;Antonio Rodà,A Measure of Melodic Similarity based on a Graph Representation of the Music Structure.,2009,https://doi.org/10.5281/zenodo.1415010,"Nicola Orio, University of Padova, ITA, education;Antonio Rod`a, University of Udine, ITA, education","Content-based music retrieval requires to deﬁne a similarity measure between music documents. In this paper, we propose a novel similarity measure between melodic content, as represented in symbolic notation, that takes into account musicological aspects on the structural function of the melodic elements. The approach is based on the representation of a collection of music scores with a graph structure, where terminal nodes directly describe the music content, internal nodes represent its incremental generalization, and arcs denote the relationships among them. The similarity between two melodies can be computed by analyzing the graph structure and ﬁnding the shortest path between the corresponding nodes inside the graph. Preliminary results in terms of music similarity are presented using a small test collection."
88,W. Bas de Haas;Martin Rohrmeier;Remco C. Veltkamp;Frans Wiering,Modeling Harmonic Similarity Using a Generative Grammar of Tonal Harmony.,2009,https://doi.org/10.5281/zenodo.1416212,"W. Bas de Haas, Utrecht University, NLD, education;Martin Rohrmeier, University of Cambridge, GBR, education;Remco C. Veltkamp, Utrecht University, NLD, education;Frans Wiering, Utrecht University, NLD, education","In this paper we investigate a new approach to the similarity of tonal harmony. We create a fully functional remodeling of an earlier version of Rohrmeier’s grammar of harmony. With this grammar an automatic harmonic analysis of a sequence of symbolic chord labels is obtained in the form of a parse tree. The harmonic similarity is determined by finding and examining the largest labeled common embeddable subtree (LLCES) of two parse trees. For the calculation of the LLCES a new O(min(n, m)nm) time algorithm is presented, where n and m are the sizes of the trees. For the analysis of the LLCES we propose six distance measures that exploit several structural characteristics of the Combined LLCES. We demonstrate in a retrieval experiment that at least one of these new methods significantly outperforms a baseline string matching approach and thereby show that using additional musical knowledge from music cognitive and music theoretic models actually helps improving retrieval performance."
89,Christopher Raphael,Symbolic and Structural Representation of Melodic Expression.,2009,https://doi.org/10.5281/zenodo.1417817,"Christopher Raphael, Indiana Univ., Bloomington, USA, education","A method for expressive melody synthesis is presented seeking to capture the structural and prosodic (stress, direction, and grouping) elements of musical interpretation. The interpretation of melody is represented through a hierarchical structural decomposition and a note-level prosodic annotation. An audio performance of the melody is constructed using the time-evolving frequency and intensity functions. A method is presented that transforms the expressive annotation into the frequency and intensity functions, thus giving the audio performance. In this framework, the problem of expressive rendering is cast as estimation of structural decomposition and the prosodic annotation. Examples are presented on a dataset of around 50 folk-like melodies, realized both from hand-marked and estimated annotations."
90,Maksim Khadkevich;Maurizio Omologo,Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.,2009,https://doi.org/10.5281/zenodo.1415930,"Maksim Khadkevich, FBK-irst, Universitá degli studi di Trento, ITA, education;Maurizio Omologo, Fondazione Bruno Kessler-irst, ITA, facility","""This paper focuses on automatic extraction of acoustic chord sequences from a musical piece. Standard and factored language models are analyzed in terms of applicability to the chord recognition task. Pitch class profile vectors that represent harmonic information are extracted from the given audio signal. The resulting chord sequence is obtained by running a Viterbi decoder on trained hidden Markov models and subsequent lattice rescoring, applying the language model weight. We performed several experiments using the proposed technique. Results obtained on 175 manually-labeled songs provided an increase in accuracy of about 2%."""
91,Sam Ferguson;Densil Cabrera,Auditory Spectral Summarisation for Audio Signals with Musical Applications.,2009,https://doi.org/10.5281/zenodo.1417393,"Sam Ferguson, University of Technology, Sydney, AUS, education;Densil Cabrera, The University of Sydney, AUS, education","Methods for spectral analysis of audio signals and their graphical display are widespread. However, assessing music and audio in the visual domain involves a number of challenges in the translation between auditory images into mental or symbolically represented concepts. This paper presents a spectral analysis method that exists entirely in the auditory domain, and results in an auditory presentation of a spectrum. It aims to strip a segment of audio signal of its temporal content, resulting in a quasi-stationary signal that possesses a similar spectrum to the original signal. The method is extended and applied for the purpose of music summarisation."
92,Cynthia C. S. Liem;Alan Hanjalic,Cover Song Retrieval: A Comparative Study of System Component Choices.,2009,https://doi.org/10.5281/zenodo.1414896,"Cynthia C.S. Liem, Delft University of Technology, NLD, education;Alan Hanjalic, Delft University of Technology, NLD, education","The Cover Song Retrieval (CSR) problem has received
considerable attention in the MIREX 2006-2008 evalu-
ation sessions.
While the reported performance ﬁgures
provide a general idea about the strengths of the submit-
ted systems, it is not clear what actually causes the re-
ported performance of a certain system. In other words,
the question arises whether some system component de-
sign choices are more critical for a system’s performance
results than others. In order to obtain a better understand-
ing of the performance of current CSR approaches and
to give recommendations for future research in the ﬁeld
of CSR, we designed and performed a comparative study
involving system component design approaches from the
best-performing systems in MIREX 2006 and 2007. The
datasets used for evaluation were carefully chosen to cover
the broad spectrum of the cover song domain, while still
providing designated test cases. While the choice of the
dissimilarity assessment method was found to cause the
largest CSR performance boost and very good retrieval re-
sults were obtained on classical opus retrieval cases, results
obtained on a new test case, involving recordings originat-
ing from different microphone sets, point out new chal-
lenges in optimizing the feature representation step."
93,Peter Knees;Tim Pohle;Markus Schedl;Dominik Schnitzer;Klaus Seyerlehner;Gerhard Widmer,Augmenting Text-based Music Retrieval with Audio Similarity: Advantages and Limitations.,2009,https://doi.org/10.5281/zenodo.1418361,"P. Knees, Johannes Kepler University Linz, AUT, education;T. Pohle, Johannes Kepler University Linz, AUT, education;M. Schedl, Johannes Kepler University Linz, AUT, education;D. Schnitzer, Johannes Kepler University Linz, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility;K. Seyerlehner, Johannes Kepler University Linz, AUT, education;G. Widmer, Johannes Kepler University Linz, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility","""We investigate an approach to a music search engine that indexes music pieces based on related Web documents. This allows for searching for relevant music pieces by issuing descriptive textual queries. In this paper, we examine the effects of incorporating audio-based similarity into the text-based ranking process – either by directly modifying the retrieval process or by performing post-hoc audio-based re-ranking of the search results. The aim of this combination is to improve ranking quality by including relevant tracks that are left out by text-based retrieval approaches. Our evaluations show overall improvements but also expose limitations of these unsupervised approaches to combining sources. Evaluations are carried out on two collections, one large real-world collection containing about 35,000 tracks and on the CAL500 set."""
94,Bernhard Niedermayer,Improving Accuracy of Polyphonic Music-to-Score Alignment.,2009,https://doi.org/10.5281/zenodo.1415220,"Bernhard Niedermayer, Johannes Kepler University Linz, Austria, education","This paper presents a new method to reﬁne music-to-score alignments. The proposed system works ofﬂine in two passes, where in the ﬁrst step a state-of-the art alignment based on chroma vectors and dynamic time warping is per- formed. In the second step a non-negative matrix factor- ization is calculated within a small search window around each predicted note onset, using pretrained tone models of only those pitches which are expected to be played within that window. Note onsets are then reset according to the pitch activation patterns yielded by the matrix factoriza- tion. In doing so, we are able to resolve individual notes within a chord. We show that this method is feasible of increasing the accuracy of aligned note’s onsets which are already aligned relatively near to the real note attack. How- ever it is so far not suitable for the detection and correction of outliers which are displaced by a large timespan. We also compared our system to a reference method showing that it outperforms bandpass ﬁltering based onset detection in the reﬁnement step."
95,Kjell Lemström;Geraint A. Wiggins,Formalizing Invariances for Content-based Music Retrieval.,2009,https://doi.org/10.5281/zenodo.1418363,"Kjell Lemström, University of Helsinki, FIN, education;Geraint A. Wiggins, Goldsmiths, University of London, GBR, education","""Invariances are central concepts in content-based music re- trieval. Musical representations and similarity measures are designed to capture musically relevant invariances, such as transposition invariance. Though regularly used, their explicit deﬁnition is usually omitted because of the heavy formalism required. The lack of explicit deﬁnition, how- ever, can result in misuse or misunderstanding of the terms. We discuss the musical relevance of various musical in- variances and develop a set-theoretic formalism, for deﬁn- ing and classifying them. Using it, we deﬁne the most common invariances, and give a taxonomy which they in- habit. The taxonomy serves as a useful tool for idetinfying where work is needed to address real world problems in content-based music retrieval."""
96,Ciril Bohak;Matija Marolt,Calculating Similarity of Folk Song Variants with Melody-based Features.,2009,https://doi.org/10.5281/zenodo.1416516,"Ciril Bohak, University of Ljubljana, SVN, education;Matija Marolt, University of Ljubljana, SVN, education","""As folk songs live largely through oral transmission, there usually is no standard form of a song - each performance of a folk song may be unique. Different interpretations of the same song are called song variants, all variants of a song belong to the same variant type. In the paper, we explore how various melody-based features relate to folk song variants. Specifically, we explore whether we can derive a melodic similarity measure that would correlate to variant types in the sense that it would measure songs belonging to the same variant type as more similar, in contrast to songs from different variant types. The measure would be useful for folk song retrieval based on variant types, classification of unknown tunes, as well as a measure of similarity between variant types. We experimented with a number of melodic features calculated from symbolic representations of folk song melodies and combined them into a melody-based folk song similarity measure. We evaluated the measure on the task of classifying an unknown melody into a set of existing variant types. We show that the proposed measure gives the correct variant type in the top 10 list for 68% of queries in our data set."""
97,Jan Weil;Thomas Sikora;Jean-Louis Durrieu;Gaël Richard,Automatic Generation of Lead Sheets from Polyphonic Music Signals.,2009,https://doi.org/10.5281/zenodo.1414758,"Jan Weil, Technische Universität Berlin, DEU, education;Thomas Sikora, Technische Universität Berlin, DEU, education;J.-L. Durrieu, Telecom ParisTech, CNRS LTCI, FRA, education;Gaël Richard, Telecom ParisTech, CNRS LTCI, FRA, education","A lead sheet is a type of music notation which summarizes the content of a song. The usual elements that are reproduced are the melody, chords, tempo, time signature, style and the lyrics, if any. In this paper we propose a system that aims at transcribing both the melody and the associated chords in a beat-synchronous framework. A beat tracker identifies the pulse positions and thus defines a beat grid on which the chord sequence and the melody notes are mapped. The harmonic changes are used to estimate the time signature and the down beats as well as the key of the piece. The different modules perform very well on each of the different tasks, and the lead sheets that were rendered show the potential of the approaches adopted in this paper."
98,Jeremy Reed;Yushi Ueda;Sabato Marco Siniscalchi;Yuuki Uchiyama;Shigeki Sagayama;Chin-Hui Lee,Minimum Classification Error Training to Improve Isolated Chord Recognition.,2009,https://doi.org/10.5281/zenodo.1417163,"J.T. Reed, Georgia Institute of Technology, USA, education;Yushi Ueda, The University of Tokyo, JPN, education;S. Siniscalchi, Norwegian University of Science and Technology, NOR, education;Yuki Uchiyama, The University of Tokyo, JPN, education;Shigeki Sagayama, The University of Tokyo, JPN, education;C.-H. Lee, Georgia Institute of Technology, USA, education","Audio chord detection is the combination of two separate tasks: recognizing what chords are played and determining when chords are played. Most current audio chord detection algorithms use hidden Markov model (HMM) classifiers because of the task similarity with automatic speech recognition. For most speech recognition algorithms, the performance is measured by word error rate; i.e., only the identity of recognized segments is considered because word boundaries in continuous speech are often ambiguous. In contrast, audio chord detection performance is typically measured in terms of frame error rate, which considers both timing and classification. This paper treats these two tasks separately and focuses on the first problem; i.e., classifying the correct chords given boundary information. The best performing chroma/HMM chord detection algorithm, as measured in the 2008 MIREX Audio Chord Detection Contest, is used as the baseline in this paper. Further improvements are made to reduce feature correlation, account for differences in tuning, and incorporate minimum classification error (MCE) training in obtaining chord HMMs. Experiments demonstrate that classification rates can be improved with tuning compensation and MCE discriminative training."
99,Anssi Klapuri,A Method for Visualizing the Pitch Content of Polyphonic Music Signals.,2009,https://doi.org/10.5281/zenodo.1415036,"Anssi Klapuri, Tampere University of Technology, FIN, education","This paper proposes a method for visualizing the pitch content of polyphonic music signals. More specifically, a model is proposed for calculating the salience of pitch candidates within a given pitch range, and an optimization technique is proposed to find the parameters of the model. The aim is to produce a continuous function which shows peaks at the positions of true pitches and where spurious peaks at multiples and submultiples of the true pitches are suppressed. The proposed method was evaluated using synthesized MIDI signals, for which it outperformed a baseline method in terms of precision and recall. A straightforward visualization technique is proposed to render the pitch salience function on the traditional staves when the musical key and barline information is available."
100,Tuomas Eerola;Olivier Lartillot;Petri Toiviainen,Prediction of Multidimensional Emotional Ratings in Music from Audio Using Multivariate Regression Models.,2009,https://doi.org/10.5281/zenodo.1416730,"Tuomas Eerola, University of Jyväskylä, Finland, education;Olivier Lartillot, University of Jyväskylä, Finland, education;Petri Toiviainen, University of Jyväskylä, Finland, education","Content-based prediction of musical emotions and moods has a large number of exciting applications in Music Information Retrieval. However, what should be predicted, and precisely how, remain a challenge in the ﬁeld. We provide an empirical comparison of two common paradigms of emotion representation in music, opposing a multidimensional space to a set of basic emotions. New ground-truth data consisting of ﬁlm soundtracks was used to assess the compatibility of these models. The ﬁndings suggest that the two are highly compatible and a quantitative mapping between the two is provided. Next we propose a model predicting perceived emotions based on a set of features extracted from the audio. The feature selection and transformation is given special emphasis and three separate data reduction techniques are compared (stepwise regression, principal component analysis, and partial least squares regression). Best linear models consisting of 2-5 predictors from the data reduction process were able to account for between 58 and 85% of the variance. In general, partial least squares models performed the best and the data transformation has a signiﬁcant role in building linear models."
101,Beinan Li;Jordan B. L. Smith;Ichiro Fujinaga,Optical Audio Reconstruction for Stereo Phonograph Records Using White Light Interferometry.,2009,https://doi.org/10.5281/zenodo.1416230,"Beinan Li, McGill University, CAN, education;Jordan B. L. Smith, McGill University, CAN, education;Ichiro Fujinaga, McGill University, CAN, education","Our work focuses on optically reconstructing the stereo audio signal of a 33 rpm long-playing (LP) record using a white-light interferometry-based approach. Previously, a theoretical framework was presented, alongside the primitive reconstruction result from a few cycles of a stereo sinusoidal test signal. To reconstruct an audible duration of a longer stereo signal requires tackling new problems, such as disc warping, image alignment, and eliminating the effects of noise and broken grooves. This paper proposes solutions to these problems, and presents the complete workflow of our Optical Audio Reconstruction (OAR) system."
102,Noam Koenigstein;Yuval Shavitt,Song Ranking based on Piracy in Peer-to-Peer Networks.,2009,https://doi.org/10.5281/zenodo.1417038,"Noam Koenigstein, Tel Aviv University, Israel, education;Yuval Shavitt, Tel Aviv University, Israel, education","Music sales are loosing their role as a means for music dis-
semination but are still used by the music industry for rank-
ing artist success, e.g., in the Billboard Magazine chart.
Thus, it was suggested recently to use social networks as
an alternative ranking system; a suggestion which is prob-
lematic due to the ease of manipulating the list and the dif-
ﬁculty of implementation. In this work we suggest to use
logs of queries from peer-to-peer ﬁle-sharing systems for
ranking song success. We show that the trend and ﬂuctua-
tions of the popularity of a song in the Billboard list have
strong correlation (0.89) to the ones in a list built from the
P2P network, and that the P2P list has a week advantage
over the Billboard list. Namely, music sales are strongly
correlated with music piracy."
103,Matthias Robine;Pierre Hanna;Mathieu Lagrange,Meter Class Profiles for Music Similarity and Retrieval.,2009,https://doi.org/10.5281/zenodo.1415788,"Matthias Robine, University of Bordeaux, FRA, education;Pierre Hanna, University of Bordeaux, FRA, education;Mathieu Lagrange, Telecom ParisTech, FRA, education","Rhythm is one of the main properties of Western tonal music. Existing content-based retrieval systems generally deal with melody or style. A few existing ones based on meter or rhythm characteristics have been recently proposed but they require a precise analysis, or they rely on a low-level descriptor. In this paper, we propose a mid-level descriptor: the Meter Class Profile (MCP). The MCP is centered on the tempo and represents the strength of beat multiples, including the measure rate, and the beat subdivisions. The MCP coefficients are estimated by means of the autocorrelation and the Fourier transform of the onset detection curve. Experiments on synthetic and real databases are presented, and the results demonstrate the efficacy of the MCP descriptor in clustering and retrieval of songs according to their metric properties."
104,Christian Fremerey;Michael Clausen;Sebastian Ewert;Meinard Müller,Sheet Music-Audio Identification.,2009,https://doi.org/10.5281/zenodo.1416742,"Christian Fremerey, Bonn University, DEU, education;Michael Clausen, Bonn University, DEU, education;Sebastian Ewert, Bonn University, DEU, education;Meinard Müller, Saarland University and MPI Informatik, DEU, education","In this paper, we introduce and discuss the task of sheet music-audio identification. Given a query consisting of a sequence of bars from a sheet music representation, the task is to find corresponding sections within an audio interpretation of the same piece. Two approaches are proposed: a semi-automatic approach using synchronization and a fully automatic approach using matching techniques. A workflow is described that allows for evaluating the matching approach using the results of the more reliable synchronization approach. This workflow makes it possible to handle even complex queries from orchestral scores. Furthermore, we present an evaluation procedure, where we investigate several matching parameters and tempo estimation strategies. Our experiments have been conducted on a dataset comprising pieces of various instrumentations and complexity."
105,Byeong-jun Han;Seungmin Rho;Roger B. Dannenberg;Eenjun Hwang,SMERS: Music Emotion Recognition Using Support Vector Regression.,2009,https://doi.org/10.5281/zenodo.1415674,"Byeong-jun Han, Korea University, KOR, education;Seungmin Rho, Korea University, KOR, education;Roger B. Dannenberg, Carnegie Mellon University, USA, education;Eenjun Hwang, Korea University, KOR, education","Music emotion plays an important role in music retrieval, 
mood detection and other music-related applications. 
Many issues for music emotion recognition have been 
addressed by different disciplines such as physiology, 
psychology, cognitive science and musicology. We 
present a support vector regression (SVR) based music 
emotion recognition system. The recognition process 
consists of three steps: (i) seven distinct features are ex-
tracted from music; (ii) those features are mapped into 
eleven emotion categories on Thayer’s two-dimensional 
emotion model; (iii) two regression functions are trained 
using SVR and then arousal and valence values are pre-
dicted. We have tested our SVR-based emotion classifier 
in both Cartesian and polar coordinate system empirically. 
The result indicates the SVR classifier in the polar repre-
sentation produces satisfactory result which reaches 
94.55% accuracy superior to the SVR (in Cartesian) and 
other machine learning classification algorithms such as 
SVM and GMM.  "
106,Kerstin Bischoff;Claudiu S. Firan;Raluca Paiu;Wolfgang Nejdl;Cyril Laurier;Mohamed Sordo,Music Mood and Theme Classification - a Hybrid Approach.,2009,https://doi.org/10.5281/zenodo.1417317,"Kerstin Bischoff, L3S Research Center, DEU, facility;Claudiu S. Firan, L3S Research Center, DEU, facility;Raluca Paiu, L3S Research Center, DEU, facility;Wolfgang Nejdl, L3S Research Center, DEU, facility;Cyril Laurier, Universitat Pompeu Fabra, ESP, education;Mohamed Sordo, Universitat Pompeu Fabra, ESP, education","Music perception is highly intertwined with both emotions and context. Not surprisingly, many of the users’ informa- tion seeking actions aim at retrieving music songs based on these perceptual dimensions – moods and themes, ex- pressing how people feel about music or which situations they associate it with. In order to successfully support mu- sic retrieval along these dimensions, powerful methods are needed. Still, most existing approaches aiming at inferring some of the songs’ latent characteristics focus on identi- fying musical genres. In this paper we aim at bridging this gap between users’ information needs and indexed mu- sic features by developing algorithms for classifying mu- sic songs by moods and themes. We extend existing ap- proaches by also considering the songs’ thematic dimen- sions and by using social data from the Last.fm music por- tal, as support for the classiﬁcation tasks. Our methods exploit both audio features and collaborative user annota- tions, fusing them to improve overall performance. Eval- uation performed against the AllMusic.com ground truth shows that both kinds of information are complementary and should be merged for enhanced classiﬁcation accuracy."
107,Joachim Ganseman;Paul Scheunders;Wim D'haes,Using XML-Formatted Scores in Real-Time Applications.,2009,https://doi.org/10.5281/zenodo.1417893,"Joachim Ganseman, Paul Scheunders, University of Antwerp, BEL, education;Wim D’haes, Mu Technologies NV, BEL, company","""In this paper we present fast and scalable methods to access relevant data from music scores stored in an XML based notation format, with the explicit goal of using scores in real-time audio processing frameworks. Quick and easy access is important when accessing or traversing a score, for instance for real-time playback. Any time complexity improvement in these contexts is valuable, while memory constraints are usually less important. We show that with some well chosen design choices and precomputation of the necessary data, runtime time complexity of several key score manipulation operations can be reduced to a level that allows use in a real-time context."""
108,Amelie Anglade;Rafael Ramírez 0001;Simon Dixon,Genre Classification Using Harmony Rules Induced from Automatic Chord Transcriptions.,2009,https://doi.org/10.5281/zenodo.1414944,"Amélie Anglade, Queen Mary University of London, GBR, education;Rafael Ramirez, Universitat Pompeu Fabra, ESP, education;Simon Dixon, Queen Mary University of London, GBR, education","We present an automatic genre classification technique making use of frequent chord sequences that can be applied on symbolic as well as audio data. We adopt a first-order logic representation of harmony and musical genres: pieces of music are represented as lists of chords and musical genres are seen as context-free definite clause grammars using subsequences of these chord lists. To induce the context-free definite clause grammars characterising the genres we use a first-order logic decision tree induction algorithm. We report on the adaptation of this classification framework to audio data using an automatic chord transcription algorithm. We also introduce a high-level harmony representation scheme which describes the chords in term of both their degrees and chord categories. When compared to another high-level harmony representation scheme used in a previous study, it obtains better classification accuracies and shorter run times. We test this framework on 856 audio files synthesized from Band in a Box files and covering 3 main genres, and 9 subgenres. We perform 3-way and 2-way classification tasks on these audio files and obtain good classification results: between 67% and 79% accuracy for the 2-way classification tasks and between 58% and 72% accuracy for the 3-way classification tasks."
109,Carles Fernandes Julià;Sergi Jordà,SongExplorer: A Tabletop Application for Exploring Large Collections of Songs.,2009,https://doi.org/10.5281/zenodo.1418177,"Carles F. Julià, Universitat Pompeu Fabra, ESP, education;Sergi Jordà, Universitat Pompeu Fabra, ESP, education","This paper presents SongExplorer, a system for the exploration of large music collections on tabletop interfaces. SongExplorer addresses the problem of finding new interesting songs on large music databases, from an interaction design perspective. Using high level descriptors of musical songs, SongExplorer creates a coherent 2D map based on similarity, in which neighboring songs tend to be more similar. All songs are represented as throbbing circles that highlight their more relevant high-level properties, and the resulting music map is browseable and zoomable by the users who can use their fingers as well as specially designed tangible pucks, for helping them to find interesting music, independently of their previous knowledge of the collection. SongExplorer also offers basic player capabilities, allowing the users to organize the songs they have just discovered into playlists which can be manipulated as well as played and displayed. In this paper, the system hardware, software and interaction design are explained, and the usability tests carried are presented. Finally, conclusions and future work are discussed."
110,Woojay Jeon;Changxue Ma;Yan Ming Cheng,An Efficient Signal-Matching Approach to Melody Indexing and Search Using Continuous Pitch Contours and Wavelets.,2009,https://doi.org/10.5281/zenodo.1415054,"Woojay Jeon, Motorola, Inc., USA, company;Changxue Ma, Motorola, Inc., USA, company;Yan Ming Cheng, Motorola, Inc., USA, company","""We describe a method of indexing and efﬁciently searching music melodies based on their continuous dominant fundamental frequency (f0) contours without obtaining note-level transcriptions. Each f0 contour is encoded by a redundant set of wavelet coefﬁcients that represent its shape in level-normalized form at various locations and time scales. This allows a query melody to be exhaustively compared with variable-length portions of a target melody at arbitrary locations while accounting for differences in key and tempo. The method is applied in a Query-by-Humming (QBH) system where users may search a database of recorded pop songs by humming or singing an arbitrary part of the melody of an intended song. The system has fast retrieval times because the wavelet coefﬁcients can be effectively indexed in a binary tree and a vector distance measure instead of dynamic programming is used for comparisons. Using automatic pitch extraction to obtain all f0 contours from acoustic data, the method demonstrates practical performance in an experiment with an existing monophonic data set and in a preliminary experiment with real-world polyphonic music."""
111,Özgür Izmirli,Tonal-Atonal Classification of Music Audio Using Diffusion Maps.,2009,https://doi.org/10.5281/zenodo.1416936,"Özgür İzmirli, Connecticut College, USA, education","""In this paper we look at the problem of classifying music audio as tonal or atonal by learning a low-dimensional structure representing tonal relationships among keys. We use a training set composed of tonal pieces which includes all major and minor keys. A kernel eigenmap based method is used for structure learning and discovery. Specifically, a Diffusion Maps (DM) framework is used and its parameter tuning is discussed. Since these methods do not scale well with increasing data size, it becomes infeasible to use these methods in online applications. In order to facilitate on-line classification an out-of-sample extension to the DM framework is given. The learned structure of tonal relationships is presented and a simple scheme for classification of tonal-atonal pieces is proposed. Evaluation results show that the method is able to perform at an accuracy above 90% with the current data set."""
112,Tae Hong Park;Zhiye Li;Wen Wu,Easy Does It: The Electro-Acoustic Music Analysis Toolbox.,2009,https://doi.org/10.5281/zenodo.1416674,"Tae Hong Park, Tulane University, USA, education;Zhiye Li, Tulane University, USA, education;Wen Wu, Tulane University, USA, education","In this paper we present the EASY (Electro-Acoustic muSic analYsis) Toolbox software system for assisting electro-acoustic music analysis. The primary aims of the system are to present perceptually relevant features and audio descriptors via visual designs to gain more insight into electro-acoustic music works and provide easy-to-use “click-and-go” software interface paradigms for practical use of the system by non-experts and experts alike. The development of the EASY system exploits MIR techniques with particular emphasis on the electro-acoustic music repertoire – musical pieces that concentrate on timbral dimensions rather than traditional elements such as pitch, melody, harmony, and rhythm. The project was mainly inspired by the lack of software tools available for aiding electro-acoustic music analysis. The system’s frameworks, feature analysis algorithms, along with the initial analyses of pieces are presented here."
113,Mika Kuuskankare;Mikael Laurson,MIR in ENP - Rule-based Music Information Retrieval from Symbolic Music Notation.,2009,https://doi.org/10.5281/zenodo.1417277,"Mika Kuuskankare, Sibelius Academy, Centre for Music and Technology, FIN, education;Mikael Laurson, Sibelius Academy, Centre for Music and Technology, FIN, education","""Symbolic music information retrieval is one of the most underrepresented areas in the ﬁeld of MIR. Here, symbolic music means common practice music notation–the musician readable format. In this paper we introduce a novel rule-based symbolic music retrieval mechanism. The Scripting system–ENP-Script–is augmented with MIR functionality. It allows us to perform sophisticated retrieval operations on symbolic musical scores prepared with the help of the music notation system ENP. We will also give a special attention to visualization of the query results. All the statistical queries, such as histograms, are visualized with the help of common music notation where appropriate. N-grams and more complex queries–the ones dealing with voice leading, for example–are visualized directly in the score. Our aim is to demonstrate the power and expressivity of the combination of common music notation and a rule-based scripting language through several challenging examples."""
114,Min-Yian Su;Yi-Hsuan Yang;Yu-Ching Lin;Homer H. Chen,An Integrated Approach to Music Boundary Detection.,2009,https://doi.org/10.5281/zenodo.1417585,"Min-Yian Su, National Taiwan University, TWN, education;Yi-Hsuan Yang, National Taiwan University, TWN, education;Yu-Ching Lin, National Taiwan University, TWN, education;Homer Chen, National Taiwan University, TWN, education","Music boundary detection is a fundamental step of music analysis and summarization. Existing works use either unsupervised or supervised methodologies to detect boundary. In this paper, we propose an integrated approach that takes advantage of both methodologies. In particular, a graph-theoretic approach is proposed to fuse the results of an unsupervised model and a supervised one by the knowledge of the typical length of a music section. To further improve accuracy, a number of novel mid-level features are developed and incorporated to the boundary detection framework. Evaluation result on the RWC dataset shows the effectiveness of the proposed approach."
115,Hussein Hirjee;Daniel G. Brown 0001,Automatic Detection of Internal and Imperfect Rhymes in Rap Lyrics.,2009,https://doi.org/10.5281/zenodo.1416160,"Hussein Hirjee, University of Waterloo, CAN, education;Daniel G. Brown, University of Waterloo, CAN, education","Imperfect and internal rhymes are two important features in rap music often ignored in the music information retrieval community. We develop a method of scoring potential rhymes using a probabilistic model based on phoneme frequencies in rap lyrics. We use this scoring scheme to automatically identify internal and line-ﬁnal rhymes in song lyrics and demonstrate the performance of this method compared to rules-based models. Higher level rhyme features are produced and used to compare rhyming styles in song lyrics from different genres, and for different rap artists."
116,Verena Thomas;Christian Fremerey;David Damm;Michael Clausen,Slave: A Score-Lyrics-Audio-Video-Explorer.,2009,https://doi.org/10.5281/zenodo.1418029,"Verena Thomas, University of Bonn, DEU, education;Christian Fremerey, University of Bonn, DEU, education;David Damm, University of Bonn, DEU, education;Michael Clausen, University of Bonn, DEU, education","""We introduce the music exploration system SLAVE, which is based upon previous developments of our group. SLAVE manages multimedia music collections and allows for multimodal navigation, playback, and visualization in an efficient and user-friendly manner. 1 While previously the focus of our system development has been the simultaneous exploration of digitized sheet music and audio, with SLAVE we enhance the functionalities by video and lyrics to achieve a more comprehensive music interaction. In this paper, we concentrate on two aspects. Firstly, we integrate video documents into our framework. Secondly, we introduce a graphical user interface for semi-automatic feature extraction, indexing, and synchronization of heterogeneous music collections. The output of this GUI is used by SLAVE to offer both high quality audio and video playback with time-synchronous display of digitized sheet music and content-based search."""
117,Eric Nichols;Donald Byrd,Lyric Extraction and Recognition on Digital Images of Early Music Sources.,2009,https://doi.org/10.5281/zenodo.1417046,"John Ashley Burgoyne, McGill University, CAN, education;Johanna Devaney, McGill University, CAN, education;Yue Ouyang, McGill University, CAN, education;Laurent Pugin, McGill University, CAN, education;Tristan Himmelman, McGill University, CAN, education;Ichiro Fujinaga, McGill University, CAN, education","Optical music recognition (OMR) is one of the most promising tools for generating large-scale, distributable libraries of musical data. Much OMR work has focussed on instrumental music, avoiding a special challenge vocal music poses for OMR: lyric recognition. Lyrics complicate the page layout, making it more difﬁcult to identify the regions of the page that carry musical notation. Furthermore, users expect a complete OMR process for vocal music to include recognition of the lyrics, reuniﬁcation of syllables when they have been separated, and alignment of these lyrics with the recognised music. Unusual layouts and inconsistent practises for syllabiﬁcation, however, make lyric recognition more challenging than traditional optical character recognition (OCR). This paper surveys historical approaches to lyric recognition, outlines open challenges, and presents a new approach to extracting text lines in medieval manuscripts, one of the frontiers of OMR research today."
118,Ruben Hillewaere;Bernard Manderick;Darrell Conklin,Global Feature Versus Event Models for Folk Song Classification.,2009,https://doi.org/10.5281/zenodo.1417829,"Ruben Hillewaere, Vrije Universiteit Brussel, BEL, education;Bernard Manderick, Vrije Universiteit Brussel, BEL, education;Darrell Conklin, City University London, GBR, education","Music classiﬁcation has been widely investigated in the past few years using a variety of machine learning approaches. In this study, a corpus of 3367 folk songs, divided into six geographic regions, has been created and is used to evaluate two popular yet contrasting methods for symbolic melody classiﬁcation. For the task of folk song classification, a global feature approach, which summarizes a melody as a feature vector, is outperformed by an event model of abstract event features. The best accuracy obtained on the folk song corpus was achieved with an ensemble of event models. These results indicate that the event model should be the default model of choice for folk song classification."
119,Meinard Müller;Peter Grosche;Frans Wiering,Robust Segmentation and Annotation of Folk Song Recordings.,2009,https://doi.org/10.5281/zenodo.1417099,"Meinard Müller, Saarland University, DEU, education, MPI Informatik, DEU, facility;Peter Grosche, Saarland University, DEU, education, MPI Informatik, DEU, facility;Frans Wiering, Utrecht University, NLD, education","Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of score-based transcriptions. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer studied in the actual folk song research though they still may contain valuable information. In this paper, we introduce an automated approach for segmenting folk song recordings into its constituent stanzas, which can then be made accessible to folk song researchers by means of suitable visualization, searching, and navigation interfaces. Performed by elderly non-professional singers, the main challenge with the recordings is that most singers have serious problems with the intonation, fluctuating with their voices even over several semitones throughout a song. Using a combination of robust audio features along with various cleaning and audio matching strategies, our approach yields accurate segmentations even in the presence of strong deviations."
120,Korinna Bade;Andreas Nürnberger;Sebastian Stober;Jörg Garbers;Frans Wiering,Supporting Folk-Song Research by Automatic Metric Learning and Ranking.,2009,https://doi.org/10.5281/zenodo.1418165,"Korinna Bade, Otto-von-Guericke University Magdeburg, DEU, education;Andreas Nürnberger, Otto-von-Guericke University Magdeburg, DEU, education;Sebastian Stober, Otto-von-Guericke University Magdeburg, DEU, education;Jörg Garbers, Utrecht University, NLD, education;Frans Wiering, Utrecht University, NLD, education","""In folk song research, appropriate similarity measures can be of great help, e.g. for classiﬁcation of new tunes. Several measures have been developed so far. However, a particular musicological way of classifying songs is usually not directly reﬂected by just a single one of these measures. We show how a weighted linear combination of different basic similarity measures can be automatically adapted to a speciﬁc retrieval task by learning this metric based on a special type of constraints. Further, we describe how these constraints are derived from information provided by experts. In experiments on a folk song database, we show that the proposed approach outperforms the underlying basic similarity measures and study the effect of different levels of adaptation on the performance of the retrieval system."""
121,Sally Jo Cunningham;David M. Nichols,Exploring Social Music Behavior: An Investigation of Music Selection at Parties.,2009,https://doi.org/10.5281/zenodo.1416410,"Sally Jo Cunningham, University of Waikato, NZL, education;David M. Nichols, University of Waikato, NZL, education","This paper builds an understanding how music is cur-
rently listened to by small (fewer than 10 individuals) to 
medium-sized (10 to 40 individuals) gatherings of peo-
ple—how songs are chosen for playing, how the music 
fits in with other activities of group members, who sup-
plies the music, the hardware/software that supports song 
selection and presentation. This fine-grained context em-
erges from a qualitative analysis of a rich set of partici-
pant observations and interviews focusing on the selec-
tion of songs to play at social gatherings. We suggest fea-
tures for software to support music playing at parties."
122,Emilia Gómez;Martín Haro;Perfecto Herrera,Music and Geography: Content Description of Musical Audio from Different Parts of the World.,2009,https://doi.org/10.5281/zenodo.1416010,"Emilia Gómez, Universitat Pompeu Fabra, ESP, education;Martín Haro, Universitat Pompeu Fabra, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, ESP, education","""This paper analyses how audio features related to different musical facets can be useful for the comparative analysis and classiﬁcation of music from diverse parts of the world. The music collection under study gathers around 6,000 pieces, including traditional music from different geographical zones and countries, as well as a varied set of Western musical styles. We achieve promising results when trying to automatically distinguish music from Western and non-Western traditions. A 86.68% of accuracy is obtained using only 23 audio features, which are representative of distinct musical facets (timbre, tonality, rhythm), indicating their complementarity for music description. We also analyze the relative performance of the different facets and the capability of various descriptors to identify certain types of music. We ﬁnally present some results on the relationship between geographical location and musical features in terms of extracted descriptors. All the reported outcomes demonstrate that automatic description of audio signals together with data mining techniques provide means to characterize huge music collections from different traditions, complementing ethnomusicological manual analysis and providing a link between music and geography."""
123,Polina Proutskova;Michael A. Casey,You Call That Singing? Ensemble Classification for Multi-Cultural Collections of Music Recordings.,2009,https://doi.org/10.5281/zenodo.1418183,"Polina Proutskova, Goldsmiths, London, UK, education;Michael Casey, Dartmouth College, USA, education","The wide range of vocal styles, musical textures and re- cording techniques found in ethnomusicological field re- cordings leads us to consider the problem of automatic- ally labeling the content to know whether a recording is a song or instrumental work. Furthermore, if it is a song, we are interested in labeling aspects of the vocal texture: e.g. solo, choral, acapella or singing with instruments. We present evidence to suggest that automatic annotation is feasible for recorded collections exhibiting a wide range of recording techniques and representing musical cultures from around the world. Our experiments used the Alan Lomax Cantometrics training tapes data set, to encourage future comparative evaluations. Experiments were con- ducted with a labeled subset consisting of several hun- dred tracks, annotated at the track and frame levels, as acapella singing, singing plus instruments or instruments only. We trained frame-by-frame SVM classifiers using MFCC features on positive and negative exemplars for two tasks: per-frame labeling of singing and acapella singing. In a further experiment, the frame-by-frame clas- sifier outputs were integrated to estimate the predominant content of whole tracks. Our results show that frame-by- frame classifiers achieved 71% frame accuracy and whole track classifier integration achieved 88% accuracy. We conclude with an analysis of classifier errors suggesting avenues for developing more robust features and classifi- er strategies for large ethnographically diverse collec- tions."
