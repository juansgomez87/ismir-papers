Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Diego Furtado Silva;Chin-Chia Michael Yeh;Gustavo E. A. P. A. Batista;Eamonn J. Keogh,SiMPle: Assessing Music Similarity Using Subsequences Joins.,2016,https://doi.org/10.5281/zenodo.1415012,"Diego F. Silva, Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, BRA, education;Chin-Chia M. Yeh, Department of Computer Science and Engineering, University of California, Riverside, USA, education;Gustavo E. A. P. A. Batista, Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, BRA, education;Eamonn Keogh, Department of Computer Science and Engineering, University of California, Riverside, USA, education","Most algorithms for music information retrieval are based on the analysis of the similarity between feature sets extracted from the raw audio. A common approach to assessing similarities within or between recordings is by creating similarity matrices. However, this approach requires quadratic space for each comparison and typically requires a costly post-processing of the matrix. In this work, we propose a simple and efficient representation based on a subsequence similarity join, which may be used in several music information retrieval tasks. We apply our method to the cover song recognition problem and demonstrate that it is superior to state-of-the-art algorithms. In addition, we demonstrate how the proposed representation can be exploited for multiple applications in music processing."
1,Sebastian Ewert;Siying Wang;Meinard Müller;Mark B. Sandler,Score-Informed Identification of Missing and Extra Notes in Piano Recordings.,2016,https://doi.org/10.5281/zenodo.1418317,"Sebastian Ewert, Centre for Digital Music (C4DM), Queen Mary University of London, GBR, education;Siying Wang, Centre for Digital Music (C4DM), Queen Mary University of London, GBR, education;Meinard Müller, International Audio Laboratories Erlangen, DEU, facility;Mark Sandler, Centre for Digital Music (C4DM), Queen Mary University of London, GBR, education","A main goal in music tuition is to enable a student to play a score without mistakes, where common mistakes include missing notes or playing additional extra ones. To automatically detect these mistakes, a first idea is to use a music transcription method to detect notes played in an audio recording and to compare the results with a corresponding score. However, as the number of transcription errors produced by standard methods is often considerably higher than the number of actual mistakes, the results are often of limited use. In contrast, our method exploits that the score already provides rough information about what we seek to detect in the audio, which allows us to construct a tailored transcription method. In particular, we employ score-informed source separation techniques to learn for each score pitch a set of templates capturing the spectral properties of that pitch. After extrapolating the resulting template dictionary to pitches not in the score, we estimate the activity of each MIDI pitch over time. Finally, making again use of the score, we choose for each pitch an individualized threshold to differentiate note onsets from spurious activity in an optimized way. We indicate the accuracy of our approach on a dataset of piano pieces commonly used in education."
2,Filip Korzeniowski;Gerhard Widmer,Feature Learning for Chord Recognition: The Deep Chroma Extractor.,2016,https://doi.org/10.5281/zenodo.1416314,"Filip Korzeniowski, Johannes Kepler University Linz, AUT, education;Gerhard Widmer, Johannes Kepler University Linz, AUT, education","We explore frame-level audio feature learning for chord recognition using artiﬁcial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artiﬁcial neural networks. It is trained to compute chroma features that encode harmonic information important for chord recognition, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and resolve harmonic ambiguities. We compare the resulting features to hand-crafted ones by using a simple linear frame-wise classiﬁer for chord recognition on various data sets. The results show that the learned feature extractor produces superior chroma vectors for chord recognition."
3,Jan Schlüter,Learning to Pinpoint Singing Voice from Weakly Labeled Examples.,2016,https://doi.org/10.5281/zenodo.1417651,"Jan Schlüter, Austrian Research Institute for Artificial Intelligence, Vienna, Austria, facility","Building an instrument detector usually requires temporally accurate ground truth that is expensive to create. However, song-wise information on the presence of instruments is often easily available. In this work, we investigate how well we can train a singing voice detection system merely from song-wise annotations of vocal presence. Using convolutional neural networks, multiple-instance learning and saliency maps, we can not only detect singing voice in a test signal with a temporal accuracy close to the state-of-the-art, but also localize the spectral bins with precision and recall close to a recent source separation method. Our recipe may provide a basis for other sequence labeling tasks, for improving source separation or for inspecting neural networks trained on auditory spectrograms."
4,Pierre Beauguitte;Bryan Duggan;John D. Kelleher,A Corpus of Annotated Irish Traditional Dance Music Recordings: Design and Benchmark Evaluations.,2016,https://doi.org/10.5281/zenodo.1417333,"Pierre Beauguitte, Dublin Institute of Technology, IRL, education;Bryan Duggan, Dublin Institute of Technology, IRL, education;John Kelleher, Dublin Institute of Technology, IRL, education","An emerging trend in music information retrieval (MIR)
is the use of supervised machine learning to train automatic
music transcription models. A prerequisite of adopting a
machine learning methodology is the availability of anno-
tated corpora. However, different genres of music have dif-
ferent characteristics and modelling these characteristics is
an important part of creating state of the art MIR systems.
Consequently, although some music corpora are available
the use of these corpora is tied to the speciﬁc music genre,
instrument type and recording context the corpus covers.
This paper introduces the ﬁrst corpus of annotations of au-
dio recordings of Irish traditional dance music that cov-
ers multiple instrument types and both solo studio and live
session recordings. We ﬁrst discuss the considerations that
motivated our design choices in developing the corpus. We
then benchmark a number of automatic music transcription
algorithms against the corpus."
5,Andrew John Lambert;Tillman Weyde;Newton Armstrong,Adaptive Frequency Neural Networks for Dynamic Pulse and Metre Perception.,2016,https://doi.org/10.5281/zenodo.1418305,"Andrew J. Lambert, City University London, GBR, education;Tillman Weyde, City University London, GBR, education;Newton Armstrong, City University London, GBR, education","Beat induction, the means by which humans listen to music and perceive a steady pulse, is achieved via a perceptual and cognitive process. Computationally modelling this phenomenon is an open problem, especially when processing expressive shaping of the music such as tempo change. To meet this challenge we propose Adaptive Frequency Neural Networks (AFNNs), an extension of Gradient Frequency Neural Networks (GFNNs). GFNNs are based on neurodynamic models and have been applied successfully to a range of difficult music perception problems including those with syncopated and polyrhythmic stimuli. AFNNs extend GFNNs by applying a Hebbian learning rule to the oscillator frequencies. Thus the frequencies in an AFNN adapt to the stimulus through an attraction to local areas of resonance, and allow for a great dimensionality reduction in the network. Where previous work with GFNNs has focused on frequency and amplitude responses, we also consider phase information as critical for pulse perception. Evaluating the time-based output, we find significantly improved responses of AFNNs compared to GFNNs to stimuli with both steady and varying pulse frequencies. This leads us to believe that AFNNs could replace the linear filtering methods commonly used in beat tracking and tempo estimation systems, and lead to more accurate methods."
6,Cárthach Ó Nuanáin;Perfecto Herrera;Sergi Jordà,An Evaluation Framework and Case Study for Rhythmic Concatenative Synthesis.,2016,https://doi.org/10.5281/zenodo.1415648,"C´arthach ´O Nuan´ain, Universitat Pompeu Fabra, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, ESP, education;Sergi Jord`a, Universitat Pompeu Fabra, ESP, education","""In this paper we present and report on a methodology for evaluating a creative MIR-based application of concatenative synthesis. After reviewing many existing applications of concatenative synthesis we have developed an application that specifically addresses loop-based rhythmic pattern generation. We describe how such a system could be evaluated with respect to its its objective retrieval performance and subjective responses of humans in a listener survey. Applying this evaluation strategy produced positive findings to help verify and validate the objectives of our system. We discuss the results of the evaluation and draw conclusions by contrasting the objective analysis with the subjective impressions of the users."""
7,Alo Allik;György Fazekas;Mark B. Sandler,An Ontology for Audio Features.,2016,https://doi.org/10.5281/zenodo.1416226,"Alo Allik, Queen Mary University of London, GBR, education;György Fazekas, Queen Mary University of London, GBR, education;Mark Sandler, Queen Mary University of London, GBR, education","A plurality of audio feature extraction toolsets and feature datasets are used by the MIR community. Their different conceptual organisation of features and output formats however present difficulties in exchanging or comparing data, while very limited means are provided to link features with content and provenance. These issues are hindering research reproducibility and the use of multiple tools in combination. We propose novel Semantic Web ontologies (1) to provide a common structure for feature data formats and (2) to represent computational workflows of audio features facilitating their comparison. The Audio Feature Ontology provides a descriptive framework for expressing different conceptualisations of and designing linked data formats for content-based audio features. To accommodate different views in organising features, the ontology does not impose a strict hierarchical structure, leaving this open to task and tool specific ontologies that derive from a common vocabulary. The ontologies are based on the analysis of existing feature extraction tools and the MIR literature, which was instrumental in guiding the design process. They are harmonised into a library of modular interlinked ontologies that describe the different entities and activities involved in music creation, production and consumption."
8,Daniel Stoller;Simon Dixon,Analysis and Classification of Phonation Modes In Singing.,2016,https://doi.org/10.5281/zenodo.1416772,"Daniel Stoller, Queen Mary University of London, GBR, education;Simon Dixon, Queen Mary University of London, GBR, education","Phonation mode is an expressive aspect of the singing voice and can be described using the four categories neutral, breathy, pressed and ﬂow. Previous attempts at automatically classifying the phonation mode on a dataset containing vowels sung by a female professional have been lacking in accuracy or have not sufﬁciently investigated the characteristic features of the different phonation modes which enable successful classiﬁcation. In this paper, we extract a large range of features from this dataset, including specialised descriptors of pressedness and breathiness, to analyse their explanatory power and robustness against changes of pitch and vowel. We train and optimise a feed-forward neural network (NN) with one hidden layer on all features using cross validation to achieve a mean F-measure above 0.85 and an improved performance compared to previous work. Applying feature selection based on mutual information and retaining the nine highest ranked features as input to a NN results in a mean F-measure of 0.78, demonstrating the suitability of these features to discriminate between phonation modes. Training and pruning a decision tree yields a simple rule set based only on cepstral peak prominence (CPP), temporal ﬂatness and average energy that correctly categorises 78% of the recordings."
9,Jiajie Dai;Simon Dixon,Analysis of Vocal Imitations of Pitch Trajectories.,2016,https://doi.org/10.5281/zenodo.1416318,"Jiajie Dai, Centre for Digital Music, Queen Mary University of London, United Kingdom, education;Simon Dixon, Centre for Digital Music, Queen Mary University of London, United Kingdom, education","In this paper, we analyse the pitch trajectories of vocal imitations by non-poor singers. A group of 43 selected singers was asked to vocally imitate a set of stimuli. Five stimulus types were used: a constant pitch (stable), a constant pitch preceded by a pitch glide (head), a constant pitch followed by a pitch glide (tail), a pitch ramp and a pitch with vibrato; with parameters for main pitch, transient length and pitch difference. Two conditions were tested: singing simultaneously with the stimulus, and singing alternately, between repetitions of the stimulus. After automatic pitch-tracking and manual checking of the data, we calculated intonation accuracy and precision, and modelled the note trajectories according to the stimulus types. We modelled pitch error with a linear mixed-effects model, and tested factors for significant effects using one-way analysis of variance. The results indicate: (1) Significant factors include stimulus type, main pitch, repetition, condition and musical training background, while order of stimuli, gender and age do not have any significant effect. (2) The ramp, vibrato and tail stimuli have significantly greater absolute pitch errors than the stable and head stimuli. (3) Pitch error shows a small but significant linear trend with pitch difference. (4) Notes with shorter transient duration are more accurate."
10,Gabriel Vigliensoni;Ichiro Fujinaga,"Automatic Music Recommendation Systems: Do Demographic, Profiling, and Contextual Features Improve Their Performance?.",2016,https://doi.org/10.5281/zenodo.1417073,"Gabriel Vigliensoni, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada, education;Ichiro Fujinaga, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada, education","Traditional automatic music recommendation systems’ performance typically rely on the accuracy of statistical models learned from past preferences of users on music items. However, additional sources of data such as demographic attributes of listeners, their listening behaviour, and their listening contexts encode information about listeners, and their listening habits, that may be used to improve the accuracy of music recommendation models. In this paper we introduce a large dataset of music listening histories with listeners’ demographic information, and a set of features to characterize aspects of people’s listening behaviour. The longevity of the collected listening histories, covering over two years, allows the retrieval of basic forms of listening context. We use this dataset in the evaluation of accuracy of a music artist recommendation model learned from past preferences of listeners on music items and their interaction with several combinations of people’s demographic, profiling, and contextual features. Our results indicate that using listeners’ self-declared age, country, and gender improve the recommendation accuracy by 8 percent. When a new profiling feature termed exploratoryness was added, the accuracy of the model increased by 12 percent."
11,Yen-Cheng Lu;Chih-Wei Wu;Alexander Lerch;Chang-Tien Lu,Automatic Outlier Detection in Music Genre Datasets.,2016,https://doi.org/10.5281/zenodo.1418193,"Yen-Cheng Lu, Department of Computer Science, Virginia Tech, USA, education;Chih-Wei Wu, Center for Music Technology, Georgia Institute of Technology, USA, education;Chang-Tien Lu, Department of Computer Science, Virginia Tech, USA, education;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology, USA, education","Outlier detection, also known as anomaly detection, is an important topic that has been studied for decades. An outlier detection system is able to identify anomalies in a dataset and thus improve data integrity by removing the detected outliers. It has been successfully applied to different types of data in various ﬁelds such as cyber-security, ﬁnance, and transportation. In the ﬁeld of Music Information Retrieval (MIR), however, the number of related studies is small. In this paper, we introduce different state-of-the-art outlier detection techniques and evaluate their viability in the context of music datasets. More speciﬁcally, we present a comparative study of 6 outlier detection algorithms applied to a Music Genre Recognition (MGR) dataset. It is determined how well algorithms can identify mislabeled or corrupted ﬁles, and how much the quality of the dataset can be improved. Results indicate that state-of-the-art anomaly detection systems have problems identifying anomalies in MGR datasets reliably."
12,Luwei Yang;Khalid Z. Rajab;Elaine Chew,AVA: An Interactive System for Visual and Quantitative Analyses of Vibrato and Portamento Performance Styles.,2016,https://doi.org/10.5281/zenodo.1415592,"Luwei Yang, Centre for Digital Music, Queen Mary University of London, GBR, education;Khalid Z. Rajab, Antennas & Electromagnetics Group, Queen Mary University of London, GBR, education;Elaine Chew, Centre for Digital Music, Queen Mary University of London, GBR, education","Vibratos and portamenti are important expressive features for characterizing performance style on instruments capable of continuous pitch variation such as strings and voice. Accurate study of these features is impeded by time consuming manual annotations. We present AVA, an interactive tool for automated detection, analysis, and visualization of vibratos and portamenti. The system implements a Filter Diagonalization Method (FDM)-based and a Hidden Markov Model-based method for vibrato and portamento detection. Vibrato parameters are reported directly from the FDM, and portamento parameters are given by the best fit Logistic Model. The graphical user interface (GUI) allows the user to edit the detection results, to view each vibrato or portamento, and to read the output parameters. The entire set of results can also be written to a text file for further statistical analysis. Applications of AVA include music summarization, similarity assessment, music learning, and musicological analysis. We demonstrate AVA’s utility by using it to analyze vibratos and portamenti in solo performances of two Beijing opera roles and two string instruments, erhu and violin."
13,Gissel Velarde;Tillman Weyde;Carlos Eduardo Cancino Chacón;David Meredith 0001;Maarten Grachten,Composer Recognition Based on 2D-Filtered Piano-Rolls.,2016,https://doi.org/10.5281/zenodo.1417641,"Gissel Velarde, Aalborg University, DNK, education;Tillman Weyde, City University London, GBR, education;Carlos Cancino Chacón, Austrian Research Institute for Artificial Intelligence, AUT, facility;David Meredith, Aalborg University, DNK, education;Maarten Grachten, Austrian Research Institute for Artificial Intelligence, AUT, facility","We propose a method for music classiﬁcation based on the use of convolutional models on symbolic pitch–time representations (i.e. piano-rolls) which we apply to composer recognition. An excerpt of a piece to be classiﬁed is ﬁrst sampled to a 2D pitch–time representation which is then subjected to various transformations, including convolu- tion with predeﬁned ﬁlters (Morlet or Gaussian) and clas- siﬁed by means of support vector machines. We combine classiﬁers based on different pitch representations (MIDI and morphetic pitch) and different ﬁlter types and conﬁg- urations. The method does not require parsing of the mu- sic into separate voices, or extraction of any other prede- ﬁned features prior to processing; instead it is based on the analysis of texture in a 2D pitch–time representation. We show that ﬁltering signiﬁcantly improves recognition and that the method proves robust to encoding, transposition and amount of information. On discriminating between Haydn and Mozart string quartet movements, our best clas- siﬁer reaches state-of-the-art performance in leave-one-out cross validation."
14,Kristina Andersen;Peter Knees,Conversations with Expert Users in Music Retrieval and Research Challenges for Creative MIR.,2016,https://doi.org/10.5281/zenodo.1418323,"Kristina Andersen, Studio for Electro Instrumental Music (STEIM), NLD, facility;Peter Knees, Johannes Kepler University Linz, AUT, education","Sample retrieval remains a central problem in the creative process of making electronic dance music. This paper describes the findings from a series of interview sessions involving users working creatively with electronic music. We conducted in-depth interviews with expert users on location at the Red Bull Music Academies in 2014 and 2015. When asked about their wishes and expectations for future technological developments in interfaces, most participants mentioned very practical requirements of storing and retrieving files. A central aspect of the desired systems is the need to provide increased flow and unbroken periods of concentration and creativity. From the interviews, it becomes clear that for Creative MIR, and in particular, for music interfaces for creative expression, traditional requirements and paradigms for music and audio retrieval differ to those from consumer-centered MIR tasks such as playlist generation and recommendation and that new paradigms need to be considered. Despite all technical aspects being controllable by the experts themselves, searching for sounds to use in composition remains a largely semantic process. From the outcomes of the interviews, we outline a series of possible conclusions and areas and pose two research challenges for future developments of sample retrieval interfaces in the creative domain."
15,Florian Krebs;Sebastian Böck;Matthias Dorfer;Gerhard Widmer,Downbeat Tracking Using Beat Synchronous Features with Recurrent Neural Networks.,2016,https://doi.org/10.5281/zenodo.1417819,"Florian Krebs, Johannes Kepler University Linz, AUT, education;Sebastian Böck, Johannes Kepler University Linz, AUT, education;Matthias Dorfer, Johannes Kepler University Linz, AUT, education;Gerhard Widmer, Johannes Kepler University Linz, AUT, education","In this paper, we propose a system that extracts the downbeat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results."
16,Julien Osmalskyj;Marc Van Droogenbroeck;Jean-Jacques Embrechts,Enhancing Cover Song Identification with Hierarchical Rank Aggregation.,2016,https://doi.org/10.5281/zenodo.1418109,"Julien Osmalskyj, University of Liège, BEL, education;Marc Van Droogenbroeck, University of Liège, BEL, education;Jean-Jaques Embrechts, University of Liège, BEL, education","Cover song identification involves calculating pairwise similarities between a query audio track and a database of reference tracks. While most authors make exclusively use of chroma features, recent work tends to demonstrate that combining similarity estimators based on multiple audio features increases the performance. We improve this approach by using a hierarchical rank aggregation method for combining estimators based on different features. More precisely, we first aggregate estimators based on global features such as the tempo, the duration, the overall loudness, the number of beats, and the average chroma vector. Then, we aggregate the resulting composite estimator with four popular state-of-the-art methods based on chromas as well as timbre sequences. We further introduce a refinement step for the rank aggregation called “local Kemenization” and quantify its benefit for cover song identification. The performance of our method is evaluated on the Second Hand Song dataset. Our experiments show a significant improvement of the performance, up to an increase of more than 200 % of the number of queries identified in the Top-1, compared to previous results."
17,Tim Tse;Justin Salamon;Alex C. Williams;Helga Jiang;Edith Law,Ensemble: A Hybrid Human-Machine System for Generating Melody Scores from Audio.,2016,https://doi.org/10.5281/zenodo.1416708,"Tim Tse, University of Waterloo, CAN, education;Justin Salamon, New York University, USA, education;Alex Williams, University of Waterloo, CAN, education;Helga Jiang, University of Waterloo, CAN, education;Edith Law, University of Waterloo, CAN, education","Music transcription is a highly complex task that is difﬁcult for automated algorithms, and equally challenging to people, even those with many years of musical training. Furthermore, there is a shortage of high-quality datasets for training automated transcription algorithms. In this research, we explore a semi-automated, crowdsourced approach to generate music transcriptions, by ﬁrst running an automatic melody transcription algorithm on a (polyphonic) song to produce a series of discrete notes representing the melody, and then soliciting the crowd to correct this melody. We present a novel web-based interface that enables the crowd to correct transcriptions, report results from an experiment to understand the capabilities of non-experts to perform this challenging task, and characterize the characteristics and actions of workers and how they correlate with transcription performance."
18,Sergio Oramas;Luis Espinosa Anke;Aonghus Lawlor;Xavier Serra;Horacio Saggion,Exploring Customer Reviews for Music Genre Classification and Evolutionary Studies.,2016,https://doi.org/10.5281/zenodo.1415544,"Sergio Oramas, Music Technology Group, Universitat Pompeu Fabra, ESP, education;Luis Espinosa-Anke, TALN Group, Universitat Pompeu Fabra, ESP, education;Aonghus Lawlor, Insight Centre for Data Analytics, University College of Dublin, IRL, education;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, ESP, education;Horacio Saggion, TALN Group, Universitat Pompeu Fabra, ESP, education","In this paper, we explore a large multimodal dataset of about 65k albums constructed from a combination of Ama- zon customer reviews, MusicBrainz metadata and Acous- ticBrainz audio descriptors. Review texts are further en- riched with named entity disambiguation along with po- larity information derived from an aspect-based sentiment analysis framework. This dataset constitutes the corner- stone of two main contributions: First, we perform ex- periments on music genre classiﬁcation, exploring a va- riety of feature types, including semantic, sentimental and acoustic features. These experiments show that modeling semantic information contributes to outperforming strong bag-of-words baselines. Second, we provide a diachronic study of the criticism of music genres via a quantitative analysis of the polarity associated to musical aspects over time. Our analysis hints at a potential correlation between key cultural and geopolitical events and the language and evolving sentiments found in music reviews."
19,Jan Hajic Jr.;Jiri Novotný;Pavel Pecina;Jaroslav Pokorný,Further Steps Towards a Standard Testbed for Optical Music Recognition.,2016,https://doi.org/10.5281/zenodo.1418161,"Jan Hajič jr., Charles University, Institute of Formal and Applied Linguistics, CZE, education;Jiří Novotný, Charles University, Department of Software Engineering, CZE, education;Pavel Pecina, Charles University, Institute of Formal and Applied Linguistics, CZE, education;Jaroslav Pokorný, Charles University, Department of Software Engineering, CZE, education","Evaluating Optical Music Recognition (OMR) is notoriously difﬁcult and automated end-to-end OMR evaluation metrics are not available to guide development. In “Towards a Standard Testbed for Optical Music Recognition: Deﬁnitions, Metrics, and Page Images”, Byrd and Simonsen recently stress that a benchmarking standard is needed in the OMR community, both with regards to data and evaluation metrics. We build on their analysis and deﬁnitions and present a prototype of an OMR benchmark. We do not, however, presume to present a complete solution to the complex problem of OMR benchmarking. Our contributions are: (a) an attempt to deﬁne a multi-level OMR benchmark dataset and a practical prototype implementation for both printed and handwritten scores, (b) a corpus-based methodology for assessing automated evaluation metrics, and an underlying corpus of over 1000 qualiﬁed relative cost-to-correct judgments. We then assess several straightforward automated MusicXML evaluation metrics against this corpus to establish a baseline over which further metrics can improve."
20,Nicolas Guiomard-Kagan;Mathieu Giraud;Richard Groult;Florence Levé,Improving Voice Separation by Better Connecting Contigs.,2016,https://doi.org/10.5281/zenodo.1417825,"Nicolas Guiomard-Kagan, Univ. Picardie Jules Verne, FRA, education, CRIStAL, UMR CNRS 9189, Univ. Lille, FRA, education;Mathieu Giraud, Univ. Picardie Jules Verne, FRA, education, CRIStAL, UMR CNRS 9189, Univ. Lille, FRA, education;Richard Groult, Univ. Picardie Jules Verne, FRA, education;Florence Levé, Univ. Picardie Jules Verne, FRA, education, CRIStAL, UMR CNRS 9189, Univ. Lille, FRA, education","""Separating a polyphonic symbolic score into monophonic voices or streams helps to understand the music and may simplify further pattern matching. One of the best ways to compute this separation, as proposed by Chew and Wu in 2005 [2], is to first identify contigs that are portions of the music score with a constant number of voices, then to progressively connect these contigs. This raises two questions: Which contigs should be connected first? And, how should these two contigs be connected? Here we propose to answer simultaneously these two questions by considering a set of musical features that measures the quality of any connection. The coefficients weighting the features are optimized through a genetic algorithm. We benchmark the resulting connection policy on corpora containing fugues of the Well-Tempered Clavier by J. S. Bach as well as on string quartets, and we compare it against previously proposed policies [2, 9]. The contig connection is improved, particularly when one takes into account the whole content of voice fragments to assess the quality of their possible connection."""
21,Tsubasa Tanaka;Brian Bemman;David Meredith 0001,Integer Programming Formulation of the Problem of Generating Milton Babbitt's All-Partition Arrays.,2016,https://doi.org/10.5281/zenodo.1416164,"Tsubasa Tanaka, STMS Lab : IRCAM, CNRS, UPMC, FRA, facility;Brian Bemman, Aalborg University, DNK, education;David Meredith, Aalborg University, DNK, education","Milton Babbitt (1916–2011) was a composer of twelve-tone serial music noted for creating the all-partition array. The problem of generating an all-partition array involves finding a rectangular array of pitch-class integers that can be partitioned into regions, each of which represents a distinct integer partition of 12. Integer programming (IP) has proven to be effective for solving such combinatorial problems, however, it has never before been applied to the problem addressed in this paper. We introduce a new way of viewing this problem as one in which restricted overlaps between integer partition regions are allowed. This permits us to describe the problem using a set of linear constraints necessary for IP. In particular, we show that this problem can be defined as a special case of the well-known problem of set-covering (SCP), modified with additional constraints. Due to the difficulty of the problem, we have yet to discover a solution. However, we assess the potential practicality of our method by running it on smaller similar problems."
22,Hendrik Vincent Koops;W. Bas de Haas;Dimitrios Bountouridis;Anja Volk,Integration and Quality Assessment of Heterogeneous Chord Sequences Using Data Fusion.,2016,https://doi.org/10.5281/zenodo.1415954,"Hendrik Vincent Koops, Department of Information and Computing Sciences, Utrecht University, NLD, education;W. Bas de Haas, Chordify, NLD, company;Dimitrios Bountouridis, Department of Information and Computing Sciences, Utrecht University, NLD, education;Anja Volk, Department of Information and Computing Sciences, Utrecht University, NLD, education","Two heads are better than one, and the many are smarter than the few. Integrating knowledge from multiple sources has shown to increase retrieval and classiﬁcation accu- racy in many domains. The recent explosion of crowd- sourced information, such as on websites hosting chords and tabs for popular songs, calls for sophisticated algo- rithms for data-driven quality assessment and data integra- tion to create better, and more reliable data. In this pa- per, we propose to integrate the heterogeneous output of multiple automatic chord extraction algorithms using data fusion. First we show that data fusion creates signiﬁcantly better chord label sequences from multiple sources, outper- forming its source material, majority voting and random source integration. Second, we show that data fusion is capable of assessing the quality of sources with high pre- cision from source agreement, without any ground-truth knowledge. Our study contributes to a growing body of work showing the beneﬁts of integrating knowledge from multiple sources in an advanced way."
23,Reinhard Sonnleitner;Andreas Arzt;Gerhard Widmer,Landmark-Based Audio Fingerprinting for DJ Mix Monitoring.,2016,https://doi.org/10.5281/zenodo.1417008,"Reinhard Sonnleitner, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence, AUT, facility;Andreas Arzt, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence, AUT, facility;Gerhard Widmer, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence, AUT, facility","Recently, the media monitoring industry shows increased interest in applying automated audio identification systems for revenue distribution of DJ performances played in discotheques. DJ mixes incorporate a wide variety of signal modifications, e.g. pitch shifting, tempo modifications, cross-fading and beat-matching. These signal modifications are expected to be more severe than what is usually encountered in the monitoring of radio and TV broadcasts. The monitoring of DJ mixes presents a hard challenge for automated music identification systems, which need to be robust to various signal modifications while maintaining a high level of specificity to avoid false revenue assignment. In this work we assess the fitness of three landmark-based audio fingerprinting systems with different properties on real-world data – DJ mixes that were performed in discotheques. To enable the research community to evaluate systems on DJ mixes, we also create and publish a freely available, creative-commons licensed dataset of DJ mixes along with their reference tracks and song-border annotations. Experiments on these datasets reveal that a recent quad-based method achieves considerably higher performance on this task than the other methods."
24,Rafael Valle;Daniel J. Fremont;Ilge Akkaya;Alexandre Donzé;Adrian Freed;Sanjit A. Seshia,Learning and Visualizing Music Specifications Using Pattern Graphs.,2016,https://doi.org/10.5281/zenodo.1414954,"Rafael Valle, UC Berkeley, CNMAT, USA, education;Daniel J. Fremont, UC Berkeley, USA, education;Ilge Akkaya, UC Berkeley, USA, education;Alexandre Donze, UC Berkeley, USA, education;Adrian Freed, UC Berkeley, CNMAT, USA, education;Sanjit S. Seshia, UC Berkeley, USA, education","We describe a system to learn and visualize speciﬁcations from song(s) in symbolic and audio formats. The core of our approach is based on a software engineering procedure called speciﬁcation mining. Our procedure extracts patterns from feature vectors and uses them to build pattern graphs. The feature vectors are created by segmenting song(s) and extracting time and and frequency domain features from them, such as chromagrams, chord degree and interval classiﬁcation. The pattern graphs built on these feature vectors provide the likelihood of a pattern between nodes, as well as start and ending nodes. The pattern graphs learned from a song(s) describe formal speciﬁcations that can be used for human interpretable quantitatively and qualitatively song comparison or to perform supervisory control in machine improvisation. We offer results in song summarization, song and style validation and machine improvisation with formal speciﬁcations."
25,Ben Fields;Christophe Rhodes,Listen To Me - Don't Listen To Me: What Communities of Critics Tell Us About Music.,2016,https://doi.org/10.5281/zenodo.1417801,"Ben Fields, Goldsmiths University of London, GBR, education;Christophe Rhodes, Goldsmiths University of London, GBR, education","Social knowledge and data sharing on the Web takes many forms. So too do the ways people share ideas and opinions. In this paper we examine one such emerging form: the amateur critic. In particular, we examine genius.com, a website which allows its users to annotate and explain the meaning of segments of lyrics in music and other written works. We describe a novel dataset of approximately 700,000 users’ activity on genius.com, their social connections, and song annotation activity. The dataset encompasses over 120,000 songs, with more than 3 million unique annotations. Using this dataset, we model overlap in interest or expertise through the proxy of co-annotation. This is the basis for a complex network model of the activity on genius.com, which is then used for community detection. We introduce a new measure of network community activity: community skew. Through this analysis we draw a comparison of between co-annotation and notions of genre and categorisation in music. We show a new view on the social constructs of genre in music."
26,Romain Hennequin;François Rigaud,Long-Term Reverberation Modeling for Under-Determined Audio Source Separation with Application to Vocal Melody Extraction.,2016,https://doi.org/10.5281/zenodo.1417489,"Romain Hennequin, Deezer R&D, FRA, company;Franc¸ois Rigaud, Audionamix R&D, FRA, company","In this paper, we present a way to model long-term reverberation effects in under-determined source separation algorithms based on a non-negative decomposition framework. A general model for the sources affected by reverberation is introduced and update rules for the estimation of the parameters are presented. Combined with a well-known source-ﬁlter model for singing voice, an application to the extraction of reverberated vocal tracks from polyphonic music signals is proposed. Finally, an objective evaluation of this application is described. Performance improvements are obtained compared to the same model without reverberation modeling, in particular by signiﬁcantly reducing the amount of interference between sources."
27,Elliot Creager;Noah D. Stein;Roland Badeau;Philippe Depalle,Nonnegative Tensor Factorization with Frequency Modulation Cues for Blind Audio Source Separation.,2016,https://doi.org/10.5281/zenodo.1415736,"Elliot Creager, Analog Devices Lyric Labs, USA, company, CIRMMT, McGill University, CAN, education;Noah D. Stein, Analog Devices Lyric Labs, USA, company;Roland Badeau, LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, FRA, education, CIRMMT, McGill University, CAN, education;Philippe Depalle, CIRMMT, McGill University, CAN, education","We present Vibrato Nonnegative Tensor Factorization, an algorithm for single-channel unsupervised audio source separation with an application to separating instrumental or vocal sources with nonstationary pitch from music recordings. Our approach extends Nonnegative Matrix Factorization for audio modeling by including local estimates of frequency modulation as cues in the separation. This permits the modeling and unsupervised separation of vibrato or glissando musical sources, which is not possible with the basic matrix factorization formulation. The algorithm factorizes a sparse nonnegative tensor comprising the audio spectrogram and local frequency-slope-to-frequency ratios, which are estimated at each time-frequency bin using the Distributed Derivative Method. The use of local frequency modulations as separation cues is motivated by the principle of common fate partial grouping from Auditory Scene Analysis, which hypothesizes that each latent source in a mixture is characterized perceptually by coherent frequency and amplitude modulations shared by its component partials. We derive multiplicative factor updates by Minorization-Maximization, which guarantees convergence to a local optimum by iteration. We then compare our method to the baseline on two separation tasks: one considers synthetic vibrato notes, while the other considers vibrato string instrument recordings."
28,Chih-Wei Wu;Alexander Lerch,On Drum Playing Technique Detection in Polyphonic Mixtures.,2016,https://doi.org/10.5281/zenodo.1416680,"Chih-Wei Wu, Georgia Institute of Technology, Center for Music Technology, USA, facility;Alexander Lerch, Georgia Institute of Technology, Center for Music Technology, USA, facility","In this paper, the problem of drum playing technique detection in polyphonic mixtures of music is addressed. We focus on the identiﬁcation of 4 rudimentary techniques: strike, buzz roll, ﬂam, and drag. The speciﬁcs and the challenges of this task are being discussed, and different sets of features are compared, including various features extracted from NMF-based activation functions, as well as baseline spectral features. We investigate the capabil- ities and limitations of the presented system in the case of real-world recordings and polyphonic mixtures. To de- sign and evaluate the system, two datasets are introduced: a training dataset generated from individual drum hits, and ad- ditional annotations of the well-known ENST drum dataset minus one subset as test dataset. The results demonstrate issues with the traditionally used spectral features, and in- dicate the potential of using NMF activation functions for playing technique detection, however, the performance of polyphonic music still leaves room for future improvement."
29,I-Ting Liu;Richard Randall,Predicting Missing Music Components with Bidirectional Long Short-Term Memory Neural Networks.,2016,https://doi.org/10.5281/zenodo.1417239,"I-Ting Liu, Carnegie Mellon University, USA, education;Richard Randall, Carnegie Mellon University, USA, education","Successfully predicting missing components (entire parts or voices) from complex multipart musical textures has attracted researchers of music information retrieval and music theory. However, these applications were limited to either two-part melody and accompaniment (MA) textures or four-part Soprano-Alto-Tenor-Bass (SATB) textures. This paper proposes a robust framework applicable to both textures using a Bidirectional Long-Short Term Memory (BLSTM) recurrent neural network. The BLSTM system was evaluated using frame-wise accuracies on the Nottingham Folk Song dataset and J. S. Bach Chorales. Experimental results demonstrated that adding bidirectional links to the neural network improves prediction accuracy by 3% on average. Specifically, BLSTM outperforms other neural-network based methods by 4.6% on average for four-part SATB and two-part MA textures (employing a transition matrix). The high accuracies obtained with BLSTM on both two-part and four-part textures indicated that BLSTM is the most robust and applicable structure for predicting missing components from multi-part musical textures."
30,Vinutha T. P.;Suryanarayana Sankagiri;Kaustuv Kanti Ganguli;Preeti Rao,Structural Segmentation and Visualization of Sitar and Sarod Concert Audio.,2016,https://doi.org/10.5281/zenodo.1414924,"Vinutha T.P., IIT Bombay, IND, education;Suryanarayana Sankagiri, IIT Bombay, IND, education;Kaustuv Kanti Ganguli, IIT Bombay, IND, education;Preeti Rao, IIT Bombay, IND, education","Hindustani classical instrumental concerts follow an episodic development that, musicologically, is described via changes in the rhythmic structure. Uncovering this structure in a musically relevant form can provide for powerful visual representations of the concert audio that is of potential value in music appreciation and pedagogy. We investigate the structural analysis of the metered section (gat) of concerts of two plucked string instruments, the sitar and sarod. A prominent aspect of the gat is the interplay between the melody soloist and the accompanying drummer (tabla). The tempo as provided by the tabla together with the rhythmic density of the sitar/sarod plucks serve as the main dimensions that predict the transition between concert sections. We present methods to access the stream of tabla onsets separately from the sitar/sarod onsets, addressing challenges that arise in the instrument separation. Further, the robust detection of tempo and the estimation of rhythmic density of sitar/sarod plucks are discussed. A case study of a fully annotated concert is presented, and is followed by results of achieved segmentation accuracy on a database of sitar and sarod gats across artists."
31,Jonathan Driedger;Stefan Balke;Sebastian Ewert;Meinard Müller,Template-Based Vibrato Analysis in Complex Music Signals.,2016,https://doi.org/10.5281/zenodo.1417006,"Jonathan Driedger, International Audio Laboratories Erlangen, DEU, facility;Stefan Balke, International Audio Laboratories Erlangen, DEU, facility;Sebastian Ewert, Queen Mary University of London, GBR, education;Meinard Müller, International Audio Laboratories Erlangen, DEU, facility","""The automated analysis of vibrato in complex music signals is a highly challenging task. A common strategy is to proceed in a two-step fashion. First, a fundamental frequency (F0) trajectory for the musical voice that is likely to exhibit vibrato is estimated. In a second step, the trajectory is then analyzed with respect to periodic frequency modulations. As a major drawback, however, such a method cannot recover from errors made in the inherently difficult first step, which severely limits the performance during the second step. In this work, we present a novel vibrato analysis approach that avoids the first error-prone F0-estimation step. Our core idea is to perform the analysis directly on a signal’s spectrogram representation where vibrato is evident in the form of characteristic spectro-temporal patterns. We detect and parameterize these patterns by locally comparing the spectrogram with a predefined set of vibrato templates. Our systematic experiments indicate that this approach is more robust than F0-based strategies."""
32,Stefan Balke;Jonathan Driedger;Jakob Abeßer;Christian Dittmar;Meinard Müller,Towards Evaluating Multiple Predominant Melody Annotations in Jazz Recordings.,2016,https://doi.org/10.5281/zenodo.1415076,"Stefan Balke, International Audio Laboratories Erlangen, Germany, facility;Jonathan Driedger, International Audio Laboratories Erlangen, Germany, facility;Jakob Abeßer, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany, facility;Christian Dittmar, International Audio Laboratories Erlangen, Germany, facility;Meinard Müller, International Audio Laboratories Erlangen, Germany, facility","Melody estimation algorithms are typically evaluated by separately assessing the task of voice activity detection and fundamental frequency estimation. For both subtasks, computed results are typically compared to a single human reference annotation. This is problematic since different human experts may differ in how they specify a predominant melody, thus leading to a pool of equally valid reference annotations. In this paper, we address the problem of evaluating melody extraction algorithms within a jazz music scenario. Using four human and two automatically computed annotations, we discuss the limitations of standard evaluation measures and introduce an adaptation of Fleiss’ kappa that can better account for multiple reference annotations. Our experiments not only highlight the behavior of the different evaluation measures, but also give deeper insights into the melody extraction task."
33,Sebastian Böck;Florian Krebs;Gerhard Widmer,Joint Beat and Downbeat Tracking with Recurrent Neural Networks.,2016,https://doi.org/10.5281/zenodo.1415836,"Sebastian Böck, Johannes Kepler University Linz, AUT, education;Florian Krebs, Johannes Kepler University Linz, AUT, education;Gerhard Widmer, Johannes Kepler University Linz, AUT, education","""In this paper we present a novel method for jointly extracting beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectrograms is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and downbeat positions to the global best solution. We ﬁnd that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles."""
34,Andre Holzapfel;Thomas Grill,Bayesian Meter Tracking on Learned Signal Representations.,2016,https://doi.org/10.5281/zenodo.1417263,"Andre Holzapfel, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility;Thomas Grill, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility","Most music exhibits a pulsating temporal structure, known as meter. Consequently, the task of meter tracking is of great importance for the domain of Music Information Retrieval. In our contribution, we specifically focus on Indian art musics, where meter is conceptualized at several hierarchical levels, and a diverse variety of metrical hierarchies exist, which poses a challenge for state of the art analysis methods. To this end, for the first time, we combine Convolutional Neural Networks (CNN), allowing to transcend manually tailored signal representations, with subsequent Dynamic Bayesian Tracking (BT), modeling the recurrent metrical structure in music. Our approach estimates meter structures simultaneously at two metrical levels. The results constitute a clear advance in meter tracking performance for Indian art music, and we also demonstrate that these results generalize to a set of Ballroom dances. Furthermore, the incorporation of neural network output allows a computationally efficient inference. We expect the combination of learned signal representations through CNNs and higher-level temporal modeling to be applicable to all styles of metered music, provided the availability of sufficient training data."
35,Frederic Font;Xavier Serra,Tempo Estimation for Music Loops and a Simple Confidence Measure.,2016,https://doi.org/10.5281/zenodo.1417659,"Frederic Font, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Tempo estimation is a common task within the music information retrieval community, but existing works are rarely evaluated with datasets of music loops and the algorithms are not tailored to this particular type of content. In addition to this, existing works on tempo estimation do not put an emphasis on providing a conﬁdence value that indicates how reliable their tempo estimations are. In current music creation contexts, it is common for users to search for and use loops shared in online repositories. These loops are typically not produced by professionals and lack annotations. Hence, the existence of reliable tempo estimation algorithms becomes necessary to enhance the reusability of loops shared in such repositories. In this paper, we test six existing tempo estimation algorithms against four music loop datasets containing more than 35k loops. We also propose a simple and computationally cheap conﬁdence measure that can be applied to any existing algorithm to estimate the reliability of their tempo predictions when applied to music loops. We analyse the accuracy of the algorithms in combination with our proposed conﬁdence measure, and see that we can signiﬁcantly improve the algorithms’ performance when only considering music loops with high estimated conﬁdence."
36,Sebastian Stober;Thomas Prätzlich;Meinard Müller,Brain Beats: Tempo Extraction from EEG Data.,2016,https://doi.org/10.5281/zenodo.1416128,"Sebastian Stober, University of Potsdam, DEU, education;Thomas Pr¨atzlich, International Audio Laboratories Erlangen, DEU, education;Meinard M¨uller, International Audio Laboratories Erlangen, DEU, education","This paper addresses the question how music information retrieval techniques originally developed to process audio recordings can be adapted for the analysis of corresponding brain activity data. In particular, we conducted a case study applying beat tracking techniques to extract the tempo from electroencephalography (EEG) recordings obtained from people listening to music stimuli. We point out similarities and differences in processing audio and EEG data and show to which extent the tempo can be successfully extracted from EEG signals. Furthermore, we demonstrate how the tempo extraction from EEG signals can be stabilized by applying different fusion approaches on the mid-level tempogram features."
37,Brian McFee;Eric J. Humphrey;Julián Urbano,A Plan for Sustainable MIR Evaluation.,2016,https://doi.org/10.5281/zenodo.1417775,"Brian McFee, New York University, USA, education;Eric J. Humphrey, Spotify, Ltd., company;Julián Urbano, Universitat Pompeu Fabra, ESP, education","The Music Information Retrieval Evaluation eXchange (MIREX) is a valuable community service, having established standard datasets, metrics, baselines, methodologies, and infrastructure for comparing MIR methods. While MIREX has managed to successfully maintain operations for over a decade, its long-term sustainability is at risk. The imposed constraint that input data cannot be made freely available to participants necessitates that all algorithms run on centralized computational resources, which are administered by a limited number of people. This incurs an approximately linear cost with the number of submissions, exacting significant tolls on both human and financial resources, such that the current paradigm becomes less tenable as participation increases. To alleviate the recurring costs of future evaluation campaigns, we propose a distributed, community-centric paradigm for system evaluation, built upon the principles of openness, transparency, reproducibility, and incremental evaluation. We argue that this proposal has the potential to reduce operating costs to sustainable levels. Moreover, the proposed paradigm would improve scalability, and eventually result in the release of large, open datasets for improving both MIR techniques and evaluation methods."
38,Andrew Demetriou;Martha Larson;Cynthia C. S. Liem,Go with the Flow: When Listeners Use Music as Technology.,2016,https://doi.org/10.5281/zenodo.1415528,"Andrew Demetriou, Delft University of Technology, NLD, education;Martha Larson, Delft University of Technology, NLD, education, Radboud University, NLD, education;Cynthia C. S. Liem, Delft University of Technology, NLD, education","Music has been shown to have a profound effect on listeners’ internal states as evidenced by neuroscience research. Listeners report selecting and listening to music with specific intent, thereby using music as a tool to achieve desired psychological effects within a given context. In light of these observations, we argue that music information retrieval research must revisit the dominant assumption that listening to music is only an end unto itself. Instead, researchers should embrace the idea that music is also a technology used by listeners to achieve a specific desired internal state, given a particular set of circumstances and a desired goal. This paper focuses on listening to music in isolation (i.e., when the user listens to music by themselves with headphones) and surveys research from the fields of social psychology and neuroscience to build a case for a new line of research in music information retrieval on the ability of music to produce flow states in listeners. We argue that interdisciplinary collaboration is necessary in order to develop the understanding and techniques necessary to allow listeners to exploit the full potential of music as psychological technology."
39,Jin Ha Lee;Yea-Seul Kim;Chris Hubbles,A Look at the Cloud from Both Sides Now: An Analysis of Cloud Music Service Usage.,2016,https://doi.org/10.5281/zenodo.1417627,"Jin Ha Lee, University of Washington, USA, education;Yea-Seul Kim, University of Washington, USA, education;Chris Hubbles, University of Washington, USA, education","Despite the increasing popularity of cloud-based music services, few studies have examined how users select and utilize these services, how they manage and access their music collections in the cloud, and the issues or challenges they are facing within these services. In this paper, we present findings from an online survey with 198 responses collected from users of commercial cloud music services, exploring their selection criteria, use patterns, perceived limitations, and future predictions. We also investigate differences in these aspects by age and gender. Our results elucidate previously under-studied changes in music consumption, music listening behaviors, and music technology adoption. The findings also provide insights into how to improve the future design of cloud-based music services, and have broader implications for any cloud-based services designed for managing and accessing personal media collections."
40,Yuta Ojima;Eita Nakamura;Katsutoshi Itoyama;Kazuyoshi Yoshii,"A Hierarchical Bayesian Model of Chords, Pitches, and Spectrograms for Multipitch Analysis.",2016,https://doi.org/10.5281/zenodo.1414968,"Yuta Ojima, Graduate School of Informatics, Kyoto University, Japan, education;Eita Nakamura, Graduate School of Informatics, Kyoto University, Japan, education;Katsutoshi Itoyama, Graduate School of Informatics, Kyoto University, Japan, education;Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, Japan, education","This paper presents a statistical multipitch analyzer that can simultaneously estimate pitches and chords (typical pitch combinations) from music audio signals in an unsupervised manner. A popular approach to multipitch analysis is to perform nonnegative matrix factorization (NMF) for estimating the temporal activations of semitone-level pitches and then execute thresholding for making a piano-roll representation. The major problems of this cascading approach are that an optimal threshold is hard to determine for each musical piece and that musically inappropriate pitch combinations are allowed to appear. To solve these problems, we propose a probabilistic generative model that fuses an acoustic model (NMF) for a music spectrogram with a language model (hidden Markov model; HMM) for pitch locations in a hierarchical Bayesian manner. More specifically, binary variables indicating the existences of pitches are introduced into the framework of NMF. The latent grammatical structures of those variables are regulated by an HMM that encodes chord progressions and pitch co-occurrences (chord components). Given a music spectrogram, all the latent variables (pitches and chords) are estimated jointly by using Gibbs sampling. The experimental results showed the great potential of the proposed method for unified music transcription and grammar induction."
41,Michele Buccoli;Massimiliano Zanoni;György Fazekas;Augusto Sarti;Mark B. Sandler,A Higher-Dimensional Expansion of Affective Norms for English Terms for Music Tagging.,2016,https://doi.org/10.5281/zenodo.1415194,"Michele Buccoli, Politecnico di Milano, ITA, education;Massimiliano Zanoni, Politecnico di Milano, ITA, education;György Fazekas, Queen Mary, University of London, GBR, education;Augusto Sarti, Politecnico di Milano, ITA, education;Mark Sandler, Queen Mary, University of London, GBR, education","The Valence, Arousal and Dominance (VAD) model for emotion representation is widely used in music analysis. The ANEW dataset is composed of more than 2000 emotion related descriptors annotated in the VAD space. However, due to the low number of dimensions of the VAD model, the distribution of terms of the ANEW dataset tends to be compact and cluttered. In this work, we aim at finding a possibly higher-dimensional transformation of the VAD space, where the terms of the ANEW dataset are better organised conceptually and bear more relevance to music tagging. Our approach involves the use of a kernel expansion of the ANEW dataset to exploit a higher number of dimensions, and the application of distance learning techniques to find a distance metric that is consistent with the semantic similarity among terms. In order to train the distance learning algorithms, we collect information on the semantic similarity from human annotation and editorial tags. We evaluate the quality of the method by clustering the terms in the found high-dimensional domain. Our approach exhibits promising results with objective and subjective performance metrics, showing that a higher dimensional space could be useful to model semantic similarity among terms of the ANEW dataset."
42,Chia-Hao Chung;Jing-Kai Lou;Homer H. Chen,"A Latent Representation of Users, Sessions, and Songs for Listening Behavior Analysis.",2016,https://doi.org/10.5281/zenodo.1416878,"Chia-Hao Chung, National Taiwan University, TWN, education;Jing-Kai Lou, KKBOX Inc., TWN, company;Homer Chen, National Taiwan University, TWN, education","""Understanding user listening behaviors is important to the personalization of music recommendation. In this paper, we present an approach that discovers user behavior from a large-scale, real-world listening record. The proposed approach generates a latent representation of users, listening sessions, and songs, where each of these objects is represented as a point in the multi-dimensional latent space. Since the distance between two points is an indication of the similarity of the two corresponding objects, it becomes extremely simple to evaluate the similarity between songs or the matching of songs with the user preference. By exploiting this feature, we provide a two-dimensional user behavior analysis framework for music recommendation. Exploring the relationships between user preference and the contextual or temporal information in the session data through this framework significantly facilitates personalized music recommendation. We provide experimental results to illustrate the strengths of the proposed approach for user behavior analysis."""
43,Vincent Besson;Marco Gurrieri;Philippe Rigaux;Alice Tacaille;Virginie Thion,A Methodology for Quality Assessment in Collaborative Score Libraries.,2016,https://doi.org/10.5281/zenodo.1418105,"Vincent Besson, CESR, Univ. Tours, FRA, education;Marco Gurrieri, CESR, Univ. Tours, FRA, education;Philippe Rigaux, CEDRIC/CNAM, Paris, FRA, education;Alice Tacaille, IReMus, Sorbonne Universités, Paris, FRA, education, IRISA, Univ. Rennes 1, Lannion, FRA, education;Virginie Thion, IRISA, Univ. Rennes 1, Lannion, FRA, education","""We examine quality issues raised by the development of XML-based Digital Score Libraries. Based on the authors’ practical experience, the paper exposes the quality shortcomings inherent to the complexity of music encoding, and the lack of support from state-of-the-art formats. We also identify the various facets of the “quality” concept with respect to usages and motivations. We finally propose a general methodology to introduce quality management as a first-level concern in the management of score collections."""
44,Katherine M. Kinnaird,Aligned Hierarchies: A Multi-Scale Structure-Based Representation for Music-Based Data Streams.,2016,https://doi.org/10.5281/zenodo.1417405,"Katherine M. Kinnaird, Macalester College, USA, education","We introduce aligned hierarchies, a low-dimensional representation for music-based data streams, such as recordings of songs or digitized representations of scores. The aligned hierarchies encode all hierarchical decompositions of repeated elements from a high-dimensional and noisy music-based data stream into one object. These aligned hierarchies can be embedded into a classification space with a natural notion of distance. We construct the aligned hierarchies by finding, encoding, and synthesizing all repeated structure present in a music-based data stream. For a data set of digitized scores, we conducted experiments addressing the fingerprint task that achieved perfect precision-recall values. These experiments provide an initial proof of concept for the aligned hierarchies addressing MIR tasks."
45,Francisco Rodríguez-Algarra;Bob L. Sturm;Hugo Maruri-Aguilar,Analysing Scattering-Based Music Content Analysis Systems: Where's the Music?.,2016,https://doi.org/10.5281/zenodo.1414724,"Francisco Rodríguez-Algarra, Centre for Digital Music, Queen Mary University of London, GBR, education;Bob L. Sturm, Centre for Digital Music, Queen Mary University of London, GBR, education;Hugo Maruri-Aguilar, School of Mathematical Sciences, Queen Mary University of London, GBR, education","Music content analysis (MCA) systems built using scattering transform features are reported quite successful in the GTZAN benchmark music dataset. In this paper, we seek to answer why. We first analyse the feature extraction and classification components of scattering-based MCA systems. This guides us to perform intervention experiments on three factors: train/test partition, classifier and recording spectrum. The partition intervention shows a decrease in the amount of reproduced ground truth by the resulting systems. We then replace the learning algorithm with a binary decision tree, and identify the impact of specific feature dimensions. We finally alter the spectral content related to such dimensions, which reveals that these scattering-based systems exploit acoustic information below 20 Hz to reproduce GTZAN ground truth. The source code to reproduce our experiments is available online."
46,Anders Elowsson,Beat Tracking with a Cepstroid Invariant Neural Network.,2016,https://doi.org/10.5281/zenodo.1416054,"Anders Elowsson, KTH Royal Institute of Technology, SWE, education","We present a novel rhythm tracking architecture that learns how to track tempo and beats through layered learning. A basic assumption of the system is that humans understand rhythm by letting salient periodicities in the music act as a framework, upon which the rhythmical structure is interpreted. Therefore, the system estimates the cepstroid (the most salient periodicity of the music), and uses a neural network that is invariant with regards to the cepstroid length. The input of the network consists mainly of features that capture onset characteristics along time, such as spectral differences. The invariant properties of the network are achieved by subsampling the input vectors with a hop size derived from a musically relevant subdivision of the computed cepstroid of each song. The output is filtered to detect relevant periodicities and then used in conjunction with two additional networks, which estimates the speed and tempo of the music, to predict the final beat positions. We show that the architecture has a high performance on music with public annotations."
47,Anna M. Kruspe,Bootstrapping a System for Phoneme Recognition and Keyword Spotting in Unaccompanied Singing.,2016,https://doi.org/10.5281/zenodo.1417553,"Anna M. Kruspe, Fraunhofer IDMT, Germany, facility","Speech recognition in singing is still a largely unsolved problem. Acoustic models trained on speech usually produce unsatisfactory results when used for phoneme recognition in singing. On the ﬂipside, there is no phonetically annotated singing data set that could be used to train more accurate acoustic models for this task. In this paper, we attempt to solve this problem using the DAMP data set which contains a large number of recordings of amateur singing in good quality. We ﬁrst align them to the matching textual lyrics using an acoustic model trained on speech. We then use the resulting phoneme alignment to train new acoustic models using only subsets of the DAMP singing data. These models are then tested for phoneme recognition and, on top of that, keyword spotting. Evaluation is performed for different subsets of DAMP and for an unrelated set of the vocal tracks of commercial pop songs. Results are compared to those obtained with acoustic models trained on the TIMIT speech data set and on a version of TIMIT augmented for singing. Our new approach shows signiﬁcant improvements over both."
48,Eva Zangerle;Martin Pichl;Benedikt Hupfauf;Günther Specht,Can Microblogs Predict Music Charts? An Analysis of the Relationship Between #Nowplaying Tweets and Music Charts.,2016,https://doi.org/10.5281/zenodo.1417881,"Eva Zangerle, University of Innsbruck, AUT, education;Martin Pichl, University of Innsbruck, AUT, education;Benedikt Hupfauf, University of Innsbruck, AUT, education;Günther Specht, University of Innsbruck, AUT, education","Twitter is one of the leading social media platforms, where hundreds of millions of tweets cover a wide range of topics, including the music a user is listening to. Such #nowplaying tweets may serve as an indicator for future charts, however, this has not been thoroughly studied yet. Therefore, we investigate to which extent such tweets correlate with the Billboard Hot 100 charts and whether they allow for music charts prediction. The analysis is based on #nowplaying tweets and the Billboard charts of the years 2014 and 2015. We analyze three different aspects in regards to the time series representing #nowplaying tweets and the Billboard charts: (i) the correlation of Twitter and the Billboard charts, (ii) the temporal relation between those two and (iii) the prediction performance in regards to charts positions of tracks. We ﬁnd that while there is a mild correlation between tweets and the charts, there is a temporal lag between these two time series for 90% of all tracks. As for the predictive power of Twitter, we ﬁnd that incorporating Twitter information in a multivariate model results in a signiﬁcant decrease of both the mean RMSE as well as the variance of rank predictions."
49,Ricardo Scholz;Geber Ramalho;Giordano Cabral,Cross Task Study on MIREX Recent Results: An Index for Evolution Measurement and Some Stagnation Hypotheses.,2016,https://doi.org/10.5281/zenodo.1416898,"Ricardo Scholz, Universidade Federal de Pernambuco, BRA, education;Geber Ramalho, Universidade Federal de Pernambuco, BRA, education;Giordano Cabral, Universidade Federal de Pernambuco, BRA, education","In the last 20 years, Music Information Retrieval (MIR) has been an expanding research field, and the MIREX competition has become the main evaluation venue in MIR field. Analyzing recent results for various tasks of MIREX (MIR Evaluation eXchange), we observed that the evolution of task solutions follows two different patterns: for some tasks, the results apparently hit stagnation, whereas for others, they seem getting better over time. In this paper, (a) we compile the MIREX results of the last 6 years, (b) we propose a configurable quantitative index for evolution trend measurement of MIREX tasks, and (c) we discuss possible explanations or hypotheses for the stagnation phenomena hitting some of them. This paper hopes to incite a debate in the MIR research community about the progress in the field and how to adequately measure evolution trends."
50,Dmitry Bogdanov;Alastair Porter;Perfecto Herrera;Xavier Serra,Cross-Collection Evaluation for Music Classification Tasks.,2016,https://doi.org/10.5281/zenodo.1418131,"Dmitry Bogdanov, Universitat Pompeu Fabra, ESP, education;Alastair Porter, Universitat Pompeu Fabra, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Many studies in music classiﬁcation are concerned with obtaining the highest possible cross-validation result. However, some studies have noted that cross-validation may be prone to biases and that additional evaluations based on independent out-of-sample data are desirable. In this paper we present a methodology and software tools for cross-collection evaluation for music classiﬁcation tasks. The tools allow users to conduct large-scale evaluations of classiﬁer models trained within the AcousticBrainz platform, given an independent source of ground-truth annotations, and its mapping with the classes used for model training. To demonstrate the application of this methodology we evaluate ﬁve models trained on genre datasets commonly used by researchers for genre classiﬁcation, and use collaborative tags from Last.fm as an independent source of ground truth. We study a number of evaluation strategies using our tools on validation sets from 240,000 to 1,740,000 music recordings and discuss the results."
51,Simon Durand;Slim Essid,Downbeat Detection with Conditional Random Fields and Deep Learned Features.,2016,https://doi.org/10.5281/zenodo.1417739,"Simon Durand, LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, FRA, education;Slim Essid, LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, FRA, education","In this paper, we introduce a novel Conditional Random Field (CRF) system that detects the downbeat sequence of musical audio signals. Feature functions are computed from four deep learned representations based on harmony, rhythm, melody and bass content to take advantage of the high-level and multi-faceted aspect of this task. Downbeats being dynamic, the powerful CRF classification system allows us to combine our features with an adapted temporal model in a fully data-driven fashion. Some meters being under-represented in our training set, we show that data augmentation enables a statistically significant improvement of the results by taking into account class imbalance. An evaluation of different configurations of our system on nine datasets shows its efficiency and potential over a heuristic based approach and four downbeat tracking algorithms."
52,Li Su;Tsung-Ying Chuang;Yi-Hsuan Yang,"Exploiting Frequency, Periodicity and Harmonicity Using Advanced Time-Frequency Concentration Techniques for Multipitch Estimation of Choir and Symphony.",2016,https://doi.org/10.5281/zenodo.1414838,"Li Su, Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan, facility;Tsung-Ying Chuang, Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan, facility;Yi-Hsuan Yang, Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan, facility","To advance research on automatic music transcription (AMT), it is important to have labeled datasets with suf- ﬁcient diversity and complexity that support the creation and evaluation of robust algorithms to deal with issues seen in real-world polyphonic music signals. In this paper, we propose new datasets and investigate signal processing algorithms for multipitch estimation (MPE) in choral and symphony music, which have been seldom considered in AMT research. We observe that MPE in these two types of music is challenging because of not only the high polyphony number, but also the possible imprecision in pitch for notes sung or played by multiple singers or musicians in unison. To improve the robustness of pitch estimation, experiments show that it is beneﬁcial to measure pitch saliency by jointly considering frequency, periodicity and harmonicity information. Moreover, we can improve the localization and stability of pitch by the multi-taper methods and nonlinear time-frequency reas- signment techniques such as the Concentration of Time and Frequency (ConceFT) transform. We show that the proposed unsupervised methods to MPE compare favor- ably with, if not superior to, state-of-the-art supervised methods in various types of music signals from both existing and the newly created datasets."
53,Hendrik Schreiber,Genre Ontology Learning: Comparing Curated with Crowd-Sourced Ontologies.,2016,https://doi.org/10.5281/zenodo.1417479,"Hendrik Schreiber, tagtraum industries incorporated, USA, company","""The Semantic Web has made it possible to automatically ﬁnd meaningful connections between musical pieces which can be used to infer their degree of similarity. Similarity in turn, can be used by recommender systems driving music discovery or playlist generation. One useful facet of knowledge for this purpose are ﬁne-grained genres and their inter-relationships. In this paper we present a method for learning genre ontologies from crowd-sourced genre labels, exploiting genre co-occurrence rates. Using both lexical and conceptual similarity measures, we show that the quality of such learned ontologies is comparable with manually created ones. In the process, we document properties of current reference genre ontologies, in particular a high degree of disconnectivity. Further, motivated by shortcomings of the established taxonomic precision measure, we deﬁne a novel measure for highly disconnected ontologies."""
54,Clement Laroche;Hélène Papadopoulos;Matthieu Kowalski;Gaël Richard,Genre Specific Dictionaries for Harmonic/Percussive Source Separation.,2016,https://doi.org/10.5281/zenodo.1417147,"Clément Laroche, LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay, FRA, education, Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, FRA, education;Hélène Papadopoulos, Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, FRA, education;Matthieu Kowalski, Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, FRA, education, Parietal project-team, INRIA, CEA-Saclay, FRA, facility;Gaël Richard, LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay, FRA, education","Blind source separation usually obtains limited performance on real and polyphonic music signals. To overcome these limitations, it is common to rely on prior knowledge under the form of side information as in Informed Source Separation or on machine learning paradigms applied on a training database. In the context of source separation based on factorization models such as the Non-negative Matrix Factorization, this supervision can be introduced by learning specific dictionaries. However, due to the large diversity of musical signals it is not easy to build sufficiently compact and precise dictionaries that will well characterize the large array of audio sources. In this paper, we argue that it is relevant to construct genre-specific dictionaries. Indeed, we show on a task of harmonic/percussive source separation that the dictionaries built on genre-specific training subsets yield better performances than cross-genre dictionaries."
55,Giuseppe Bandiera;Oriol Romani Picas;Hiroshi Tokuda;Wataru Hariya;Koji Oishi;Xavier Serra,Good-sounds.org: A Framework to Explore Goodness in Instrumental Sounds.,2016,https://doi.org/10.5281/zenodo.1416864,"Giuseppe Bandiera, Universitat Pompeu Fabra, ESP, education;Oriol Romani Picas, Universitat Pompeu Fabra, ESP, education;Hiroshi Tokuda, KORG Inc., JPN, company;Wataru Hariya, KORG Inc., JPN, company;Koji Oishi, KORG Inc., JPN, company;Xavier Serra, Universitat Pompeu Fabra, ESP, education","We introduce good-sounds.org, a community driven framework based on freesound.org to explore the concept of goodness in instrumental sounds. Goodness is considered here as the common agreed basic sound quality of an instrument without taking into consideration musical expressiveness. Musicians upload their sounds and vote on existing sounds, and from the collected data the system is able to develop sound goodness measures of relevance for music education applications. The core of the system is a database of sounds, together with audio features extracted from them using MTG’s Essentia library and user annotations related to the goodness of the sounds. The web frontend provides useful data visualizations of the sound attributes and tools to facilitate user interaction. To evaluate the framework, we carried out an experiment to rate sound goodness of single notes of nine orchestral instruments. In it, users rated the sounds using an AB vote over a set of sound attributes defined to be of relevance in the characterization of single notes of instrumental sounds. With the obtained votes we built a ranking of the sounds for each attribute and developed a model that rates the goodness for each of the selected sound attributes. Using this approach, we have succeeded in obtaining results comparable to a model that was built from expert generated evaluations."
56,Thomas Hedges;Geraint A. Wiggins,Improving Predictions of Derived Viewpoints in Multiple Viewpoints Systems.,2016,https://doi.org/10.5281/zenodo.1416630,"Thomas Hedges, Queen Mary University of London, GBR, education;Geraint Wiggins, Queen Mary University of London, GBR, education","This paper presents and tests a method for improving the predictive power of derived viewpoints in multiple viewpoints systems. Multiple viewpoint systems are a well established method for the statistical modelling of sequential symbolic musical data. A useful class of viewpoints known as derived viewpoints map symbols from a basic event space to a viewpoint-specific domain. Probability estimates are calculated in the derived viewpoint domain before an inverse function maps back to the basic event space to complete the model. Since an element in the derived viewpoint domain can potentially map onto multiple basic elements, probability mass is distributed between the basic elements with a uniform distribution. As an alternative, this paper proposes a distribution weighted by zero-order frequencies of the basic elements to inform this probability mapping. Results show this improves the predictive performance for certain derived viewpoints, allowing them to be selected in viewpoint selection."
57,T. J. Tsai;Thomas Prätzlich;Meinard Müller,Known Artist Live Song ID: A Hashprint Approach.,2016,https://doi.org/10.5281/zenodo.1418223,"TJ Tsai, University of California Berkeley, USA, education;Thomas Prätzlich, International Audio Laboratories Erlangen, Germany, facility;Meinard Müller, International Audio Laboratories Erlangen, Germany, facility","The goal of live song identiﬁcation is to recognize a song based on a short, noisy cell phone recording of a live performance. We propose a system for known-artist live song identiﬁcation and provide empirical evidence of its feasibility. The proposed system represents audio as a sequence of hashprints, which are binary ﬁngerprints that are derived from applying a set of spectro-temporal ﬁlters to a spectrogram representation. The spectro-temporal ﬁlters can be learned in an unsupervised manner on a small amount of data, and can thus tailor its representation to each artist. Matching is performed using a cross-correlation approach with downsampling and rescoring. We evaluate our approach on the Gracenote live song identiﬁcation benchmark data set, and compare our results to ﬁve other baseline systems. Compared to the previous state-of-the-art, the proposed system improves the mean reciprocal rank from .68 to .79, while simultaneously reducing the average runtime per query from 10 seconds down to 0.9 seconds."
58,Il-Young Jeong;Kyogu Lee,Learning Temporal Features Using a Deep Neural Network and its Application to Music Genre Classification.,2016,https://doi.org/10.5281/zenodo.1416416,"Il-Young Jeong, Graduate School of Convergence Science and Technology, Seoul National University, KOR, education;Kyogu Lee, Graduate School of Convergence Science and Technology, Seoul National University, KOR, education","In this paper, we describe a framework for temporal feature learning from audio with a deep neural network, and apply it to music genre classiﬁcation. To this end, we revisit the conventional spectral feature learning framework, and reformulate it in the cepstral modulation spectrum domain, which has been successfully used in many speech and music-related applications for temporal feature extraction. Experimental results using the GTZAN dataset show that the temporal features learned from the proposed method are able to obtain classiﬁcation accuracy comparable to that of the learned spectral features."
59,W. Bas de Haas;Anja Volk,Meter Detection in Symbolic Music Using Inner Metric Analysis.,2016,https://doi.org/10.5281/zenodo.1417345,"W. Bas de Haas, Utrecht University, NLD, education;Anja Volk, Utrecht University, NLD, education","In this paper we present PRIMA: a new model tailored to symbolic music that detects the meter and the ﬁrst down- beat position of a piece. Given onset data, the metrical structure of a piece is interpreted using the Inner Metric Analysis (IMA) model. IMA identiﬁes the strong and weak metrical positions in a piece by performing a periodicity analysis, resulting in a weight proﬁle for the entire piece. Next, we reduce IMA to a feature vector and model the detection of the meter and its ﬁrst downbeat position prob- abilistically. In order to solve the meter detection prob- lem effectively, we explore various feature selection and parameter optimisation strategies, including Genetic, Max- imum Likelihood, and Expectation-Maximisation algo- rithms. PRIMA is evaluated on two datasets of MIDI ﬁles: a corpus of ragtime pieces, and a newly assembled pop dataset. We show that PRIMA outperforms autocorrelation- based meter detection as implemented in the MIDItoolbox on these datasets."
60,Gen Hori;Shigeki Sagayama,Minimax Viterbi Algorithm for HMM-Based Guitar Fingering Decision.,2016,https://doi.org/10.5281/zenodo.1417639,"Gen Hori, Asia University, JPN, education, RIKEN, JPN, facility;Shigeki Sagayama, Meiji University, JPN, education","Previous works on automatic ﬁngering decision for string instruments have been mainly based on path optimization by minimizing the difﬁculty of a whole phrase that is typ- ically deﬁned as the sum of the difﬁculties of moves re- quired for playing the phrase. However, from a practical viewpoint of beginner players, it is more important to min- imize the maximum difﬁculty of a move required for play- ing the phrase, that is, to make the most difﬁcult move easier. To this end, we introduce a variant of the Viterbi algorithm (termed the “minimax Viterbi algorithm”) that ﬁnds the path of the hidden states that maximizes the min- imum transition probability (not the product of the transi- tion probabilities) and apply it to HMM-based guitar ﬁn- gering decision. We compare the resulting ﬁngerings by the conventional Viterbi algorithm and our proposed min- imax Viterbi algorithm to show the appropriateness of our new method."
61,João Paulo V. Cardoso;Luciana Fujii Pontello;Pedro H. F. Holanda;Bruno Guilherme;Olga Goussevskaia;Ana Paula Couto da Silva,Mixtape: Direction-Based Navigation in Large Media Collections.,2016,https://doi.org/10.5281/zenodo.1416072,"João Paulo V. Cardoso, Universidade Federal de Minas Gerais, BRA, education;Luciana Fujii Pontello, Universidade Federal de Minas Gerais, BRA, education;Pedro H. F. Holanda, Universidade Federal de Minas Gerais, BRA, education;Bruno Guilherme, Universidade Federal de Minas Gerais, BRA, education;Olga Goussevskaia, Universidade Federal de Minas Gerais, BRA, education;Ana Paula C. da Silva, Universidade Federal de Minas Gerais, BRA, education","In this work we explore the increasing demand for novel user interfaces to navigate large media collections. We implement a scalable data structure to store and retrieve similarity information and propose a novel navigation framework that uses geometric vector operations and real-time user feedback to direct the outcome. In particular, we implement this framework in the domain of music. To evaluate the effectiveness of the navigation process, we propose an automatic evaluation framework, based on synthetic user profiles, which allows to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape 1 , a simple web application that allows users to create playlists by providing real-time feedback through liking and skipping patterns."
62,Ryo Nishikimi;Eita Nakamura;Katsutoshi Itoyama;Kazuyoshi Yoshii,Musical Note Estimation for F0 Trajectories of Singing Voices Based on a Bayesian Semi-Beat-Synchronous HMM.,2016,https://doi.org/10.5281/zenodo.1418023,"Ryo Nishikimi, Graduate School of Informatics, Kyoto University, JPN, education;Eita Nakamura, Graduate School of Informatics, Kyoto University, JPN, education;Katsutoshi Itoyama, Graduate School of Informatics, Kyoto University, JPN, education;Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, JPN, education","""This paper presents a statistical method that estimates a sequence of discrete musical notes from a temporal trajectory of vocal F0s. Since considerable effort has been devoted to estimate the frame-level F0s of singing voices from music audio signals, we tackle musical note estimation for those F0s to obtain a symbolic musical score. A na¨ıve approach to musical note estimation is to quantize the vocal F0s at a semitone level in every time unit (e.g., half beat). This approach, however, fails when the vocal F0s are signiﬁcantly deviated from those speciﬁed by a musical score. The onsets of musical notes are often delayed or advanced from beat times and the vocal F0s ﬂuctuate according to singing expressions. To deal with these deviations, we propose a Bayesian hidden Markov model that allows musical notes to change in semi-synchronization with beat times. Both the semitone-level F0s and onset deviations of musical notes are regarded as latent variables and the frequency deviations are modeled by an emission distribution. The musical notes and their onset and frequency deviations are jointly estimated by using Gibbs sampling. Experimental results showed that the proposed method improved the accuracy of musical note estimation against baseline methods."""
63,Maria Panteli;Simon Dixon,On the Evaluation of Rhythmic and Melodic Descriptors for Music Similarity.,2016,https://doi.org/10.5281/zenodo.1417555,"Maria Panteli, Centre for Digital Music, Queen Mary University of London, United Kingdom, education;Simon Dixon, Centre for Digital Music, Queen Mary University of London, United Kingdom, education","In exploratory studies of large music collections where often no ground truth is available, it is essential to evaluate the suitability of the underlying methods prior to drawing any conclusions. In this study we focus on the evaluation of audio features that can be used for rhythmic and melodic content description and similarity estimation. We select a set of state-of-the-art rhythmic and melodic descriptors and assess their invariance with respect to transformations of timbre, recording quality, tempo and pitch. We create a dataset of synthesised audio and investigate which features are invariant to the aforementioned transformations and whether invariance is affected by characteristics of the music style and the monophonic or polyphonic character of the audio recording. From the descriptors tested, the scale transform performed best for rhythm classification and retrieval and pitch bihistogram performed best for melody. The proposed evaluation strategy can inform decisions in the feature design process leading to significant improvement in the reliability of the features."
64,Rainer Kelz;Matthias Dorfer;Filip Korzeniowski;Sebastian Böck;Andreas Arzt;Gerhard Widmer,On the Potential of Simple Framewise Approaches to Piano Transcription.,2016,https://doi.org/10.5281/zenodo.1416488,"Rainer Kelz, Johannes Kepler University Linz, AUT, education;Matthias Dorfer, Johannes Kepler University Linz, AUT, education;Filip Korzeniowski, Johannes Kepler University Linz, AUT, education;Sebastian Böck, Johannes Kepler University Linz, AUT, education;Andreas Arzt, Johannes Kepler University Linz, AUT, education;Gerhard Widmer, Johannes Kepler University Linz, AUT, education","In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset – without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve."
65,Jeff Gregorio;Youngmoo Kim,Phrase-Level Audio Segmentation of Jazz Improvisations Informed by Symbolic Data.,2016,https://doi.org/10.5281/zenodo.1414790,"Jeff Gregorio, Drexel University, USA, education;Youngmoo E. Kim, Drexel University, USA, education","Computational music structure analysis encompasses any model attempting to organize music into qualitatively salient structural units, which can include anything in the heirarchy of large scale form, down to individual phrases and notes. While much existing audio-based segmenta- tion work attempts to capture repetition and homogeneity cues useful at the form and thematic level, the time scales involved in phrase-level segmenation and the avoidance of repetition in improvised music necessitate alternate ap- proaches in approaching jazz structure analysis. Recently, the Weimar Jazz Database has provided transcriptions of solos by a variety of eminent jazz performers. Utilizing a subset of these transcriptions aligned to their associated audio sources, we propose a model based on supervised training of a Hidden Markov Model with ground-truth state sequences designed to encode melodic contours appearing frequently in jazz improvisations. Results indicate that rep- resenting likely melodic contours in this way allows a low- level audio feature set containing primarily timbral and harmonic information to more accurately predict phrase boundaries."
66,Bob L. Sturm,Revisiting Priorities: Improving MIR Evaluation Practices.,2016,https://doi.org/10.5281/zenodo.1416726,"Bob L. Sturm, Centre for Digital Music, Queen Mary University of London, GBR, education","While there is a consensus that evaluation practices in music informatics (MIR) must be improved, there is no consensus about what should be prioritised in order to do so. Priorities include: 1) improving data; 2) improving figures of merit; 3) employing formal statistical testing; 4) employing cross-validation; and/or 5) implementing transparent, central and immediate evaluation. In this position paper, I argue how these priorities treat only the symptoms of the problem and not its cause: MIR lacks a formal evaluation framework relevant to its aims. I argue that the principal priority is to adapt and integrate the formal design of experiments (DOE) into the MIR research pipeline. Since the aim of DOE is to help one produce the most reliable evidence at the least cost, it stands to reason that DOE will make a significant contribution to MIR. Accomplishing this, however, will not be easy, and will require far more effort than is currently being devoted to it."
67,Prem Seetharaman;Bryan Pardo,Simultaneous Separation and Segmentation in Layered Music.,2016,https://doi.org/10.5281/zenodo.1417987,"Prem Seetharaman, Northwestern University, USA, education;Bryan Pardo, Northwestern University, USA, education","In many pieces of music, the composer signals how individual sonic elements (samples, loops, the trumpet section) should be grouped by introducing sources or groups in a layered manner. We propose to discover and leverage the layering structure and use it for both structural segmentation and source separation. We use reconstruction error from non-negative matrix factorization (NMF) to guide structure discovery. Reconstruction error spikes at moments of significant sonic change. This guides segmentation and also lets us group basis sets for NMF. The number of sources, the types of sources, and when the sources are active are not known in advance. The only information is a specific type of layering structure. There is no separate training phase to learn a good basis set. No prior seeding of the NMF matrices is required. Unlike standard approaches to NMF there is no need for a post-processor to partition the learned basis functions by group. Source groups are learned automatically from the data. We evaluate our method on mixtures consisting of looping source groups. This separation approach outperforms a standard clustering NMF source separation approach on such mixtures. We find our segmentation approach is competitive with state-of-the-art segmentation methods on this dataset."
68,Patricio López-Serrano;Christian Dittmar;Jonathan Driedger;Meinard Müller,Towards Modeling and Decomposing Loop-Based Electronic Music.,2016,https://doi.org/10.5281/zenodo.1417999,"Patricio López-Serrano, International Audio Laboratories Erlangen, Germany, facility;Christian Dittmar, International Audio Laboratories Erlangen, Germany, facility;Jonathan Driedger, International Audio Laboratories Erlangen, Germany, facility;Meinard Müller, International Audio Laboratories Erlangen, Germany, facility","Electronic Music (EM) is a popular family of genres which has increasingly received attention as a research subject in the ﬁeld of MIR. A fundamental structural unit in EM are loops—audio fragments whose length can span several seconds. The devices commonly used to produce EM, such as sequencers and digital audio workstations, impose a musical structure in which loops are repeatedly triggered and overlaid. This particular structure allows new perspectives on well-known MIR tasks. In this paper we ﬁrst review a prototypical production technique for EM from which we derive a simpliﬁed model. We then use our model to illustrate approaches for the following task: given a set of loops that were used to produce a track, decompose the track by ﬁnding the points in time at which each loop was activated. To this end, we repurpose established MIR techniques such as ﬁngerprinting and non-negative matrix factor deconvolution."
69,Jorge Calvo-Zaragoza;David Rizo;José Manuel Iñesta Quereda,Two (Note) Heads Are Better Than One: Pen-Based Multimodal Interaction with Music Scores.,2016,https://doi.org/10.5281/zenodo.1417469,"Jorge Calvo-Zaragoza, University of Alicante, ESP, education;David Rizo, University of Alicante, ESP, education;Jose M. Iñesta, University of Alicante, ESP, education","Digitizing early music sources requires new ways of dealing with musical documents. Assuming that current technologies cannot guarantee a perfect automatic transcription, our intention is to develop an interactive system in which user and software collaborate to complete the task. Since conventional score post-editing might be tedious, the user is allowed to interact using an electronic pen. Although this provides a more ergonomic interface, this interaction must be decoded as well. In our framework, the user traces the symbols using the electronic pen over a digital surface, which provides both the underlying image (ofﬂine data) and the drawing made by the e-pen (online data) to improve classiﬁcation. Applying this methodology over 70 scores of the target musical archive, a dataset of 10 230 bimodal samples of 30 different symbols was obtained and made available for research purposes. This paper presents experimental results on classiﬁcation over this dataset, in which symbols are recognized by combining the two modalities. This combination of modes has demonstrated its good performance, decreasing the error rate of using each modality separately and achieving an almost error-free performance."
70,Christof Weiß;Vlora Arifi-Müller;Thomas Prätzlich;Rainer Kleinertz;Meinard Müller,Analyzing Measure Annotations for Western Classical Music Recordings.,2016,https://doi.org/10.5281/zenodo.1417449,"Christof Weiß, International Audio Laboratories Erlangen, Germany, facility;Vlora Arifi-Müller, International Audio Laboratories Erlangen, Germany, facility;Thomas Prätzlich, International Audio Laboratories Erlangen, Germany, facility;Rainer Kleinertz, Institut für Musikwissenschaft, Saarland University, Germany, education;Meinard Müller, International Audio Laboratories Erlangen, Germany, facility","This paper approaches the problem of annotating measure positions in Western classical music recordings. Such annotations can be useful for navigation, segmentation, and cross-version analysis of music in different types of representations. In a case study based on Wagner’s opera “Die Walk¨ure”, we analyze two types of annotations. First, we report on an experiment where several human listeners generated annotations in a manual fashion. Second, we examine computer-generated annotations which were obtained by using score-to-audio alignment techniques. As one main contribution of this paper, we discuss the inconsistencies of the different annotations and study possible musical reasons for deviations. As another contribution, we propose a kernel-based method for automatically estimating conﬁdences of the computed annotations which may serve as a ﬁrst step towards improving the quality of this automatic method."
71,David Lewis 0001;Tim Crawford;Daniel Müllensiefen,Instrumental Idiom in the 16th Century: Embellishment Patterns in Arrangements of Vocal Music.,2016,https://doi.org/10.5281/zenodo.1415964,"David Lewis, Goldsmiths, University of London, GBR, education;Tim Crawford, Goldsmiths, University of London, GBR, education;Daniel Müllensiefen, Goldsmiths, University of London, GBR, education","Much surviving 16th-century instrumental music consists of arrangements (‘intabulations’) of vocal music, in tablature for solo lute. Intabulating involved deciding what to omit from a score to fit the instrument, and making it fit under the hand. Notes were usually added as embellishments to the original plain score, using idiomatic patterns, typically at cadences, but often filling simple intervals in the vocal parts with faster notes.  
Here we test whether such patterns are both characteristic of lute intabulations as a class (vs original lute music) and of different genres within that class. We use patterns identified in the musicological literature to search two annotated corpora of encoded lute music using the SIA(M)ESE algorithm. Diatonic patterns occur in many chromatic forms, accidentals being added depending how the arranger applied the conventions of musica ficta. Rhythms must be applied at three different scales as notation is inconsistent across the repertory. This produced over 88,000 short melodic queries to search in two corpora totalling just over 6,000 encodings of lute pieces. We show that our method clearly discriminates between intabulations and original music for the lute (p < .001); it also can distinguish sacred and secular genres within the vocal models (p < .001)."
72,Andre Holzapfel;Emmanouil Benetos,The Sousta Corpus: Beat-Informed Automatic Transcription of Traditional Dance Tunes.,2016,https://doi.org/10.5281/zenodo.1416938,"Andre Holzapfel, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility;Emmanouil Benetos, Queen Mary University of London, GBR, education","In this paper, we present a new corpus for research in computational ethnomusicology and automatic music transcription, consisting of traditional dance tunes from Crete. This rich dataset includes audio recordings, scores transcribed by ethnomusicologists and aligned to the audio performances, and meter annotations. A second contribution of this paper is the creation of an automatic music transcription system able to support the detection of multiple pitches produced by lyra (a bowed string instrument). Furthermore, the transcription system is able to cope with deviations from standard tuning, and provides temporally quantized notes by combining the output of the multi-pitch detection stage with a state-of-the-art meter tracking algorithm. Experiments carried out for note tracking using 25ms onset tolerance reach 41.1% using information from the multi-pitch detection stage only, 54.6% when integrating beat information, and 57.9% when also supporting tuning estimation. The produced meter aligned transcriptions can be used to generate staff notation, a fact that increases the value of the system for studies in ethnomusicology."
73,Maria Panteli;Emmanouil Benetos;Simon Dixon,Learning a Feature Space for Similarity in World Music.,2016,https://doi.org/10.5281/zenodo.1415216,"Maria Panteli, Centre for Digital Music, Queen Mary University of London, GBR, education;Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London, GBR, education;Simon Dixon, Centre for Digital Music, Queen Mary University of London, GBR, education","In this study we investigate computational methods for assessing music similarity in world music. We use state-of-the-art audio features to describe musical content in world music recordings. Our music collection is a subset of the Smithsonian Folkways Recordings with audio examples from 31 countries from around the world. Using supervised and unsupervised dimensionality reduction techniques we learn feature representations for music similarity. We evaluate how well music styles separate in this learned space with a classification experiment. We obtained moderate performance classifying the recordings by country. Analysis of misclassifications revealed cases of geographical or cultural proximity. We further evaluate the learned space by detecting outliers, i.e. identifying recordings that stand out in the collection. We use a data mining technique based on Mahalanobis distances to detect outliers and perform a listening experiment in the ‘odd one out’ style to evaluate our findings. We are able to detect, amongst others, recordings of non-musical content as outliers as well as music with distinct timbral and harmonic content. The listening experiment reveals moderate agreement between subjects’ ratings and our outlier estimation."
74,Oriol Nieto;Juan Pablo Bello,Systematic Exploration of Computational Music Structure Research.,2016,https://doi.org/10.5281/zenodo.1417661,"Oriol Nieto, Pandora Media, Inc., USA, company;Juan Pablo Bello, Music and Audio Research Laboratory, New York University, USA, education","In this work we present a framework containing open source implementations of multiple music structural segmentation algorithms and employ it to explore the hyper parameters of features, algorithms, evaluation metrics, datasets, and annotations of this MIR task. Besides testing and discussing the relative importance of the moving parts of the computational music structure eco-system, we also shed light on its current major challenges. Additionally, a new dataset containing multiple structural annotations for tracks that are particularly ambiguous to analyze is introduced, and used to quantify the impact of specific annotators when assessing automatic approaches to this task. Results suggest that more than one annotation per track is necessary to fully address the problem of ambiguity in music structure research."
75,Jordan B. L. Smith;Masataka Goto,Using Priors to Improve Estimates of Music Structure.,2016,https://doi.org/10.5281/zenodo.1416916,"Jordan B. L. Smith, National Institute of Advanced Industrial Science and Technology (AIST), Japan, facility;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan, facility","Existing collections of annotations of musical structure possess many strong regularities: for example, the lengths of segments are approximately log-normally distributed, as is the number of segments per annotation; and the lengths of two adjacent segments are highly likely to have an integer ratio. Since many aspects of structural annotations are highly regular, but few of these regularities are taken into account by current algorithms, we propose several methods of improving predictions of musical structure by using their likelihood according to prior distributions. We test the use of priors to improve a committee of basic segmentation algorithms, and to improve a committee of cutting-edge approaches submitted to MIREX. In both cases, we are unable to improve on the best committee member, meaning that our proposed approach is outperformed by simple parameter tuning. The same negative result was found despite incorporating the priors in multiple ways. To explain the result, we show that although there is a correlation overall between output accuracy and prior likelihood, the weakness of the correlation in the high-likelihood region makes the proposed method infeasible. We suggest that to improve on the state of the art using prior likelihoods, these ought to be incorporated at a deeper level of the algorithm."
76,Mi Tian;Mark B. Sandler,Music Structural Segmentation Across Genres with Gammatone Features.,2016,https://doi.org/10.5281/zenodo.1417713,"Mi Tian, Queen Mary University of London, GBR, education;Mark B. Sandler, Queen Mary University of London, GBR, education","Music structural segmentation (MSS) studies to date mainly employ audio features describing the timbral, harmonic or rhythmic aspects of the music and are evaluated using datasets consisting primarily of Western music. A new dataset of Chinese traditional Jingju music with structural annotations is introduced in this paper to complement the existing evaluation framework. We discuss some statistics of the annotations analysing the inter-annotator agreements. We present two auditory features derived from the Gammatone filters based respectively on the cepstral analysis and the spectral contrast description. The Gammatone features and two commonly used features, Mel-frequency cepstral coefficients (MFCCs) and chromagram, are evaluated on the Jingju dataset as well as two existing used ones using several state-of-the-art algorithms. The investigated Gammatone features outperform MFCCs and chromagram when evaluated on the Jingju dataset and show similar performance with the Western datasets. We identify the presented Gammatone features as effective structure descriptors, especially for music lacking notable timbral or harmonic sectional variations. Results also indicate that the design of audio features and segmentation algorithms should be adapted to specific music genres to interpret individual structural patterns."
77,Juan J. Bosch;Rachel M. Bittner;Justin Salamon;Emilia Gómez,A Comparison of Melody Extraction Methods Based on Source-Filter Modelling.,2016,https://doi.org/10.5281/zenodo.1418167,"Juan J. Bosch, Universitat Pompeu Fabra, ESP, education;Rachel M. Bittner, New York University, USA, education;Justin Salamon, New York University, USA, education;Emilia Gómez, Universitat Pompeu Fabra, ESP, education","This work explores the use of source-ﬁlter models for pitch salience estimation and their combination with different pitch tracking and voicing estimation methods for automatic melody extraction. Source-ﬁlter models are used to create a mid-level representation of pitch that implicitly incorporates timbre information. The spectrogram of a musical audio signal is modelled as the sum of the leading voice (produced by human voice or pitched musical instruments) and accompaniment. The leading voice is then modelled with a Smoothed Instantaneous Mixture Model (SIMM) based on a source-ﬁlter model. The main advantage of such a pitch salience function is that it enhances the leading voice even without explicitly separating it from the rest of the signal. We show that this is beneﬁcial for melody extraction, increasing pitch estimation accuracy and reducing octave errors in comparison with simpler pitch salience functions. The adequate combination with voicing detection techniques based on pitch contour characterisation leads to signiﬁcant improvements over state-of-the-art methods, for both vocal and instrumental music."
78,Markus Schedl;Hamid Eghbal-Zadeh;Emilia Gómez;Marko Tkalcic,An Analysis of Agreement in Classical Music Perception and its Relationship to Listener Characteristics.,2016,https://doi.org/10.5281/zenodo.1417559,"Markus Schedl, Johannes Kepler University, AUT, education;Hamid Eghbal-Zadeh, Johannes Kepler University, AUT, education;Emilia Gómez, Universitat Pompeu Fabra, ESP, education;Marko Tkalčič, Free University of Bozen–Bolzano, ITA, education","We present a study, carried out on 241 participants, which investigates on classical music material the agreement of listeners on perceptual music aspects (related to emotion, tempo, complexity, and instrumentation) and the relationship between listener characteristics and these aspects. For the currently popular task of music emotion recognition, the former question is particularly important when defining a ground truth of emotions perceived in a given music collection. We characterize listeners via a range of factors, including demographics, musical inclination, experience, and education, and personality traits. Participants rate the music material under investigation, i.e., 15 expert-defined segments of Beethoven’s 3rd symphony, “Eroica”, in terms of 10 emotions, perceived tempo, complexity, and number of instrument groups. Our study indicates only slight agreement on most perceptual aspects, but significant correlations between several listener characteristics and perceptual qualities."
79,Tian Cheng 0001;Matthias Mauch;Emmanouil Benetos;Simon Dixon,An Attack/Decay Model for Piano Transcription.,2016,https://doi.org/10.5281/zenodo.1418153,"Tian Cheng, Centre for Digital Music, Queen Mary University of London, GBR, facility;Matthias Mauch, Centre for Digital Music, Queen Mary University of London, GBR, facility;Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London, GBR, facility;Simon Dixon, Centre for Digital Music, Queen Mary University of London, GBR, facility","We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activation that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcription is performed in a supervised way, with the training and test datasets produced by the same piano. First we train parameters for the attack and decay components on isolated notes, then update only the note activations for transcription. Experiments show that the proposed model achieves 82% on note-wise and 79% on frame-wise F-measures on the ‘ENSTDkCl’ subset of the MAPS database, outperforming the current published state of the art."
80,Carl Southall;Ryan Stables;Jason Hockman,Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks.,2016,https://doi.org/10.5281/zenodo.1416244,"Carl Southall, Birmingham City University, GBR, education;Ryan Stables, Birmingham City University, GBR, education;Jason Hockman, Birmingham City University, GBR, education","Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive instruments in audio recordings. Neural networks have already been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We propose the use of neural networks for ADT in order to exploit their ability to capture a complex configuration of features associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neural network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suitable for online operation. In both systems, a separate network is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilising the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respectively. The results demonstrate the effectiveness of the presented methods for solo percussion and a capacity for identifying snare drums, which are historically the most difficult drum class to detect."
81,R. Michael Winters;Siddharth Gururani;Alexander Lerch,"Automatic Practice Logging: Introduction, Dataset & Preliminary Study.",2016,https://doi.org/10.5281/zenodo.1416224,"R. Michael Winters, Georgia Tech Center for Music Technology (GTCMT), USA, facility;Siddharth Gururani, Georgia Tech Center for Music Technology (GTCMT), USA, facility;Alexander Lerch, Georgia Tech Center for Music Technology (GTCMT), USA, facility","Musicians spend countless hours practicing their instruments. To document and organize this time, musicians commonly use practice charts to log their practice. However, manual techniques require time, dedication, and experience to master, are prone to fallacy and omission, and ultimately can not describe the subtle variations in each repetition. This paper presents an alternative: by analyzing and classifying the audio recorded while practicing, logging could occur automatically, with levels of detail, accuracy, and ease that would not be possible otherwise. Towards this goal, we introduce the problem of Automatic Practice Logging (APL), including a discussion of the benefits and unique challenges it raises. We then describe a new dataset of over 600 annotated recordings of solo piano practice, which can be used to design and evaluate APL systems. After framing our approach to the problem, we present an algorithm designed to align short segments of practice audio with reference recordings using pitch chroma and dynamic time warping."
82,Kaustuv Kanti Ganguli;Sankalp Gulati;Xavier Serra;Preeti Rao,Data-Driven Exploration of Melodic Structure in Hindustani Music.,2016,https://doi.org/10.5281/zenodo.1416520,"Kaustuv Kanti Ganguli, Indian Institute of Technology Bombay, IND, education;Sankalp Gulati, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education;Preeti Rao, Indian Institute of Technology Bombay, IND, education","Indian art music is quintessentially an improvisatory music form in which the line between ‘ﬁxed’ and ‘free’ is extremely subtle. In a r¯aga performance, the melody is loosely constrained by the chosen composition but otherwise improvised in accordance with the r¯aga grammar. One of the melodic aspects that is governed by this grammar is the manner in which a melody evolves in time in the course of a performance. In this work, we aim to discover such implicit patterns or regularities present in the temporal evolution of vocal melodies of Hindustani music. We start by applying existing tools and techniques used in music information retrieval to a collection of concerts recordings of ¯al¯ap performances by renowned khayal vocal artists. We use svara-based and svara duration-based melodic features to study and quantify the manifestation of concepts such as v¯adi, samv¯adi, ny¯as and graha svara in the vocal performances. We show that the discovered patterns corroborate the musicological ﬁndings that describe the “unfolding” of a r¯aga in vocal performances of Hindustani music. The patterns discovered from the vocal melodies might help music students to learn improvisation and can complement the oral music pedagogy followed in this music tradition."
83,Vincent Lostanlen;Carmine-Emanuele Cella,Deep Convolutional Networks on the Pitch Spiral For Music Instrument Recognition.,2016,https://doi.org/10.5281/zenodo.1416928,"Vincent Lostanlen, École normale supérieure, PSL Research University, CNRS, Paris, France, education;Carmine-Emanuele Cella, École normale supérieure, PSL Research University, CNRS, Paris, France, education","Musical performance combines a wide range of pitches, nuances, and expressive techniques. Audio-based classiﬁcation of musical instruments thus requires to build signal representations that are invariant to such transformations. This article investigates the construction of learned convolutional architectures for instrument recognition, given a limited amount of annotated training data. In this context, we benchmark three different weight sharing strategies for deep convolutional networks in the time-frequency domain: temporal kernels; time-frequency kernels; and a linear combination of time-frequency kernels which are one octave apart, akin to a Shepard pitch spiral. We provide an acoustical interpretation of these strategies within the source-ﬁlter framework of quasi-harmonic sounds with a ﬁxed spectral envelope, which are archetypal of musical notes. The best classiﬁcation accuracy is obtained by hybridizing all three convolutional layers into a single deep learning architecture."
84,Bartlomiej Stasiak,DTV-Based Melody Cutting for DTW-Based Melody Search and Indexing in QbH Systems.,2016,https://doi.org/10.5281/zenodo.1415704,"Bartłomiej Stasiak, Lodz University of Technology, POL, education","Melody analysis is an important processing step in several areas of Music Information Retrieval (MIR). Processing the pitch values extracted from raw input audio signal may be computationally complex as it requires substantial effort to reduce the uncertainty resulting i.a. from tempo variability and transpositions. A typical example is the melody matching problem in Query-by-Humming (QbH) systems, where Dynamic Time Warping (DTW) and note-based approaches are typically applied. In this work we present a new, simple and efficient method of investigating the melody content which may be used for approximate, preliminary matching of melodies irrespective of their tempo and length. The proposed solution is based on Discrete Total Variation (DTV) of the melody pitch vector, which may be computed in linear time. We demonstrate its practical application for finding the appropriate melody cutting points in the R∗-tree-based DTW indexing framework. The experimental validation is based on a dataset of 4431 queries and over 4000 template melodies, constructed specially for testing Query-by-Humming systems."
85,John Fuller;Lauren Hubener;Yea-Seul Kim;Jin Ha Lee,Elucidating User Behavior in Music Services Through Persona and Gender.,2016,https://doi.org/10.5281/zenodo.1415928,"John Fuller, University of Washington, USA, education;Lauren Hubener, University of Washington, USA, education;Yea-Seul Kim, University of Washington, USA, education;Jin Ha Lee, University of Washington, USA, education","Prior user studies in the music information retrieval field have identified different personas representing the needs, goals, and characteristics of specific user groups for a user-centered design of music services. However, these personas were derived from a qualitative study involving a small number of participants and their generalizability has not been tested. The objectives of this study are to explore the applicability of seven user personas, developed in prior research, with a larger group of users and to identify the correlation between personas and the use of different types of music services. In total, 962 individuals were surveyed in order to understand their behaviors and preferences when interacting with music streaming services. Using a stratified sampling framework, key characteristics of each persona were extracted to classify users into specific persona groups. Responses were also analyzed in relation to gender, which yielded significant differences. Our findings support the development of more targeted approaches in music services rather than a universal service model."
86,Nazareno Andrade;Flavio Figueiredo,Exploring the Latent Structure of Collaborations in Music Recordings: A Case Study in Jazz.,2016,https://doi.org/10.5281/zenodo.1416176,"Nazareno Andrade, Universidade Federal de Campina Grande, BRA, education;Flavio Figueiredo, IBM Research - Brazil, BRA, company","Music records are largely a byproduct of collaborative efforts. Understanding how musicians collaborate to create records provides a step to understand the social production of music. This work leverages recent methods from trajectory mining to investigate how musicians have collaborated over time to record albums. Our case study analyzes data from the Discogs.com database from the Jazz domain. Our analysis examines how to explore the latent structure of collaboration between leading artists or bands and instrumentists over time. Moreover, we leverage the latent structure of our dataset to perform large-scale quantitative analyses of typical collaboration dynamics in different artist communities."
87,Phillip B. Kirlin,Global Properties of Expert and Algorithmic Hierarchical Music Analyses.,2016,https://doi.org/10.5281/zenodo.1416118,"Phillip B. Kirlin, Rhodes College, USA, education","In recent years, advances in machine learning and increases in data set sizes have produced a number of viable algorithms for analyzing music in a hierarchical fashion according to the guidelines of music theory. Many of these algorithms, however, are based on techniques that rely on a series of local decisions to construct a complete music analysis, resulting in analyses that are not guaranteed to resemble ground-truth analyses in their large-scale organizational shapes or structures. In this paper, we examine a number of hierarchical music analysis data sets — drawing from Schenkerian analysis and other analytical systems based on A Generative Theory of Tonal Music — to study three global properties calculated from the shapes of the analyses. The major finding presented in this work is that it is possible for an algorithm that only makes local decisions to produce analyses that resemble expert analyses with regards to the three global properties in question. We also illustrate specific similarities and differences in these properties across both ground-truth and algorithmically-produced analyses."
88,Liang Chen;Erik Stolterman;Christopher Raphael,Human-Interactive Optical Music Recognition.,2016,https://doi.org/10.5281/zenodo.1416184,"Liang Chen, Indiana University Bloomington, USA, education;Erik Stolterman, Indiana University Bloomington, USA, education;Christopher Raphael, Indiana University Bloomington, USA, education","""We propose a human-driven Optical Music Recognition (OMR) system that creates symbolic music data from common Western notation scores. Despite decades of development, OMR still remains largely unsolved as state-of-the-art automatic systems are unable to give reliable and useful results on a wide range of documents. For this reason our system, Ceres, combines human input and machine recognition to efﬁciently generate high-quality symbolic data. We propose a scheme for human-in-the-loop recognition allowing the user to constrain the recognition in two ways. The human actions allow the user to impose either a pixel labeling or model constraint, while the system re-recognizes subject to these constraints. We present evaluation based on different users’ log data using both Ceres and Sibelius software to produce the same music documents. We conclude that our system shows promise for transcrib-ing complicated music scores with high accuracy."""
89,Jack Atherton;Blair Kaneshiro,I Said it First: Topological Analysis of Lyrical Influence Networks.,2016,https://doi.org/10.5281/zenodo.1418047,"Jack Atherton, Center for Computer Research in Music and Acoustics, Stanford University, USA, facility;Blair Kaneshiro, Center for Computer Research in Music and Acoustics, Stanford University, USA, facility","""We present an analysis of musical inﬂuence using intact lyrics of over 550,000 songs, extending existing research on lyrics through a novel approach using directed net- works. We form networks of lyrical inﬂuence over time at the level of three-word phrases, weighted by tf-idf. An edge reduction analysis of strongly connected compo- nents suggests highly central artist, songwriter, and genre network topologies. Visualizations of the genre network based on multidimensional scaling conﬁrm network cen- trality and provide insight into the most inﬂuential genres at the heart of the network. Next, we present metrics for in- ﬂuence and self-referential behavior, examining their inter- actions with network centrality and with the genre diversity of songwriters. Here, we uncover a negative correlation between songwriters’ genre diversity and the robustness of their connections. By examining trends among the data for top genres, songwriters, and artists, we address questions related to clustering, inﬂuence, and isolation of nodes in the networks. We conclude by discussing promising future applications of lyrical inﬂuence networks in music infor- mation retrieval research. The networks constructed in this study are made publicly available for research purposes."""
90,Elad Liebman;Peter Stone;Corey N. White,Impact of Music on Decision Making in Quantitative Tasks.,2016,https://doi.org/10.5281/zenodo.1417743,"Elad Liebman, The University of Texas at Austin, USA, education;Peter Stone, The University of Texas at Austin, USA, education;Corey N. White, Syracuse University, USA, education","The goal of this study is to explore which aspects of
people’s analytical decision making are affected when ex-
posed to music. To this end, we apply a stochastic sequen-
tial model of simple decisions, the drift-diffusion model
(DDM), to understand risky decision behavior. Numerous
studies have demonstrated that mood can affect emotional
and cognitive processing, but the exact nature of the impact
music has on decision making in quantitative tasks has not
been sufﬁciently studied. In our experiment, participants
decided whether to accept or reject multiple bets with dif-
ferent risk vs. reward ratios while listening to music that
was chosen to induce positive or negative mood. Our re-
sults indicate that music indeed alters people’s behavior in
a surprising way - happy music made people make bet-
ter choices. In other words, it made people more likely
to accept good bets and reject bad bets. The DDM de-
composition indicated the effect focused primarily on both
the caution and the information processing aspects of de-
cision making. To further understand the correspondence
between auditory features and decision making, we stud-
ied how individual aspects of music affect response pat-
terns. Our results are particularly interesting when com-
pared with recent results regarding the impact of music
on emotional processing, as they illustrate that music af-
fects analytical decision making in a fundamentally differ-
ent way, hinting at a different psychological mechanism
that music impacts."
91,Simon Waloschek;Axel Berndt;Benjamin W. Bohl;Aristotelis Hadjakos,Interactive Scores in Classical Music Production.,2016,https://doi.org/10.5281/zenodo.1416710,"Simon Waloschek, University of Music Detmold, Germany, education;Axel Berndt, University of Music Detmold, Germany, education;Benjamin W. Bohl, University of Music Detmold, Germany, education;Aristotelis Hadjakos, University of Music Detmold, Germany, education","The recording of classical music is mostly centered around the score of a composition. During editing of these recordings, however, further technical visualizations are used. Introducing digital interactive scores to the recording and editing process can enhance the workflow significantly and speed up the production process. This paper gives a short introduction to the recording process and outlines possibilities that arise with interactive scores. Current related music information retrieval research is discussed, showing a potential path to score-based editing."
92,Helena Bantulà;Sergio I. Giraldo;Rafael Ramírez 0001,Jazz Ensemble Expressive Performance Modeling.,2016,https://doi.org/10.5281/zenodo.1415876,"Bantula, Helena, Universitat Pompeu Fabra, ESP, education;Giraldo, Sergio, Universitat Pompeu Fabra, ESP, education;Ramirez, Rafael, Universitat Pompeu Fabra, ESP, education","Computational expressive music performance studies the analysis and characterisation of the deviations that a musician introduces when performing a musical piece. It has been studied in a classical context where timing and dynamic deviations are modeled using machine learning techniques. In jazz music, work has been done previously on the study of ornament prediction in guitar performance, as well as in saxophone expressive modeling. However, little work has been done on expressive ensemble performance. In this work, we analysed the musical expressivity of jazz guitar and piano from two different perspectives: solo and ensemble performance. The aim of this paper is to study the influence of piano accompaniment into the performance of a guitar melody and vice versa. Based on a set of recordings made by professional musicians, we extracted descriptors from the score, we transcribed the guitar and the piano performances and calculated performance actions for both instruments. We applied machine learning techniques to train models for each performance action, taking into account both solo and ensemble descriptors. Finally, we compared the accuracy of the induced models. The accuracy of most models increased when ensemble information was considered, which can be explained by the interaction between musicians."
93,Daniel Shanahan;Kerstin Neubarth;Darrell Conklin,Mining Musical Traits of Social Functions in Native American Music.,2016,https://doi.org/10.5281/zenodo.1416408,"Daniel Shanahan, Louisiana State University, USA, education;Kerstin Neubarth, Canterbury Christ Church University, United Kingdom, education;Darrell Conklin, University of the Basque Country UPV/EHU, Spain, education, IKERBASQUE, Basque Foundation for Science, Spain, facility","Native American music is perhaps one of the most documented repertoires of indigenous folk music, being the subject of empirical ethnomusicological analyses for significant portions of the early 20th century. However, it has been largely neglected in more recent computational research, partly due to a lack of encoded data. In this paper we use the symbolic encoding of Frances Densmore’s collection of over 2000 songs, digitized between 1998 and 2014, to examine the relationship between internal musical features and social function. More specifically, this paper applies contrast data mining to discover global feature patterns that describe generalized social functions. Extracted patterns are discussed with reference to early ethnomusicological work and recent approaches to music, emotion, and ethology. A more general aim of this paper is to provide a methodology in which contrast data mining can be used to further examine the interactions between musical features and external factors such as social function, geography, language, and emotion."
94,Flavio Figueiredo;Bruno Ribeiro 0001;Christos Faloutsos;Nazareno Andrade;Jussara M. Almeida,Mining Online Music Listening Trajectories.,2016,https://doi.org/10.5281/zenodo.1417275,"Flavio Figueiredo, IBM Research, BRA, company;Bruno Ribeiro, Purdue University, USA, education;Christos Faloutsos, Carnegie Mellon University, USA, education;Nazareno Andrade, Universidade Federal de Campina Grande, BRA, education;Jussara M. Almeida, Universidade Federal de Minas Gerais, BRA, education","Understanding the listening habits of users is a valuable undertaking for musicology researchers, artists, consumers and online businesses alike. With the rise of Online Music Streaming Services (OMSSs), large amounts of user behavioral data can be exploited for this task. In this paper, we present SWIFT-FLOWS, an approach that models user listening habits in regards to how user attention transitions between artists. SWIFT-FLOWS combines recent advances in trajectory mining, coupled with modulated Markov models as a means to capture both how users switch attention from one artist to another, as well as how users fixate their attention in a single artist over short or large periods of time. We employ SWIFT-FLOWS on OMSSs datasets showing that it provides: (1) semantically meaningful representation of habits; (2) accurately models the attention span of users."
95,Tomoyasu Nakano;Daichi Mochihashi;Kazuyoshi Yoshii;Masataka Goto,Musical Typicality: How Many Similar Songs Exist?.,2016,https://doi.org/10.5281/zenodo.1417915,"Tomoyasu Nakano, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility;Daichi Mochihashi, The Institute of Statistical Mathematics, JPN, education;Kazuyoshi Yoshii, Kyoto University, JPN, education;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility","We propose a method for estimating the musical “typicality” of a song from an information theoretic perspective. While musical similarity compares just two songs, musical typicality quantifies how many of the songs in a set are similar. It can be used not only to express the uniqueness of a song but also to recommend one that is representative of a set. Building on the type theory in information theory (Cover & Thomas 2006), we use a Bayesian generative model of musical features and compute the typicality of a song as the sum of the probabilities of the songs that share the type of the given song. To evaluate estimated results, we focused on vocal timbre which can be evaluated quantitatively by using the singer’s gender. Estimated typicality is evaluated against the Pearson correlation coefficient between the computed typicality and the ratio of the number of male singers to female singers of a song-set. Our result shows that the proposed measure works more effectively to estimate musical typicality than the previous model based simply on generative probabilities."
96,Jeremy Hyrkas;Bill Howe,MusicDB: A Platform for Longitudinal Music Analytics.,2016,https://doi.org/10.5281/zenodo.1417381,"Jeremy Hyrkas, University of Washington, USA, education;Bill Howe, University of Washington, USA, education","""With public data sources such as Million Song dataset, researchers can now study longitudinal questions about the patterns of popular music, but the scale and complexity of the data complicate analysis. We propose MusicDB, a new approach for longitudinal music analytics that adapts techniques from relational databases to the music setting. By representing song timeseries data relationally, we aim to dramatically decrease the programming effort required for complex analytics while significantly improving scalability. We show how our platform can improve performance by reducing the amount of data accessed for many common analytics tasks, and how such tasks can be implemented quickly in relational languages — variants of SQL. We further show that expressing music analytics tasks over relational representations allows the system to automatically parallelize and optimize the resulting programs to improve performance. We evaluate our system by expressing complex analytics tasks including calculating song density and beat-aligning features and showing significant performance improvements over previous results. Finally, we evaluate expressiveness by reproducing the results from a recent analysis of longitudinal music trends using the Million Song dataset."""
97,Hamid Eghbal-zadeh;Gerhard Widmer,Noise Robust Music Artist Recognition Using I-Vector Features.,2016,https://doi.org/10.5281/zenodo.1417199,"Hamid Eghbal-zadeh, Johannes Kepler University of Linz, AUT, education;Gerhard Widmer, Johannes Kepler University of Linz, AUT, education","In music information retrieval (MIR), dealing with different types of noise is important and the MIR models are frequently used in noisy environments such as live performances. Recently, i-vector features have shown great promise for some major tasks in MIR, such as music similarity and artist recognition. In this paper, we introduce a novel noise-robust music artist recognition system using i-vector features. Our method uses a short sample of noise to learn the parameters of noise, then using a Maximum A Postriori (MAP) estimation it estimates clean i-vectors given noisy i-vectors. We examine the performance of multiple systems confronted with different kinds of additive noise in a clean training - noisy testing scenario. Using open-source tools, we have synthesized 12 different noisy versions from a standard 20-class music artist recognition dataset encountered with 4 different kinds of additive noise with 3 different Signal-to-Noise-Ratio (SNR). Using these datasets, we carried out music artist recognition experiments comparing the proposed method with the state-of-the-art. The results suggest that the proposed method outperforms the state-of-the-art."
98,Georgi Dzhambazov;Ajay Srinivasamurthy;Sertan Sentürk;Xavier Serra,On the Use of Note Onsets for Improved Lyrics-To-Audio Alignment in Turkish Makam Music.,2016,https://doi.org/10.5281/zenodo.1415988,"Georgi Dzhambazov, Universitat Pompeu Fabra, ESP, education;Ajay Srinivasamurthy, Universitat Pompeu Fabra, ESP, education;Sertan Şentürk, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Lyrics-to-audio alignment aims to automatically match given lyrics and musical audio. In this work we extend a state of the art approach for lyrics-to-audio alignment with information about note onsets. In particular, we consider the fact that transition to next lyrics syllable usually implies transition to a new musical note. To this end we formulate rules that guide the transition between consecutive phonemes when a note onset is present. These rules are incorporated into the transition matrix of a variable-time hidden Markov model (VTHMM) phonetic recognizer based on MFCCs. An estimated melodic contour is input to an automatic note transcription algorithm, from which the note onsets are derived. The proposed approach is evaluated on 12 a cappella audio recordings of Turkish Makam music using a phrase-level accuracy measure. Evaluation of the alignment is also presented on a polyphonic version of the dataset in order to assess how degradation in the extracted onsets affects performance. Results show that the proposed model outperforms a baseline approach unaware of onset transition rules. To the best of our knowledge, this is the one of the ﬁrst approaches tackling lyrics tracking, which combines timbral features with a melodic feature in the alignment process itself."
99,Raphaël Fournier-S'niehotta;Philippe Rigaux;Nicolas Travers,Querying XML Score Databases: XQuery is not Enough!.,2016,https://doi.org/10.5281/zenodo.1416976,"Raphaël Fournier-S’niehotta, CNAM, FRA, education;Philippe Rigaux, CNAM, FRA, education;Nicolas Travers, CNAM, FRA, education","""The paper addresses issues related to the design of query languages for searching and restructuring collections of XML-encoded music scores. We advocate against a direct approach based on XQuery, and propose a more powerful strategy that first extracts a structured representation of music notation from score encodings, and then manipulates this representation in closed form with dedicated operators. The paper exposes the content model, the resulting language, and describes our implementation on top of a large Digital Score Library (DSL)."""
100,Richard Vogl;Matthias Dorfer;Peter Knees,Recurrent Neural Networks for Drum Transcription.,2016,https://doi.org/10.5281/zenodo.1417613,"Richard Vogl, Johannes Kepler University Linz, AUT, education;Matthias Dorfer, Johannes Kepler University Linz, AUT, education;Peter Knees, Johannes Kepler University Linz, AUT, education","Music transcription is a core task in the ﬁeld of music information retrieval. Transcribing the drum tracks of music pieces is a well-deﬁned sub-task. The symbolic representation of a drum track contains much useful information about the piece, like meter, tempo, as well as various style and genre cues. This work introduces a novel approach for drum transcription using recurrent neural networks. We claim that recurrent neural networks can be trained to identify the onsets of percussive instruments based on general properties of their sound. Different architectures of recurrent neural networks are compared and evaluated using a well-known dataset. The outcomes are compared to results of a state-of-the-art approach on the same dataset. Furthermore, the ability of the networks to generalize is demonstrated using a second, independent dataset. The experiments yield promising results: while F-measures higher than state-of-the-art results are achieved, the networks are capable of generalizing reasonably well."
101,François Rigaud;Mathieu Radenen,Singing Voice Melody Transcription Using Deep Neural Networks.,2016,https://doi.org/10.5281/zenodo.1418051,"François Rigaud, Audionamix R&D, FRA, company;Mathieu Radenen, Audionamix R&D, FRA, company","This paper presents a system for the transcription of singing voice melodies in polyphonic music signals based on Deep Neural Network (DNN) models. In particular, a new DNN system is introduced for performing the f0 estimation of the melody, and another DNN, inspired from recent studies, is learned for segmenting vocal sequences. Preparation of the data and learning conﬁgurations related to the speciﬁcity of both tasks are described. The performance of the melody f0 estimation system is compared with a state-of-the-art method and exhibits highest accuracy through a better generalization on two different music databases. Insights into the global functioning of this DNN are proposed. Finally, an evaluation of the global system combining the two DNNs for singing voice melody transcription is presented."
102,Kai-Chun Hsu;Chih-Shan Lin;Tai-Shih Chi,Sparse Coding Based Music Genre Classification Using Spectro-Temporal Modulations.,2016,https://doi.org/10.5281/zenodo.1418099,"Kai-Chun Hsu, National Chiao Tung University, TWN, education;Chih-Shan Lin, National Chiao Tung University, TWN, education;Tai-Shih Chi, National Chiao Tung University, TWN, education","Spectro-temporal modulations (STMs) of the sound convey timbre and rhythm information so that they are intuitively useful for automatic music genre classification. The STMs are usually extracted from a time-frequency representation of the acoustic signal. In this paper, we investigate the efficacy of two kinds of STM features, the Gabor features and the rate-scale (RS) features, selectively extracted from various time-frequency representations, including the short-time Fourier transform (STFT) spectrogram, the constant-Q transform (CQT) spectrogram and the auditory (AUD) spectrogram, in recognizing the music genre. In our system, the dictionary learning and sparse coding techniques are adopted for training the support vector machine (SVM) classifier. Both spectral-type features and modulation-type features are used to test the system. Experiment results show that the RS features extracted from the log. magnituded CQT spectrogram produce the highest recognition rate in classifying the music genre."
103,Sankalp Gulati;Joan Serrà;Kaustuv Kanti Ganguli;Sertan Sentürk;Xavier Serra,Time-Delayed Melody Surfaces for Rāga Recognition.,2016,https://doi.org/10.5281/zenodo.1417905,"Sankalp Gulati, Universitat Pompeu Fabra, ESP, education;Joan Serr`a, Telefonica Research, ESP, company;Kaustuv K Ganguli, Indian Institute of Technology Bombay, IND, education;Sertan S¸ent¨urk, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","R¯aga is the melodic framework of Indian art music. It is a core concept used in composition, performance, organi- zation, and pedagogy. Automatic r¯aga recognition is thus a fundamental information retrieval task in Indian art music. In this paper, we propose the time-delayed melody surface (TDMS), a novel feature based on delay coordinates that captures the melodic outline of a r¯aga. A TDMS describes both the tonal and the temporal characteristics of a melody, using only an estimation of the predominant pitch. Considering a simple k-nearest neighbor classiﬁer, TDMSs out- perform the state-of-the-art for r¯aga recognition by a large margin. We obtain 98% accuracy on a Hindustani music dataset of 300 recordings and 30 r¯agas, and 87% accuracy on a Carnatic music dataset of 480 recordings and 40 r¯agas. TDMSs are simple to implement, fast to compute, and have a musically meaningful interpretation. Since the concepts and formulation behind the TDMS are generic and widely applicable, we envision its usage in other music traditions beyond Indian art music."
104,Andrea Cogliati;David Temperley;Zhiyao Duan,Transcribing Human Piano Performances into Music Notation.,2016,https://doi.org/10.5281/zenodo.1416466,"Andrea Cogliati, University of Rochester, USA, education;David Temperley, Eastman School of Music, University of Rochester, USA, education;Zhiyao Duan, University of Rochester, USA, education","Automatic music transcription aims to transcribe musical performances into music notation. However, existing transcription systems that have been described in research papers typically focus on multi-F0 estimation from audio and only output notes in absolute terms, showing frequency and absolute time (a piano-roll representation), but not in musical terms, with spelling distinctions (e.g., A♭ versus G♯) and quantized meter. To complete the transcription process, one would need to convert the piano-roll representation into a properly formatted and musically meaningful musical score. This process is non-trivial and largely unresearched. In this paper we present a system that generates music notation output from human-recorded MIDI performances of piano music. We show that the correct estimation of the meter, harmony and streams in a piano performance provides a solid foundation to produce a properly formatted score. In a blind evaluation by professional music theorists, the proposed method outperforms two commercial programs and an open source program in terms of pitch notation and rhythmic notation, and ties for the top in terms of overall voicing and staff placement."
105,Xiao Hu 0001;Kahyun Choi;Jin Ha Lee;Audrey Laplante;Yun Hao;Sally Jo Cunningham;J. Stephen Downie,WiMIR: An Informetric Study On Women Authors In ISMIR.,2016,https://doi.org/10.5281/zenodo.1414832,"Xiao Hu, University of Hong Kong, HKG, education;Kahyun Choi, University of Illinois, USA, education;Jin Ha Lee, University of Washington, USA, education;Audrey Laplante, Université de Montréal, CAN, education;Yun Hao, University of Illinois, USA, education;Sally Jo Cunningham, University of Waikato, NZL, education;J. Stephen Downie, University of Illinois, USA, education","The Music Information Retrieval (MIR) community is becoming increasingly aware of a gender imbalance evident in ISMIR participation and publication. This paper reports upon a comprehensive informetric study of the publication, authorship and citation characteristics of female researchers in the context of the ISMIR conferences. All 1,610 papers in the ISMIR proceedings written by 1,910 unique authors from 2000 to 2015 were collected and analyzed. Only 14.1% of all papers were led by female researchers. Temporal analysis shows that the percentage of lead female authors has not improved over the years, but more papers have appeared with female co-authors in very recent years. Topics and citation numbers are also analyzed and compared between female and male authors to identify research emphasis and to measure impact. The results show that the most prolific authors of both genders published similar numbers of ISMIR papers and the citation counts of lead authors in both genders had no significant difference. We also analyzed the collaboration patterns to discover whether gender is related to the number of collaborators. Implications of these findings are discussed and suggestions are proposed on how to continue encouraging and supporting female participation in the MIR field."
106,Ryan Groves,Automatic Melodic Reduction Using a Supervised Probabilistic Context-Free Grammar.,2016,https://doi.org/10.5281/zenodo.1416924,"Ryan Groves, ","This research explores a Natural Language Processing technique utilized for the automatic reduction of melodies: the Probabilistic Context-Free Grammar (PCFG). Automatic melodic reduction was previously explored by means of a probabilistic grammar [11] [1]. However, each of these methods used unsupervised learning to estimate the probabilities for the grammar rules, and thus a corpus-based evaluation was not performed. A dataset of analyses using the Generative Theory of Tonal Music (GTTM) exists [13], which contains 300 Western tonal melodies and their corresponding melodic reductions in tree format. In this work, supervised learning is used to train a PCFG for the task of melodic reduction, using the tree analyses provided by the GTTM dataset. The resulting model is evaluated on its ability to create accurate reduction trees, based on a node-by-node comparison with ground-truth trees. Multiple data representations are explored, and example output reductions are shown. Motivations for performing melodic reduction include melodic identification and similarity, efficient storage of melodies, automatic composition, variation matching, and automatic harmonic analysis."
107,Patrick Gray;Razvan C. Bunescu,A Neural Greedy Model for Voice Separation in Symbolic Music.,2016,https://doi.org/10.5281/zenodo.1417251,"Patrick Gray, Ohio University, USA, education;Razvan Bunescu, Ohio University, USA, education","Music is often experienced as a simultaneous progression of multiple streams of notes, or voices. The automatic separation of music into voices is complicated by the fact that music spans a voice-leading continuum ranging from monophonic, to homophonic, to polyphonic, often within the same work. We address this diversity by deﬁning voice separation as the task of partitioning music into streams that exhibit both a high degree of external perceptual sepa- ration from the other streams and a high degree of internal perceptual consistency, to the maximum degree that is pos- sible in the given musical input. Equipped with this task deﬁnition, we manually annotated a corpus of popular mu- sic and used it to train a neural network with one hidden layer that is connected to a diverse set of perceptually in- formed input features. The trained neural model greedily assigns notes to voices in a left to right traversal of the in- put chord sequence. When evaluated on the extraction of consecutive within voice note pairs, the model obtains over 91% F-measure, surpassing a strong baseline based on an iterative application of an envelope extraction function."
108,Matthias Dorfer;Andreas Arzt;Gerhard Widmer,Towards Score Following In Sheet Music Images.,2016,https://doi.org/10.5281/zenodo.1415548,"Matthias Dorfer, Johannes Kepler University Linz, AUT, education;Andreas Arzt, Johannes Kepler University Linz, AUT, education;Gerhard Widmer, Johannes Kepler University Linz, AUT, education","This paper addresses the matching of short music audio snippets to the corresponding pixel location in images of sheet music. A system is presented that simultaneously learns to read notes, listens to music and matches the currently played music to its corresponding notes in the sheet. It consists of an end-to-end multi-modal convolutional neural network that takes as input images of sheet music and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (covering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks – which have proven to be powerful image processing models – working with sheet music becomes feasible and a promising future research direction."
109,Colin Raffel;Daniel P. W. Ellis,Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.,2016,https://doi.org/10.5281/zenodo.1418233,"Colin Raffel, Columbia University, USA, education;Daniel P. W. Ellis, Columbia University, USA, education",MIDI ﬁles abound and provide a bounty of information for music informatics. We enumerate the types of information available in MIDI ﬁles and describe the steps necessary for utilizing them. We also quantify the reliability of this data by comparing it to human-annotated ground truth. The results suggest that developing better methods to leverage information present in MIDI ﬁles will facilitate the creation of MIDI-derived ground truth for audio content-based MIR.
110,Keunwoo Choi;György Fazekas;Mark B. Sandler,Automatic Tagging Using Deep Convolutional Neural Networks.,2016,https://doi.org/10.5281/zenodo.1416254,"Keunwoo Choi, Queen Mary University of London, GBR, education;György Fazekas, Queen Mary University of London, GBR, education;Mark Sandler, Queen Mary University of London, GBR, education","We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data."
111,Jun-qi Deng;Yu-Kwong Kwok,A Hybrid Gaussian-HMM-Deep Learning Approach for Automatic Chord Estimation with Very Large Vocabulary.,2016,https://doi.org/10.5281/zenodo.1415718,"Junqi Deng, The University of Hong Kong, HKG, education;Yu-Kwong Kwok, The University of Hong Kong, HKG, education","We propose a hybrid Gaussian-HMM-Deep-Learning approach for automatic chord estimation with very large chord vocabulary. The Gaussian-HMM part is similar to Chordino, which is used as a segmentation engine to divide input audio into note spectrogram segments. Two types of deep learning models are proposed to classify these segments into chord labels, which are then connected as chord sequences. Two sets of evaluations are conducted with two large chord vocabularies. The first evaluation is conducted in a recent MIREX standard way. Results show that our approach has obvious advantage over the state-of-the-art large-vocabulary-with-inversions supportable ACE system in terms of large vocabularies, although is outperformed by in small vocabularies. Through analyzing and deducing system behaviors behind the results, we see interesting chord confusion patterns made by different systems, which conceivably point to a demand of more balanced and consistent annotated datasets for training and testing. The second evaluation preliminarily demonstrates our approach’s superiority on a jazz chord vocabulary with 36 chord types, compared with a Chordino-like Gaussian-HMM baseline system with augmented vocabulary capacity."
112,Sangeun Kum;Changheun Oh;Juhan Nam,Melody Extraction on Vocal Segments Using Multi-Column Deep Neural Networks.,2016,https://doi.org/10.5281/zenodo.1414788,"Sangeun Kum, Korea Advanced Institute of Science and Technology, KOR, education;Changheun Oh, Korea Advanced Institute of Science and Technology, KOR, education;Juhan Nam, Korea Advanced Institute of Science and Technology, KOR, education","Singing melody extraction is a task that tracks pitch contour of singing voice in polyphonic music. While the majority of melody extraction algorithms are based on computing a saliency function of pitch candidates or separating the melody source from the mixture, data-driven approaches based on classification have been rarely explored."
