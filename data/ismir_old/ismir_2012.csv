Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Nick Collins,Influence in Early Electronic Dance Music: An Audio Content Analysis Investigation.,2012,https://doi.org/10.5281/zenodo.1417621,"Nick Collins, University of Sussex, GBR, education","Audio content analysis can assist investigation of musi-
cal inﬂuence, given a corpus of date-annotated works. We
study a number of techniques which illuminate musicolog-
ical questions on genre and creative inﬂuence. By applying
machine learning tests and statistical analysis to a database
of early EDM tracks, we examine how distinct putatively
different musical genres really are, the retrospectively la-
belled Detroit techno and Chicago house being the core
case study. Further, by building predictive models based on
works from earlier years, both by a priori assumed genre
groups and by individual tracks, we examine questions of
inﬂuence, and whether Detroit techno really is a sort of
electronic future funk, and Chicago house an electronic ex-
tension of disco. We discuss the implications and prospects
for modeling musical inﬂuence."
1,Kerstin Neubarth;Izaro Goienetxea;Colin Johnson;Darrell Conklin,Association Mining of Folk Music Genres and Toponyms.,2012,https://doi.org/10.5281/zenodo.1417643,"Kerstin Neubarth, Canterbury Christ Church University, GBR, education, School of Computing, University of Kent, GBR, education;Izaro Goienetxea, Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, ESP, education;Colin G. Johnson, School of Computing, University of Kent, GBR, education;Darrell Conklin, Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, ESP, education, IKERBASQUE, Basque Foundation for Science, ESP, facility","This paper demonstrates how association rule mining can be applied to discover relations between two ontologies of folk music: a genre and a region ontology. Genre–region associations have been widely studied in folk music research but have been neglected in music information retrieval. We present a method of association rule mining with constraints consisting of rule templates and rule evaluation measures to identify different, musicologically motivated, categories of genre–region associations. The method is applied to a corpus of 1902 Basque folk tunes, and several interesting rules and rule sets are discovered."
2,Tan Hakan Özaslan;Xavier Serra;Josep Lluís Arcos,Characterization of Embellishments in Ney Performances of Makam Music in Turkey.,2012,https://doi.org/10.5281/zenodo.1416008,,
3,Yi-Hsuan Yang;Xiao Hu 0001,Cross-cultural Music Mood Classification: A Comparison on English and Chinese Songs.,2012,https://doi.org/10.5281/zenodo.1416666,"Yi-Hsuan Yang, Academia Sinica, TWN, facility;Xiao Hu, University of Denver, USA, education","""Most existing studies on music mood classification have been focusing on Western music while little research has investigated whether mood categories, audio features, and classification models developed from Western music are applicable to non-Western music. This paper attempts to answer this question through a comparative study on English and Chinese songs. Specifically, a set of Chinese pop songs were annotated using an existing mood taxonomy developed for English songs. Six sets of audio features commonly used on Western music (e.g., timbre, rhythm) were extracted from both Chinese and English songs, and mood classification performances based on these feature sets were compared. In addition, experiments were conducted to test the generalizability of classification models across English and Chinese songs. Results of this study shed light on cross-cultural applicability of research results on music mood classification."""
4,Geoffroy Peeters;Karën Fort,Towards a (Better) Definition of the Description of Annotated MIR Corpora.,2012,https://doi.org/10.5281/zenodo.1417871,"Geoffroy Peeters, STMS IRCAM-CNRS-UPMC, FRA, facility;Kar¨en Fort, INIST-CNRS & Université Paris 13, Sorbonne Paris Cité, LIPN, FRA, education","Today, annotated MIR corpora are provided by various research labs or companies, each one using its own annotation methodology, concept definitions, and formats. This is not an issue as such. However, the lack of descriptions of the methodology used—how the corpus was actually annotated, and by whom—and of the annotated concepts, i.e. what is actually described, is a problem with respect to the sustainability, usability, and sharing of the corpora. Experience shows that it is essential to define precisely how annotations are supplied and described. We propose here a survey and consolidation report on the nature of the annotated corpora used and shared in MIR, with proposals for the axis against which corpora can be described so to enable effective comparison and the inherent influence this has on tasks performed using them."
5,Diane Watson;Regan L. Mandryk,Modeling Musical Mood From Audio Features and Listening Context on an In-Situ Data Set.,2012,https://doi.org/10.5281/zenodo.1415234,"Diane Watson, University of Saskatchewan, CAN, education;Regan L. Mandryk, University of Saskatchewan, CAN, education","Real-life listening experiences contain a wide range of music types and genres.  We create the first model of musical mood using a data set gathered in-situ during a user’s daily life. We show that while audio features, song lyrics and socially created tags can be used to successfully model musical mood with classification accuracies greater than chance, adding contextual information such as the listener’s affective state or listening context can improve classification accuracy. We successfully classify musical arousal with a classification accuracy of 67% and musical valence with an accuracy of 75% when using both musical features and listening context."
6,Eric Battenberg;David Wessel,Analyzing Drum Patterns Using Conditional Deep Belief Networks.,2012,https://doi.org/10.5281/zenodo.1417955,"Eric Battenberg, University of California, Berkeley, USA, education;David Wessel, University of California, Berkeley, USA, education","We present a system for the high-level analysis of beat-synchronous drum patterns to be used as part of a comprehensive rhythmic understanding system. We use a multi-layer neural network, which is greedily pre-trained layer-by-layer using restriced Boltzmann machines (RBMs), in order to model the contextual time-sequence information of a drum pattern. For the input layer of the network, we use a conditional RBM, which has been shown to be an effective generative model of multi-dimensional sequences. Subsequent layers of the neural network can be pre-trained as conditional or standard RBMs in order to learn higher-level rhythmic features. We show that this model can be fine-tuned in a discriminative manner to make accurate predictions about beat-measure alignment. The model generalizes well to multiple rhythmic styles due to the distributed state-space of the multi-layer neural network. In addition, the outputs of the discriminative network can serve as posterior probabilities over beat-alignment labels. These posterior probabilities can be used for Viterbi decoding in a hidden Markov model in order to maintain temporal continuity of the predicted information."
7,Erdem Ünal;Baris Bozkurt;Mustafa Kemal Karaosmanoglu,N-gram Based Statistical Makam Detection on Makam Music in Turkey Using Symbolic Data.,2012,https://doi.org/10.5281/zenodo.1417459,"Erdem Ünal, TÜBİTAK-BİLGEM, TUR, facility;Barış Bozkurt, Bahçeşehir University, TUR, education;M. Kemal Karaosmanoğlu, Yildiz Technical University, TUR, education","This work studies the effect of different score representations and the potential of n-grams in makam classification for traditional makam music in Turkey. While makams are defined with various characteristics including a distinct set of pitches, pitch hierarchy, melodic direction, typical phrases and typical makam transitions, such characteristics result in certain n-gram distributions which can be used for makam detection effectively. 13 popular makams, some of which are very similar to each other, are used in this study. Using the leave-one-out strategy, makam models are created statistically and tested against the left out music piece. Tests indicate that n-gram based statistical modeling and perplexity based similarity metric can be effectively used for makam detection. However the main dimension that cannot be captured is the overall progression which is the most unique feature for classification of close makams that uses the same scale notes as well as the same tonic."
8,Sebastian Böck;Florian Krebs;Markus Schedl,Evaluating the Online Capabilities of Onset Detection Methods.,2012,https://doi.org/10.5281/zenodo.1416036,"Sebastian Böck, Johannes Kepler University, AUT, education;Florian Krebs, Johannes Kepler University, AUT, education;Markus Schedl, Johannes Kepler University, AUT, education","In this paper, we evaluate various onset detection algorithms in terms of their online capabilities. Most methods use some kind of normalization over time, which renders them unusable for online tasks. We modified existing methods to enable online application and evaluated their performance on a large dataset consisting of 27,774 annotated onsets. We focus particularly on the incorporated preprocessing and peak detection methods. We show that, with the right choice of parameters, the maximum achievable performance is in the same range as that of offline algorithms, and that preprocessing can improve the results considerably. Furthermore, we propose a new onset detection method based on the common spectral flux and a new peak-picking method which outperforms traditional methods both online and offline and works with audio signals of various volume levels."
9,Peter Grosche;Joan Serrà;Meinard Müller;Josep Lluís Arcos,Structure-Based Audio Fingerprinting for Music Retrieval.,2012,https://doi.org/10.5281/zenodo.1416170,"Peter Grosche, Saarland University, DEU, education, MPI Informatik, DEU, facility;Joan Serr`a, Artificial Intelligence Research Institute (IIIA-CSIC), ESP, facility;Meinard Müller, Bonn University, DEU, education, MPI Informatik, DEU, facility;Josep Ll. Arcos, Artificial Intelligence Research Institute (IIIA-CSIC), ESP, facility","Content-based approaches to music retrieval are of great relevance as they do not require any kind of manually generated annotations. In this paper, we introduce the concept of structure fingerprints, which are compact descriptors of the musical structure of an audio recording. Given a recorded music performance, structure fingerprints facilitate the retrieval of other performances sharing the same underlying structure. Avoiding any explicit determination of musical structure, our fingerprints can be thought of as a probability density function derived from a self-similarity matrix. We show that the proposed fingerprints can be compared by using simple Euclidean distances without using any kind of complex warping operations required in previous approaches. Experiments on a collection of Chopin Mazurkas reveal that structure fingerprints facilitate robust and efficient content-based music retrieval. Furthermore, we give a musically informed discussion that also deepens the understanding of this popular Mazurka dataset."
10,Özgür Izmirli;Gyanendra Sharma,Bridging Printed Music and Audio Through Alignment Using a Mid-level Score Representation.,2012,https://doi.org/10.5281/zenodo.1414770,"Özgür İzmirli, Connecticut College, USA, education;Gyanendra Sharma, Connecticut College, USA, education","We present a system that utilizes a mid-level score representation for aligning printed music to its audio rendition. The mid-level representation is designed to capture an approximation to the musical events present in the printed score. It consists of a template based note detection frontend that seeks to detect notes without regard to musical duration, accidentals or the key signature. The presented method is designed for the commonly used grand staff and the approach is extendable to other types of scores. The image processing consists of page segmentation into lines followed by multiple stages that optimally orient the lines and establish a reference grid to be used in the note identification stage. Both the audio and the printed score are converted into compatible frequency representations. Alignment is performed using dynamic time warping with a specially designed distance measure. The insufficient pitch resolution due to the reductive nature of the mid-level representation is compensated by this pitch tolerant distance measure. Evaluation is carried out at the beat level using annotated scores and audio. The results demonstrate that the approach provides an efficient and practical alternative to methods that rely on symbolic MIDI-like information through OMR methods for alignment."
11,Pablo Sprechmann;Alexander M. Bronstein;Guillermo Sapiro,Real-time Online Singing Voice Separation from Monaural Recordings Using Robust Low-rank Modeling.,2012,https://doi.org/10.5281/zenodo.1414820,"Pablo Sprechmann, University of Minnesota, USA, education;Alex Bronstein, Tel Aviv University, ISR, education;Guillermo Sapiro, University of Minnesota, USA, education","Separating the leading vocals from the musical accompaniment is a challenging task that appears naturally in several music processing applications. Robust principal component analysis (RPCA) has been recently employed to this problem producing very successful results. The method decomposes the signal into a low-rank component corresponding to the accompaniment with its repetitive structure, and a sparse component corresponding to the voice with its quasi-harmonic structure. In this paper we first introduce a non-negative variant of RPCA, termed as robust low-rank non-negative matrix factorization (RNMF). This new framework better suits audio applications. We then propose two efficient feed-forward architectures that approximate the RPCA and RNMF with low latency and a fraction of the complexity of the original optimization method. These approximants allow incorporating elements of unsupervised, semi- and fully-supervised learning into the RPCA and RNMF frameworks. Our basic implementation shows several orders of magnitude speedup compared to the exact solvers with no performance degradation, and allows online and faster-than-real-time processing. Evaluation on the MIR-1K dataset demonstrates state-of-the-art performance."
12,Frederic Font;Joan Serrà;Xavier Serra,Folksonomy-based Tag Recommendation for Online Audio Clip Sharing.,2012,https://doi.org/10.5281/zenodo.1415700,"Frederic Font, Universitat Pompeu Fabra, ESP, education;Joan Serr`a, Artificial Intelligence Research Institute (IIIA-CSIC), ESP, facility;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Collaborative tagging has emerged as an efﬁcient way to semantically describe online resources shared by a community of users. However, tag descriptions present some drawbacks such as tag scarcity or concept inconsistencies. In these situations, tag recommendation strategies can help users in adding meaningful tags to the resources being described. Freesound is an online audio clip sharing site that uses collaborative tagging to describe a collection of more than 140,000 sound samples. In this paper we propose four algorithm variants for tag recommendation based on tag co-occurrence in the Freesound folksonomy. On the basis of removing a number of tags that have to be later predicted by the algorithms, we ﬁnd that using ranks instead of raw tag similarities produces statistically signiﬁcant improvements. Moreover, we show how speciﬁc strategies for selecting the appropriate number of tags to be recommended can signiﬁcantly improve algorithms’ performance. These two aspects provide insight into some of the most basic components of tag recommendation systems, and we plan to exploit them in future real-world deployments."
13,Kazuyoshi Yoshii;Masataka Goto,Infinite Composite Autoregressive Models for Music Signal Analysis.,2012,https://doi.org/10.5281/zenodo.1414992,"Kazuyoshi Yoshii, National Institute of Advanced Industrial Science and Technology (AIST), Japan, facility;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan, facility","This paper presents novel probabilistic models that can be used to estimate multiple fundamental frequencies (F0s) from polyphonic audio signals. These models are nonparametric Bayesian extensions of nonnegative matrix factorization (NMF) based on the source-ﬁlter paradigm, and in them an amplitude or power spectrogram is decomposed as the product of two kinds of spectral atoms (sources and ﬁlters) and time-varying gains of source-ﬁlter pairs. In this study we model musical instruments as autoregressive systems that combine two types of sources—periodic signals (comb-shaped densities) and white noise (ﬂat density)—with all-pole ﬁlters representing resonance characteristics. One of the main problems with such composite autoregressive models (CARMs) is that the numbers of sources and ﬁlters should be given in advance. To solve this problem, we propose nonparametric Bayesian models based on gamma processes and efﬁcient variational and multiplicative learning algorithms. These inﬁnite CARMs (iCARMs) can discover appropriate numbers of sources and ﬁlters in a data-driven manner. We report the experimental results of multipitch analysis on the MAPS piano database."
14,Timothée Gerber;Martin Dutasta;Laurent Girin;Cédric Févotte,Professionally-produced Music Separation Guided by Covers.,2012,https://doi.org/10.5281/zenodo.1417157,"Timothée Gerber, Grenoble-INP, GIPSA-lab, FRA, education;Martin Dutasta, Grenoble-INP, GIPSA-lab, FRA, education;Laurent Girin, Grenoble-INP, GIPSA-lab, FRA, education;Cédric Févotte, TELECOM ParisTech, CNRS LTCI, FRA, education","This paper addresses the problem of demixing professionally produced music, i.e., recovering the musical source signals that compose a (2-channel stereo) commercial mix signal. Inspired by previous studies using MIDI synthesized or hummed signals as external references, we propose to use the multitrack signals of a cover interpretation to guide the separation process with a relevant initialization. This process is carried out within the framework of the multichannel convolutive NMF model and associated EM/MU estimation algorithms. Although subject to the limitations of the convolutive assumption, our experiments confirm the potential of using multitrack cover signals for source separation of commercial music."
15,Daichi Sakaue;Takuma Otsuka;Katsutoshi Itoyama;Hiroshi G. Okuno,Bayesian Nonnegative Harmonic-Temporal Factorization and Its Application to Multipitch Analysis.,2012,https://doi.org/10.5281/zenodo.1418163,"Daichi Sakaue, Graduate School of Informatics, Kyoto University, JPN, education;Takuma Otsuka, Graduate School of Informatics, Kyoto University, JPN, education;Katsutoshi Itoyama, Graduate School of Informatics, Kyoto University, JPN, education;Hiroshi G. Okuno, Graduate School of Informatics, Kyoto University, JPN, education","Since important musical features are mutually dependent, their relations should be analyzed simultaneously. Their Bayesian analysis is particularly important to reveal their statistical relation. As the first step for a unified music content analyzer, we focus on the harmonic and temporal structures of the wavelet spectrogram obtained from harmonic sounds. In this paper, we present a new Bayesian multipitch analyzer, called Bayesian nonnegative harmonic-temporal factorization (BNHTF). BNHTF models the harmonic and temporal structures separately based on Gaussian mixture model. The input signal is assumed to contain a finite number of harmonic sounds. Each harmonic sound is assumed to emit a large number of sound quanta over the time-log-frequency domain. The observation probability is expressed as the product of two Gaussian mixtures. The number of quanta is calculated in the ϵ-neighborhood of each grid point on the spectrogram. BNHTF integrates latent harmonic allocation (LHA) and nonnegative matrix factorization (NMF) to estimate both the observation probability and the number of quanta. The model is optimized by newly designed deterministic procedures with several approximations for the variational Bayesian inference. Results of experiments on multipitch estimation with 40 musical pieces showed that BNHTF outperforms the conventional method by 0.018 in terms of F-measure on average."
16,Meinard Müller;Nanzhu Jiang,A Scape Plot Representation for Visualizing Repetitive Structures of Music Recordings.,2012,https://doi.org/10.5281/zenodo.1416866,"Meinard Müller, Bonn University, DEU, education, MPI Informatik, DEU, facility;Nanzhu Jiang, Saarland University, DEU, education, MPI Informatik, DEU, facility","The development of automated methods for revealing the repetitive structure of a given music recording is of central importance in music information retrieval. In this paper, we present a novel scape plot representation that allows for visualizing repetitive structures of the entire music recording in a hierarchical, compact, and intuitive way. In a scape plot, each point corresponds to an audio segment identified by its center and length. As our main contribution, we assign to each point a color value so that two segment properties become apparent. Firstly, we use the lightness component of the color to indicate the repetitiveness of the encoded segment, where we revert to a recently introduced fitness measure. Secondly, we use the hue component of the color to reveal the relations between different segments. To this end, we introduce a novel grouping procedure that automatically maps related segments to similar hue values. By discussing a number of popular and classical music examples, we illustrate the potential and visual appeal of our representation and also indicate limitations."
17,Daniel Wolff;Sebastian Stober;Andreas Nürnberger;Tillman Weyde,A Systematic Comparison of Music Similarity Adaptation Approaches.,2012,https://doi.org/10.5281/zenodo.1416600,"Daniel Wolff, City University London, UK, education;Tillman Weyde, City University London, UK, education;Sebastian Stober, Otto-von-Guericke-Universität Magdeburg, DE, education;Andreas Nürnberger, Otto-von-Guericke-Universität Magdeburg, DE, education","""In order to support individual user perspectives and different retrieval tasks, music similarity can no longer be considered as a static element of Music Information Retrieval (MIR) systems. Various approaches have been proposed recently that allow dynamic adaptation of music similarity measures. This paper provides a systematic comparison of algorithms for metric learning and higher-level facet distance weighting on the MagnaTagATune dataset. A cross-validation variant taking into account clip availability is presented. Applied on user generated similarity data, its effect on adaptation performance is analyzed. Special attention is paid to the amount of training data necessary for making similarity predictions on unknown data, the number of model parameters and the amount of information available about the music itself."""
18,Yizhao Ni;Matt McVicar;Raúl Santos-Rodriguez;Tijl De Bie,Using Hyper-genre Training to Explore Genre Information for Automatic Chord Estimation.,2012,https://doi.org/10.5281/zenodo.1415932,"Yizhao Ni, University of Bristol, GBR, education;Matt Mcvicar, University of Bristol, GBR, education;Raúl Santos-Rodríguez, University of Bristol, GBR, education;Tijl De Bie, University of Bristol, GBR, education","Recently a large amount of new chord annotations have been made available. This raises hopes for further development in automatic chord estimation. While more data seems to imply better performance, a major challenge however, is the wide variety of genres covered by these new data. As a result, the genre-independent training scheme as is common today is bound to fail. In this paper we investigate various options for exploring genre information for chord estimation, while also maximally exploiting the full dataset. More speciﬁcally, we propose a hyper-genre training scheme in which each genre cluster has its own parameters, tied together by hyper parameters as a Bayesian prior. The results are promising, showing signiﬁcant improvements over other prevailing training schemes."
19,Augustin Lefèvre;Francis R. Bach;Cédric Févotte,Semi-supervised NMF with Time-frequency Annotations for Single-channel Source Separation.,2012,https://doi.org/10.5281/zenodo.1415156,"Augustin Lef`evre, INRIA, FRA, facility;Francis Bach, INRIA, FRA, facility;C´edric F´evotte, LTCI/Telecom ParisTech, FRA, education","We formulate a novel extension of nonnegative matrix fac-
torization (NMF) to take into account partial information
on source-speciﬁc activity in the spectrogram. This infor-
mation comes in the form of masking coefﬁcients, such as
those found in an ideal binary mask. We show that state-of-
the-art results in source separation may be achieved with
only a limited amount of correct annotation, and further-
more our algorithm is robust to incorrect annotations. Since
in practice ideal annotations are not observed, we propose
several supervision scenarios to estimate the ideal mask-
ing coefﬁcients.
First, manual annotations by a trained
user on a dedicated graphical user interface are shown to
provide satisfactory performance although they are prone
to errors. Second, we investigate simple learning strate-
gies to predict the Wiener coefﬁcients based on local in-
formation around a given time-frequency bin of the spec-
trogram. Results on single-channel source separation show
that time-frequency annotations allow to disambiguate the
source separation problem, and learned annotations open
the way for a completely unsupervised learning procedure
for source separation with no human intervention."
20,Gregory Burlet;Alastair Porter;Andrew Hankinson;Ichiro Fujinaga,Neon.js: Neume Editor Online.,2012,https://doi.org/10.5281/zenodo.1418367,"Gregory Burlet, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, Québec, Canada, education;Alastair Porter, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, Québec, Canada, education;Andrew Hankinson, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, Québec, Canada, education;Ichiro Fujinaga, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, Québec, Canada, education","This paper introduces Neon.js, a browser-based music notation editor written in JavaScript. The editor can be used to manipulate digitally encoded musical scores in square-note notation. This type of notation presents certain challenges to a music notation editor, since many neumes (groups of pitches) are ligatures—continuous graphical symbols that represent multiple notes. Neon.js will serve as a component within an online optical music recognition framework. The primary purpose of the editor is to provide a readily accessible interface to easily correct errors made in the process of optical music recognition. In this context, we envision an environment that promotes crowdsourcing to further the creation of editable and searchable online symbolic music collections and for generating and editing ground-truth data to train optical music recognition algorithms."
21,Hélène Papadopoulos;George Tzanetakis,Modeling Chord and Key Structure with Markov Logic.,2012,https://doi.org/10.5281/zenodo.1416724,,
22,David Meredith 0001,A Geometric Language for Representing Structure in Polyphonic Music.,2012,https://doi.org/10.5281/zenodo.1414736,"David Meredith, Aalborg University, DNK, education","In 1981, Deutsch and Feroe proposed a formal language for representing melodic pitch structure that employed the powerful concept of hierarchically-related pitch alphabets. However, neither rhythmic structure nor pitch structure in polyphonic music can be adequately represented using this language. A new language is proposed here that incorporates certain features of Deutsch and Feroe’s model but extends and generalises it to allow for the representation of both rhythm and pitch structure in polyphonic music. The new language adopts a geometric approach in which a passage of polyphonic music is represented as a set of multi-dimensional points, generated by performing transformations on component patterns. The language introduces the concept of a periodic mask, a generalisation of Deutsch and Feroe’s notion of a pitch alphabet, that can be applied to any dimension of a geometric representation, allowing for both rhythms and pitch collections to be represented parsimoniously in a uniform way."
23,Jan Wülfing;Martin A. Riedmiller,Unsupervised Learning of Local Features for Music Classification.,2012,https://doi.org/10.5281/zenodo.1414782,"Jan W¨ulﬁng, University of Freiburg, DEU, education;Martin Riedmiller, University of Freiburg, DEU, education","""In this work we investigate the applicability of unsupervised feature learning methods to the task of automatic genre prediction of music pieces. More specifically we evaluate a framework that recently has been successfully used to recognize objects in images. We first extract local patches from the time-frequency transformed audio signal, which are then pre-processed and used for unsupervised learning of an overcomplete dictionary of local features. For learning we either use a bootstrapped k-means clustering approach or select features randomly. We further extract feature responses in a convolutional manner and train a linear SVM for classification. We extensively evaluate the approach on the GTZAN dataset, emphasizing the influence of important design choices such as dimensionality reduction, pooling and patch dimension on the classification accuracy. We show that convolutional extraction of local feature responses is crucial to reach high performance. Furthermore we find that using this approach, simple and fast learning techniques such as k-means or randomly selected features are competitive with previously published results which also learn features from audio signals."""
24,Yupeng Gu;Christopher Raphael,Modeling Piano Interpretation Using Switching Kalman Filter.,2012,https://doi.org/10.5281/zenodo.1415028,"Yupeng Gu, Indiana University, USA, education;Christopher Raphael, Indiana University, USA, education","An approach of parsing piano music interpretation is presented. We focus mainly on quantifying expressive timing activities. A small number of different expressive timing behaviors (constant, slowing down, speeding up, accent) are deﬁned in order to explain the tempo discretely. Given a MIDI performance of a piano music, we simultaneously estimate both discrete variables that corresponds to the behaviors and continuous variables that describe tempo. A graphical model is introduced to represent the evolution of the discrete behaviors and tempo progression. We demonstrate a computational method that acquires the approximate most likely conﬁguration of the discrete behaviors and the hidden continuous variable tempo. This conﬁguration represent a “smoothed” version of the performance which greatly reduces parametrization while retaining most of its musicality. Experiments are presented on several MIDI piano music performed on a digital piano. An user study is performed to evaluate our method."
25,Rong Jin;Christopher Raphael,Interpreting Rhythm in Optical Music Recognition.,2012,https://doi.org/10.5281/zenodo.1415848,"Rong Jin, School of Informatics and Computing, Indiana University, Bloomington, USA, education;Christopher Raphael, School of Informatics and Computing, Indiana University, Bloomington, USA, education","We present a method for understanding the rhythmic content of a collection of identified symbols in optical music recognition, designed for polyphonic music. Our object of study is a measure of music symbols. Our model explains the symbols as a collection of voices, while the number of voices is variable throughout a measure. We introduce a dynamic programming framework that identifies the best-scoring interpretation subject to the constraint that each voice accounts for the musical time indicated by the known time signature. Our approach applies as well to the situation in which their are multiple possible hypotheses for each symbol, and thus combines interpretation with recognition in a top-down manner. We present experiments demonstrating a nearly 4-fold decrease in the number of false positive symbols with monophonic music, identify missing tuplets, and show preliminary results with polyphonic music."
26,José R. Zapata;Andre Holzapfel;Matthew E. P. Davies;João Lobato Oliveira;Fabien Gouyon,Assigning a Confidence Threshold on Automatic Beat Annotation in Large Datasets.,2012,https://doi.org/10.5281/zenodo.1415080,"José R. Zapata, Universitat Pompeu Fabra, ESP, education;André Holzapfel, Universitat Pompeu Fabra, ESP, education;Matthew E.P. Davies, INESC TEC, PRT, facility;João L.Oliveira, University of Porto, PRT, education;Fabien Gouyon, INESC TEC, PRT, facility","In this paper we establish a threshold for perceptually acceptable beat tracking based on the mutual agreement of a committee of beat trackers. In the first step we use an existing annotated dataset to show that mutual agreement can be used to select one committee member as the most reliable beat tracker for a song. Then we conduct a listening test using a subset of the Million Song Dataset to establish a threshold which results in acceptable quality of the chosen beat output. For both datasets, we obtain a percentage of trackable music of about 73%, and we investigate which data tags are related to acceptable and problematic beat tracking. The results indicate that current datasets are biased towards genres which tend to be easy for beat tracking. The proposed methods provide a means to automatically obtain a confidence value for beat tracking in non-annotated data and to choose between a number of beat tracker outputs."
27,Matthias Mauch;Simon Dixon,A Corpus-based Study of Rhythm Patterns.,2012,https://doi.org/10.5281/zenodo.1414848,"Matthias Mauch, Centre for Digital Music, Queen Mary University of London, GBR, facility;Simon Dixon, Centre for Digital Music, Queen Mary University of London, GBR, facility","We present a corpus-based study of musical rhythm, based
on a collection of 4.8 million bar-length drum patterns
extracted from 48,176 pieces of symbolic music.
Ap-
proaches to the analysis of rhythm in music information
retrieval to date have focussed on low-level features for re-
trieval or on the detection of tempo, beats and drums in
audio recordings. Musicological approaches are usually
concerned with the description or implementation of man-
made music theories. In this paper, we present a quantita-
tive bottom-up approach to the study of rhythm that relies
upon well-understood statistical methods from natural lan-
guage processing. We adapt these methods to our corpus of
music, based on the realisation that—unlike words—bar-
length drum patterns can be systematically decomposed
into sub-patterns both in time and by instrument. We show
that, in some respects, our rhythm corpus behaves like nat-
ural language corpora, particularly in the sparsity of vo-
cabulary. The same methods that detect word collocations
allow us to quantify and rank idiomatic combinations of
drum patterns. In other respects, our corpus has proper-
ties absent from language corpora, in particular, the high
amount of repetition and strong mutual information rates
between drum instruments. Our ﬁndings may be of direct
interest to musicians and musicologists, and can inform the
design of ground truth corpora and computational models
of musical rhythm."
28,Jason Hockman;Matthew E. P. Davies;Ichiro Fujinaga,"One in the Jungle: Downbeat Detection in Hardcore, Jungle, and Drum and Bass.",2012,https://doi.org/10.5281/zenodo.1417054,"Jason A. Hockman, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Canada, facility, Distributed Digital Archives and Libraries (DDMAL), McGill University, Canada, education;Matthew E.P. Davies, Sound and Music Computing Group, INESC TEC, Portugal, facility;Ichiro Fujinaga, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Canada, facility, Distributed Digital Archives and Libraries (DDMAL), McGill University, Canada, education","Hardcore, jungle, and drum and bass (HJDB) are fast-paced electronic dance music genres that often employ resequenced breakbeats or drum samples from jazz and funk percussionist solos. We present a style-speciﬁc method for downbeat detection speciﬁcally designed for HJDB. The presented method combines three forms of metrical information in the prediction of downbeats: low-level onset event information; periodicity information from beat tracking; and high-level information from a regression model trained with classic breakbeats. In an evaluation using 206 HJDB pieces, we demonstrate superior accuracy of our style speciﬁc method over four general downbeat detection algorithms. We present this result to motivate the need for style-speciﬁc knowledge and techniques for improved downbeat detection."
29,Arthur Flexer;Dominik Schnitzer;Jan Schlueter,A MIREX Meta-analysis of Hubness in Audio Music Similarity.,2012,https://doi.org/10.5281/zenodo.1417865,"Arthur Flexer, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility;Dominik Schnitzer, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility;Jan Schlüter, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility","We use results from the 2011 MIREX “Audio Music Similarity and Retrieval” task for a meta analysis of the hub phenomenon. Hub songs appear similar to an undesirably high number of other songs due to a problem of measuring distances in high dimensional spaces. Comparing 17 algorithms we are able to confirm that different algorithms produce very different degrees of hubness. We also show that hub songs exhibit less perceptual similarity to the songs they are close to, according to an audio similarity function, than non-hub songs. Application of the recently introduced method of “mutual proximity” is able to decisively improve this situation."
30,Julián Urbano;J. Stephen Downie;Brian McFee;Markus Schedl,How Significant is Statistically Significant? The case of Audio Music Similarity and Retrieval.,2012,https://doi.org/10.5281/zenodo.1418055,"Julián Urbano, University Carlos III of Madrid, ESP, education;Brian McFee, University of California at San Diego, USA, education;J. Stephen Downie, University of Illinois at Urbana-Champaign, USA, education;Markus Schedl, Johannes Kepler University Linz, AUT, education","The principal goal of the annual Music Information Re-
trieval Evaluation eXchange (MIREX) experiments is to
determine which systems perform well and which systems
perform poorly on a range of MIR tasks. However, there
has been no systematic analysis regarding how well these
evaluation results translate into real-world user satisfac-
tion. For most researchers, reaching statistical signiﬁcance
in the evaluation results is usually the most important goal,
but in this paper we show that indicators of statistical sig-
niﬁcance (i.e., small p-value) are eventually of secondary
importance.
Researchers who want to predict the real-
world implications of formal evaluations should properly
report upon practical signiﬁcance (i.e., large effect-size).
Using data from the 18 systems submitted to the MIREX
2011 Audio Music Similarity and Retrieval task, we ran
an experiment with 100 real-world users that allows us to
explicitly map system performance onto user satisfaction.
Based upon 2,200 judgments, the results show that abso-
lute system performance needs to be quite large for users
to be satisﬁed, and differences between systems have to be
very large for users to actually prefer the supposedly better
system. The results also suggest a practical upper bound of
80% on user satisfaction with the current deﬁnition of the
task. Reﬂecting upon these ﬁndings, we make some rec-
ommendations for future evaluation experiments and the
reporting and interpretation of results in peer-reviewing."
31,Justin Salamon;Geoffroy Peeters;Axel Röbel,Statistical Characterisation of Melodic Pitch Contours and its Application for Melody Extraction.,2012,https://doi.org/10.5281/zenodo.1416462,,
32,Joe Cheri Ross;Vinutha T. P.;Preeti Rao,Detecting Melodic Motifs from Audio for Hindustani Classical Music.,2012,https://doi.org/10.5281/zenodo.1417587,"Joe Cheri Ross, Indian Institute of Technology Bombay, IND, education;Vinutha T. P., Indian Institute of Technology Bombay, IND, education;Preeti Rao, Indian Institute of Technology Bombay, IND, education","Melodic motifs form essential building blocks in Indian 
Classical music. The motifs, or key phrases, provide 
strong cues to the identity of the underlying raga in both 
Hindustani and Carnatic styles of Indian music.  Thus the 
automatic detection of such recurring basic melodic 
shapes from audio is of relevance in music information 
retrieval. The extraction of melodic attributes from poly-
phonic audio and the variability inherent in the perfor-
mance, which does not follow a predefined score, make 
the task particularly challenging. In this work, we consid-
er the segmentation of selected melodic motifs from au-
dio signals by computing similarity measures on time se-
ries of automatically detected pitch values. The methods 
are investigated in the context of detecting the signature 
phrase of Hindustani vocal music compositions (bandish) 
within and across performances.  "
33,Gopala K. Koduri;Joan Serrà;Xavier Serra,Characterization of Intonation in Carnatic Music by Parametrizing Pitch Histograms.,2012,https://doi.org/10.5281/zenodo.1416902,"Gopala K. Koduri, Universitat Pompeu Fabra, ESP, education;Joan Serr`a, Artificial Intelligence Research Institute (IIIA-CSIC), ESP, facility;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Intonation is an important concept in Carnatic music that is characteristic of a raaga, and intrinsic to the musical expression of a performer. In this paper we approach the description of intonation from a computational perspective, obtaining a compact representation of the pitch track of a recording. First, we extract pitch contours from automatically selected voice segments. Then, we obtain a a pitch histogram of its full pitch-range, normalized by the tonic frequency, from which each prominent peak is automatically labelled and parametrized. We validate such parametrization by considering an explorative classification task: three raagas are disambiguated using the characterization of a single peak (a task that would seriously challenge a more naïve parametrization). Results show consistent improvements for this particular task. Furthermore, we perform a qualitative assessment on a larger collection of raagas, showing the discriminative power of the entire representation. The proposed generic parametrization of the intonation histogram should be useful for musically relevant tasks such as performer and instrument characterization."
34,Nicolas Boulanger-Lewandowski;Yoshua Bengio;Pascal Vincent,Discriminative Non-negative Matrix Factorization for Multiple Pitch Estimation.,2012,https://doi.org/10.5281/zenodo.1417435,"Nicolas Boulanger-Lewandowski, Université de Montréal, CAN, education;Yoshua Bengio, Université de Montréal, CAN, education;Pascal Vincent, Université de Montréal, CAN, education","In this paper, we present a supervised method to improve the multiple pitch estimation accuracy of the non-negative matrix factorization (NMF) algorithm. The idea is to extend the sparse NMF framework by incorporating pitch information present in time-aligned musical scores in order to extract features that enforce the separability between pitch labels. We introduce two discriminative criteria that maximize inter-class scatter and quantify the predictive potential of a given decomposition using logistic regressors. Those criteria are applied to both the latent variable and the deterministic autoencoder views of NMF, and we devise efficient update rules for each. We evaluate our method on three polyphonic datasets of piano recordings and orchestral instrument mixes. Both models greatly enhance the quality of the basis spectra learned by NMF and the accuracy of multiple pitch estimation."
35,Wolfgang Fohl;Andreas Meisel;Ivan Turkalj,A Feature Relevance Study for Guitar Tone Classification.,2012,https://doi.org/10.5281/zenodo.1414750,"Wolfgang Fohl, HAW Hamburg University of Applied Sciences, DEU, education;Ivan Turkalj, HAW Hamburg University of Applied Sciences, DEU, education;Andreas Meisel, HAW Hamburg University of Applied Sciences, DEU, education","A series of experiments on the automatic classiﬁcation of classical guitar sounds with support vector machines has been carried out to investigate the relevance of the features and to minimise the feature set for successful classiﬁcation. Features used for classiﬁcation were the time series of the partial tone amplitudes, and of the MFCCs, and the energy distribution of the nontonal percussive sound that is produced in the attack phase of the tone. Furthermore the inﬂuence of sound parameters as timbre, player, fret position and string number on the recognition rate is investigated. Finally, several nonlinear kernels are compared in their classiﬁcation performance. It turns out, that a selection of 505 features out of the full feature set of 1155 elements does only reduce the recognition rate of a linear SVM from 82% to 78%. With the use of a polynomial instead of a linear kernel the recognition rate with the reduced feature set can even be increased to 84%."
36,Ruben Hillewaere;Bernard Manderick;Darrell Conklin,String Methods for Folk Tune Genre Classification.,2012,https://doi.org/10.5281/zenodo.1416690,"Ruben Hillewaere, Vrije Universiteit Brussel, BEL, education;Bernard Manderick, Vrije Universiteit Brussel, BEL, education;Darrell Conklin, Universidad del País Vasco UPV/EHU, ESP, education, IKERBASQUE, Basque Foundation for Science, ESP, facility","In folk song research, string methods have been widely used to retrieve highly similar tunes or to perform tune family classiﬁcation. In this study, we investigate how various string methods perform on a fundamentally different classiﬁcation task, which is to classify folk tunes into genres, the genres being the dance types of the tunes. A new data set Dance-9 is therefore introduced. The different string method classiﬁcation accuracies are compared with each other and also with n-gram models and global feature models which have been proven to be useful in previous folk song research. They are shown to yield similar results to the global feature models, but are outperformed by the n-gram models."
37,Mustafa Kemal Karaosmanoglu,A Turkish Makam Music Symbolic Database for Music Information Retrieval: SymbTr.,2012,https://doi.org/10.5281/zenodo.1416722,"M. Kemal Karaosmanoğlu, Yıldız Technical University, TUR, education","""Turkish makam music needs a comprehensive database for public consumption, to be used in MIR. This article introduces SymbTr, a Turkish Makam Music Symbolic Representation Database, aimed at filling this void. SymbTr consists of musical information in text, PDF, and MIDI formats. Raw data, drawn from reliable sources, and consisting of 1,700 musical pieces in Turkish art and folk music was processed featuring distinct examples in 155 diverse makams, 100 usuls and 48 forms. Special care was devoted to selection of works that scatter across a broad historical time span and were among those still performed today. Total number of musical notes in these pieces was 630,000, corresponding to a nominal playback time of 72 hours. Synthesized sounds particular to Turkish makam music were used in MIDI playback, and transcription/playback errors were corrected by input from experts. Symbolic representation data, open to the public, is output from a computer program developed exclusively for Turkish makam music. SymbTr was designed as a wholesome representation of aforementioned distinct auditory and visual features that distinguish Turkish makam music from other music genres. This article explains the database format in detail, and also provides, through examples, statistical information on pitch/interval allocation and distribution."""
38,Yoko Anan;Kohei Hatano;Hideo Bannai;Masayuki Takeda;Ken Satoh,Polyphonic Music Classification on Symbolic Data Using Dissimilarity Functions.,2012,https://doi.org/10.5281/zenodo.1415672,"Yoko Anan, Kyushu University, JPN, education;Kohei Hatano, Kyushu University, JPN, education;Hideo Bannai, Kyushu University, JPN, education;Masayuki Takeda, Kyushu University, JPN, education;Ken Satoh, National Institute of Informatics, JPN, facility","This paper addresses the polyphonic music classification problem on symbolic data. A new method is proposed which converts music pieces into binary chroma vector sequences and then classifies them by applying the dissimilarity-based classification method TWIST proposed in our previous work. One advantage of using TWIST is that it works with any dissimilarity measure. Computational experiments show that the proposed method drastically outperforms SVM and k-NN, the state-of-the-art classification methods."
39,Frédéric Bimbot;Emmanuel Deruty;Gabriel Sargent;Emmanuel Vincent,"Semiotic Structure Labeling of Music Pieces: Concepts, Methods and Annotation Conventions.",2012,https://doi.org/10.5281/zenodo.1416412,"Frédéric BIMBOT, IRISA / METISS, CNRS - UMR 6074, France, facility;Emmanuel DERUTY, INRIA / METISS, Rennes, France, facility;Gabriel SARGENT, IRISA / METISS, Université Rennes 1, France, education;Emmanuel VINCENT, INRIA / METISS, Rennes, France, facility","Music structure description, i.e. the task of representing the high-level organization of music pieces in a concise, generic and reproducible way, is currently a scientific challenge both algorithmically and conceptually. In this paper, we focus on semiotic structure, i.e. the description of similarities and internal relationships within a music piece, as a low-rate stream of arbitrary symbols from a limited alphabet and we address methodological questions related to annotation. We formulate the labeling task as a blind demodulation problem, whose goal is to identify a minimal set of semiotic codewords, whose realizations within the music piece are subject to a number of connotative variations viewed as modulations. The determination of labels is achieved by combining morphological, paradigmatic and syntagmatic considerations relying respectively on (i) a morphological model of semiotic blocks in order to define their individual properties, (ii) the support of prototypical structural patterns to guide the comparison between blocks and (iii) a methodology for the determination of distinctive features across semiotic classes. Specific notations are introduced to account for unresolvable semiotic ambiguities, which are occasional but must be considered as inherent to the music matter itself. A set of 500 music pieces labeled in accordance with the proposed concepts and annotation conventions is being released with this article."
40,Thierry Bertin-Mahieux;Daniel P. W. Ellis,Large-Scale Cover Song Recognition Using the 2D Fourier Transform Magnitude.,2012,https://doi.org/10.5281/zenodo.1414956,"Thierry Bertin-Mahieux, Columbia University, USA, education;Daniel P.W. Ellis, Columbia University, USA, education","Large-scale cover song recognition involves calculating item-to-item similarities that can accommodate differences in timing and tempo, rendering simple Euclidean measures unsuitable. Expensive solutions such as dynamic time warping do not scale to million of instances, making them inappropriate for commercial-scale applications. In this work, we transform a beat-synchronous chroma matrix with a 2D Fourier transform and show that the resulting representation has properties that ﬁt the cover song recognition task. We can also apply PCA to efﬁciently scale comparisons. We report the best results to date on the largest available dataset of around 18,000 cover songs amid one million tracks, giving a mean average precision of 3.0%."
41,Ching-Hua Chuan;Elaine Chew,Creating Ground Truth for Audio Key Finding: When the Title Key May Not Be the Key.,2012,https://doi.org/10.5281/zenodo.1414972,"Ching-Hua Chuan, University of North Florida, USA, education;Elaine Chew, Queen Mary, University of London, GBR, education","""In this paper, we present an effective and efficient way to create an accurately labeled dataset to advance audio key finding research. The MIREX audio key finding contest has been held twice using classical compositions for which the key is designated in the title. The problem with this accepted practice is that the title key may not be the perceived key in the audio excerpt. To reduce manual annotation, which is costly, we use a confusion index generated by existing audio key finding algorithms to determine if an audio excerpt requires manual annotation. We collected 3224 excerpts and identified 727 excerpts requiring manual annotation. We evaluate the algorithms’ performance on these challenging cases using the title keys, and the re-labeled keys. The musicians who aurally identify the key also provide comments on the reasons for their choice. The relabeling process reveals the mismatch between title and perceived keys to be caused by tuning practices (in 471 of the 727 excerpts, 64.79%), and other factors (188 excerpts, 25.86%) including key modulation and intonation choices. The remaining 68 challenging cases provide useful information for algorithm design."""
42,Jin Ha Lee;Nichole Maiman Waterman,Understanding User Requirements for Music Information Services.,2012,https://doi.org/10.5281/zenodo.1417625,"Jin Ha Lee, University of Washington, USA, education;Nichole Maiman Waterman, University of Washington, USA, education","User studies in the music information retrieval and music digital library fields have been gradually increasing in recent years, but large-scale studies that can help detect common user behaviors are still lacking. We have conducted a large-scale user survey in which we asked numerous questions related to users’ music needs, uses, seeking, and management behaviors. In this paper, we present our preliminary findings, specifically focusing on the responses to questions of users’ favorite music related websites/applications and the reasons why they like them. We provide a list of popular music services, as well as an analysis of how these services are used, and what qualities are valued. Our findings suggest several trends in the types of music services people like: an increase in the popularity of music streaming and mobile music consumption, the emergence of new functionality, such as music identification and cloud music services, an appreciation of music videos, serendipitous discovery of music, and customizability, as well as users’ changing expectations of particular types of music information."
43,Sally Jo Cunningham;David Bainbridge 0001;J. Stephen Downie,The Impact of MIREX on Scholarly Research (2005 - 2010).,2012,https://doi.org/10.5281/zenodo.1418217,"Sally Jo Cunningham, University of Waikato, NZL, education;David Bainbridge, University of Waikato, NZL, education;J. Stephen Downie, University of Illinois, USA, education","""This paper explores the impact of the MIREX (Music Information Retrieval Evaluation eXchange) evaluation initiative on scholarly research. Impact is assessed through a bibliometric evaluation of both the MIREX extended abstracts and the papers citing the MIREX results, the trial framework and methodology, or MIREX datasets. Impact is examined through number of publications and citation analysis. We further explore the primary publication venues for MIREX results, the geographic distribution of both MIREX contributors and researchers citing MIREX results, and the spread of MIREX-based research beyond the MIREX contributor teams. This analysis indicates that research in this area is highly collaborative, has achieved an international dissemination, and has grown to have a significant profile in the research literature."""
44,Rafael Cabredo;Roberto S. Legaspi;Paul Salvador Inventado;Masayuki Numao,An Emotion Model for Music Using Brain Waves.,2012,https://doi.org/10.5281/zenodo.1416398,"Rafael Cabredo, Institute of Scientific and Industrial Research, Osaka University, Japan, education, Center for Empathic Human-Computer Interactions, De La Salle University, Philippines, education;Roberto Legaspi, Institute of Scientific and Industrial Research, Osaka University, Japan, education;Paul Salvador Inventado, Institute of Scientific and Industrial Research, Osaka University, Japan, education, Center for Empathic Human-Computer Interactions, De La Salle University, Philippines, education;Masayuki Numao, Institute of Scientific and Industrial Research, Osaka University, Japan, education",Every person reacts differently to music. The task then is to identify a speciﬁc set of music features that have a signiﬁcant effect on emotion for an individual. Previous research have used self-reported emotions or tags to annotate short segments of music using discrete labels. Our approach uses an electroencephalograph to record the subject’s reaction to music. Emotion spectrum analysis method is used to analyse the electric potentials and provide continuous-valued annotations of four emotional states for different segments of the music. Music features are obtained by processing music information from the MIDI ﬁles which are separated into several segments using a windowing technique. The music features extracted are used in two separate supervised classiﬁcation algorithms to build the emotion models. Classiﬁers have a minimum error rate of 5% predicting the emotion labels.
45,Yannis Panagakis;Constantine Kotropoulos,Music Structure Analysis by Ridge Regression of Beat-synchronous Audio Features.,2012,https://doi.org/10.5281/zenodo.1417059,"Yannis Panagakis, Aristotle University of Thessaloniki, GRC, education;Constantine Kotropoulos, Aristotle University of Thessaloniki, GRC, education","A novel unsupervised method for automatic music structure analysis is proposed. Three types of audio features, namely the mel-frequency cepstral coefficients, the chroma features, and the auditory temporal modulations are employed in order to form beat-synchronous feature sequences modeling the audio signal. Assume that the feature vectors from each segment lie in a subspace and the song as a whole occupies the union of several subspaces. Then any feature vector can be represented as a linear combination of the feature vectors stemming from the same subspace. The coefficients of such a linear combination are found by solving an appropriate ridge regression problem, resulting to the ridge representation (RR) of the audio features. The RR yields an affinity matrix with nonzero within-subspace affinities and zero between-subspace ones, revealing the structure of the music recording. The segmentation of the feature sequence into music segments is found by applying the normalized cuts algorithm to the RR-based affinity matrix. In the same context, the combination of multiple audio features is investigated as well. The proposed method is referred to as ridge regression-based music structure analysis (RRMSA). State-of-the-art performance is reported for the RRMSA by conducting experiments on the manually annotated Beatles benchmark dataset."
46,Cyril Joder;Björn W. Schuller,Score-Informed Leading Voice Separation from Monaural Audio.,2012,https://doi.org/10.5281/zenodo.1415882,"Cyril Joder, Institute for Human-Machine Communication, Technische Universität München, Germany, education;Björn Schuller, Institute for Human-Machine Communication, Technische Universität München, Germany, education","Separating the leading voice from a musical recording seems to be natural to the human ear. Yet, it remains a difficult problem for automatic systems, in particular in the blind case, where no information is known about the signal. However, in the case where a musical score is available, one can take advantage of this additional information. In this paper, we present a novel application of this idea for leading voice separation exploiting a temporally-aligned MIDI Score. The model used is based on Nonnegative Matrix Factorization (NMF), whose solo part is represented by a source-filter model. We exploit the score information by constraining the source activations to conform to the aligned MIDI file. Experiments run on a database of real popular songs show that the use of these constraints can significantly improve the separation quality, in terms of both signal-based and perceptual evaluation metrics."
47,George Sioros;Andre Holzapfel;Carlos Guedes,On Measuring Syncopation to Drive an Interactive Music System.,2012,https://doi.org/10.5281/zenodo.1416194,"George Sioros, Faculdade de Engenharia da Universidade do Porto, PRT, education;André Holzapfel, Music Technology Group, Universitat Pompeu Fabra, ESP, education;Carlos Guedes, Faculdade de Engenharia da Universidade do Porto / INESC Porto, PRT, education","In this paper we address the problem of measuring synco-
pation in order to mediate a musically meaningful interac-
tion between a live music performance and an automati-
cally generated rhythm. To this end we present a simple, 
yet effective interactive music system we developed. We 
shed some light on the complex nature of syncopation by 
looking into MIDI data from drum loops and whole 
songs. We conclude that segregation into individual 
rhythmic layers is necessary in order to measure the syn-
copation of a music ensemble. This implies that measuring 
syncopation on polyphonic audio signals is not yet tractable 
using the current state-of-the-art in audio analysis.  "
48,Justin Salamon;Julián Urbano,Current Challenges in the Evaluation of Predominant Melody Extraction Algorithms.,2012,https://doi.org/10.5281/zenodo.1418041,"Justin Salamon, Universitat Pompeu Fabra, Barcelona, Spain, education;Julián Urbano, University Carlos III of Madrid, Leganés, Spain, education","In this paper we analyze the reliability of the evaluation of Audio Melody Extraction algorithms. We focus on the procedures and collections currently used as part of the annual Music Information Retrieval Evaluation eXchange (MIREX), which has become the de-facto benchmark for evaluating and comparing melody extraction algorithms. We study several factors: the duration of the audio clips, time offsets in the ground truth annotations, and the size and musical content of the collection. The results show that the clips currently used are too short to predict performance on full songs, highlighting the paramount need to use complete musical pieces. Concerning the ground truth, we show how a minor error, specifically a time offset between the annotation and the audio, can have a dramatic effect on the results, emphasizing the importance of establishing a common protocol for ground truth annotation and system output. We also show that results based on the small ADC04, MIREX05 and INDIAN08 collections are unreliable, while the MIREX09 collections are larger than necessary. This evidences the need for new and larger collections containing realistic music material, for reliable and meaningful evaluation of Audio Melody Extraction."
49,W. Bas de Haas;José Pedro Magalhães;Frans Wiering,Improving Audio Chord Transcription by Exploiting Harmonic and Metric Knowledge.,2012,https://doi.org/10.5281/zenodo.1417541,"W. Bas de Haas, Utrecht University, NLD, education;José Pedro Magalhães, University of Oxford, GBR, education;Frans Wiering, Utrecht University, NLD, education","We present a new system for chord transcription from polyphonic musical audio that uses domain-specific knowledge about tonal harmony and metrical position to improve chord transcription performance. Low-level pulse and spectral features are extracted from an audio source using the Vamp plugin architecture. Subsequently, for each beat-synchronised chromagram we compute a list of chord candidates matching that chromagram, together with the confidence in each candidate. When one particular chord candidate matches the chromagram significantly better than all others, this chord is selected to represent the segment. However, when multiple chords match the chromagram similarly well, we use a formal music theoretical model of tonal harmony to select the chord candidate that best matches the sequence based on the surrounding chords. In an experiment we show that exploiting metrical and harmonic knowledge yields statistically significant chord transcription improvements on a corpus of 217 Beatles, Queen, and Zweieck songs."
50,Aggelos Gkiokas;Vassilios Katsouros;George Carayannis,Reducing Tempo Octave Errors by Periodicity Vector Coding And SVM Learning.,2012,https://doi.org/10.5281/zenodo.1417439,"Aggelos Gkiokas, Institute for Language and Speech Processing / R.C. Athena, GRC, facility, National Technical University of Athens, GRC, education;Vassilis Katsouros, Institute for Language and Speech Processing / R.C. Athena, GRC, facility;George Carayannis, National Technical University of Athens, GRC, education","""In this paper we present a method for learning tempo classes in order to reduce tempo octave errors. There are two main contributions of this paper in the rhythm analysis field. Firstly, a novel technique is proposed to code the rhythm periodicity functions of a music signal. Target tempi range is divided into overlapping “tempo bands” and the periodicity function is filtered by triangular masks aligned to those tempo bands, in order to calculate the respective saliencies, followed by the application of the DCT transform on band strengths. The second contribution is the adoption of Support Vector Machines to learn broad tempo classes from the coded periodicity vectors. Training instances are assigned a tempo class according to annotated tempo. The classes are assumed to correspond to “music speed”. At classification phase, each target excerpt is assigned a tempo class label by the SVM. Target periodicity vector is masked by the predicted tempo class range, and tempo is estimated by peak picking in the reduced periodicity vector. The proposed method was evaluated on the benchmark ISMIR 2004 Tempo Induction Evaluation Exchange Dataset for both tempo class and tempo value estimation tasks. Results indicate that the proposed approach provides an efficient framework to tackle the tempo estimation task."""
51,Hirokazu Kameoka;Kazuki Ochiai;Masahiro Nakano;Masato Tsuchiya;Shigeki Sagayama,Context-free 2D Tree Structure Model of Musical Notes for Bayesian Modeling of Polyphonic Spectrograms.,2012,https://doi.org/10.5281/zenodo.1415252,"Hirokazu Kameoka, Graduate School of Information Science and Technology, The University of Tokyo, JPN, education, NTT Communication Science Laboratories, NTT Corporation, JPN, company;Kazuki Ochiai, Graduate School of Information Science and Technology, The University of Tokyo, JPN, education;Masahiro Nakano, NTT Communication Science Laboratories, NTT Corporation, JPN, company;Masato Tsuchiya, Graduate School of Information Science and Technology, The University of Tokyo, JPN, education;Shigeki Sagayama, Graduate School of Information Science and Technology, The University of Tokyo, JPN, education","This paper proposes a Bayesian model for automatic music transcription. Automatic music transcription involves several subproblems that are interdependent of each other: multiple fundamental frequency estimation, onset detection, and rhythm/tempo recognition. In general, simultaneous estimation is preferable when several estimation problems have chicken-and-egg relationships. This paper proposes modeling the generative process of an entire music spectrogram by combining the sub-process by which a musically natural tempo curve is generated, the sub-process by which a set of note onset positions is generated based on a 2-dimensional tree structure representation of music, and the sub-process by which a music spectrogram is generated according to the tempo curve and the note onset positions. Most conventional approaches to music transcription perform note extraction prior to structure analysis, but accurate note extraction has been a difficult task. By contrast, thanks to the combined generative model, the present method performs note extraction and structure estimation simultaneously and thus the optimal solution is obtained within a unified framework. We show some of the transcription results obtained with the present method."
52,Oriol Nieto;Eric J. Humphrey;Juan Pablo Bello,Compressing Music Recordings into Audio Summaries.,2012,https://doi.org/10.5281/zenodo.1415802,"Oriol Nieto, New York University, USA, education;Eric J. Humphrey, New York University, USA, education;Juan Pablo Bello, New York University, USA, education","We present a criterion to generate audible summaries of music recordings that optimally explain a given track with mutually disjoint segments of itself. We represent audio as sequences of beat-synchronous harmonic features and use an exhaustive search to identify the best summary. To demonstrate the merit of this approach, we evaluate the criterion and show consistency across a collection of multiple recordings of different works. Finally, we present a fast algorithm that approximates the exhaustive search and allows us to automatically learn the hyperparameters of the algorithm for a given track."
53,Mert Bay;Andreas F. Ehmann;James W. Beauchamp;Paris Smaragdis;J. Stephen Downie,Second Fiddle is Important Too: Pitch Tracking Individual Voices in Polyphonic Music.,2012,https://doi.org/10.5281/zenodo.1418121,"Mert Bay, University of Illinois at Urbana-Champaign, USA, education;Andreas F. Ehmann, University of Illinois at Urbana-Champaign, USA, education;James W. Beauchamp, University of Illinois at Urbana-Champaign, USA, education;Paris Smaragdis, University of Illinois at Urbana-Champaign, USA, education;J. Stephen Downie, University of Illinois at Urbana-Champaign, USA, education","Recently, there has been much interest in automatic pitch
estimation and note tracking of polyphonic music. To date,
however, most techniques produce a representation where
pitch estimates are not associated with any particular in-
strument or voice. Therefore, the actual tracks for each
instrument are not readily accessible. Access to individ-
ual tracks is needed for more complete music transcrip-
tion and additionally will provide a window to the anal-
ysis of higher constructs such as counterpoint and instru-
ment theme imitation during a composition. In this paper,
we present a method for tracking the pitches (F0s) of indi-
vidual instruments in polyphonic music. The system uses
a pre-learned dictionary of spectral basis vectors for each
note for a variety of musical instruments. The method then
formulates the tracking of pitches of individual voices in
a probabilistic manner by attempting to explain the input
spectrum as the most likely combination of musical instru-
ments and notes drawn from the dictionary. The method
has been evaluated on a subset of the MIREX multiple-F0
estimation test dataset, showing promising results."
54,Erik M. Schmidt;Jeffrey J. Scott;Youngmoo E. Kim,Feature Learning in Dynamic Environments: Modeling the Acoustic Structure of Musical Emotion.,2012,https://doi.org/10.5281/zenodo.1414846,"Erik M. Schmidt, Drexel University, USA, education;Jeffrey Scott, Drexel University, USA, education;Youngmoo E. Kim, Drexel University, USA, education","While emotion-based music organization is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant feature representation for music emotion recognition has yet emerged. Much of the difficulty in developing emotion-based features is the ambiguity of the ground-truth. Even using the smallest time window, opinions about emotion are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled human response labels to music in the arousal-valence (A-V) emotion space with time-varying stochastic distributions. Current methods for automatic detection of emotion in music seek performance increases by combining several feature domains (e.g. loudness, timbre, harmony, rhythm). Such work has focused largely in dimensionality reduction for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this work, we seek to employ regression-based deep belief networks to learn features directly from magnitude spectra. Taking into account the dynamic nature of music, we investigate combining multiple timescales of aggregated magnitude spectra as a basis for feature learning."
55,Xiao Hu 0001;Noriko Kando,User-centered Measures vs. System Effectiveness in Finding Similar Songs.,2012,https://doi.org/10.5281/zenodo.1416868,"Xiao Hu, The University of Hong Kong, HKG, education;Noriko Kando, National Institute of Informatics, JPN, facility","User evaluation in the domain of Music Information Retrieval (MIR) has been very scarce, while algorithms and systems in MIR have been improving rapidly. With the maturity of system-centered evaluation in MIR, time is ripe for MIR evaluation to involve users. In this study, we compare user-centered measures to a system effectiveness measure on the task of retrieving similar songs. To collect user-centered measures, we conducted a user experiment with 50 participants using a set of music retrieval systems that have been evaluated by a system-centered approach in the Music Information Retrieval Evaluation eXchange (MIREX). The results reveal weak correlation between user-centered measures and system effectiveness. It is also found that user-centered measures can disclose difference between systems when there was no difference on system-effectiveness."
56,Rudolf Mayer;Andreas Rauber,Towards Time-resilient MIR Processes.,2012,https://doi.org/10.5281/zenodo.1416310,"Rudolf Mayer, Secure Business Austria, AUT, company;Andreas Rauber, Secure Business Austria, AUT, company","In experimental sciences, under which we may likely sub-
sume most research areas in MIR, repeatability is one of
the key cornerstones of validating research and measuring
progress. Yet, due to the complexity of typical MIR exper-
iments, ensuring the capability of re-running any experi-
ment, achieving exactly identical outputs is challenging at
best. Performance differences observed may be attributed
to incomplete documentation of the process, slight vari-
ations in data (preprocessing) or software libraries used,
and others. Digital preservation aims at keeping digital
objects authentically accessible and usable over long time
spans. While traditionally focussed on individual objects,
research is now moving towards the preservation of entire
processes. In this paper we present the challenges of pre-
serving a classical MIR process, i.e. music genre classi-
ﬁcations, discuss the kinds of context information to be
captured, as well as means to validate the re-execution of a
preserved process."
57,Brian McFee;Gert R. G. Lanckriet,Hypergraph Models of Playlist Dialects.,2012,https://doi.org/10.5281/zenodo.1415618,"Brian McFee, University of California, San Diego, USA, education;Gert Lanckriet, University of California, San Diego, USA, education","Playlist generation is an important task in music informa-
tion retrieval. While previous work has treated a playlist
collection as an undifferentiated whole, we propose to build
playlist models which are tuned to speciﬁc categories or
dialects of playlists. Toward this end, we develop a general
class of ﬂexible and scalable playlist models based upon
hypergraph random walks. To evaluate the proposed mod-
els, we present a large corpus of categorically annotated,
user-generated playlists. Experimental results indicate that
category-speciﬁc models can provide substantial improve-
ments in accuracy over global playlist models."
58,Joshua L. Moore;Shuo Chen;Thorsten Joachims;Douglas Turnbull,Learning to Embed Songs and Tags for Playlist Prediction.,2012,https://doi.org/10.5281/zenodo.1416966,"Joshua L. Moore, Cornell University, USA, education;Shuo Chen, Cornell University, USA, education;Thorsten Joachims, Cornell University, USA, education;Douglas Turnbull, Ithaca College, USA, education","Automatically generated playlists have become an important medium for accessing and exploring large collections of music. In this paper, we present a probabilistic model for generating coherent playlists by embedding songs and social tags in a unified metric space. We show how the embedding can be learned from example playlists, providing the metric space with a probabilistic meaning for song/song, song/tag, and tag/tag distances. This enables at least three types of inference. First, our models can generate new playlists, outperforming conventional n-gram models in terms of predictive likelihood by orders of magnitude. Second, the learned tag embeddings provide a generalizing representation for embedding new songs, allowing it to create playlists even for songs it has never observed in training. Third, we show that the embedding space provides an effective metric for matching songs to natural-language queries, even if tags for a large fraction of the songs are missing."
59,Mohamed Sordo;Joan Serrà;Gopala K. Koduri;Xavier Serra,Extracting Semantic Information from an Online Carnatic Music Forum.,2012,https://doi.org/10.5281/zenodo.1416828,"Mohamed Sordo, Universitat Pompeu Fabra, ESP, education;Joan Serr`a, Artificial Intelligence Research Institute (IIIA-CSIC), ESP, facility;Gopala K. Koduri, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","By mining user-generated text content we can obtain music-related information that could not otherwise be extracted from audio signals or symbolic score representations. In this paper we propose a methodology for extracting music-related semantic information from an online discussion forum, rasikas.org, dedicated to the Carnatic music tradition. We first define a dictionary of relevant terms within categories such as raagas, taalas, performers, composers, and instruments, and create a complex network representation by matching such dictionary against the forum posts. This network representation is used to identify popular terms within the forum, as well as relevant co-occurrences and semantic relationships. This way, for instance, we are able to learn the instrument played by a performer with 95% accuracy, to discover the confusion between two raagas with different naming conventions, or to infer semantic relationships regarding lineage or musical influence. This contribution is a first step towards the automatic creation of ontologies for specific musical cultures."
60,Robert Macrae;Simon Dixon,Ranking Lyrics for Online Search.,2012,https://doi.org/10.5281/zenodo.1416168,"Robert Macrae, Queen Mary University of London, GBR, education;Simon Dixon, Queen Mary University of London, GBR, education","""When someone wishes to ﬁnd the lyrics for a song they typically go online and use a search engine. There are a large number of lyrics available on the internet as the effort required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the user with customary search engine page ranking formula deciding the ordering of these results based on links, views, clicks, etc. However the content, and speciﬁcally, the accuracy of the lyrics in question are not analysed or used in any way to determine the rank of the lyrics, despite this being of con- cern to the searcher. In this work, we show that online lyrics are often inaccurate and the ranking methods used by search engines do not distinguish the more accurate an- notations. We present an alternative method for ranking lyrics based purely on the collection of lyrics themselves using the Lyrics Concurrence."""
61,Jill Palzkill Woelfer;Jin Ha Lee,The Role Of Music in the Lives of Homeless Young People: A Preliminary Report.,2012,https://doi.org/10.5281/zenodo.1415864,"Jill Palzkill Woelfer, University of Washington, USA, education;Jin Ha Lee, University of Washington, USA, education","""This paper is a preliminary report of findings in an on-
going study of the role of music in the lives of homeless 
young people which is taking place in Vancouver, British 
Columbia and Seattle, WA. One hundred homeless young 
people in Vancouver took part in online surveys, 20 of 
these young people participated in interviews and 64 
completed design activities. Surveys included demo-
graphic and music questions. Interviews consisted of 
questions about music listening and preferences. In the 
design activities, participants envisioned a music device 
and provided a drawing and a scenario. Since the study is 
on-going, findings are limited to descriptive analysis of 
survey data supplemented with interview data. These 
findings provide initial insights into music listening be-
haviors, social aspects of shared music interests, and pre-
ferred music genres, bands and artists, and moods."""
62,Mohsen Kamalzadeh;Dominikus Baur;Torsten Möller,A Survey on Music Listening and Management Behaviours.,2012,https://doi.org/10.5281/zenodo.1415742,"Mohsen Kamalzadeh, Simon Fraser University, CAN, education;Dominikus Baur, University of Calgary, CAN, education;Torsten M¨oller, Simon Fraser University, CAN, education","We report the results of a survey on music listening and management behaviours. The survey was conducted online with 222 participants with mostly technical backgrounds drawn from a college age population. The median size of ofﬂine music collections was found to be roughly 2540 songs (sum of physical media and digital ﬁles). The major ﬁndings of our survey show that elements such as familiarity of songs, how distracting they are, how much they match the listener’s mood, and the desire of changing the mood within one listening session, are all affected by the activity during which music is listened to. While people want to have options for manipulating the above elements to control their experience, they prefer a minimal amount of interaction in general. Current music players lack such ﬂexibility in their controls. Finally, online recommender systems have not gained much popularity thus far."
63,Emmanouil Benetos;Simon Dixon;Dimitrios Giannoulis;Holger Kirchhoff;Anssi Klapuri,Automatic Music Transcription: Breaking the Glass Ceiling.,2012,https://doi.org/10.5281/zenodo.1415088,"Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London, GBR, education;Simon Dixon, Centre for Digital Music, Queen Mary University of London, GBR, education;Dimitrios Giannoulis, Centre for Digital Music, Queen Mary University of London, GBR, education;Holger Kirchhoff, Centre for Digital Music, Queen Mary University of London, GBR, education;Anssi Klapuri, Centre for Digital Music, Queen Mary University of London, GBR, education","Automatic music transcription is considered by many to be the Holy Grail in the ﬁeld of music signal analysis. However, the performance of transcription systems is still signiﬁcantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, al- though the ﬁeld is still very active. In this paper we analyse limitations of current methods and identify promising di- rections for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. In order to over- come the limited performance of transcription systems, al- gorithms have to be tailored to speciﬁc use-cases. Semi- automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich po- tential source of training data, via forced alignment of au- dio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information across different methods and musical aspects."
64,Markus Schedl;Arthur Flexer,Putting the User in the Center of Music Information Retrieval.,2012,https://doi.org/10.5281/zenodo.1417941,"Markus Schedl, Johannes Kepler University, AUT, education;Arthur Flexer, Austrian Research Institute for Artificial Intelligence, AUT, facility","Personalized and context-aware music retrieval and recommendation algorithms ideally provide music that perfectly fits the individual listener in each imaginable situation and for each of her information or entertainment need. Although first steps towards such systems have recently been presented at ISMIR and similar venues, this vision is still far away from being a reality. In this paper, we investigate and discuss literature on the topic of user-centric music retrieval and reflect on why the breakthrough in this field has not been achieved yet. Given the different expertises of the authors, we shed light on why this topic is a particularly challenging one, taking a psychological and a computer science view. Whereas the psychological point of view is mainly concerned with proper experimental design, the computer science aspect centers on modeling and machine learning problems. We further present our ideas on aspects vital to consider when elaborating user-aware music retrieval systems, and we also describe promising evaluation methodologies, since accurately evaluating personalized systems is a notably challenging task."
65,Jin Ha Lee;Sally Jo Cunningham,The Impact (or Non-impact) of User Studies in Music Information Retrieval.,2012,https://doi.org/10.5281/zenodo.1416272,"Jin Ha Lee, University of Washington, USA, education;Sally Jo Cunningham, University of Waikato, NZL, education","Most Music Information Retrieval (MIR) researchers will agree that understanding users' needs and behaviors is critical for developing a good MIR system. The number of user studies in the MIR domain has been gradually increasing since the early 2000s reflecting the need for empirical studies of users. However, despite the growing number of user studies and the wide recognition of their importance, it is unclear how large their impact has been in the field; on how systems are developed, evaluation tasks are created, and how we understand critical concepts such as music similarity or music mood. In this paper, we present our analysis on the growth, publication and citation patterns, and design of 155 user studies. This is followed by a discussion of a number of issues/challenges in conducting MIR user studies and distributing the research results. We conclude by making recommendations to increase the visibility and impact of user studies in the field."
66,Jean-Julien Aucouturier;Emmanuel Bigand,Mel Cepstrum & Ann Ova: The Difficult Dialog Between MIR and Music Cognition.,2012,https://doi.org/10.5281/zenodo.1417179,"Jean-Julien Aucouturier, LEAD/CNRS UMR 5022, University of Burgundy, FRA, education;Emmanuel Bigand, LEAD/CNRS UMR 5022, University of Burgundy, FRA, education",""""""
67,Eric J. Humphrey;Juan Pablo Bello;Yann LeCun,Moving Beyond Feature Design: Deep Architectures and Automatic Feature Learning in Music Informatics.,2012,https://doi.org/10.5281/zenodo.1415726,"Eric J. Humphrey, Music and Audio Research Lab, NYU, USA, education;Juan Pablo Bello, Music and Audio Research Lab, NYU, USA, education;Yann LeCun, Courant School of Computer Science, NYU, USA, education","The short history of content-based music informatics research is dominated by hand-crafted feature design, and our community has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding diminishing returns. This deceleration is largely due to the tandem of heuristic feature design and shallow processing architectures. We systematically discard hopefully irrelevant information while simultaneously calling upon creativity, intuition, or sheer luck to craft useful representations, gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of deep learning, it has only recently started to be explored in our field. By reviewing deep architectures and feature learning, we hope to raise awareness in our community about alternative approaches to solving MIR challenges, new and old alike."
68,Kevin R. Page;Benjamin Fields;David De Roure;Tim Crawford;J. Stephen Downie,"Reuse, Remix, Repeat: the Workflows of MIR.",2012,https://doi.org/10.5281/zenodo.1417299,"Kevin R. Page, Oxford e-Research Centre, University of Oxford, GBR, education;Ben Fields, Musicmetric (Semetric Ltd.), GBR, company, Department of Computing, Goldsmiths, University of London, GBR, education;David De Roure, Oxford e-Research Centre, University of Oxford, GBR, education;Tim Crawford, Department of Computing, Goldsmiths, University of London, GBR, education;J. Stephen Downie, Graduate School of Library and Information Sciences, University of Illinois, USA, education","Many solutions for the reuse and remixing of MIR meth-
ods and the tools implementing them have been introduced
over recent years. Proposals for achieving the necessary
interoperability have ranged from shared software libraries
and interfaces, through common frameworks and portals,
to standardised ﬁle formats and metadata. Each proposal
shares the desire to reuse and combine repurposable com-
ponents into assemblies (or “workﬂows”) that can be used
in novel and possibly more ambitious ways. Reuse and
remixing also have great implications for the process of
MIR research. The encapsulation of any algorithm and its
operation – including inputs, parameters, and outputs – is
fundamental to the repeatability and reproducibility of any
experiment. This is desirable both for the open and reliable
evaluation of algorithms (e.g. in MIREX) and for the ad-
vancement of MIR by building more effectively upon prior
research. At present there is no clear best practice widely
adopted throughout the community. Should this be consid-
ered a failure? Are there limits to interoperability unique to
MIR, and how might they be overcome? In this paper we
assess contemporary MIR solutions to these issues, align-
ing them with the emerging notion of Research Objects for
reproducible research in other domains, and propose their
adoption as a route to reuse in MIR."
69,Holger Kirchhoff;Simon Dixon;Anssi Klapuri,Multi-Template Shift-Variant Non-Negative Matrix Deconvolution for Semi-Automatic Music Transcription.,2012,https://doi.org/10.5281/zenodo.1418207,"Holger Kirchhoff, Queen Mary University of London, Centre for Digital Music, GBR, education;Simon Dixon, Queen Mary University of London, Centre for Digital Music, GBR, education;Anssi Klapuri, Queen Mary University of London, Centre for Digital Music, GBR, education","For the task of semi-automatic music transcription, we extended our framework for shift-variant non-negative matrix deconvolution (svNMD) to work with multiple templates per instrument and pitch. A k-means clustering based learning algorithm is proposed that infers the templates from the data based on the provided user information. We experimentally explored the maximum achievable transcription accuracy of the algorithm and evaluated the prospective performance in a realistic setting. The results showed a clear superiority of the Itakura-Saito divergence over the Kullback-Leibler divergence and a consistent improvement of the maximum achievable accuracy when each pitch is represented by more than one spectral template."
70,Aggelos Pikrakis;Francisco Gómez 0001;Sergio Oramas;José Miguel Díaz-Báñez;Joaquín Mora;Francisco Escobar-Borrego;Emilia Gómez;Justin Salamon,Tracking Melodic Patterns in Flamenco Singing by Analyzing Polyphonic Music Recordings.,2012,https://doi.org/10.5281/zenodo.1415560,"A. Pikrakis, University of Piraeus, GRC, education;F. Gómez, Polytechnic University of Madrid, ESP, education;S. Oramas, Polytechnic University of Madrid, ESP, education;J. M. D. Báñez, University of Sevilla, ESP, education;J. Mora, University of Sevilla, ESP, education;F. Escobar, University of Sevilla, ESP, education;E. Gómez, Universitat Pompeu Fabra, ESP, education;J. Salamon, Universitat Pompeu Fabra, ESP, education","The purpose of this paper is to present an algorithmic pipeline for melodic pattern detection in audio files. Our method follows a two-stage approach: first, vocal pitch sequences are extracted from the audio recordings by means of a predominant fundamental frequency estimation technique; second, instances of the patterns are detected directly in the pitch sequences by means of a dynamic programming algorithm which is robust to pitch estimation errors. In order to test the proposed method, an analysis of characteristic melodic patterns in the context of the flamenco fandango style was performed. To this end, a number of such patterns were defined in symbolic format by flamenco experts and were later detected in music corpora, which were composed of un-segmented audio recordings taken from two fandango styles, namely Valverde fandangos and Huelva capital fandangos. These two styles are representative of the fandango tradition and also differ with respect to their musical characteristics. Finally, the strategy in the evaluation of the algorithm performance was discussed by flamenco experts and their conclusions are presented in this paper."
71,Meinard Müller;Thomas Prätzlich;Jonathan Driedger,A Cross-version Approach for Stabilizing Tempo-based Novelty Detection.,2012,https://doi.org/10.5281/zenodo.1417753,"Meinard Müller, Bonn University, DEU, education, MPI Informatik, DEU, facility;Thomas Prätzlich, Saarland University, DEU, education, MPI Informatik, DEU, facility;Jonathan Driedger, Bonn University, DEU, education, Saarland University, DEU, education","The task of novelty detection with the objective of detecting changes regarding musical properties such as harmony, dynamics, timbre, or tempo is of fundamental importance when analyzing structural properties of music recordings. But for a specific audio version of a given piece of music, the novelty detection result may also crucially depend on the individual performance style of the musician. This particularly holds true for tempo-related properties, which may vary significantly across different performances of the same piece of music. In this paper, we show that tempo-based novelty detection can be stabilized and improved by simultaneously analyzing a set of different performances. We first warp the version-dependent novelty curves onto a common musical time axis, and then combine the individual curves to produce a single fusion curve. Our hypothesis is that musically relevant points of novelty tend to be consistent across different performances. This hypothesis is supported by our experiments in the context of music structure analysis, where the cross-version fusion curves yield, on average, better results than the novelty curves obtained from individual recordings."
72,Andreas Arzt;Sebastian Böck;Gerhard Widmer,Fast Identification of Piece and Score Position via Symbolic Fingerprinting.,2012,https://doi.org/10.5281/zenodo.1417022,"Andreas Arzt, Johannes Kepler University, AUT, education;Sebastian Böck, Johannes Kepler University, AUT, education;Gerhard Widmer, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility","In this paper we present a novel algorithm that, given a short snippet of an audio performance (piano music, for the time being), identiﬁes the piece and the score position. Instead of using audio matching methods we propose a combination of a state-of-the-art music transcription algorithm and a new symbolic ﬁngerprinting method. The resulting system is usable in both on-line and off-line scenarios and thus may be of use in many application areas. As the evaluation shows the system operates with only minimal lag and achieves high precision even with very short queries."
73,Geoffroy Peeters;Frédéric Cornu;Christophe Charbuillet;Damien Tardieu;Juan José Burred;Marie Vian;Valérie Botherel;Jean-Bernard Rault;Jean-Philippe Cabanal,"A Multimedia Search and Navigation Prototype, Including Music and Video-clips.",2012,https://doi.org/10.5281/zenodo.1417761,"G. Peeters, STMS IRCAM-CNRS-UPMC, FRA, facility;F. Cornu, STMS IRCAM-CNRS-UPMC, FRA, facility;Ch. Charbuillet, STMS IRCAM-CNRS-UPMC, FRA, facility;D. Tardieu, STMS IRCAM-CNRS-UPMC, FRA, facility;J.J. Burred, STMS IRCAM-CNRS-UPMC, FRA, facility;M. Vian, Bertin Technologies, FRA, company;V. Botherel, Orange-Labs, FRA, company;J.-B. Rault, Orange-Labs, FRA, company;J.-Ph. Cabanal, Orange-Labs, FRA, company","Moving music indexing technologies developed in a research lab to their integration and use in the context of a third-party search and navigation engine that indexes music files, archives of TV music programs and video-clips, involves a set of choices and works that we relate here. First one has to choose technologies that perform well, which are scalable (in terms of computation time of extraction and item comparison for search-by-similarity), and which are not sensitive to media quality (being able to process equally music files or audio tracks from video archives). These technologies must be applied to estimate tags chosen to be understandable and useful for users (the specific genre and mood tags or other content-descriptions). For training the related technologies, relevant and reliable annotated corpus must be created. For using them, relevant user-scenarios must be created and friendly Graphical User-Interface designed. In this paper, we share the experience we had in a recent project on integrating six state-of-the-art music-indexing technologies in a multimedia search and navigation prototype."
74,Ruofeng Chen;Weibin Shen;Ajay Srinivasamurthy;Parag Chordia,Chord Recognition Using Duration-explicit Hidden Markov Models.,2012,https://doi.org/10.5281/zenodo.1417077,"Ruofeng Chen, Georgia Tech Center for Music Technology, USA, facility;Weibin Shen, Georgia Tech Center for Music Technology, USA, facility;Ajay Srinivasamurthy, Georgia Tech Center for Music Technology, USA, facility;Parag Chordia, Smule Inc., USA, company","We present an audio chord recognition system based on a generalization of the Hidden Markov Model (HMM) in which the duration of chords is explicitly considered - a type of HMM referred to as a hidden semi-Markov model, or duration-explicit HMM (DHMM). We ﬁnd that such a system recognizes chords at a level consistent with the state-of-the-art systems – 84.23% on Uspop dataset at the major/minor level. The duration distribution is estimated from chord duration histograms on the training data. It is found that the state-of-the-art recognition result can be im- proved upon by using several duration distributions, which are found automatically by clustering song-level duration histograms. The paper further describes experiments which shed light on the extent to which context information, in the sense of transition matrices, is useful for the audio chord recognition task. We present evidence that the con- text provides surprisingly little improvement in performance, compared to isolated frame-wise recognition with simple smoothing. We discuss possible reasons for this, such as the inherent entropy of chord sequences in our training database."
75,Ciril Bohak;Matija Marolt,Finding Repeating Stanzas in Folk Songs.,2012,https://doi.org/10.5281/zenodo.1417597,"Ciril Bohak, University of Ljubljana, SVN, education;Matija Marolt, University of Ljubljana, SVN, education","Folk songs are typically composed of repeating parts - stanzas. To ﬁnd such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of speciﬁc issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several methods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method consists of several steps. In the ﬁrst step breathing (vocal) pauses are detected, which represent the candidate beginnings of individual stanzas. Next, a similarity measure is calculated between the ﬁrst and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate beginnings represent the actual boundaries between stanzas, a scoring function is deﬁned based on the calculated similarities between stanzas. A peak picking method is used in combination with global thresholding for the ﬁnal selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive."
76,Mathieu Giraud;Richard Groult;Florence Levé,Detecting Episodes with Harmonic Sequences for Fugue Analysis.,2012,https://doi.org/10.5281/zenodo.1416596,"Mathieu Giraud, LIFL, CNRS, Université Lille 1, FRA, education, INRIA Lille, FRA, facility;Richard Groult, MIS, Université Picardie Jules Verne, FRA, education;Florence Levé, MIS, Université Picardie Jules Verne, FRA, education","Fugues alternate between instances of the subject and of other patterns, such as the counter-subject, and modulatory sections called episodes. The episodes play an important role in the overall design of a fugue: detecting them may help the analysis of the fugue, in complement to a subject and a counter-subject detection. We propose an algorithm to retrieve episodes in the fugues of the first book of Bach’s Well-Tempered Clavier, starting from a symbolic score which is already track-separated. The algorithm does not use any information on subject or counter-subject occurrences, but tries to detect partial harmonic sequences, that is similar pitch contour in at least two voices. For this, it uses a substitution function considering “quantized partially overlapping intervals” [14] and a strict length matching for all notes, except for the first and the last one. On half of the tested fugues, the algorithm has correct or good results, enabling to sketch the design of the fugue."
77,Tom O'Hara;Nico Schüler;Yijuan Lu;Dan Tamir,Inferring Chord Sequence Meanings via Lyrics: Process and Evaluation.,2012,https://doi.org/10.5281/zenodo.1417975,"Tom O’Hara, Texas State University, USA, education;Nico Schüler, Texas State University, USA, education;Yijuan Lu, Texas State University, USA, education;Dan E. Tamir, Texas State University, USA, education","We improve upon our simple approach for learning the “associational meaning” of chord sequences from lyrics based on contingency statistics induced over a set of lyrics with chord annotations. Specifically, we refine this process by using word alignment tools developed for statistical machine translation, and we also use a much larger set of chord annotations. In addition, objective evaluation measures are included. Thus, this work validates a novel application of lexicon induction techniques over parallel corpora to a domain outside of natural language learning. To confirm the associations commonly attributed to major versus minor chords (i.e., happy and sad, respectively), we compare the inferred word associations against synonyms reflecting this dichotomy. To evaluate meanings associated with chord sequences, we check how often tagged chords occur in songs labeled with the same overall meaning."
78,Alexander Schindler;Rudolf Mayer;Andreas Rauber,Facilitating Comprehensive Benchmarking Experiments on the Million Song Dataset.,2012,https://doi.org/10.5281/zenodo.1417521,"Alexander Schindler, Vienna University of Technology, AUT, education;Rudolf Mayer, Vienna University of Technology, AUT, education;Andreas Rauber, Vienna University of Technology, AUT, education","The Million Song Dataset (MSD), a collection of one million music pieces, enables a new era of research of Music Information Retrieval methods for large-scale applications. It comes as a collection of meta-data such as the song names, artists and albums, together with a set of features extracted with the The Echo Nest services, such as loudness, tempo, and MFCC-like features."
79,Andrew Robertson,Decoding Tempo and Timing Variations in Music Recordings from Beat Annotations.,2012,https://doi.org/10.5281/zenodo.1416806,"Andrew Robertson, School of Electronic Engineering and Computer Science, education","This paper addresses the problem of determining tempo and timing data from a list of beat annotations. Whilst an approximation to the tempo can be calculated from the inter-beat interval, the annotations also include timing variations due to expressively timed events, phase shifts and errors in the annotation times. These deviations tend to propagate into the tempo graph and so tempo analysis methods tend to average over recent inter-beat intervals. However, whilst this minimises the effect such timing deviations have on the local tempo estimate, it also obscures the expressive timing devices used by the performer. Here we propose a more formal method for calculation of the optimal tempo path through use of an appropriate cost function that incorporates tempo change, phase shift and expressive timing."
80,Katerina Kosta;Marco Marchini;Hendrik Purwins,Unsupervised Chord-Sequence Generation from an Audio Example.,2012,https://doi.org/10.5281/zenodo.1415534,"Katerina Kosta, Centre for Digital Music, Queen Mary, University of London, GBR, education;Marco Marchini, Music Technology Group, Universitat Pompeu Fabra, ESP, education;Hendrik Purwins, Music Technology Group, Universitat Pompeu Fabra, ESP, education; Neurotechnology Group, Berlin Institute of Technology, DEU, education","A system is presented that generates a sound sequence from an original audio chord sequence, having the following characteristics: The generation can be arbitrarily long, preserves certain musical characteristics of the original and has a reasonable degree of interestingness. The procedure comprises the following steps: 1) chord segmentation by onset detection, 2) representation as Constant Q Profiles, 3) multi-level clustering, 4) cluster level selection, 5) metrical analysis, 6) building of a suffix tree, 7) generation heuristics. The system can be seen as a computational model of the cognition of harmony consisting of an unsupervised formation of harmonic categories (via multi-level clustering) and a sequence learning module (via suffix trees) which in turn controls the harmonic categorization in a top-down manner (via a measure of regularity). In the final synthesis, the system recombines the audio material derived from the sample itself and it is able to learn various harmonic styles. The system is applied to various musical styles and is then evaluated subjectively by musicians and non-musicians, showing that it is capable of producing sequences that maintain certain musical characteristics of the original."
81,Michael Terrell;György Fazekas;Andrew Simpson;Jordan B. L. Smith;Simon Dixon,Listening Level Changes Music Similarity.,2012,https://doi.org/10.5281/zenodo.1415710,"Michael J. Terrell, Queen Mary University of London, GBR, education;György Fazekas, Queen Mary University of London, GBR, education;Andrew J. R. Simpson, Queen Mary University of London, GBR, education;Jordan Smith, Queen Mary University of London, GBR, education;Simon Dixon, Queen Mary University of London, GBR, education","We examine the effect of listening level, i.e. the abso-
lute sound pressure level at which sounds are reproduced,
on music similarity, and in particular, on playlist gener-
ation. Current methods commonly use similarity metrics
based on Mel-frequency cepstral coefﬁcients (MFCCs), which
are derived from the objective frequency spectrum of a
sound. We follow this approach, but use the level-dependent
auditory spectrum, evaluated using the loudness models of
Glasberg and Moore, at three listening levels, to produce
auditory spectrum cepstral coefﬁcients (ASCCs). The AS-
CCs are used to generate sets of playlists at each listen-
ing level, using a typical method, and these playlists were
found to differ greatly. From this we conclude that music
recommendation systems could be made more perceptu-
ally relevant if listening level information were included.
We discuss the ﬁndings in relation to other ﬁelds within
MIR where inclusion of listening level might also be of
beneﬁt."
82,Luis Jure;Ernesto López;Martín Rocamora;Pablo Cancela;Haldo Sponton;Ignacio Irigaray,Pitch Content Visualization Tools for Music Performance Analysis.,2012,https://doi.org/10.5281/zenodo.1414860,"Luis Jure, School of Music, Universidad de la República, URY, education;Ernesto López, Faculty of Engineering, Universidad de la República, URY, education;Martín Rocamora, School of Music, Universidad de la República, URY, education, Faculty of Engineering, Universidad de la República, URY, education;Pablo Cancela, Faculty of Engineering, Universidad de la República, URY, education;Haldo Sponton, Faculty of Engineering, Universidad de la República, URY, education;Ignacio Irigaray, Faculty of Engineering, Universidad de la República, URY, education","This work deals with pitch content visualization tools for the analysis of music performance from audio recordings. An existing computational method for the representation of pitch contours is brieﬂy reviewed. Its application to music analysis is exempliﬁed with two pieces of non-notated music: a ﬁeld recording of a folkloric form of polyphonic singing and a commercial recording by a noted blues musician. Both examples have vocal parts exhibiting complex pitch evolution, difﬁcult to analyze and notate with precision using Western common music notation. By using novel time-frequency analysis techniques that improve the location of the components of a harmonic sound, the melodic content representation implemented here allows a detailed study of aspects related to pitch intonation and tuning. This in turn permits an objective measurement of essential musical characteristics that are difﬁcult or impossible to properly evaluate by subjective perception alone, and which are often not accounted for in traditional musicological analysis. Two software tools are released that allow the practical use of the described methods."
83,Justin Salamon;Sankalp Gulati;Xavier Serra,A Multipitch Approach to Tonic Identification in Indian Classical Music.,2012,https://doi.org/10.5281/zenodo.1415646,,
84,Laurent Pugin;Johannes Kepper;Perry Roland;Maja Hartwig;Andrew Hankinson,Separating Presentation and Content in MEI.,2012,https://doi.org/10.5281/zenodo.1416978,"Laurent Pugin, Swiss RISM, Fribourg University, CHE, education;Johannes Kepper, Edirom, DEU, company;Perry Roland, University of Virginia, USA, education;Maja Hartwig, Edirom, DEU, company;Andrew Hankinson, McGill University, Schulich School of Music, CAN, education","Common Western music notation is traditionally organized on staves that can be grouped into systems. When multiple systems appear on a page, they are arranged from the top to the bottom of the page, similar to lines of words in a text document. Encoding music notation documents for printing requires this arrangement to be captured. However, in the music notation model proposed by the Music Encoding Initiative (MEI), the hierarchy of the XML sub-tree representing the music emphasizes the content rather than the layout. Since systems and pages do not coincide with the musical content, they are encoded in a secondary hierarchy that contains very limited information. In this paper, we present a complementary solution for augmenting the level of detail of the layout of musical documents; that is, the layout information can be encoded in a separate sub-tree with cross-references to other elements holding the musical content. The major advantage of the proposed solution is that it enables multiple layout descriptions, each describing a different visual instantiation of the same musical content."
85,Johanna Devaney;Michael I. Mandel;Ichiro Fujinaga,A Study of Intonation in Three-Part Singing using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT).,2012,https://doi.org/10.5281/zenodo.1416210,"Johanna Devaney, CNMAT, UC Berkeley, USA, education, School of Music, The Ohio State University, USA, education;Michael Mandel, Audience Inc., USA, company, College of Engineering, The Ohio State University, USA, education;Ichiro Fujinaga, CIRMMT, CAN, facility, Schulich School of Music, McGill University, CAN, education","This paper introduces the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT), is a MATLAB toolkit for accurately aligning monophonic audio to MIDI scores as well as extracting and analyzing timing-, pitch-, and dynamics-related performance data from the aligned recordings. This paper also presents the results of an analysis performed with AMPACT on an experiment studying intonation in three-part singing. The experiment examines the interval size and drift in four ensembles’ performances of a short exercise by Benedetti, which was designed to highlight the conflict between Just Intonation tuning and pitch drift."
86,Carlos Rosão;Ricardo Ribeiro 0001;David Martins de Matos,Influence of Peak Selection Methods on Onset Detection.,2012,https://doi.org/10.5281/zenodo.1417271,"Carlos Ros˜ao, ISCTE-IUL, PRT, education, L2F/INESC-ID Lisboa, PRT, facility;Ricardo Ribeiro, ISCTE-IUL, PRT, education, L2F/INESC-ID Lisboa, PRT, facility;David Martins de Matos, IST/UTL, PRT, education, L2F/INESC-ID Lisboa, PRT, facility","Finding the starting time of musical notes in an audio signal, that is, to perform onset detection, is an important task as this information can be used as the basis for high-level musical processing tasks. Many different methods exist to perform onset detection. However their results depend on a Peak Selection step that makes the decision whether an onset is present at some point in time. In this paper we review a number of different Peak Selection methods and compare their influence in the performance of different onset detection methods and on 4 distinct onset classes. Our results show that the post-processing method used deeply influences both positively and negatively the results obtained."
87,Yading Song;Simon Dixon;Marcus Pearce,Evaluation of Musical Features for Emotion Classification.,2012,https://doi.org/10.5281/zenodo.1415854,"Yading Song, Centre for Digital Music, Queen Mary University of London, GBR, education;Simon Dixon, Centre for Digital Music, Queen Mary University of London, GBR, education;Marcus Pearce, Centre for Digital Music, Queen Mary University of London, GBR, education","Because music conveys and evokes feelings, a wealth of
research has been performed on music emotion recogni-
tion. Previous research has shown that musical mood is
linked to features based on rhythm, timbre, spectrum and
lyrics. For example, sad music correlates with slow tempo,
while happy music is generally faster. However, only lim-
ited success has been obtained in learning automatic classi-
ﬁers of emotion in music. In this paper, we collect a ground
truth data set of 2904 songs that have been tagged with one
of the four words “happy”, “sad”, “angry” and “relaxed”,
on the Last.FM web site. An excerpt of the audio is then
retrieved from 7Digital.com, and various sets of audio fea-
tures are extracted using standard algorithms. Two clas-
siﬁers are trained using support vector machines with the
polynomial and radial basis function kernels, and these are
tested with 10-fold cross validation. Our results show that
spectral features outperform those based on rhythm, dy-
namics, and, to a lesser extent, harmony. We also ﬁnd that
the polynomial kernel gives better results than the radial
basis function, and that the fusion of different feature sets
does not always lead to improved classiﬁcation."
88,Benjamin Martin 0001;Daniel G. Brown 0001;Pierre Hanna;Pascal Ferraro,BLAST for Audio Sequences Alignment: A Fast Scalable Cover Identification Tool.,2012,https://doi.org/10.5281/zenodo.1417687,"Benjamin Martin, Université de Bordeaux, FRA, education;Pierre Hanna, Université de Bordeaux, FRA, education;Pascal Ferraro, Université de Bordeaux, FRA, education;Daniel G. Brown, University of Waterloo, CAN, education","Searching for similarities in large musical databases is common for applications such as cover song identification. These methods typically use dynamic programming to align the shared musical motifs between subparts of two recordings. Such music local alignment methods are slow, as are the bioinformatics algorithms they are closely related to. We have adapted the ideas of the Basic Local Alignment Search Tool (BLAST) for biosequence alignment to the domain of aligning sequences of chroma features. Our tool allows local music sequence alignment in near-linear time. It identifies small regions of exact match between sequences, called seeds, and builds local alignments that include these seeds. Seed determination is a key issue for the accuracy of the method and closely depends on the database, the representation and the application. We introduce a particular seeding approach for cover detection, and evaluate it on both a 2000-piece training set and the million song dataset (MSD). We show that the heuristic alignment drastically improves time computation for cover song detection. Alignment sensitivity is still very high on the small database, but is dramatically weakened on the MSD, due to differences in chroma features. We discuss the impact of different choices of these features on alignment of musical pieces."
89,Xiao Hu 0001;Jin Ha Lee,A Cross-cultural Study of Music Mood Perception between American and Chinese Listeners.,2012,https://doi.org/10.5281/zenodo.1417799,"Xiao Hu, The University of Hong Kong, HKG, education;Jin Ha Lee, University of Washington, USA, education","Music mood has been recognized as an important access point for music and many online music services support browsing by mood. However, how people judge music mood has not been well studied in the Music Information Retrieval (MIR) domain. In particular, people's cultural background is often assumed to be an important factor in music mood perception, but this assumption has not been verified by empirical studies. This paper reports on a study comparing mood judgments on a set of 30 songs by American and Chinese people. Results show that mood judgments do indeed differ between American and Chinese respondents. Furthermore, respondents’ mood judgments tended to agree more with other respondents from the same culture than those from the other group. Both the song characteristics (e.g., genre, lyrical or instrumental) and the non-cultural background of the respondents (e.g., age, gender, familiarity with the songs) were analyzed to further examine the difference in mood judgments. Findings of this study help further our understanding on how cultural background affects mood perception. Also discussed in this paper are implications for designing MIR systems for cross-cultural music mood classification and recommendation."
90,Brandon Mechtley;Andreas Spanias;Perry Cook,Shortest Path Techniques for Annotation and Retrieval of Environmental Sounds.,2012,https://doi.org/10.5281/zenodo.1416926,"Brandon Mechtley, Arizona State University, USA, education;Perry Cook, Princeton University, USA, education;Andreas Spanias, Arizona State University, USA, education","Many techniques for text-based retrieval and automatic annotation of music and sound effects rely on learning with explicit generalization, training individual classifiers for each tag. Non-parametric approaches, where queries are individually"
91,Emanuele Coviello;Yonatan Vaizman;Antoni B. Chan;Gert R. G. Lanckriet,Multivariate Autoregressive Mixture Models for Music Auto-Tagging.,2012,https://doi.org/10.5281/zenodo.1416640,"Emanuele Coviello, University of California, San Diego, USA, education;Yonatan Vaizman, University of California, San Diego, USA, education;Antoni B. Chan, City University of Hong Kong, HKG, education;Gert R.G. Lanckriet, University of California, San Diego, USA, education","We propose the multivariate autoregressive model for con-
tent based music auto-tagging. At the song level our ap-
proach leverages the multivariate autoregressive mixture
(ARM) model, a generative time-series model for audio,
which assumes each feature vector in an audio fragment is
a linear function of previous feature vectors. To tackle tag-
model estimation, we propose an efﬁcient hierarchical EM
algorithm for ARMs (HEM-ARM), which summarizes the
acoustic information common to the ARMs modeling the
individual songs associated with a tag. We compare the
ARM model with the recently proposed dynamic texture
mixture (DTM) model. We hence investigate the relative
merits of different modeling choices for music time-series:
i) the ﬂexibility of selecting higher memory order in ARM,
ii) the capability of DTM to learn speciﬁc frequency ba-
sis for each particular tag and iii) the effect of the hidden
layer of the DT versus the time efﬁciency of learning and
inference with fully observable AR components. Finally,
we experiment with a support vector machine (SVM) ap-
proach that classiﬁes songs based on a kernel calculated on
the frequency responses of the corresponding song ARMs.
We show that the proposed approach outperforms SVMs
trained on a different kernel function, based on a compet-
ing generative model."
92,Philippe Hamel;Yoshua Bengio;Douglas Eck,Building Musically-relevant Audio Features through Multiple Timescale Representations.,2012,https://doi.org/10.5281/zenodo.1416530,"Philippe Hamel, Université de Montréal, CAN, education;Yoshua Bengio, Université de Montréal, CAN, education;Douglas Eck, Google Inc., USA, company","Low-level aspects of music audio such as timbre, loudness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and require a better representation of time dynamics. For various music information retrieval tasks, one would benefit from modelling both low and high level aspects in a unified feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale features. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for automatic tag annotation."
93,Juan J. Bosch;Jordi Janer;Ferdinand Fuhrmann;Perfecto Herrera,A Comparison of Sound Segregation Techniques for Predominant Instrument Recognition in Musical Audio Signals.,2012,https://doi.org/10.5281/zenodo.1416076,"Juan J. Bosch, Universitat Pompeu Fabra, Music Technology Group, ESP, education;Jordi Janer, Universitat Pompeu Fabra, Music Technology Group, ESP, education;Ferdinand Fuhrmann, Universitat Pompeu Fabra, Music Technology Group, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, Music Technology Group, ESP, education","""The authors address the identification of predominant music instruments in polytimbral audio by previously dividing the original signal into several streams. Several strategies are evaluated, ranging from low to high complexity with respect to the segregation algorithm and models used for classification. The dataset of interest is built from professionally produced recordings, which typically pose problems to state-of-art source separation algorithms. The recognition results are improved a 19% with a simple sound segregation pre-step using only panning information, in comparison to the original algorithm. In order to further improve the results, we evaluated the use of a complex source separation as a pre-step. The results showed that the performance was only enhanced if the recognition models are trained with the features extracted from the separated audio streams. In this way, the typical errors of state-of-art separation algorithms are acknowledged, and the performance of the original instrument recognition algorithm is improved in up to 32%."""
94,Juhan Nam;Jorge Herrera;Malcolm Slaney;Julius O. Smith,Learning Sparse Feature Representations for Music Annotation and Retrieval.,2012,https://doi.org/10.5281/zenodo.1415202,"Juhan Nam, Stanford University, USA, education;Jorge Herrera, Stanford University, USA, education;Malcolm Slaney, Yahoo! Research, Stanford University, USA, company, education;Julius Smith, Stanford University, USA, education","""We present a data-processing pipeline based on sparse feature learning and describe its applications to music annotation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are hand-crafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features automatically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning algorithms to music data, in particular, focusing on a high-dimensional sparse-feature representation. Our experiments show that, using only a linear classiﬁer, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and retrieval systems."""
95,Véronique Sébastien;Henri Ralambondrainy;Olivier Sébastien;Noël Conruyt,Score Analyzer: Automatically Determining Scores Difficulty Level for Instrumental e-Learning.,2012,https://doi.org/10.5281/zenodo.1416518,"Véronique Sébastien, University of Reunion Island, REU, education;Henri Ralambondrainy, University of Reunion Island, REU, education;Olivier Sébastien, University of Reunion Island, REU, education;Noël Conruyt, University of Reunion Island, REU, education","""Nowadays, huge sheet music collections exist on the Web, allowing people to access public domain scores for free. However, beginners may be lost in finding a score appropriate to their instrument level, and should often rely on themselves to start out on the chosen piece. In this instrumental e-Learning context, we propose a Score Analyzer prototype in order to automatically extract the difficulty level of a MusicXML piece and suggest advice thanks to a Musical Sign Base (MSB). To do so, we first review methods related to score performance information retrieval. We then identify seven criteria to characterize technical instrumental difficulties and propose methods to extract them from a MusicXML score. The relevance of these criteria is then evaluated through a Principal Components Analysis and compared to human estimations. Lastly we discuss the integration of this work to @-MUSE, a collaborative score annotation platform based on multimedia contents indexation."""
96,Andrew Hankinson;John Ashley Burgoyne;Gabriel Vigliensoni;Alastair Porter;Jessica Thompson 0001;Wendy Liu;Remi Chiu;Ichiro Fujinaga,Digital Document Image Retrieval Using Optical Music Recognition.,2012,https://doi.org/10.5281/zenodo.1415562,"Andrew Hankinson, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education;John Ashley Burgoyne, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education;Gabriel Vigliensoni, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education;Alastair Porter, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education;Jessica Thompson, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education;Wendy Liu, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education;Remi Chiu, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education;Ichiro Fujinaga, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada, education","Optical music recognition (OMR) and optical character recognition (OCR) have traditionally been used for document transcription—that is, extracting text or symbolic music from page images for use in an editor while discarding all spatial relationships between the transcribed notation and the original image. In this paper we discuss how OCR has shifted fundamentally from a transcription tool to an indexing tool for document image collections resulting from large digitization efforts. OMR tools and procedures, in contrast, are still focused on small-scale modes of operation. We argue that a shift in OMR development towards document image indexing would present new opportunities for searching, browsing, and analyzing large musical document collections. We present a prototype system we built to evaluate the tools and to develop practices needed to process print and manuscript sources."
97,Zafar Rafii;Bryan Pardo,Music/Voice Separation Using the Similarity Matrix.,2012,https://doi.org/10.5281/zenodo.1417631,"Zafar RAFII, Northwestern University, USA, education;Bryan PARDO, Northwestern University, USA, education","Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient."
98,Polina Proutskova;Christophe Rhodes;Geraint A. Wiggins;Tim Crawford,Breathy or Resonant - A Controlled and Curated Dataset for Phonation Mode Detection in Singing.,2012,https://doi.org/10.5281/zenodo.1415872,"Polina Proutskova, Goldsmiths, University of London, GBR, education;Christophe Rhodes, Goldsmiths, University of London, GBR, education;Geraint Wiggins, Queen Mary, University of London, GBR, education;Tim Crawford, Goldsmiths, University of London, GBR, education","This paper presents a new reference dataset of sus-
tained, sung vowels with attached labels indicating the 
phonation mode. The dataset is intended for training com-
putational models for automated phonation mode detec-
tion. 
Four phonation modes are distinguished by Johan Sun-
dberg  [15]:  breathy,  neutral,  flow  (or  resonant)  and 
pressed. The presented dataset consists of ca. 700 record-
ings of nine vowels from several languages, sung at vari-
ous pitches in various phonation modes. The recorded 
sounds were produced by one female singer under con-
trolled conditions, following recommendations by voice 
acoustics researchers. 
While datasets on phonation modes in speech exist, 
such resources for singing are not available. Our dataset 
closes this gap and offers researchers in various discip-
lines a reference and a training set. It will be made avail-
able online under Creative Commons license. Also, the 
format of the dataset is extensible. Further content addi-
tions and future support for the dataset are planned."
99,Mathieu Lagrange;Alexey Ozerov;Emmanuel Vincent,Robust Singer Identification in Polyphonic Music using Melody Enhancement and Uncertainty-based Learning.,2012,https://doi.org/10.5281/zenodo.1416526,"Mathieu Lagrange, STMS - IRCAM, CNRS - UPMC, France, education;Alexey Ozerov, Technicolor Research & Innovation, France, company;Emmanuel Vincent, INRIA, Centre de Rennes - Bretagne Atlantique, France, facility","Enhancing speciﬁc parts of a polyphonic music signal is believed to be a promising way of breaking the glass ceiling that most Music Information Retrieval (MIR) sys- tems are now facing. The use of signal enhancement as a pre-processing step has led to limited improvement though, because distortions inevitably remain in the enhanced sig- nals that may propagate to the subsequent feature extrac- tion and classiﬁcation stages. Previous studies attempting to reduce the impact of these distortions have relied on the use of feature weighting or missing feature theory. Based on advances in the ﬁeld of noise-robust speech recognition, we represent the uncertainty about the enhanced signals via a Gaussian distribution instead that is subsequently prop- agated to the features and to the classiﬁer. We introduce new methods to estimate the uncertainty from the signal in a fully automatic manner and to learn the classiﬁer directly from polyphonic data. We illustrate the results by consid- ering the task of identifying, from a given set of singers, which one is singing at a given time in a given song. Exper- imental results demonstrate the relevance of our approach."
100,Emilia Gómez;Francisco J. Cañadas-Quesada;Justin Salamon;Jordi Bonada;Pedro Vera-Candeas;Pablo Cabañas Molero,Predominant Fundamental Frequency Estimation vs Singing Voice Separation for the Automatic Transcription of Accompanied Flamenco Singing.,2012,https://doi.org/10.5281/zenodo.1416990,"E. Gómez, Universitat Pompeu Fabra, ESP, education;F. Cañadas, University of Jaen, ESP, education;J. Salamon, Universitat Pompeu Fabra, ESP, education;J. Bonada, Universitat Pompeu Fabra, ESP, education;P. Vera, University of Jaen, ESP, education;P. Cabañas, University of Jaen, ESP, education","This work evaluates two strategies for predominant fundamental frequency (f0) estimation in the context of melodic transcription from flamenco singing with guitar accompaniment. The first strategy extracts the f0 from salient pitch contours computed from the mixed spectrum; the second separates the voice from the guitar and then performs monophonic f0 estimation. We integrate both approaches with an automatic transcription system, which first estimates the tuning frequency and then implements an iterative strategy for note segmentation and labeling. We evaluate them on a flamenco music collection, including a wide range of singers and recording conditions. Both strategies achieve satisfying results. The separation-based approach yields a good overall accuracy (76.81%), although instrumental segments have to be manually located. The predominant f0 estimator yields slightly higher accuracy (79.72%) but does not require any manual annotation. Furthermore, its accuracy increases (84.68%) if we adapt some algorithm parameters to each analyzed excerpt. Most transcription errors are due to incorrect f0 estimations (typically octave and voicing errors in strong presence of guitar) and incorrect note segmentation in highly ornamented sections. Our study confirms the difficulty of transcribing flamenco singing and the need for repertoire-specific and assisted algorithms for improving state-of-the-art methods."
