Unnamed: 0,Authors,Title,Year,Link,Authors with Affiliations,Abstract
0,Dan Ringwalt;Roger B. Dannenberg,Image Quality Estimation for Multi-Score OMR.,2015,https://doi.org/10.5281/zenodo.1414890,"Dan Ringwalt, Carnegie Mellon University, USA, education;Roger B. Dannenberg, Carnegie Mellon University, USA, education","Optical music recognition (OMR) is the recognition of images of musical scores. Recent research has suggested aligning the results of OMR from multiple scores of the same work (multi-score OMR, MS-OMR) to improve accuracy. As a simpler alternative, we have developed features which predict the quality of a given score, allowing us to select the highest-quality score to use for OMR. Furthermore, quality may be used to weight each score in an alignment, which should improve existing systems’ robustness. Using commercial OMR software on a test set of MIDI recordings and multiple corresponding scores, our predicted OMR accuracy is weakly but signiﬁcantly correlated with the true accuracy. Improved features should be able to produce highly consistent results."
1,Daniel Wolff;Andrew MacFarlane 0001;Tillman Weyde,Comparative Music Similarity Modelling Using Transfer Learning Across User Groups.,2015,https://doi.org/10.5281/zenodo.1417835,"Daniel Wolff, City University London, GBR, education;Andrew MacFarlane, City University London, GBR, education;Tillman Weyde, City University London, GBR, education","We introduce a new application of transfer learning for training and comparing music similarity models based on relative user data: The proposed Relative Information-Theoretic Metric Learning (RITML) algorithm adapts a Mahalanobis distance using an iterative application of the ITML algorithm, thereby extending it to relative similarity data. RITML supports transfer learning by training models with respect to a given template model that can provide prior information for regularisation. With this feature we use information from larger datasets to build better models for more specific datasets, such as user groups from different cultures or of different age. We then evaluate what model parameters, in this case acoustic features, are relevant for the specific models when compared to the general user data."
2,Matthew Prockup;Andreas F. Ehmann;Fabien Gouyon;Erik M. Schmidt;Òscar Celma;Youngmoo E. Kim,Modeling Genre with the Music Genome Project: Comparing Human-Labeled Attributes and Audio Features.,2015,https://doi.org/10.5281/zenodo.1417523,"Matthew Prockup, Drexel University, USA, education;Andreas F. Ehmann, Pandora Media Inc., USA, company;Fabien Gouyon, Pandora Media Inc., USA, company;Erik M. Schmidt, Pandora Media Inc., USA, company;Oscar Celma, Pandora Media Inc., USA, company;Youngmoo E. Kim, Drexel University, USA, education","Genre provides one of the most convenient categorizations of music, but it is often regarded as a poorly deﬁned or largely subjective musical construct. In this work, we provide evidence that musical genres can to a large ex- tent be objectively modeled via a combination of musi- cal attributes. We employ a data-driven approach utiliz- ing a subset of 48 hand-labeled musical attributes com- prising instrumentation, timbre, and rhythm across more than one million examples from Pandorar Internet Ra- dio’s Music Genome Projectr. A set of audio features motivated by timbre and rhythm are then implemented to model genre both directly and through audio-driven mod- els derived from the hand-labeled musical attributes. In most cases, machine learning models built directly from hand-labeled attributes outperform models based on audio features. Among the audio-based models, those that com- bine audio features and learned musical attributes perform better than those derived from audio features alone."
3,Christopher J. Tralie;Paul Bendich,Cover Song Identification with Timbral Shape Sequences.,2015,https://doi.org/10.5281/zenodo.1416824,"Christopher J. Tralie, Duke University, USA, education;Paul Bendich, Duke University, USA, education","We introduce a novel low level feature for identifying cover songs which quantiﬁes the relative changes in the smoothed frequency spectrum of a song. Our key insight is that a sliding window representation of a chunk of audio can be viewed as a time-ordered point cloud in high dimensions. For corresponding chunks of audio between different versions of the same song, these point clouds are approximately rotated, translated, and scaled copies of each other. If we treat MFCC embeddings as point clouds and cast the problem as a relative shape sequence, we are able to correctly identify 42/80 cover songs in the “Covers 80” dataset. By contrast, all other work to date on cover songs exclusively relies on matching note sequences from Chroma derived features."
4,Christof Weiß;Maximilian Schaab,On the Impact of Key Detection Performance for Identifying Classical Music Styles.,2015,https://doi.org/10.5281/zenodo.1416246,"Christof Weiß, Fraunhofer Institute for Digital Media Technology Ilmenau, DEU, facility;Maximilian Schaab, Fraunhofer Institute for Digital Media Technology Ilmenau, DEU, facility","We study the automatic identification of Western classical music styles by directly using chroma histograms as classification features. Thereby, we evaluate the benefits of knowing a piece’s global key for estimating key-related pitch classes. First, we present four automatic key detection systems. We compare their performance on suitable datasets of classical music and optimize the algorithms’ free parameters. Using a second dataset, we evaluate automatic classification into the four style periods Baroque, Classical, Romantic, and Modern. To that end, we calculate global chroma statistics of each audio track. We then split up the tracks according to major and minor keys and circularly shift the chroma histograms with respect to the tonic note. Based on these features, we train two individual classifier models for major and minor keys. We test the efficiency of four chroma extraction algorithms for classification. Furthermore, we evaluate the impact of key detection performance on the classification results. Additionally, we compare the key-related chroma features to other chroma-based features. We obtain improved performance when using an efficient key detection method for shifting the chroma histograms."
5,Xinquan Zhou;Alexander Lerch,Chord Detection Using Deep Learning.,2015,https://doi.org/10.5281/zenodo.1416968,"Xinquan Zhou, Center for Music Technology, Georgia Institute of Technology, USA, facility;Alexander Lerch, Center for Music Technology, Georgia Institute of Technology, USA, facility","In this paper, we utilize deep learning to learn high-level features for audio chord detection. The learned features, obtained by a deep network in bottleneck architecture, give promising results and outperform state-of-the-art systems. We present and evaluate the results for various methods and conﬁgurations, including input pre-processing, a bottleneck architecture, and SVMs vs. HMMs for chord classiﬁcation."
6,Cameron Summers;Phillip Popp,Temporal Music Context Identification with User Listening Data.,2015,https://doi.org/10.5281/zenodo.1415938,"Cameron Summers, Gracenote, company;Phillip Popp, Gracenote, company","""The times when music is played can indicate context for listeners. From the peaceful song for waking up each morning to the traditional song for celebrating a holiday to an up-beat song for enjoying the summer, the relationship between the music and the temporal context is clearly important. For music search and recommendation systems, an understanding of these relationships provides a richer environment to discover and listen. But with the large number of tracks available in music catalogues today, manually labeling track-temporal context associations is difficult, time consuming, and costly. This paper examines track-day contexts with the purpose of identifying relationships with specific music tracks. Improvements are made to an existing method for classifying Christmas tracks and a generalization to the approach is shown that allows automated discovery of music for any day of the year. Analyzing the top 50 tracks obtained from this method for three well-known holidays, Halloween, Saint Patrick’s Day, and July 4th, precision@50 was 95%, 99%, and 73%, respectively."""
7,Andreu Vall;Marcin Skowron;Peter Knees;Markus Schedl,Improving Music Recommendations with a Weighted Factorization of the Tagging Activity.,2015,https://doi.org/10.5281/zenodo.1416802,"Andreu Vall, Johannes Kepler University, AUT, education;Marcin Skowron, Johannes Kepler University, AUT, education;Peter Knees, Johannes Kepler University, AUT, education;Markus Schedl, Johannes Kepler University, AUT, education","Collaborative ﬁltering systems for music recommendations are often based on implicit feedback derived from listening activity. Hybrid approaches further incorporate additional sources of information in order to improve the quality of the recommendations. In the context of a music streaming service, we present a hybrid model based on matrix fac- torization techniques that fuses the implicit feedback de- rived from the users’ listening activity with the tags that users have given to musical items. In contrast to exist- ing work, we introduce a novel approach to exploit tags by performing a weighted factorization of the tagging ac- tivity. We evaluate the model for the task of artist recom- mendation, using the expected percentile rank as metric, extended with conﬁdence intervals to enable the compar- ison between models. Thus, our contribution is twofold: (1) we introduce a novel model that uses tags to improve music recommendations and (2) we extend the evaluation methodology to compare the performance of different rec- ommender systems."
8,Florian Krebs;Sebastian Böck;Gerhard Widmer,An Efficient State-Space Model for Joint Tempo and Meter Tracking.,2015,https://doi.org/10.5281/zenodo.1414966,"Florian Krebs, Johannes Kepler University, Linz, Austria, education;Sebastian Böck, Johannes Kepler University, Linz, Austria, education;Gerhard Widmer, Johannes Kepler University, Linz, Austria, education","Dynamic Bayesian networks (e.g., Hidden Markov Models) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhythmic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic parameters from a piece of music. While this allows the mutual dependencies between these parameters to be exploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We incorporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets."
9,Yu-Hui Huang;Xuanli Chen;Serafina Beck;David Burn;Luc J. Van Gool,Automatic Handwritten Mensural Notation Interpreter: From Manuscript to MIDI Performance.,2015,https://doi.org/10.5281/zenodo.1418267,"Yu-Hui Huang, KU Leuven, BEL, education;Xuanli Chen, KU Leuven, BEL, education;Serafina Beck, KU Leuven, BEL, education;David Burn, KU Leuven, BEL, education;Luc Van Gool, ETH Zürich, CHE, education","""This paper presents a novel automatic recognition framework for hand-written mensural music. It takes a scanned manuscript as input and yields as output modern music scores. Compared to the previous mensural Optical Music Recognition (OMR) systems, ours shows not only promising performance in music recognition, but also works as a complete pipeline which integrates both recognition and transcription. There are three main parts in this pipeline: i) region-of-interest detection, ii) music symbol detection and classification, and iii) transcription to modern music. In addition to the output in modern notation, our system can generate a MIDI file as well. It provides an easy platform for the musicologists to analyze old manuscripts. Moreover, it renders these valuable cultural heritage resources available to non-specialists as well, as they can now access such ancient music in a better understandable form."""
10,Kazuyoshi Yoshii;Katsutoshi Itoyama;Masataka Goto,Infinite Superimposed Discrete All-Pole Modeling for Multipitch Analysis of Wavelet Spectrograms.,2015,https://doi.org/10.5281/zenodo.1417575,"Kazuyoshi Yoshii, Graduate School of Informatics, Kyoto University, JPN, education;Katsutoshi Itoyama, Graduate School of Informatics, Kyoto University, JPN, education;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), JPN, facility","This paper presents a statistical multipich analyzer based on a source-ﬁlter model that decomposes a target music audio signal in terms of three major kinds of sound quantities: pitch (fundamental frequency: F0), timbre (spectral envelope), and intensity (amplitude). If the spectral envelope of an isolated sound is represented by an all-pole ﬁlter, linear predictive coding (LPC) can be used for ﬁlter estimation in the linear-frequency domain. The main problem of LPC is that although only the amplitudes of harmonic partials are reliable samples drawn from the spectral envelope, the whole spectrum is used for ﬁlter estimation. To solve this problem, we propose an inﬁnite superimposed discrete all-pole (iSDAP) model that, given a music signal, can estimate an appropriate number of superimposed harmonic structures whose harmonic partials are drawn from a limited number of spectral envelopes. Our nonparametric Bayesian source-ﬁlter model is formulated in the log-frequency domain that better suits the frequency characteristics of human audition. Experimental results showed that the proposed model outperformed the counterpart model formulated in the linear frequency domain."
11,Laura Risk;Lillio Mok;Andrew Hankinson;Julie Cumming,Melodic Similarity in Traditional French-Canadian Instrumental Dance Tunes.,2015,https://doi.org/10.5281/zenodo.1414858,"Laura Risk, Schulich School of Music, McGill University, CAN, education, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), CAN, facility;Lillio Mok, Schulich School of Music, McGill University, CAN, education, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), CAN, facility;Andrew Hankinson, Schulich School of Music, McGill University, CAN, education, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), CAN, facility;Julie Cumming, Schulich School of Music, McGill University, CAN, education, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), CAN, facility","Commercial recordings of French-Canadian instrumental dance tunes represent a varied and complex corpus of study. This was a primarily aural tradition, transmitted from performer to performer with few notated sources until the late 20th century. Practitioners routinely combined tune segments to create new tunes and personalized settings of existing tunes. This has resulted in a corpus that exhibits an extreme amount of variation, even among tunes with the same name. In addition, the same tune or tune segment may appear under several different names. Previous attempts at building systems for automated retrieval and ranking of instrumental dance tunes perform well for near-exact matching of tunes, but do not work as well in retrieving and ranking, in order of most to least similar, variants of a tune; especially those with variations as extreme as this particular corpus. In this paper we will describe a new approach capable of ranked retrieval of variant tunes, and demonstrate its effectiveness on a transcribed corpus of incipits."
12,Sergio Oramas;Mohamed Sordo;Luis Espinosa Anke;Xavier Serra,A Semantic-Based Approach for Artist Similarity.,2015,https://doi.org/10.5281/zenodo.1415976,"Sergio Oramas, Music Technology Group, Universitat Pompeu Fabra, ESP, education;Mohamed Sordo, Center for Computational Science, University of Miami, USA, education;Luis Espinosa-Anke, TALN Group, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, ESP, education","This paper describes and evaluates a method for computing artist similarity from a set of artist biographies. The proposed method aims at leveraging semantic information present in these biographies, and can be divided in three main steps, namely: (1) entity linking, i.e. detecting mentions to named entities in the text and linking them to an external knowledge base; (2) deriving a knowledge representation from these mentions in the form of a semantic graph or a mapping to a vector-space model; and (3) computing semantic similarity between documents. We test this approach on a corpus of 188 artist biographies and a slightly larger dataset of 2,336 artists, both gathered from Last.fm. The former is mapped to the MIREX Audio and Music Similarity evaluation dataset, so that its similarity judgments can be used as ground truth. For the latter dataset we use the similarity between artists as provided by the Last.fm API. Our evaluation results show that an approach that computes similarity over a graph of entities and semantic categories clearly outperforms a baseline that exploits word co-occurrences and latent factors."
13,Shuo Zhang;Rafael Caro Repetto;Xavier Serra,Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing.,2015,https://doi.org/10.5281/zenodo.1416066,"Shuo Zhang, Universitat Pompeu Fabra, ESP, education;Rafael Caro Repetto, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","The similarity between linguistic tones and melodic pitch contours in Beijing Opera can be captured either by the contour shape of single syllable units, or by the pairwise pitch height relations in adjacent syllable units. In this paper, we investigate the latter problem with a novel machine learning approach, using techniques from time series data mining. Approximately 1300 pairwise contour segments are extracted from a selection of 20 arias. We then formulate the problem as a supervised machine-learning task of predicting types of pairwise melodic relations based on linguistic tone information. The results give a comparative view of fixed and mixed-effects models that achieved around 70% of maximum accuracy. We discuss the superiority of the current method to that of the unsupervised learning in single-syllable-unit contour analysis of similarity in Beijing Opera."
14,Graham Percival;Satoru Fukayama;Masataka Goto,Song2Quartet: A System for Generating String Quartet Cover Songs from Polyphonic Audio of Popular Music.,2015,https://doi.org/10.5281/zenodo.1415014,"Graham Percival, National Institute of Advanced Industrial Science and Technology (AIST), Japan, facility;Satoru Fukayama, National Institute of Advanced Industrial Science and Technology (AIST), Japan, facility;Masataka Goto, National Institute of Advanced Industrial Science and Technology (AIST), Japan, facility","We present Song2Quartet, a system for generating string quartet versions of popular songs by combining probabilistic models estimated from a corpus of symbolic classical music with the target audio file of any song. Song2Quartet allows users to add novelty to listening experience of their favorite songs and gain familiarity with string quartets. Previous work in automatic arrangement of music only used symbolic scores to achieve a particular musical style; our challenge is to also consider audio features of the target popular song. In addition to typical audio music content analysis such as beat and chord estimation, we also use time-frequency spectral analysis in order to better reflect partial phrases of the song in its cover version. Song2Quartet produces a probabilistic network of possible musical notes at every sixteenth note for each accompanying instrument of the quartet by combining beats, chords, and spectrogram from the target song with Markov chains estimated from our corpora of quartet music. As a result, the musical score of the cover version can be generated by finding the optimal paths through these networks. We show that the generated results follow the conventions of classical string quartet music while retaining some partial phrases and chord voicings from the target audio."
15,Jan Schlüter;Thomas Grill,Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks.,2015,https://doi.org/10.5281/zenodo.1417745,"Jan Schlüter, Austrian Research Institute for Artificial Intelligence, AUT, facility;Thomas Grill, Austrian Research Institute for Artificial Intelligence, AUT, facility","In computer vision, state-of-the-art object recognition systems rely on label-preserving image transformations such as scaling and rotation to augment the training datasets. The additional training examples help the system to learn invariances that are difficult to build into the model, and improve generalization to unseen data. To the best of our knowledge, this approach has not been systematically explored for music signals. Using the problem of singing voice detection with neural networks as an example, we apply a range of label-preserving audio transformations to assess their utility for music data augmentation. In line with recent research in speech recognition, we find pitch shifting to be the most helpful augmentation method. Combined with time stretching and random frequency filtering, we achieve a reduction in classification error between 10 and 30%, reaching the state of the art on two public datasets. We expect that audio data augmentation would yield significant gains for several other sequence labelling and event detection tasks in music information retrieval."
16,Siddharth Sigtia;Nicolas Boulanger-Lewandowski;Simon Dixon,Audio Chord Recognition with a Hybrid Recurrent Neural Network.,2015,https://doi.org/10.5281/zenodo.1416594,"Siddharth Sigtia, Centre for Digital Music, Queen Mary University of London, UK, education;Nicolas Boulanger-Lewandowski, Université de Montréal, CAN, education, Google Inc., USA, company;Simon Dixon, Centre for Digital Music, Queen Mary University of London, UK, education","In this paper, we present a novel architecture for audio chord estimation using a hybrid recurrent neural network. The architecture replaces hidden Markov models (HMMs) with recurrent neural network (RNN) based language models for modelling temporal dependencies between chords. We demonstrate the ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic signal, eliminating the need for a complex feature extraction stage. For the hybrid RNN architecture, inference over the output variables of interest is performed using beam search. In addition to the hybrid model, we propose a modification to beam search using a hash table which yields improved results while reducing memory requirements by an order of magnitude, thus making the proposed model suitable for real-time applications. We evaluate our model's performance on a dataset with publicly available annotations and demonstrate that the performance is comparable to existing state of the art approaches for chord recognition."
17,Beatrix Vad;Daniel Boland;John Williamson;Roderick Murray-Smith;Peter Berg Steffensen,Design and Evaluation of a Probabilistic Music Projection Interface.,2015,https://doi.org/10.5281/zenodo.1416670,"Beatrix Vad, School of Computing Science, University of Glasgow, United Kingdom, education;Daniel Boland, School of Computing Science, University of Glasgow, United Kingdom, education;John Williamson, School of Computing Science, University of Glasgow, United Kingdom, education;Roderick Murray-Smith, School of Computing Science, University of Glasgow, United Kingdom, education;Peter Berg Steffensen, Syntonetic A/S, Copenhagen, Denmark, company","We describe the design and evaluation of a probabilistic interface for music exploration and casual playlist generation. Predicted subjective features, such as mood and genre, inferred from low-level audio features create a 34-dimensional feature space. We use a nonlinear dimensionality reduction algorithm to create 2D music maps of tracks, and augment these with visualisations of probabilistic mappings of selected features and their uncertainty. We evaluated the system in a longitudinal trial in users’ homes over several weeks. Users said they had fun with the interface and liked the casual nature of the playlist generation. Users preferred to generate playlists from a local neighbourhood of the map, rather than from a trajectory, using neighbourhood selection more than three times more often than path selection. Probabilistic highlighting of subjective features led to more focused exploration in mouse activity logs, and 6 of 8 users said they preferred the probabilistic highlighting mode."
18,Asterios I. Zacharakis;Maximos A. Kaliakatsos-Papakostas;Emilios Cambouropoulos,Conceptual Blending in Music Cadences: A Formal Model and Subjective Evaluation.,2015,https://doi.org/10.5281/zenodo.1416056,"Asterios Zacharakis, Aristotle University of Thessaloniki, GRC, education;Maximos Kaliakatsos-Papakostas, Aristotle University of Thessaloniki, GRC, education;Emilios Cambouropoulos, Aristotle University of Thessaloniki, GRC, education","Conceptual blending is a cognitive theory whereby elements from diverse, but structurally-related, mental spaces are ‘blended’ giving rise to new conceptual spaces. This study focuses on structural blending utilising an algorithmic formalisation for conceptual blending applied to harmonic concepts. More speciﬁcally, it investigates the ability of the system to produce meaningful blends between harmonic cadences, which arguably constitute the most fundamental harmonic concept. The system creates a variety of blends combining elements of the penultimate chords of two input cadences and it further estimates the expected relationships between the produced blends. Then, a preliminary subjective evaluation of the proposed blending system is presented. A pairwise dissimilarity listening test was conducted using original and blended cadences as stimuli. Subsequent multidimensional scaling analysis produced spatial conﬁgurations for both behavioural data and dissimilarity estimations by the algorithm. Comparison of the two conﬁgurations showed that the system is capable of making fair predictions of the perceived dissimilarities between the blended cadences. This implies that this conceptual blending approach is able to create perceptually meaningful blends based on self-evaluation of its outcome."
19,Jeongsoo Park 0001;Kyogu Lee,Harmonic-Percussive Source Separation Using Harmonicity and Sparsity Constraints.,2015,https://doi.org/10.5281/zenodo.1417323,"Jeongsoo Park, Seoul National University, KOR, education;Kyogu Lee, Seoul National University, KOR, education","In this paper, we propose a novel approach to harmonic-percussive sound separation (HPSS) using Non-negative Matrix Factorization (NMF) with sparsity and harmonicity constraints. Conventional HPSS methods have focused on temporal continuity of harmonic components and spectral continuity of percussive components. However, it may not be appropriate to use them to separate time-varying harmonic signals such as vocals, vibratos, and glissandos, as they lack in temporal continuity. Based on the observation that the spectral distributions of harmonic and percussive signals differ – i.e., harmonic components have harmonic and sparse structure while percussive components are broadband – we propose an algorithm that successfully separates the rapidly time-varying harmonic signals from the percussive ones by imposing different constraints on the two groups of spectral bases. Experiments with real recordings as well as synthesized sounds show that the proposed method outperforms the conventional methods."
20,Wai Man Szeto;Kin Hong Wong,A Hierarchical Bayesian Framework for Score-Informed Source Separation of Piano Music Signals.,2015,https://doi.org/10.5281/zenodo.1417061,"Wai Man SZETO, The Chinese University of Hong Kong, CHN, education;Kin Hong WONG, The Chinese University of Hong Kong, CHN, education","Here we propose a score-informed monaural source separation system to extract every tone from a mixture of piano tone signals. Two sinusoidal models in our earlier work are employed in the above-mentioned system to represent piano tones: the General Model and the Piano Model. The General Model, a variant of sinusoidal modeling, can represent a single tone with high modeling quality, yet it fails to separate mixtures of tones due to the overlapping partials. The Piano Model, on the other hand, is an instrument-specific model tailored for piano. Its modeling quality is lower but it can learn from training data (consisting entirely of isolated tones), resolve the overlapping partials and thus separate the mixtures. We formulate a new hierarchical Bayesian framework to run both Models in the source separation process so that the mixtures with overlapping partials can be separated with high quality. The results show that our proposed system gives robust and accurate separation of piano tone signal mixtures (including octaves) while achieving significantly better quality than those reported in related work done previously."
21,Patrick E. Savage;Quentin D. Atkinson,Automatic Tune Family Identification by Musical Sequence Alignment.,2015,https://doi.org/10.5281/zenodo.1417135,"Patrick E. Savage, Tokyo University of the Arts, JPN, education;Quentin D. Atkinson, Auckland University, NZL, education","Musics, like languages and genes, evolve through a process of transmission, variation, and selection. Evolution of musical tune families has been studied qualitatively for over a century, but quantitative analysis has been hampered by an inability to objectively distinguish between musical similarities that are due to chance and those that are due to descent from a common ancestor. Here we propose an automated method to identify tune families by adapting genetic sequence alignment algorithms designed for automatic identification and alignment of protein families. We tested the effectiveness of our method against a high-quality ground-truth dataset of 26 folk tunes from four diverse tune families (two English, two Japanese) that had previously been identified and aligned manually by expert musicologists. We tested different combinations of parameters related to sequence alignment and to modeling of pitch, rhythm, and text to find the combination that best matched the ground-truth classifications. The best-performing automated model correctly grouped 100% (26/26) of the tunes in terms of overall similarity to other tunes, identifying 85% (22/26) of these tunes as forming distinct tune families. The success of our approach on a diverse, cross-cultural ground-truth dataset suggests promise for future automated reconstruction of musical evolution on a wide scale."
22,Igor Vatolkin;Günter Rudolph;Claus Weihs,Evaluation of Album Effect for Feature Selection in Music Genre Recognition.,2015,https://doi.org/10.5281/zenodo.1416328,"Igor Vatolkin, TU Dortmund, DEU, education;Günter Rudolph, TU Dortmund, DEU, education;Claus Weihs, TU Dortmund, DEU, education","With an increasing number of available music characteristics, feature selection becomes more important for various categorisation tasks, helping to identify relevant features and remove irrelevant and redundant ones. Another advantage is the decrease of runtime and storage demands. However, sometimes feature selection may lead to “over-optimisation” when data in the optimisation set is too different from data in the independent validation set. In this paper, we extend our previous work on feature selection for music genre recognition and focus on so-called “album effect” meaning that optimised classification models may overemphasize relevant characteristics of particular artists and albums rather than learning relevant properties of genres. For that case we examine the performance of classification models on two validation sets after the optimisation with feature selection: the first set with tracks not used for training and feature selection but randomly selected from the same albums, and the second set with tracks selected from other albums. As it can be expected, the classification performance on the second set decreases. Nevertheless, in almost all cases the feature selection remains beneficial compared to complete feature sets and a baseline using MFCCs, if applied for an ensemble of classifiers, proving robust generalisation performance."
23,Cheng-i Wang;Jennifer Hsu;Shlomo Dubnov,Music Pattern Discovery with Variable Markov Oracle: A Unified Approach to Symbolic and Audio Representations.,2015,https://doi.org/10.5281/zenodo.1416480,"Cheng-i Wang, University of California, San Diego, USA, education;Jennifer Hsu, University of California, San Diego, USA, education;Shlomo Dubnov, University of California, San Diego, USA, education","This paper presents a framework for automatically discovering patterns in a polyphonic music piece. The proposed framework is capable of handling both symbolic and audio representations. Chroma features are post-processed with heuristics stemming from musical knowledge and fed into the pattern discovery framework. The pattern-ﬁnding algorithm is based on Variable Markov Oracle. The Variable Markov Oracle data structure is capable of locating repeated sufﬁxes within a time series, thus making it an appropriate tool for the pattern discovery task. Evaluation of the proposed framework is performed on the JKU Patterns Development Dataset with state of the art performance."
24,Rodrigo Schramm;Helena de Souza Nunes;Cláudio Rosito Jung,Automatic Solfège Assessment.,2015,https://doi.org/10.5281/zenodo.1414720,"Rodrigo Schramm, Institute of Informatics, Federal University of Rio Grande do Sul, Brazil, education;Helena de Souza Nunes, Department of Music, Federal University of Rio Grande do Sul of Music, Brazil, education;Cláudio Rosito Jung, Institute of Informatics, Federal University of Rio Grande do Sul, Brazil, education","This paper presents a note-by-note approach for automatic solfège assessment. The proposed system uses melodic transcription techniques to extract the sung notes from the audio signal, and the sequence of melodic segments is subsequently processed by a two stage algorithm. On the first stage, an aggregation process is introduced to perform the temporal alignment between the transcribed melody and the music score (ground truth). This stage implicitly aggregates and links the best combination of the extracted melodic segments with the expected note in the ground truth. On the second stage, a statistical method is used to evaluate the accuracy of each detected sung note. The technique is implemented using a Bayesian classifier, which is trained using an audio dataset containing individual scores provided by a committee of expert listeners. These individual scores were measured at each musical note, regarding the pitch, onset, and offset accuracy. Experimental results indicate that the classification scheme is suitable to be used as an assessment tool, providing useful feedback to the student."
25,Felipe Vieira;Nazareno Andrade,Evaluating Conflict Management Mechanisms for Online Social Jukeboxes.,2015,https://doi.org/10.5281/zenodo.1416232,"Felipe Vieira, Universidade Federal de Campina Grande, BRA, education;Nazareno Andrade, Universidade Federal de Campina Grande, BRA, education","Social music listening is a prevalent and often fruitful experience. Social jukeboxes are systems that enable social music listening with listeners collaboratively choosing the music to be played. Naturally, because music tastes are diverse, using social jukeboxes often involves conﬂicting interests. Because of that, virtually all social jukeboxes incorporate conﬂict management mechanisms. In contrast with their widespread use, however, little attention has been given to evaluating how different conﬂict management mechanisms function to preserve the positive experience of music listeners. This paper presents an experiment with three conﬂict management mechanisms and three groups of listeners. The mechanisms were chosen to represent those most commonly used in the state of the practice. Our study employs a mixed-methods approach to quantitatively analyze listeners’ satisfaction and to examine their impressions and views on conﬂict, conﬂict management mechanisms, and social jukeboxing."
26,Ajay Srinivasamurthy;Andre Holzapfel;Ali Taylan Cemgil;Xavier Serra,Particle Filters for Efficient Meter Tracking with Dynamic Bayesian Networks.,2015,https://doi.org/10.5281/zenodo.1416736,"Ajay Srinivasamurthy, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, education;Andre Holzapfel, Dept. of Computer Engineering, Boğaziçi University, Istanbul, Turkey, education;Ali Taylan Cemgil, Dept. of Computer Engineering, Boğaziçi University, Istanbul, Turkey, education;Xavier Serra, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, education","Recent approaches in meter tracking have successfully applied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the application of exact inference is computationally demanding. More efficient approximate inference algorithms using particle filters (PF) can be developed to overcome this limitation. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an existing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Filter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter tracking accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We document that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music."
27,Mohamed Sordo;Mitsunori Ogihara;Stefan Wuchty,Analysis of the Evolution of Research Groups and Topics in the ISMIR Conference.,2015,https://doi.org/10.5281/zenodo.1416354,"Mohamed Sordo, University of Miami, USA, education;Mitsunori Ogihara, University of Miami, USA, education;Stefan Wuchty, University of Miami, USA, education","We present an analysis of the topics and research groups that participated in the ISMIR conference over the last 15 years, based on its proceedings. While we ﬁrst investigate the topological changes of the co-authorship network as well as topics over time, we also identify groups of re- searchers, allowing us to investigate their evolution and topic dependence. Notably, we ﬁnd that large groups last longer if they actively alter their membership. Further- more, such groups tend to cover a wider selection of topics, suggesting that a change of members as well as of research topics increases their adaptability. In turn, smaller groups show the opposite behavior, persisting longer if their mem- bership is altered minimally and focus on a smaller set of topics. Finally, by analyzing the effect of group size and lifespan on research impact, we observed that papers penned by medium sized and long lasting groups tend to have a citation advantage."
28,Kevin R. Page;Terhi Nurmikko-Fuller;Carolin Rindfleisch;David M. Weigl;Richard Lewis 0001;Laurence Dreyfus;David De Roure,A Toolkit for Live Annotation of Opera Performance: Experiences Capturing Wagner's Ring Cycle.,2015,https://doi.org/10.5281/zenodo.1415582,"Kevin R. Page, University of Oxford, GBR, education;Terhi Nurmikko-Fuller, University of Oxford, GBR, education;Carolin Rindﬂeisch, University of Oxford, GBR, education;David M. Weigl, University of Oxford, GBR, education;Richard Lewis, Goldsmiths, University of London, GBR, education;Laurence Dreyfus, University of Oxford, GBR, education;David De Roure, University of Oxford, GBR, education","Performance of a musical work potentially provides a rich source of multimedia material for future investigation, both for musicologists’ study of reception and perception, and in improvement of computational methods applied to its analysis. This is particularly true of music theatre, where a traditional recording cannot sufﬁciently capture the ephem- eral phenomena unique to each staging. In this paper we introduce a toolkit developed with, and used by, a musi- cologist throughout a complete multi-day production of Richard Wagner’s Der Ring des Nibelungen. The toolkit is centred on a tablet-based score interface through which the scholar makes notes on the scenic setting of the perfor- mance as it unfolds, supplemented by a variety of digital data gathered to structure and index the annotations. We report on our experience developing a system suitable for real-time use by the musicologist, structuring the data for reuse and further investigation using semantic web tech- nologies, and of the practical challenges and compromises of ﬁeldwork within a working theatre. Finally we con- sider the utility of our tooling from both a user perspective and through an initial quantitative investigation of the data gathered."
29,Marcelo E. Rodríguez-López;Anja Volk,Selective Acquisition Techniques for Enculturation-Based Melodic Phrase Segmentation.,2015,https://doi.org/10.5281/zenodo.1415064,"Marcelo E. Rodríguez-López, Utrecht University, NLD, education;Anja Volk, Utrecht University, NLD, education","Automatic melody segmentation is an important yet unsolved problem in Music Information Retrieval. Research in the field of Music Cognition suggests that previous listening experience plays a considerable role in the perception of melodic segment structure. At present automatic melody segmenters that model listening experience commonly do so using unsupervised statistical learning with ‘non-selective’ information acquisition techniques, i.e. the learners gather and store information indiscriminately into memory."
30,Jan Van Balen;John Ashley Burgoyne;Dimitrios Bountouridis;Daniel Müllensiefen;Remco C. Veltkamp,Corpus Analysis Tools for Computational Hook Discovery.,2015,https://doi.org/10.5281/zenodo.1415038,"Jan Van Balen, Department of Information and Computing Sciences, Utrecht University, NLD, education;John Ashley Burgoyne, Music Cognition Group, University of Amsterdam, NLD, education;Dimitrios Bountouridis, Department of Information and Computing Sciences, Utrecht University, NLD, education;Daniel M¨ullensiefen, Department of Psychology, Goldsmiths, University of London, GBR, education;Remco C. Veltkamp, Department of Information and Computing Sciences, Utrecht University, NLD, education","Compared to studies with symbolic music data, advances in music description from audio have overwhelmingly focused on ground truth reconstruction and maximizing prediction accuracy, with only a small fraction of studies using audio description to gain insight into musical data. We present a strategy for the corpus analysis of audio data that is optimized for interpretable results. The approach brings two previously unexplored concepts to the audio domain: audio bigram distributions, and the use of corpus-relative or “second-order” descriptors. To test the real-world applicability of our method, we present an experiment in which we model song recognition data collected in a widely-played music game. By using the proposed corpus analysis pipeline we are able to present a cognitively adequate analysis that allows a model interpretation in terms of the listening history and experience of our participants. We find that our corpus-based audio features are able to explain a comparable amount of variance to symbolic features for this task when used alone and that they can supplement symbolic features profitably when the two types of features are used in tandem. Finally, we highlight new insights into what makes music recognizable."
31,Colin Raffel;Daniel P. W. Ellis,Large-Scale Content-Based Matching of MIDI and Audio Files.,2015,https://doi.org/10.5281/zenodo.1417371,"Colin Raffel, Columbia University, USA, education;Daniel P. W. Ellis, Columbia University, USA, education","MIDI ﬁles, when paired with corresponding audio record- ings, can be used as ground truth for many music infor- mation retrieval tasks. We present a system which can efﬁciently match and align MIDI ﬁles to entries in a large corpus of audio content based solely on content, i.e., with- out using any metadata. The core of our approach is a con- volutional network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space. Once represented in this way, we can efﬁciently perform large-scale dynamic time warping searches to match MIDI data to audio recordings. We evaluate our approach on the task of matching a huge corpus of MIDI ﬁles to the Million Song Dataset."
32,Hendrik Schreiber,Improving Genre Annotations for the Million Song Dataset.,2015,https://doi.org/10.5281/zenodo.1414760,"Hendrik Schreiber, tagtraum industries incorporated, USA, company","Any automatic music genre recognition (MGR) system must show its value in tests against a ground truth dataset. Recently, the public dataset most often used for this purpose has been proven problematic, because of mislabeling, duplications, and its relatively small size. Another dataset, the Million Song Dataset (MSD), a collection of features and metadata for one million tracks, unfortunately does not contain readily accessible genre labels. Therefore, multiple attempts have been made to add song-level genre annotations, which are required for supervised machine learning tasks. Thus far, the quality of these annotations has not been evaluated. In this paper we present a method for creating additional genre annotations for the MSD from databases, which contain multiple, crowd-sourced genre labels per song (Last.fm, beaTunes). Based on label co-occurrence rates, we derive taxonomies, which allow inference of top-level genres. These are most often used in MGR systems. We then combine multiple datasets using majority voting. This both promises a more reliable ground truth and allows the evaluation of the newly generated and pre-existing datasets. To facilitate further research, all derived genre annotations are publicly available on our website."
33,Brian McFee;Eric J. Humphrey;Juan Pablo Bello,A Software Framework for Musical Data Augmentation.,2015,https://doi.org/10.5281/zenodo.1418365,"Brian McFee, Center for Data Science, New York University, USA, education; Music and Audio Research Laboratory, New York University, USA, education;Eric J. Humphrey, Music and Audio Research Laboratory, New York University, USA, education; MuseAmi, Inc., USA, company;Juan P. Bello, Music and Audio Research Laboratory, New York University, USA, education","Predictive models for music annotation tasks are practically limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of “data augmentation” — supplementing a training set with carefully perturbed samples — has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated perturbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals."
34,Chih-Wei Wu;Alexander Lerch,Drum Transcription Using Partially Fixed Non-Negative Matrix Factorization with Template Adaptation.,2015,https://doi.org/10.5281/zenodo.1417839,"Chih-Wei Wu, Georgia Institute of Technology, Center for Music Technology, USA, facility;Alexander Lerch, Georgia Institute of Technology, Center for Music Technology, USA, facility","In this paper, a template adaptive drum transcription algorithm using partially fixed Non-negative Matrix Factorization (NMF) is presented. The proposed method detects percussive events in complex mixtures of music with a minimal training set. The algorithm decomposes the music signal into two dictionaries: a percussive dictionary initialized with pre-defined drum templates and a harmonic dictionary initialized with undefined entries. The harmonic dictionary is adapted to the non-percussive music content in a standard NMF procedure. The percussive dictionary is adapted to each individual signal in an iterative scheme: it is fixed during the decomposition process, and is updated based on the result of the previous convergence. Two template adaptation methods are proposed to provide more flexibility and robustness in the case of unknown data. The performance of the proposed system has been evaluated and compared to state of the art systems. The results show that template adaptation improves the transcription performance, and the detection accuracy is in the same range as more complex systems."
35,Leonardo O. Nunes;Martín Rocamora;Luis Jure;Luiz W. P. Biscainho,Beat and Downbeat Tracking Based on Rhythmic Patterns Applied to the Uruguayan Candombe Drumming.,2015,https://doi.org/10.5281/zenodo.1415728,"Leonardo Nunes, ATL-Brazil, Microsoft, company;Martín Rocamora, Universidad de la República, URY, education;Luis Jure, Universidad de la República, URY, education;Luiz W. P. Biscainho, Federal Univ. of Rio de Janeiro, BRA, education","Computational analysis of the rhythmic/metrical structure of music from recorded audio is a hot research topic in music information retrieval. Recent research has explored the explicit modeling of characteristic rhythmic patterns as a way to improve upon existing beat-tracking algorithms, which typically fail on dealing with syncopated or poly- rhythmic music. This work takes the Uruguayan Candombe drumming (an afro-rooted rhythm from Latin America) as a case study. After analyzing the aspects that make this music genre troublesome for usual algorithmic approaches and describing its basic rhythmic patterns, the paper proposes a supervised scheme for rhythmic pattern tracking that aims at finding the metric structure from a Candombe recording, including beat and downbeat phases. Then it evaluates and compares the performance of the method with those of general-purpose beat-tracking algorithms through a set of experiments involving a database of annotated recordings totaling over two hours of audio. The results of this work reinforce the advantages of tracking rhythmic patterns (possibly learned from annotated music) when it comes to automatically following complex rhythms. A software implementation of the proposal as well as the annotated database utilized are available to the research community with the publication of this paper."
36,Christian Dittmar;Martin Pfleiderer;Meinard Müller,Automated Estimation of Ride Cymbal Swing Ratios in Jazz Recordings.,2015,https://doi.org/10.5281/zenodo.1418149,"Christian Dittmar, International Audio Laboratories Erlangen, DEU, facility;Martin Pﬂeiderer, The Liszt University of Music Weimar, DEU, education;Meinard M¨uller, International Audio Laboratories Erlangen, DEU, facility","In this paper, we propose a new method suitable for the automatic analysis of microtiming played by drummers in jazz recordings. Specifically, we aim to estimate the drummers’ swing ratio in excerpts of jazz recordings taken from the Weimar Jazz Database. A first approach is based on automatic detection of ride cymbal (RC) onsets and evaluation of relative time intervals between them. However, small errors in the onset detection propagate considerably into the swing ratio estimates. As our main technical contribution, we propose to use the log-lag autocorrelation function (LLACF) as a mid-level representation for estimating swing ratios, circumventing the error-prone detection of RC onsets. In our experiments, the LLACF-based swing ratio estimates prove to be more reliable than the ones based on RC onset detection. Therefore, the LLACF seems to be the method of choice to process large amounts of jazz recordings. Finally, we indicate some implications of our method for microtiming studies in jazz research."
37,Che-Yuan Liang;Li Su;Yi-Hsuan Yang;Hsin-Ming Lin,Musical Offset Detection of Pitched Instruments: The Case of Violin.,2015,https://doi.org/10.5281/zenodo.1416834,"Che-Yuan Liang, Academia Sinica, TWN, facility;Li Su, Academia Sinica, TWN, facility;Yi-Hsuan Yang, Academia Sinica, TWN, facility;Hsin-Ming Lin, University of California, San Diego, USA, education","Musical offset detection is an integral part of a music signal processing system that requires complete characterization of note events. However, unlike onset detection, offset detection has seldom been the subject of an in-depth study in the music information retrieval community, possibly because of the ambiguity involved in the determination of offset times in music. This paper presents a preliminary study aiming at discussing ways to annotate and to evaluate offset times for pitched non-percussive instruments. Moreover, we conduct a case study of offset detection in violin recordings by evaluating a number of energy, spectral flux, and pitch based methods using a new dataset covering 6 different violin playing techniques. The new dataset, which is going to be shared with the research community, consists of 63 violin recordings that are thoroughly annotated based on perceptual loudness and note transition. The offset detection methods, which are adapted from well-known methods for onset detection, are evaluated using an onset-aware method we propose for this task. Result shows that the accuracy of offset detection is highly dependent on the playing techniques involved. Moreover, pitch-based methods can better get rid of the soft-decaying behavior of offsets and achieve the best result among others."
38,Bill Z. Manaris;Seth Stoudenmier,Specter: Combining Music Information Retrieval with Sound Spatialization.,2015,https://doi.org/10.5281/zenodo.1415596,"Bill Manaris, College of Charleston, USA, education;Seth Stoudenmier, College of Charleston, USA, education","Specter combines music information retrieval (MIR) with sound spatialization to provide a simple, yet versatile environment to experiment with sound spatialization for music composition and live performance. Through various interfaces and sensors, users may position sounds at arbitrary locations and trajectories in a three-dimensional plane. The system utilizes the JythonMusic environment for symbolic music processing, music information retrieval, and live audio manipulation. It also incorporates Iannix, a 3D graphical, open-source sequencer, for real-time generation, manipulation, and storing of sound trajectory scores. Finally, through Glaser, a sound manipulation instrument, Specter renders the various sounds in space. The system architecture supports different sound spatialization techniques including Ambisonics and Vector Based Amplitude Panning.  Various interfaces are discussed, including a Kinect-based sensor system, a Leap-Motion-based hand-tracking interface, and a smartphone-based OSC controller.  Finally, we present Migrant, a music composition, which utilizes and demonstrates Specter’s ability to combine MIR techniques with sound spatialization through inexpensive, minimal hardware."
39,Dawen Liang;Minshu Zhan;Daniel P. W. Ellis,Content-Aware Collaborative Music Recommendation Using Pre-trained Neural Networks.,2015,https://doi.org/10.5281/zenodo.1416308,"Dawen Liang, Columbia University, USA, education;Minshu Zhan, Columbia University, USA, education;Daniel P. W. Ellis, Columbia University, USA, education","Although content is fundamental to our music listening preferences, the leading performance in music recommendation is achieved by collaborative-ﬁltering-based methods which exploit the similarity patterns in user’s listening history rather than the audio content of songs. Meanwhile, collaborative ﬁltering has the well-known “cold-start” problem, i.e., it is unable to work with new songs that no one has listened to. Efforts on incorporating content information into collaborative ﬁltering methods have shown success in many non-musical applications, such as scientiﬁc article recommendation. Inspired by the related work, we train a neural network on semantic tagging information as a content model and use it as a prior in a collaborative ﬁltering model. Such a system still allows the user listening data to “speak for itself”. The proposed system is evaluated on the Million Song Dataset and shows comparably better result than the collaborative ﬁltering approaches, in addition to the favorable performance in the cold-start case."
40,Cynthia C. S. Liem;Alan Hanjalic,Comparative Analysis of Orchestral Performance Recordings: An Image-Based Approach.,2015,https://doi.org/10.5281/zenodo.1416248,"Cynthia C. S. Liem, Delft University of Technology, NLD, education;Alan Hanjalic, Delft University of Technology, NLD, education","Traditionally, the computer-assisted comparison of multiple performances of the same piece focused on performances on single instruments. Due to data availability, there also has been a strong bias towards analyzing piano performances, in which local timing, dynamics and articulation are important expressive performance features. In this paper, we consider the problem of analyzing multiple performances of the same symphonic piece, performed by different orchestras and different conductors. While differences between interpretations in this genre may include commonly studied features on timing, dynamics and articulation, the timbre of the orchestra and choices of balance within the ensemble are other important aspects distinguishing different orchestral interpretations from one another. While it is hard to model these higher-level aspects as explicit audio features, they can usually be noted visually in spectrogram plots. We therefore propose a method to compare orchestra performances by examining visual spectrogram characteristics. Inspired by eigenfaces in human face recognition, we apply Principal Components Analysis on synchronized performance fragments to localize areas of cross-performance variation in time and frequency. We discuss how this information can be used to examine performer differences, and how beyond pairwise comparison, relative differences can be studied between multiple performances in a corpus at once."
41,Bernhard Lehner;Gerhard Widmer,Monaural Blind Source Separation in the Context of Vocal Detection.,2015,https://doi.org/10.5281/zenodo.1415940,"Bernhard Lehner, Johannes Kepler University of Linz, AUT, education;Gerhard Widmer, Johannes Kepler University of Linz, AUT, education","""In this paper, we evaluate the usefulness of several monaural blind source separation (BSS) algorithms in the context of vocal detection (VD). BSS is the problem of recovering several sources, given only a mixture. VD is the problem of automatically identifying the parts in a mixed audio signal, where at least one person is singing. We compare the results of three different strategies for utilising the estimated singing voice signals from four state-of-the-art source separation algorithms. In order to assess the performance of those strategies on an internal data set, we use two different feature sets, each fed to two different classifiers. After selecting the most promising approach, the results on two publicly available data sets are presented. In an additional experiment, we use the improved VD for a simple post-processing technique: For the final estimation of the source signals, we decide to use either silence, or the mixed, or the separated signals, according to the VD. The results of traditionally used BSS evaluation methods suggest that this is useful for both the estimated background signals, as well as for the estimated vocals."""
42,Yin-Jyun Luo;Li Su;Yi-Hsuan Yang;Tai-Shih Chi,Detection of Common Mistakes in Novice Violin Playing.,2015,https://doi.org/10.5281/zenodo.1415968,"Yin-Jyun Luo, National Chiao Tung University, TWN, education, Research Center for Information Technology Innovation, Academia Sinica, TWN, facility;Li Su, Research Center for Information Technology Innovation, Academia Sinica, TWN, facility;Yi-Hsuan Yang, Research Center for Information Technology Innovation, Academia Sinica, TWN, facility;Tai-Shih Chi, National Chiao Tung University, TWN, education","Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musical instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset comprising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are generated from the same feature set with different scales, including two note-level representations and three segment-level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vector machine for classification. Results show that the F-measures using different feature representations can vary up to 20% for two types of playing mistakes. It demonstrates the different sensitivities of each feature representation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed."
43,Dimos Makris;Maximos A. Kaliakatsos-Papakostas;Emilios Cambouropoulos,Probabilistic Modular Bass Voice Leading in Melodic Harmonisation.,2015,https://doi.org/10.5281/zenodo.1416374,"Dimos Makris, Ionian University, GRC, education;Maximos Kaliakatsos-Papakostas, Aristotle University of Thessaloniki, GRC, education;Emilios Cambouropoulos, Aristotle University of Thessaloniki, GRC, education","Probabilistic methodologies provide successful tools for automated music composition, such as melodic harmonisation, since they capture statistical rules of the music idioms they are trained with. Proposed methodologies focus either on specific aspects of harmony (e.g., generating abstract chord symbols) or incorporate the determination of many harmonic characteristics in a single probabilistic generative scheme. This paper addresses the problem of assigning voice leading focussing on the bass voice, i.e., the realisation of the actual bass pitches of an abstract chord sequence, under the scope of a modular melodic harmonisation system where different aspects of the generative process are arranged by different modules. The proposed technique defines the motion of the bass voice according to several statistical aspects: melody voice contour, previous bass line motion, bass-to-melody distances and statistics regarding inversions and note doublings in chords. The aforementioned aspects of voicing are modular, i.e., each criterion is defined by independent statistical learning tools. Experimental results on diverse music idioms indicate that the proposed methodology captures efficiently the voice layout characteristics of each idiom, whilst additional analyses on separate statistically trained modules reveal distinctive aspects of each idiom. The proposed system is designed to be flexible and adaptable (for instance, for the generation of novel blended melodic harmonisations)."
44,Anis Khlif;Vidhyasaharan Sethu,An Iterative Multi Range Non-Negative Matrix Factorization Algorithm for Polyphonic Music Transcription.,2015,https://doi.org/10.5281/zenodo.1416686,"Anis Khlif, ´Ecole des Mines ParisTech, France, education;Vidhyasaharan Sethu, University of New South Wales, Australia, education","""This article presents a novel iterative algorithm based on Non-negative Matrix Factorisation (NMF) that is particularly well suited to the task of automatic music transcription (AMT). Compared with previous NMF based techniques, this one does not aim at factorizing the time-frequency representation of the entire musical signal into a combination of the possible set of notes. Instead, the proposed algorithm proceeds iteratively by initially decomposing a part of the time-frequency representation into a combination of a small subset of all possible notes then reinvesting this information in the following step involving a large subset of notes. Specifically, starting with the lowest octave of notes that is of interest, each iteration increases the set of notes under consideration by an octave. The resolution of a lower dimensionality problem used to properly initialize matrices for a more complex problem, results in a gain of some percent in the transcription accuracy."""
45,Anna M. Kruspe,"Training Phoneme Models for Singing with ""Songified"" Speech Data.",2015,https://doi.org/10.5281/zenodo.1416858,"Anna M. Kruspe, Fraunhofer IDMT, Germany, facility","Speech recognition in singing is a task that has not been widely researched so far. Singing possesses several characteristics that differentiate it from speech. Therefore, algorithms and models that were developed for speech usually perform worse on singing. One of the bottlenecks in many algorithms is the recognition of phonemes in singing. We noticed that this recognition step can be improved when using singing data in model training, but to our knowledge, there are no large datasets of singing data annotated with phonemes. However, such data does exist for speech. We therefore propose to make phoneme recognition models more robust for singing by training them on speech data that has artiﬁcially been made more “song-like”. We test two main modiﬁcations on speech data: Time stretching and pitch shifting. Artiﬁcial vibrato is also tested. We then evaluate models trained on different combinations of these modiﬁed speech recordings. The utilized modeling algorithms are Neural Networks and Deep Belief Networks."
46,Rong Jin;Christopher Raphael,Graph-Based Rhythm Interpretation.,2015,https://doi.org/10.5281/zenodo.1418287,"Rong Jin, Indiana University, School of Informatics and Computing, USA, education;Christopher Raphael, Indiana University, School of Informatics and Computing, USA, education","We present a system that interprets the notated rhythm obtained from optical music recognition (OMR). Our approach represents the notes and rests in a system measure as the vertices of a graph. We connect the graph by adding voice edges and coincidence edges between pairs of vertices, while the rhythmic interpretation follows simply from the connected graph. The graph identification problem is cast as an optimization where each potential edge is scored according to its plausibility. We seek the optimally scoring graph where the score is represented as a sum of edge scores. Experiments were performed on about 60 score pages showing that our system can handle difficult rhythmic situations including multiple voices, voices that merge and split, voices spanning two staves, and missing tuplets."
47,Jonathan Driedger;Thomas Prätzlich;Meinard Müller,Let it Bee - Towards NMF-Inspired Audio Mosaicing.,2015,https://doi.org/10.5281/zenodo.1415698,"Jonathan Driedger, International Audio Laboratories Erlangen, DEU, facility;Thomas Prätzlich, International Audio Laboratories Erlangen, DEU, facility;Meinard Müller, International Audio Laboratories Erlangen, DEU, facility","A swarm of bees buzzing “Let it be” by the Beatles or the wind gently howling the romantic “Gute Nacht” by Schubert – these are examples of audio mosaics as we want to create them. Given a target and a source recording, the goal of audio mosaicing is to generate a mosaic recording that conveys musical aspects (like melody and rhythm) of the target, using sound components taken from the source. In this work, we propose a novel approach for automatically generating audio mosaics with the objective to preserve the source’s timbre in the mosaic. Inspired by algorithms for non-negative matrix factorization (NMF), our idea is to use update rules to learn an activation matrix that, when multiplied with the spectrogram of the source recording, resembles the spectrogram of the target recording. However, when applying the original NMF procedure, the resulting mosaic does not adequately reflect the source’s timbre. As our main technical contribution, we propose an extended set of update rules for the iterative learning procedure that supports the development of sparse diagonal structures in the activation matrix. We show how these structures better retain the source’s timbral characteristics in the resulting mosaic."
48,Andreas Arzt;Gerhard Widmer,Real-Time Music Tracking Using Multiple Performances as a Reference.,2015,https://doi.org/10.5281/zenodo.1417205,"Andreas Arzt, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility;Gerhard Widmer, Johannes Kepler University, AUT, education, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility","In general, algorithms for real-time music tracking directly use a symbolic representation of the score, or a synthesised version thereof, as a reference for the on-line alignment process. In this paper we present an alternative approach. First, different performances of the piece in question are collected and aligned (off-line) to the symbolic score. Then, multiple instances of the on-line tracking algorithm (each using a different performance as a reference) are used to follow the live performance, and their output is combined to come up with the current position in the score. As the evaluation shows, this strategy improves both the robustness and the precision, especially on pieces that are generally hard to track (e.g. pieces with extreme, abrupt tempo changes, or orchestral pieces with a high degree of polyphony). Finally, we describe a real-world application, where this music tracking algorithm was used to follow a world-famous orchestra in a concert hall in order to show synchronised visual content (the sheet music, explanatory text and videos) to members of the audience."
49,Peter Knees;Ángel Faraldo;Perfecto Herrera;Richard Vogl;Sebastian Böck;Florian Hörschläger;Mickael Le Goff,Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections.,2015,https://doi.org/10.5281/zenodo.1414996,"Peter Knees, Johannes Kepler University, AUT, education;´Angel Faraldo, Universitat Pompeu Fabra, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, ESP, education;Richard Vogl, Johannes Kepler University, AUT, education;Sebastian Böck, Johannes Kepler University, AUT, education;Florian Hörschlöager, Johannes Kepler University, AUT, education;Mickael Le Goff, Native Instruments GmbH, DEU, company","We present two new data sets for automatic evaluation of tempo estimation and key detection algorithms. In contrast to existing collections, both released data sets focus on electronic dance music (EDM). The data sets have been automatically created from user feedback and annotations extracted from web sources. More precisely, we utilize user corrections submitted to an online forum to report wrong tempo and key annotations on the Beatport website. Beatport is a digital record store targeted at DJs and focusing on EDM genres. For all annotated tracks in the data sets, samples of at least one-minute-length can be freely downloaded. For key detection, further ground truth is extracted from expert annotations manually assigned to Beatport tracks for benchmarking purposes. The set for tempo estimation comprises 664 tracks and the set for key detection 604 tracks. We detail the creation process of both data sets and perform extensive benchmarks using state-of-the-art algorithms from both academic research and commercial products."
50,Taku Kuribayashi;Yasuhito Asano;Masatoshi Yoshikawa,Towards Support for Understanding Classical Music: Alignment of Content Descriptions on the Web.,2015,https://doi.org/10.5281/zenodo.1416138,"Taku Kuribayashi, Accenture Japan Ltd, JPN, company;Yasuhito Asano, Graduate School of Informatics, Kyoto University, JPN, education;Masatoshi Yoshikawa, Graduate School of Informatics, Kyoto University, JPN, education","Supporting the understanding of classical music is an important topic that involves various research fields such as text analysis and acoustics analysis. Content descriptions are explanations of classical music compositions that help a person to understand technical aspects of the music. Recently, Kuribayashi et al. proposed a method for obtaining content descriptions from the web. However, the content descriptions on a single page frequently explain a specific part of a composition only. Therefore, a person who wants to fully understand the composition suffers from a time-consuming task, which seems almost impossible for a novice of classical music. To integrate the content descriptions obtained from multiple pages, we propose a method for aligning each pair of paragraphs of such descriptions. Using dynamic time warping-based method along with our new ideas, (a) a distribution-based distance measure named w2DD, and (b) the concept of passage expressions, it is possible to align content descriptions of classical music better than when using cutting-edge text analysis methods. Our method can be extended in future studies to create applications systems to integrate descriptions with musical scores and performances."
51,Sergio Oramas;Francisco Gómez 0001;Emilia Gómez;Joaquín Mora,FlaBase: Towards the Creation of a Flamenco Music Knowledge Base.,2015,https://doi.org/10.5281/zenodo.1417183,"Sergio Oramas, Universitat Pompeu Fabra, ESP, education;Francisco Gómez, Technical University of Madrid, ESP, education;Emilia Gómez, Universitat Pompeu Fabra, ESP, education;Joaquín Mora, University of Sevilla, ESP, education","Online information about ﬂamenco music is scattered over different sites and knowledge bases. Unfortunately, there is no common repository that indexes all these data. In this work, information related to ﬂamenco music is gathered from general knowledge bases (e.g., Wikipedia, DBpedia), music encyclopedias (e.g., MusicBrainz), and specialized ﬂamenco websites, and is then integrated into a new knowledge base called FlaBase. As resources from different data sources do not share common identiﬁers, a process of pair-wise entity resolution has been performed. FlaBase contains information about 1,174 artists, 76 palos (ﬂamenco genres), 2,913 albums, 14,078 tracks, and 771 Andalusian locations. It is freely available in RDF and JSON formats. In addition, a method for entity recognition and disambiguation for FlaBase has been created. The system can recognize and disambiguate FlaBase entity references in Spanish texts with an f-measure value of 0.77. We applied it to biographical texts present in Flabase. By using the extracted information, the knowledge base is populated with relevant information and a semantic graph is created connecting the entities of FlaBase. Artists relevance is then computed over the graph and evaluated according to a ﬂamenco expert criteria. Accuracy of results shows a high degree of quality and completeness of the knowledge base."
52,Swapnil Gupta;Ajay Srinivasamurthy;Manoj Kumar P. A.;Hema A. Murthy;Xavier Serra,Discovery of Syllabic Percussion Patterns in Tabla Solo Recordings.,2015,https://doi.org/10.5281/zenodo.1416558,"Swapnil Gupta, Universitat Pompeu Fabra, ESP, education;Ajay Srinivasamurthy, Universitat Pompeu Fabra, ESP, education;Manoj Kumar, Indian Institute of Technology Madras, IND, education;Hema A. Murthy, Indian Institute of Technology Madras, IND, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","We address the unexplored problem of percussion pattern discovery in Indian art music. Percussion in Indian art mu- sic uses onomatopoeic oral mnemonic syllables for the trans- mission of repertoire and technique. This is utilized for the task of percussion pattern discovery from audio recordings. From a parallel corpus of audio and expert curated scores for 38 tabla solo recordings, we use the scores to build a set of most frequent syllabic patterns of different lengths. From this set, we manually select a subset of musically rep- resentative query patterns. To discover these query patterns in an audio recording, we use syllable-level hidden Markov models (HMM) to automatically transcribe the recording into a syllable sequence, in which we search for the query pattern instances using a Rough Longest Common Subse- quence (RLCS) approach. We show that the use of RLCS makes the approach robust to errors in automatic transcrip- tion, significantly improving the pattern recall rate and F- measure. We further propose possible enhancements to im- prove the results."
53,Eita Nakamura;Philippe Cuvillier;Arshia Cont;Nobutaka Ono;Shigeki Sagayama,Autoregressive Hidden Semi-Markov Model of Symbolic Music Performance for Score Following.,2015,https://doi.org/10.5281/zenodo.1417030,"Eita Nakamura, National Institute of Informatics, JPN, facility;Philippe Cuvillier, Inria MuTant Project-Team, Ircam/UPMC/CNRS UMR STMS, FRA, facility;Arshia Cont, Inria MuTant Project-Team, Ircam/UPMC/CNRS UMR STMS, FRA, facility;Nobutaka Ono, National Institute of Informatics, JPN, facility;Shigeki Sagayama, Meiji University, JPN, education","A stochastic model of symbolic (MIDI) performance of polyphonic scores is presented and applied to score following. Stochastic modelling has been one of the most successful strategies in this field. We describe the performance as a hierarchical process of performer’s progression in the score and the production of performed notes, and represent the process as an extension of the hidden semi-Markov model. The model is compared with a previously studied model based on hidden Markov model (HMM), and reasons are given that the present model is advantageous for score following especially for scores with trills, tremolos, and arpeggios. This is also confirmed empirically by comparing the accuracy of score following and analysing the errors. We also provide a hybrid of this model and the HMM-based model which is computationally more efficient and retains the advantages of the former model. The present model yields one of the state-of-the-art score following algorithms for symbolic performance and can possibly be applicable for other music recognition problems."
54,Chuan-Lung Lee;Yin-Tzu Lin;Zun-Ren Yao;Feng-Yi Lee;Ja-Ling Wu,Automatic Mashup Creation by Considering both Vertical and Horizontal Mashabilities.,2015,https://doi.org/10.5281/zenodo.1416712,"Chuan-Lung Lee, National Taiwan University, TWN, education;Yin-Tzu Lin, National Taiwan University, TWN, education;Zun-Ren Yao, National Taiwan University, TWN, education;Feng-Yi Lee, National Taiwan University, TWN, education;Ja-Ling Wu, National Taiwan University, TWN, education","In this paper, we proposed a system to effectively create music mashups – a kind of re-created music that is made by mixing parts of multiple existing music pieces. Unlike previous studies which merely generate mashups by overlaying music segments on one single base track, the proposed system creates mashups with multiple background (e.g. instrumental) and lead (e.g. vocal) track segments. So, besides the suitability between the vertically overlaid tracks (i.e. vertical mashability) used in previous studies, we proposed to further consider the suitability between the horizontally connected consecutive music segments (i.e. horizontal mashability) when searching for proper music segments to be combined. On the vertical side, two new factors: “harmonic change balance” and “volume weight” have been considered. On the horizontal side, the methods used in the studies of medley creation are incorporated. Combining vertical and horizontal mashabilities together, we defined four levels of mashability that may be encountered and found the proper solution to each of them. Subjective evaluations showed that the proposed four levels of mashability can appropriately reflect the degrees of listening enjoyment. Besides, by taking the newly proposed vertical mashability measurement into account, the improvement in user satisfaction is statistically significant."
55,Brian McFee;Oriol Nieto;Juan Pablo Bello,Hierarchical Evaluation of Segment Boundary Detection.,2015,https://doi.org/10.5281/zenodo.1414866,"Brian McFee, Center for Data Science, New York University, USA, education; Music and Audio Research Laboratory, New York University, USA, education;Oriol Nieto, Music and Audio Research Laboratory, New York University, USA, education;Juan P. Bello, Music and Audio Research Laboratory, New York University, USA, education","Structure in music is traditionally analyzed hierarchically: large-scale sections can be sub-divided and reﬁned down to the short melodic ideas at the motivic level. However, typical algorithmic approaches to structural annotation produce ﬂat temporal partitions of a track, which are commonly evaluated against a similarly ﬂat, human-produced annotation. Evaluating structure analysis as represented by ﬂat annotations effectively discards all notions of structural depth in the evaluation. Although collections of hierarchical structure annotations have been recently published, no techniques yet exist to measure an algorithm’s accuracy against these rich structural annotations. In this work, we propose a method to evaluate structural boundary detection with hierarchical annotations. The proposed method transforms boundary detection into a ranking problem, and facilitates the comparison of both ﬂat and hierarchical annotations. We demonstrate the behavior of the proposed method with various synthetic and real examples drawn from the SALAMI dataset."
56,Masaki Otsuka;Tetsuro Kitahara,Improving MIDI Guitar's Accuracy with NMF and Neural Net.,2015,https://doi.org/10.5281/zenodo.1417531,"Masaki Otsuka, Graduate School of Integrated Basic Sciences, Nihon University, JPN, education;Tetsuro Kitahara, Graduate School of Integrated Basic Sciences, Nihon University, JPN, education","In this paper, we propose a method for improving the accuracy of MIDI guitars. MIDI guitars are useful tools for various purposes from inputting MIDI data to enjoying a jam session system, but existing MIDI guitars do not have sufficient accuracy in converting the performance to an MIDI form. In this paper, we make an attempt on improving the accuracy of a MIDI guitar by integrating it with an audio transcription method based on non-negative matrix factorization (NMF). First, we investigate an NMF-based algorithm for transcribing guitar performances. Although the NMF is a promising method, an effective post-process (i.e., converting the NMF’s output to an MIDI form) is a non-trivial problem. We propose use of a neural network for this conversion. Next, we investigate a method for integrating the outputs of the MIDI guitar and NMF. Because they have different tendencies in wrong outputs, we take an policy of outputting only common parts in the two outputs. Experimental results showed that the F-score of our method was 0.626 whereas those of the MIDI-guitar-only and NMF-and-neural-network-only methods were 0.347 and 0.526, respectively."
57,Jiajie Dai;Matthias Mauch;Simon Dixon,Analysis of Intonation Trajectories in Solo Singing.,2015,https://doi.org/10.5281/zenodo.1417169,"Jiajie Dai, Centre for Digital Music, Queen Mary University of London, GBR, education;Matthias Mauch, Centre for Digital Music, Queen Mary University of London, GBR, education;Simon Dixon, Centre for Digital Music, Queen Mary University of London, GBR, education","We present a new dataset for singing analysis and modelling, and an exploratory analysis of pitch accuracy and pitch trajectories. Shortened versions of three pieces from The Sound of Music were selected: “Edelweiss”, “Do-Re-Mi” and “My Favourite Things”. 39 participants sang three repetitions of each excerpt without accompaniment, resulting in a dataset of 21762 notes in 117 recordings. To obtain pitch estimates we used the Tony software’s automatic transcription and manual correction tools. Pitch accuracy was measured in terms of pitch error and interval error. We show that singers’ pitch accuracy correlates significantly with self-reported singing skill and musical training. Larger intervals led to larger errors, and the tritone interval in particular led to average errors of one third of a semitone. Note duration (or inter-onset interval) had a significant effect on pitch accuracy, with greater accuracy on longer notes. To model drift in the tonal centre over time, we present a sliding window model which reveals patterns in the pitch errors of some singers. Based on the trajectory, we propose a measure for the magnitude of drift: tonal reference deviation (TRD). The data and software are freely available."
58,Maximos A. Kaliakatsos-Papakostas;Asterios I. Zacharakis;Costas Tsougras;Emilios Cambouropoulos,Evaluating the General Chord Type Representation in Tonal Music and Organising GCT Chord Labels in Functional Chord Categories.,2015,https://doi.org/10.5281/zenodo.1417379,"Maximos Kaliakatsos-Papakostas, School of Music Studies, Aristotle University of Thessaloniki, GRC, education;Asterios Zacharakis, School of Music Studies, Aristotle University of Thessaloniki, GRC, education;Costas Tsougras, School of Music Studies, Aristotle University of Thessaloniki, GRC, education;Emilios Cambouropoulos, School of Music Studies, Aristotle University of Thessaloniki, GRC, education","The General Chord Type (GCT) representation is appropriate for encoding tone simultaneities in any harmonic context (such as tonal, modal, jazz, octatonic, atonal). The GCT allows the re-arrangement of the notes of a harmonic sonority such that abstract idiom-speciﬁc types of chords may be derived. This encoding is inspired by the standard roman numeral chord type labelling and is, therefore, ideal for hierarchic harmonic systems such as the tonal system and its many variations; at the same time, it adjusts to any other harmonic system such as post-tonal, atonal music, or traditional polyphonic systems. In this paper the descriptive potential of the GCT is assessed in the tonal idiom by comparing GCT harmonic labels with human expert annotations (Kostka & Payne harmonic dataset). Additionally, novel methods for grouping and clustering chords, according to their GCT encoding and their functional role in chord sequences, are introduced. The results of both harmonic labelling and functional clustering indicate that the GCT representation constitutes a suitable scheme for representing effectively harmony in computational systems."
59,Athanasios Lykartsis;Chih-Wei Wu;Alexander Lerch,Beat Histogram Features from NMF-Based Novelty Functions for Music Classification.,2015,https://doi.org/10.5281/zenodo.1418145,"Athanasios Lykartsis, Technische Universität Berlin, DEU, education;Chih-Wei Wu, Georgia Institute of Technology, USA, education;Alexander Lerch, Georgia Institute of Technology, USA, education","""In this paper we present novel rhythm features derived from drum tracks extracted from polyphonic music and evaluate them in a genre classiﬁcation task. Musical excerpts are analyzed using an optimized, partially ﬁxed Non-Negative Matrix Factorization (NMF) method and beat histogram features are calculated on basis of the resulting activation functions for each one out of three drum tracks extracted (Hi-Hat, Snare Drum and Bass Drum). The features are eval- uated on two widely used genre datasets (GTZAN and Ball- room) using standard classiﬁcation methods, concerning the achieved overall classiﬁcation accuracy. Furthermore, their suitability in distinguishing between rhythmically sim- ilar genres and the performance of the features resulting from individual activation functions is discussed. Results show that the presented NMF-based beat histogram features can provide comparable performance to other classiﬁcation systems, while considering strictly drum patterns."""
60,Diego Furtado Silva;Vinícius M. A. de Souza;Gustavo E. A. P. A. Batista,Music Shapelets for Fast Cover Song Recognition.,2015,https://doi.org/10.5281/zenodo.1416236,"Diego F. Silva, Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo, BRA, education;Vinícius M. A. Souza, Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo, BRA, education;Gustavo E. A. P. A. Batista, Instituto de Ciências Matemáticas e de Computação – Universidade de São Paulo, BRA, education","A cover song is a new performance or recording of a previously recorded music by an artist other than the original one. The automatic identification of cover songs is useful for a wide range of tasks, from fans looking for new versions of their favorite songs to organizations involved in licensing copyrighted songs. This is a difficult task given that a cover may differ from the original song in key, timbre, tempo, structure, arrangement and even language of the vocals. Cover song identification has attracted some attention recently. However, most of the state-of-the-art approaches are based on similarity search, which involves a large number of similarity computations to retrieve potential cover versions for a query recording. In this paper, we adapt the idea of time series shapelets for content-based music retrieval. Our proposal adds a training phase that finds small excerpts of feature vectors that best describe each song. We demonstrate that we can use such small segments to identify cover songs with higher identification rates and more than one order of magnitude faster than methods that use features to describe the whole music."
61,Marius Miron;Julio José Carabias-Orti;Jordi Janer,Improving Score-Informed Source Separation for Classical Music through Note Refinement.,2015,https://doi.org/10.5281/zenodo.1417689,"Marius Miron, Universitat Pompeu Fabra, ESP, education;Julio José Carabias-Orti, Universitat Pompeu Fabra, ESP, education;Jordi Janer, Universitat Pompeu Fabra, ESP, education","Signal decomposition methods such as Non-negative Matrix Factorization (NMF) demonstrated to be a suitable approach for music signal processing applications, including sound source separation. To better control this decomposition, NMF has been extended using prior knowledge and parametric models. In fact, using score information considerably improved separation results. Nevertheless, one of the main problems of using score information is the misalignment between the score and the actual performance. A potential solution to this problem is the use of audio to score alignment systems. However, most of them rely on a tolerance window that clearly affects the separation results. To overcome this problem, we propose a novel method to refine the aligned score at note level by detecting both, onset and offset for each note present in the score. Note refinement is achieved by detecting shapes and contours in the estimated instrument-wise time activation (gains) matrix. Decomposition is performed in a supervised way, using training instrument models and coarsely-aligned score information. The detected contours define time-frequency note boundaries, and they increase the sparsity. Finally, we have evaluated our method for informed source separation using a dataset of Bach chorales obtaining satisfactory results, especially in terms of SIR."
62,Charles Inskip;Frans Wiering,In Their Own Words: Using Text Analysis to Identify Musicologists' Attitudes towards Technology.,2015,https://doi.org/10.5281/zenodo.1416422,"Charles Inskip, University College London, GBR, education;Frans Wiering, Universiteit Utrecht, NLD, education","A widely distributed online survey gathered quantitative and qualitative data relating to the use of technology in the research practices of musicologists. This survey builds on existing work in the digital humanities and provides insights into the specific nature of musicology in relation to use and perceptions of technology. Analysis of the data (n=621) notes the preferences in resource format and the digital skills of the survey participants. The themes of comments on rewards, benefits, frustrations, risks, and limitations are explored using an h-point approach derived from applied linguistics. It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities, and that efforts should be made into supporting development of their digital skills and providing usable, useful and reliable software created with a ‘musicology-centred’ design approach. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level."
63,Julien Osmalskyj;Peter Foster;Simon Dixon;Jean-Jacques Embrechts,Combining Features for Cover Song Identification.,2015,https://doi.org/10.5281/zenodo.1417295,"Julien Osmalskyj, University of Liège, BEL, education;Peter Foster, Queen Mary University of London, GBR, education;Simon Dixon, Queen Mary University of London, GBR, education;Jean-Jacques Embrechts, University of Liège, BEL, education","""In this paper, we evaluate a set of methods for combining features for cover song identiﬁcation. We ﬁrst create multiple classiﬁers based on global tempo, duration, loudness, beats and chroma average features, training a random forest for each feature. Subsequently, we evaluate standard combination rules for merging these single classiﬁers into a composite classiﬁer based on global features. We further obtain two higher level classiﬁers based on chroma features: one based on comparing histograms of quantized chroma features, and a second one based on computing cross-correlations between sequences of chroma features, to account for temporal information. For combining the latter chroma-based classiﬁers with the composite classiﬁer based on global features, we use standard rank aggregation methods adapted from the information retrieval literature. We evaluate performance with the Second Hand Song dataset, where we quantify performance using multiple statistics. We observe that each combination rule outperforms single methods in terms of the total number of identiﬁed queries. Experiments with rank aggregation methods show an increase of up to 23.5 % of the number of identiﬁed queries, compared to single classiﬁers."""
64,Bochen Li;Zhiyao Duan,Score Following for Piano Performances with Sustain-Pedal Effects.,2015,https://doi.org/10.5281/zenodo.1418127,"Bochen Li, University of Rochester, Department of Electrical and Computer Engineering, USA, education;Zhiyao Duan, University of Rochester, Department of Electrical and Computer Engineering, USA, education","One challenge in score following (i.e., mapping audio frames to score positions in real time) for piano performances is the mismatch between audio and score caused by the us- age of the sustain pedal. When the pedal is pressed, notes played will continue to sound until the string vibration nat- urally ceases. This makes the notes longer than their no- tated lengths and overlap with later notes. In this paper, we propose an approach to address this problem. Given that the most competitive wrong score positions for each audio frame are the ones before the correct position due to the sustained sounds, we remove partials of sustained notes and only retain partials of “new notes” in the audio repre- sentation. This operation reduces sustain-pedal effects by weakening the match between the audio frame and previ- ous wrong score positions, hence encourages the system to align to the correct score position. We implement this idea based on a state-of-the-art score following framework. Ex- periments on synthetic and real piano performances from the MAPS dataset show signiﬁcant improvements on both alignment accuracy and robustness."
65,Jin Ha Lee;Rachel Price,Understanding Users of Commercial Music Services through Personas: Design Implications.,2015,https://doi.org/10.5281/zenodo.1418297,"Jin Ha Lee, University of Washington, USA, education;Rachel Price, University of Washington, USA, education","""Most of the previous literature on music users’ needs, 
habits, and interactions with music information retrieval 
(MIR) systems focuses on investigating user groups of 
particular demographics or testing the usability of specif-
ic interfaces/systems. In order to improve our understand-
ing of how users’ personalities and characteristics affect 
their needs and interactions with MIR systems, we con-
ducted a qualitative user study across multiple commer-
cial music services, utilizing interviews and think-aloud 
sessions. Based on the empirical user data, we have de-
veloped seven personas. These personas offer a deeper 
understanding of the different types of MIR system users 
and the relative importance of various design implications 
for each user type. Implications for system design include 
a renegotiation of our understanding of desired user en-
gagement, especially with the habit of context-switching, 
designing systems for specialized uses, and addressing 
user concerns around privacy, transparency, and control."""
66,Hendrik Vincent Koops;Anja Volk;W. Bas de Haas,Corpus-Based Rhythmic Pattern Analysis of Ragtime Syncopation.,2015,https://doi.org/10.5281/zenodo.1417213,"Hendrik Vincent Koops, Utrecht University, NLD, education;Anja Volk, Utrecht University, NLD, education;W. Bas de Haas, Utrecht University, NLD, education","This paper presents a corpus-based study on rhythmic patterns in the RAG-collection of approximately 11.000 symbolically encoded ragtime pieces. While characteristic musical features that define ragtime as a genre have been debated since its inception, musicologists argue that specific syncopation patterns are most typical for this genre. Therefore, we investigate the use of syncopation patterns in the RAG-collection from its beginnings until the present time in this paper. Using computational methods, this paper provides an overview on the use of rhythmical patterns of the ragtime genre, thereby offering valuable new insights that complement musicological hypotheses about this genre. Specifically, we measure the amount of syncopation for each bar using Longuet-Higgins and Lee’s model of syncopation, determine the most frequent rhythmic patterns, and discuss the role of a specific short-long-short syncopation pattern that musicologists argue is characteristic for ragtime. A comparison between the ragtime (pre-1920) and modern (post-1920) era shows that the two eras differ in syncopation pattern use. Onset density and amount of syncopation increase after 1920. Moreover, our study confirms the musicological hypothesis on the important role of the short-long-short syncopation pattern in ragtime. These findings are pivotal in developing ragtime genre-specific features."
67,Nicolas Guiomard-Kagan;Mathieu Giraud;Richard Groult;Florence Levé,Comparing Voice and Stream Segmentation Algorithms.,2015,https://doi.org/10.5281/zenodo.1414916,"Nicolas Guiomard-Kagan, MIS, U. Picardie Jules Verne, FRA, education;Mathieu Giraud, CRIStAL (CNRS, U. Lille), FRA, education;Richard Groult, MIS, U. Picardie Jules Verne (UPJV), FRA, education;Florence Levé, MIS, U. Picardie Jules Verne (UPJV), FRA, education","Voice and stream segmentation algorithms group notes from polyphonic data into relevant units, providing a bet- ter understanding of a musical score. Voice segmentation algorithms usually extract voices from the beginning to the end of the piece, whereas stream segmentation algorithms identify smaller segments. In both cases, the goal can be to obtain mostly monophonic units, but streams with poly- phonic data are also relevant. These algorithms usually cluster contiguous notes with close pitches. We propose an independent evaluation of four of these algorithms (Tem- perley, Chew and Wu, Ishigaki et al., and Rafailidis et al.) using several evaluation metrics. We benchmark the al- gorithms on a corpus containing the 48 fugues of Well- Tempered Clavier by J. S. Bach as well as 97 ﬁles of pop- ular music containing actual polyphonic information. We discuss how to compare together voice and stream segmen- tation algorithms, and discuss their strengths and weak- nesses."
68,Rachel M. Bittner;Justin Salamon;Slim Essid;Juan Pablo Bello,Melody Extraction by Contour Classification.,2015,https://doi.org/10.5281/zenodo.1416322,"Rachel M. Bittner, Music and Audio Research Lab, New York University, USA, facility;Justin Salamon, Music and Audio Research Lab, New York University, USA, facility; Center for Urban Science and Progress, New York University, USA, facility;Slim Essid, Télécom Paris-Tech, FRA, education;Juan P. Bello, Music and Audio Research Lab, New York University, USA, facility","Due to the scarcity of labeled data, most melody extraction algorithms do not rely on fully data-driven processing blocks but rather on careful engineering. For example, the Melodia melody extraction algorithm employs a pitch contour selection stage that relies on a number of heuristics for selecting the melodic output. In this paper we explore the use of a discriminative model to perform purely data-driven melodic contour selection. Specifically, a discriminative binary classifier is trained to distinguish melodic from non-melodic contours. This classifier is then used to predict likelihoods for a track’s extracted contours, and these scores are decoded to generate a single melody output. The results are compared with the Melodia algorithm and with a generative model used in a previous study. We show that the discriminative model outperforms the generative model in terms of contour classification accuracy, and the melody output from our proposed system performs comparatively to Melodia. The results are complemented with error analysis and avenues for future improvements."
69,Rafael Caro Repetto;Rong Gong;Nadine Kroher;Xavier Serra,Comparison of the Singing Style of Two Jingju Schools.,2015,https://doi.org/10.5281/zenodo.1416692,"Rafael Caro Repetto, Universitat Pompeu Fabra, ESP, education;Rong Gong, Universitat Pompeu Fabra, ESP, education;Nadine Kroher, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Performing schools (liupai) in jingju (also known as Pe-
king or Beijing opera) are one of the most important ele-
ments for the appreciation of this genre among connois-
seurs. In the current paper, we study the potential of MIR 
techniques for supporting and enhancing musicological 
descriptions of the singing style of two of the most re-
nowned jingju schools for the dan role-type, namely Mei 
and Cheng schools. To this aim, from the characteristics 
commonly used for describing singing style in musico-
logical literature, we have selected those that can be stud-
ied using standard audio features. We have selected eight 
recordings from our jingju music research corpus and 
have applied current algorithms for the measurement of 
the selected features. Obtained results support the de-
scriptions from musicological sources in all cases but 
one, and also add precision to them by providing specific 
measurements. Besides, our methodology suggests some 
characteristics not accounted for in our musicological 
sources. Finally, we discuss the need for engaging jingju 
experts in our future research and applying this approach 
for musicological and educational purposes as a way of 
better validating our methodology."
70,Victor Padilla;Alex McLean;Alan Marsden;Kia Ng,Improving Optical Music Recognition by Combining Outputs from Multiple Sources.,2015,https://doi.org/10.5281/zenodo.1416258,"Victor Padilla, Lancaster University, GBR, education;Alex McLean, University of Leeds, GBR, education;Alan Marsden, Lancaster University, GBR, education;Kia Ng, University of Leeds, GBR, education","Current software for Optical Music Recognition (OMR) produces outputs with too many errors that render it an unrealistic option for the production of a large corpus of symbolic music files. In this paper, we propose a system which applies image pre-processing techniques to scans of scores and combines the outputs of different commercial OMR programs when applied to images of different scores of the same piece of music. As a result of this procedure, the combined output has around 50% fewer errors when compared to the output of any one OMR program. Image pre-processing splits scores into separate movements and sections and removes ossia staves which confuse OMR software. Post-processing aligns the outputs from different OMR programs and from different sources, rejecting outputs with the most errors and using majority voting to determine the likely correct details. Our software produces output in MusicXML, concentrating on accurate pitch and rhythm and ignoring grace notes. Results of tests on the six string quartets by Mozart dedicated to Joseph Haydn and the first six piano sonatas by Mozart are presented, showing an average recognition rate of around 95%."
71,Richard F. E. Sutcliffe;Tim Crawford;Chris Fox;Deane L. Root;Eduard H. Hovy;Richard Lewis 0001,Relating Natural Language Text to Musical Passages.,2015,https://doi.org/10.5281/zenodo.1415270,"Richard Sutcliffe, University of Essex, UK, education;Tim Crawford, Goldsmiths, University of London, UK, education;Chris Fox, University of Essex, UK, education;Deane L. Root, University of Pittsburgh, USA, education;Eduard Hovy, Carnegie-Mellon Univ, USA, education;Richard Lewis, Goldsmiths, University of London, UK, education","""There is a vast body of musicological literature containing detailed analyses of musical works. These texts make frequent references to musical passages in scores by means of natural language phrases. Our long-term aim is to investigate whether these phrases can be linked automatically to the musical passages to which they refer. As a first step, we have organised for two years running a shared evaluation in which participants must develop software to identify passages in a MusicXML score based on a short noun phrase in English. In this paper, we present the rationale for this work, discuss the kind of references to musical passages which can occur in actual scholarly texts, describe the first two years of the evaluation and finally appraise the results to establish what progress we have made."""
72,Thomas Grill;Jan Schlüter,Music Boundary Detection Using Neural Networks on Combined Features and Two-Level Annotations.,2015,https://doi.org/10.5281/zenodo.1417461,"Thomas Grill, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility;Jan Schlüter, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, facility","The determination of structural boundaries is a key task for understanding the structure of a musical piece, but it is also highly ambiguous. Recently, Convolutional Neural Networks (CNN) trained on spectrogram features and human annotations have been successfully used to tackle the problem, but still fall clearly behind human performance. We expand on the CNN approach by combining spectrograms with self-similarity lag matrices as audio features, thereby capturing more facets of the underlying structural information. Furthermore, in order to consider the hierarchical nature of structural organization, we explore different strategies to learn from the two-level annotations of main and secondary boundaries available in the SALAMI structural annotation dataset. We show that both measures improve boundary recognition performance, resulting in a significant improvement over the previous state of the art. As a side-effect, our algorithm can predict boundaries on two different structural levels, equivalent to the training data."
73,Blair Kaneshiro;Jacek P. Dmochowski,Neuroimaging Methods for Music Information Retrieval: Current Findings and Future Prospects.,2015,https://doi.org/10.5281/zenodo.1416082,"Blair Kaneshiro, Center for Computer Research in Music and Acoustics, Stanford University, USA, education;Jacek P. Dmochowski, Department of Psychology, Stanford University, USA, education","Over the past decade and a half, music information re-
trieval (MIR) has grown into a robust, cross-disciplinary
ﬁeld spanning a variety of research domains.
Collabo-
rations between MIR and neuroscience researchers, how-
ever, are still rare, and to date only a few studies using
approaches from one domain have successfully reached
an audience in the other. In this paper, we take an initial
step toward bridging these two ﬁelds by reviewing studies
from the music neuroscience literature, with an emphasis
on imaging modalities and analysis techniques that might
be of practical interest to the MIR community. We show
that certain approaches currently used in a neuroscientiﬁc
setting align with those used in MIR research, and discuss
implications for potential areas of future research. We ad-
ditionally consider the impact of disparate research objec-
tives between the two ﬁelds, and how such a discrepancy
may have hindered cross-discipline output thus far. It is
hoped that a heightened awareness of this literature will
foster interaction and collaboration between MIR and neu-
roscience researchers, leading to advances in both ﬁelds
that would not have been achieved independently."
74,Arthur Flexer,Improving Visualization of High-Dimensional Music Similarity Spaces.,2015,https://doi.org/10.5281/zenodo.1416472,"Arthur Flexer, Austrian Research Institute for Artificial Intelligence, AUT, facility",Visualizations of music databases are a popular form of interface allowing intuitive exploration of music catalogs. They are often based on lower dimensional projections of high dimensional music similarity spaces. Such similarity spaces have already been shown to be negatively impacted by so-called hubs and anti-hubs. These are points that appear very close or very far to many other data points due to a problem of measuring distances in high-dimensional spaces. We present an empirical study on how this phenomenon impacts three popular approaches to compute two-dimensional visualizations of music databases. We also show how the negative impact of hubs and anti-hubs can be reduced by re-scaling the high dimensional spaces before low dimensional projection.
75,Hamid Eghbal-zadeh;Bernhard Lehner;Markus Schedl;Gerhard Widmer,I-Vectors for Timbre-Based Music Similarity and Music Artist Classification.,2015,https://doi.org/10.5281/zenodo.1416762,"Hamid Eghbal-zadeh, Johannes Kepler University of Linz, AUT, education;Bernhard Lehner, Johannes Kepler University of Linz, AUT, education;Markus Schedl, Johannes Kepler University of Linz, AUT, education;Gerhard Widmer, Johannes Kepler University of Linz, AUT, education","""In this paper, we present a novel approach to extract song-level descriptors built from frame-level timbral features such as Mel-frequency cepstral coefﬁcient (MFCC). These descriptors are called identity vectors or i-vectors and are the results of a factor analysis procedure applied on frame-level features. The i-vectors provide a low-dimensional and ﬁxed-length representation for each song and can be used in a supervised and unsupervised manner. First, we use the i-vectors for an unsupervised music similarity estimation, where we calculate the distance between i-vectors in order to predict the genre of songs. Second, for a supervised artist classiﬁcation task we report the performance measures using multiple classiﬁers trained on the i-vectors. Standard datasets for each task are used to evaluate our method and the results are compared with the state of the art. By only using timbral information, we already achieved the state of the art performance in music similar- ity (which uses extra information such as rhythm). In artist classiﬁcation using timbre descriptors, our method outper- formed the state of the art."""
76,Dylan Freedman;Eddie Kohler;Hans Tutschku,Correlating Extracted and Ground-Truth Harmonic Data in Music Retrieval Tasks.,2015,https://doi.org/10.5281/zenodo.1418111,"Dylan Freedman, Harvard University, USA, education;Eddie Kohler, Harvard University, USA, education;Hans Tutschku, Harvard University, USA, education","""We show that traditional music information retrieval tasks with well-chosen parameters perform similarly using computationally extracted chord annotations and ground-truth annotations. Using a collection of Billboard songs with provided ground-truth chord labels, we use established chord identification algorithms to produce a corresponding extracted chord label dataset. We implement methods to compare chord progressions between two songs on the basis of their optimal local alignment scores. We create a set of chord progression comparison parameters defined by chord distance metrics, gap costs, and normalization measures and run a black-box global optimization algorithm to stochastically search for the best parameter set to maximize the rank correlation for two harmonic retrieval tasks across the ground-truth and extracted chord Billboard datasets. The first task evaluates chord progression similarity between all pairwise combinations of songs, separately ranks results for ground-truth and extracted chord labels, and returns a rank correlation coefficient. The second task queries the set of songs with fabricated chord progressions, ranks each query’s results across ground-truth and extracted chord labels, and returns rank correlations. The end results suggest that practical retrieval systems can be constructed to work effectively without the guide of human ground-truthing."""
77,Martin Gasser;Andreas Arzt;Thassilo Gadermaier;Maarten Grachten;Gerhard Widmer,Classical Music on the Web - User Interfaces and Data Representations.,2015,https://doi.org/10.5281/zenodo.1417717,"Martin Gasser, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility, Johannes Kepler Universität, AUT, education;Andreas Arzt, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility, Johannes Kepler Universität, AUT, education;Thassilo Gadermaier, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility, Johannes Kepler Universität, AUT, education;Maarten Grachten, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility, Johannes Kepler Universität, AUT, education;Gerhard Widmer, Austrian Research Institute for Artificial Intelligence (OFAI), AUT, facility, Johannes Kepler Universität, AUT, education","We present a set of web-based user interfaces for explorative analysis and visualization of classical orchestral music and a web API that serves as a backend to those applications; we describe use cases that motivated our developments within the PHENICX project, which promotes a vital interaction between Music Information Retrieval research groups and a world-renowned symphony orchestra."
78,Mutian Fu;Guangyu Xia;Roger B. Dannenberg;Larry A. Wasserman,A Statistical View on the Expressive Timing of Piano Rolled Chords.,2015,https://doi.org/10.5281/zenodo.1417034,"Mutian Fu, School of Music, Carnegie Mellon University, USA, education;Guangyu Xia, School of Computer Science, Carnegie Mellon University, USA, education;Roger Dannenberg, School of Computer Science, Carnegie Mellon University, USA, education;Larry Wasserman, School of Computer Science, Carnegie Mellon University, USA, education","Rolled or arpeggiated chords are notated chords performed by playing the notes sequentially, usually from lowest to highest in pitch. Arpeggiation is a characteristic of musical expression, or expressive timing, in piano performance. However, very few studies have investigated rolled chord performance. In this paper, we investigate two expressive timing properties of piano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can functionally replace the onsets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We ask two research questions. First, what is the equivalent onset of a rolled chord? Second, are the onset spans of different chords interpreted in the same way? The first question is answered by local tempo estimation while the second question is answered by Analysis of Variance. Also, we contribute a piano duet dataset for rolled chords analysis and other studies on expressive music performance. The dataset contains three pieces of music, each performed multiple times by different pairs of musicians."
79,Srikanth Cherla;Son N. Tran;Tillman Weyde;Artur S. d'Avila Garcez,Hybrid Long- and Short-Term Models of Folk Melodies.,2015,https://doi.org/10.5281/zenodo.1417315,"Srikanth Cherla, City University London, GBR, education;Son N. Tran, City University London, GBR, education;Tillman Weyde, City University London, GBR, education;Artur d’Avila Garcez, City University London, GBR, education","""In this paper, we present the results of a study on dynamic models for predicting sequences of musical pitch in melodies. Such models predict a probability distribution over the possible values of the next pitch in a sequence, which is obtained by combining the prediction of two components (1) a long-term model (LTM) learned ofﬂine on a corpus of melodies, as well as (2) a short-term model (STM) which incorporates context-speciﬁc information available during prediction. Both the LTM and the STM learn regularities in pitch sequences solely from data. The models are combined in an ensemble, wherein they are weighted by the relative entropies of their respective predictions. Going by previous work that demonstrates the success of Connectionist LTMs, we employ the recently proposed Recurrent Temporal Discriminative Restricted Boltzmann Machine (RTDRBM) as the LTM here. While it is indeed possible for the same model to also serve as an STM, our experiments showed that n-gram models tended to learn faster than the RTDRBM in an online setting and that the hybrid of an RTDRBM LTM and an n-gram STM gives us the best predictive performance yet on a corpus of monophonic chorale and folk melodies."""
80,Kaustuv Kanti Ganguli;Abhinav Rastogi;Vedhas Pandit;Prithvi Kantan;Preeti Rao,Efficient Melodic Query Based Audio Search for Hindustani Vocal Compositions.,2015,https://doi.org/10.5281/zenodo.1417203,"Kaustuv Kanti Ganguli, Indian Institute of Technology Bombay, IND, education;Abhinav Rastogi, Stanford University, USA, education;Vedhas Pandit, Indian Institute of Technology Bombay, IND, education;Prithvi Kantan, Indian Institute of Technology Bombay, IND, education;Preeti Rao, Indian Institute of Technology Bombay, IND, education","Time-series pattern matching methods that incorporate time warping have recently been used with varying degrees of success on tasks of search and discovery of melodic phrases from audio for Indian classical vocal music. While these methods perform effectively due to the minimal assumptions they place on the nature of the sampled pitch temporal trajectories, their practical applicability to retrieval tasks on real-world databases is seriously limited by their prohibitively large computational complexity. While dimensionality reduction of the time-series to discrete symbol strings is a standard approach that can exploit computational gains from the data compression as well as the availability of efficient string matching algorithms, the compressed representation of the pitch time series itself is not well understood given the pervasiveness of pitch inflections in the melodic shape of the raga phrases. We propose methods that are informed by domain knowledge to design the representation and to optimize parameter settings for the subsequent string matching algorithm. The methods are evaluated in the context of an audio query based search for Hindustani vocal compositions in audio recordings via the mukhda (refrain of the song). We present results that demonstrate performance close to that achieved by time-series matching but at orders of magnitude reduction in complexity."
81,Ning Chen;J. Stephen Downie;Haidong Xiao;Yu Zhu;Jie Zhu,Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC) Model for Pop Cover Song Recognition.,2015,https://doi.org/10.5281/zenodo.1416096,"Ning Chen, East China Univ. of Sci. and Tech., CHN, education;J. Stephen Downie, UIUC, USA, education;Haidong Xiao, Shanghai Advanced Research Institute, Chinese Academy of Sciences, CHN, facility;Yu Zhu, East China Univ. of Sci. and Tech., CHN, education;Jie Zhu, Shanghai Jiao Tong University, CHN, education","Most of the features of Cover Song Identiﬁcation (CSI),
for example, Pitch Class Proﬁle (PCP) related features, are
based on the musical facets shared among cover versions:
melody evolution and harmonic progression. In this work,
the perceptual feature was studied for CSI. Our idea was
to modify the Perceptual Linear Prediction (PLP) model in
the ﬁeld of Automatic Speech Recognition (ASR) by (a)
introducing new research achievements in psychophysics,
and (b) considering the difference between speech and
music signals to make it consistent with human hearing
and more suitable for music signal analysis. Furthermore,
the obtained Linear Prediction Coefﬁcients (LPCs) were
mapped to LPC cepstrum coefﬁcients, on which liftering
was applied, to boost the timbre invariance of the resultant
feature: Modiﬁed Perceptual Linear Prediction Liftered
Cepstrum (MPLPLC). Experimental results showed that
both LPC cepstrum coefﬁcients mapping and cepstrum lif-
tering were crucial in ensuring the identiﬁcation power of
the MPLPLC feature. The MPLPLC feature outperformed
state-of-the-art features in the context of CSI and in re-
sisting instrumental accompaniment variation. This study
veriﬁes that the mature techniques in the ASR or Compu-
tational Auditory Scene Analysis (CASA) ﬁelds may be
modiﬁed and included to enhance the performance of the
Music Information Retrieval (MIR) scheme."
82,Shrey Dutta;Krishnaraj Sekhar PV;Hema A. Murthy,Raga Verification in Carnatic Music Using Longest Common Segment Set.,2015,https://doi.org/10.5281/zenodo.1417751,"Shrey Dutta, Indian Institute of Technology, Madras, IND, education;Krishnaraj Sekhar PV, Indian Institute of Technology, Madras, IND, education;Hema A. Murthy, Indian Institute of Technology, Madras, IND, education","There are at least 100 r¯agas that are regularly performed in Carnatic music concerts. The audience determines the identity of r¯agas within a few seconds of listening to an item. Most of the audience consists of people who are only avid listeners and not performers. In this paper, an attempt is made to mimic the listener. A r¯aga veriﬁcation framework is therefore suggested. The r¯aga veriﬁcation system assumes that a speciﬁc r¯aga is claimed based on similarity of movements and motivic pat- terns. The system then checks whether this claimed r¯aga is correct. For every r¯aga, a set of cohorts are chosen. A r¯aga and its cohorts are represented using pallavi lines of com- positions. A novel approach for matching, called Longest Common Segment Set (LCSS), is introduced. The LCSS scores for a r¯aga are then normalized with respect to its cohorts in two different ways. The resulting systems and a baseline system are compared for two partitionings of a dataset. A dataset of 30 r¯agas from Charsur Foundation 1 is used for analysis. An equal error rate (EER) of 12% is obtained."
83,Yucong Jiang;Christopher Raphael,Instrument Identification in Optical Music Recognition.,2015,https://doi.org/10.5281/zenodo.1416116,"Yucong Jiang, Indiana University, Bloomington, USA, education;Christopher Raphael, Indiana University, Bloomington, USA, education","We present a method for recognizing and interpreting the text labels for the instruments in an orchestra score, thereby associating staves with instruments. This task is one of many necessary in optical music recognition. Our approach treats the score system as the basic unit of processing. A graph structure describes the possible orderings of instruments in the system. Each instrument may apply to several staves, may be represented with several possible text strings, and may appear at several possible positions relative to the staves. We ﬁnd the optimal labeling of staves using a globally optimal dynamic programming approach that embeds simple template-based optical character recognition within the overall recognition scheme. When given an entire score, we simultaneously optimize on the text labeling for each system, as well as the character template models, thus adapting to the font at hand. Our implementation alternately optimizes over the text label identiﬁcation and re-estimates the character templates. Experiments are presented on 10 different scores showing a signiﬁcant improvement due to adaptation."
84,Christian Dittmar;Bernhard Lehner;Thomas Prätzlich;Meinard Müller;Gerhard Widmer,Cross-Version Singing Voice Detection in Classical Opera Recordings.,2015,https://doi.org/10.5281/zenodo.1416958,"Christian Dittmar, International Audio Laboratories, Erlangen, Germany, facility;Bernhard Lehner, Johannes Kepler University, Linz, Austria, education;Thomas Pr¨atzlich, International Audio Laboratories, Erlangen, Germany, facility;Meinard M¨uller, International Audio Laboratories, Erlangen, Germany, facility;Gerhard Widmer, Johannes Kepler University, Linz, Austria, education","In the ﬁeld of Music Information Retrieval (MIR), the automated detection of the singing voice within a given music recording constitutes a challenging and important research problem. In this study, our goal is to ﬁnd those segments within a classical opera recording, where one or several singers are active. As our main contributions, we ﬁrst propose a novel audio feature that extends a state-of-the-art feature set that has previously been applied to singing voice detection in popular music recordings. Second, we describe a simple bootstrapping procedure that helps to improve the results in the case that the test data is not reﬂected well by the training data. Third, we show that a cross-version approach can help to stabilize the results even further."
85,Sebastian Böck;Florian Krebs;Gerhard Widmer,Accurate Tempo Estimation Based on Recurrent Neural Networks and Resonating Comb Filters.,2015,https://doi.org/10.5281/zenodo.1416026,"Sebastian Böck, Johannes Kepler University, Linz, Austria, education;Florian Krebs, Johannes Kepler University, Linz, Austria, education;Gerhard Widmer, Johannes Kepler University, Linz, Austria, education","""In this paper we present a new tempo estimation algorithm which uses a bank of resonating comb ﬁlters to determine the dominant periodicity of a musical excerpt. Unlike existing (comb ﬁlter based) approaches, we do not use hand-crafted features derived from the audio signal, but rather let a recurrent neural network learn an intermediate beat-level representation of the signal and use this information as input to the comb ﬁlter bank. While most approaches apply complex post-processing to the output of the comb ﬁlter bank like tracking multiple time scales, processing different accent bands, modelling metrical relations, categorising the excerpts into slow / fast or any other advanced processing, we achieve state-of-the-art performance on nine of ten datasets by simply reporting the highest resonator’s histogram peak."""
86,Erik Duval;Marnix van Berchum;Anja Jentzsch;Gonzalo Alberto Parra Chico;Andreas Drakos,Musicology of Early Music with Europeana Tools and Services.,2015,https://doi.org/10.5281/zenodo.1417509,"Erik Duval, KU Leuven, BEL, education;Marnix van Berchum, KNAW-DANS, Utrecht University, NLD, education;Anja Jentzsch, Open Knowledge Foundation, DEU, facility;Gonzalo Alberto Parra Chico, KU Leuven, BEL, education;Andreas Drakos, AgroKnow, GRC, company","The Europeana repository hosts large collections of digitized music manuscripts and prints. This paper investigates how tools and services for this repository can enable Early Music musicologists to carry out their research in a more effective or efficient way, or to carry out research that is impossible to do without such tools or services. We report on the methodology, user-centered development of a suite of tools that we have integrated loosely, in order to experiment with this specific target audience and an evaluation of the impact that such tools may have on how these musicologists carry out their research. Positive feedback relates to the automation of data sharing between the loosely coupled tools and support for an integrated workflow. Participants in this study wanted to have the ability to work not only with individual items, but also with collections of such items. The use of search facets to filter, and visualization around time and place were positively evaluated, as was the use of Optical Music Recognition and computer-supported analysis of music scores. The musicologists were not convinced of the value of activity streams. They also wanted a less strictly linear organization of their workflow and the ability to not only consume items from the repository, but to also push their research results back into the Europeana repository."
87,Hye-Seung Cho;Jun-Yong Lee;Hyoung-Gook Kim,Singing Voice Separation from Monaural Music Based on Kernel Back-Fitting Using Beta-Order Spectral Amplitude Estimation.,2015,https://doi.org/10.5281/zenodo.1417044,"Hye-Seung Cho, Kwangwoon University, KOR, education;Jun-Yong Lee, Kwangwoon University, KOR, education;Hyoung-Gook Kim, Kwangwoon University, KOR, education","""Separating the leading singing voice from the musical background from a monaural recording is a challenging task that appears naturally in several music processing applications. Recently, kernel additive modeling with generalized spatial Wiener filtering (GW) was presented for music/voice separation. In this paper, an adaptive auditory filtering based on β-order minimum mean-square error spectral amplitude estimation (bSA) is applied to the kernel additive modeling for improving the singing voice separation performance from monaural music signal. The proposed algorithm is composed of five modules: short time Fourier transform, music/voice separation based on bSA, determination of back-fitting, back-fitting, and inverse short time Fourier transform. In the proposed method, the Singular Value Decomposition (SVD)-based factorized spectral amplitude exponent β for each kernel component is adaptively calculated for effective bSA-based auditory filtering performance during kernel back-fitting. Using a back-fitting threshold, the kernel back-fitting process can automatically be iteratively performed until convergence. Experimental results show that the proposed method achieves better separation performance than GW based on kernel additive modeling."""
88,Andie Sigler;Jon Wild;Eliot Handelman,Schematizing the Treatment of Dissonance in 16th-Century Counterpoint.,2015,https://doi.org/10.5281/zenodo.1417369,"Andie Sigler, School of Computer Science, McGill University, CAN, education, Computing Music, , facility;Jon Wild, Schulich School of Music, McGill University, CAN, education;Eliot Handelman, Computing Music, , facility","""We describe a computational project concerning labeling of dissonance treatments – schematic descriptions of the uses of dissonances. We use automatic score annotation and database methods to develop schemata for a large corpus of 16th-century polyphonic music. We then apply structural techniques to investigate coincidence of schemata, and to extrapolate from found structures to unused possibilities."""
89,Jotthi Bansal;Matthew Woolhouse,Predictive Power of Personality on Music-Genre Exclusivity.,2015,https://doi.org/10.5281/zenodo.1417467,"Jotthi Bansal, McMaster University, CAN, education;Matthew Woolhouse, McMaster University, CAN, education","Studies reveal a strong relationship between personality and preferred musical genre. Our study explored this relationship using a new methodology: genre dispersion among people’s mobile-phone music collections. By analyzing the download behaviours of genre-deﬁned user subgroups, we investigated the following questions: (1) do genre-preferring subgroups show distinct patterns of genre consumption and genre exclusivity; (2) does genre exclusivity re- late to Big Five personality factors? We hypothesized that genre-preferring subgroups would vary in genre exclusiv- ity, and that their degree of exclusivity would be linearly associated with the openness personality factor (if people have open personalities, they should be “open” to differ- ent musical styles). Consistent with our hypothesis, results showed that greater genre inclusivity, i.e. many genres in people’s music collections, positively associated with openness and (unexpectedly) agreeableness, suggesting that individuals with high openness and agreeableness have wid- er musical tastes than those with low openness and agree- ableness. Our study corroborated previous research link- ing genre preference and personality, and revealed, in a novel way, the predictive power of personality on music- consumption."
90,Berit Janssen;Peter van Kranenburg;Anja Volk,A Comparison of Symbolic Similarity Measures for Finding Occurrences of Melodic Segments.,2015,https://doi.org/10.5281/zenodo.1418357,"Berit Janssen, Meertens Institute, NLD, facility;Peter van Kranenburg, Meertens Institute, NLD, facility;Anja Volk, Utrecht University, NLD, education","""To ﬁnd occurrences of melodic segments, such as themes, phrases and motifs, in musical works, a well-performing similarity measure is needed to support human analysis of large music corpora. We evaluate the performance of a range of melodic similarity measures to ﬁnd occurrences of phrases in folk song melodies. We compare the similar- ity measures correlation distance, city-block distance, Eu- clidean distance and alignment, proposed for melody com- parison in computational ethnomusicology; furthermore Implication-Realization structure alignment and B-spline alignment, forming successful approaches in symbolic mel- odic similarity; moreover, wavelet transform and the ge- ometric approach Structure Induction, having performed well in musical pattern discovery. We evaluate the suc- cess of the different similarity measures through observing retrieval success in relation to human annotations. Our re- sults show that local alignment and SIAM perform on an almost equal level to human annotators."""
91,Daniel Gómez-Marín;Sergi Jordà;Perfecto Herrera,PAD and SAD: Two Awareness-Weighted Rhythmic Similarity Distances.,2015,https://doi.org/10.5281/zenodo.1416286,"Daniel Gómez-Marín, Universitat Pompeu Fabra, ESP, education;Sergi Jordà, Universitat Pompeu Fabra, ESP, education;Perfecto Herrera, Universitat Pompeu Fabra, ESP, education","Measuring rhythm similarity is relevant for the analysis and generation of music. Existing similarity metrics tend to consider our perception of rhythms as being in time without discriminating the importance of some regions over others. In a previously reported experiment we observed that measures of similarity may differ given the presence or absence of a pulse inducing sound and the importance of those measures is not constant along the pattern. These results are now reinterpreted by reﬁning the previously proposed metrics. We consider that the perceptual contribution of each beat to the measured similarity is non-homogeneous but might indeed depend on the temporal positions of the beat along the bar. We show that with these improvements, the correlation between the previously evaluated experimental similarity and predictions based on our metrics increases substantially. We conclude by discussing a possible new methodology for evaluating rhythmic similarity between audio loops."
92,Eric J. Humphrey;Juan Pablo Bello,Four Timely Insights on Automatic Chord Estimation.,2015,https://doi.org/10.5281/zenodo.1417549,"Eric J. Humphrey, Music and Audio Research Laboratory, New York University, USA, education, MuseAmi, Inc., USA, company;Juan P. Bello, Music and Audio Research Laboratory, New York University, USA, education","Automatic chord estimation (ACE) is a hallmark research topic in content-based music informatics, but like many other tasks, system performance appears to be converging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite arguably achieving some of the highest results to date, both approaches plateau well short of having solved the problem. Therefore, this work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that invalidate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, standard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conventional approaches conflate the competing goals of recognition and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjective, making the very notion of “ground truth” annotations tenuous. Synthesizing these observations, this paper offers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large."
93,Sankalp Gulati;Joan Serrà;Xavier Serra,Improving Melodic Similarity in Indian Art Music Using Culture-Specific Melodic Characteristics.,2015,https://doi.org/10.5281/zenodo.1418261,"Sankalp Gulati, Universitat Pompeu Fabra, ESP, education;Joan Serr`a, Telefonica Research, ESP, company;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Detecting the occurrences of r¯ags’ characteristic melodic phrases from polyphonic audio recordings is a fundamental task for the analysis and retrieval of Indian art music. We propose an abstraction process and a complexity weighting scheme which improve melodic similarity by exploiting speciﬁc melodic characteristics in this music. In addition, we propose a tetrachord normalization to handle transposed phrase occurrences. The melodic abstraction is based on the partial transcription of the steady regions in the melody, followed by a duration truncation step. The proposed complexity weighting accounts for the differences in the melodic complexities of the phrases, a crucial aspect known to distinguish phrases in Carnatic music. For evaluation we use over 5 hours of audio data comprising 625 annotated melodic phrases belonging to 10 different phrase categories. Results show that the proposed melodic abstraction and complexity weighting schemes significantly improve the phrase detection accuracy, and that tetrachord normalization is a successful strategy for dealing with transposed phrase occurrences in Carnatic music. In the future, it would be worthwhile to explore the applicability of the proposed approach to other melody dominant music traditions such as Flamenco, Beijing opera and Turkish Makam music."
94,Georgi Dzhambazov;Sertan Sentürk;Xavier Serra,Searching Lyrical Phrases in A-Capella Turkish Makam Recordings.,2015,https://doi.org/10.5281/zenodo.1417921,"Georgi Dzhambazov, Universitat Pompeu Fabra, ESP, education;Sertan S¸ent¨urk, Universitat Pompeu Fabra, ESP, education;Xavier Serra, Universitat Pompeu Fabra, ESP, education","Search by lyrics, the problem of locating the exact occurrences of a phrase from lyrics in musical audio, is a recently emerging research topic. Unlike key-phrases in speech, lyrical key-phrases have durations that bear important relation to other musical aspects like the structure of a composition. In this work we propose an approach that address the differences of syllable durations, specific for singing. First a phrase is expanded to MFCC-based phoneme models, trained on speech. Then, we apply dynamic time warping between the phrase and audio to estimate candidate audio segments in the given audio recording. Next, the retrieved audio segments are ranked by means of a novel score-informed hidden Markov model, in which durations of the syllables within a phrase are explicitly modeled. The proposed approach is evaluated on 12 a-capella audio recordings of Turkish Makam music. Relying on standard speech phonetic models, we arrive at promising results that outperform a baseline approach unaware of lyrics durations. To the best of our knowledge, this is the first work tackling the problem of search by lyrical key-phrases. We expect that it can serve as a baseline for further research on singing material with similar musical characteristics."
95,Robert J. Ellis;Zhe Xing;Jiakun Fang;Ye Wang,Quantifying Lexical Novelty in Song Lyrics.,2015,https://doi.org/10.5281/zenodo.1417577,"Robert J Ellis, School of Computing, National University of Singapore, SGP, education;Zhe Xing, School of Computing, National University of Singapore, SGP, education;Jiakun Fang, School of Computing, National University of Singapore, SGP, education;Ye Wang, School of Computing, National University of Singapore, SGP, education","Novelty is an important psychological construct that affects both perceptual and behavioral processes.  Here, we propose a lexical novelty score (LNS) for a song’s lyric, based on the statistical properties of a corpus of 275,905 lyrics (available at www.smcnus.org/lyrics/).  A lyric-level LNS was derived as a function of the inverse document frequencies of its unique words.  An artist-level LNS was then computed using the LNSs of lyrics uniquely associated with each artist.  Statistical tests were performed to determine whether lyrics and artists on Billboard Magazine’s lists of “All-Time Top 100” songs and artists had significantly lower LNSs than “non-top” songs and artists.  An affirmative and highly consistent answer was found in both cases.  These results highlight the potential utility of the LNS as a feature for MIR."
96,Emmanouil Benetos;Tillman Weyde,An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription.,2015,https://doi.org/10.5281/zenodo.1418017,"Emmanouil Benetos, Queen Mary University of London, GBR, education;Tillman Weyde, City University London, GBR, education","In this paper, an efficient, general-purpose model for multiple instrument polyphonic music transcription is proposed. The model is based on probabilistic latent component analysis and supports the use of sound state spectral templates, which represent the temporal evolution of each note (e.g. attack, sustain, decay). As input, a variable-Q transform (VQT) time-frequency representation is used. Computational efficiency is achieved by supporting the use of pre-extracted and pre-shifted sound state templates. Two variants are presented: without temporal constraints and with hidden Markov model-based constraints controlling the appearance of sound states. Experiments are performed on benchmark transcription datasets: MAPS, TRIOS, MIREX multiF0, and Bach10; results on multi-pitch detection and instrument assignment show that the proposed models outperform the state-of-the-art for multiple-instrument transcription and is more than 20 times faster compared to a previous sound state-based model. We finally show that a VQT representation can lead to improved multi-pitch detection performance compared with constant-Q representations."
97,Yuan-Ping Chen;Li Su;Yi-Hsuan Yang,Electric Guitar Playing Technique Detection in Real-World Recording Based on F0 Sequence Pattern Recognition.,2015,https://doi.org/10.5281/zenodo.1414806,"Yuan-Ping Chen, Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan, facility;Li Su, Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan, facility;Yi-Hsuan Yang, Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan, facility","For a complete transcription of a guitar performance, the detection of playing techniques such as bend and vibrato is important, because playing techniques suggest how the melody is interpreted through the manipulation of the guitar strings. While existing work mostly focused on playing technique detection for individual single notes, this paper attempts to expand this endeavor to recordings of guitar solo tracks. Specifically, we treat the task as a time sequence pattern recognition problem, and develop a two-stage framework for detecting five fundamental playing techniques used by the electric guitar. Given an audio track, the first stage identifies prominent candidates by analyzing the extracted melody contour, and the second stage applies a pre-trained classifier to the candidates for playing technique detection using a set of timbre and pitch features. The effectiveness of the proposed framework is validated on a new dataset comprising of 42 electric guitar solo tracks without accompaniment, each of which covers 10 to 25 notes. The best average F-score achieves 74% in two-fold cross validation. Furthermore, we also evaluate the performance of the proposed framework for bend detection in five studio mixtures, to discuss how it can be applied in transcribing real-world electric guitar solos with accompaniment."
98,Phillip B. Kirlin;David L. Thomas,Extending a Model of Monophonic Hierarchical Music Analysis to Homophony.,2015,https://doi.org/10.5281/zenodo.1416296,"Phillip B. Kirlin, Rhodes College, USA, education;David L. Thomas, Rhodes College, USA, education","Computers are now powerful enough and data sets large enough to enable completely data-driven studies of Schenkerian analysis, the most well-established variety of hierarchical music analysis. In particular, we now have probabilistic models that can be trained via machine learn- ing algorithms to analyze music in a hierarchical fashion as a music theorist would. Most of these models, however, only analyze the monophonic melodic content of the mu- sic, as opposed to taking all of the musical voices into ac- count. In this paper, we explore the feasibility of extending a probabilistic model developed for analyzing monophonic music to function with homophonic music. We present de- tails of the new model, an algorithm for determining the most probable analysis of the music, and a number of ex- periments evaluating the quality of the analyses predicted by the model. We also describe how varying the way the model interprets rests in the input music affects the result- ing analyses produced."
99,Emmanuel Deruty;François Pachet,The MIR Perspective on the Evolution of Dynamics in Mainstream Music.,2015,https://doi.org/10.5281/zenodo.1417995,"Emmanuel Deruty, Sony Computer Science Laboratory / Akoustic Arts, FRA, company;François Pachet, Sony Computer Science Laboratory, FRA, company","Understanding the evolution of mainstream music is of 
high interest for the music production industry. In this 
context, we argue that a MIR perspective may be used to 
highlight, in particular, relations between dynamics and 
various properties of mainstream music. We illustrate this 
claim with two results obtained from a diachronic 
analysis performed on 7200 tracks released between 1967 
and 2014. This analysis suggests that 1) the so-called 
“loudness war” has peaked in 2007, and 2) its influence 
has been important enough to override the impact of 
genre on dynamics. In other words, dynamics in 
mainstream music are primarily related to a track’s year 
of release, rather than to its genre."
100,Johanna Devaney;Claire Arthur;Nathaniel Condit-Schultz;Kirsten Nisula,Theme And Variation Encodings with Roman Numerals (TAVERN): A New Data Set for Symbolic Music Analysis.,2015,https://doi.org/10.5281/zenodo.1417497,"Johanna Devaney, School of Music, Ohio State University, USA, education;Claire Arthur, School of Music, Ohio State University, USA, education;Nathaniel Condit-Schultz, School of Music, Ohio State University, USA, education;Kirsten Nisula, School of Music, Ohio State University, USA, education","The Theme And Variation Encodings with Roman Numerals (TAVERN) dataset consists of 27 complete sets of theme and variations for piano composed between 1765 and 1810 by Mozart and Beethoven. In these theme and variation sets, comparable harmonic structures are realized in different ways. This facilitates an evaluation of the effectiveness of automatic analysis algorithms in generalizing across different musical textures. The pieces are encoded in standard **kern format, with analyses jointly encoded using an extension to **kern. The harmonic content of the music was analyzed with both Roman numerals and function labels in duplicate by two different expert analyzers. The pieces are divided into musical phrases, allowing for multiple-levels of automatic analysis, including chord labeling and phrase parsing. This paper describes the content of the dataset in detail, including the types of chords represented, and discusses the ways in which the analyzers sometimes disagreed on the lower-level harmonic content (the Roman numerals) while converging at similar high-level structures (the function of the chords within the phrase)."
101,Isabel Barbancho;Lorenzo J. Tardón;Ana M. Barbancho;Mateu Sbert,Benford's Law for Music Analysis.,2015,https://doi.org/10.5281/zenodo.1417012,"Isabel Barbancho, Universidad de Málaga, ESP, education;Lorenzo J. Tardón, Universidad de Málaga, ESP, education;Ana M. Barbancho, Universidad de Málaga, ESP, education;Mateu Sbert, University of Girona, ESP, education","Benford’s law deﬁnes a peculiar distribution of the leading digits of a set of numbers. The behavior is logarithmic, with the leading digit 1 reﬂecting largest probability of occurrence and the remaining ones showing decreasing probabilities of appearance following a logarithmic trend. Many discussions have been carried out about the application of Benford’s law to many different ﬁelds. In this paper, a novel exploitation of Benford’s law for the analysis of audio signals is proposed. Three new audio features based on the evaluation of the degree of agreement of a certain audio dataset to Benford’s law are presented. These new proposed features are succesfully tested in two concrete audio tasks: the detection of artiﬁcially assembled chords and the estimation of the quality of the MIDI conversions."
102,Julio José Carabias-Orti;Francisco J. Rodríguez-Serrano;Pedro Vera-Candeas;Nicolás Ruiz-Reyes;Francisco J. Cañadas-Quesada,An Audio to Score Alignment Framework Using Spectral Factorization and Dynamic Time Warping.,2015,https://doi.org/10.5281/zenodo.1418371,"J.J. Carabias-Orti, Music Technology Group (MTG), Universitat Pompeu Fabra, Spain, education;F.J. Rodriguez-Serrano, Polytechnical School of Linares, Universidad de Jaen, Spain, education;P. Vera-Candeas, Polytechnical School of Linares, Universidad de Jaen, Spain, education;N. Ruiz-Reyes, Polytechnical School of Linares, Universidad de Jaen, Spain, education;F.J. Ca˜nadas-Quesada, Polytechnical School of Linares, Universidad de Jaen, Spain, education","""In this paper, we present an audio to score alignment framework based on spectral factorization and online Dynamic Time Warping (DTW). The proposed framework has two separated stages: preprocessing and alignment. In the ﬁrst stage, we use Non-negative Matrix Factorization (NMF) to learn spectral patterns (i.e. basis functions) associated to each combination of concurrent notes in the score. In the second stage, a low latency signal decomposition method with ﬁxed spectral patterns per combination of notes is used over the magnitude spectrogram of the input signal resulting in a divergence matrix that can be interpreted as the cost of the matching for each combination of notes at each frame. Finally, a Dynamic Time Warping (DTW) approach has been used to ﬁnd the path with the minimum cost and then determine the relation between the performance and the musical score times. Our framework have been evaluated using a dataset of baroque-era pieces and compared to other systems, yielding solid results and performance."""
103,Daniel Matz;Estefanía Cano;Jakob Abeßer,New Sonorities for Early Jazz Recordings Using Sound Source Separation and Automatic Mixing Tools.,2015,https://doi.org/10.5281/zenodo.1416568,"Daniel Matz, University of Applied Sciences, DEU, education;Estefanía Cano, Fraunhofer IDMT, DEU, facility;Jakob Abeßer, Fraunhofer IDMT, DEU, facility","In this paper, a framework for automatic mixing of early jazz recordings is presented. In particular, we propose the use of sound source separation techniques as a pre-processing step of the mixing process. In addition to an initial solo and accompaniment separation step, the proposed mixing framework is composed of six processing blocks: harmonic-percussive separation (HPS), cross-adaptive multi-track scaling (CAMTS), cross-adaptive equalizer (CAEQ), cross-adaptive dynamic spectral panning (CADSP), automatic excitation (AE), and time-frequency selective panning (TFSP). The effects of the different processing steps in the final quality of the mix are evaluated through a listening test procedure. The results show that the desired quality improvements in terms of sound balance, transparency, stereo impression, timbre, and overall impression can be achieved with the proposed framework."
104,Peter Jancovic;Münevver Köküer;Wrena Baptiste,Automatic Transcription of Ornamented Irish Traditional Flute Music Using Hidden Markov Models.,2015,https://doi.org/10.5281/zenodo.1415116,"Peter Jančovič, University of Birmingham, GBR, education;M¨unevver K¨ok¨uer, Birmingham City University, GBR, education;Wrena Baptiste, University of Birmingham, GBR, education","This paper presents an automatic system for note transcription of Irish traditional flute music containing ornamentation. This is a challenging problem due to the soft nature of onsets and short durations of ornaments. The proposed automatic transcription system is based on hidden Markov models, with separate models being built for notes and for single-note ornaments. Mel-frequency cepstral coefficients are employed to represent the acoustic signal. Different setups of parameters in feature extraction and acoustic modelling are explored. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD. The performance of the system is evaluated in terms of the transcription of notes as well as detection of onsets. It is demonstrated that the proposed system can achieve a very good note transcription and onset detection performance. Over 28% relative improvement in terms of the F-measure is achieved for onset detection in comparison to conventional onset detection methods based on signal energy and fundamental frequency."
105,Sebastian Stober;Avital Sternin;Adrian M. Owen;Jessica A. Grahn,Towards Music Imagery Information Retrieval: Introducing the OpenMIIR Dataset of EEG Recordings from Music Perception and Imagination.,2015,https://doi.org/10.5281/zenodo.1416270,"Sebastian Stober, Brain and Mind Institute, Department of Psychology, Western University, CAN, education;Avital Sternin, Brain and Mind Institute, Department of Psychology, Western University, CAN, education;Adrian M. Owen, Brain and Mind Institute, Department of Psychology, Western University, CAN, education;Jessica A. Grahn, Brain and Mind Institute, Department of Psychology, Western University, CAN, education","Music imagery information retrieval (MIIR) systems may one day be able to recognize a song from only our thoughts. As a step towards such technology, we are presenting a public domain dataset of electroencephalography (EEG) recordings taken during music perception and imagination. We acquired this data during an ongoing study that so far comprises 10 subjects listening to and imagining 12 short music fragments – each 7–16s long – taken from well-known pieces. These stimuli were selected from different genres and systematically vary along musical dimensions such as meter, tempo and the presence of lyrics. This way, various retrieval scenarios can be addressed and the success of classifying based on specific dimensions can be tested. The dataset is aimed to enable music information retrieval researchers interested in these new MIIR challenges to easily test and adapt their existing approaches for music analysis like fingerprinting, beat tracking, or tempo estimation on EEG data."
106,Anna Aljanaki;Frans Wiering;Remco C. Veltkamp,Emotion Based Segmentation of Musical Audio.,2015,https://doi.org/10.5281/zenodo.1418201,"Anna Aljanaki, Utrecht University, NLD, education;Frans Wiering, Utrecht University, NLD, education;Remco C. Veltkamp, Utrecht University, NLD, education","The dominant approach to musical emotion variation detection tracks emotion over time continuously and usually deals with time resolutions of one second. In this paper we discuss the problems associated with this approach and propose to move to bigger time resolutions when tracking emotion over time. We argue that it is more natural from the listener’s point of view to regard emotional variation in music as a progression of emotionally stable segments. In order to enable such tracking of emotion over time it is necessary to segment music at the emotional boundaries. To address this problem we conduct a formal evaluation of different segmentation methods as applied to a task of emotional boundary detection. We collect emotional boundary annotations from three annotators for 52 musical pieces from the RWC music collection that already have structural annotations from the SALAMI dataset. We investigate how well structural segmentation explains emotional segmentation and find that there is a large overlap, though about a quarter of emotional boundaries do not coincide with structural ones. We also study inter-annotator agreement on emotional segmentation. Lastly, we evaluate different unsupervised segmentation methods when applied to emotional boundary detection and find that, in terms of F-measure, the Structural Features method performs best."
107,Jin Ha Lee;Xiao Hu 0001;Kahyun Choi;J. Stephen Downie,MIREX Grand Challenge 2014 User Experience: Qualitative Analysis of User Feedback.,2015,https://doi.org/10.5281/zenodo.1417103,"Jin Ha Lee, University of Washington, USA, education;Xiao Hu, University of Hong Kong, HKG, education;Kahyun Choi, University of Illinois, USA, education;J. Stephen Downie, University of Illinois, USA, education","Evaluation has always been fundamental to the Music Information Retrieval (MIR) community, as evidenced by the popularity of the Music Information Retrieval Evaluation eXchange (MIREX). However, prior MIREX tasks have primarily focused on testing specialized MIR algorithms that sit on the back end of systems. Not until the Grand Challenge 2014 User Experience (GC14UX) task had the users’ overall interaction and experience with complete systems been formally evaluated. Three systems were evaluated based on five criteria. This paper reports the results of GC14UX, with a special focus on the qualitative analysis of 99 free text responses collected from evaluators. The analysis revealed additional user opinions, not fully captured by score ratings on the given criteria, and demonstrated the challenge of evaluating a variety of systems with different user goals. We conclude with a discussion on the implications of findings and recommendations for future UX evaluation tasks, including adding new criteria: Aesthetics, Performance, and Utility."
108,Alastair Porter;Dmitry Bogdanov;Robert Kaye;Roman Tsukanov;Xavier Serra,AcousticBrainz: A Community Platform for Gathering Music Information Obtained from Audio.,2015,https://doi.org/10.5281/zenodo.1414938,"Alastair Porter, Universitat Pompeu Fabra, ESP, education;Dmitry Bogdanov, Universitat Pompeu Fabra, ESP, education;Robert Kaye, MetaBrainz Foundation, , facility;Roman Tsukanov, MetaBrainz Foundation, , facility;Xavier Serra, Universitat Pompeu Fabra, ESP, education","We introduce the AcousticBrainz project, an open platform for gathering music information. At its core, AcousticBrainz is a database of music descriptors computed from audio recordings using a number of state-of-the-art Music Information Retrieval algorithms. Users run a supplied feature extractor on audio files and upload the analysis results to the AcousticBrainz server. All submissions include a MusicBrainz identifier allowing them to be linked to various sources of editorial information. The feature extractor is based on the open source Essentia audio analysis library. From the data submitted by the community, we run classifiers aimed at adding musically relevant semantic information. These classifiers can be developed by the community using tools available on the AcousticBrainz website. All data in AcousticBrainz is freely available and can be accessed through the website or API. For AcousticBrainz to be successful we need to have an active community that contributes to and uses this platform, and it is this community that will define the actual uses and applications of its data."
109,Elad Liebman;Peter Stone;Corey N. White,How Music Alters Decision Making - Impact of Music Stimuli on Emotional Classification.,2015,https://doi.org/10.5281/zenodo.1414908,"Elad Liebman, The University of Texas at Austin, USA, education;Peter Stone, The University of Texas at Austin, USA, education;Corey N. White, Syracuse University, USA, education","Numerous studies have demonstrated that mood can affect emotional processing. The goal of this study was to explore which components of the decision process are affected when exposed to music; we do so within the context of a stochastic sequential model of simple decisions, the drift-diffusion model (DDM). In our experiment, participants decided whether words were emotionally positive or negative while listening to music that was chosen to induce positive or negative mood. The behavioral results show that the music manipulation was effective, as participants were biased to label words positive in the positive music condition. The DDM shows that this bias was driven by a change in the starting point of evidence accumulation, which indicates an a priori response bias. In contrast, there was no evidence that music affected how participants evaluated the emotional content of the stimuli. To better understand the correspondence between auditory features and decision-making, we proceeded to study how individual aspects of music affect response patterns. Our results have implications for future studies of the connection between music and mood."
110,Mark S. Melenhorst;Cynthia C. S. Liem,Put the Concert Attendee in the Spotlight. A User-Centered Design and Development Approach for Classical Concert Applications.,2015,https://doi.org/10.5281/zenodo.1416820,"Mark S. Melenhorst, Delft University of Technology, NLD, education;Cynthia C. S. Liem, Delft University of Technology, NLD, education","""As the importance of real-life use cases in the music information retrieval (MIR) field is increasing, so does the importance of understanding user needs. The development of innovative real-life applications that draw on MIR technology requires a user-centered design and development approach that assesses user needs and aligns them with technological and academic ambitions in the MIR domain. In this paper we present such an approach, and apply it to the development of technological applications to enrich classical symphonic concerts. A user-driven approach is particularly important in this area, as orchestras need to innovate the concert experience to meet the needs and expectations of younger generations without alienating the current audience. We illustrate this approach with the results of five focus groups for three audience segments, which allow us to formulate informed user requirements for classical concert applications."""
111,Pei-Ching Li;Li Su;Yi-Hsuan Yang;Alvin W. Y. Su,Analysis of Expressive Musical Terms in Violin Using Score-Informed and Expression-Based Audio Features.,2015,https://doi.org/10.5281/zenodo.1416784,"Pei-Ching Li, National Cheng-Kung University, TWN, education;Li Su, Academia Sinica, TWN, facility;Yi-Hsuan Yang, Academia Sinica, TWN, facility;Alvin W. Y. Su, National Cheng-Kung University, TWN, education","The manipulation of different interpretational factors, including dynamics, duration, and vibrato, constitutes the realization of different expressions in music. Therefore, a deeper understanding of the workings of these factors is critical for advanced expressive synthesis and computer-aided music education. In this paper, we propose the novel task of automatic expressive musical term classification as a direct means to study the interpretational factors. Specifically, we consider up to 10 expressive musical terms, such as Scherzando and Tranquillo, and compile a new dataset of solo violin excerpts featuring the realization of different expressive terms by different musicians for the same set of classical music pieces. Under a score-informed scheme, we design and evaluate a number of note-level features characterizing the interpretational aspects of music for the classification task. Our evaluation shows that the proposed features lead to significantly higher classification accuracy than a baseline feature set commonly used in music information retrieval tasks. Moreover, taking the contrast of feature values between an expressive and its corresponding non-expressive version (if given) of a music piece greatly improves the accuracy in classifying the presented expressive one. We also draw insights from analyzing the feature relevance and the class-wise accuracy of the prediction."
112,Guangyu Xia;Yun Wang;Roger B. Dannenberg;Geoffrey Gordon,Spectral Learning for Expressive Interactive Ensemble Music Performance.,2015,https://doi.org/10.5281/zenodo.1415806,"Guangyu Xia, School of Computer Science, Carnegie Mellon University, USA, education;Yun Wang, School of Computer Science, Carnegie Mellon University, USA, education;Roger Dannenberg, School of Computer Science, Carnegie Mellon University, USA, education;Geoffrey Gordon, School of Computer Science, Carnegie Mellon University, USA, education","""We apply machine learning to a database of recorded ensemble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers’ musical expression as co-evolving time series and learn their interactive relationship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspondence not only between different performers but also between the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to generate a more human-like interaction."""
113,Jakob Abeßer;Estefanía Cano;Klaus Frieler;Martin Pfleiderer;Wolf-Georg Zaddach,Score-Informed Analysis of Intonation and Pitch Modulation in Jazz Solos.,2015,https://doi.org/10.5281/zenodo.1416836,"Jakob Abeßer, University of Music Franz Liszt, Weimar, Germany, education, Fraunhofer IDMT, Ilmenau, Germany, facility;Estefanía Cano, Fraunhofer IDMT, Ilmenau, Germany, facility;Klaus Frieler, University of Music Franz Liszt, Weimar, Germany, education;Martin Pﬂeiderer, University of Music Franz Liszt, Weimar, Germany, education;Wolf-Georg Zaddach, University of Music Franz Liszt, Weimar, Germany, education","The paper presents new approaches for analyzing the characteristics of intonation and pitch modulation of woodwind and brass solos in jazz recordings. To this end, we use score-informed analysis techniques for source separation and fundamental frequency tracking. After splitting the audio into a solo and a backing track, a reference tuning frequency is estimated from the backing track. Next, we compute the fundamental frequency contour for each tone in the solo and a set of features describing its temporal shape. Based on this data, we first investigate, whether the tuning frequencies of jazz recordings changed over the decades of the last century. Second, we analyze whether the intonation is artist-specific. Finally, we examine how the modulation frequency of vibrato tones depends on contextual parameters such as pitch, duration, and tempo as well as the performing artist."
